{"id": "2509.15248", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.15248", "abs": "https://arxiv.org/abs/2509.15248", "authors": ["Zitong Yang", "Aonan Zhang", "Hong Liu", "Tatsunori Hashimoto", "Emmanuel Candès", "Chong Wang", "Ruoming Pang"], "title": "Synthetic bootstrapped pretraining", "comment": null, "summary": "We introduce Synthetic Bootstrapped Pretraining (SBP), a language model (LM)\npretraining procedure that first learns a model of relations between documents\nfrom the pretraining dataset and then leverages it to synthesize a vast new\ncorpus for joint training. While the standard pretraining teaches LMs to learn\ncausal correlations among tokens within a single document, it is not designed\nto efficiently model the rich, learnable inter-document correlations that can\npotentially lead to better performance. We validate SBP by designing a\ncompute-matched pretraining setup and pretrain a 3B-parameter model on up to 1T\ntokens from scratch. We find SBP consistently improves upon a strong repetition\nbaseline and delivers a significant fraction of performance improvement\nattainable by an oracle upper bound with access to 20x more unique data.\nQualitative analysis reveals that the synthesized documents go beyond mere\nparaphrases -- SBP first abstracts a core concept from the seed material and\nthen crafts a new narration on top of it. Besides strong empirical performance,\nSBP admits a natural Bayesian interpretation: the synthesizer implicitly learns\nto abstract the latent concepts shared between related documents.", "AI": {"tldr": "本研究表明，通过提出SBP，可以有效提高语言模型在建模文档间复杂关系上的能力，实验证明，在控制计算资源一致的情况下，SBP可以达成显著的性能提升。", "motivation": "SBP的动机是解决标准预训练未能有效建模文档间复杂关系的问题，从而提升模型的性能。", "method": "我们提出了Synthetic Bootstrapped Pretraining (SBP)，这是一种语言模型预训练过程，首先从预训练数据集中学习文档之间的关系模型，然后利用该模型合成一个巨大的新语料库进行联合训练。标准预训练主要教授语言模型学习单个文档内tokens间的因果关联，而SBP旨在有效建模文档间丰富的可学习关联。", "result": "实验结果表明，SBP在控制计算资源一致的情况下，预训练了一个30亿参数的从零开始的模型，用到了多达1万亿token的数据。SBP相对于重复基线有一致的改进，并实现了具有访问20倍更多独立数据的oracle上限可达到性能提升的一大部分。", "conclusion": "定性分析揭示，合成文档不仅限于简单的释义，SBP首先从种子材料中提炼出核心概念，然后在此基础上创作新的叙述。SBP不仅表现出色，而且可自然地解释为贝叶斯模型：合成器隐式地学习抽取相关文档间共享的潜在概念。"}}
{"id": "2509.15255", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.15255", "abs": "https://arxiv.org/abs/2509.15255", "authors": ["Tandin Wangchuk", "Tad Gonsalves"], "title": "Comparative Analysis of Tokenization Algorithms for Low-Resource Language Dzongkha", "comment": "10 Pages", "summary": "Large Language Models (LLMs) are gaining popularity and improving rapidly.\nTokenizers are crucial components of natural language processing, especially\nfor LLMs. Tokenizers break down input text into tokens that models can easily\nprocess while ensuring the text is accurately represented, capturing its\nmeaning and structure. Effective tokenizers enhance the capabilities of LLMs by\nimproving a model's understanding of context and semantics, ultimately leading\nto better performance in various downstream tasks, such as translation,\nclassification, sentiment analysis, and text generation. Most pre-trained\ntokenizers are suitable for high-resource languages like English but perform\npoorly for low-resource languages. Dzongkha, Bhutan's national language spoken\nby around seven hundred thousand people, is a low-resource language, and its\nlinguistic complexity poses unique NLP challenges. Despite some progress,\nsignificant research in Dzongkha NLP is lacking, particularly in tokenization.\nThis study evaluates the training and performance of three common tokenization\nalgorithms in comparison to other popular methods. Specifically, Byte-Pair\nEncoding (BPE), WordPiece, and SentencePiece (Unigram) were evaluated for their\nsuitability for Dzongkha. Performance was assessed using metrics like Subword\nFertility, Proportion of Continued Words, Normalized Sequence Length, and\nexecution time. The results show that while all three algorithms demonstrate\npotential, SentencePiece is the most effective for Dzongkha tokenization,\npaving the way for further NLP advancements. This underscores the need for\ntailored approaches for low-resource languages and ongoing research. In this\nstudy, we presented three tokenization algorithms for Dzongkha, paving the way\nfor building Dzongkha Large Language Models.", "AI": {"tldr": "文章评估了三种分词算法在不丹语言Dzongkha上的性能并发现SentencePiece算法为最优，强调了为资源有限的语言进行定制化NLP研究的重要性。", "motivation": "由于宗喀语（不丹的官方语言，约有70万人使用）是资源有限的语言，其语言复杂性提出了独特的自然语言处理（NLP）挑战。尽管取得了一些进展，但在宗喀语NLP中的分词研究仍然不足。此研究旨在填补这一空白，推动Dzongkha语言模型的发展。", "method": "此研究評估了三种常见的分词算法（Byte-Pair Encoding, WordPiece 和 SentencePiece）在宗喀语中的训练和性能，并将其与其他流行的方法进行比较。性能评估的指标包括子词生育能力、单词连续比例、归一化序列长度以及执行时间。", "result": "研究结果表明，所有三种算法都显示出潜力，但SentencePiece对于宗喀语分词最为有效。", "conclusion": "这项研究强调了为资源有限的语言开发定制方法的需求以及持续研究的重要性，为构建Dzongkha大型语言模型铺平了道路。"}}
{"id": "2509.15260", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.15260", "abs": "https://arxiv.org/abs/2509.15260", "authors": ["Yujia Hu", "Ming Shan Hee", "Preslav Nakov", "Roy Ka-Wei Lee"], "title": "Toxicity Red-Teaming: Benchmarking LLM Safety in Singapore's Low-Resource Languages", "comment": "9 pages, EMNLP 2025", "summary": "The advancement of Large Language Models (LLMs) has transformed natural\nlanguage processing; however, their safety mechanisms remain under-explored in\nlow-resource, multilingual settings. Here, we aim to bridge this gap. In\nparticular, we introduce \\textsf{SGToxicGuard}, a novel dataset and evaluation\nframework for benchmarking LLM safety in Singapore's diverse linguistic\ncontext, including Singlish, Chinese, Malay, and Tamil. SGToxicGuard adopts a\nred-teaming approach to systematically probe LLM vulnerabilities in three\nreal-world scenarios: \\textit{conversation}, \\textit{question-answering}, and\n\\textit{content composition}. We conduct extensive experiments with\nstate-of-the-art multilingual LLMs, and the results uncover critical gaps in\ntheir safety guardrails. By offering actionable insights into cultural\nsensitivity and toxicity mitigation, we lay the foundation for safer and more\ninclusive AI systems in linguistically diverse environments.\\footnote{Link to\nthe dataset: https://github.com/Social-AI-Studio/SGToxicGuard.}\n\\textcolor{red}{Disclaimer: This paper contains sensitive content that may be\ndisturbing to some readers.}", "AI": {"tldr": "本文介绍了SGToxicGuard数据集和评估框架，用于衡量LLM在新加坡多元语言环境中的安全性。", "motivation": "本研究旨在填补LLMs在低资源、多语言环境中的安全性研究空白。", "method": "SGToxicGuard采用红队方法系统地探究了LLM在对话、问答和内容创作三种现实场景下的脆弱性。", "result": "实验结果揭示了先进多语言LLMs在安全护栏方面的关键差距。", "conclusion": "该研究为在语言多样化的环境中创建更安全、更具包容性的AI系统奠定了基础。"}}
{"id": "2509.15335", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.15335", "abs": "https://arxiv.org/abs/2509.15335", "authors": ["Charlott Jakob", "David Harbecke", "Patrick Parschan", "Pia Wenzel Neves", "Vera Schmitt"], "title": "PolBiX: Detecting LLMs' Political Bias in Fact-Checking through X-phemisms", "comment": null, "summary": "Large Language Models are increasingly used in applications requiring\nobjective assessment, which could be compromised by political bias. Many\nstudies found preferences for left-leaning positions in LLMs, but downstream\neffects on tasks like fact-checking remain underexplored. In this study, we\nsystematically investigate political bias through exchanging words with\neuphemisms or dysphemisms in German claims. We construct minimal pairs of\nfactually equivalent claims that differ in political connotation, to assess the\nconsistency of LLMs in classifying them as true or false. We evaluate six LLMs\nand find that, more than political leaning, the presence of judgmental words\nsignificantly influences truthfulness assessment. While a few models show\ntendencies of political bias, this is not mitigated by explicitly calling for\nobjectivism in prompts.", "AI": {"tldr": "研究发现，大语言模型的真实性评估受判断性词语影响比受政治倾向影响更大，且这种偏见难以通过要求客观性的提示来缓解。", "motivation": "虽然许多研究表明LLM存在偏向左翼的观点，但它们在诸如事实核查等任务上受政治偏见影响的情况尚未得到充分探讨。", "method": "通过交换德语声明中的同义词与贬义词来系统地调查政治偏见，构建在政治内涵上有所不同但事实等价的最小配对声明，以评估LLM将其分类为真或假的一致性。", "result": "评估了六个LLM，发现比起政治倾向，判断性词语的存在显著影响真实性评估。有些模型表现出政治偏见，但即使在提示中明确要求客观性也无法缓解这种偏见。", "conclusion": "LLM的真实性评估受到判断性词语的影响比政治倾向更大，且这种偏见难以通过提示中的客观性要求来减轻。"}}
{"id": "2509.15234", "categories": ["cs.CV", "68T07, 68U10, 92C55", "I.2.10; I.2.7"], "pdf": "https://arxiv.org/pdf/2509.15234", "abs": "https://arxiv.org/abs/2509.15234", "authors": ["Hanbin Ko", "Gihun Cho", "Inhyeok Baek", "Donguk Kim", "Joonbeom Koo", "Changi Kim", "Dongheon Lee", "Chang Min Park"], "title": "Exploring the Capabilities of LLM Encoders for Image-Text Retrieval in Chest X-rays", "comment": "24 pages, 2 figures, under review", "summary": "Vision-language pretraining has advanced image-text alignment, yet progress\nin radiology remains constrained by the heterogeneity of clinical reports,\nincluding abbreviations, impression-only notes, and stylistic variability.\nUnlike general-domain settings where more data often leads to better\nperformance, naively scaling to large collections of noisy reports can plateau\nor even degrade model learning. We ask whether large language model (LLM)\nencoders can provide robust clinical representations that transfer across\ndiverse styles and better guide image-text alignment. We introduce LLM2VEC4CXR,\na domain-adapted LLM encoder for chest X-ray reports, and LLM2CLIP4CXR, a\ndual-tower framework that couples this encoder with a vision backbone.\nLLM2VEC4CXR improves clinical text understanding over BERT-based baselines,\nhandles abbreviations and style variation, and achieves strong clinical\nalignment on report-level metrics. LLM2CLIP4CXR leverages these embeddings to\nboost retrieval accuracy and clinically oriented scores, with stronger\ncross-dataset generalization than prior medical CLIP variants. Trained on 1.6M\nCXR studies from public and private sources with heterogeneous and noisy\nreports, our models demonstrate that robustness -- not scale alone -- is the\nkey to effective multimodal learning. We release models to support further\nresearch in medical image-text representation learning.", "AI": {"tldr": "我们通过LLM2VEC4CXR和LLM2CLIP4CXR模型，展示了大规模语言模型在处理异质性和噪音报告时的稳健性对于医学图像-文本对齐的重要性。", "motivation": "尽管视觉-语言预训练在图像-文本对齐上取得了进展，但在放射学中的进展受到临床报告异质性的限制。放射学报告中包括缩写、仅有印象的笔记以及样式上的多变性。与一般领域设定中数据量越大数据性能越好的情况不同，在放射学中盲目地扩大到大量复杂报告可能使模型性能停滞甚至下降。我们想要探究大规模语言模型（LLM）编码器是否能提供稳健的临床表示，以支持跨多样风格的图像-文本对齐。", "method": "我们提出了LLM2VEC4CXR，一种针对胸透报告的领域适应的大语言模型编码器，以及LLM2CLIP4CXR，一个将该编码器与视觉骨干相结合的双塔框架。LLM2VEC4CXR改进了对临床文本的理解，处理缩写和风格差异，并在报告级别的指标上展示了强大的临床一致性。LLM2CLIP4CXR利用这些嵌入式表示提升检索精度和临床导向的得分，展现出比先前的医疗CLIP变体更强的跨数据集泛化能力。", "result": "我们的模型在160万公共和私人来源的胸透研究数据上进行训练，其中包含异质且复杂的报告。结果显示，我们的模型在医学图像-文本表示学习中，稳健性——而非单纯尺度——是实现有效的多模态学习的关键。", "conclusion": "研究表明，使用领域适应的大型语言模型编码器LLM2VEC4CXR能够提升对医疗文本的理解和风格变异的处理能力，同时与视觉骨干结合的框架LLM2CLIP4CXR在图像文本对齐上表现优秀，证明了在医疗图像文本学习中，模型的稳健性是取得优秀效果的关键因素。"}}
{"id": "2509.15339", "categories": ["cs.CL", "68T50", "I.2.7"], "pdf": "https://arxiv.org/pdf/2509.15339", "abs": "https://arxiv.org/abs/2509.15339", "authors": ["Yeongbin Seo", "Dongha Lee", "Jinyoung Yeo"], "title": "Quantifying Self-Awareness of Knowledge in Large Language Models", "comment": null, "summary": "Hallucination prediction in large language models (LLMs) is often interpreted\nas a sign of self-awareness. However, we argue that such performance can arise\nfrom question-side shortcuts rather than true model-side introspection. To\ndisentangle these factors, we propose the Approximate Question-side Effect\n(AQE), which quantifies the contribution of question-awareness. Our analysis\nacross multiple datasets reveals that much of the reported success stems from\nexploiting superficial patterns in questions. We further introduce SCAO\n(Semantic Compression by Answering in One word), a method that enhances the use\nof model-side signals. Experiments show that SCAO achieves strong and\nconsistent performance, particularly in settings with reduced question-side\ncues, highlighting its effectiveness in fostering genuine self-awareness in\nLLMs.", "AI": {"tldr": "本文提出了一种量化问题意识贡献的方法AQE，分析了当前大语言模型的幻觉预测现象，发现这种性能更多是利用问题线索。为了改进这一点，又提出了SCAO，显示了其在促进模型真实自我意识方面的潜力。", "motivation": "目的是解开问题线索和真正模型内部审视之间的因素，以更准确地理解大语言模型中的幻觉预测现象是否真正代表模型的自我意识。", "method": "文中提出了AQE（Approximate Question-side Effect），用于量化问题意识的贡献。此外，还介绍了SCAO（Semantic Compression by Answering in One word）方法，旨在提高模型内部信号的利用效果。", "result": "分析结果表明，许多报告的成功都源于利用问题中的浅层模式。实验显示，SCAO在减少问题线索的设置下表现强劲且一致，这突显了其在促进大型语言模型真实性自我意识方面的有效性。", "conclusion": "研究表明，大语言模型中的幻觉预测更多可能是由于利用问题中的浅层模式而非真正意义上的自我意识。SCAO方法有助于减少这种偏见，促进模型的真实性自我意识。"}}
{"id": "2509.15235", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.15235", "abs": "https://arxiv.org/abs/2509.15235", "authors": ["Jialiang Kang", "Han Shu", "Wenshuo Li", "Yingjie Zhai", "Xinghao Chen"], "title": "ViSpec: Accelerating Vision-Language Models with Vision-Aware Speculative Decoding", "comment": "12 pages, 4 figures", "summary": "Speculative decoding is a widely adopted technique for accelerating inference\nin large language models (LLMs), yet its application to vision-language models\n(VLMs) remains underexplored, with existing methods achieving only modest\nspeedups (<1.5x). This gap is increasingly significant as multimodal\ncapabilities become central to large-scale models. We hypothesize that large\nVLMs can effectively filter redundant image information layer by layer without\ncompromising textual comprehension, whereas smaller draft models struggle to do\nso. To address this, we introduce Vision-Aware Speculative Decoding (ViSpec), a\nnovel framework tailored for VLMs. ViSpec employs a lightweight vision adaptor\nmodule to compress image tokens into a compact representation, which is\nseamlessly integrated into the draft model's attention mechanism while\npreserving original image positional information. Additionally, we extract a\nglobal feature vector for each input image and augment all subsequent text\ntokens with this feature to enhance multimodal coherence. To overcome the\nscarcity of multimodal datasets with long assistant responses, we curate a\nspecialized training dataset by repurposing existing datasets and generating\nextended outputs using the target VLM with modified prompts. Our training\nstrategy mitigates the risk of the draft model exploiting direct access to the\ntarget model's hidden states, which could otherwise lead to shortcut learning\nwhen training solely on target model outputs. Extensive experiments validate\nViSpec, achieving, to our knowledge, the first substantial speedup in VLM\nspeculative decoding.", "AI": {"tldr": "本文提出了一种名为ViSpec的新框架，用来加速视觉语言模型的推测解码过程，通过轻量级的视觉适配器将图像压缩，并增强多模态一致性。实验结果表明，这是首次在VLM中实现显著加速的技术。", "motivation": "鉴于目前针对视觉语言模型（VLM）的推测解码技术尚未得到充分探索，现有的方法仅实现了有限的加速效果，本文旨在填补大型多模态能力模型中的这一空白。我们假设大型VLM可以逐层有效过滤冗余图像信息，而不影响文本理解，而较小的草图模型则难以做到这一点。", "method": "提出了一种针对视觉语言模型的新型框架——视觉感知推测解码（ViSpec），该框架使用轻量级的视觉适配模块将图像令牌压缩成紧凑表示，并将其无缝集成到草图模型的注意力机制中，同时保持原始图像位置信息。此外，为每张输入图像提取全局特征向量，并将其与随后的所有文本标记相结合，以增强多模态一致性。为了克服长助手响应的多模态数据集稀缺问题，通过重新利用现有的数据集并使用带有修改提示的目标VLM生成扩展输出来编制专门的训练数据集。训练策略通过在仅基于目标模型输出进行训练时防止草图模型利用对目标模型隐藏状态的直接访问来减轻捷径学习的风险。", "result": "广泛的实验验证了ViSpec，实现了据我们所知，首个在视觉语言模型推测解码中的实质性加速。", "conclusion": "研究表明，视觉感知推测解码（ViSpec）框架能够显著加速视觉语言模型（VLM）的推理过程，这是首次在VLM中实现显著加速的技术。"}}
{"id": "2509.15350", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.15350", "abs": "https://arxiv.org/abs/2509.15350", "authors": ["Yitong Wang", "Zhongping Zhang", "Margherita Piana", "Zheng Zhou", "Peter Gerstoft", "Bryan A. Plummer"], "title": "Real, Fake, or Manipulated? Detecting Machine-Influenced Text", "comment": "Accepted to EMNLP 2025 Findings", "summary": "Large Language Model (LLMs) can be used to write or modify documents,\npresenting a challenge for understanding the intent behind their use. For\nexample, benign uses may involve using LLM on a human-written document to\nimprove its grammar or to translate it into another language. However, a\ndocument entirely produced by a LLM may be more likely to be used to spread\nmisinformation than simple translation (\\eg, from use by malicious actors or\nsimply by hallucinating). Prior works in Machine Generated Text (MGT) detection\nmostly focus on simply identifying whether a document was human or machine\nwritten, ignoring these fine-grained uses. In this paper, we introduce a\nHiErarchical, length-RObust machine-influenced text detector (HERO), which\nlearns to separate text samples of varying lengths from four primary types:\nhuman-written, machine-generated, machine-polished, and machine-translated.\nHERO accomplishes this by combining predictions from length-specialist models\nthat have been trained with Subcategory Guidance. Specifically, for categories\nthat are easily confused (\\eg, different source languages), our Subcategory\nGuidance module encourages separation of the fine-grained categories, boosting\nperformance. Extensive experiments across five LLMs and six domains demonstrate\nthe benefits of our HERO, outperforming the state-of-the-art by 2.5-3 mAP on\naverage.", "AI": {"tldr": "本文提出了一种细粒度的机器影响文本检测方法HERO，可以区分人撰写文本、机器生成文本、机器润色文本和机器翻译文本，解决了以往只关注是否机器生成的问题。", "motivation": "先前的机器生成文本（MGT）检测工作主要集中在识别文档是人还是机器撰写的，忽略了这些细粒度的使用情况。该研究旨在解决这一不足，提供更细致的文本检测。", "method": "介绍了一种分层的、长度稳健的机器影响文本检测器（HERO），用于区分四种主要类型的文本样本：人写、机器生成、机器润色和机器翻译。HERO通过结合长度专长模型的预测完成此任务，这些模型接受子类别指导进行训练。特别是在容易混淆的类别上（例如，不同的源语言），子类别指导模块鼓励细粒度类别的分离，从而提高性能。", "result": "实验结果表明，HERO在区分四种类型文本的任务上表现出色，平均比现有技术高出2.5-3 mAP。", "conclusion": "广泛的实验表明HERO在不同大小的语言模型和六个领域中都优于现有最先进技术，平均提升了2.5-3 mAP。"}}
{"id": "2509.15241", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.15241", "abs": "https://arxiv.org/abs/2509.15241", "authors": ["Shreyash Verma", "Amit Kesari", "Vinayak Trivedi", "Anupam Purwar", "Ratnesh Jamidar"], "title": "M-PACE: Mother Child Framework for Multimodal Compliance", "comment": "The M-PACE framework uses a \"mother-child\" AI model system to\n  automate and unify compliance checks for ads, reducing costs while\n  maintaining high accuracy", "summary": "Ensuring that multi-modal content adheres to brand, legal, or\nplatform-specific compliance standards is an increasingly complex challenge\nacross domains. Traditional compliance frameworks typically rely on disjointed,\nmulti-stage pipelines that integrate separate modules for image classification,\ntext extraction, audio transcription, hand-crafted checks, and rule-based\nmerges. This architectural fragmentation increases operational overhead,\nhampers scalability, and hinders the ability to adapt to dynamic guidelines\nefficiently. With the emergence of Multimodal Large Language Models (MLLMs),\nthere is growing potential to unify these workflows under a single,\ngeneral-purpose framework capable of jointly processing visual and textual\ncontent. In light of this, we propose Multimodal Parameter Agnostic Compliance\nEngine (M-PACE), a framework designed for assessing attributes across\nvision-language inputs in a single pass. As a representative use case, we apply\nM-PACE to advertisement compliance, demonstrating its ability to evaluate over\n15 compliance-related attributes. To support structured evaluation, we\nintroduce a human-annotated benchmark enriched with augmented samples that\nsimulate challenging real-world conditions, including visual obstructions and\nprofanity injection. M-PACE employs a mother-child MLLM setup, demonstrating\nthat a stronger parent MLLM evaluating the outputs of smaller child models can\nsignificantly reduce dependence on human reviewers, thereby automating quality\ncontrol. Our analysis reveals that inference costs reduce by over 31 times,\nwith the most efficient models (Gemini 2.0 Flash as child MLLM selected by\nmother MLLM) operating at 0.0005 per image, compared to 0.0159 for Gemini 2.5\nPro with comparable accuracy, highlighting the trade-off between cost and\noutput quality achieved in real time by M-PACE in real life deployment over\nadvertising data.", "AI": {"tldr": "论文提出M-PACE，一种可以更高效处理多模态内容合规问题的框架，适用于广告行业，显著降低成本并提高效率。", "motivation": "传统合规框架通常依赖于分散的多阶段管道，这增加了操作开销，限制了扩展性，并且难以适应动态规范。因此，作者提出M-PACE框架以应对跨多模态内容的复杂合规挑战。", "method": "提出了多模态参数无关合规引擎（M-PACE），该框架能够在一次通过中评估视觉和文本输入的各种属性。其应用案例是广告合规评估，能够评估超过15个与合规相关的属性。M-PACE使用母子多模态大型语言模型（MLLM）架构，显示出较强的模型可以评估较小模型的输出，从而减少对人类评审员的依赖，实现质量控制的自动化。", "result": "M-PACE在广告数据上的分析表明，推理成本降低了超过31倍，其中最高效的模型（由母MLLM选择的Gemini 2.0 Flash作为子模型）每个图像的成本仅为0.0005，相比具有相当精度的Gemini 2.5 Pro降低了0.0154，从而实现了成本和输出质量之间的权衡。", "conclusion": "M-PACE能够在一次通过中有效地处理跨多模态输入的属性评估，特别是在广告合规方面显示出显著的成本效率和质量控制能力。"}}
{"id": "2509.15361", "categories": ["cs.CL", "cs.AI", "cs.MM"], "pdf": "https://arxiv.org/pdf/2509.15361", "abs": "https://arxiv.org/abs/2509.15361", "authors": ["Zichen Wu", "Hsiu-Yuan Huang", "Yunfang Wu"], "title": "Beyond Spurious Signals: Debiasing Multimodal Large Language Models via Counterfactual Inference and Adaptive Expert Routing", "comment": "Accepted by EMNLP 2025 Findings", "summary": "Multimodal Large Language Models (MLLMs) have shown substantial capabilities\nin integrating visual and textual information, yet frequently rely on spurious\ncorrelations, undermining their robustness and generalization in complex\nmultimodal reasoning tasks. This paper addresses the critical challenge of\nsuperficial correlation bias in MLLMs through a novel causal mediation-based\ndebiasing framework. Specially, we distinguishing core semantics from spurious\ntextual and visual contexts via counterfactual examples to activate\ntraining-stage debiasing and employ a Mixture-of-Experts (MoE) architecture\nwith dynamic routing to selectively engages modality-specific debiasing\nexperts. Empirical evaluation on multimodal sarcasm detection and sentiment\nanalysis tasks demonstrates that our framework significantly surpasses unimodal\ndebiasing strategies and existing state-of-the-art models.", "AI": {"tldr": "本研究提出了一种新的基于因果中介的去偏框架，通过动态路由选择性地激活模态特定的去偏专家，成功降低了MLLMs中偶然相关性偏差，显著提升了模型性能。", "motivation": "尽管多模态大型语言模型（MLLMs）在整合视觉和文本信息方面展现了显著能力，但它们常依赖于偶然的相关性，从而削弱了其在复杂多模态推理任务中的鲁棒性和泛化能力。", "method": "本研究提出了一种基于因果中介的去偏框架，通过反事实示例区分核心语义和文本及视觉中的偶然上下文，以激活训练阶段的去偏，并采用具有动态路由的专家混合（MoE）架构，选择性地参与模态特定的去偏专家。", "result": "实验结果表明，在多模态讽刺检测和情感分析任务中，该框架显著优于单模态去偏策略和现有最先进模型。", "conclusion": "通过反事实示例与动态路由相结合，本研究提出的去偏框架有效地降低了MLLMs中的偶然相关性偏差，提升了模型在复杂多模态任务中的鲁棒性和泛化能力。"}}
{"id": "2509.15242", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.15242", "abs": "https://arxiv.org/abs/2509.15242", "authors": ["Jaydeep Rade", "Md Hasibul Hasan Hasib", "Meric Ozturk", "Baboucarr Faal", "Sheng Yang", "Dipali G. Sashital", "Vincenzo Venditti", "Baoyu Chen", "Soumik Sarkar", "Adarsh Krishnamurthy", "Anwesha Sarkar"], "title": "ProFusion: 3D Reconstruction of Protein Complex Structures from Multi-view AFM Images", "comment": null, "summary": "AI-based in silico methods have improved protein structure prediction but\noften struggle with large protein complexes (PCs) involving multiple\ninteracting proteins due to missing 3D spatial cues. Experimental techniques\nlike Cryo-EM are accurate but costly and time-consuming. We present ProFusion,\na hybrid framework that integrates a deep learning model with Atomic Force\nMicroscopy (AFM), which provides high-resolution height maps from random\norientations, naturally yielding multi-view data for 3D reconstruction.\nHowever, generating a large-scale AFM imaging data set sufficient to train deep\nlearning models is impractical. Therefore, we developed a virtual AFM framework\nthat simulates the imaging process and generated a dataset of ~542,000 proteins\nwith multi-view synthetic AFM images. We train a conditional diffusion model to\nsynthesize novel views from unposed inputs and an instance-specific Neural\nRadiance Field (NeRF) model to reconstruct 3D structures. Our reconstructed 3D\nprotein structures achieve an average Chamfer Distance within the AFM imaging\nresolution, reflecting high structural fidelity. Our method is extensively\nvalidated on experimental AFM images of various PCs, demonstrating strong\npotential for accurate, cost-effective protein complex structure prediction and\nrapid iterative validation using AFM experiments.", "AI": {"tldr": "This paper presents ProFusion, a hybrid AI framework combining a deep learning model with AFM to predict 3D structures of large protein complexes with high fidelity and at a lower cost than experimental techniques.", "motivation": "The motivation is to improve the prediction of protein complex structures using AI, as existing in silico methods struggle with large protein complexes due to the lack of 3D spatial cues. Experimental methods like Cryo-EM are accurate but expensive and time-consuming.", "method": "ProFusion is a hybrid framework integrating a deep learning model with Atomic Force Microscopy (AFM), using a dataset of ~542,000 proteins with multi-view synthetic AFM images generated by a virtual AFM framework.", "result": "The method is validated with an average Chamfer Distance within the resolution of AFM imaging, showing high fidelity in the 3D reconstruction of protein complexes.", "conclusion": "The framework achieves an average Chamfer Distance within the AFM imaging resolution, demonstrating high structural fidelity and potential for rapid iterative validation of protein complex structures using AFM experiments."}}
{"id": "2509.15362", "categories": ["cs.CL", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.15362", "abs": "https://arxiv.org/abs/2509.15362", "authors": ["Yaya Sy", "Dioula Doucouré", "Christophe Cerisara", "Irina Illina"], "title": "Speech Language Models for Under-Represented Languages: Insights from Wolof", "comment": null, "summary": "We present our journey in training a speech language model for Wolof, an\nunderrepresented language spoken in West Africa, and share key insights. We\nfirst emphasize the importance of collecting large-scale, spontaneous,\nhigh-quality speech data, and show that continued pretraining HuBERT on this\ndataset outperforms both the base model and African-centric models on ASR. We\nthen integrate this speech encoder into a Wolof LLM to train the first Speech\nLLM for this language, extending its capabilities to tasks such as speech\ntranslation. Furthermore, we explore training the Speech LLM to perform\nmulti-step Chain-of-Thought before transcribing or translating. Our results\nshow that the Speech LLM not only improves speech recognition but also performs\nwell in speech translation. The models and the code will be openly shared.", "AI": {"tldr": "本研究通过大规模高质量语音数据预训练HuBERT，并将其集成进Wolof语言的大规模语言模型中，从而开发出首个Wolof语言的语音LLM，并展示了其在语音识别和翻译上的优异表现。", "motivation": "鉴于Wolof作为欠代表性语言所面临的挑战，研究旨在通过创新方法提高其语音技术的应用范围和性能。", "method": "本研究首先强调了收集大规模、自发且高质量的语音数据的重要性，并展示了在该数据集上继续预训练HuBERT模型的表现优于基础模型和专注于非洲的语言模型。接着，将此语音编码器整合进Wolof语言的大规模语言模型中，训练出首个为Wolof语言服务的语音LLM，并将其功能扩展至语音翻译等任务。此外，研究还探索了训练语音LLM执行多步骤链式思维（Chain-of-Thought）的任务，在转录或翻译之前。", "result": "实验结果表明，语音LLM不仅在语音识别任务上表现优异，在语音翻译任务上也有良好的表现。", "conclusion": "本研究展示了通过大规模高质量的语音数据预训练并集成到大规模语言模型中，可以有效地提升欠代表性语言Wolof的语音处理能力，并将其拓展至多个任务领域。"}}
{"id": "2509.15243", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.15243", "abs": "https://arxiv.org/abs/2509.15243", "authors": ["Muhammad Imran", "Yugyung Lee"], "title": "Multi-Modal Interpretability for Enhanced Localization in Vision-Language Models", "comment": "8 pages, 6 figures, 3 tables", "summary": "Recent advances in vision-language models have significantly expanded the\nfrontiers of automated image analysis. However, applying these models in\nsafety-critical contexts remains challenging due to the complex relationships\nbetween objects, subtle visual cues, and the heightened demand for transparency\nand reliability. This paper presents the Multi-Modal Explainable Learning\n(MMEL) framework, designed to enhance the interpretability of vision-language\nmodels while maintaining high performance. Building upon prior work in\ngradient-based explanations for transformer architectures (Grad-eclip), MMEL\nintroduces a novel Hierarchical Semantic Relationship Module that enhances\nmodel interpretability through multi-scale feature processing, adaptive\nattention weighting, and cross-modal alignment. Our approach processes features\nat multiple semantic levels to capture relationships between image regions at\ndifferent granularities, applying learnable layer-specific weights to balance\ncontributions across the model's depth. This results in more comprehensive\nvisual explanations that highlight both primary objects and their contextual\nrelationships with improved precision. Through extensive experiments on\nstandard datasets, we demonstrate that by incorporating semantic relationship\ninformation into gradient-based attribution maps, MMEL produces more focused\nand contextually aware visualizations that better reflect how vision-language\nmodels process complex scenes. The MMEL framework generalizes across various\ndomains, offering valuable insights into model decisions for applications\nrequiring high interpretability and reliability.", "AI": {"tldr": "The paper introduces MMEL, a framework that improves the interpretability of vision-language models through a Hierarchical Semantic Relationship Module, achieving higher performance and reliability in safety-critical applications.", "motivation": "To address the challenges in applying vision-language models in safety-critical contexts due to the need for transparency and reliability.", "method": "MMEL builds on Grad-eclip and introduces a Hierarchical Semantic Relationship Module that processes features at multiple semantic levels to capture relationships between image regions, using adaptive attention weighting and multi-scale feature processing.", "result": "Experiments show that MMEL produces focused and contextually aware visualizations that better reflect how vision-language models process complex scenes, providing valuable insights into model decisions.", "conclusion": "MMEL offers enhanced interpretability while maintaining high performance, making vision-language models more suitable for applications requiring high reliability."}}
{"id": "2509.15373", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.15373", "abs": "https://arxiv.org/abs/2509.15373", "authors": ["Katsumi Ibaraki", "David Chiang"], "title": "Frustratingly Easy Data Augmentation for Low-Resource ASR", "comment": "5 pages, 2 figures, 2 tables, submitted to ICASSP 2026", "summary": "This paper introduces three self-contained data augmentation methods for\nlow-resource Automatic Speech Recognition (ASR). Our techniques first generate\nnovel text--using gloss-based replacement, random replacement, or an LLM-based\napproach--and then apply Text-to-Speech (TTS) to produce synthetic audio. We\napply these methods, which leverage only the original annotated data, to four\nlanguages with extremely limited resources (Vatlongos, Nashta, Shinekhen\nBuryat, and Kakabe). Fine-tuning a pretrained Wav2Vec2-XLSR-53 model on a\ncombination of the original audio and generated synthetic data yields\nsignificant performance gains, including a 14.3% absolute WER reduction for\nNashta. The methods prove effective across all four low-resource languages and\nalso show utility for high-resource languages like English, demonstrating their\nbroad applicability.", "AI": {"tldr": "This paper outlines three data augmentation methods for ASR in low-resource settings, significantly improving performance through synthetic data generation and fine-tuning of the Wav2Vec2-XLSR-53 model.", "motivation": "The motivation behind this research is to address the scarcity of annotated data in low-resource languages, which is a critical issue for the development of ASR models.", "method": "The paper proposes three data augmentation techniques for low-resource ASR tasks: gloss-based replacement, random replacement, and an LLM-based approach. These methods generate new textual data which is then transformed into synthetic audio using TTS.", "result": "By integrating the original annotated data with the newly generated synthetic data, the fine-tuned Wav2Vec2-XLSR-53 model exhibited marked performance improvements across four low-resource languages, notably a 14.3% decrease in WER for Nashta.", "conclusion": "The data augmentation methods introduced in the paper proved effective not only in boosting the performance of low-resource languages but also showed promise when applied to high-resource languages like English, indicating broad applicability."}}
{"id": "2509.15250", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.15250", "abs": "https://arxiv.org/abs/2509.15250", "authors": ["Wenda Qin", "Andrea Burns", "Bryan A. Plummer", "Margrit Betke"], "title": "Walk and Read Less: Improving the Efficiency of Vision-and-Language Navigation via Tuning-Free Multimodal Token Pruning", "comment": "Accepted to ACL 2024 Findings. Data and code to be released at\n  https://github.com/wdqin/VLN-NAP", "summary": "Large models achieve strong performance on Vision-and-Language Navigation\n(VLN) tasks, but are costly to run in resource-limited environments. Token\npruning offers appealing tradeoffs for efficiency with minimal performance loss\nby reducing model input size, but prior work overlooks VLN-specific challenges.\nFor example, information loss from pruning can effectively increase\ncomputational cost due to longer walks. Thus, the inability to identify\nuninformative tokens undermines the supposed efficiency gains from pruning. To\naddress this, we propose Navigation-Aware Pruning (NAP), which uses\nnavigation-specific traits to simplify the pruning process by pre-filtering\ntokens into foreground and background. For example, image views are filtered\nbased on whether the agent can navigate in that direction. We also extract\nnavigation-relevant instructions using a Large Language Model. After filtering,\nwe focus pruning on background tokens, minimizing information loss. To further\nhelp avoid increases in navigation length, we discourage backtracking by\nremoving low-importance navigation nodes. Experiments on standard VLN\nbenchmarks show NAP significantly outperforms prior work, preserving higher\nsuccess rates while saving more than 50% FLOPS.", "AI": {"tldr": "提出了一种导航感知剪枝（NAP）方法，该方法利用导航特定特性过滤并简化剪枝过程，避免信息丢失和增加计算成本，实验表明NAP在标准VLN基准上表现优于之前的工作，同时保存超过50%的计算量。", "motivation": "大型模型在视觉和语言导航任务中表现出色，但在资源有限的环境中运行成本高昂。现有的剪枝技术往往忽视了视觉和语言导航中的特定挑战，导致剪枝后的信息损失增加了计算成本，因此需要提出一种新的剪枝方法来解决这一问题。", "method": "Structure", "result": "{ \"tldr\": \"提出了一种导航感知剪枝（NAP）方法，该方法利用导航特定特性过滤并简化剪枝过程，避免信息丢失和增加计算成本，实验表明NAP在标准VLN基准上表现优于之前的工作，同时保存超过50%的计算量。\", \"motivation\": \"大型模型在视觉和语言导航任务中表现出色，但在资源有限的环境中运行成本高昂。现有的剪枝技术往往忽视了视觉和语言导航中的特定挑战，导致剪枝后的信息损失增加了计算成本，因此需要提出一种新的剪枝方法来解决这一问题。\", \"method\": \"NAP方法通过将令牌预分类为前景和背景，并使用大型语言模型提取导航相关信息，主要集中在剪枝背景令牌来最小化信息损失。同时，它还通过移除低重要性的导航节点来避免倒退，进一步缩短导航路径长度。\", \"result\": \"实验结果表明，在标准的VLN基准测试中，NAP相比之前的工作不仅提高了成功率，而且还节省了超过50%的计算量（FLOPS）。\", \"conclusion\": \"通过实验验证了NAP方法的有效性，证明该方法在保持甚至提高导航任务性能的同时，能够显著降低计算成本。\" }", "conclusion": "通过实验验证了NAP方法的有效性，证明该方法在保持甚至提高导航任务性能的同时，能够显著降低计算成本。"}}
{"id": "2509.15403", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.15403", "abs": "https://arxiv.org/abs/2509.15403", "authors": ["Yangyi Li", "Mengdi Huai"], "title": "Quantifying Uncertainty in Natural Language Explanations of Large Language Models for Question Answering", "comment": null, "summary": "Large language models (LLMs) have shown strong capabilities, enabling\nconcise, context-aware answers in question answering (QA) tasks. The lack of\ntransparency in complex LLMs has inspired extensive research aimed at\ndeveloping methods to explain large language behaviors. Among existing\nexplanation methods, natural language explanations stand out due to their\nability to explain LLMs in a self-explanatory manner and enable the\nunderstanding of model behaviors even when the models are closed-source.\nHowever, despite these promising advancements, there is no existing work\nstudying how to provide valid uncertainty guarantees for these generated\nnatural language explanations. Such uncertainty quantification is critical in\nunderstanding the confidence behind these explanations. Notably, generating\nvalid uncertainty estimates for natural language explanations is particularly\nchallenging due to the auto-regressive generation process of LLMs and the\npresence of noise in medical inquiries. To bridge this gap, in this work, we\nfirst propose a novel uncertainty estimation framework for these generated\nnatural language explanations, which provides valid uncertainty guarantees in a\npost-hoc and model-agnostic manner. Additionally, we also design a novel robust\nuncertainty estimation method that maintains valid uncertainty guarantees even\nunder noise. Extensive experiments on QA tasks demonstrate the desired\nperformance of our methods.", "AI": {"tldr": "本文提出了一种新的不确定性估计框架，用于大语言模型生成的自然语言解释，并设计了一种鲁棒的不确定性估计方法，即使在有噪声的情况下也能保持有效的不确定性保证。实验表明，这些方法在问答任务上是有效的。", "motivation": "虽然自然语言解释在解释大语言模型时表现良好，但此前的研究并未提供其不确定性保证，这对于理解解释的信任度至关重要。", "method": "Structure", "result": "{\n  \"tldr\": \"本文提出了一种新的不确定性估计框架，用于大语言模型生成的自然语言解释，该框架以一种事后且模型无关的方式提供有效的不确定性保证。此外，还设计了一种新的鲁棒不确定性估计方法，即使在有噪音的情况下也能保持有效的不确定性保证。实验结果表明了方法的有效性。\", \n  \"motivation\": \"尽管大语言模型具备强大的能力，但缺乏透明性。虽然自然语言解释是一种流行的解释方法，但目前尚无研究探讨如何为这些解释提供有效的不确定性保证，这对于理解解释的置信度至关重要。因此，本文提出了解决这一问题的方法框架。\", \n  \"method\": \"本文提出了一个新型的不确定性估计框架，用于评估在大语言模型生成自然语言解释的不确定性，并设计了一种在有噪声情况下仍能保持有效性的鲁棒不确定性估计方法。\", \n  \"result\": \"大量的实验表明，本文提出的方法在问答任务上性能良好。\", \n  \"conclusion\": \"提出的方法框架和不确定估计方法有效填补了提供自然语言解释不确定性保证的空白，且具备鲁棒性和泛化性。实验结果支持了这种方法的有效性。\"}\n}", "conclusion": "本文提出的方法不仅填补了研究空白，还表现出良好的鲁棒性和泛化性能。实验结果支持这些方法的有效性。"}}
{"id": "2509.15257", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.15257", "abs": "https://arxiv.org/abs/2509.15257", "authors": ["Silpa Vadakkeeveetil Sreelatha", "Sauradip Nag", "Muhammad Awais", "Serge Belongie", "Anjan Dutta"], "title": "RespoDiff: Dual-Module Bottleneck Transformation for Responsible & Faithful T2I Generation", "comment": null, "summary": "The rapid advancement of diffusion models has enabled high-fidelity and\nsemantically rich text-to-image generation; however, ensuring fairness and\nsafety remains an open challenge. Existing methods typically improve fairness\nand safety at the expense of semantic fidelity and image quality. In this work,\nwe propose RespoDiff, a novel framework for responsible text-to-image\ngeneration that incorporates a dual-module transformation on the intermediate\nbottleneck representations of diffusion models. Our approach introduces two\ndistinct learnable modules: one focused on capturing and enforcing responsible\nconcepts, such as fairness and safety, and the other dedicated to maintaining\nsemantic alignment with neutral prompts. To facilitate the dual learning\nprocess, we introduce a novel score-matching objective that enables effective\ncoordination between the modules. Our method outperforms state-of-the-art\nmethods in responsible generation by ensuring semantic alignment while\noptimizing both objectives without compromising image fidelity. Our approach\nimproves responsible and semantically coherent generation by 20% across\ndiverse, unseen prompts. Moreover, it integrates seamlessly into large-scale\nmodels like SDXL, enhancing fairness and safety. Code will be released upon\nacceptance.", "AI": {"tldr": "论文介绍了一种新的框架RespoDiff，该框架通过双模块转换提升文本到图像生成的负责任属性和图像质量，同时保持语义保真度，且适用于大型模型。", "motivation": "当前，扩散模型快速发展，但在文本到图像生成中保证公平性和安全性仍然是一个挑战。现有方法倾向于牺牲语义保真度和图像质量以改善公平性和安全性。因此，为了平衡这两方面的要求，提出了RespoDiff框架。", "method": "RespoDiff框架引入了两个可学习模块：一个专注于捕获和实施负责概念，另一个则致力于保持与中性提示的语义一致。通过引入新颖的评分匹配目标，这两个模块可以互相协调。", "result": "该论文提出了一种名为RespoDiff的新框架，用于负责任的文本到图像生成，它在扩散模型的中间瓶颈表示上引入了双模块转换。该方法通过同时优化责任概念（如公平性和安全性）和语义对齐，提高了生成图像的质量，且在多种未见过的提示下提高了20%的责任性和语义连贯性。此外，该方法可以无缝集成到大型模型中，如SDXL，从而增强其公平性和安全性。代码将在论文被接受后发布。", "conclusion": "该论文证明了RespoDiff框架通过新颖的双模块方法提高了语义对齐和公平性、安全性，相较于现有最佳方法有显著的进步。"}}
{"id": "2509.15419", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.15419", "abs": "https://arxiv.org/abs/2509.15419", "authors": ["Claudio Benzoni", "Martina Langhals", "Martin Boeker", "Luise Modersohn", "Máté E. Maros"], "title": "Deep learning and abstractive summarisation for radiological reports: an empirical study for adapting the PEGASUS models' family with scarce data", "comment": "14 pages, 4 figures, and 3 tables", "summary": "Regardless of the rapid development of artificial intelligence, abstractive\nsummarisation is still challenging for sensitive and data-restrictive domains\nlike medicine. With the increasing number of imaging, the relevance of\nautomated tools for complex medical text summarisation is expected to become\nhighly relevant. In this paper, we investigated the adaptation via fine-tuning\nprocess of a non-domain-specific abstractive summarisation encoder-decoder\nmodel family, and gave insights to practitioners on how to avoid over- and\nunderfitting. We used PEGASUS and PEGASUS-X, on a medium-sized radiological\nreports public dataset. For each model, we comprehensively evaluated two\ndifferent checkpoints with varying sizes of the same training data. We\nmonitored the models' performances with lexical and semantic metrics during the\ntraining history on the fixed-size validation set. PEGASUS exhibited different\nphases, which can be related to epoch-wise double-descent, or\npeak-drop-recovery behaviour. For PEGASUS-X, we found that using a larger\ncheckpoint led to a performance detriment. This work highlights the challenges\nand risks of fine-tuning models with high expressivity when dealing with scarce\ntraining data, and lays the groundwork for future investigations into more\nrobust fine-tuning strategies for summarisation models in specialised domains.", "AI": {"tldr": "The paper investigates the fine-tuning process of PEGASUS and PEGASUS-X for abstractive summarization in radiology, highlighting difficulties and risks in fine-tuning models with scarce data.", "motivation": "The paper aims to address the challenges in abstractive summarization for sensitive domains like medicine and to provide insights on avoiding over- and underfitting during the fine-tuning process.", "method": "We used PEGASUS and PEGASUS-X, on a medium-sized radiological reports public dataset. For each model, we comprehensively evaluated two different checkpoints with varying sizes of the same training data.", "result": "For PEGASUS, different phases were observed during training which can be related to epoch-wise double-descent or peak-drop-recovery behaviour. For PEGASUS-X, using a larger checkpoint led to a decline in performance.", "conclusion": "This work highlights the challenges and risks of fine-tuning models with high expressivity when dealing with scarce training data, and lays the groundwork for future investigations into more robust fine-tuning strategies for summarisation models in specialised domains."}}
{"id": "2509.15267", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.15267", "abs": "https://arxiv.org/abs/2509.15267", "authors": ["Valeria Pais", "Luis Oala", "Daniele Faccio", "Marco Aversa"], "title": "Autoguided Online Data Curation for Diffusion Model Training", "comment": "Accepted non-archival paper at ICCV 2025 Workshop on Curated Data for\n  Efficient Learning (CDEL)", "summary": "The costs of generative model compute rekindled promises and hopes for\nefficient data curation. In this work, we investigate whether recently\ndeveloped autoguidance and online data selection methods can improve the time\nand sample efficiency of training generative diffusion models. We integrate\njoint example selection (JEST) and autoguidance into a unified code base for\nfast ablation and benchmarking. We evaluate combinations of data curation on a\ncontrolled 2-D synthetic data generation task as well as (3x64x64)-D image\ngeneration. Our comparisons are made at equal wall-clock time and equal number\nof samples, explicitly accounting for the overhead of selection. Across\nexperiments, autoguidance consistently improves sample quality and diversity.\nEarly AJEST (applying selection only at the beginning of training) can match or\nmodestly exceed autoguidance alone in data efficiency on both tasks. However,\nits time overhead and added complexity make autoguidance or uniform random data\nselection preferable in most situations. These findings suggest that while\ntargeted online selection can yield efficiency gains in early training, robust\nsample quality improvements are primarily driven by autoguidance. We discuss\nlimitations and scope, and outline when data selection may be beneficial.", "AI": {"tldr": "研究发现，在提高生成扩散模型训练效率方面，自动引导比联合示例选择更为有效，具有更优的样本质量和多样性。", "motivation": "我们在这项工作中探讨了最近开发的自动引导和在线数据选择方法是否能够提高生成扩散模型的训练的时间和样本效率。", "method": "我们整合了联合示例选择（JEST）和自动引导（autoguidance），形成了统一的代码库用于快速消融和基准测试。我们在受控的2D合成数据生成任务以及(3x64x64)-D图像生成任务下，评估了不同数据整理方式的组合效果。我们的比较在相等的总运行时间和相等样本数量的基础上进行，具体考虑到选择操作的额外开销。", "result": "实验结果显示，自动引导始终提高了样本质量和多样性。早期AJEST（仅在训练初期应用选择）在样本效率上与仅使用自动引导相比，可以匹敌或适度超越。然而，由于其较高的时间开销和复杂性，在多数场合下，自动引导或统一随机数据选择更可取。", "conclusion": "这些发现表明，尽管针对早期训练的在线选择能带来效率增益，但强大的样本质量改进主要源自自动引导。我们讨论了这些方法的局限性和适用范围，并概述了数据选择可能有益的情况。"}}
{"id": "2509.15430", "categories": ["cs.CL", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.15430", "abs": "https://arxiv.org/abs/2509.15430", "authors": ["Liuyuan Jiang", "Xiaodong Cui", "Brian Kingsbury", "Tianyi Chen", "Lisha Chen"], "title": "BiRQ: Bi-Level Self-Labeling Random Quantization for Self-Supervised Speech Recognition", "comment": "5 pages including reference", "summary": "Speech is a rich signal, and labeled audio-text pairs are costly, making\nself-supervised learning essential for scalable representation learning. A core\nchallenge in speech SSL is generating pseudo-labels that are both informative\nand efficient: strong labels, such as those used in HuBERT, improve downstream\nperformance but rely on external encoders and multi-stage pipelines, while\nefficient methods like BEST-RQ achieve simplicity at the cost of weaker labels.\nWe propose BiRQ, a bilevel SSL framework that combines the efficiency of\nBEST-RQ with the refinement benefits of HuBERT-style label enhancement. The key\nidea is to reuse part of the model itself as a pseudo-label generator:\nintermediate representations are discretized by a random-projection quantizer\nto produce enhanced labels, while anchoring labels derived directly from the\nraw input stabilize training and prevent collapse. Training is formulated as an\nefficient first-order bilevel optimization problem, solved end-to-end with\ndifferentiable Gumbel-softmax selection. This design eliminates the need for\nexternal label encoders, reduces memory cost, and enables iterative label\nrefinement in an end-to-end fashion. BiRQ consistently improves over BEST-RQ\nwhile maintaining low complexity and computational efficiency. We validate our\nmethod on various datasets, including 960-hour LibriSpeech, 150-hour AMI\nmeetings and 5,000-hour YODAS, demonstrating consistent gains over BEST-RQ.", "AI": {"tldr": "BiRQ combines the computational efficiency of BEST-RQ with the label refinement of HuBERT for speech self-supervised learning, improving performance on large datasets while keeping complexity low.", "motivation": "The motivation behind BiRQ is to address the core challenge in speech self-supervised learning of generating pseudo-labels that are both informative and efficient, as previous methods trade off between strong labels and computational efficiency.", "method": "BiRQ is a bilevel self-supervised learning framework for speech processing. It integrates the efficiency of BEST-RQ with the label refinement benefits of HuBERT by using a part of the model itself to generate pseudo-labels. Intermediate representations are quantized to create enhanced labels, with anchoring labels derived from raw input for training stability.", "result": "BiRQ demonstrates consistent improvements over BEST-RQ across various datasets, maintaining a low complexity and computational efficiency. It was tested on large-scale datasets such as 960-hour LibriSpeech, 150-hour AMI meetings, and 5,000-hour YODAS.", "conclusion": "BiRQ enhances the quality of pseudo-labels for self-supervised learning in speech processing, improving downstream speech recognition performance without increasing computational complexity or memory cost, validating its approach through improvements over BEST-RQ on large speech datasets."}}
{"id": "2509.15270", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.15270", "abs": "https://arxiv.org/abs/2509.15270", "authors": ["Emanuele Ricco", "Elia Onofri", "Lorenzo Cima", "Stefano Cresci", "Roberto Di Pietro"], "title": "PRISM: Phase-enhanced Radial-based Image Signature Mapping framework for fingerprinting AI-generated images", "comment": null, "summary": "A critical need has emerged for generative AI: attribution methods. That is,\nsolutions that can identify the model originating AI-generated content. This\nfeature, generally relevant in multimodal applications, is especially sensitive\nin commercial settings where users subscribe to paid proprietary services and\nexpect guarantees about the source of the content they receive. To address\nthese issues, we introduce PRISM, a scalable Phase-enhanced Radial-based Image\nSignature Mapping framework for fingerprinting AI-generated images. PRISM is\nbased on a radial reduction of the discrete Fourier transform that leverages\namplitude and phase information to capture model-specific signatures. The\noutput of the above process is subsequently clustered via linear discriminant\nanalysis to achieve reliable model attribution in diverse settings, even if the\nmodel's internal details are inaccessible. To support our work, we construct\nPRISM-36K, a novel dataset of 36,000 images generated by six text-to-image GAN-\nand diffusion-based models. On this dataset, PRISM achieves an attribution\naccuracy of 92.04%. We additionally evaluate our method on four benchmarks from\nthe literature, reaching an average accuracy of 81.60%. Finally, we evaluate\nour methodology also in the binary task of detecting real vs fake images,\nachieving an average accuracy of 88.41%. We obtain our best result on GenImage\nwith an accuracy of 95.06%, whereas the original benchmark achieved 82.20%. Our\nresults demonstrate the effectiveness of frequency-domain fingerprinting for\ncross-architecture and cross-dataset model attribution, offering a viable\nsolution for enforcing accountability and trust in generative AI systems.", "AI": {"tldr": "PRISM, a new attribution method for AI-generated images, demonstrates high accuracy in model fingerprinting and real vs fake image detection, fostering accountability in AI systems.", "motivation": "There is a growing need for attribution methods in generative AI to ensure credibility and traceability, especially in commercial settings where the source of AI-generated content is crucial.", "method": "PRISM, a Phase-enhanced Radial-based Image Signature Mapping framework, is used for fingerprinting AI-generated images by leveraging amplitude and phase information derived from the discrete Fourier transform.", "result": "PRISM achieves a 92.04% accuracy on the PRISM-36K dataset, an average of 81.60% on four benchmarks, and an 88.41% accuracy in detecting real vs fake images, showcasing its effectiveness in model attribution.", "conclusion": "The frequency-domain fingerprinting method proposed by PRISM offers a reliable solution for identifying the models responsible for generating AI content, which can enforce accountability and trust in generative AI systems."}}
{"id": "2509.15447", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.15447", "abs": "https://arxiv.org/abs/2509.15447", "authors": ["Caitlin Cisar", "Emily Sheffield", "Joshua Drake", "Alden Harrell", "Subramanian Chidambaram", "Nikita Nangia", "Vinayak Arannil", "Alex Williams"], "title": "PILOT: Steering Synthetic Data Generation with Psychological & Linguistic Output Targeting", "comment": null, "summary": "Generative AI applications commonly leverage user personas as a steering\nmechanism for synthetic data generation, but reliance on natural language\nrepresentations forces models to make unintended inferences about which\nattributes to emphasize, limiting precise control over outputs. We introduce\nPILOT (Psychological and Linguistic Output Targeting), a two-phase framework\nfor steering large language models with structured psycholinguistic profiles.\nIn Phase 1, PILOT translates natural language persona descriptions into\nmultidimensional profiles with normalized scores across linguistic and\npsychological dimensions. In Phase 2, these profiles guide generation along\nmeasurable axes of variation. We evaluate PILOT across three state-of-the-art\nLLMs (Mistral Large 2, Deepseek-R1, LLaMA 3.3 70B) using 25 synthetic personas\nunder three conditions: Natural-language Persona Steering (NPS), Schema-Based\nSteering (SBS), and Hybrid Persona-Schema Steering (HPS). Results demonstrate\nthat schema-based approaches significantly reduce artificial-sounding persona\nrepetition while improving output coherence, with silhouette scores increasing\nfrom 0.098 to 0.237 and topic purity from 0.773 to 0.957. Our analysis reveals\na fundamental trade-off: SBS produces more concise outputs with higher topical\nconsistency, while NPS offers greater lexical diversity but reduced\npredictability. HPS achieves a balance between these extremes, maintaining\noutput variety while preserving structural consistency. Expert linguistic\nevaluation confirms that PILOT maintains high response quality across all\nconditions, with no statistically significant differences between steering\napproaches.", "AI": {"tldr": "PILOT是一种通过多维心理语言学配置文件来精确控制大型语言模型生成的新方法，它改进了传统自然语言角色描述的不足，通过实验在多个大语言模型上验证了其有效性和优势。", "motivation": "虽然生成性AI应用通常利用用户角色作为合成数据生成的引导机制，但是对自然语言表示的依赖迫使模型在哪些属性上进行强调做出无意中的推断，从而限制了对输出的精确控制。", "method": "PILOT (Psychological and Linguistic Output Targeting) 是一个两阶段框架，用于利用结构化的心理语言学配置文件来引导大型语言模型。第一阶段将自然语言人物描述转换为跨语言和心理维度的多维配置文件。第二阶段则根据这些配置文件，沿着可量化的变异轴线引导生成。", "result": "研究结果表明，基于模式的方法显著降低了人工角色重复，同时提高了输出的一致性，轮廓分数从0.098增加到0.237，主题纯度从0.773增加到0.957。同时，混合角色-模式引导实现了在这两个极端之间的平衡，保持了输出多样性的同时保留了结构一致性。", "conclusion": "专家语言评估证实，PILOT在所有条件下都保持了高质量的响应，且在不同引导方法之间没有显著的统计差异。这表明PILOT在增强生成内容的结构一致性和多样性之间找到了良好的平衡。"}}
{"id": "2509.15271", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.15271", "abs": "https://arxiv.org/abs/2509.15271", "authors": ["Sebastian Ray Mason", "Anders Gjølbye", "Phillip Chavarria Højbjerg", "Lenka Tětková", "Lars Kai Hansen"], "title": "Large Vision Models Can Solve Mental Rotation Problems", "comment": null, "summary": "Mental rotation is a key test of spatial reasoning in humans and has been\ncentral to understanding how perception supports cognition. Despite the success\nof modern vision transformers, it is still unclear how well these models\ndevelop similar abilities. In this work, we present a systematic evaluation of\nViT, CLIP, DINOv2, and DINOv3 across a range of mental-rotation tasks, from\nsimple block structures similar to those used by Shepard and Metzler to study\nhuman cognition, to more complex block figures, three types of text, and\nphoto-realistic objects. By probing model representations layer by layer, we\nexamine where and how these networks succeed. We find that i) self-supervised\nViTs capture geometric structure better than supervised ViTs; ii) intermediate\nlayers perform better than final layers; iii) task difficulty increases with\nrotation complexity and occlusion, mirroring human reaction times and\nsuggesting similar constraints in embedding space representations.", "AI": {"tldr": "本研究系统地评估了几种视觉变压器模型在心理旋转任务上的表现，发现自监督学习的ViT模型在捕捉几何结构方面更优，并揭示了这些模型与人类处理相似任务时的相似性。", "motivation": "尽管现代视觉变压器在其他计算机视觉任务中取得了成功，但目前还不清楚这些模型在多大程度上具备类似于人类的空间推理能力，特别是心理旋转的任务。因此，本研究旨在通过评估几种模型在心理旋转任务中的表现来解答这个问题。", "method": "该研究对ViT、CLIP、DINOv2和DINOv3模型进行了系统性的评估，这些模型在一系列心理旋转任务上的表现被测试，任务涵盖了从简单的积木结构到更复杂的积木图，以及三种不同类型的文本和逼真的照片对象。通过对模型表示层的逐层探究，研究分析了这些网络成功的位置和方式。", "result": "研究发现，i) 自监督的ViT模型比监督式ViT模型更好地捕捉几何结构；ii) 中间层的表现优于最终层；iii) 任务难度随着旋转复杂性和遮挡的增加而增加，这与人类的反应时间和嵌入空间表征中的类似约束吻合。", "conclusion": "通过对这些模型的系统评估，研究得出，自监督学习模型表现更好，中间层在网络任务中表现最佳，而且任务难度与旋转的复杂性和遮挡有关，这与人类的反应时间相吻合，暗示了这些模型在处理心理旋转任务时与人类认知的一些相似之处。"}}
{"id": "2509.15476", "categories": ["cs.CL", "cs.MM"], "pdf": "https://arxiv.org/pdf/2509.15476", "abs": "https://arxiv.org/abs/2509.15476", "authors": ["Zhu Li", "Xiyuan Gao", "Yuqing Zhang", "Shekhar Nayak", "Matt Coler"], "title": "Evaluating Multimodal Large Language Models on Spoken Sarcasm Understanding", "comment": null, "summary": "Sarcasm detection remains a challenge in natural language understanding, as\nsarcastic intent often relies on subtle cross-modal cues spanning text, speech,\nand vision. While prior work has primarily focused on textual or visual-textual\nsarcasm, comprehensive audio-visual-textual sarcasm understanding remains\nunderexplored. In this paper, we systematically evaluate large language models\n(LLMs) and multimodal LLMs for sarcasm detection on English (MUStARD++) and\nChinese (MCSD 1.0) in zero-shot, few-shot, and LoRA fine-tuning settings. In\naddition to direct classification, we explore models as feature encoders,\nintegrating their representations through a collaborative gating fusion module.\nExperimental results show that audio-based models achieve the strongest\nunimodal performance, while text-audio and audio-vision combinations outperform\nunimodal and trimodal models. Furthermore, MLLMs such as Qwen-Omni show\ncompetitive zero-shot and fine-tuned performance. Our findings highlight the\npotential of MLLMs for cross-lingual, audio-visual-textual sarcasm\nunderstanding.", "AI": {"tldr": "该论文评估了大规模语言模型和多模态LLMs在讽刺检测任务上的表现，并展示了多模态模型在跨模态讽刺理解中的潜力。", "motivation": "讽刺的意图通常依赖于跨越文本、语音和视觉的微妙的跨模态线索，而之前的大多数工作主要集中于文本的或视听文本讽刺的理解，缺乏综合的音频-视觉-文本讽刺理解。", "method": "使用大规模语言模型（LLMs）和多模态LLMs在零样本、少量样本和LoRA微调设置下对英语（MUStARD++）和中文（MCSD 1.0）中的讽刺检测进行了系统的评估。除了直接分类外，还探讨了将模型作为特征编码器，通过协作门控融合模块整合其表示的方法。", "result": "实验结果显示音频模型在单模态性能中表现最好，而文字-音频和音频-视觉组合超过了单模态和三模态模型。多模态语言模型（如Qwen-Omni）在零样本和微调设置下表现出了竞争力。", "conclusion": "该研究发现突显了多模态语言模型在跨语言、音频-视觉-文本讽刺理解的潜力。"}}
{"id": "2509.15272", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.15272", "abs": "https://arxiv.org/abs/2509.15272", "authors": ["Yannis Kaltampanidis", "Alexandros Doumanoglou", "Dimitrios Zarpalas"], "title": "Which Direction to Choose? An Analysis on the Representation Power of Self-Supervised ViTs in Downstream Tasks", "comment": "24 pages, XAI 2025", "summary": "Self-Supervised Learning (SSL) for Vision Transformers (ViTs) has recently\ndemonstrated considerable potential as a pre-training strategy for a variety of\ncomputer vision tasks, including image classification and segmentation, both in\nstandard and few-shot downstream contexts. Two pre-training objectives dominate\nthe landscape of SSL techniques: Contrastive Learning and Masked Image\nModeling. Features (or tokens) extracted from the final transformer attention\nblock -- specifically, the keys, queries, and values -- as well as features\nobtained after the final block's feed-forward layer, have become a common\nfoundation for addressing downstream tasks. However, in many existing\napproaches, these pre-trained ViT features are further processed through\nadditional transformation layers, often involving lightweight heads or combined\nwith distillation, to achieve superior task performance. Although such methods\ncan improve task outcomes, to the best of our knowledge, a comprehensive\nanalysis of the intrinsic representation capabilities of unaltered ViT features\nhas yet to be conducted. This study aims to bridge this gap by systematically\nevaluating the use of these unmodified features across image classification and\nsegmentation tasks, in both standard and few-shot contexts. The classification\nand segmentation rules that we use are either hyperplane based (as in logistic\nregression) or cosine-similarity based, both of which rely on the presence of\ninterpretable directions in the ViT's latent space. Based on the previous rules\nand without the use of additional feature transformations, we conduct an\nanalysis across token types, tasks, and pre-trained ViT models. This study\nprovides insights into the optimal choice for token type and decision rule\nbased on the task, context, and the pre-training objective, while reporting\ndetailed findings on two widely-used datasets.", "AI": {"tldr": "本研究系统性地评估未修改的ViT特征在多种任务上的直接应用，排除了额外的特征变换，给出了如何选择令牌类型、决策规则等任务优化策略。", "motivation": "当前方法通常通过增加额外的变换层来改进任务性能。然而，尚未进行对未改变的ViT特征的内在表示能力的全面分析。本研究旨在通过系统评估未修改的Vi特特征来填补这一空白。", "method": "该论文系统评估了未经修改的ViT特征在图像分类和分割任务中的使用情况，包括标准和少样本场景。通过基于超平面（如逻辑回归）或余弦相似性的分类和分割规则来分析不同令牌类型、任务和预训练ViT模型的性能。", "result": "论文提供了关于令牌类型选择、决策规则、任务类型、场景和预训练目标之间关系的见解。同时，论文在两个广泛使用的数据集上报告了详细的发现。", "conclusion": "该研究揭示了未修改过的ViT特征在图像分类和分割任务上的性能，并为未来研究提供了基础。"}}
{"id": "2509.15478", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.15478", "abs": "https://arxiv.org/abs/2509.15478", "authors": ["Madison Van Doren", "Casey Ford", "Emily Dix"], "title": "Red Teaming Multimodal Language Models: Evaluating Harm Across Prompt Modalities and Models", "comment": null, "summary": "Multimodal large language models (MLLMs) are increasingly used in real world\napplications, yet their safety under adversarial conditions remains\nunderexplored. This study evaluates the harmlessness of four leading MLLMs\n(GPT-4o, Claude Sonnet 3.5, Pixtral 12B, and Qwen VL Plus) when exposed to\nadversarial prompts across text-only and multimodal formats. A team of 26 red\nteamers generated 726 prompts targeting three harm categories: illegal\nactivity, disinformation, and unethical behaviour. These prompts were submitted\nto each model, and 17 annotators rated 2,904 model outputs for harmfulness\nusing a 5-point scale. Results show significant differences in vulnerability\nacross models and modalities. Pixtral 12B exhibited the highest rate of harmful\nresponses (~62%), while Claude Sonnet 3.5 was the most resistant (~10%).\nContrary to expectations, text-only prompts were slightly more effective at\nbypassing safety mechanisms than multimodal ones. Statistical analysis\nconfirmed that both model type and input modality were significant predictors\nof harmfulness. These findings underscore the urgent need for robust,\nmultimodal safety benchmarks as MLLMs are deployed more widely.", "AI": {"tldr": "The study evaluates the safety of four leading MLLMs under adversarial conditions, finding significant differences in their vulnerability to harmful outputs.", "motivation": "To assess the harmlessness of MLLMs when exposed to adversarial prompts, addressing the underexplored area of their safety.", "method": "A team generated adversarial prompts targeting illegal activity, disinformation, and unethical behavior, which were rated for harmfulness by annotators.", "result": "Pixtral 12B showed the highest rate of harmful responses (~62%), while Claude Sonnet 3.5 was the most resistant (~10%). Text-only prompts were slightly more effective at bypassing safety mechanisms than multimodal ones.", "conclusion": "The study highlights the need for robust safety benchmarks for MLLMs in diverse applications."}}
{"id": "2509.15293", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2509.15293", "abs": "https://arxiv.org/abs/2509.15293", "authors": ["Dinura Dissanayake", "Ahmed Heakl", "Omkar Thawakar", "Noor Ahsan", "Ritesh Thawkar", "Ketan More", "Jean Lahoud", "Rao Anwer", "Hisham Cholakkal", "Ivan Laptev", "Fahad Shahbaz Khan", "Salman Khan"], "title": "How Good are Foundation Models in Step-by-Step Embodied Reasoning?", "comment": null, "summary": "Embodied agents operating in the physical world must make decisions that are\nnot only effective but also safe, spatially coherent, and grounded in context.\nWhile recent advances in large multimodal models (LMMs) have shown promising\ncapabilities in visual understanding and language generation, their ability to\nperform structured reasoning for real-world embodied tasks remains\nunderexplored. In this work, we aim to understand how well foundation models\ncan perform step-by-step reasoning in embodied environments. To this end, we\npropose the Foundation Model Embodied Reasoning (FoMER) benchmark, designed to\nevaluate the reasoning capabilities of LMMs in complex embodied decision-making\nscenarios. Our benchmark spans a diverse set of tasks that require agents to\ninterpret multimodal observations, reason about physical constraints and\nsafety, and generate valid next actions in natural language. We present (i) a\nlarge-scale, curated suite of embodied reasoning tasks, (ii) a novel evaluation\nframework that disentangles perceptual grounding from action reasoning, and\n(iii) empirical analysis of several leading LMMs under this setting. Our\nbenchmark includes over 1.1k samples with detailed step-by-step reasoning\nacross 10 tasks and 8 embodiments, covering three different robot types. Our\nresults highlight both the potential and current limitations of LMMs in\nembodied reasoning, pointing towards key challenges and opportunities for\nfuture research in robot intelligence. Our data and code will be made publicly\navailable.", "AI": {"tldr": "研究提出了FoMER基准测试以评估LMMs在实体环境中的推理能力，指出了LMMs在实体推理方面的能力和限制，为未来机器人智能研究提供了方向。", "motivation": "在实体环境中执行决策的实体代理不仅需要有效，还需要安全和具有空间一致性。虽然大型多模态模型在视觉理解和语言生成方面显示出很大的潜力，但在实体任务中的结构化推理能力仍需进一步探索。", "method": "本研究提出了Foundation Model Embodied Reasoning (FoMER)基准测试，用于评估大型多模态模型（LMMs）在复杂实体决策场景中的推理能力。该基准测试包括一套大规模且精心策划的实体推理任务，一个将感知接地与行动推理分离的新颖评估框架，并对几种当前领先的LMMs进行了实证分析。", "result": "该基准测试包括超过1.1k个样本，涵盖了10个任务和8种实体形态，涉及三种不同类型的机器人。结果表明LMMs在实体推理方面既有潜力也有局限性。", "conclusion": "FoMER基准测试揭示了LMMs在实体推理任务中的表现，指出了未来机器人智能研究的关键挑战和机遇。研究的数据和代码将公开提供。"}}
{"id": "2509.15485", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.15485", "abs": "https://arxiv.org/abs/2509.15485", "authors": ["Ahmed Abdou"], "title": "mucAI at BAREC Shared Task 2025: Towards Uncertainty Aware Arabic Readability Assessment", "comment": null, "summary": "We present a simple, model-agnostic post-processing technique for\nfine-grained Arabic readability classification in the BAREC 2025 Shared Task\n(19 ordinal levels). Our method applies conformal prediction to generate\nprediction sets with coverage guarantees, then computes weighted averages using\nsoftmax-renormalized probabilities over the conformal sets. This\nuncertainty-aware decoding improves Quadratic Weighted Kappa (QWK) by reducing\nhigh-penalty misclassifications to nearer levels. Our approach shows consistent\nQWK improvements of 1-3 points across different base models. In the strict\ntrack, our submission achieves QWK scores of 84.9\\%(test) and 85.7\\% (blind\ntest) for sentence level, and 73.3\\% for document level. For Arabic educational\nassessment, this enables human reviewers to focus on a handful of plausible\nlevels, combining statistical guarantees with practical usability.", "AI": {"tldr": "提出了一种简单的后处理技术，用于提高细粒度阿拉伯语可读性分类的QWK分数。这种方法减少了高惩罚误分类，对不同模型提高了1-3个QWK分数点，并在BAREC 2025共享任务中取得了优异的QWK分数。", "motivation": "我们的方法旨在通过减少高惩罚的误分类并提高QWK分数，改善细粒度阿拉伯语可读性分类效果。", "method": "我们提出了一种简单的、与模型无关的后处理技术，应用于细粒度阿拉伯语可读性分类任务。该方法通过生成具有覆盖保证的预测集来应用符合性预测，然后使用softmax重归一化概率的加权平均计算。这种方法提高了不确定性意识解码，减少了高惩罚误分类，从而改善了Quadratic Weighted Kappa (QWK)分数。", "result": "对不同的基础模型，我们的方法一致改善了QWK分数，提高了1-3个分数点。严格赛道的提交在句子级别上实现了84.9%(测试集)和85.7%(盲测集)的QWK分数，在文档级别上达到了73.3%的QWK分数。", "conclusion": "此方法为阿拉伯语教育评估提供了一种结合统计保证和实际实用性的方案，使人工审阅者能够关注少数几个可能的等级。"}}
{"id": "2509.15330", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.15330", "abs": "https://arxiv.org/abs/2509.15330", "authors": ["Min Zhang", "Bo Jiang", "Jie Zhou", "Yimeng Liu", "Xin Lin"], "title": "CoDoL: Conditional Domain Prompt Learning for Out-of-Distribution Generalization", "comment": null, "summary": "Recent advances in pre-training vision-language models (VLMs), e.g.,\ncontrastive language-image pre-training (CLIP) methods, have shown great\npotential in learning out-of-distribution (OOD) representations. Despite\nshowing competitive performance, the prompt-based CLIP methods still suffer\nfrom: i) inaccurate text descriptions, which leads to degraded accuracy and\nrobustness, and poses a challenge for zero-shot CLIP methods. ii) limited\nvision-language embedding alignment, which significantly affects the\ngeneralization performance. To tackle the above issues, this paper proposes a\nnovel Conditional Domain prompt Learning (CoDoL) method, which utilizes\nreadily-available domain information to form prompts and improves the\nvision-language embedding alignment for improving OOD generalization. To\ncapture both instance-specific and domain-specific information, we further\npropose a lightweight Domain Meta Network (DMN) to generate input-conditional\ntokens for images in each domain. Extensive experiments on four OOD benchmarks\n(PACS, VLCS, OfficeHome and DigitDG) validate the effectiveness of our proposed\nCoDoL in terms of improving the vision-language embedding alignment as well as\nthe out-of-distribution generalization performance.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2509.15515", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.15515", "abs": "https://arxiv.org/abs/2509.15515", "authors": ["Hantao Yang", "Hong Xie", "Defu Lian", "Enhong Chen"], "title": "LLM Cache Bandit Revisited: Addressing Query Heterogeneity for Cost-Effective LLM Inference", "comment": null, "summary": "This paper revisits the LLM cache bandit problem, with a special focus on\naddressing the query heterogeneity for cost-effective LLM inference. Previous\nworks often assume uniform query sizes. Heterogeneous query sizes introduce a\ncombinatorial structure for cache selection, making the cache replacement\nprocess more computationally and statistically challenging. We treat optimal\ncache selection as a knapsack problem and employ an accumulation-based strategy\nto effectively balance computational overhead and cache updates. In theoretical\nanalysis, we prove that the regret of our algorithm achieves an $O(\\sqrt{MNT})$\nbound, improving the coefficient of $\\sqrt{MN}$ compared to the $O(MN\\sqrt{T})$\nresult in Berkeley, where $N$ is the total number of queries and $M$ is the\ncache size. Additionally, we also provide a problem-dependent bound, which was\nabsent in previous works. The experiment rely on real-world data show that our\nalgorithm reduces the total cost by approximately 12\\%.", "AI": {"tldr": "本文解决了大型语言模型（LLM）在异构查询条件下的成本效益推理问题，提出了一种新的基于累积的策略来处理缓存选择问题，并通过理论分析和实验证明了其有效性和先进性。", "motivation": "该论文重新审视了大型语言模型（LLM）缓存多臂赌博问题，特别是关注解决成本效益的LLM推理中的查询异构性问题。", "method": "我们将缓存选择视为背包问题，并采用基于累积的策略来有效平衡计算开销和缓存更新。", "result": "理论上，我们证明了算法的后悔值达到了$O(\\sqrt{MNT})$的界，相对于Berkeley的$O(MN\\sqrt{T})$结果，在$\\sqrt{MN}$系数上有所改进。此外，我们还提供了一个问题依赖的界，这是之前工作所没有的。实验结果显示，我们的算法减少了大约12%的总成本。", "conclusion": "本文提出的方法相较于之前的工作，在理论分析上有了改进，并且在现实数据集上的实验验证了其可以减少大约12%的总成本，展示出实际应用效果。"}}
{"id": "2509.15333", "categories": ["cs.CV", "cs.AI", "cs.LG", "eess.IV"], "pdf": "https://arxiv.org/pdf/2509.15333", "abs": "https://arxiv.org/abs/2509.15333", "authors": ["Yulin Wang", "Yang Yue", "Yang Yue", "Huanqian Wang", "Haojun Jiang", "Yizeng Han", "Zanlin Ni", "Yifan Pu", "Minglei Shi", "Rui Lu", "Qisen Yang", "Andrew Zhao", "Zhuofan Xia", "Shiji Song", "Gao Huang"], "title": "Emulating Human-like Adaptive Vision for Efficient and Flexible Machine Visual Perception", "comment": null, "summary": "Human vision is highly adaptive, efficiently sampling intricate environments\nby sequentially fixating on task-relevant regions. In contrast, prevailing\nmachine vision models passively process entire scenes at once, resulting in\nexcessive resource demands scaling with spatial-temporal input resolution and\nmodel size, yielding critical limitations impeding both future advancements and\nreal-world application. Here we introduce AdaptiveNN, a general framework\naiming to drive a paradigm shift from 'passive' to 'active, adaptive' vision\nmodels. AdaptiveNN formulates visual perception as a coarse-to-fine sequential\ndecision-making process, progressively identifying and attending to regions\npertinent to the task, incrementally combining information across fixations,\nand actively concluding observation when sufficient. We establish a theory\nintegrating representation learning with self-rewarding reinforcement learning,\nenabling end-to-end training of the non-differentiable AdaptiveNN without\nadditional supervision on fixation locations. We assess AdaptiveNN on 17\nbenchmarks spanning 9 tasks, including large-scale visual recognition,\nfine-grained discrimination, visual search, processing images from real driving\nand medical scenarios, language-driven embodied AI, and side-by-side\ncomparisons with humans. AdaptiveNN achieves up to 28x inference cost reduction\nwithout sacrificing accuracy, flexibly adapts to varying task demands and\nresource budgets without retraining, and provides enhanced interpretability via\nits fixation patterns, demonstrating a promising avenue toward efficient,\nflexible, and interpretable computer vision. Furthermore, AdaptiveNN exhibits\nclosely human-like perceptual behaviors in many cases, revealing its potential\nas a valuable tool for investigating visual cognition. Code is available at\nhttps://github.com/LeapLabTHU/AdaptiveNN.", "AI": {"tldr": "AdaptiveNN是一种新的框架，通过模拟人类的视觉适应性来进行更高效和灵活的机器学习，显著降低了计算成本，同时保持了精确度，是未来计算机视觉发展的一个有潜力的方向。", "motivation": "当前的机器视觉模型被动地一次性处理整个场景，导致资源需求随空间-时间输入分辨率和模型尺寸的增加而增加，这对未来的进步和实际应用造成了阻碍。", "method": "AdaptiveNN,一种旨在推动从'被动'到'主动,适应性'视觉模型范式转变的通用框架。它将视觉感知形式化为一个从粗到细的顺序决策过程，逐步识别和关注与任务相关的区域，逐步结合各次注视的信息，并主动结束观察当达到足够的信息量时。该框架包含了集成表示学习与自我奖励的强化学习的理论，以便适应不可微分的AdaptiveNN的端到端训练。", "result": "AdaptiveNN在17个跨9个任务的基准测试中进行了评估，这些任务包括大规模视觉识别，细粒度辨别，视觉搜索，处理来自真实驾驶和医疗场景的图像，语言驱动的具身AI以及与人类的并排比较。AdaptiveNN实现了最高达28倍的推理成本减少而不会牺牲精度，能够灵活地适应不同的任务需求和资源预算，无需重新训练，并通过注视模式提供了增强的可解释性。", "conclusion": "AdaptiveNN展示了作为一种高效的，灵活的和可解释的计算机视觉方法的前景途径，并且在许多情况下表现出与人类视觉感知行为相似的特点。这些特征使其成为研究视觉认知的有价值的工具。"}}
{"id": "2509.15518", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.15518", "abs": "https://arxiv.org/abs/2509.15518", "authors": ["Siyang Wu", "Zhewei Sun"], "title": "How do Language Models Generate Slang: A Systematic Comparison between Human and Machine-Generated Slang Usages", "comment": null, "summary": "Slang is a commonly used type of informal language that poses a daunting\nchallenge to NLP systems. Recent advances in large language models (LLMs),\nhowever, have made the problem more approachable. While LLM agents are becoming\nmore widely applied to intermediary tasks such as slang detection and slang\ninterpretation, their generalizability and reliability are heavily dependent on\nwhether these models have captured structural knowledge about slang that align\nwell with human attested slang usages. To answer this question, we contribute a\nsystematic comparison between human and machine-generated slang usages. Our\nevaluative framework focuses on three core aspects: 1) Characteristics of the\nusages that reflect systematic biases in how machines perceive slang, 2)\nCreativity reflected by both lexical coinages and word reuses employed by the\nslang usages, and 3) Informativeness of the slang usages when used as\ngold-standard examples for model distillation. By comparing human-attested\nslang usages from the Online Slang Dictionary (OSD) and slang generated by\nGPT-4o and Llama-3, we find significant biases in how LLMs perceive slang. Our\nresults suggest that while LLMs have captured significant knowledge about the\ncreative aspects of slang, such knowledge does not align with humans\nsufficiently to enable LLMs for extrapolative tasks such as linguistic\nanalyses.", "AI": {"tldr": "该研究表明，尽管大型语言模型在某些方面掌握了俚语的创意使用，但其认知方式仍存在显著偏见，限制了它们在语言分析任务中的应用。", "motivation": "尽管大型语言模型（LLMs）的进步使其能更好地应对俚语挑战，但这些模型的泛化能力和可靠性取决于其是否捕捉到了与人类使用相吻合的俚语结构知识。因此，研究旨在系统比较人类和机器生成的俚语用法，以评估LLMs在处理俚语方面的表现。", "method": "该研究通过比较人类和机器生成的俚语用法来系统性地评估大型语言模型对俚语的理解能力。评估框架关注三个方面：1) 机器对俚语的系统性偏见特征，2) 通过词汇创造和词汇再利用反映的创造力，3) 俚语用作模型蒸馏的黄金标准示例时提供的信息量。", "result": "研究结果表明，尽管大型语言模型（LLMs）捕捉到了俚语创造方面的大量知识，但这些知识与人类的认知并不充分吻合，这使得LLMs在诸如语言分析等外推任务上的表现能力受到限制。", "conclusion": "研究发现，虽然LLMs在处理俚语的创造性方面表现出了显著的知识，但这些知识并未与人类认知充分对齐，从而限制了模型在更广泛语言任务上的表现。"}}
{"id": "2509.15342", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.15342", "abs": "https://arxiv.org/abs/2509.15342", "authors": ["Jiuyi Xu", "Qing Jin", "Meida Chen", "Andrew Feng", "Yang Sui", "Yangming Shi"], "title": "LowDiff: Efficient Diffusion Sampling with Low-Resolution Condition", "comment": null, "summary": "Diffusion models have achieved remarkable success in image generation but\ntheir practical application is often hindered by the slow sampling speed. Prior\nefforts of improving efficiency primarily focus on compressing models or\nreducing the total number of denoising steps, largely neglecting the\npossibility to leverage multiple input resolutions in the generation process.\nIn this work, we propose LowDiff, a novel and efficient diffusion framework\nbased on a cascaded approach by generating increasingly higher resolution\noutputs. Besides, LowDiff employs a unified model to progressively refine\nimages from low resolution to the desired resolution. With the proposed\narchitecture design and generation techniques, we achieve comparable or even\nsuperior performance with much fewer high-resolution sampling steps. LowDiff is\napplicable to diffusion models in both pixel space and latent space. Extensive\nexperiments on both conditional and unconditional generation tasks across\nCIFAR-10, FFHQ and ImageNet demonstrate the effectiveness and generality of our\nmethod. Results show over 50% throughput improvement across all datasets and\nsettings while maintaining comparable or better quality. On unconditional\nCIFAR-10, LowDiff achieves an FID of 2.11 and IS of 9.87, while on conditional\nCIFAR-10, an FID of 1.94 and IS of 10.03. On FFHQ 64x64, LowDiff achieves an\nFID of 2.43, and on ImageNet 256x256, LowDiff built on LightningDiT-B/1\nproduces high-quality samples with a FID of 4.00 and an IS of 195.06, together\nwith substantial efficiency gains.", "AI": {"tldr": "提出LowDiff，一种基于级联方法和逐步细化的新型高效扩散框架，能够提高扩散模型在图像生成任务中的效率，同时保持或改善生成质量。", "motivation": "扩散模型在图像生成领域取得了显著的成功，但缓慢的采样速度限制了其实际应用。以前的努力主要集中在压缩模型或减少总的去噪步骤，而忽视了在生成过程中利用多种输入分辨率的潜力。", "method": "LowDiff是一种基于级联方法的创新且高效的扩散框架，通过生成分辨率逐渐增加的输出来操作，同时使用一个统一的模型逐步将图像从低分辨率细化到所需分辨率。", "result": "实验结果表明，相比于其他设置，在所有数据集上，我们的方法的吞吐量都提高了50%以上，同时保持了相当或更好的质量。", "conclusion": "LowDiff方法证明了在保持生成质量的同时，通过级联方法和使用统一的逐步细化模型来提高扩散模型效率的有效性。"}}
{"id": "2509.15549", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.15549", "abs": "https://arxiv.org/abs/2509.15549", "authors": ["Chunguang Zhao", "Yilun Liu", "Pufan Zeng", "Yuanchang Luo", "Shimin Tao", "Minggui He", "Weibin Meng", "Song Xu", "Ziang Chen", "Chen Liu", "Hongxia Ma", "Li Zhang", "Boxing Chen", "Daimeng Wei"], "title": "A method for improving multilingual quality and diversity of instruction fine-tuning datasets", "comment": null, "summary": "Multilingual Instruction Fine-Tuning (IFT) is essential for enabling large\nlanguage models (LLMs) to generalize effectively across diverse linguistic and\ncultural contexts. However, the scarcity of high-quality multilingual training\ndata and corresponding building method remains a critical bottleneck. While\ndata selection has shown promise in English settings, existing methods often\nfail to generalize across languages due to reliance on simplistic heuristics or\nlanguage-specific assumptions. In this work, we introduce Multilingual Data\nQuality and Diversity (M-DaQ), a novel method for improving LLMs\nmultilinguality, by selecting high-quality and semantically diverse\nmultilingual IFT samples. We further conduct the first systematic investigation\nof the Superficial Alignment Hypothesis (SAH) in multilingual setting.\nEmpirical results across 18 languages demonstrate that models fine-tuned with\nM-DaQ method achieve significant performance gains over vanilla baselines over\n60% win rate. Human evaluations further validate these gains, highlighting the\nincrement of cultural points in the response. We release the M-DaQ code to\nsupport future research.", "AI": {"tldr": "Introducing M-DaQ, a method for selecting high-quality multilingual data to improve the performance of large language models, which shows significant gains across 18 languages.", "motivation": "To address the challenges in achieving effective multilinguality in large language models due to the lack of high-quality multilingual training data and the limitations of existing data selection methods.", "method": "Multilingual Data Quality and Diversity (M-DaQ), a method for selecting high-quality and semantically diverse multilingual Instruction Fine-Tuning (IFT) samples to enhance the performance of large language models across multiple languages.", "result": "Empirical results across 18 languages show that models fine-tuned with M-DaQ achieve significant performance gains over vanilla baselines with a win rate over 60%. Human evaluations further confirm these improvements, particularly noting an increase in culturally relevant content in responses.", "conclusion": "The introduction of M-DaQ demonstrates its effectiveness in improving the multilinguality of large language models, providing a robust method for enhancing performance and cultural relevance across diverse languages."}}
{"id": "2509.15357", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.15357", "abs": "https://arxiv.org/abs/2509.15357", "authors": ["Yu Chang", "Jiahao Chen", "Anzhe Cheng", "Paul Bogdan"], "title": "MaskAttn-SDXL: Controllable Region-Level Text-To-Image Generation", "comment": "Submitted to ICASSP 2026", "summary": "Text-to-image diffusion models achieve impressive realism but often suffer\nfrom compositional failures on prompts with multiple objects, attributes, and\nspatial relations, resulting in cross-token interference where entities\nentangle, attributes mix across objects, and spatial cues are violated. To\naddress these failures, we propose MaskAttn-SDXL,a region-level gating\nmechanism applied to the cross-attention logits of Stable Diffusion XL(SDXL)'s\nUNet. MaskAttn-SDXL learns a binary mask per layer, injecting it into each\ncross-attention logit map before softmax to sparsify token-to-latent\ninteractions so that only semantically relevant connections remain active. The\nmethod requires no positional encodings, auxiliary tokens, or external region\nmasks, and preserves the original inference path with negligible overhead. In\npractice, our model improves spatial compliance and attribute binding in\nmulti-object prompts while preserving overall image quality and diversity.\nThese findings demonstrate that logit-level maksed cross-attention is an\ndata-efficient primitve for enforcing compositional control, and our method\nthus serves as a practical extension for spatial control in text-to-image\ngeneration.", "AI": {"tldr": "MaskAttn-SDXL通过一种区域级别的门控机制减少跨令牌干扰，改善了多对象提示的属性绑定和空间一致性，有效地增强了文本到图像生成中的组合控制。", "motivation": "由于多对象、属性和空间关系提示导致的跨令牌干扰，现有文本到图像扩散模型在合成方面有所欠缺，作者试图通过MaskAttn-SDXL来解决这些问题。", "method": "MaskAttn-SDXL是一种应用于Stable Diffusion XL (SDXL) UNet的交叉注意力对数门控机制，通过学习每一层的二进制掩码，向每个交叉注意力对数图注入掩码以稀疏化token到潜变量的交互，确保只有语义相关的连接保持活跃。这种方法不需要位置编码、辅助token或外部区域掩码，并且保持原始推理路径的同时几乎没有增加额外开销。", "result": "MaskAttn-SDXL提高了多对象提示的空间一致性和属性绑定，同时保留了整体图像质量和多样性。", "conclusion": "MaskAttn-SDXL证明了在logit级别应用掩码交叉注意力是一种高效的数据驱动方式，能够实现组合控制，为文本到图像生成的空间控制提供了实用扩展。"}}
{"id": "2509.15550", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.15550", "abs": "https://arxiv.org/abs/2509.15550", "authors": ["Xiaowei Zhu", "Yubing Ren", "Fang Fang", "Qingfeng Tan", "Shi Wang", "Yanan Cao"], "title": "DNA-DetectLLM: Unveiling AI-Generated Text via a DNA-Inspired Mutation-Repair Paradigm", "comment": "NeurIPS 2025 Spotlight", "summary": "The rapid advancement of large language models (LLMs) has blurred the line\nbetween AI-generated and human-written text. This progress brings societal\nrisks such as misinformation, authorship ambiguity, and intellectual property\nconcerns, highlighting the urgent need for reliable AI-generated text detection\nmethods. However, recent advances in generative language modeling have resulted\nin significant overlap between the feature distributions of human-written and\nAI-generated text, blurring classification boundaries and making accurate\ndetection increasingly challenging. To address the above challenges, we propose\na DNA-inspired perspective, leveraging a repair-based process to directly and\ninterpretably capture the intrinsic differences between human-written and\nAI-generated text. Building on this perspective, we introduce DNA-DetectLLM, a\nzero-shot detection method for distinguishing AI-generated and human-written\ntext. The method constructs an ideal AI-generated sequence for each input,\niteratively repairs non-optimal tokens, and quantifies the cumulative repair\neffort as an interpretable detection signal. Empirical evaluations demonstrate\nthat our method achieves state-of-the-art detection performance and exhibits\nstrong robustness against various adversarial attacks and input lengths.\nSpecifically, DNA-DetectLLM achieves relative improvements of 5.55% in AUROC\nand 2.08% in F1 score across multiple public benchmark datasets.", "AI": {"tldr": "我们提出了一种名为DNA-DetectLLM的零样本方法，用于检测AI生成的文本，该方法通过构建理想的AI生成序列并通过迭代修复非最优标记来捕捉人写文本和AI生成文本之间的差异。实验证明，该方法在多个公共基准数据集上取得了比之前最好的检测性能高出5.55%和2.08%的表现，并且能抵御各种对抗攻击。", "motivation": "随着大型语言模型的快速发展，AI生成文本与人类编写文本之间的界限越来越模糊，这带来了诸如虚假信息、作者身份模糊和知识产权问题等社会风险，表明需要可靠的方法来检测AI生成的文本。然而，最近生成语言模型的进步使得人类编写和AI生成文本的特征分布重叠显著，使得准确检测越来越具有挑战性。", "method": "我们提出了一种DNA启发的视角，通过基于修复过程直接且可解释地捕捉人写文本和AI生成文本之间的内在差异。具体来说，我们引入了DNA-DetectLLM这一零样本检测方法，该方法为每个输入构建一个理想的AI生成序列，迭代修复非最优标记，并将累积修复努力量作为可解释的检测信号。", "result": "实验证明，我们的方法实现了最先进的检测性能，表现出对各种对抗攻击和输入长度的强鲁棒性。DNA-DetectLLM在多个公开数据集上实现了5.55%的AUROC和2.08%的F1评分的相对改进。", "conclusion": "该研究提出了DNA-DetectLLM作为先进的文本检测方法，通过基于DNA修复理念设计模型，提升了对AI生成文本检测的准确性与鲁棒性。"}}
{"id": "2509.15391", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.15391", "abs": "https://arxiv.org/abs/2509.15391", "authors": ["Mst Tasnim Pervin", "George Bebis", "Fang Jiang", "Alireza Tavakkoli"], "title": "RaceGAN: A Framework for Preserving Individuality while Converting Racial Information for Image-to-Image Translation", "comment": null, "summary": "Generative adversarial networks (GANs) have demonstrated significant progress\nin unpaired image-to-image translation in recent years for several\napplications. CycleGAN was the first to lead the way, although it was\nrestricted to a pair of domains. StarGAN overcame this constraint by tackling\nimage-to-image translation across various domains, although it was not able to\nmap in-depth low-level style changes for these domains. Style mapping via\nreference-guided image synthesis has been made possible by the innovations of\nStarGANv2 and StyleGAN. However, these models do not maintain individuality and\nneed an extra reference image in addition to the input. Our study aims to\ntranslate racial traits by means of multi-domain image-to-image translation. We\npresent RaceGAN, a novel framework capable of mapping style codes over several\ndomains during racial attribute translation while maintaining individuality and\nhigh level semantics without relying on a reference image. RaceGAN outperforms\nother models in translating racial features (i.e., Asian, White, and Black)\nwhen tested on Chicago Face Dataset. We also give quantitative findings\nutilizing InceptionReNetv2-based classification to demonstrate the\neffectiveness of our racial translation. Moreover, we investigate how well the\nmodel partitions the latent space into distinct clusters of faces for each\nethnic group.", "AI": {"tldr": "本研究提出RaceGAN，一种可以跨多个领域映射样式代码、保持个体性和高级语义且无需参考图像的新框架，用于翻译种族特征，并在测试中显示出优越的性能。", "motivation": "CycleGAN和StarGAN在图像到图像翻译方面取得了显著进展，但前者限于两个领域，后者无法深入映射低层样式变化，且需要额外的参考图像。因此，我们的研究旨在通过多领域图像到图像翻译来翻译种族特征，并解决上述问题。", "method": "本研究提出了一种名为RaceGAN的新框架，旨在通过多领域图像到图像翻译来翻译种族特征。该框架能够在种族属性翻译过程中跨多个领域映射样式代码，同时保持个体性和高级语义，且无需依赖参考图像。", "result": "测试表明，RaceGAN在翻译种族特征（如亚洲、白人和黑人）方面优于其他模型，并使用基于InceptionReNetv2的分类给出了定量结果，证明了其种族翻译的有效性。", "conclusion": "RaceGAN成功地实现了保持个体性和高级语义的多领域种族特征翻译，而无需依赖参考图像，在种族特征转换方面表现出色。"}}
{"id": "2509.15556", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.15556", "abs": "https://arxiv.org/abs/2509.15556", "authors": ["Ping Guo", "Yubing Ren", "Binbin Liu", "Fengze Liu", "Haobin Lin", "Yifan Zhang", "Bingni Zhang", "Taifeng Wang", "Yin Zheng"], "title": "Exploring Polyglot Harmony: On Multilingual Data Allocation for Large Language Models Pretraining", "comment": null, "summary": "Large language models (LLMs) have become integral to a wide range of\napplications worldwide, driving an unprecedented global demand for effective\nmultilingual capabilities. Central to achieving robust multilingual performance\nis the strategic allocation of language proportions within training corpora.\nHowever, determining optimal language ratios is highly challenging due to\nintricate cross-lingual interactions and sensitivity to dataset scale. This\npaper introduces Climb (Cross-Lingual Interaction-aware Multilingual\nBalancing), a novel framework designed to systematically optimize multilingual\ndata allocation. At its core, Climb introduces a cross-lingual\ninteraction-aware language ratio, explicitly quantifying each language's\neffective allocation by capturing inter-language dependencies. Leveraging this\nratio, Climb proposes a principled two-step optimization procedure--first\nequalizing marginal benefits across languages, then maximizing the magnitude of\nthe resulting language allocation vectors--significantly simplifying the\ninherently complex multilingual optimization problem. Extensive experiments\nconfirm that Climb can accurately measure cross-lingual interactions across\nvarious multilingual settings. LLMs trained with Climb-derived proportions\nconsistently achieve state-of-the-art multilingual performance, even achieving\ncompetitive performance with open-sourced LLMs trained with more tokens.", "AI": {"tldr": "本文介绍了一种名为Climb的框架，用于优化多语言数据的分配，以解决大型语言模型中语言比例分配的问题，实现更好的多语言性能。", "motivation": "大型语言模型(LLMs)在全球范围内广泛的应用和有效多语言能力的需求推动了其发展。但要实现稳健的多语言性能，一个核心问题是如何在训练语料库中战略性地分配语言比例，这由于跨语言交互的复杂性和数据集规模的敏感性非常具有挑战性。", "method": "Climb框架通过引入跨语言交互感知的语言比例，量化每种语言的有效分配，捕捉语言间的依赖关系。该框架提出一个双步优化过程：首先使各语言的边际效益均衡化，然后最大化生成的语言分配向量的大小，从而简化复杂的多语言优化问题。", "result": "实验结果证明Climb可以准确测量各种多语言设置中的跨语言交互情况。使用Climb得出的比例训练的LLM可以稳定地达到最先进的多语言性能，甚至在使用较少token的情况下与开源LLM具有竞争力。", "conclusion": "Climb框架提供了一种新的方法来解决多语言性能优化问题，通过双步优化过程简化了复杂问题，证明了其在提高多语言性能方面的有效性和竞争力。"}}
{"id": "2509.15393", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.15393", "abs": "https://arxiv.org/abs/2509.15393", "authors": ["Kunal Rathore", "Prasad Tadepalli"], "title": "Generating Part-Based Global Explanations Via Correspondence", "comment": null, "summary": "Deep learning models are notoriously opaque. Existing explanation methods\noften focus on localized visual explanations for individual images.\nConcept-based explanations, while offering global insights, require extensive\nannotations, incurring significant labeling cost. We propose an approach that\nleverages user-defined part labels from a limited set of images and efficiently\ntransfers them to a larger dataset. This enables the generation of global\nsymbolic explanations by aggregating part-based local explanations, ultimately\nproviding human-understandable explanations for model decisions on a large\nscale.", "AI": {"tldr": "提出了利用用户定义的部分标签进行局部解释并聚合生成全局符号解释的方法，从而在大规模数据上提供可理解的模型决策解释。", "motivation": "现有的解释方法通常只关注个体图像的局部视觉解释，而概念解释虽然提供全局见解，但需要大量的注释。", "method": "使用少量图像中用户定义的部分标签，并高效地将它们转移到更大的数据集上，通过聚合基于部分的局部解释来生成全局符号解释。", "result": "能够在大规模数据上提供人类可理解的模型决策解释。", "conclusion": "该方法有效地解决了现有解释方法的局限，提供了一种成本高效且易于理解的全局解释方法。"}}
{"id": "2509.15560", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.15560", "abs": "https://arxiv.org/abs/2509.15560", "authors": ["Gary Lupyan", "Hunter Gentry", "Martin Zettersten"], "title": "How important is language for human-like intelligence?", "comment": null, "summary": "We use language to communicate our thoughts. But is language merely the\nexpression of thoughts, which are themselves produced by other, nonlinguistic\nparts of our minds? Or does language play a more transformative role in human\ncognition, allowing us to have thoughts that we otherwise could (or would) not\nhave? Recent developments in artificial intelligence (AI) and cognitive science\nhave reinvigorated this old question. We argue that language may hold the key\nto the emergence of both more general AI systems and central aspects of human\nintelligence. We highlight two related properties of language that make it such\na powerful tool for developing domain--general abilities. First, language\noffers compact representations that make it easier to represent and reason\nabout many abstract concepts (e.g., exact numerosity). Second, these compressed\nrepresentations are the iterated output of collective minds. In learning a\nlanguage, we learn a treasure trove of culturally evolved abstractions. Taken\ntogether, these properties mean that a sufficiently powerful learning system\nexposed to language--whether biological or artificial--learns a compressed\nmodel of the world, reverse engineering many of the conceptual and causal\nstructures that support human (and human-like) thought.", "AI": {"tldr": "语言可能对于更通用的人工智能系统的出现和人类智能的关键方面是至关重要的。语言的两个关键特性，即紧凑表示和集体心智迭代输出，使得它成为开发通用能力的强大工具。", "motivation": "随着人工智能和认知科学的发展，重新提出了语言对人类认知的影响问题，以及语言是否在人脑中扮演了更具有变革性的角色。", "method": "通过分析语言在人工智能和认知科学中的作用来探讨语言对思维的影响。认为语言对于更通用的人工智能系统的出现和人类智能的核心方面可能是关键。文章指出语言的两个相关特性使其成为开发通用能力的强大工具：紧凑的表示形式和集体心智的迭代输出。", "result": "语言作为集体心智迭代输出的紧凑表示形式，可以反向工程出支持人类或其他人类类似思维的概念和因果结构。从而，强大的学习系统（无论是生物系统还是人工系统）在接触语言后，可以学习到一个压缩的世界模型。", "conclusion": "语言不仅是思维方式的表达，而且还对思维的形成起到了关键作用，尤其是在发展通用能力和推动人工智能系统的发展上。"}}
{"id": "2509.15406", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.15406", "abs": "https://arxiv.org/abs/2509.15406", "authors": ["Hui Xu", "Chi Liu", "Congcong Zhu", "Minghao Wang", "Youyang Qu", "Longxiang Gao"], "title": "Causal Fingerprints of AI Generative Models", "comment": "5 page. In submission", "summary": "AI generative models leave implicit traces in their generated images, which\nare commonly referred to as model fingerprints and are exploited for source\nattribution. Prior methods rely on model-specific cues or synthesis artifacts,\nyielding limited fingerprints that may generalize poorly across different\ngenerative models. We argue that a complete model fingerprint should reflect\nthe causality between image provenance and model traces, a direction largely\nunexplored. To this end, we conceptualize the \\emph{causal fingerprint} of\ngenerative models, and propose a causality-decoupling framework that\ndisentangles it from image-specific content and style in a semantic-invariant\nlatent space derived from pre-trained diffusion reconstruction residual. We\nfurther enhance fingerprint granularity with diverse feature representations.\nWe validate causality by assessing attribution performance across\nrepresentative GANs and diffusion models and by achieving source anonymization\nusing counterfactual examples generated from causal fingerprints. Experiments\nshow our approach outperforms existing methods in model attribution, indicating\nstrong potential for forgery detection, model copyright tracing, and identity\nprotection.", "AI": {"tldr": "我们提出了因果指纹的概念和相关的解耦框架，用于改进生成模型的来源归因，并且在模型归因方面优于现有方法。", "motivation": "现有的方法依赖于模型特定的线索或合成缺陷，导致指纹可能在不同的生成模型之间泛化性不好。我们主张一个完整的模型指纹应该反映出图像来源与模型痕迹之间的因果关系，这是一个尚未被广泛探索的方向。", "method": "我们提出了一种因果解耦框架，该框架可以在从预先训练的扩散重建残差中得到的语义不变的潜在空间中，将生成模型的因果指纹与图像特定内容和风格解耦。为了增强指纹的细节，我们还引入了多样的特征表示。", "result": "通过评估代表性对抗网络和扩散模型的归属性能以及使用因果指纹生成反现实样本来实现来源匿名化，我们的方法在模型归属方面超越了现有的方法。", "conclusion": "实验表明，我们的方法在模型归属方面优于现有的方法，显示出在伪造检测、模型版权跟踪和身份保护方面的强大潜力。"}}
{"id": "2509.15568", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.15568", "abs": "https://arxiv.org/abs/2509.15568", "authors": ["Junlong Jia", "Xing Wu", "Chaochen Gao", "Ziyang Chen", "Zijia Lin", "Zhongzhi Li", "Weinong Wang", "Haotian Xu", "Donghui Jin", "Debing Zhang", "Binghui Guo"], "title": "LiteLong: Resource-Efficient Long-Context Data Synthesis for LLMs", "comment": "work in progress", "summary": "High-quality long-context data is essential for training large language\nmodels (LLMs) capable of processing extensive documents, yet existing synthesis\napproaches using relevance-based aggregation face challenges of computational\nefficiency. We present LiteLong, a resource-efficient method for synthesizing\nlong-context data through structured topic organization and multi-agent debate.\nOur approach leverages the BISAC book classification system to provide a\ncomprehensive hierarchical topic organization, and then employs a debate\nmechanism with multiple LLMs to generate diverse, high-quality topics within\nthis structure. For each topic, we use lightweight BM25 retrieval to obtain\nrelevant documents and concatenate them into 128K-token training samples.\nExperiments on HELMET and Ruler benchmarks demonstrate that LiteLong achieves\ncompetitive long-context performance and can seamlessly integrate with other\nlong-dependency enhancement methods. LiteLong makes high-quality long-context\ndata synthesis more accessible by reducing both computational and data\nengineering costs, facilitating further research in long-context language\ntraining.", "AI": {"tldr": "LiteLong利用BISAC图书分类系统进行结构化主题组织和多代理辩论生成高质量长上下文数据，成本比传统方法低，在HELMET和Ruler基准上取得优秀效果。", "motivation": "解决当前基于相关性聚合方法生成长上下文数据计算效率低的问题，为训练能够处理长文档的LLMs提供高质量的长上下文数据。", "method": "采用BISAC图书分类系统进行主题层次组织，利用多代理辩论生成多样化话题，并使用轻量级BM25检索获取相关文档，生成128K-token的训练样本。", "result": "研究提出了LiteLong方法，利用结构化主题组织和多代理辩论机制生成适合训练大语言模型（LLMs）的高质量长上下文数据。通过使用BISAC图书分类系统进行主题组织，并结合多LLM辩论生成多样化内容。实验表明，LiteLong在HELMET和Ruler基准测试上表现出优异长上下文性能，且计算和数据工程成本减少，推进了长上下文语言模型训练研究。", "conclusion": "LiteLong通过结构化主题组织和低成本计算减少了生成高质量长上下文数据的难度，这有助于推进长上下文语言模型的研究。"}}
{"id": "2509.15416", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.15416", "abs": "https://arxiv.org/abs/2509.15416", "authors": ["Moinak Bhattacharya", "Angelica P. Kurtz", "Fabio M. Iwamoto", "Prateek Prasanna", "Gagandeep Singh"], "title": "NeuroRAD-FM: A Foundation Model for Neuro-Oncology with Distributionally Robust Training", "comment": null, "summary": "Neuro-oncology poses unique challenges for machine learning due to\nheterogeneous data and tumor complexity, limiting the ability of foundation\nmodels (FMs) to generalize across cohorts. Existing FMs also perform poorly in\npredicting uncommon molecular markers, which are essential for treatment\nresponse and risk stratification. To address these gaps, we developed a\nneuro-oncology specific FM with a distributionally robust loss function,\nenabling accurate estimation of tumor phenotypes while maintaining\ncross-institution generalization. We pretrained self-supervised backbones\n(BYOL, DINO, MAE, MoCo) on multi-institutional brain tumor MRI and applied\ndistributionally robust optimization (DRO) to mitigate site and class\nimbalance. Downstream tasks included molecular classification of common markers\n(MGMT, IDH1, 1p/19q, EGFR), uncommon alterations (ATRX, TP53, CDKN2A/2B, TERT),\ncontinuous markers (Ki-67, TP53), and overall survival prediction in IDH1\nwild-type glioblastoma at UCSF, UPenn, and CUIMC. Our method improved molecular\nprediction and reduced site-specific embedding differences. At CUIMC, mean\nbalanced accuracy rose from 0.744 to 0.785 and AUC from 0.656 to 0.676, with\nthe largest gains for underrepresented endpoints (CDKN2A/2B accuracy 0.86 to\n0.92, AUC 0.73 to 0.92; ATRX AUC 0.69 to 0.82; Ki-67 accuracy 0.60 to 0.69).\nFor survival, c-index improved at all sites: CUIMC 0.592 to 0.597, UPenn 0.647\nto 0.672, UCSF 0.600 to 0.627. Grad-CAM highlighted tumor and peri-tumoral\nregions, confirming interpretability. Overall, coupling FMs with DRO yields\nmore site-invariant representations, improves prediction of common and uncommon\nmarkers, and enhances survival discrimination, underscoring the need for\nprospective validation and integration of longitudinal and interventional\nsignals to advance precision neuro-oncology.", "AI": {"tldr": "通过使用分布鲁棒优化方法改进神经肿瘤学中的基础模型，以提高对常见和罕见分子标记的预测能力，以及生存预测的准确性。", "motivation": "现有的基础模型（FMs）在神经肿瘤学领域的应用由于异质性数据和肿瘤复杂性而受到限制，特别是在预测罕见分子标记上表现较差。这些标记对于治疗反应和风险分层至关重要。", "method": "我们开发了一个针对神经肿瘤学的特定基础模型（FMs），使用了分布鲁棒优化（DRO）来解决肿瘤异质性和数据多样性带来的挑战，并减轻训练数据中的站点和类别不平衡问题。我们在来自多机构的脑肿瘤MRI上预训练了自监督的骨干模型（BYOL, DINO, MAE, MoCo），并在神经肿瘤学的下游任务中进行了分子分类和生存预测的改进，包括常见和少见的分子标记。", "result": "实验结果显示，在CUIMC站点，平均平衡准确率从0.744提高到0.785，AUC值从0.656提高到0.676。罕见结局的预测也获得较大改进，如CDKN2A/2B的准确率从0.86提升至0.92，AUC从0.73提高到0.92；ATRX的AUC从0.69提升到0.82；Ki-67准确率从0.60提升到0.69。生存预测的c指数也在所有站点得到提升。", "conclusion": "我们的方法提高了分子预测精度，并减少了站点特异性嵌入差异。总体而言，结合FMs与DRO提供了更稳健的站点不变表示，改善了常见和罕见标记的预测，增强了生存预判，这表明需要更多的前瞻性验证和纵向信号的整合，以促进精准的神经肿瘤学发展。"}}
