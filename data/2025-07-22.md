<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 40]
- [cs.CV](#cs.CV) [Total: 43]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [DeepWriter: A Fact-Grounded Multimodal Writing Assistant Based On Offline Knowledge Base](https://arxiv.org/abs/2507.14189)
*Song Mao,Lejun Cheng,Pinlong Cai,Guohang Yan,Ding Wang,Botian Shi*

Main category: cs.CL

> 本文介绍了DeepWriter，一种可以解决大型语言模型在特定领域如金融、医疗和法律作为写作助手时遇到的问题的定制化、多模态、长文档写作助手。通过结合文本和视觉元素生成连贯、事实依据充分的文档，并通过分层知识表示提高检索效率和准确性。实验表明，DeepWriter生成的金融报告优于现有的基线方法。

<details>
  <summary>Details</summary>

**Motivation:** 大型语言模型虽然在各种应用中表现卓越，但在特定领域如金融、医疗和法律等作为写作助手时往往会受到缺乏深厚专业知识和倾向于产生虚假信息的困扰。现有解决方案如检索增强生成（RAG）在多次检索过程中可能存在一致性问题，而基于在线搜索的方法往往因为网络内容的不可靠性而导致质量下降。

**Method:** DeepWriter采用了一个新颖的流水线，主要包括任务分解、大纲生成、多模态检索和分段写作反思。通过深入挖掘结构化语料库中的信息并结合文本和视觉元素，DeepWriter能够生成连贯、事实依据充分且专业性高的文档。同时，提出了一种分层知识表示方法来提高检索效率和准确性。

**Result:** 实验结果显示，DeepWriter生成的金融报告质量高且可以验证，其在事实准确性和生成内容质量方面超越了现有的基线方法。

**Conclusion:** 通过采用定制化、多模态、长文档写作助手DeepWriter，在线外知识库的操作方式，研究解决了现有大型语言模型在特定领域应用中的挑战，并实验证明了其生成的文档在质量上的优势。

**Abstract:** Large Language Models (LLMs) have demonstrated remarkable capabilities in
various applications. However, their use as writing assistants in specialized
domains like finance, medicine, and law is often hampered by a lack of deep
domain-specific knowledge and a tendency to hallucinate. Existing solutions,
such as Retrieval-Augmented Generation (RAG), can suffer from inconsistency
across multiple retrieval steps, while online search-based methods often
degrade quality due to unreliable web content. To address these challenges, we
introduce DeepWriter, a customizable, multimodal, long-form writing assistant
that operates on a curated, offline knowledge base. DeepWriter leverages a
novel pipeline that involves task decomposition, outline generation, multimodal
retrieval, and section-by-section composition with reflection. By deeply mining
information from a structured corpus and incorporating both textual and visual
elements, DeepWriter generates coherent, factually grounded, and
professional-grade documents. We also propose a hierarchical knowledge
representation to enhance retrieval efficiency and accuracy. Our experiments on
financial report generation demonstrate that DeepWriter produces high-quality,
verifiable articles that surpasses existing baselines in factual accuracy and
generated content quality.

</details>


### [2] [Retention analysis of edited knowledge after fine-tuning](https://arxiv.org/abs/2507.14198)
*Fufang Wen,Shichang Zhang*

Main category: cs.CL

> 研究探讨了微调对LLMs编辑知识的影响，发现编辑知识容易在微调中被遗忘，而冻结特定层能提高知识保留。这个发现对未来的编辑方法稳健性提升具有重要的指导意义。

<details>
  <summary>Details</summary>

**Motivation:** 目前对微调对先前编辑知识的影响了解不足。研究这个领域可以揭示当前编辑方法的关键局限性，并指出评估编辑的稳健性在下游任务微调下的重要性。

**Method:** 此研究系统性地探讨了不同的微调目标与各种模型编辑技术之间的交互作用。

**Result:** 研究发现，编辑后的知识在微调过程中比模型预训练获得的内在知识更容易遗忘。冻结与编辑内容相关联的层可以显著提高知识保留。

**Conclusion:** 编辑的知识在微调过程中比模型预训练获得的内在知识更容易遗忘。冻结与编辑内容相关联的层可以显著提高知识保留，这为未来编辑方法的稳健性提供了见解。

**Abstract:** Large language models (LLMs) store vast amounts of knowledge, which often
requires updates to correct factual errors, incorporate newly acquired
information, or adapt model behavior. Model editing methods have emerged as
efficient solutions for such updates, offering localized and precise knowledge
modification at significantly lower computational cost than continual training.
In parallel, LLMs are frequently fine-tuned for a wide range of downstream
tasks. However, the effect of fine-tuning on previously edited knowledge
remains poorly understood. In this work, we systematically investigate how
different fine-tuning objectives interact with various model editing
techniques. Our findings show that edited knowledge is substantially more
susceptible to forgetting during fine-tuning than intrinsic knowledge acquired
through pre-training. This analysis highlights a key limitation of current
editing approaches and suggests that evaluating edit robustness under
downstream fine-tuning is critical for their practical deployment. We further
find that freezing layers associated with edited content can significantly
improve knowledge retention, offering insight into how future editing methods
might be made more robust.

</details>


### [3] [Open-Source LLMs Collaboration Beats Closed-Source LLMs: A Scalable Multi-Agent System](https://arxiv.org/abs/2507.14200)
*Shengji Tang,Jianjian Cao,Weihao Lin,Jiale Hong,Bo Zhang,Shuyue Hu,Lei Bai,Tao Chen,Wanli Ouyang,Peng Ye*

Main category: cs.CL

> 本论文提出了SMACS框架，通过整合多个开源LLM和使用新的选择和后处理机制来超越闭源LLM的性能。实验结果表明，SMACS在多个基准测试中超越了领先的闭源LLM。

<details>
  <summary>Details</summary>

**Motivation:** 探索是否能利用多个开源LLM来匹配或超越闭源LLM的性能。

**Method:** 提出SMACS框架，包括基于检索的前向选择（RPS）和探索与利用驱动的后向增强（EPE）。

**Result:** 在八个主流基准测试中，SMACS整合了15个开源LLM，超越了2025年的领先闭源LLM，如Claude-3.7-Sonnet、GPT-4.1和GPT-o3-mini，表现出色。

**Conclusion:** SMACS框架不仅在与闭源LLM的对比中表现出优越性，还在开源和闭源LLM的最佳结果平均值上有所提升。代码将在GitHub上发布。

**Abstract:** This paper aims to demonstrate the potential and strengths of open-source
collectives. It leads to a promising question: Can we harness multiple
open-source LLMs to match or even beat the closed-source LLMs? To answer this,
we propose SMACS, a scalable multi-agent collaboration system (MACS) framework
with high performance. Specifically, for continuous integration of new LLMs and
generalization to diverse questions, we first propose a Retrieval-based Prior
Selection (RPS), which assigns a proxy performance score to each LLM to select
the Top-k LLMs at the instance level for any given question. Then, we propose
an Exploration-Exploitation-Driven Posterior Enhancement (EPE), encouraging the
generation of diverse responses through prior dropping and selecting the
high-quality response via a hybrid posterior score. Experiments on eight
mainstream benchmarks validate the effectiveness of our SMACS: by integrating
fifteen open-source LLMs, SMACS outperforms leading closed-source LLMs in 2025,
e.g., Claude-3.7-Sonnet (+12.73%), GPT-4.1(+5.36%) and GPT-o3-mini(+5.28%)
across multiple tasks. Remarkably, it even exceeds the average of best results
of different datasets from both open-source LLMs (+2.86%) and closed-source
LLMs (+2.04%), pushing the upper bound of intelligence. Code will be released
at https://github.com/magent4aci/SMACS.

</details>


### [4] [Let's Measure the Elephant in the Room: Facilitating Personalized Automated Analysis of Privacy Policies at Scale](https://arxiv.org/abs/2507.14214)
*Rui Zhao,Vladyslav Melnychuk,Jun Zhao,Jesse Wright,Nigel Shadbolt*

Main category: cs.CL

> PoliAnalyzer是一种神经符号系统，利用NLP技术和逻辑推理来分析个性化的隐私政策并生成合规报告。实验表明，该系统在多项任务中具有90-100%的F1得分，平均减少了用户在理解隐私政策时95.2%的认知负担，并揭示了隐私政策中存在的用户难以接受的数据使用实践。

<details>
  <summary>Details</summary>

**Motivation:** 虽然现代人拥有多个在线账户，但他们很少阅读这些网站的条款和条件或隐私政策，尽管他们声称会这样做。作者旨在研发一个工具帮助用户分析这些复杂的隐私政策文档，以便更好地保护他们的数据隐私。

**Method:** PoliAnalyzer系统使用自然语言处理技术从文本中提取数据使用实践的形式化表示，通过逻辑推理对比用户偏好与形式化的隐私政策表示，从而生成合规报告。系统扩展了现有的形式化数据使用政策语言，用于建模隐私政策和用户偏好。

**Result:** 通过评估，PoliAnalyzer在识别相关的数据使用实践上显示了90-100%准确的F1得分，成功地将用户需要理解的冲突部分减少到4.8%。此外，它还揭示了一些隐私政策中常见的、有悖于用户期望的数据使用实践。

**Conclusion:** 这项研究展示了PoliAnalyzer可用于大规模的个性化隐私政策分析，有望帮助用户更好地控制自己的数据并促使社会对平台的数据使用行为进行讨论。

**Abstract:** In modern times, people have numerous online accounts, but they rarely read
the Terms of Service or Privacy Policy of those sites despite claiming
otherwise. This paper introduces PoliAnalyzer, a neuro-symbolic system that
assists users with personalized privacy policy analysis. PoliAnalyzer uses
Natural Language Processing (NLP) to extract formal representations of data
usage practices from policy texts. In favor of deterministic, logical inference
is applied to compare user preferences with the formal privacy policy
representation and produce a compliance report. To achieve this, we extend an
existing formal Data Terms of Use policy language to model privacy policies as
app policies and user preferences as data policies. In our evaluation using our
enriched PolicyIE dataset curated by legal experts, PoliAnalyzer demonstrated
high accuracy in identifying relevant data usage practices, achieving F1-score
of 90-100% across most tasks. Additionally, we demonstrate how PoliAnalyzer can
model diverse user data-sharing preferences, derived from prior research as 23
user profiles, and perform compliance analysis against the top 100 most-visited
websites. This analysis revealed that, on average, 95.2% of a privacy policy's
segments do not conflict with the analyzed user preferences, enabling users to
concentrate on understanding the 4.8% (636 / 13205) that violates preferences,
significantly reducing cognitive burden. Further, we identified common
practices in privacy policies that violate user expectations - such as the
sharing of location data with 3rd parties. This paper demonstrates that
PoliAnalyzer can support automated personalized privacy policy analysis at
scale using off-the-shelf NLP tools. This sheds light on a pathway to help
individuals regain control over their data and encourage societal discussions
on platform data practices to promote a fairer power dynamic.

</details>


### [5] [Beyond Architectures: Evaluating the Role of Contextual Embeddings in Detecting Bipolar Disorder on Social Media](https://arxiv.org/abs/2507.14231)
*Khalid Hasan,Jamil Saquer*

Main category: cs.CL

> 本研究通过高级NLP模型在识别社交网络文本中的双相情感障碍迹象上获得了高F1得分，表明上下文语言模型在精神健康应用中有很高的潜力。

<details>
  <summary>Details</summary>

**Motivation:** 双相情感障碍是一种常被误诊的精神疾病，由于早期症状微妙且社会上存在污名化现象。本研究旨在探索高级自然语言处理（NLP）模型用于识别双相情感障碍的用户生成社交媒体文本中的病症迹象。

**Method:** 本研究使用了基于变压器的模型（如BERT、RoBERTa、ALBERT、ELECTRA、DistilBERT）和基于长短时记忆（LSTM）的模型，分别采用上下文化词嵌入（BERT）和静态词嵌入（GloVe、Word2Vec），应用于一个大型且已标注的Reddit帖子数据集上。

**Result:** 实验结果显示，RoBERTa在变压器模型中表现最佳，F1得分为约98%；而使用BERT嵌入的LSTM模型获得几乎相同的结果。相比之下，使用静态嵌入的LSTMs在捕捉有意义的模式方面失败，得分为接近零的F1。DistilBERT在效率和准确性之间提供了最佳平衡。

**Conclusion:** 本研究为精神健康NLP应用中的模型选择提供了可行的见解，并验证了上下文化语言模型在早期双相情感障碍筛查中的潜力。

**Abstract:** Bipolar disorder is a chronic mental illness frequently underdiagnosed due to
subtle early symptoms and social stigma. This paper explores the advanced
natural language processing (NLP) models for recognizing signs of bipolar
disorder based on user-generated social media text. We conduct a comprehensive
evaluation of transformer-based models (BERT, RoBERTa, ALBERT, ELECTRA,
DistilBERT) and Long Short Term Memory (LSTM) models based on contextualized
(BERT) and static (GloVe, Word2Vec) word embeddings. Experiments were performed
on a large, annotated dataset of Reddit posts after confirming their validity
through sentiment variance and judgmental analysis. Our results demonstrate
that RoBERTa achieves the highest performance among transformer models with an
F1 score of ~98% while LSTM models using BERT embeddings yield nearly identical
results. In contrast, LSTMs trained on static embeddings fail to capture
meaningful patterns, scoring near-zero F1. These findings underscore the
critical role of contextual language modeling in detecting bipolar disorder. In
addition, we report model training times and highlight that DistilBERT offers
an optimal balance between efficiency and accuracy. In general, our study
offers actionable insights for model selection in mental health NLP
applications and validates the potential of contextualized language models to
support early bipolar disorder screening.

</details>


### [6] [Language Models Change Facts Based on the Way You Talk](https://arxiv.org/abs/2507.14238)
*Matthew Kearney,Reuben Binns,Yarin Gal*

Main category: cs.CL

> 本研究发现，大型语言模型在不同应用场景中对用户身份信息的偏见表现显著，可能引发严重的社会问题。

<details>
  <summary>Details</summary>

**Motivation:** 尽管近期研究表明LLMs能够通过语言模式推断出文本作者的身份信息，但关于它们如何在实际应用中利用这些信息知之甚少。

**Method:** 该研究首次对五个高风险的应用场景（医学、法律、政治、政府福利、薪资）中的大型语言模型（LLMs）进行了全面分析，探讨了用户文本中的身份标记如何影响LLMs的决策。

**Result:** 研究发现，LLMs对用户查询中的身份标记极其敏感，种族、性别和年龄一致地影响了LLMs在这些应用中的响应。例如，提供医疗建议时，对相同症状的不同族裔个体应用不同的护理标准；回答事实性问题时，对老年（青年）个体的倾向性更接近保守（自由）政治立场；建议为非白人求职者提供较低的薪资，为女性提供比男性更高的薪资。

**Conclusion:** 这些偏见可能导致医疗保健的不利差异，加剧工资差距，并为不同身份的人创建不同的政治现实。建议在未来的部署前，应对LLMs的用户应用进行类似的彻底评估。

**Abstract:** Large language models (LLMs) are increasingly being used in user-facing
applications, from providing medical consultations to job interview advice.
Recent research suggests that these models are becoming increasingly proficient
at inferring identity information about the author of a piece of text from
linguistic patterns as subtle as the choice of a few words. However, little is
known about how LLMs use this information in their decision-making in
real-world applications. We perform the first comprehensive analysis of how
identity markers present in a user's writing bias LLM responses across five
different high-stakes LLM applications in the domains of medicine, law,
politics, government benefits, and job salaries. We find that LLMs are
extremely sensitive to markers of identity in user queries and that race,
gender, and age consistently influence LLM responses in these applications. For
instance, when providing medical advice, we find that models apply different
standards of care to individuals of different ethnicities for the same
symptoms; we find that LLMs are more likely to alter answers to align with a
conservative (liberal) political worldview when asked factual questions by
older (younger) individuals; and that LLMs recommend lower salaries for
non-White job applicants and higher salaries for women compared to men. Taken
together, these biases mean that the use of off-the-shelf LLMs for these
applications may cause harmful differences in medical care, foster wage gaps,
and create different political factual realities for people of different
identities. Beyond providing an analysis, we also provide new tools for
evaluating how subtle encoding of identity in users' language choices impacts
model decisions. Given the serious implications of these findings, we recommend
that similar thorough assessments of LLM use in user-facing applications are
conducted before future deployment.

</details>


### [7] [CCL-XCoT: An Efficient Cross-Lingual Knowledge Transfer Method for Mitigating Hallucination Generation](https://arxiv.org/abs/2507.14239)
*Weihua Zheng,Roy Ka-Wei Lee,Zhengyuan Liu,Kui Wu,AiTi Aw,Bowei Zou*

Main category: cs.CL

> 提出CCL-XCoT，一个两阶段微调框架，减少多语言大语言模型中的幻觉现象，通过课程对比学习和跨语言思维链提示策略提高跨语言事实知识的迁移。

<details>
  <summary>Details</summary>

**Motivation:** 多语言大语言模型容易产生幻觉，特别是在低资源语言中，这主要归因于训练数据不平衡。这些幻觉现象在特定领域的生成任务中尤为重要。

**Method:** 我们的方法CCL-XCoT是一个两阶段的微调框架，旨在减少多语言大语言模型中的幻觉现象。第一阶段通过基于课程的对比学习和下一步预测增强跨语言语义对齐。第二阶段引入跨语言思维链提示策略，指导模型先在高资源语言中进行推理，然后再在目标的低资源语言中生成答案。

**Result:** 实验结果显示，CCL-XCoT将幻觉率降低了高达62%，并且显著提高了跨语言配对的事实知识转移，无需依赖外部检索或多模型集成。

**Conclusion:** CCL-XCoT通过两阶段微调框架减少了多语言大语言模型中的幻觉现象，并提高了事实知识的跨语言迁移效率。

**Abstract:** Multilingual Large Language Models(MLLMs) demonstrate strong generalization
across languages, yet they remain prone to hallucinations, especially in
low-resource languages, due to training data imbalances. These hallucinations,
which include inaccurate or fabricated outputs, are particularly problematic in
domain-specific generation tasks (Chataigner et al., 2024). To address this
challenge, we propose CCL-XCoT(Curriculum-based Contrastive Learning-based
Cross-lingual Chain-of-Thought), a two-stage fine-tuning framework for
mitigating hallucination in MLLMs. Our approach first enhances cross-lingual
semantic alignment through curriculum-based contrastive learning combined with
next-token prediction during continued pre-training. Building on this
foundation, we then introduce a cross-lingual Chain-of-Thought (XCoT) prompting
strategy during instruction fine-tuning, which guides the model to reason in a
high-resource language before generating answers in the target low-resource
language. Experimental results show that CCL-XCoT reduces hallucination rates
by up to 62% and substantially improves factual knowledge transfer across
language pairs, without relying on external retrieval or multi-model ensembles.

</details>


### [8] [HuggingGraph: Understanding the Supply Chain of LLM Ecosystem](https://arxiv.org/abs/2507.14240)
*Mohammad Shahedur Rahman,Peng Gao,Yuede Ji*

Main category: cs.CL

> 研究LLM供应链中模型和数据集之间的关系，构建并分析了包含大量节点和边的有向异构图，揭示了LLM供应链的重要特性和发展动态。

<details>
  <summary>Details</summary>

**Motivation:** 由于许多大型语言模型（LLMs）是从基础模型、预训练模型和外部数据集中构建的，它们可能继承了以前模型或数据集中的漏洞、偏见或恶意组件。因此，了解这些组件的来源和发展以更好地检测潜在风险、提高模型公平性并确保合规性的需求变得至关重要。

**Method:** 设计了一种系统收集LLM供应链数据的方法，并使用这些数据构建了一个有向异构图来建模模型与数据集之间的关系，生成了一个包含397,376个节点和453,469条边的结构。

**Result:** 分析中发现了一些关键点，例如: (i) LLM供应链图很大，稀疏，并遵循幂律度分布；(ii) 它有一个紧密相连的核心部分和一个碎片化的外围部分；(iii) 数据集在训练中扮演了关键角色；(iv) 模型与数据集之间存在着强烈的相互依赖性；以及(v) 该图是动态的，每天都会更新，反映出生态系统的持续演变。

**Conclusion:** 本项目研究了模型与数据集之间的关系，作为LLM供应链的核心组成部分，通过构建和分析有向异构图，揭示了LLM供应链的动态性和复杂性。

**Abstract:** Large language models (LLMs) leverage deep learning to process and predict
sequences of words from context, enabling them to perform various NLP tasks,
such as translation, summarization, question answering, and content generation.
However, the growing size and complexity of developing, training, and deploying
advanced LLMs require extensive computational resources and large datasets.
This creates a barrier for users. As a result, platforms that host models and
datasets are widely used. For example, Hugging Face, one of the most popular
platforms, hosted 1.8 million models and 450K datasets by June 2025, with no
sign of slowing down. Since many LLMs are built from base models, pre-trained
models, and external datasets, they can inherit vulnerabilities, biases, or
malicious components from earlier models or datasets. Therefore, it is critical
to understand the origin and development of these components to better detect
potential risks, improve model fairness, and ensure compliance. Motivated by
this, our project aims to study the relationships between models and datasets,
which are core components of the LLM supply chain. First, we design a method to
systematically collect LLM supply chain data. Using this data, we build a
directed heterogeneous graph to model the relationships between models and
datasets, resulting in a structure with 397,376 nodes and 453,469 edges. We
then perform various analyses and uncover several findings, such as: (i) the
LLM supply chain graph is large, sparse, and follows a power-law degree
distribution; (ii) it features a densely connected core and a fragmented
periphery; (iii) datasets play pivotal roles in training; (iv) strong
interdependence exists between models and datasets; and (v) the graph is
dynamic, with daily updates reflecting the ecosystem's ongoing evolution.

</details>


### [9] [Promptomatix: An Automatic Prompt Optimization Framework for Large Language Models](https://arxiv.org/abs/2507.14241)
*Rithesh Murthy,Ming Zhu,Liangwei Yang,Jielin Qiu,Juntao Tan,Shelby Heinecke,Huan Wang,Caiming Xiong,Silvio Savarese*

Main category: cs.CL

> Error

<details>
  <summary>Details</summary>

**Motivation:** Error

**Method:** Error

**Result:** Error

**Conclusion:** Error

**Abstract:** Large Language Models (LLMs) perform best with well-crafted prompts, yet
prompt engineering remains manual, inconsistent, and inaccessible to
non-experts. We introduce Promptomatix, an automatic prompt optimization
framework that transforms natural language task descriptions into high-quality
prompts without requiring manual tuning or domain expertise. Promptomatix
supports both a lightweight meta-prompt-based optimizer and a DSPy-powered
compiler, with modular design enabling future extension to more advanced
frameworks. The system analyzes user intent, generates synthetic training data,
selects prompting strategies, and refines prompts using cost-aware objectives.
Evaluated across 5 task categories, Promptomatix achieves competitive or
superior performance compared to existing libraries, while reducing prompt
length and computational overhead making prompt optimization scalable and
efficient.

</details>


### [10] [In-Depth and In-Breadth: Pre-training Multimodal Language Models Customized for Comprehensive Chart Understanding](https://arxiv.org/abs/2507.14298)
*Wan-Cyuan Fan,Yen-Chun Chen,Mengchen Liu,Alexander Jacobson,Lu Yuan,Leonid Sigal*

Main category: cs.CL

> 本文提出了ChartScope，一种优化的LVLM，用于跨多种图表类型的深入图表理解，通过新的数据生成管道和双路径训练策略，显著提高了广泛的图表理解能力，并建立了一个新的评估基准ChartDQA。

<details>
  <summary>Details</summary>

**Motivation:** 现有方法在定制LVLM进行特定领域的任务（如科学图表理解）方面取得了有前景的结果，但存在依赖少数图表类型配对数据和缺少针对图表数据对齐的预训练等主要限制，这限制了其泛化能力和对底层数据的理解。本文提出的方法旨在克服这些问题。

**Method:** 本文介绍了ChartScope，一种专门为跨多种图表类型的深入图表理解优化的大型视觉语言模型（LVLM）。提出了一个高效的数据生成管道，能够合成多种类型图表的配对数据，并提出了一种新颖的双路径训练策略，该策略使模型能够简洁地捕捉到重要数据细节，同时通过考虑底层数据，保持强大的推理能力。

**Result:** 实验结果表明，ChartScope在多种类型的图表理解上取得了显著的性能提升。

**Conclusion:** 实验表明，提出的方法在广泛的图表类型上显著增强了理解能力。

**Abstract:** Recent methods for customizing Large Vision Language Models (LVLMs) for
domain-specific tasks have shown promising results in scientific chart
comprehension. However, existing approaches face two major limitations: First,
they rely on paired data from only a few chart types, limiting generalization
to wide range of chart types. Secondly, they lack targeted pre-training for
chart-data alignment, which hampers the model's understanding of underlying
data. In this paper, we introduce ChartScope, an LVLM optimized for in-depth
chart comprehension across diverse chart types. We propose an efficient data
generation pipeline that synthesizes paired data for a wide range of chart
types, along with a novel Dual-Path training strategy that enabling the model
to succinctly capture essential data details while preserving robust reasoning
capabilities by incorporating reasoning over the underlying data. Lastly, we
establish ChartDQA, a new benchmark for evaluating not only question-answering
at different levels but also underlying data understanding. Experimental
results demonstrate that ChartScope significantly enhances comprehension on a
wide range of chart types. The code and data are available at
https://davidhalladay.github.io/chartscope_demo.

</details>


### [11] [Aligning Large Language Models to Low-Resource Languages through LLM-Based Selective Translation: A Systematic Study](https://arxiv.org/abs/2507.14304)
*Rakesh Paul,Anusha Kamath,Kanishk Singla,Raviraj Joshi,Utkarsh Vaidya,Sanjay Singh Chauhan,Niranjan Wartikar*

Main category: cs.CL

> The paper investigates LLM-based selective translation for aligning multilingual large language models, focusing on preserving non-translatable content and improving performance in low-resource languages like Hindi.

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to address the performance gap between English and non-English languages in multilingual LLMs, especially in low-resource settings, by effectively aligning these models.

**Method:** The method involves using selective translation techniques to translate only translatable parts of alignment datasets, combining this approach with the original English data for alignment, and conducting experiments with Google Cloud Translation and Llama-3.1-405B.

**Result:** The results indicate that selective translation can effectively preserve important non-translatable content and enhance the alignment of multilingual LLMs, particularly in low-resource languages such as Hindi.

**Conclusion:** The conclusion is that LLM-based selective translation holds potential as a practical method for enhancing multilingual alignment in large language models, effectively bridging the performance gap in low-resource languages.

**Abstract:** Multilingual large language models (LLMs) often demonstrate a performance gap
between English and non-English languages, particularly in low-resource
settings. Aligning these models to low-resource languages is essential yet
challenging due to limited high-quality data. While English alignment datasets
are readily available, curating equivalent data in other languages is expensive
and time-consuming. A common workaround is to translate existing English
alignment data; however, standard translation techniques often fail to preserve
critical elements such as code, mathematical expressions, and structured
formats like JSON. In this work, we investigate LLM-based selective
translation, a technique that selectively translates only the translatable
parts of a text while preserving non-translatable content and sentence
structure. We conduct a systematic study to explore key questions around this
approach, including its effectiveness compared to vanilla translation, the
importance of filtering noisy outputs, and the benefits of mixing translated
samples with original English data during alignment. Our experiments focus on
the low-resource Indic language Hindi and compare translations generated by
Google Cloud Translation (GCP) and Llama-3.1-405B. The results highlight the
promise of selective translation as a practical and effective method for
improving multilingual alignment in LLMs.

</details>


### [12] [How LLMs Comprehend Temporal Meaning in Narratives: A Case Study in Cognitive Evaluation of LLMs](https://arxiv.org/abs/2507.14307)
*Karin de Langis,Jong Inn Park,Andreas Schramm,Bin Hu,Khanh Chi Le,Michael Mensink,Ahn Thu Tong,Dongyeop Kang*

Main category: cs.CL

> 研究发现LLMs在处理叙事方面的历时意义时，与人类有根本的不同，且缺乏强大的叙事理解能力，开发了评估LLMs认知和语言能力的标准实验框架。

<details>
  <summary>Details</summary>

**Motivation:** 研究LLM处理叙事中语言方面的历时意义的方式，以确定其行为与人类认知的相似度以及高级模式识别的差异。

**Method:** 使用专家循环探测管道进行了一系列针对性实验，以评估LLMs是否以与人类相似的方式构建语义表示和语用推理。

**Result:** 发现LLMs过度依赖原型性，产生不一致的时间态判断，并且在基于方面的因果推理方面存在困难，这些结果引起对其全面理解叙事能力的担忧。

**Conclusion:** LLMs在处理叙事方面的历时意义方面与人类存在根本差异，并且在全面理解叙事方面的能力有限。

**Abstract:** Large language models (LLMs) exhibit increasingly sophisticated linguistic
capabilities, yet the extent to which these behaviors reflect human-like
cognition versus advanced pattern recognition remains an open question. In this
study, we investigate how LLMs process the temporal meaning of linguistic
aspect in narratives that were previously used in human studies. Using an
Expert-in-the-Loop probing pipeline, we conduct a series of targeted
experiments to assess whether LLMs construct semantic representations and
pragmatic inferences in a human-like manner. Our findings show that LLMs
over-rely on prototypicality, produce inconsistent aspectual judgments, and
struggle with causal reasoning derived from aspect, raising concerns about
their ability to fully comprehend narratives. These results suggest that LLMs
process aspect fundamentally differently from humans and lack robust narrative
understanding. Beyond these empirical findings, we develop a standardized
experimental framework for the reliable assessment of LLMs' cognitive and
linguistic capabilities.

</details>


### [13] [What Makes You CLIC: Detection of Croatian Clickbait Headlines](https://arxiv.org/abs/2507.14314)
*Marija Anđedelić,Dominik Šipek,Laura Majer,Jan Šnajder*

Main category: cs.CL

> 本文构建了CLIC数据集，用于检测克罗地亚语的点击诱饵，并比较了微调BERTi'c模型与上下文学习方法的表现，发现微调模型效果更优。

<details>
  <summary>Details</summary>

**Motivation:** 在线新闻网站主要依赖广告收入，新闻标题过多使用吸引眼球的方式（点击诱饵），这对维持数字媒体的信息质量和读者信任构成挑战。本文旨在通过自动检测来解决这一问题。

**Method:** 本文构建了一个名为CLIC的新数据集，用于检测克罗地亚新闻标题中的点击诱饵，时间跨度为20年，并涉及主流和边缘新闻来源。研究比较了微调BERTi'c模型与基于LLM的上下文学习方法的表现，并分析了点击诱饵的语义特性。

**Result:** 研究发现，近一半的分析标题包含点击诱饵，并且微调模型的性能优于一般的LLM。

**Conclusion:** 本文证实了微调模型在点击诱饵检测任务上的表现优于通用的大语言模型，特别是在资源较少的语种中。此外，对点击诱饵的语义特性进行了深入分析。

**Abstract:** Online news outlets operate predominantly on an advertising-based revenue
model, compelling journalists to create headlines that are often scandalous,
intriguing, and provocative -- commonly referred to as clickbait. Automatic
detection of clickbait headlines is essential for preserving information
quality and reader trust in digital media and requires both contextual
understanding and world knowledge. For this task, particularly in
less-resourced languages, it remains unclear whether fine-tuned methods or
in-context learning (ICL) yield better results. In this paper, we compile CLIC,
a novel dataset for clickbait detection of Croatian news headlines spanning a
20-year period and encompassing mainstream and fringe outlets. We fine-tune the
BERTi\'c model on this task and compare its performance to LLM-based ICL
methods with prompts both in Croatian and English. Finally, we analyze the
linguistic properties of clickbait. We find that nearly half of the analyzed
headlines contain clickbait, and that finetuned models deliver better results
than general LLMs.

</details>


### [14] [Can LLMs Infer Personality from Real World Conversations?](https://arxiv.org/abs/2507.14355)
*Jianfeng Zhu,Ruoming Jin,Karin G. Coifman*

Main category: cs.CL

> 本研究展示了虽然大语言模型在零样本提示下对于性格特征预测时能够展示一定的效果，但与实测得分的对应性依旧有限，尤其是在个性特征水平准确性方面的表现不足。研究表明，需要进一步基于证据的方法来改进大语言模型在心理学应用中的表现。

<details>
  <summary>Details</summary>

**Motivation:** 本文提出了一种使用大语言模型进行个性特征评估的新基准，解决之前研究依靠合成数据或缺乏心理测量有效性的真实数据的问题。该研究聚焦于生成真人访谈数据集，用于评估大语言模型在个性评估中的有效性。

**Method:** 研究使用了三个最先进的大语言模型（GPT-4.1 Mini、Meta-LLaMA 和DeepSeek）来进行零样本提示和思考链提示下的大五特质推理评估。测试集包含了555个半结构化访谈，参与者自报了BFI-10项目性格量表得分。

**Result:** 研究表明，尽管大语言模型如GPT-4.1 Mini、Meta-LLaMA 和DeepSeek在个性评估方面显示出高复测可靠性，但在构建有效性方面表现有限。模型预测与真实分数的相关性较弱（最大皮尔森相关系数$r = 0.27$），且预测结果多偏向中等或较高的个性特征水平。通过思考链提示和延长输入上下文可以适度改善分布对齐，但不提高个性特征水平的准确性。研究强调了当前基于LLMs的个性推理的局限性，并指出需要基于证据的研发应用于心理评估。

**Conclusion:** 研究结论指出，当前的大语言模型在进行个性特征推理中存在显著局限，这一结果显示了在该领域内目前基于模型推理的挑战。未来的研究需聚焦于更为严谨的数据集设计以及更加精细化的技术改进以提高证明有效性，特别是在个性特征水平准确性上的提升。

**Abstract:** Large Language Models (LLMs) such as OpenAI's GPT-4 and Meta's LLaMA offer a
promising approach for scalable personality assessment from open-ended
language. However, inferring personality traits remains challenging, and
earlier work often relied on synthetic data or social media text lacking
psychometric validity. We introduce a real-world benchmark of 555
semi-structured interviews with BFI-10 self-report scores for evaluating
LLM-based personality inference. Three state-of-the-art LLMs (GPT-4.1 Mini,
Meta-LLaMA, and DeepSeek) were tested using zero-shot prompting for BFI-10 item
prediction and both zero-shot and chain-of-thought prompting for Big Five trait
inference. All models showed high test-retest reliability, but construct
validity was limited: correlations with ground-truth scores were weak (max
Pearson's $r = 0.27$), interrater agreement was low (Cohen's $\kappa < 0.10$),
and predictions were biased toward moderate or high trait levels.
Chain-of-thought prompting and longer input context modestly improved
distributional alignment, but not trait-level accuracy. These results
underscore limitations in current LLM-based personality inference and highlight
the need for evidence-based development for psychological applications.

</details>


### [15] [Text-to-SQL for Enterprise Data Analytics](https://arxiv.org/abs/2507.14372)
*Albert Chen,Manas Bundele,Gaurav Ahlawat,Patrick Stetz,Zhitao Wang,Qiang Fei,Donghoon Jung,Audrey Chu,Bharadwaj Jayaraman,Ayushi Panth,Yatin Arora,Sourav Jain,Renjith Varma,Alexey Ilin,Iuliia Melnychuk,Chelsea Chueh,Joyan Sil,Xiaofeng Wang*

Main category: cs.CL

> 

<details>
  <summary>Details</summary>

**Motivation:** 

**Method:** 

**Result:** {
  "tldr": "The paper introduces a knowledge graph-based chatbot for LinkedIn, which assists internal teams with self-serving data insights from a large data lake. The system includes a Text-to-SQL agent and an interactive chatbot interface. It has over 300 weekly users with 53% of its responses correct or close to correct on an internal benchmark.",
  "motivation": "The main motivation is to address the difficulty in building an effective enterprise solution using large language models for Text-to-SQL tasks. The authors aim to support internal teams at LinkedIn in querying a large, dynamic data lake efficiently.",
  "method": "The method involves three components: a knowledge graph for capturing database semantics through clustering, a Text-to-SQL agent for query generation and correction, and an interactive chatbot for various user intents from data discovery to debugging.",
  "result": "The chatbot has over 300 weekly users, and expert review indicates that 53% of its responses are correct or close to correct on an internal benchmark. The authors also identify key knowledge graph and model components through ablation studies.",
  "conclusion": "The paper concludes that the proposed approach offers a practical path for developing enterprise Text-to-SQL solutions by integrating a knowledge graph, Text-to-SQL agent, and interactive chatbot which demonstrates good performance and user engagement for internal use at LinkedIn.",
  "tool_used": "Structure function was used to break down the paper's content. Additional insights come from the original content. "
}

**Conclusion:** 

**Abstract:** The introduction of large language models has brought rapid progress on
Text-to-SQL benchmarks, but it is not yet easy to build a working enterprise
solution. In this paper, we present insights from building an internal chatbot
that enables LinkedIn's product managers, engineers, and operations teams to
self-serve data insights from a large, dynamic data lake. Our approach features
three components. First, we construct a knowledge graph that captures
up-to-date semantics by indexing database metadata, historical query logs,
wikis, and code. We apply clustering to identify relevant tables for each team
or product area. Second, we build a Text-to-SQL agent that retrieves and ranks
context from the knowledge graph, writes a query, and automatically corrects
hallucinations and syntax errors. Third, we build an interactive chatbot that
supports various user intents, from data discovery to query writing to
debugging, and displays responses in rich UI elements to encourage follow-up
chats. Our chatbot has over 300 weekly users. Expert review shows that 53% of
its responses are correct or close to correct on an internal benchmark set.
Through ablation studies, we identify the most important knowledge graph and
modeling components, offering a practical path for developing enterprise
Text-to-SQL solutions.

</details>


### [16] [Error-Aware Curriculum Learning for Biomedical Relation Classification](https://arxiv.org/abs/2507.14374)
*Sinchani Chakraborty,Sudeshna Sarkar,Pawan Goyal*

Main category: cs.CL

> 本研究提出了一种基于教师-学生框架的方法，通过大型语言模型提供结构化指导，以改善生物医学文本中的关系分类，并在多个数据集上实现先进性能。

<details>
  <summary>Details</summary>

**Motivation:** 本研究旨在改善生物医学文本中的关系分类，这对于构建知识图谱以及药物再利用和临床决策支持等应用至关重要。

**Method:** 本研究提出了一种基于教师-学生框架的错误感知关系分类（RC）方法，通过大型语言模型（GPT-4o）提供结构指导。教师模型分析学生模型的预测错误，分类错误类型，分配难度分数，并生成针对性的补救措施，如句子重写和知识图谱增强建议。这些增强的标注结果用于通过指令调优培训首个学生模型，之后该模型会为更大规模的数据集标注难度评分和增强输入。第二个学生模型通过基于难度顺序的课程学习进行培训，以促进稳健和逐步的学习进展。

**Result:** 该方法在4个PPI数据集中的5个以及DDI数据集上达到了新的最先进的性能，在ChemProt数据集上保持了竞争力。

**Conclusion:** 提出的错误感知教师-学生框架有助于改善生物医学文本中的关系分类性能，特别是在特定的数据集上取得了显著的性能提升。

**Abstract:** Relation Classification (RC) in biomedical texts is essential for
constructing knowledge graphs and enabling applications such as drug
repurposing and clinical decision-making. We propose an error-aware
teacher--student framework that improves RC through structured guidance from a
large language model (GPT-4o). Prediction failures from a baseline student
model are analyzed by the teacher to classify error types, assign difficulty
scores, and generate targeted remediations, including sentence rewrites and
suggestions for KG-based enrichment. These enriched annotations are used to
train a first student model via instruction tuning. This model then annotates a
broader dataset with difficulty scores and remediation-enhanced inputs. A
second student is subsequently trained via curriculum learning on this dataset,
ordered by difficulty, to promote robust and progressive learning. We also
construct a heterogeneous biomedical knowledge graph from PubMed abstracts to
support context-aware RC. Our approach achieves new state-of-the-art
performance on 4 of 5 PPI datasets and the DDI dataset, while remaining
competitive on ChemProt.

</details>


### [17] [X-Intelligence 3.0: Training and Evaluating Reasoning LLM for Semiconductor Display](https://arxiv.org/abs/2507.14430)
*Xiaolin Yan,Yangxing Liu,Jiazhang Zheng,Chi Liu,Mingyu Du,Caisheng Chen,Haoyang Liu,Ming Ding,Yuan Li,Qiuping Liao,Linfeng Li,Zhili Mei,Siyu Wan,Li Li,Ruyi Zhong,Jiangling Yu,Xule Liu,Huihui Hu,Jiameng Yue,Ruohui Cheng,Qi Yang,Liangqing Wu,Ke Zhu,Chi Zhang,Chufei Jing,Yifan Zhou,Yan Liang,Dongdong Li,Zhaohui Wang,Bin Zhao,Mingzhou Wu,Mingzhong Zhou,Peng Du,Zuomin Liao,Chao Dai,Pengfei Liang,Xiaoguang Zhu,Yu Zhang,Yu Gu,Kun Pan,Yuan Wu,Yanqing Guan,Shaojing Wu,Zikang Feng,Xianze Ma,Peishan Cheng,Wenjuan Jiang,Jing Ba,Huihao Yu,Zeping Hu,Yuan Xu,Zhiwei Liu,He Wang,Zhenguo Lin,Ming Liu,Yanhong Meng*

Main category: cs.CL

> 研究开发了专为半导体显示行业设计的X-Intelligence 3.0模型，该模型在多个评估中优于现有的大型模型，证明了其在该领域的卓越效率。

<details>
  <summary>Details</summary>

**Motivation:** 尽管大型语言模型在解决复杂问题方面表现出了显著的优势，但它们在半导体显示行业的有效性仍然有限，主要是由于缺乏特定领域的训练和专业知识。本研究旨在填补这一空白。

**Method:** 本研究开发了X-Intelligence 3.0，一种专为半导体显示行业设计的高性能推理模型。该模型通过监督微调和强化学习，利用精心策划的行业知识库来提升其推理和理解能力。同时，引入了自动评估框架和领域特定的检索增强生成机制来进一步优化性能。

**Result:** 尽管X-Intelligence 3.0的参数量相对较少，仅为32亿，但它在多种评估中优于现有的DeepSeek-R1-671B模型，显示了其出色的效率。

**Conclusion:** X-Intelligence 3.0代表了解决半导体显示行业中长期存在的推理挑战的强大解决方案，展示了其在领域特定任务中的高效性能。

**Abstract:** Large language models (LLMs) have recently achieved significant advances in
reasoning and demonstrated their advantages in solving challenging problems.
Yet, their effectiveness in the semiconductor display industry remains limited
due to a lack of domain-specific training and expertise. To bridge this gap, we
present X-Intelligence 3.0, the first high-performance reasoning model
specifically developed for the semiconductor display industry. This model is
designed to deliver expert-level understanding and reasoning for the industry's
complex challenges. Leveraging a carefully curated industry knowledge base, the
model undergoes supervised fine-tuning and reinforcement learning to enhance
its reasoning and comprehension capabilities. To further accelerate
development, we implemented an automated evaluation framework that simulates
expert-level assessments. We also integrated a domain-specific
retrieval-augmented generation (RAG) mechanism, resulting in notable
performance gains on benchmark datasets. Despite its relatively compact size of
32 billion parameters, X-Intelligence 3.0 outperforms SOTA DeepSeek-R1-671B
across multiple evaluations. This demonstrates its exceptional efficiency and
establishes it as a powerful solution to the longstanding reasoning challenges
faced by the semiconductor display industry.

</details>


### [18] [XL-DURel: Finetuning Sentence Transformers for Ordinal Word-in-Context Classification](https://arxiv.org/abs/2507.14578)
*Sachin Yadav,Dominik Schlechtweg*

Main category: cs.CL

> 提出XL-DURel模型，通过对有序词上下文分类的优化，超越了现有模型的表现，并实现了二元和有序任务的统一处理。

<details>
  <summary>Details</summary>

**Motivation:** 旨在通过优化有序单词上下文分类来改进现有模型，并探索二元和有序任务之间的关系。

**Method:** 提出了一种名为XL-DURel的微调多语言Sentence Transformer模型，专门用于优化有序词上下文分类。测试了几种不同的回归和排序任务的损失函数。

**Result:** 在基于复数空间角度距离的排序目标下，新的模型在有序和二元数据上超过了之前的模型。

**Conclusion:** 二元词上下文可以被视为有序词上下文的一个特例，对更通用的有序任务进行优化可以改进二元任务的表现，实现了不同任务形式下的统一词上下文建模方法。

**Abstract:** We propose XL-DURel, a finetuned, multilingual Sentence Transformer model
optimized for ordinal Word-in-Context classification. We test several loss
functions for regression and ranking tasks managing to outperform previous
models on ordinal and binary data with a ranking objective based on angular
distance in complex space. We further show that binary WiC can be treated as a
special case of ordinal WiC and that optimizing models for the general ordinal
task improves performance on the more specific binary task. This paves the way
for a unified treatment of WiC modeling across different task formulations.

</details>


### [19] [Exploring Human-AI Complementarity in CPS Diagnosis Using Unimodal and Multimodal BERT Models](https://arxiv.org/abs/2507.14579)
*Kester Wong,Sahan Bulathwela,Mutlu Cukurova*

Main category: cs.CL

> 该论文扩展了AudiBERT模型在检测协作问题解决(CPS)指标方面的研究，指出AudiBERT在社会认知维度上比BERT模型有显著提升，但在情感维度上没有显著改进。此外，大数据训练与模型召回率表现相关，而BERT模型精度与人类编码者的高度一致同意相关。文章最后提出了一种实现人工智能和人为补充的结构化方法，并强调了模型可解释性的重要性。

<details>
  <summary>Details</summary>

**Motivation:** 该研究旨在解决使用机器学习技术从对话中检测CPS指标的难题，进一步探讨在CPS诊断任务中如何充分利用人工智能和人为优势，并解决多重模态方法统计显著性的不明确问题。

**Method:** 该研究使用了AudiBERT模型，一种基于BERT的变体，能够集成语音和声学-音系音频特征，对CPS指标进行诊断。还进行了相关分析，探讨了训练数据规模与模型性能，以及BERT模型精度与人为编码者之间的一致性。

**Result:** 研究发现，AudiBERT在划分社会认知维度类别上比BERT有显著改进，但在情感维度上没有类似改进。更大的训练数据与模型的高召回率相关，且BERT模型的精度与人编码者的一致性高。但是使用BERT模型诊断在CPS诊断中在不同维度的效果不一致。

**Conclusion:** 文章提出了实现人工智能和人为补充以改进CPS诊断的结构化方法，强调了模型的可解释性对于支持人类在反思编码过程中的参与和能动性的重要性。

**Abstract:** Detecting collaborative problem solving (CPS) indicators from dialogue using
machine learning techniques is a significant challenge for the field of AI in
Education. Recent studies have explored the use of Bidirectional Encoder
Representations from Transformers (BERT) models on transcription data to
reliably detect meaningful CPS indicators. A notable advancement involved the
multimodal BERT variant, AudiBERT, which integrates speech and
acoustic-prosodic audio features to enhance CPS diagnosis. Although initial
results demonstrated multimodal improvements, the statistical significance of
these enhancements remained unclear, and there was insufficient guidance on
leveraging human-AI complementarity for CPS diagnosis tasks. This workshop
paper extends the previous research by highlighting that the AudiBERT model not
only improved the classification of classes that were sparse in the dataset,
but it also had statistically significant class-wise improvements over the BERT
model for classifications in the social-cognitive dimension. However, similar
significant class-wise improvements over the BERT model were not observed for
classifications in the affective dimension. A correlation analysis highlighted
that larger training data was significantly associated with higher recall
performance for both the AudiBERT and BERT models. Additionally, the precision
of the BERT model was significantly associated with high inter-rater agreement
among human coders. When employing the BERT model to diagnose indicators within
these subskills that were well-detected by the AudiBERT model, the performance
across all indicators was inconsistent. We conclude the paper by outlining a
structured approach towards achieving human-AI complementarity for CPS
diagnosis, highlighting the crucial inclusion of model explainability to
support human agency and engagement in the reflective coding process.

</details>


### [20] [Explainable Collaborative Problem Solving Diagnosis with BERT using SHAP and its Implications for Teacher Adoption](https://arxiv.org/abs/2507.14584)
*Kester Wong,Sahan Bulathwela,Mutlu Cukurova*

Main category: cs.CL

> 本研究通过SHAP分析了不同标记化单词对BERT模型在CPS分类中的影响，发现良好分类的表现并不等同于合理解释，存在一些无意义的单词影响分类结果。

<details>
  <summary>Details</summary>

**Motivation:** 提升基于BERT的CPS诊断的可解释性对于更好地向最终用户（如老师）提供信息至关重要，这可以增强信任并促进教育领域的更广泛应用。

**Method:** 本研究采用SHapley Additive exPlanations (SHAP)方法来分析在转录数据中不同标记化单词如何影响BERT模型对协作问题解决（CPS）过程的分类。

**Result:** 研究表明，良好的分类表现不一定与合理的分类解释相关，某些标记化单词被频繁用来影响分类决策。发现了一个虽然对分类有正面影响但实际上不具有语义意义的标记化单词。

**Conclusion:** 研究结果揭示，模型在分类中是否恰当地使用了单词与类别数量相关，这提示未来需要探讨集成模型架构并重视人机互补来提升CPS诊断，因为精细区分CPS子技能仍然需要人类的推理。

**Abstract:** The use of Bidirectional Encoder Representations from Transformers (BERT)
model and its variants for classifying collaborative problem solving (CPS) has
been extensively explored within the AI in Education community. However,
limited attention has been given to understanding how individual tokenised
words in the dataset contribute to the model's classification decisions.
Enhancing the explainability of BERT-based CPS diagnostics is essential to
better inform end users such as teachers, thereby fostering greater trust and
facilitating wider adoption in education. This study undertook a preliminary
step towards model transparency and explainability by using SHapley Additive
exPlanations (SHAP) to examine how different tokenised words in transcription
data contributed to a BERT model's classification of CPS processes. The
findings suggested that well-performing classifications did not necessarily
equate to a reasonable explanation for the classification decisions. Particular
tokenised words were used frequently to affect classifications. The analysis
also identified a spurious word, which contributed positively to the
classification but was not semantically meaningful to the class. While such
model transparency is unlikely to be useful to an end user to improve their
practice, it can help them not to overrely on LLM diagnostics and ignore their
human expertise. We conclude the workshop paper by noting that the extent to
which the model appropriately uses the tokens for its classification is
associated with the number of classes involved. It calls for an investigation
into the exploration of ensemble model architectures and the involvement of
human-AI complementarity for CPS diagnosis, since considerable human reasoning
is still required for fine-grained discrimination of CPS subskills.

</details>


### [21] [Backtranslation and paraphrasing in the LLM era? Comparing data augmentation methods for emotion classification](https://arxiv.org/abs/2507.14590)
*Łukasz Radliński,Mateusz Guściora,Jan Kocoń*

Main category: cs.CL

> 此论文探讨了NLP领域中通过数据增强方法解决数据稀缺和类别不平衡问题，特别是使用了大型语言模型如GPT。研究表明，传统的改写和回译方法可以与纯生成方法媲美，甚至在某些情况下表现更好。

<details>
  <summary>Details</summary>

**Motivation:** 解决NLP领域的数据稀缺和类别不平衡问题，评估通过大型语言模型实现的传统数据增强方法的有效性。

**Method:** 选取了解决数据稀缺并利用ChatGPT的方法，并选取示例数据集，进行了一系列对比四种不同数据增强方法的实验。

**Result:** 实验结果表明，回译和改写方法可以与从零或少量示例生成的方法相媲美，甚至在某些情况下表现更好。

**Conclusion:** 传统数据增强方法（如改写和回译）即使在最新的语言模型背景下，依旧能产生高质量的数据，并可能更有效地提高分类性能。

**Abstract:** Numerous domain-specific machine learning tasks struggle with data scarcity
and class imbalance. This paper systematically explores data augmentation
methods for NLP, particularly through large language models like GPT. The
purpose of this paper is to examine and evaluate whether traditional methods
such as paraphrasing and backtranslation can leverage a new generation of
models to achieve comparable performance to purely generative methods. Methods
aimed at solving the problem of data scarcity and utilizing ChatGPT were
chosen, as well as an exemplary dataset. We conducted a series of experiments
comparing four different approaches to data augmentation in multiple
experimental setups. We then evaluated the results both in terms of the quality
of generated data and its impact on classification performance. The key
findings indicate that backtranslation and paraphrasing can yield comparable or
even better results than zero and a few-shot generation of examples.

</details>


### [22] [Retrieval-Augmented Clinical Benchmarking for Contextual Model Testing in Kenyan Primary Care: A Methodology Paper](https://arxiv.org/abs/2507.14615)
*Fred Mutisya,Shikoh Gitau,Christine Syovata,Diana Oigara,Ibrahim Matende,Muna Aden,Munira Ali,Ryan Nyotu,Diana Marion,Job Nyangena,Nasubo Ongoma,Keith Mbae,Elizabeth Wamicha,Eric Mibuari,Jean Philbert Nsengemana,Talkmore Chidede*

Main category: cs.CL

> 该研究提出了一种方法，用于创建基于肯尼亚国家指南的医疗场景和评估框架，旨在评估LLMs在非洲医疗场景中的表现，发现存在显著表现差距。

<details>
  <summary>Details</summary>

**Motivation:** 大规模语言模型（LLMs）对于改善低资源环境下的医疗服务质量显示出潜力，但其在非洲初级护理中的效果尚未得到充分研究。

**Method:** 该论文提出了一种用于创建基准数据集和评估框架的方法，专注于肯尼亚二级和三级临床护理。该方法使用检索增强生成（RAG）将临床问题与肯尼亚的国家指南相联系，以确保符合当地标准。这些指南被数字化、分块并进行语义检索。Gemini Flash 2.0 Lite 被引导使用指南摘录生成现实的临床场景、多项选择题及基于理由的答案，语言为英语和斯瓦希里语。该数据集由肯尼亚医生合作创建和细化，并经盲审专家评审，确保临床准确性、清晰度和文化适当性。

**Result:** 研究结果揭示了将LLMs应用于本地化场景时存在显著的表现差距，这与LLMs在非洲医学内容上的准确性低于美国基准的发现相一致。

**Conclusion:** 该工作提供了一个可复制的指南驱动型动态基准测试模型，以支持在非洲健康系统中安全地部署AI。

**Abstract:** Large Language Models(LLMs) hold promise for improving healthcare access in
low-resource settings, but their effectiveness in African primary care remains
underexplored. We present a methodology for creating a benchmark dataset and
evaluation framework focused on Kenyan Level 2 and 3 clinical care. Our
approach uses retrieval augmented generation (RAG) to ground clinical questions
in Kenya's national guidelines, ensuring alignment with local standards. These
guidelines were digitized, chunked, and indexed for semantic retrieval. Gemini
Flash 2.0 Lite was then prompted with guideline excerpts to generate realistic
clinical scenarios, multiple-choice questions, and rationale based answers in
English and Swahili. Kenyan physicians co-created and refined the dataset, and
a blinded expert review process ensured clinical accuracy, clarity, and
cultural appropriateness. The resulting Alama Health QA dataset includes
thousands of regulator-aligned question answer pairs across common outpatient
conditions. Beyond accuracy, we introduce evaluation metrics that test clinical
reasoning, safety, and adaptability such as rare case detection (Needle in the
Haystack), stepwise logic (Decision Points), and contextual adaptability.
Initial results reveal significant performance gaps when LLMs are applied to
localized scenarios, consistent with findings that LLM accuracy is lower on
African medical content than on US-based benchmarks. This work offers a
replicable model for guideline-driven, dynamic benchmarking to support safe AI
deployment in African health systems.

</details>


### [23] [Linear Relational Decoding of Morphology in Language Models](https://arxiv.org/abs/2507.14640)
*Eric Xia,Jugal Kalita*

Main category: cs.CL

> 本文发现，通过适应更大的类比测试集，线性变换Ws能够准确预测许多关系下的最终对象状态，特别是在形态学关系上达90%的保真度。这表明语言模型中的一些概念关系在潜在空间中是可解释的，并通过跨层线性转换稀疏编码。

<details>
  <summary>Details</summary>

**Motivation:** 研究旨在探索语言模型中潜在空间的表示方式，尤其是对于概念关系的线性变换是否可以准确重建对象状态。

**Method:** 使用适应性的Bigger Analogy Test Set，通过线性变换Ws（s代表主体标记的中间层表示，W从模型导数得出）来预测最终对象状态。

**Result:** 该线性技术在形态学关系上实现了90%的保真度，并且在多语言和模型上获得类似结果。

**Conclusion:** 语言模型中的部分概念关系，如形态学关系，在潜在空间中有可解释性，可通过跨层线性变换稀疏编码表示。

**Abstract:** A two-part affine approximation has been found to be a good approximation for
transformer computations over certain subject object relations. Adapting the
Bigger Analogy Test Set, we show that the linear transformation Ws, where s is
a middle layer representation of a subject token and W is derived from model
derivatives, is also able to accurately reproduce final object states for many
relations. This linear technique is able to achieve 90% faithfulness on
morphological relations, and we show similar findings multi-lingually and
across models. Our findings indicate that some conceptual relationships in
language models, such as morphology, are readily interpretable from latent
space, and are sparsely encoded by cross-layer linear transformations.

</details>


### [24] [Cleanse: Uncertainty Estimation Approach Using Clustering-based Semantic Consistency in LLMs](https://arxiv.org/abs/2507.14649)
*Minsuh Joo,Hyunsoo Cho*

Main category: cs.CL

> 研究提出了一种新的不确定性估计方法Cleanse，该方法利用聚类分析来量化大型语言模型隐藏嵌入中的语义一致性，从而区分准确和不准确的回答。

<details>
  <summary>Details</summary>

**Motivation:** 大型语言模型在生成回答时存在幻觉现象，即生成不准确的响应，这是一个重要的安全问题。为此，研究提出了一种有效的不确定性度量方法。

**Method:** 使用聚类方法对大型语言模型（LLMs）生成的隐藏嵌入的语义一致性进行量化，以评估不确定性。这种方法称作Clustering-based semantic consistency（Cleanse）。

**Result:** 通过使用四个现成的模型（LLaMA-7B, LLaMA-13B, LLaMA2-7B 和 Mistral-7B）和两个问答基准（SQuAD 和 CoQA）验证了Cleanse在检测幻觉现象的有效性。

**Conclusion:** 实验结果表明，Cleanse是一种有效的方法，能够帮助检测和减轻大型语言模型中的幻觉现象，增强其可靠性和安全性。

**Abstract:** Despite the outstanding performance of large language models (LLMs) across
various NLP tasks, hallucinations in LLMs--where LLMs generate inaccurate
responses--remains as a critical problem as it can be directly connected to a
crisis of building safe and reliable LLMs. Uncertainty estimation is primarily
used to measure hallucination levels in LLM responses so that correct and
incorrect answers can be distinguished clearly. This study proposes an
effective uncertainty estimation approach, \textbf{Cl}ust\textbf{e}ring-based
sem\textbf{an}tic con\textbf{s}ist\textbf{e}ncy (\textbf{Cleanse}). Cleanse
quantifies the uncertainty with the proportion of the intra-cluster consistency
in the total consistency between LLM hidden embeddings which contain adequate
semantic information of generations, by employing clustering. The effectiveness
of Cleanse for detecting hallucination is validated using four off-the-shelf
models, LLaMA-7B, LLaMA-13B, LLaMA2-7B and Mistral-7B and two
question-answering benchmarks, SQuAD and CoQA.

</details>


### [25] [Mangosteen: An Open Thai Corpus for Language Model Pretraining](https://arxiv.org/abs/2507.14664)
*Wannaphong Phatthiyaphaibun,Can Udomcharoenchaikit,Pakpoom Singkorapoom,Kunat Pipatanakul,Ekapol Chuangsuwanich,Peerat Limkonchotiwat,Sarana Nutanong*

Main category: cs.CL

> 研究者们构建了含有470亿个token的Mangosteen泰文语料库，使用改进的Dolma管道，并公开管道、清理记录、语料库快照和检查站，提升了语言模型的质量。

<details>
  <summary>Details</summary>

**Motivation:** 现有的大规模语料库依赖于以英文为中心或语言无关的管线，这些管线的启发法无法捕捉泰国语言特点或文化细微之处，因此需要构建一个透明、高质量的泰国语语料库。

**Method:** 通过适应泰国语言的Dolma管线构建了Mangosteen：一个47 billion-token的泰语文本语料库。该管线包括自定义的基于规则的语言ID、改进的C4/Gopher质量过滤器、泰国训练的内容过滤器，以及从维基百科、皇家公报文本、OCR提取的书籍和CC许可的YouTube字幕等非网络来源进行筛选。

**Result:** 通过使用GPT-2的系统消融实验，该管线将CommonCrawl从202M文件减少到25M文件，同时将SEA-HELM NLG分数从3提高到11；一个在Mangosteen上持续预训练的8B参数SEA-LION模型在泰国基准测试中超过了SEA-LION-v3和Llama-3.1约4分。

**Conclusion:** 全栈的管线代码、清洗清单、语料库快照和所有检查点都已经公开发布，为未来的泰国和区域LLM研究提供了一个可完全复制的基础。

**Abstract:** Pre-training data shapes a language model's quality, but raw web text is
noisy and demands careful cleaning. Existing large-scale corpora rely on
English-centric or language-agnostic pipelines whose heuristics do not capture
Thai script or cultural nuances, leaving risky material such as gambling
content untreated. Prior Thai-specific efforts customize pipelines or build new
ones, yet seldom release their data or document design choices, hindering
reproducibility and raising the question of how to construct a transparent,
high-quality Thai corpus. We introduce Mangosteen: a 47 billion-token Thai
corpus built through a Thai-adapted Dolma pipeline that includes custom
rule-based language ID, revised C4/Gopher quality filters, and Thai-trained
content filters, plus curated non-web sources such as Wikipedia, Royal Gazette
texts, OCR-extracted books, and CC-licensed YouTube subtitles. Systematic
ablations using GPT-2 show the pipeline trims CommonCrawl from 202M to 25M
documents while raising SEA-HELM NLG from 3 to 11; an 8B-parameter SEA-LION
model continually pre-trained on Mangosteen then surpasses SEA-LION-v3 and
Llama-3.1 by about four points on Thai benchmarks. We release the full pipeline
code, cleaning manifests, corpus snapshot, and all checkpoints, providing a
fully reproducible foundation for future Thai and regional LLM research.

</details>


### [26] [Large Language Models as Medical Codes Selectors: a benchmark using the International Classification of Primary Care](https://arxiv.org/abs/2507.14681)
*Vinicius Anjos de Almeida,Vinicius de Camargo,Raquel Gómez-Bravo,Egbert van der Haring,Kees van Boven,Marcelo Finger,Luis Fernandez Lopez*

Main category: cs.CL

> 大型语言模型（LLMs）在为巴西葡萄牙语的临床表达式分配ICPC-2代码方面显示出强大的自动化潜力，无需进行微调。最佳模型如gpt-4.5-preview、o3和gemini-2.5-pro达到了超过0.85的F1分数。

<details>
  <summary>Details</summary>

**Motivation:** 大型语言模型在为巴西葡萄牙语的临床表达式分配ICPC-2代码方面显示出强大的自动化潜力，无需进行微调。最佳模型如gpt-4.5-preview、o3和gemini-2.5-pro达到了超过0.85的F1分数。研究评估了大型语言模型在使用特定领域搜索引擎的输出来分配ICPC-2编码方面的潜力，这种编码在医疗保健数据中用于研究、质量监控和政策制定。使用包含437个巴西葡萄牙语临床表达式的标签数据集，每个表达式都标记了ICPC-2代码。利用OpenAI的text-embedding-3-large检索73,563个标签的概念，对33个语言模型进行提示和检索结果，以选择最佳匹配的ICPC-2代码。28个模型的F1分数大于0.8，其中10个模型超过0.85。最佳模型包括gpt-4.5-preview, o3, 和gemini-2.5-pro。检索器的优化可以显著提升性能。大多数模型返回正确的代码格式，较小模型（<3B）在格式化和输入长度上存在问题。虽然缺少端到端的临床验证，LLMs在不进行微调的情况下展示了程序自动化的潜力。此工作提供了基准测试，但结果受数据集范围和设置影响，需要更广泛的多语言评估。

**Method:** Structure

**Result:** {
  "tldr": "大型语言模型（LLMs）在为巴西葡萄牙语的临床表达式分配ICPC-2代码方面显示出强大的自动化潜力，无需进行微调。最佳模型如gpt-4.5-preview、o3和gemini-2.5-pro达到了超过0.85的F1分数。",
  "motivation": "研究评估了大型语言模型在使用特定领域搜索引擎的输出来分配ICPC-2编码方面的潜力，这种编码在医疗保健数据中用于研究、质量监控和政策制定。",
  "method": "使用包含437个巴西葡萄牙语临床表达式的标签数据集，每个表达式都标记了ICPC-2代码。利用OpenAI的text-embedding-3-large检索73,563个标签的概念，对33个语言模型进行提示和检索结果，以选择最佳匹配的ICPC-2代码。",
  "result": "28个模型的F1分数大于0.8，其中10个模型超过0.85。最佳模型包括gpt-4.5-preview, o3, 和gemini-2.5-pro。检索器的优化可以显著提升性能。大多数模型返回正确的代码格式，较小模型（<3B）在格式化和输入长度上存在问题。",
  "conclusion": "虽然缺少端到端的临床验证，LLMs在不进行微调的情况下展示了程序自动化的潜力。此工作提供了基准测试，但结果受数据集范围和设置影响，需要更广泛的多语言评估。\n"}

**Conclusion:** 虽然缺少端到端的临床验证，LLMs在不进行微调的情况下展示了程序自动化的潜力。此工作提供了基准测试，但结果受数据集范围和设置影响，需要更广泛的多语言评估。

**Abstract:** Background: Medical coding structures healthcare data for research, quality
monitoring, and policy. This study assesses the potential of large language
models (LLMs) to assign ICPC-2 codes using the output of a domain-specific
search engine.
  Methods: A dataset of 437 Brazilian Portuguese clinical expressions, each
annotated with ICPC-2 codes, was used. A semantic search engine (OpenAI's
text-embedding-3-large) retrieved candidates from 73,563 labeled concepts.
Thirty-three LLMs were prompted with each query and retrieved results to select
the best-matching ICPC-2 code. Performance was evaluated using F1-score, along
with token usage, cost, response time, and format adherence.
  Results: Twenty-eight models achieved F1-score > 0.8; ten exceeded 0.85. Top
performers included gpt-4.5-preview, o3, and gemini-2.5-pro. Retriever
optimization can improve performance by up to 4 points. Most models returned
valid codes in the expected format, with reduced hallucinations. Smaller models
(<3B) struggled with formatting and input length.
  Conclusions: LLMs show strong potential for automating ICPC-2 coding, even
without fine-tuning. This work offers a benchmark and highlights challenges,
but findings are limited by dataset scope and setup. Broader, multilingual,
end-to-end evaluations are needed for clinical validation.

</details>


### [27] [MiroMind-M1: An Open-Source Advancement in Mathematical Reasoning via Context-Aware Multi-Stage Policy Optimization](https://arxiv.org/abs/2507.14683)
*Xingxuan Li,Yao Xiao,Dianwen Ng,Hai Ye,Yue Deng,Xiang Lin,Bin Wang,Zhanfeng Mo,Chong Zhang,Yueyi Zhang,Zonglin Yang,Ruilin Li,Lei Lei,Shihao Xu,Han Zhao,Weiling Chen,Feng Ji,Lidong Bing*

Main category: cs.CL

> MiroMind-M1是一个开源推理语言模型系列，采用两阶段训练方法达到最佳数学推理性能，目的是提高推理语言模型的透明度和可重复性。

<details>
  <summary>Details</summary>

**Motivation:** 该研究旨在提升推理语言模型（RLM）开发的透明度和可重复性，尤其是在数学推理领域。

**Method:** 该研究开发了MiroMind-M1系列，这是一种基于Qwen-2.5的开源推理语言模型，通过两阶段训练（SFT阶段和RLVR阶段）并引入了Context-Aware Multi-Stage Policy Optimization算法来提升训练效率和模型性能。

**Result:** MiroMind-M1系列模型在AIME24、AIME25和MATH基准测试中达到了最佳或具有竞争力的性能，并且在Qwen-2.5基础上的开源模型中具有更高的代币效率。

**Conclusion:** MiroMind-M1系列模型展现出强大的数学推理能力，同时通过开放完整的技术栈支持进一步研究和社区发展。

**Abstract:** Large language models have recently evolved from fluent text generation to
advanced reasoning across diverse domains, giving rise to reasoning language
models. Among these domains, mathematical reasoning serves as a representative
benchmark as it requires precise multi-step logic and abstract reasoning, which
can be generalized to other tasks. While closed-source RLMs such as GPT-o3
demonstrate impressive reasoning capabilities, their proprietary nature limits
transparency and reproducibility. Although many open-source projects aim to
close this gap, most of them lack sufficient openness by omitting critical
resources such as datasets and detailed training configurations, which hinders
reproducibility. To contribute toward greater transparency in RLM development,
we introduce the MiroMind-M1 series, a set of fully open-source RLMs built on
the Qwen-2.5 backbone that match or exceed the performance of existing
open-source RLMs. Specifically, our models are trained in two stages: SFT on a
carefully curated corpus of 719K math-reasoning problems with verified CoT
trajectories, followed by RLVR on 62K challenging and verifiable problems. To
enhance the robustness and efficiency of the RLVR process, we introduce
Context-Aware Multi-Stage Policy Optimization, an algorithm that integrates
length-progressive training with an adaptive repetition penalty to encourage
context-aware RL training. Our model achieves state-of-the-art or competitive
performance and superior token efficiency among Qwen-2.5-based open-source 7B
and 32B models on the AIME24, AIME25, and MATH benchmarks. To facilitate
reproducibility, we release the complete stack: models (MiroMind-M1-SFT-7B,
MiroMind-M1-RL-7B, MiroMind-M1-RL-32B); datasets (MiroMind-M1-SFT-719K,
MiroMind-M1-RL-62K); and all training and evaluation configurations. We hope
these resources will support further research and foster community advancement.

</details>


### [28] [Mind the Gap: A Review of Arabic Post-Training Datasets and Their Limitations](https://arxiv.org/abs/2507.14688)
*Mohammed Alkhowaiter,Norah Alshahrani,Saied Alshahrani,Reem I. Masoud,Alaa Alzahrani,Deema Alnuhait,Emad A. Alghamdi,Khalid Almubarak*

Main category: cs.CL

> 本文审查了Hugging Face Hub上的阿拉伯语后训练数据集，发现了任务多样性不足、文档和注释不一致或缺失、社区采用率低等关键空白，并提出了未来相关数据集开发的建议。

<details>
  <summary>Details</summary>

**Motivation:** 本文旨在审查并填补阿拉伯语大语言模型后训练数据集开发中的一些关键空白，这些数据集的质量直接影响着模型与人类指令的一致性及其在各种任务上的表现。

**Method:** 此论文通过四个关键维度对Hugging Face Hub上公开的阿拉伯语后训练数据集进行了审查，分别是LLM能力、导向性、对齐性和鲁棒性。每个数据集都根据流行度、实用采用情况、时效性和维护情况、文档和注释质量、许可透明度和科学贡献进行了严格评估。

**Result:** 审查结果揭示了阿拉伯语后训练数据集中存在任务多样性不足、文档和注释不一致或缺失以及社区采纳率低等一系列关键空白，这些问题可能限制了阿拉伯语大语言模型的能力和表现。

**Conclusion:** 文中讨论了这些审查发现对阿拉伯语大语言模型及其未来发展的潜在影响，并建议未来应致力于数据集的多元化以及文档和注释的质量提升。

**Abstract:** Post-training has emerged as a crucial technique for aligning pre-trained
Large Language Models (LLMs) with human instructions, significantly enhancing
their performance across a wide range of tasks. Central to this process is the
quality and diversity of post-training datasets. This paper presents a review
of publicly available Arabic post-training datasets on the Hugging Face Hub,
organized along four key dimensions: (1) LLM Capabilities (e.g., Question
Answering, Translation, Reasoning, Summarization, Dialogue, Code Generation,
and Function Calling); (2) Steerability (e.g., persona and system prompts); (3)
Alignment (e.g., cultural, safety, ethics, and fairness), and (4) Robustness.
Each dataset is rigorously evaluated based on popularity, practical adoption,
recency and maintenance, documentation and annotation quality, licensing
transparency, and scientific contribution. Our review revealed critical gaps in
the development of Arabic post-training datasets, including limited task
diversity, inconsistent or missing documentation and annotation, and low
adoption across the community. Finally, the paper discusses the implications of
these gaps on the progress of Arabic LLMs and applications while providing
concrete recommendations for future efforts in post-training dataset
development.

</details>


### [29] [Rethinking Suicidal Ideation Detection: A Trustworthy Annotation Framework and Cross-Lingual Model Evaluation](https://arxiv.org/abs/2507.14693)
*Amina Dzafic,Merve Kavut,Ulya Bayram*

Main category: cs.CL

> 研究团队创建了一种新型的土耳其语自杀意念数据集，并提出了一个资源高效的注释框架，以解决注释数据稀缺和语言限制的问题，并展示了流行模型在迁移学习中的表现问题，强调了数据集和模型透明度的要求。

<details>
  <summary>Details</summary>

**Motivation:** 解决自杀意念检测在实时预防自杀方面的两大未充分探索的挑战：有限的语言覆盖范围和不可靠的注释实践，并强调对注释和评估的更严谨、包容语言的方法的需要，特别是针对心理健康领域的自然语言处理（NLP）。

**Method:** 通过构建一个来源于社交媒体帖子的新型土耳其语自杀意念语料库并引入一个涉及三位人类注释员和两个大型语言模型（LLMs）的资源高效的注释框架来解决注释数据的稀缺性和语言限制问题。此外，通过双向评估标签可靠性和模型一致性来解决其他问题，使用迁移学习通过八个预训练的情感和情绪分类器对这个数据集和其他三种流行的英语自杀意念检测数据集进行分析，以评估注释一致性和标杆模型性能。

**Result:** 研究人员发现，在没有经过训练的情况下，流行的模型在零样本迁移学习中的性能存在问题，强调了在心理健康NLP领域中对模型训练和数据集构建的透明度要求，以及对数据和模型可靠性的优先考虑。

**Conclusion:** 研究结果强调了心理健康NLP领域中对注释和评估的更严谨、包容语言的方法的需要，特别提出对模型训练和数据集构建的透明度要求，是为了确保数据和模型的可靠性。这些发现对全球范围内通过人工智能实现自杀预防具有重要意义。

**Abstract:** Suicidal ideation detection is critical for real-time suicide prevention, yet
its progress faces two under-explored challenges: limited language coverage and
unreliable annotation practices. Most available datasets are in English, but
even among these, high-quality, human-annotated data remains scarce. As a
result, many studies rely on available pre-labeled datasets without examining
their annotation process or label reliability. The lack of datasets in other
languages further limits the global realization of suicide prevention via
artificial intelligence (AI). In this study, we address one of these gaps by
constructing a novel Turkish suicidal ideation corpus derived from social media
posts and introducing a resource-efficient annotation framework involving three
human annotators and two large language models (LLMs). We then address the
remaining gaps by performing a bidirectional evaluation of label reliability
and model consistency across this dataset and three popular English suicidal
ideation detection datasets, using transfer learning through eight pre-trained
sentiment and emotion classifiers. These transformers help assess annotation
consistency and benchmark model performance against manually labeled data. Our
findings underscore the need for more rigorous, language-inclusive approaches
to annotation and evaluation in mental health natural language processing (NLP)
while demonstrating the questionable performance of popular models with
zero-shot transfer learning. We advocate for transparency in model training and
dataset construction in mental health NLP, prioritizing data and model
reliability.

</details>


### [30] [Disparities in Peer Review Tone and the Role of Reviewer Anonymity](https://arxiv.org/abs/2507.14741)
*Maria Sahakyan,Bedoor AlShebli*

Main category: cs.CL

> 通过对超过80,000篇来自两个主要期刊的同行评审进行分析，本研究揭示了语言如何在作者性别、种族和机构隶属等方面存在差异，同时通过实名和匿名的评审比较，揭示了评审者身份披露对评价语言的影响，并提出了同行评审改革的必要性。

<details>
  <summary>Details</summary>

**Motivation:** 尽管结构不公在同行评审中被广泛讨论，但对语言本身如何强化差异的关注甚少。本研究旨在通过语用分析揭示同行评审中潜藏的偏见，并挑战匿名性在公平性方面的传统假设。

**Method:** 本研究使用自然语言处理和大规模统计建模方法，对超过80,000篇来自两个主要期刊的同行评审进行了全面的语用分析。研究探讨了回顾语气、情感以及支持性语言如何在作者性别、种族和机构隶属等方面存在差异，并通过包括匿名和实名评审在内的数据集，揭示了评审者身份披露对评价语言的影响。

**Result:** 研究结果展现了评审语气、情感和支持性语言如何按作者性别、种族和机构背景变化，并通过比较实名和匿名评审，揭示了评审者身份的揭示如何影响评估语言。这些结果质疑了匿名性在公平中的假设，并指出同行评审过程中存在的隐性偏见。

**Conclusion:** 研究发现揭露了同行反馈中存在的隐藏偏见，并挑战了匿名性在公平性中的作用的常规假设。随着学术出版业正面对改革，本研究提出了关于评审政策如何影响职业轨迹和科学进步的关键问题。

**Abstract:** The peer review process is often regarded as the gatekeeper of scientific
integrity, yet increasing evidence suggests that it is not immune to bias.
Although structural inequities in peer review have been widely debated, much
less attention has been paid to the subtle ways in which language itself may
reinforce disparities. This study undertakes one of the most comprehensive
linguistic analyses of peer review to date, examining more than 80,000 reviews
in two major journals. Using natural language processing and large-scale
statistical modeling, it uncovers how review tone, sentiment, and supportive
language vary across author demographics, including gender, race, and
institutional affiliation. Using a data set that includes both anonymous and
signed reviews, this research also reveals how the disclosure of reviewer
identity shapes the language of evaluation. The findings not only expose hidden
biases in peer feedback, but also challenge conventional assumptions about
anonymity's role in fairness. As academic publishing grapples with reform,
these insights raise critical questions about how review policies shape career
trajectories and scientific progress.

</details>


### [31] [On the robustness of modeling grounded word learning through a child's egocentric input](https://arxiv.org/abs/2507.14749)
*Wai Keen Vong,Brenden M. Lake*

Main category: cs.CL

> 研究通过使用自动语音转录方法处理SAYCam数据集，并生成用于训练和评估的多模态数据集，结果表明多模态神经网络可以从每个孩子的发育经历中稳健地学习并泛化词-指称映射，同时也强调了学习过程中的个体差异。

<details>
  <summary>Details</summary>

**Motivation:** 为了进一步理解机器学习在人类语言习得中的作用，并验证多模态神经网络方法在不同儿童身上的学习模式的稳健性。

**Method:** 通过将自动语音转录方法应用于包含超过500小时视频数据的SAYCam数据集，研究人员生成了用于训练和评估的多模态视觉和语言数据集，并探索了一系列神经网络配置，以检验模拟词汇学习的稳健性。

**Result:** 结果表明，针对每个孩子的发展经历进行训练的网络，可以在多种网络架构中获得并泛化词-指称映射。这验证了多模态神经网络在基于情境的学习中的稳健性，同时也强调了当模型根据每个孩子的发育经历进行训练时出现的个体差异。

**Conclusion:** 这些发现验证了多模态神经网络在基于情境的词汇学习中稳健性的适应性，同时也强调了模型学习过程中基于每个孩子独特经历的个体差异的显著性。

**Abstract:** What insights can machine learning bring to understanding human language
acquisition? Large language and multimodal models have achieved remarkable
capabilities, but their reliance on massive training datasets creates a
fundamental mismatch with children, who succeed in acquiring language from
comparatively limited input. To help bridge this gap, researchers have
increasingly trained neural networks using data similar in quantity and quality
to children's input. Taking this approach to the limit, Vong et al. (2024)
showed that a multimodal neural network trained on 61 hours of visual and
linguistic input extracted from just one child's developmental experience could
acquire word-referent mappings. However, whether this approach's success
reflects the idiosyncrasies of a single child's experience, or whether it would
show consistent and robust learning patterns across multiple children's
experiences was not explored. In this article, we applied automated speech
transcription methods to the entirety of the SAYCam dataset, consisting of over
500 hours of video data spread across all three children. Using these automated
transcriptions, we generated multi-modal vision-and-language datasets for both
training and evaluation, and explored a range of neural network configurations
to examine the robustness of simulated word learning. Our findings demonstrate
that networks trained on automatically transcribed data from each child can
acquire and generalize word-referent mappings across multiple network
architectures. These results validate the robustness of multimodal neural
networks for grounded word learning, while highlighting the individual
differences that emerge in how models learn when trained on each child's
developmental experiences.

</details>


### [32] [GRACE: Generative Recommendation via Journey-Aware Sparse Attention on Chain-of-Thought Tokenization](https://arxiv.org/abs/2507.14758)
*Luyi Ma,Wanjia Zhang,Kai Zhao,Abhishek Kulkarni,Lalitesh Morishetti,Anjana Ganesh,Ashish Ranjan,Aashika Padmanabhan,Jianpeng Xu,Jason Cho,Praveen Kanumala,Kaushiki Nag,Sumit Dutta,Kamiya Motwani,Malay Patel,Evren Korpeoglu,Sushant Kumar,Kannan Achan*

Main category: cs.CL

> GRACE是一种为提高多行为序列推荐系统性能而提出的生成推荐框架，其通过改进的标记化方法和优化的注意力机制来实现这一目的，并在实验中表现出优于现有模型的表现。

<details>
  <summary>Details</summary>

**Motivation:** 生成模型在多行为推荐系统中展现出了潜力，但由于缺乏显式信息、高计算成本和有限的多尺度建模能力，其采用受到了限制。

**Method:** GRACE框架采用了混合的Chain-of-Thought (CoT) 标记化方法和Journey-Aware Sparse Attention (JSA)机制，以分别提高生成的可解释性和解决标准注意力机制的效率问题。

**Result:** 该论文提出了一种名为GRACE的生成推荐框架，通过引入一种混合的CoT标记化方法，并设计了JSA机制以解决标准注意力机制的低效问题，从而改善多行为序列推荐系统。实验结果表明，GRACE在两个现实世界数据集上显著优于现有最佳基线模型。

**Conclusion:** 实验结果显示，GRACE在特定领域内的标准化评估指标上超过现有最佳模型，同时减少了注意力计算成本。

**Abstract:** Generative models have recently demonstrated strong potential in
multi-behavior recommendation systems, leveraging the expressive power of
transformers and tokenization to generate personalized item sequences. However,
their adoption is hindered by (1) the lack of explicit information for token
reasoning, (2) high computational costs due to quadratic attention complexity
and dense sequence representations after tokenization, and (3) limited
multi-scale modeling over user history. In this work, we propose GRACE
(Generative Recommendation via journey-aware sparse Attention on
Chain-of-thought tokEnization), a novel generative framework for multi-behavior
sequential recommendation. GRACE introduces a hybrid Chain-of-Thought (CoT)
tokenization method that encodes user-item interactions with explicit
attributes from product knowledge graphs (e.g., category, brand, price) over
semantic tokenization, enabling interpretable and behavior-aligned generation.
To address the inefficiency of standard attention, we design a Journey-Aware
Sparse Attention (JSA) mechanism, which selectively attends to compressed,
intra-, inter-, and current-context segments in the tokenized sequence.
Experiments on two real-world datasets show that GRACE significantly
outperforms state-of-the-art baselines, achieving up to +106.9% HR@10 and
+106.7% NDCG@10 improvement over the state-of-the-art baseline on the Home
domain, and +22.1% HR@10 on the Electronics domain. GRACE also reduces
attention computation by up to 48% with long sequences.

</details>


### [33] [FastLongSpeech: Enhancing Large Speech-Language Models for Efficient Long-Speech Processing](https://arxiv.org/abs/2507.14815)
*Shoutao Guo,Shaolei Zhang,Qingkai Fang,Zhengrui Ma,Min Zhang,Yang Feng*

Main category: cs.CL

> 提出FastLongSpeech框架，通过动态压缩技术优化LSLMs处理长语音序列的能力。

<details>
  <summary>Details</summary>

**Motivation:** 当前LSLMs在处理长语音方面面临挑战，主要问题是长语音训练数据缺乏和计算成本高昂，FastLongSpeech框架旨在解决这些问题。

**Method:** FastLongSpeech框架使用迭代融合策略压缩长语音序列，并通过动态压缩训练方法使模型适应长语音输入，这种方法利用短语音序列的不同压缩比例进行训练。

**Result:** 该研究介绍了FastLongSpeech框架，旨在通过创新的方法提升大型语音-语言模型（LSLMs）处理长语音序列的能力。通过迭代融合策略和动态压缩训练方法，FastLongSpeech能够在不依赖特定长语音训练数据的情况下优化模型性能。此外，研究还开发了一个长语音理解基准测试LongSpeech-Eval，实验结果表明该方法在处理长语音和短语音任务时表现优异，同时大幅提升了推理效率。

**Conclusion:** FastLongSpeech框架能够有效提升LSLMs处理长语音序列的能力，同时保持对短语音任务的高效处理，展示了其在语音理解与生成领域的显著优势。

**Abstract:** The rapid advancement of Large Language Models (LLMs) has spurred significant
progress in Large Speech-Language Models (LSLMs), enhancing their capabilities
in both speech understanding and generation. While existing LSLMs often
concentrate on augmenting speech generation or tackling a diverse array of
short-speech tasks, the efficient processing of long-form speech remains a
critical yet underexplored challenge. This gap is primarily attributed to the
scarcity of long-speech training datasets and the high computational costs
associated with long sequences. To address these limitations, we introduce
FastLongSpeech, a novel framework designed to extend LSLM capabilities for
efficient long-speech processing without necessitating dedicated long-speech
training data. FastLongSpeech incorporates an iterative fusion strategy that
can compress excessively long-speech sequences into manageable lengths. To
adapt LSLMs for long-speech inputs, it introduces a dynamic compression
training approach, which exposes the model to short-speech sequences at varying
compression ratios, thereby transferring the capabilities of LSLMs to
long-speech tasks. To assess the long-speech capabilities of LSLMs, we develop
a long-speech understanding benchmark called LongSpeech-Eval. Experiments show
that our method exhibits strong performance in both long-speech and
short-speech tasks, while greatly improving inference efficiency.

</details>


### [34] [Doc2Chart: Intent-Driven Zero-Shot Chart Generation from Documents](https://arxiv.org/abs/2507.14819)
*Akriti Jain,Pritika Ramu,Aparna Garimella,Apoorv Saxena*

Main category: cs.CL

> 论文提出了一个两阶段的无监督框架来基于用户给定的意图从长文档中生成图表，并设计了一种新的衡量生成图表数据准确性的指标。实验结果在特定的准确性和图表类型选择上比基线方法高出9到17个百分点。

<details>
  <summary>Details</summary>

**Motivation:** 该研究旨在解决从长文档中基于用户意图生成数据图表的问题，而不像过去那样需要用户手动选取相关内容。提出了一种在零样本设定下基于意图从文档生成图表的方法。

**Method:** 在该论文中，作者提出了一个无监督的、分两个阶段的框架来解决基于意图的图表生成问题。在这个框架中，首先使用大语言模型（LLMs）从文档中抽取相关信息，然后通过迭代验证和优化抽取的数据。第二阶段，一个由启发式指导的模块会根据选择的适当图表类型来生成最终的代码。

**Result:** 作者使用他们整理的1,242个包含金融和科学领域意图、文档和图表的元组数据集评估了这种方法。结果显示这种方法在图表数据的准确性和图表类型的选择上显著优于单次生成图表的方法和基于查询的检索方法，分别高出9和17个百分点。

**Conclusion:** 这项研究展示了可从长文档中基于意图生成数据图表的有效方法，性能超越现有办法。通过提出新方法和评估指标，该研究为未来基于文档数据的可视化任务提供了坚实的基础。

**Abstract:** Large Language Models (LLMs) have demonstrated strong capabilities in
transforming text descriptions or tables to data visualizations via
instruction-tuning methods. However, it is not straightforward to apply these
methods directly for a more real-world use case of visualizing data from long
documents based on user-given intents, as opposed to the user pre-selecting the
relevant content manually. We introduce the task of intent-based chart
generation from documents: given a user-specified intent and document(s), the
goal is to generate a chart adhering to the intent and grounded on the
document(s) in a zero-shot setting. We propose an unsupervised, two-staged
framework in which an LLM first extracts relevant information from the
document(s) by decomposing the intent and iteratively validates and refines
this data. Next, a heuristic-guided module selects an appropriate chart type
before final code generation. To assess the data accuracy of the generated
charts, we propose an attribution-based metric that uses a structured textual
representation of charts, instead of relying on visual decoding metrics that
often fail to capture the chart data effectively. To validate our approach, we
curate a dataset comprising of 1,242 $<$intent, document, charts$>$ tuples from
two domains, finance and scientific, in contrast to the existing datasets that
are largely limited to parallel text descriptions/ tables and their
corresponding charts. We compare our approach with baselines using single-shot
chart generation using LLMs and query-based retrieval methods; our method
outperforms by upto $9$ points and $17$ points in terms of chart data accuracy
and chart type respectively over the best baselines.

</details>


### [35] [Beyond Isolated Capabilities: Bridging Long CoT Reasoning and Long-Context Understanding](https://arxiv.org/abs/2507.14849)
*Yifei Wang*

Main category: cs.CL

> 本研究展示了推理提炼方法对于提升模型长上下文理解能力的显著效果，特别是在多文档问答任务中。

<details>
  <summary>Details</summary>

**Motivation:** 鉴于Retrieval-Augmented Generation (RAG)系统的日益重要性，这些系统在生成可靠回复时，需要高效地获取和利用上下文信息。本研究旨在探讨扩展的长-CoT过程如何影响长上下文理解。

**Method:** 本研究通过使用一系列由Deepseek-R1（以其卓越的推理能力而闻名）提炼出的开源模型，进行综合调查，评估这些模型在多文档问答任务中从扩展上下文中提取和整合相关信息的能力。

**Result:** 研究表明，通过提炼推理模式，可以显著提高对长上下文的理解。提炼过程促进了上下文分析和信息解析过程中的更详细和明确的推理过程，有效缓解了长期以来困扰长上下文模型的"中间迷失"问题。

**Conclusion:** 研究表明，推理提炼方法显著提升了小模型在长上下文理解上的能力，为提高模型的整体表现提供了新的可能性。

**Abstract:** Reasoning distillation has emerged as an effective approach to enhance the
reasoning capabilities of smaller language models. However, the impact of
large-scale reasoning distillation on other critical abilities, particularly
in-context retrieval and reasoning, remains unexplored. This gap in
understanding is particularly significant given the increasing importance of
Retrieval-Augmented Generation (RAG) systems, where efficient acquisition and
utilization of contextual information are paramount for generating reliable
responses. Motivated by the need to understand how the extended long-CoT
process influences long-context comprehension, we conduct a comprehensive
investigation using a series of open-source models distilled from Deepseek-R1,
renowned for its exceptional reasoning capabilities. Our study focuses on
evaluating these models' performance in extracting and integrating relevant
information from extended contexts through multi-document question and
answering tasks. Through rigorous experimentation, we demonstrate that
distilled reasoning patterns significantly improve long-context understanding.
Our analysis reveals that distillation fosters greater long-context awareness
by promoting more detailed and explicit reasoning processes during context
analysis and information parsing. This advancement effectively mitigates the
persistent "lost in the middle" issue that has hindered long-context models.

</details>


### [36] [Tiny language models](https://arxiv.org/abs/2507.14871)
*Ronit D. Gross,Yarden Tzach,Tal Halevi,Ella Koresh,Ido Kanter*

Main category: cs.CL

> 此研究探讨了微型语言模型（TLMs）是否能展示与大语言模型（LLMs）相似的关键特征，并证明了预训练的重要性，展示了TLMs实现低延迟和高分类准确性的潜力。

<details>
  <summary>Details</summary>

**Motivation:** 由于大语言模型（LLMs）的预训练需要极大的计算资源，只有少数主导公司能够进行。这限制了更广泛的研究参与，因此存在着迫切需要寻找更加可访问的替代方案。本研究旨在探索微型语言模型（TLMs）是否可以提供这种解决方案。

**Method:** 本文研究了微型语言模型（TLMs）是否展示了与大语言模型（LLMs）相似的关键特征。实验通过预训练和非预训练的微型语言模型在分类任务上的性能差距进行了评估，使用了BERT-6及其变种，在维基百科数据子集上进行预训练，并在FewRel、AGNews和DBPedia分类任务上进行性能评估。

**Result:** 研究发现，微型语言模型（TLMs）在预训练和非预训练模型之间的性能差距显著，说明了即使在较小的规模下，预训练也十分有效。性能差距随着预训练数据集规模的增加和预训练数据集与分类数据集之间的令牌重叠度的增加而增大。此外，通过多个独立预训练和较浅结构的软委员会可以复制深度TLM架构的分类准确性，从而实现低延迟的TLMs而不影响分类准确性。

**Conclusion:** 研究结果表明，微型语言模型（TLMs）在NLP领域具有显著的潜力。它们不仅可以降低大语言模型（LLMs）预训练所需的大量计算资源需求，还可能揭示NLP机制背后的原理，尤其是所使用的生物启发模型表明TLMs可能足以让儿童或青少年习得语言。

**Abstract:** A prominent achievement of natural language processing (NLP) is its ability
to understand and generate meaningful human language. This capability relies on
complex feedforward transformer block architectures pre-trained on large
language models (LLMs). However, LLM pre-training is currently feasible only
for a few dominant companies due to the immense computational resources
required, limiting broader research participation. This creates a critical need
for more accessible alternatives. In this study, we explore whether tiny
language models (TLMs) exhibit the same key qualitative features of LLMs. We
demonstrate that TLMs exhibit a clear performance gap between pre-trained and
non-pre-trained models across classification tasks, indicating the
effectiveness of pre-training, even at a tiny scale. The performance gap
increases with the size of the pre-training dataset and with greater overlap
between tokens in the pre-training and classification datasets. Furthermore,
the classification accuracy achieved by a pre-trained deep TLM architecture can
be replicated through a soft committee of multiple, independently pre-trained
shallow architectures, enabling low-latency TLMs without affecting
classification accuracy. Our results are based on pre-training BERT-6 and
variants of BERT-1 on subsets of the Wikipedia dataset and evaluating their
performance on FewRel, AGNews, and DBPedia classification tasks. Future
research on TLM is expected to further illuminate the mechanisms underlying
NLP, especially given that its biologically inspired models suggest that TLMs
may be sufficient for children or adolescents to develop language.

</details>


### [37] [MEKiT: Multi-source Heterogeneous Knowledge Injection Method via Instruction Tuning for Emotion-Cause Pair Extraction](https://arxiv.org/abs/2507.14887)
*Shiyi Mu,Yongkang Liu,Shi Feng,Xiaocui Yang,Daling Wang,Yifei Zhang*

Main category: cs.CL

> MEKiT, a multi-source knowledge injection method, enhances large language models' performance on the ECPE task by integrating internal and external knowledge, significantly surpassing baseline models.

<details>
  <summary>Details</summary>

**Motivation:** The motivation stems from the underperformance of large language models on ECPE tasks, attributed to their limited auxiliary knowledge for emotion perception and cause reasoning compared to smaller models.

**Method:** The paper proposes MEKiT, which integrates internal emotional knowledge and external causal knowledge into large language models to improve their performance on the ECPE task. It employs instruction templates and data mixing during instruction-tuning to better identify emotions and reason about causes.

**Result:** Experimental results show that MEKiT outperforms comparative baselines and significantly enhances LLMs' performance on the ECPE task.

**Conclusion:** MEKiT demonstrates a more effective and adaptable solution for the ECPE task, proving superior over baselines by significantly improving LLMs' performance.

**Abstract:** Although large language models (LLMs) excel in text comprehension and
generation, their performance on the Emotion-Cause Pair Extraction (ECPE) task,
which requires reasoning ability, is often underperform smaller language model.
The main reason is the lack of auxiliary knowledge, which limits LLMs' ability
to effectively perceive emotions and reason causes. To address this issue, we
propose a novel \textbf{M}ulti-source h\textbf{E}terogeneous \textbf{K}nowledge
\textbf{i}njection me\textbf{T}hod, MEKiT, which integrates heterogeneous
internal emotional knowledge and external causal knowledge. Specifically, for
these two distinct aspects and structures of knowledge, we apply the approaches
of incorporating instruction templates and mixing data for instruction-tuning,
which respectively facilitate LLMs in more comprehensively identifying emotion
and accurately reasoning causes. Experimental results demonstrate that MEKiT
provides a more effective and adaptable solution for the ECPE task, exhibiting
an absolute performance advantage over compared baselines and dramatically
improving the performance of LLMs on the ECPE task.

</details>


### [38] [Sparse Autoencoder-guided Supervised Finetuning to Mitigate Unexpected Code-Switching in LLMs](https://arxiv.org/abs/2507.14894)
*Boyi Deng,Yu Wan,Baosong Yang,Fei Huang,Wenjie Wang,Fuli Feng*

Main category: cs.CL

> 研究了大型语言模型中意外的语法转换问题，提出了使用稀疏自编码器引导的监督微调方法（SASFT），有效减少语法转换，并保持多语庝能力。

<details>
  <summary>Details</summary>

**Motivation:** 大型语言模型虽然有多语言能力，但在模型响应中会不预期地切换语言，导致可读性差，现有研究对这一问题的机制分析不足且效果有限。

**Method:** 使用稀疏自编码器对意外的语法转换进行了深入分析，并发现当模型转换到某种语言时，该语言的特征表现出了过高的预激活值。基于这一发现，提出了SASFT（稀疏自编码器引导的监督微调），教导模型在训练过程中维持适当的预激活值。

**Result:** 在三种语言的五个模型上进行的实验表明，与标准的监督微调相比，SASFT能一致地将意外语法转换减少超过50%，甚至在四个案例中完全消除。此外，SASFT还能保持甚至提高模型在六个跨语言基准测试中的表现。

**Conclusion:** SASFT方法显著减少了意外的语法转换，保持或提高了语言模型的多语言性能，表明了其在解决语言模型中的联系混合问题上的有效性。

**Abstract:** Large Language Models (LLMs) have impressive multilingual capabilities, but
they suffer from unexpected code-switching, also known as language mixing,
which involves switching to unexpected languages in the model response. This
problem leads to poor readability and degrades the usability of model
responses. However, existing work on this issue lacks a mechanistic analysis
and shows limited effectiveness. In this paper, we first provide an in-depth
analysis of unexpected code-switching using sparse autoencoders and find that
when LLMs switch to a language, the features of that language exhibit excessive
pre-activation values. Based on our findings, we propose $\textbf{S}$parse
$\textbf{A}$utoencoder-guided $\textbf{S}$upervised
$\textbf{F}$ine$\textbf{t}$uning (SASFT), which teaches LLMs to maintain
appropriate pre-activation values of specific language features during
training. Experiments on five models across three languages demonstrate that
SASFT consistently reduces unexpected code-switching by more than 50\% compared
to standard supervised fine-tuning, with complete elimination in four cases.
Moreover, SASFT maintains or even improves the models' performance on six
multilingual benchmarks, showing its effectiveness in addressing code-switching
while preserving multilingual capabilities.

</details>


### [39] [From Neurons to Semantics: Evaluating Cross-Linguistic Alignment Capabilities of Large Language Models via Neurons Alignment](https://arxiv.org/abs/2507.14900)
*Chongxuan Huang,Yongshi Ye,Biao Fu,Qifeng Su,Xiaodong Shi*

Main category: cs.CL

> 提出了基于神经元状态的跨语言对齐评估方法NeuronXA，有效评估了几个多语言大模型的跨语言对齐能力，并展示了其在小数据集上的有效性。

<details>
  <summary>Details</summary>

**Motivation:** 为了克服现有跨语言对齐基准测试主要集中在句子嵌入上的局限性，这些测试受神经模型产生非平滑表示空间的影响，尤其是在低资源语言上，提出了基于神经元状态的新方法。

**Method:** 根据神经科学发现，提出基于神经元状态的跨语言对齐（NeuronXA）方法，用以评估语言模型跨语言的对齐能力。

**Result:** 在二项迁移任务和三项多语言基准测试中证明，NeuronXA在仅有100个并行句子对的情况下，与下游任务表现和迁移能力有很高的皮尔森相关性。

**Conclusion:** NeuronXA方法可以有效评估大语言模型的跨语言对齐能力和迁移能力。这种方法显示出其在跨语言对齐研究和改善多语言大型语言模型的语义理解方面的潜力。

**Abstract:** Large language models (LLMs) have demonstrated remarkable multilingual
capabilities, however, how to evaluate cross-lingual alignment remains
underexplored. Existing alignment benchmarks primarily focus on sentence
embeddings, but prior research has shown that neural models tend to induce a
non-smooth representation space, which impact of semantic alignment evaluation
on low-resource languages. Inspired by neuroscientific findings that similar
information activates overlapping neuronal regions, we propose a novel Neuron
State-Based Cross-Lingual Alignment (NeuronXA) to assess the cross-lingual a
lignment capabilities of LLMs, which offers a more semantically grounded
approach to assess cross-lingual alignment. We evaluate NeuronXA on several
prominent multilingual LLMs (LLaMA, Qwen, Mistral, GLM, and OLMo) across two
transfer tasks and three multilingual benchmarks. The results demonstrate that
with only 100 parallel sentence pairs, NeuronXA achieves a Pearson correlation
of 0.9556 with downstream tasks performance and 0.8514 with transferability.
These findings demonstrate NeuronXA's effectiveness in assessing both
cross-lingual alignment and transferability, even with a small dataset. This
highlights its potential to advance cross-lingual alignment research and to
improve the semantic understanding of multilingual LLMs.

</details>


### [40] [PromptSuite: A Task-Agnostic Framework for Multi-Prompt Generation](https://arxiv.org/abs/2507.14913)
*Eliya Habba,Noam Dahan,Gili Lior,Gabriel Stanovsky*

Main category: cs.CL

> 介绍PromptSuite框架，解决自动生成多提示以支持强健评估的挑战。

<details>
  <summary>Details</summary>

**Motivation:** 单一提示评估大语言模型可靠性低，变化小也会导致性能差异大，多提示评估难以广泛采用。

**Method:** 通过引入PromptSuite框架解决多提示评估中的生成挑战，该框架能自动生成多种提示并支持模块化设计和可控的扰动方式。

**Result:** 通过一系列案例研究证明，PromptSuite能够提供有意义的提示变化以支持强健的评估实践。

**Conclusion:** PromptSuite框架支持在各类任务和基准测试中实现强健的评估实践，并提供了Python API和用户友好的网络界面。

**Abstract:** Evaluating LLMs with a single prompt has proven unreliable, with small
changes leading to significant performance differences. However, generating the
prompt variations needed for a more robust multi-prompt evaluation is
challenging, limiting its adoption in practice. To address this, we introduce
PromptSuite, a framework that enables the automatic generation of various
prompts. PromptSuite is flexible - working out of the box on a wide range of
tasks and benchmarks. It follows a modular prompt design, allowing controlled
perturbations to each component, and is extensible, supporting the addition of
new components and perturbation types. Through a series of case studies, we
show that PromptSuite provides meaningful variations to support strong
evaluation practices. It is available through both a Python API:
https://github.com/eliyahabba/PromptSuite, and a user-friendly web interface:
https://promptsuite.streamlit.app/

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [41] [Comparative Analysis of Algorithms for the Fitting of Tessellations to 3D Image Data](https://arxiv.org/abs/2507.14268)
*Andreas Alpers,Orkun Furat,Christian Jung,Matthias Neumann,Claudia Redenbach,Aigerim Saken,Volker Schmidt*

Main category: cs.CV

> A comparative analysis of optimization-based strategies for fitting tessellation models to 3D materials, revealing trade-offs between model & optimization complexity and approximation quality.

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to review and assess various algorithmic strategies for fitting tessellation models to 3D image data of materials like polycrystals and foams, to provide guidance on method selection based on data characteristics and application needs.

**Method:** The paper uses optimization-based methods, including linear and nonlinear programming, stochastic optimization with the cross-entropy method, and gradient descent, to generate Voronoi, Laguerre, and generalized balanced power diagrams (GBPDs) approximating voxel-based grain structures.

**Result:** The comparative analysis evaluates the quality of fit using discrepancy measures that assess differences in grain volume, surface area, and topology on real-world datasets, revealing trade-offs between model complexity, optimization routine complexity, and approximation quality.

**Conclusion:** The study highlights that while more complex models and optimization routines offer improved approximation of grain structures, these come at the cost of increased computational difficulty, providing insights for selecting suitable methods for specific applications.

**Abstract:** This paper presents a comparative analysis of algorithmic strategies for
fitting tessellation models to 3D image data of materials such as polycrystals
and foams. In this steadily advancing field, we review and assess
optimization-based methods -- including linear and nonlinear programming,
stochastic optimization via the cross-entropy method, and gradient descent --
for generating Voronoi, Laguerre, and generalized balanced power diagrams
(GBPDs) that approximate voxelbased grain structures. The quality of fit is
evaluated on real-world datasets using discrepancy measures that quantify
differences in grain volume, surface area, and topology. Our results highlight
trade-offs between model complexity, the complexity of the optimization
routines involved, and the quality of approximation, providing guidance for
selecting appropriate methods based on data characteristics and application
needs.

</details>


### [42] [Semantic Segmentation based Scene Understanding in Autonomous Vehicles](https://arxiv.org/abs/2507.14303)
*Ehsan Rassekh*

Main category: cs.CV

> 本文通过研究几种不同的模型并使用各种骨干网络作为编码器，证明了选择合适的骨干模型可以显著提升场景语义分割的性能。

<details>
  <summary>Details</summary>

**Motivation:** 本研究的动力在于探究如何通过深度学习技术提高自驾车的场景理解能力，特别是通过语义分割技术来更好地理解周围环境。

**Method:** 本研究的方法是使用BDD100k数据集来研究几种有效的模型，通过语义分割来探究场景理解，并使用了若干骨干模型作为编码器。

**Result:** 实验结果表明，选择合适的骨干模型对语义分割模型的性能有显著影响。

**Conclusion:** 通过对准确率、平均IoU以及损失函数的评估，研究证实提出的模型在语义分割上的性能得到了提升。

**Abstract:** In recent years, the concept of artificial intelligence (AI) has become a
prominent keyword because it is promising in solving complex tasks. The need
for human expertise in specific areas may no longer be needed because machines
have achieved successful results using artificial intelligence and can make the
right decisions in critical situations. This process is possible with the help
of deep learning (DL), one of the most popular artificial intelligence
technologies. One of the areas in which the use of DL is used is in the
development of self-driving cars, which is very effective and important. In
this work, we propose several efficient models to investigate scene
understanding through semantic segmentation. We use the BDD100k dataset to
investigate these models. Another contribution of this work is the usage of
several Backbones as encoders for models. The obtained results show that
choosing the appropriate backbone has a great effect on the performance of the
model for semantic segmentation. Better performance in semantic segmentation
allows us to understand better the scene and the environment around the agent.
In the end, we analyze and evaluate the proposed models in terms of accuracy,
mean IoU, and loss function, and the results show that these metrics are
improved.

</details>


### [43] [CLIPTTA: Robust Contrastive Vision-Language Test-Time Adaptation](https://arxiv.org/abs/2507.14312)
*Marc Lafon,Gustavo Adolfo Vargas Hakim,Clément Rambour,Christian Desrosier,Nicolas Thome*

Main category: cs.CV

> 本文提出了CLIPTTA，一种新的基于梯度的测试时刻适应方法，提高了视觉-语言模型在不同分布变化下的泛化能力，表现优于熵基方法，并在大量数据集上展示了更强的竞争性和稳定性。

<details>
  <summary>Details</summary>

**Motivation:** 视觉-语言模型（VLMs）如CLIP在零样本设置中表现强劲，但在分布变化时却往往无法很好地泛化。现有的基于熵最小化的测试时刻适应（TTA）方法与其预训练目标不一致，限制了适应性能并导致了一些失败模式。

**Method:** CLIPTTA采用了一种新的基于梯度的测试时刻适应方法，通过利用与CLIP预训练目标对齐的软对比损失来改进视觉-语言模型的适应性能，并且在开放集设置中使用异常对比曝光（OCE）损失来改进未分布数据（OOD）的检测能力。

**Result:** CLIPTTA在涵盖了75个不同分布偏移的数据集上测试，相对于熵基目标和其他最先进的测试时刻适应方法表现出了更好的性能和稳定性。

**Conclusion:** CLIPTTA证明了其在处理分布偏移问题上的有效性和适应性，尤其是在引入了OCE损失后的开放集适应情况下的OOD检测性能。

**Abstract:** Vision-language models (VLMs) like CLIP exhibit strong zero-shot capabilities
but often fail to generalize under distribution shifts. Test-time adaptation
(TTA) allows models to update at inference time without labeled data, typically
via entropy minimization. However, this objective is fundamentally misaligned
with the contrastive image-text training of VLMs, limiting adaptation
performance and introducing failure modes such as pseudo-label drift and class
collapse. We propose CLIPTTA, a new gradient-based TTA method for
vision-language models that leverages a soft contrastive loss aligned with
CLIP's pre-training objective. We provide a theoretical analysis of CLIPTTA's
gradients, showing how its batch-aware design mitigates the risk of collapse.
We further extend CLIPTTA to the open-set setting, where both in-distribution
(ID) and out-of-distribution (OOD) samples are encountered, using an Outlier
Contrastive Exposure (OCE) loss to improve OOD detection. Evaluated on 75
datasets spanning diverse distribution shifts, CLIPTTA consistently outperforms
entropy-based objectives and is highly competitive with state-of-the-art TTA
methods, outperforming them on a large number of datasets and exhibiting more
stable performance across diverse shifts.

</details>


### [44] [A Hidden Stumbling Block in Generalized Category Discovery: Distracted Attention](https://arxiv.org/abs/2507.14315)
*Qiyu Xu,Zhanxuan Hu,Yu Duan,Ercheng Pei,Yonghang Tai*

Main category: cs.CV

> This paper proposes a novel method, AF, to improve GCD by focusing the model's attention on informative tokens rather than task-irrelevant backgrounds, achieving a 15.4% performance improvement with minimal computational overhead.

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind this work is to address a hidden stumbling block in GCD: distracted attention. Existing methods often have their models focus on task-irrelevant areas, leading to inefficient feature extraction.

**Method:** The paper proposes an adaptive mechanism named Attention Focusing (AF) to improve Generalized Category Discovery (GCD). AF includes two components: TIME and TAP. TIME evaluates the importance of tokens at multiple scales, while TAP prunes non-informative tokens based on these evaluations.

**Result:** When the proposed AF mechanism is added to the existing GCD method SimGCD, the results show a performance improvement of up to 15.4%, demonstrating the effectiveness of AF in enhancing GCD.

**Conclusion:** The paper concludes that AF is a lightweight and effective addition to GCD methods, significantly improving classification performance by reducing the effect of task-irrelevant attention, without substantial additional computational costs.

**Abstract:** Generalized Category Discovery (GCD) aims to classify unlabeled data from
both known and unknown categories by leveraging knowledge from labeled known
categories. While existing methods have made notable progress, they often
overlook a hidden stumbling block in GCD: distracted attention. Specifically,
when processing unlabeled data, models tend to focus not only on key objects in
the image but also on task-irrelevant background regions, leading to suboptimal
feature extraction. To remove this stumbling block, we propose Attention
Focusing (AF), an adaptive mechanism designed to sharpen the model's focus by
pruning non-informative tokens. AF consists of two simple yet effective
components: Token Importance Measurement (TIME) and Token Adaptive Pruning
(TAP), working in a cascade. TIME quantifies token importance across multiple
scales, while TAP prunes non-informative tokens by utilizing the multi-scale
importance scores provided by TIME. AF is a lightweight, plug-and-play module
that integrates seamlessly into existing GCD methods with minimal computational
overhead. When incorporated into one prominent GCD method, SimGCD, AF achieves
up to 15.4% performance improvement over the baseline with minimal
computational overhead. The implementation code is provided in
https://github.com/Afleve/AFGCD.

</details>


### [45] [Hallucination Score: Towards Mitigating Hallucinations in Generative Image Super-Resolution](https://arxiv.org/abs/2507.14367)
*Weiming Ren,Raghav Goyal,Zhiming Hu,Tristan Ty Aumentado-Armstrong,Iqbal Mohomed,Alex Levinshtein*

Main category: cs.CV

> 本研究提出使用大型语言模型评估幻觉现象，并提出使用深层特征距离作为奖励函数来减少生成超级分辨率模型中的幻觉，提高模型的实际应用价值。

<details>
  <summary>Details</summary>

**Motivation:** 现有的生成超级分辨率（GSR）模型在感知图像质量方面表现优异，但生成的细节与低分辨率图象（LRI）或地面真实图象（GTI）在感知上往往不匹配，这种“幻觉”现象限制了GSR模型的实际部署。

**Method:** 通过利用多模态大型语言模型（MLLM）构建一个评估幻觉视觉元素的提示，生成‘幻觉分数’（HS），并提出使用某些深层特征距离作为可微奖励函数来对齐生成超级分辨率模型以减少幻觉现象。

**Result:** 研究发现，提出的幻觉分数（HS）与人类评估密切相关，并为之前用于超分辨率（SR）模型的图像度量提供了补充见解。此外，研究发现某些深层特征距离与HS有很强的相关性，可以用来减少幻觉现象。

**Conclusion:** 通过引入幻觉分数（HS）和使用深层特征距离作为可微奖励函数，本研究为减少生成超级分辨率模型中的幻觉现象提供了新的解决思路。

**Abstract:** Generative super-resolution (GSR) currently sets the state-of-the-art in
terms of perceptual image quality, overcoming the "regression-to-the-mean" blur
of prior non-generative models. However, from a human perspective, such models
do not fully conform to the optimal balance between quality and fidelity.
Instead, a different class of artifacts, in which generated details fail to
perceptually match the low resolution image (LRI) or ground-truth image (GTI),
is a critical but under studied issue in GSR, limiting its practical
deployments. In this work, we focus on measuring, analyzing, and mitigating
these artifacts (i.e., "hallucinations"). We observe that hallucinations are
not well-characterized with existing image metrics or quality models, as they
are orthogonal to both exact fidelity and no-reference quality. Instead, we
take advantage of a multimodal large language model (MLLM) by constructing a
prompt that assesses hallucinatory visual elements and generates a
"Hallucination Score" (HS). We find that our HS is closely aligned with human
evaluations, and also provides complementary insights to prior image metrics
used for super-resolution (SR) models. In addition, we find certain deep
feature distances have strong correlations with HS. We therefore propose to
align the GSR models by using such features as differentiable reward functions
to mitigate hallucinations.

</details>


### [46] [DUSTrack: Semi-automated point tracking in ultrasound videos](https://arxiv.org/abs/2507.14368)
*Praneeth Namburi,Roger Pallarès-López,Jessica Rosendorf,Duarte Folgado,Brian W. Anthony*

Main category: cs.CV

> DUSTrack是一种结合深度学习和光学流方法的半自动工具，用于解决B模式超声视频中组织跟踪的问题，能够在多个应用中准确跟踪任意点。

<details>
  <summary>Details</summary>

**Motivation:** 为了克服B模式超声图像中的组织跟踪障碍，提高跟踪质量和鲁棒性，尤其是在研究和临床环境中量化组织动态时。

**Method:** DUSTrack是一种半自动框架，结合了深度学习与光学流技术，旨在通过提供一个图形用户界面来简化训练数据的生成，并采用了新的光学流过滤技术来减少帧间噪声。

**Result:** Ultrasound技术在医学、生物力学和运动科学中具有重要意义，但B模式超声图像中的组织跟踪由于斑点噪声、对比度低和面外运动等问题仍面临挑战。本文介绍了一个半自动的框架DUSTrack，它结合了深度学习和光流技术，用于B模式超声视频中的任意点跟踪。DUSTrack提供了一个图形用户界面，可以简化高质量的训练数据生成并支持模型的多次迭代改进。该工具包还包括一种新颖的光流滤波技术，可以减少高频率的帧间噪声同时保留快速的组织运动。结果显示，DUSTrack在准确性上优于当前的无样本点跟踪器，并且与专门方法持平，表明其作为临床上和生物力学研究中的通用工具具有潜力。通过三个用例展示了DUSTrack的多功能性：心超声图中的心壁运动跟踪、取物任务中的肌肉变形分析以及踝关节跖屈时的肌束追踪。作为开源解决方案，DUSTrack提供了一个强大的框架，用于从超声视频中量化组织运动。

**Conclusion:** DUSTrack在准确性方面优于当前的无样本点跟踪器，并与专门方法持平，它的开源性质使其成为了量化组织运动的强大和灵活的框架，展示了其在多种应用中的潜力。

**Abstract:** Ultrasound technology enables safe, non-invasive imaging of dynamic tissue
behavior, making it a valuable tool in medicine, biomechanics, and sports
science. However, accurately tracking tissue motion in B-mode ultrasound
remains challenging due to speckle noise, low edge contrast, and out-of-plane
movement. These challenges complicate the task of tracking anatomical landmarks
over time, which is essential for quantifying tissue dynamics in many clinical
and research applications. This manuscript introduces DUSTrack (Deep learning
and optical flow-based toolkit for UltraSound Tracking), a semi-automated
framework for tracking arbitrary points in B-mode ultrasound videos. We combine
deep learning with optical flow to deliver high-quality and robust tracking
across diverse anatomical structures and motion patterns. The toolkit includes
a graphical user interface that streamlines the generation of high-quality
training data and supports iterative model refinement. It also implements a
novel optical-flow-based filtering technique that reduces high-frequency
frame-to-frame noise while preserving rapid tissue motion. DUSTrack
demonstrates superior accuracy compared to contemporary zero-shot point
trackers and performs on par with specialized methods, establishing its
potential as a general and foundational tool for clinical and biomechanical
research. We demonstrate DUSTrack's versatility through three use cases:
cardiac wall motion tracking in echocardiograms, muscle deformation analysis
during reaching tasks, and fascicle tracking during ankle plantarflexion. As an
open-source solution, DUSTrack offers a powerful, flexible framework for point
tracking to quantify tissue motion from ultrasound videos. DUSTrack is
available at https://github.com/praneethnamburi/DUSTrack.

</details>


### [47] [CRAFT: A Neuro-Symbolic Framework for Visual Functional Affordance Grounding](https://arxiv.org/abs/2507.14426)
*Zhou Chen,Joe Lin,Sathyanarayanan N. Aakur*

Main category: cs.CV

> CRAFT 是一个用于可解释的操作适用性基础的神经符号框架，它结合了常识和语言模型的信息以及视觉证据，以识别场景中能够执行特定操作的对象，并通过能量基础推理循环迭代优化预测，实现在不标记多对象环境中的高准确度和增强的可解释性。

<details>
  <summary>Details</summary>

**Motivation:** 研发 CRAFT 的动机在于解决场景理解中准确性和可解释性之间的平衡问题，现有方法常常缺乏透明性或准确性，无法在不标记的多对象场景中做出有效的操作适用性判断。

**Method:** CRAFT 使用基于能量的推理回路整合来自 ConceptNet 和语言模型的结构化常识，与来自 CLIP 的视觉证据相融合，以优化对于给定动作适用对象的预测。

**Result:** 在没有标签的情况下，CRAFT 提升了多对象场景中动作适用性预测的准确性和可解释性。

**Conclusion:** CRAFT 为理解复杂场景提供了一个更加健壮且值得信赖的方法，通过其透明和目标驱动的决策过程，为提炼符号与感知结构之间的联系打下了坚实的基础。

**Abstract:** We introduce CRAFT, a neuro-symbolic framework for interpretable affordance
grounding, which identifies the objects in a scene that enable a given action
(e.g., "cut"). CRAFT integrates structured commonsense priors from ConceptNet
and language models with visual evidence from CLIP, using an energy-based
reasoning loop to refine predictions iteratively. This process yields
transparent, goal-driven decisions to ground symbolic and perceptual
structures. Experiments in multi-object, label-free settings demonstrate that
CRAFT enhances accuracy while improving interpretability, providing a step
toward robust and trustworthy scene understanding.

</details>


### [48] [Adaptive 3D Gaussian Splatting Video Streaming](https://arxiv.org/abs/2507.14432)
*Han Gong,Qiyue Li,Zhi Liu,Hao Zhou,Peng Yuan Zhou,Zhu Li,Jie Li*

Main category: cs.CV

> 本文提出了一种3DGS体积视频流传输框架，该框架解决了由于其数据量大导致的传输挑战等问题。通过实验验证，新方法在多个方面优于现有的方法。

<details>
  <summary>Details</summary>

**Motivation:** 随着3D高斯点样技术的发展，尽管其大大提高了体积视频的表现质量，但同时也带来了由于数据量大和压缩传输复杂度高而产生的传输挑战。研究旨在解决这一问题。

**Method:** 本研究采用了一种基于高斯变形场的3DGS视频构建方法，结合混杂显著性分块和3DGS视频差异化质量建模，以实现高效的数据压缩和适应带宽变化，同时保证高传输质量。

**Result:** 实验验证表明，所提出的方法在视频质量、压缩效果和传输速率等方面优于现有方法。

**Conclusion:** 我们建立了一个完整的3DGS视频流系统，并证实了其传输性能。实验结果证明了我们方法在多个方面的优越性。

**Abstract:** The advent of 3D Gaussian splatting (3DGS) has significantly enhanced the
quality of volumetric video representation. Meanwhile, in contrast to
conventional volumetric video, 3DGS video poses significant challenges for
streaming due to its substantially larger data volume and the heightened
complexity involved in compression and transmission. To address these issues,
we introduce an innovative framework for 3DGS volumetric video streaming.
Specifically, we design a 3DGS video construction method based on the Gaussian
deformation field. By employing hybrid saliency tiling and differentiated
quality modeling of 3DGS video, we achieve efficient data compression and
adaptation to bandwidth fluctuations while ensuring high transmission quality.
Then we build a complete 3DGS video streaming system and validate the
transmission performance. Through experimental evaluation, our method
demonstrated superiority over existing approaches in various aspects, including
video quality, compression effectiveness, and transmission rate.

</details>


### [49] [IRGPT: Understanding Real-world Infrared Image with Bi-cross-modal Curriculum on Large-scale Benchmark](https://arxiv.org/abs/2507.14449)
*Zhe Cao,Jin Zhang,Ruiheng Zhang*

Main category: cs.CV

> 本研究提出了一种专门处理现实世界红外图像的多模态语言模型IRGPT，通过引入真实数据集以及特定迁移学习策略，解决了现有模型因依赖合成数据而无法充分捕捉红外图像特性的问题。

<details>
  <summary>Details</summary>

**Motivation:** 现实世界的红外图像对视觉-语言模型提出了独特挑战，因其缺乏与图像对应的真实文本数据以及特定领域的特性。现有方法依赖于通过风格转移生成的合成红外图像，限制了其捕捉红外模态独特特性的能力。

**Method:** 提出IRGPT，首个用于现实世界红外图像的多模态大规模语言模型，基于包含超过260K真实图像文本对的大型红外文本数据集（IR-TD）。IR-TD数据集包含真实的红外图像与精心编写的文本配对，初始草稿来自两个互补过程：由大语言模型生成的可见图像描述和基于标注的规则描述。此外，引入了一种考虑红外-可见域和红外-文本难度分数的双跨域课程迁移学习策略。

**Result:** 在9个基准任务（如识别、定域）中，与更大规模的模型相比，IRGPT也达到了最先进的性能。

**Conclusion:** IRGPT通过新的红外文本数据集IR-TD和迁移学习策略，解决了现有方法依赖合成红外图像带来的一些局限，提高了在现实世界红外图像上的性能。

**Abstract:** Real-world infrared imagery presents unique challenges for vision-language
models due to the scarcity of aligned text data and domain-specific
characteristics. Although existing methods have advanced the field, their
reliance on synthetic infrared images generated through style transfer from
visible images, which limits their ability to capture the unique
characteristics of the infrared modality. To address this, we propose IRGPT,
the first multi-modal large language model for real-world infrared images,
built upon a large-scale InfraRed-Text Dataset (IR-TD) comprising over 260K
authentic image-text pairs. The proposed IR-TD dataset contains real infrared
images paired with meticulously handcrafted texts, where the initial drafts
originated from two complementary processes: (1) LLM-generated descriptions of
visible images, and (2) rule-based descriptions of annotations. Furthermore, we
introduce a bi-cross-modal curriculum transfer learning strategy that
systematically transfers knowledge from visible to infrared domains by
considering the difficulty scores of both infrared-visible and infrared-text.
Evaluated on a benchmark of 9 tasks (e.g., recognition, grounding), IRGPT
achieves state-of-the-art performance even compared with larger-scale models.

</details>


### [50] [GPI-Net: Gestalt-Guided Parallel Interaction Network via Orthogonal Geometric Consistency for Robust Point Cloud Registration](https://arxiv.org/abs/2507.14452)
*Weikang Gu,Mingyue Han,Li Xue,Heng Dong,Changcai Yang,Riqing Chen,Lifang Wei*

Main category: cs.CV

> 本文提出了GPI-Net，一种新的基于Gestalt原则的网络，用于改善点云注册中的高质量对应关系识别。该网络通过处理局部和全局特征的冗余度以及不同的几何特征，实现更高效的特征匹配。

<details>
  <summary>Details</summary>

**Motivation:** 为了改善点云注册中高质量对应关系的识别，解决局部特征和全局特征融合的挑战，利用Gestalt原则来促进局部和全局信息的互补通信。

**Method:** 提出了一种新的Gestalt指导的并行交互网络（GPI-Net），通过正交几何一致性来优化信息的冗余度，利用Gestalt特征注意力（GFA）块捕捉几何特征，并设计了双路径多粒度并行交互聚合（DMG）块来促进不同粒度之间的信息交换。

**Result:** 实验结果显示，所提出的GPI-Net在各种试验任务中表现优于现有方法。

**Conclusion:** GPI-Net通过融合Gestalt原则优化了几何特征的对应关系识别，在点云注册任务中表现出色。

**Abstract:** The accurate identification of high-quality correspondences is a prerequisite
task in feature-based point cloud registration. However, it is extremely
challenging to handle the fusion of local and global features due to feature
redundancy and complex spatial relationships. Given that Gestalt principles
provide key advantages in analyzing local and global relationships, we propose
a novel Gestalt-guided Parallel Interaction Network via orthogonal geometric
consistency (GPI-Net) in this paper. It utilizes Gestalt principles to
facilitate complementary communication between local and global information.
Specifically, we introduce an orthogonal integration strategy to optimally
reduce redundant information and generate a more compact global structure for
high-quality correspondences. To capture geometric features in correspondences,
we leverage a Gestalt Feature Attention (GFA) block through a hybrid
utilization of self-attention and cross-attention mechanisms. Furthermore, to
facilitate the integration of local detail information into the global
structure, we design an innovative Dual-path Multi-Granularity parallel
interaction aggregation (DMG) block to promote information exchange across
different granularities. Extensive experiments on various challenging tasks
demonstrate the superior performance of our proposed GPI-Net in comparison to
existing methods. The code will be released at https://github.com/gwk/GPI-Net.

</details>


### [51] [Adaptive 3D Gaussian Splatting Video Streaming: Visual Saliency-Aware Tiling and Meta-Learning-Based Bitrate Adaptation](https://arxiv.org/abs/2507.14454)
*Han Gong,Qiyue Li,Jie Li,Zhi Liu*

Main category: cs.CV

> 本文提出了一套解决方案，包括自适应3DGS分块技术、质量评估框架和自适应比特率算法，解决了3DGS视频流技术中的几个关键挑战，实验结果表明这些方法优于现有的技术。

<details>
  <summary>Details</summary>

**Motivation:** 尽管3DGS流媒体技术因其能够提供沉浸式3D视频体验而成为研究热点，但该领域仍处于早期发展阶段，需要进一步研究一些基本挑战，如分块、质量评估和比特率适应。因此，本文针对这些挑战提出了综合解决方案。

**Method:** 本文提出了一个自适应的3D高斯点阵化分块技术，该技术利用了关注点分析并集成了空间和时间特征。每个分块都根据需求编码为具有特定变形场和多个质量层次的版本，以实现自适应选择。此外，本文还提出了一个全新的质量评估框架，用于评估3DGS表示在流媒体期间的空间域退化以及所生成2D渲染图像的质量。最后，开发了一种基于元学习的自适应比特率算法，特别针对3DGS视频流，以适应变化的网络条件。

**Result:** 广泛的实验表明，所提出的方法在各种网络条件下显著优于最先进的方法。

**Conclusion:** 本文通过提出的自适应3DGS分块技术、质量评估框架和基于元学习的自适应比特率算法，在3D视频流技术的挑战性问题上取得了重要进展，为未来研究提供了坚实的基础。

**Abstract:** 3D Gaussian splatting video (3DGS) streaming has recently emerged as a
research hotspot in both academia and industry, owing to its impressive ability
to deliver immersive 3D video experiences. However, research in this area is
still in its early stages, and several fundamental challenges, such as tiling,
quality assessment, and bitrate adaptation, require further investigation. In
this paper, we tackle these challenges by proposing a comprehensive set of
solutions. Specifically, we propose an adaptive 3DGS tiling technique guided by
saliency analysis, which integrates both spatial and temporal features. Each
tile is encoded into versions possessing dedicated deformation fields and
multiple quality levels for adaptive selection. We also introduce a novel
quality assessment framework for 3DGS video that jointly evaluates
spatial-domain degradation in 3DGS representations during streaming and the
quality of the resulting 2D rendered images. Additionally, we develop a
meta-learning-based adaptive bitrate algorithm specifically tailored for 3DGS
video streaming, achieving optimal performance across varying network
conditions. Extensive experiments demonstrate that our proposed approaches
significantly outperform state-of-the-art methods.

</details>


### [52] [GEMINUS: Dual-aware Global and Scene-Adaptive Mixture-of-Experts for End-to-End Autonomous Driving](https://arxiv.org/abs/2507.14456)
*Chi Wan,Yixin Cui,Jiatong Du,Shuo Yang,Yulong Bai,Yanjun Huang*

Main category: cs.CV

> 论文介绍GEMINUS框架，该框架通过结合全局专家和场景适应性专家群，提高了端到端自动驾驶在多种场景中的适应性和鲁棒性。

<details>
  <summary>Details</summary>

**Motivation:** 现有的单一模式规划方法难以学习多样化的驾驶技能，影响其在复杂多变的交通环境中的表现。因此，本论文旨在提出一种能更好处理多样场景的方案。

**Method:** 提出GEMINUS框架，这是一个专家混合的端到端自动驾驶系统，包含全局专家、场景自适应专家群和双感知路由器。全局专家在整个数据集上训练，场景自适应专家群在特定场景子集上训练，双感知路由器综合考虑场景特征和路由不确定性。

**Result:** GEMINUS在Bench2Drive闭环基准测试中优于现有方法，在驾驶得分和成功率方面达到最先进水平，即使使用只有一只摄像头的视觉输入。消融研究表明相较于单专家基线，该方法驾驶得分提升了7.67%，成功率提升了22.06%，多能力平均值提升了19.41%。

**Conclusion:** GEMINUS框架通过耦合全局专家和场景自适应专家群，实现了在多样场景下的自适应和鲁棒性表现。

**Abstract:** End-to-end autonomous driving requires adaptive and robust handling of
complex and diverse traffic environments. However, prevalent single-mode
planning methods attempt to learn an overall policy while struggling to acquire
diversified driving skills to handle diverse scenarios. Therefore, this paper
proposes GEMINUS, a Mixture-of-Experts end-to-end autonomous driving framework
featuring a Global Expert, a Scene-Adaptive Experts Group, and equipped with a
Dual-aware Router. Specifically, the Global Expert is trained on the overall
dataset, possessing robust performance. The Scene-Adaptive Experts are trained
on corresponding scene subsets, achieving adaptive performance. The Dual-aware
Router simultaneously considers scenario-level features and routing uncertainty
to dynamically activate expert modules. Through the effective coupling of the
Global Expert and the Scene-Adaptive Experts Group via the Dual-aware Router,
GEMINUS achieves adaptive and robust performance in diverse scenarios. GEMINUS
outperforms existing methods in the Bench2Drive closed-loop benchmark and
achieves state-of-the-art performance in Driving Score and Success Rate, even
with only monocular vision input. Furthermore, ablation studies demonstrate
significant improvements over the original single-expert baseline: 7.67% in
Driving Score, 22.06% in Success Rate, and 19.41% in MultiAbility-Mean. The
code will be available at https://github.com/newbrains1/GEMINUS.

</details>


### [53] [VisGuard: Securing Visualization Dissemination through Tamper-Resistant Data Retrieval](https://arxiv.org/abs/2507.14459)
*Huayuan Ye,Juntong Chen,Shenzhuo Zhang,Yipeng Zhang,Changbo Wang,Chenhui Li*

Main category: cs.CV

> _visGuard是一个抗篡改的框架，用于提高可视化图像中数据检索的可靠性与鲁棒性。_

<details>
  <summary>Details</summary>

**Motivation:** 许多现有方法在将元数据嵌入图像以促进VIDR时缺乏实用性，因为它们在应对在线传播过程中常见的图像篡改行为时较为脆弱，如裁剪和编辑。为了克服这个问题，我们提出了VisGuard。

**Method:** 我们提出了VisGuard，这是一个抗篡改的可视化图像数据检索（VIDR）框架，能够可靠地将元数据链接嵌入到可视化图像中。为了增强其鲁棒性，我们提出了一些技术，包括重复数据平铺、可逆信息广播以及基于锚点的裁剪定位方案。

**Result:** 实验显示，VisGuard在数据检索准确性、嵌入能力和对抗篡改和隐写分析的安全性方面表现出色，证明了它在促进和保护可视化传播及信息传达方面的能力。

**Conclusion:** 通过VisGuard，我们可以实现交互式图表的重建、篡改检测以及版权保护等多种应用。

**Abstract:** The dissemination of visualizations is primarily in the form of raster
images, which often results in the loss of critical information such as source
code, interactive features, and metadata. While previous methods have proposed
embedding metadata into images to facilitate Visualization Image Data Retrieval
(VIDR), most existing methods lack practicability since they are fragile to
common image tampering during online distribution such as cropping and editing.
To address this issue, we propose VisGuard, a tamper-resistant VIDR framework
that reliably embeds metadata link into visualization images. The embedded data
link remains recoverable even after substantial tampering upon images. We
propose several techniques to enhance robustness, including repetitive data
tiling, invertible information broadcasting, and an anchor-based scheme for
crop localization. VisGuard enables various applications, including interactive
chart reconstruction, tampering detection, and copyright protection. We conduct
comprehensive experiments on VisGuard's superior performance in data retrieval
accuracy, embedding capacity, and security against tampering and steganalysis,
demonstrating VisGuard's competence in facilitating and safeguarding
visualization dissemination and information conveyance.

</details>


### [54] [OptiCorNet: Optimizing Sequence-Based Context Correlation for Visual Place Recognition](https://arxiv.org/abs/2507.14477)
*Zhenyu Li,Tianyi Shang,Pengjie Xu,Ruirui Zhang,Fanchen Kong*

Main category: cs.CV

> 本文提出了OptiCorNet，一种新的序列建模框架，通过结合1D卷积编码器和差分时间算子来改进视觉地点识别，以应对视点和外观变化，该框架在多个公开基准上超越了现有基准。

<details>
  <summary>Details</summary>

**Motivation:** 动力在于解决视觉地点识别（VPR）在动态和感知别名环境中的挑战，这种环境对长期定位至关重要。现有的基于深度学习的解决方案主要集中在单帧嵌入上，忽视了图像序列中存在的时间连贯性。

**Method:** 本文提出了一种名为OptiCorNet的新序列建模框架，该框架将空间特征提取和时间差异整合到一个可微分的端到端可训练模块中。核心是结合轻量级的1D卷积编码器和可学习的差分时间算子（DSD），该算子可以联合捕获短期空间上下文和长距离时间转换。DSD模块通过固定权重差分核对序列间的方向差异进行建模，继而通过LSTM进行优化和可选的残差投影，生成紧凑且具有区分度的描述符，这些描述符能够有效地应对视点和外观变化。为了进一步增强类别间的可分性，本文引入了一种四重损失，它在每个批次中优化正面相似性和多负面差异。

**Result:** 实验结果表明，本文的方法在多个公开数据集上优于现有的最先进的方法。

**Conclusion:** 全面的评估表明，与现有的最先进技术相比，本文的方法在面对具有挑战性的季节变化和视点变化时表现更为出色。

**Abstract:** Visual Place Recognition (VPR) in dynamic and perceptually aliased
environments remains a fundamental challenge for long-term localization.
Existing deep learning-based solutions predominantly focus on single-frame
embeddings, neglecting the temporal coherence present in image sequences. This
paper presents OptiCorNet, a novel sequence modeling framework that unifies
spatial feature extraction and temporal differencing into a differentiable,
end-to-end trainable module. Central to our approach is a lightweight 1D
convolutional encoder combined with a learnable differential temporal operator,
termed Differentiable Sequence Delta (DSD), which jointly captures short-term
spatial context and long-range temporal transitions. The DSD module models
directional differences across sequences via a fixed-weight differencing
kernel, followed by an LSTM-based refinement and optional residual projection,
yielding compact, discriminative descriptors robust to viewpoint and appearance
shifts. To further enhance inter-class separability, we incorporate a
quadruplet loss that optimizes both positive alignment and multi-negative
divergence within each batch. Unlike prior VPR methods that treat temporal
aggregation as post-processing, OptiCorNet learns sequence-level embeddings
directly, enabling more effective end-to-end place recognition. Comprehensive
evaluations on multiple public benchmarks demonstrate that our approach
outperforms state-of-the-art baselines under challenging seasonal and viewpoint
variations.

</details>


### [55] [DFQ-ViT: Data-Free Quantization for Vision Transformers without Fine-tuning](https://arxiv.org/abs/2507.14481)
*Yujia Tong,Jingling Yuan,Tian Zhang,Jianquan Liu,Chuang Hu*

Main category: cs.CV

> a novel DFQ method for ViTs that enhances synthetic sample quality and aligns activation distributions, surpassing state-of-the-art methods and enabling efficient deployment on edge devices

<details>
  <summary>Details</summary>

**Motivation:** to enhance the quality of synthetic data for DFQ, align the activation distributions between quantized and full-precision ViTs, and improve the performance of quantized ViTs without the need for fine-tuning or real data

**Method:** synthesize samples in order of increasing difficulty and introduce an activation correction matrix to align the intermediate layer activations of the quantized model with those of the full-precision model

**Result:** DFQ-ViT achieves superior results over existing DFQ methods and comparable performance to those quantized through real data

**Conclusion:** the proposed method significantly improves the efficiency and performance of quantizing ViTs for deployment on devices with limited resources, aligning with the principles of Green Learning

**Abstract:** Data-Free Quantization (DFQ) enables the quantization of Vision Transformers
(ViTs) without requiring access to data, allowing for the deployment of ViTs on
devices with limited resources. In DFQ, the quantization model must be
calibrated using synthetic samples, making the quality of these synthetic
samples crucial. Existing methods fail to fully capture and balance the global
and local features within the samples, resulting in limited synthetic data
quality. Moreover, we have found that during inference, there is a significant
difference in the distributions of intermediate layer activations between the
quantized and full-precision models. These issues lead to a severe performance
degradation of the quantized model. To address these problems, we propose a
pipeline for Data-Free Quantization for Vision Transformers (DFQ-ViT).
Specifically, we synthesize samples in order of increasing difficulty,
effectively enhancing the quality of synthetic data. During the calibration and
inference stage, we introduce the activation correction matrix for the
quantized model to align the intermediate layer activations with those of the
full-precision model. Extensive experiments demonstrate that DFQ-ViT achieves
remarkable superiority over existing DFQ methods and its performance is on par
with models quantized through real data. For example, the performance of DeiT-T
with 3-bit weights quantization is 4.29% higher than the state-of-the-art. Our
method eliminates the need for fine-tuning, which not only reduces
computational overhead but also lowers the deployment barriers for edge
devices. This characteristic aligns with the principles of Green Learning by
improving energy efficiency and facilitating real-world applications in
resource-constrained environments.

</details>


### [56] [Benefit from Reference: Retrieval-Augmented Cross-modal Point Cloud Completion](https://arxiv.org/abs/2507.14485)
*Hongye Hou,Liu Zhan,Yang Yang*

Main category: cs.CV

> 本文提出了一种基于检索增强的点云补全框架，通过跨模态检索技术从相似参考样本中学习结构先验信息，以增强点云补全能力。实验显示该方法在生成精细点云和处理稀疏数据及未见类别方面具有显著效果。

<details>
  <summary>Details</summary>

**Motivation:** 现有的基于跨模态学习的方法在点云补全上的生成能力有限，尤其在面对缺乏典型结构特征的点云时。

**Method:** Structure

**Result:** {"tldr": "本文提出了一种基于检索增强的点云补全框架，通过跨模态检索技术从相似参考样本中学习结构先验信息，以增强点云补全能力。实验显示该方法在生成精细点云和处理稀疏数据及未见类别方面具有显著效果。", "motivation": "现有的基于跨模态学习的方法在点云补全上的生成能力有限，尤其在面对缺乏典型结构特征的点云时。", "method": "设计了结构共享特征编码器（SSFE）和渐进检索增强生成器（PRAG），并通过双通道控制门机制增强参考样本中的相关结构特征。", "result": "实验结果表明，该方法在生成精细的点云以及处理稀疏和未见类的点云上展示了良好的效果。", "conclusion": "提出的方法通过引入跨模态检索技术，不仅增强了点云补全的性能，还提升了模型在处理未见类别数据方面的泛化能力。"}

**Conclusion:** 提出的方法通过引入跨模态检索技术，不仅增强了点云补全的性能，还提升了模型在处理未见类别数据方面的泛化能力。

**Abstract:** Completing the whole 3D structure based on an incomplete point cloud is a
challenging task, particularly when the residual point cloud lacks typical
structural characteristics. Recent methods based on cross-modal learning
attempt to introduce instance images to aid the structure feature learning.
However, they still focus on each particular input class, limiting their
generation abilities. In this work, we propose a novel retrieval-augmented
point cloud completion framework. The core idea is to incorporate cross-modal
retrieval into completion task to learn structural prior information from
similar reference samples. Specifically, we design a Structural Shared Feature
Encoder (SSFE) to jointly extract cross-modal features and reconstruct
reference features as priors. Benefiting from a dual-channel control gate in
the encoder, relevant structural features in the reference sample are enhanced
and irrelevant information interference is suppressed. In addition, we propose
a Progressive Retrieval-Augmented Generator (PRAG) that employs a hierarchical
feature fusion mechanism to integrate reference prior information with input
features from global to local. Through extensive evaluations on multiple
datasets and real-world scenes, our method shows its effectiveness in
generating fine-grained point clouds, as well as its generalization capability
in handling sparse data and unseen categories.

</details>


### [57] [Efficient Whole Slide Pathology VQA via Token Compression](https://arxiv.org/abs/2507.14497)
*Weimin Lyu,Qingqiao Hu,Kehan Qi,Zhan Shi,Wentao Huang,Saumya Gupta,Chao Chen*

Main category: cs.CV

> 提出TCP-LLaVA，一种用于全幻灯片图像视觉问答任务的多模态大语言模型架构，通过压缩贴片令牌来减少资源消耗。

<details>
  <summary>Details</summary>

**Motivation:** 解决病理学中全幻灯片图像数据集计算资源消耗高这一使用传统MLLM方法难以解决的问题。

**Method:** Structure

**Result:** <tool_call>
tokenCompressionPathologyLLaVA
{"tldr": "Proposes TCP-LLaVA, a multimodal large language model architecture for whole-slide image visual question answering which reduces resource consumption by compressing patch tokens.", "motivation": "The paper aims to solve the problem of high computational resource requirement for large visual datasets like whole-slide images in pathology using traditional MLLM methods.", "method": "TCP-LLaVA uses a trainable set of compression tokens to aggregate information and feed into the language model, inspired by the [CLS] token in BERT, which significantly reduces the input length and computational cost.", "result": "TCP-LLaVA outperformed existing MLLM baselines in VQA accuracy on ten TCGA tumor subtypes tests while decreasing resource consumption.", "conclusion": "The research successfully demonstrates that the proposed TCP-LLaVA model is effective in performing VQA tasks for WSIs with reduced resource usage."}
</tool_call>

**Conclusion:** 研究成功证明，提出的TCP-LLaVA模型在减少资源使用的同时能够有效地完成WSIs的VQA任务。

**Abstract:** Whole-slide images (WSIs) in pathology can reach up to 10,000 x 10,000
pixels, posing significant challenges for multimodal large language model
(MLLM) due to long context length and high computational demands. Previous
methods typically focus on patch-level analysis or slide-level classification
using CLIP-based models with multi-instance learning, but they lack the
generative capabilities needed for visual question answering (VQA). More recent
MLLM-based approaches address VQA by feeding thousands of patch tokens directly
into the language model, which leads to excessive resource consumption. To
address these limitations, we propose Token Compression Pathology LLaVA
(TCP-LLaVA), the first MLLM architecture to perform WSI VQA via token
compression. TCP-LLaVA introduces a set of trainable compression tokens that
aggregate visual and textual information through a modality compression module,
inspired by the [CLS] token mechanism in BERT. Only the compressed tokens are
forwarded to the LLM for answer generation, significantly reducing input length
and computational cost. Experiments on ten TCGA tumor subtypes show that
TCP-LLaVA outperforms existing MLLM baselines in VQA accuracy while reducing
training resource consumption by a substantial margin.

</details>


### [58] [Motion Segmentation and Egomotion Estimation from Event-Based Normal Flow](https://arxiv.org/abs/2507.14500)
*Zhiyuan Hua,Dehao Yuan,Cornelia Fermüller*

Main category: cs.CV

> The paper presents a framework that uses event-based normal flow to achieve accurate motion segmentation and egomotion estimation, validated through experiments on the EVIMO2v2 dataset, showcasing advantages for robotics and navigation.

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to develop a robust framework that does not heavily rely on optical flow or explicit depth estimation, which are often computationally intensive. The approach is designed to take advantage of the unique characteristics of neuromorphic vision sensors.

**Method:** This paper proposes a framework for motion segmentation and egomotion estimation using event-based normal flow, which relies on sparse, high-temporal-resolution event data and geometric constraints. It includes steps such as event over-segmentation, residual analysis for object isolation, and hierarchical clustering for segmentation refinement based on motion similarity and temporal consistency.

**Result:** Experimental results on the EVIMO2v2 dataset show high accuracy in segmentation and translational motion estimation, without the need for full optical flow computation.

**Conclusion:** The method highlights significant improvements in accuracy, especially at object boundaries, and shows promise for real-time applications in robotics and navigation.

**Abstract:** This paper introduces a robust framework for motion segmentation and
egomotion estimation using event-based normal flow, tailored specifically for
neuromorphic vision sensors. In contrast to traditional methods that rely
heavily on optical flow or explicit depth estimation, our approach exploits the
sparse, high-temporal-resolution event data and incorporates geometric
constraints between normal flow, scene structure, and inertial measurements.
The proposed optimization-based pipeline iteratively performs event
over-segmentation, isolates independently moving objects via residual analysis,
and refines segmentations using hierarchical clustering informed by motion
similarity and temporal consistency. Experimental results on the EVIMO2v2
dataset validate that our method achieves accurate segmentation and
translational motion estimation without requiring full optical flow
computation. This approach demonstrates significant advantages at object
boundaries and offers considerable potential for scalable, real-time robotic
and navigation applications.

</details>


### [59] [Advances in Feed-Forward 3D Reconstruction and View Synthesis: A Survey](https://arxiv.org/abs/2507.14501)
*Jiahui Zhang,Yuelei Li,Anpei Chen,Muyu Xu,Kunhao Liu,Jianyuan Wang,Xiao-Xiao Long,Hanxue Liang,Zexiang Xu,Hao Su,Christian Theobalt,Christian Rupprecht,Andrea Vedaldi,Hanspeter Pfister,Shijian Lu,Fangneng Zhan*

Main category: cs.CV

>  a survey on feed-forward methods for 3D reconstruction and view synthesis, addressing the limitations of traditional methods and highlighting their applications and potential future research directions.

<details>
  <summary>Details</summary>

**Motivation:**  traditional methods rely on computationally intensive iterative optimization, which limits their applicability in real-world scenarios. Recent advances in deep learning have enabled faster and more generalizable approaches.

**Method:**  feed-forward techniques for 3D reconstruction and view synthesis, including point cloud, 3D Gaussian Splatting (3DGS), Neural Radiance Fields (NeRF), etc.

**Result:**  a comprehensive review of the feed-forward techniques for 3D reconstruction and view synthesis, including key tasks such as pose-free reconstruction, dynamic 3D reconstruction, and 3D-aware image and video synthesis, and the applications in digital humans, SLAM, robotics, etc.

**Conclusion:**  discusses open research challenges and promising directions, emphasizing the potential of feed-forward approaches to advance the state of the art in 3D vision.

**Abstract:** 3D reconstruction and view synthesis are foundational problems in computer
vision, graphics, and immersive technologies such as augmented reality (AR),
virtual reality (VR), and digital twins. Traditional methods rely on
computationally intensive iterative optimization in a complex chain, limiting
their applicability in real-world scenarios. Recent advances in feed-forward
approaches, driven by deep learning, have revolutionized this field by enabling
fast and generalizable 3D reconstruction and view synthesis. This survey offers
a comprehensive review of feed-forward techniques for 3D reconstruction and
view synthesis, with a taxonomy according to the underlying representation
architectures including point cloud, 3D Gaussian Splatting (3DGS), Neural
Radiance Fields (NeRF), etc. We examine key tasks such as pose-free
reconstruction, dynamic 3D reconstruction, and 3D-aware image and video
synthesis, highlighting their applications in digital humans, SLAM, robotics,
and beyond. In addition, we review commonly used datasets with detailed
statistics, along with evaluation protocols for various downstream tasks. We
conclude by discussing open research challenges and promising directions for
future work, emphasizing the potential of feed-forward approaches to advance
the state of the art in 3D vision.

</details>


### [60] [DCHM: Depth-Consistent Human Modeling for Multiview Detection](https://arxiv.org/abs/2507.14505)
*Jiahao Ma,Tianyu Wang,Miaomiao Liu,David Ahmedt-Aristizabal,Chuong Nguyen*

Main category: cs.CV

> The paper introduces Depth-Consistent Human Modeling (DCHM), an innovative framework for accurate and noise-free pedestrian localization, outperforming existing methods and setting a new benchmark in multiview pedestrian detection.

<details>
  <summary>Details</summary>

**Motivation:** The motivation for the research is to eliminate reliance on human-labeled annotations and accurately model humans, addressing the noise introduced in previous pedestrian detection approaches and their struggle with generalizing to diverse scenes.

**Method:** Depth-Consistent Human Modeling (DCHM) is introduced, which focuses on consistent depth estimation and multiview fusion in global coordinates using a pipeline with superpixel-wise Gaussian Splatting to manage multiview depth consistency in various challenging scenarios.

**Result:** Validations demonstrate that the proposed method significantly reduces noise during human modeling and outperforms state-of-the-art approaches.

**Conclusion:** DCHM is noted as a pioneering approach for reconstructing pedestrians and performing multiview segmentation effectively under challenging conditions.

**Abstract:** Multiview pedestrian detection typically involves two stages: human modeling
and pedestrian localization. Human modeling represents pedestrians in 3D space
by fusing multiview information, making its quality crucial for detection
accuracy. However, existing methods often introduce noise and have low
precision. While some approaches reduce noise by fitting on costly multiview 3D
annotations, they often struggle to generalize across diverse scenes. To
eliminate reliance on human-labeled annotations and accurately model humans, we
propose Depth-Consistent Human Modeling (DCHM), a framework designed for
consistent depth estimation and multiview fusion in global coordinates.
Specifically, our proposed pipeline with superpixel-wise Gaussian Splatting
achieves multiview depth consistency in sparse-view, large-scaled, and crowded
scenarios, producing precise point clouds for pedestrian localization.
Extensive validations demonstrate that our method significantly reduces noise
during human modeling, outperforming previous state-of-the-art baselines.
Additionally, to our knowledge, DCHM is the first to reconstruct pedestrians
and perform multiview segmentation in such a challenging setting. Code is
available on the \href{https://jiahao-ma.github.io/DCHM/}{project page}.

</details>


### [61] [ArtiMuse: Fine-Grained Image Aesthetics Assessment with Joint Scoring and Expert-Level Understanding](https://arxiv.org/abs/2507.14533)
*Shuo Cao,Nan Ma,Jiayang Li,Xiaohui Li,Lihao Shao,Kaiwen Zhu,Yu Zhou,Yuandong Pu,Jiarui Wu,Jiaquan Wang,Bo Qu,Wenhai Wang,Yu Qiao,Dajuin Yao,Yihao Liu*

Main category: cs.CV

> 本文提出ArtiMuse，一种融合评分与专家级理解能力的图像美学评估模型，以及ArtiMuse-10K数据集，旨在解决图像美学评估中的模态偏差和细粒度属性分解的不足。

<details>
  <summary>Details</summary>

**Motivation:** 随着教育应用、艺术创作和AI生成内容技术的快速发展，对全面的图像美学评估(IAA)方法的需求增多，现有的MLLM方法虽然在感知和泛化能力上优于传统方法，但仍存在模态偏差与缺乏细粒度属性分解的问题。

**Method:** 本文提出了ArtiMuse，一种具有联合评分和专家级理解能力的MLLM基础图像美学评估模型，以及ArtiMuse-10K，一个精心策划的包含10,000张图片的图像美学数据集。

**Result:** 提出的方法文内未具体给出详细的结果，但强调了模型和数据集的创新性和对领域推进的预期价值。

**Conclusion:** ArtiMuse模型和ArtiMuse-10K数据集的公开将促进图像美学评估领域的发展。

**Abstract:** The rapid advancement of educational applications, artistic creation, and
AI-generated content (AIGC) technologies has substantially increased practical
requirements for comprehensive Image Aesthetics Assessment (IAA), particularly
demanding methods capable of delivering both quantitative scoring and
professional understanding. Multimodal Large Language Model (MLLM)-based IAA
methods demonstrate stronger perceptual and generalization capabilities
compared to traditional approaches, yet they suffer from modality bias
(score-only or text-only) and lack fine-grained attribute decomposition,
thereby failing to support further aesthetic assessment. In this paper, we
present:(1) ArtiMuse, an innovative MLLM-based IAA model with Joint Scoring and
Expert-Level Understanding capabilities; (2) ArtiMuse-10K, the first
expert-curated image aesthetic dataset comprising 10,000 images spanning 5 main
categories and 15 subcategories, each annotated by professional experts with
8-dimensional attributes analysis and a holistic score. Both the model and
dataset will be made public to advance the field.

</details>


### [62] [Real Time Captioning of Sign Language Gestures in Video Meetings](https://arxiv.org/abs/2507.14543)
*Sharanya Mukherjee,Md Hishaam Akhtar,Kannadasan R*

Main category: cs.CV

> 本文提出了一种浏览器插件，用于自动将视频通话中的手语翻译成字幕，以促进聋哑人与普通人士之间的沟通。

<details>
  <summary>Details</summary>

**Motivation:** 鉴于全球疫情导致视频通话成为日常沟通方式，而聋哑人更倾向于在视频通话中使用手语而非打字，本文旨在通过技术手段消除聋哑人和其他人之间的沟通障碍。

**Method:** 本文提出了一种浏览器插件，该插件可以自动将手语翻译成视频通话中的字幕，以便所有参与者都能理解。该方法使用了包含超过2000个单词级ASL视频的大型数据集，这些视频由100多名手语者表演。

**Result:** 通过使用大型的数据集训练，该插件能够准确地将手语动作转化为文字字幕，实现了实时的沟通辅助功能。

**Conclusion:** 该浏览器插件能够实时将手语翻译成文字字幕，有助于促进聋哑人和其他人之间的有效沟通。

**Abstract:** It has always been a rather tough task to communicate with someone possessing
a hearing impairment. One of the most tested ways to establish such a
communication is through the use of sign based languages. However, not many
people are aware of the smaller intricacies involved with sign language. Sign
language recognition using computer vision aims at eliminating the
communication barrier between deaf-mute and ordinary people so that they can
properly communicate with others. Recently the pandemic has left the whole
world shaken up and has transformed the way we communicate. Video meetings have
become essential for everyone, even people with a hearing disability. In recent
studies, it has been found that people with hearing disabilities prefer to sign
over typing during these video calls. In this paper, we are proposing a browser
extension that will automatically translate sign language to subtitles for
everyone else in the video call. The Large-scale dataset which contains more
than 2000 Word-Level ASL videos, which were performed by over 100 signers will
be used.

</details>


### [63] [Multimodal AI for Gastrointestinal Diagnostics: Tackling VQA in MEDVQA-GI 2025](https://arxiv.org/abs/2507.14544)
*Sujata Gaihre,Amir Thapa Magar,Prasuna Pokharel,Laxmi Tiwari*

Main category: cs.CV

> 我们采用了Florence模型来处理胃肠内镜检查的VQA任务，通过领域特定的数据增强提高了模型的泛化能力，实验表明该方法在官方挑战指标上达到了准确的响应。

<details>
  <summary>Details</summary>

**Motivation:** 本文旨在描述我们对ImageCLEFmed MEDVQA 2025挑战中涉及胃肠内镜检查的视觉问题回答子任务的方法。我们的研究重点是通过现有技术为医学领域的VQA提供一个强健的基线，并探索大规模多模态模型在医疗VQA中的潜力。

**Method:** 我们采用了Florence模型作为视觉问题回答（VQA）管道的基础框架，该模型是一种大规模的多模态基础模型。我们结合了强大的视觉编码器和文本编码器来解析内镜图像并生成具有临床意义的答案。为了提高泛化能力，我们应用了特定领域的数据增强，这些增强保持了医学特征的同时增加了训练多样性。

**Result:** 在KASVIR数据集上的实验表明，对Florence进行微调可以产生在官方挑战指标上的准确响应。

**Conclusion:** 我们的研究结果证实了大规模多模态模型在医学VQA中的应用潜力，提供了一个用于未来解释性、鲁棒性和临床集成研究的强基线模型。代码已公开提供。

**Abstract:** This paper describes our approach to Subtask 1 of the ImageCLEFmed MEDVQA
2025 Challenge, which targets visual question answering (VQA) for
gastrointestinal endoscopy. We adopt the Florence model-a large-scale
multimodal foundation model-as the backbone of our VQA pipeline, pairing a
powerful vision encoder with a text encoder to interpret endoscopic images and
produce clinically relevant answers. To improve generalization, we apply
domain-specific augmentations that preserve medical features while increasing
training diversity. Experiments on the KASVIR dataset show that fine-tuning
Florence yields accurate responses on the official challenge metrics. Our
results highlight the potential of large multimodal models in medical VQA and
provide a strong baseline for future work on explainability, robustness, and
clinical integration. The code is publicly available at:
https://github.com/TiwariLaxuu/VQA-Florence.git

</details>


### [64] [Synthesizing Images on Perceptual Boundaries of ANNs for Uncovering Human Perceptual Variability on Facial Expressions](https://arxiv.org/abs/2507.14549)
*Haotian Deng,Chi Zhang,Chen Wei,Quanying Liu*

Main category: cs.CV

> 研究表明，ANN难以分类的情绪刺激物导致人类参与者有更大的不确定性，这表明ANN决策边界与人类情绪感知变异之间有系统性的关联。

<details>
  <summary>Details</summary>

**Motivation:** 本研究旨在调查高感知变异性的现象，特别是当个体观看相同的刺激物时却表现出明显的情绪分类差异。

**Method:** 通过引入一种新的感知边界采样方法来生成面部表情刺激物，这些刺激物位于ANN决策边界上，从而创建了varEmotion数据集。

**Result:** 这些让ANN困惑的刺激物也引发了人类参与者更高的感知不确定性，表明了情绪感知中的计算原则的共同性。

**Conclusion:** 研究结果建立了ANN决策边界与人类感知变异性的系统联系，为情感解释的个性化建模提供了新的见解。

**Abstract:** A fundamental challenge in affective cognitive science is to develop models
that accurately capture the relationship between external emotional stimuli and
human internal experiences. While ANNs have demonstrated remarkable accuracy in
facial expression recognition, their ability to model inter-individual
differences in human perception remains underexplored. This study investigates
the phenomenon of high perceptual variability-where individuals exhibit
significant differences in emotion categorization even when viewing the same
stimulus. Inspired by the similarity between ANNs and human perception, we
hypothesize that facial expression samples that are ambiguous for ANN
classifiers also elicit divergent perceptual judgments among human observers.
To examine this hypothesis, we introduce a novel perceptual boundary sampling
method to generate facial expression stimuli that lie along ANN decision
boundaries. These ambiguous samples form the basis of the varEmotion dataset,
constructed through large-scale human behavioral experiments. Our analysis
reveals that these ANN-confusing stimuli also provoke heightened perceptual
uncertainty in human participants, highlighting shared computational principles
in emotion perception. Finally, by fine-tuning ANN representations using
behavioral data, we achieve alignment between ANN predictions and both
group-level and individual-level human perceptual patterns. Our findings
establish a systematic link between ANN decision boundaries and human
perceptual variability, offering new insights into personalized modeling of
emotional interpretation.

</details>


### [65] [Clutter Detection and Removal by Multi-Objective Analysis for Photographic Guidance](https://arxiv.org/abs/2507.14553)
*Xiaoran Wu*

Main category: cs.CV

> 本文提出了一种相机引导系统，帮助用户识别并移除照片中的杂乱，从而改进摄影效果，由两种算法支撑，试验表明系统有效提高照片质量并节省时间。

<details>
  <summary>Details</summary>

**Motivation:** 照片中的杂乱分散了摄影师向观众传达意图情感或故事的注意力，这一现象促使我们开发了一种相机指导系统，旨在提供杂乱识别和去除的解决方案及指导。

**Method:** 我们开发了一种相机指南系统，该系统通过估计和可视化物体对照片整体美学和内容的贡献来帮助用户交互式地识别杂乱。系统提供了去除杂乱的建议以及一个计算去除杂乱对象的工具。该系统基于两个技术新颖点：一种结合美学评估的杂乱区分算法和一个基于生成对抗网络的迭代图像修复算法，用于重建高分辨率图像中被移除对象的缺失区域。

**Result:** 实验结果显示，系统不仅提供了准确的算法，还给用户提供了一个灵活的交互界面，使得用户能够快速而有效地识别并处理照片中的杂乱，改善拍摄效果。

**Conclusion:** 用户研究表明，该系统提供了灵活的界面和准确的算法，有助于用户更好地识别分散注意力的元素，并在更短的时间内拍摄出质量更高的照片。

**Abstract:** Clutter in photos is a distraction preventing photographers from conveying
the intended emotions or stories to the audience. Photography amateurs
frequently include clutter in their photos due to unconscious negligence or the
lack of experience in creating a decluttered, aesthetically appealing scene for
shooting. We are thus motivated to develop a camera guidance system that
provides solutions and guidance for clutter identification and removal. We
estimate and visualize the contribution of objects to the overall aesthetics
and content of a photo, based on which users can interactively identify
clutter. Suggestions on getting rid of clutter, as well as a tool that removes
cluttered objects computationally, are provided to guide users to deal with
different kinds of clutter and improve their photographic work. Two technical
novelties underpin interactions in our system: a clutter distinguishment
algorithm with aesthetics evaluations for objects and an iterative image
inpainting algorithm based on generative adversarial nets that reconstructs
missing regions of removed objects for high-resolution images. User studies
demonstrate that our system provides flexible interfaces and accurate
algorithms that allow users to better identify distractions and take higher
quality images within less time.

</details>


### [66] [Descrip3D: Enhancing Large Language Model-based 3D Scene Understanding with Object-Level Text Descriptions](https://arxiv.org/abs/2507.14555)
*Jintang Xue,Ganning Zhao,Jie-En Yao,Hong-En Chen,Yue Hu,Meida Chen,Suya You,C. -C. Jay Kuo*

Main category: cs.CV

> Error

<details>
  <summary>Details</summary>

**Motivation:** Error

**Method:** Error

**Result:** Error

**Conclusion:** Error

**Abstract:** Understanding 3D scenes goes beyond simply recognizing objects; it requires
reasoning about the spatial and semantic relationships between them. Current 3D
scene-language models often struggle with this relational understanding,
particularly when visual embeddings alone do not adequately convey the roles
and interactions of objects. In this paper, we introduce Descrip3D, a novel and
powerful framework that explicitly encodes the relationships between objects
using natural language. Unlike previous methods that rely only on 2D and 3D
embeddings, Descrip3D enhances each object with a textual description that
captures both its intrinsic attributes and contextual relationships. These
relational cues are incorporated into the model through a dual-level
integration: embedding fusion and prompt-level injection. This allows for
unified reasoning across various tasks such as grounding, captioning, and
question answering, all without the need for task-specific heads or additional
supervision. When evaluated on five benchmark datasets, including ScanRefer,
Multi3DRefer, ScanQA, SQA3D, and Scan2Cap, Descrip3D consistently outperforms
strong baseline models, demonstrating the effectiveness of language-guided
relational representation for understanding complex indoor scenes.

</details>


### [67] [LEAD: Exploring Logit Space Evolution for Model Selection](https://arxiv.org/abs/2507.14559)
*Zixuan Hu,Xiaotong Li,Shixiang Tang,Jun Liu,Yichun Hu,Ling-Yu Duan*

Main category: cs.CV

> 本文提出了LEAD方法，通过一个理论框架和常微分方程推导成功解决了现有方法在预测模型迁移能力方面的局限性，实验表明该方法具有优秀的性能和广泛的适用性。

<details>
  <summary>Details</summary>

**Motivation:** 预训练-微调范式的成功导致了用于视觉任务的预训练模型数量激增，这给选择最合适的预训练模型带来了挑战。而现有方法通常在特征空间中使用线性变换来建模微调动力学，这与微调目标不精确对齐，也无法捕捉到优化过程中的非线性特征。

**Method:** LEAD采用基于网络输出logits的finetuning对齐方法，提出了一种理论框架来建模优化过程，并推导出一个常微分方程（ODE）来描述向最终logit状态的非线性演化过程。此外，还设计了一种类感知分解方法来考虑不同类之间的演化动力的不同。

**Result:** 在24个监督学习和自监督预训练模型上进行了全面实验，覆盖了10个下游数据集，展示了出色的性能并证明了其在低数据场景下的广泛适用性。

**Conclusion:** 提出的方法提供了一个简洁的解决方案，有效地消除了优化差距，绕过了漫长的微调过程，证明了其高性能和广泛适用性。

**Abstract:** The remarkable success of pretrain-then-finetune paradigm has led to a
proliferation of available pre-trained models for vision tasks. This surge
presents a significant challenge in efficiently choosing the most suitable
pre-trained models for downstream tasks. The critical aspect of this challenge
lies in effectively predicting the model transferability by considering the
underlying fine-tuning dynamics. Existing methods often model fine-tuning
dynamics in feature space with linear transformations, which do not precisely
align with the fine-tuning objective and fail to grasp the essential
nonlinearity from optimization. To this end, we present LEAD, a
finetuning-aligned approach based on the network output of logits. LEAD
proposes a theoretical framework to model the optimization process and derives
an ordinary differential equation (ODE) to depict the nonlinear evolution
toward the final logit state. Additionally, we design a class-aware
decomposition method to consider the varying evolution dynamics across classes
and further ensure practical applicability. Integrating the closely aligned
optimization objective and nonlinear modeling capabilities derived from the
differential equation, our method offers a concise solution to effectively
bridge the optimization gap in a single step, bypassing the lengthy fine-tuning
process. The comprehensive experiments on 24 supervised and self-supervised
pre-trained models across 10 downstream datasets demonstrate impressive
performances and showcase its broad adaptability even in low-data scenarios.

</details>


### [68] [Benchmarking GANs, Diffusion Models, and Flow Matching for T1w-to-T2w MRI Translation](https://arxiv.org/abs/2507.14575)
*Andrea Moschetto,Lemuel Puglisi,Alec Sargood,Pierluigi Dell'Acqua,Francesco Guarnera,Sebastiano Battiato,Daniele Ravì*

Main category: cs.CV

> 本文展示了基于GAN的Pix2Pix模型在T1w到T2w二维MRI转换中的优越性能，并为未来的研究和实际部署提供了指导。

<details>
  <summary>Details</summary>

**Motivation:** 随着MRI扫描时间增加和成本上升，研究计算方法合成缺失的MRI对比度成为一种减少采集时间的同时保持诊断质量的方式。通过图像对图像（I2I）转换，能够实现这一目标。

**Method:** 本研究展示了生成模型，包括生成对抗网络（GANs）、扩散模型和流匹配（FM）技术，在T1加权到T2加权（T1w到T2w）二维MRI I2I转换中的基准。所有框架在三个可供公众访问的健康成年人MRI数据集上使用相似的设置进行了实现和评估。

**Result:** 定量和定性分析表明，基于GAN的Pix2Pix模型在结构保真度、图像质量和计算效率方面优于扩散和FM方法。同时证明，基于流模型倾向于在小数据集和简单任务上过拟合，可能需要更多的数据才能匹配或超越GAN的性能。

**Conclusion:** 研究结果为在MRI工作流中部署I2I转换技术提供了实际指导，并强调了未来在跨模式医学图像合成研究中的方向。开源代码和模型在给出的链接中可以访问。

**Abstract:** Magnetic Resonance Imaging (MRI) enables the acquisition of multiple image
contrasts, such as T1-weighted (T1w) and T2-weighted (T2w) scans, each offering
distinct diagnostic insights. However, acquiring all desired modalities
increases scan time and cost, motivating research into computational methods
for cross-modal synthesis. To address this, recent approaches aim to synthesize
missing MRI contrasts from those already acquired, reducing acquisition time
while preserving diagnostic quality. Image-to-image (I2I) translation provides
a promising framework for this task. In this paper, we present a comprehensive
benchmark of generative models$\unicode{x2013}$specifically, Generative
Adversarial Networks (GANs), diffusion models, and flow matching (FM)
techniques$\unicode{x2013}$for T1w-to-T2w 2D MRI I2I translation. All
frameworks are implemented with comparable settings and evaluated on three
publicly available MRI datasets of healthy adults. Our quantitative and
qualitative analyses show that the GAN-based Pix2Pix model outperforms
diffusion and FM-based methods in terms of structural fidelity, image quality,
and computational efficiency. Consistent with existing literature, these
results suggest that flow-based models are prone to overfitting on small
datasets and simpler tasks, and may require more data to match or surpass GAN
performance. These findings offer practical guidance for deploying I2I
translation techniques in real-world MRI workflows and highlight promising
directions for future research in cross-modal medical image synthesis. Code and
models are publicly available at
https://github.com/AndreaMoschetto/medical-I2I-benchmark.

</details>


### [69] [Performance comparison of medical image classification systems using TensorFlow Keras, PyTorch, and JAX](https://arxiv.org/abs/2507.14587)
*Merjem Bećirović,Amina Kurtović,Nordin Smajlović,Medina Kapo,Amila Akagić*

Main category: cs.CV

> 通过比较TensorFlow、PyTorch和JAX在血细胞图像分类中的性能，突出JAX和PyTorch的有效性。

<details>
  <summary>Details</summary>

**Motivation:** 尽管深度学习在提高血液图像分析的准确性和效率方面显示出巨大潜力，但具体深度学习框架的性能分析仍有不足。

**Method:** 比较了TensorFlow(Keras), PyTorch和JAX这三种流行的深度学习框架在处理BloodMNIST数据集中的血细胞图像分类任务中的表现。

**Result:** 研究揭示了不同框架之间的性能差异，这种差异受到图像分辨率和框架特有优化的影响。JAX和PyTorch的分类准确率与现有基准相当，表现出它们在医疗图像分类中的高效性。

**Conclusion:** 该研究表明，在处理医学图像分类任务时，JAX和PyTorch具有高效的性能，可与当前基准相匹敌。

**Abstract:** Medical imaging plays a vital role in early disease diagnosis and monitoring.
Specifically, blood microscopy offers valuable insights into blood cell
morphology and the detection of hematological disorders. In recent years, deep
learning-based automated classification systems have demonstrated high
potential in enhancing the accuracy and efficiency of blood image analysis.
However, a detailed performance analysis of specific deep learning frameworks
appears to be lacking. This paper compares the performance of three popular
deep learning frameworks, TensorFlow with Keras, PyTorch, and JAX, in
classifying blood cell images from the publicly available BloodMNIST dataset.
The study primarily focuses on inference time differences, but also
classification performance for different image sizes. The results reveal
variations in performance across frameworks, influenced by factors such as
image resolution and framework-specific optimizations. Classification accuracy
for JAX and PyTorch was comparable to current benchmarks, showcasing the
efficiency of these frameworks for medical image classification.

</details>


### [70] [DiSCO-3D : Discovering and segmenting Sub-Concepts from Open-vocabulary queries in NeRF](https://arxiv.org/abs/2507.14596)
*Doriand Petit,Steve Bourgeois,Vincent Gay-Bellile,Florian Chabot,Loïc Barthe*

Main category: cs.CV

> DiSCO-3D 是一种用于3D开放词汇子概念发现的新方法，它结合了无监督分段和弱开放词汇指导，取得了在开放词汇和无监督分段边缘情况下的最好结果。

<details>
  <summary>Details</summary>

**Motivation:** 传统的3D分割方法要么适应特定任务目标，要么适应场景内容，但不能两者兼顾。DiSCO-3D 的动机是提出一种新方法，能同时适应场景内容和用户查询。

**Method:** DiSCO-3D 基于神经场表示，通过结合无监督分割和弱开放词汇指导来实现3D开放词汇子概念发现。

**Result:** 评估显示，DiSCO-3D 在开放词汇子概念发现中表现有效，并在开放词汇和无监督分割的边缘情况下的表现均处于最高水平。

**Conclusion:** DiSCO-3D 作为解决3D开放词汇子概念发现问题的第一种方法，证明了它在处理同时适应场景内容和用户查询的任务时的有效性和优越性。

**Abstract:** 3D semantic segmentation provides high-level scene understanding for
applications in robotics, autonomous systems, \textit{etc}. Traditional methods
adapt exclusively to either task-specific goals (open-vocabulary segmentation)
or scene content (unsupervised semantic segmentation). We propose DiSCO-3D, the
first method addressing the broader problem of 3D Open-Vocabulary Sub-concepts
Discovery, which aims to provide a 3D semantic segmentation that adapts to both
the scene and user queries. We build DiSCO-3D on Neural Fields representations,
combining unsupervised segmentation with weak open-vocabulary guidance. Our
evaluations demonstrate that DiSCO-3D achieves effective performance in
Open-Vocabulary Sub-concepts Discovery and exhibits state-of-the-art results in
the edge cases of both open-vocabulary and unsupervised segmentation.

</details>


### [71] [Exp-Graph: How Connections Learn Facial Attributes in Graph-based Expression Recognition](https://arxiv.org/abs/2507.14608)
*Nandani Sharma,Dinesh Singh*

Main category: cs.CV

> 本论文提出了一种图模型框架Exp-Graph，通过利用视觉变换器编码面部属性的局部外观，并使用图卷积网络来捕捉结构依赖，以提高面部表情识别的准确性。

<details>
  <summary>Details</summary>

**Motivation:** 面部表情识别在诸如面部动画、视频监控、情感计算、医学分析等应用中具有重要意义。由于面部属性结构会随面部表情变化，因此将结构信息融入面部属性识别中对表情识别是必要的，本论文即为此动机展开研究。

**Method:** 本论文提出了一种名为Exp-Graph的新框架，利用图模型来表示面部属性之间的结构关系，用于面部表情识别。面部特征点作为图的顶点，边则基于特征点的接近程度及局部外观的相似性来确定，此相似性通过视觉变换器进行编码。此外，还使用了图卷积网络来捕捉并整合这些结构依赖，以提升面部表情识别的准确性。

**Result:** 在Oulu-CASIA、eNTERFACE05和AFEW三个基准数据集上，Exp-Graph模型取得了98.09%、79.01%和56.39%的识别准确率，显示了其在不同环境中的泛化能力。

**Conclusion:** 本论文的研究结果证明，Exp-Graph框架利用面部属性图的表示能够学习到具有高度表达力的语义表示，并且在多个基准数据集上展示了有效性和泛化能力。

**Abstract:** Facial expression recognition is crucial for human-computer interaction
applications such as face animation, video surveillance, affective computing,
medical analysis, etc. Since the structure of facial attributes varies with
facial expressions, incorporating structural information into facial attributes
is essential for facial expression recognition. In this paper, we propose
Exp-Graph, a novel framework designed to represent the structural relationships
among facial attributes using graph-based modeling for facial expression
recognition. For facial attributes graph representation, facial landmarks are
used as the graph's vertices. At the same time, the edges are determined based
on the proximity of the facial landmark and the similarity of the local
appearance of the facial attributes encoded using the vision transformer.
Additionally, graph convolutional networks are utilized to capture and
integrate these structural dependencies into the encoding of facial attributes,
thereby enhancing the accuracy of expression recognition. Thus, Exp-Graph
learns from the facial attribute graphs highly expressive semantic
representations. On the other hand, the vision transformer and graph
convolutional blocks help the framework exploit the local and global
dependencies among the facial attributes that are essential for the recognition
of facial expressions. We conducted comprehensive evaluations of the proposed
Exp-Graph model on three benchmark datasets: Oulu-CASIA, eNTERFACE05, and AFEW.
The model achieved recognition accuracies of 98.09\%, 79.01\%, and 56.39\%,
respectively. These results indicate that Exp-Graph maintains strong
generalization capabilities across both controlled laboratory settings and
real-world, unconstrained environments, underscoring its effectiveness for
practical facial expression recognition applications.

</details>


### [72] [Depthwise-Dilated Convolutional Adapters for Medical Object Tracking and Segmentation Using the Segment Anything Model 2](https://arxiv.org/abs/2507.14613)
*Guoping Xu,Christopher Kabat,You Zhang*

Main category: cs.CV

> 本文介绍了一种用于医学视频分割和跟踪的高效框架DD-SAM2，它基于SAM2并使用Depthwise-Dilated Adapter进行改进。该方法在两个数据集上获得优秀的评估结果。

<details>
  <summary>Details</summary>

**Motivation:** 当前医学图像分割技术受模态特定设计的限制，难以适应动态的医学成像场景。现有的基于SAM2的模型需要大规模数据集进行再训练或迁移学习，这会带来高计算成本和灾难性遗忘的风险。

**Method:** 本文提出了DD-SAM2，这是一种高效的SAM2适应框架，它引入了一种称为Depthwise-Dilated Adapter (DD-Adapter) 的组件，以实现多尺度特征提取，同时保持较少的参数量开销。这使得DD-SAM2能够利用SAM2的流式记忆机制，对医学视频序列中的目标进行跟踪和分割。

**Result:** DD-SAM2在评估数据集TrackRad2025和EchoNet-Dynamic上的Dice分数分别达到了0.93和0.97，显著优于现有的方法。

**Conclusion:** 本文的工作首次系统地探索了基于适配器的SAM2医疗视频分割和跟踪细化。实现了优秀的分割效果，同时保持了参数效率，为未来的研究提出了新的方法和发展方向。

**Abstract:** Recent advances in medical image segmentation have been driven by deep
learning; however, most existing methods remain limited by modality-specific
designs and exhibit poor adaptability to dynamic medical imaging scenarios. The
Segment Anything Model 2 (SAM2) and its related variants, which introduce a
streaming memory mechanism for real-time video segmentation, present new
opportunities for prompt-based, generalizable solutions. Nevertheless, adapting
these models to medical video scenarios typically requires large-scale datasets
for retraining or transfer learning, leading to high computational costs and
the risk of catastrophic forgetting. To address these challenges, we propose
DD-SAM2, an efficient adaptation framework for SAM2 that incorporates a
Depthwise-Dilated Adapter (DD-Adapter) to enhance multi-scale feature
extraction with minimal parameter overhead. This design enables effective
fine-tuning of SAM2 on medical videos with limited training data. Unlike
existing adapter-based methods focused solely on static images, DD-SAM2 fully
exploits SAM2's streaming memory for medical video object tracking and
segmentation. Comprehensive evaluations on TrackRad2025 (tumor segmentation)
and EchoNet-Dynamic (left ventricle tracking) datasets demonstrate superior
performance, achieving Dice scores of 0.93 and 0.97, respectively. To the best
of our knowledge, this work provides an initial attempt at systematically
exploring adapter-based SAM2 fine-tuning for medical video segmentation and
tracking. Code, datasets, and models will be publicly available at
https://github.com/apple1986/DD-SAM2.

</details>


### [73] [BusterX++: Towards Unified Cross-Modal AI-Generated Content Detection and Explanation with MLLM](https://arxiv.org/abs/2507.14632)
*Haiquan Wen,Tianxiao Li,Zhenglin Huang,Yiwei He,Guangliang Cheng*

Main category: cs.CV

> 提出BusterX++框架用于合成媒体的跨模态检测，通过多种方法优化提升性能，并建立GenBuster++基准用于评估。

<details>
  <summary>Details</summary>

**Motivation:** 随着生成式AI在图像和视频合成能力的提高，带来了通过复杂假内容制造误导信息的风险。传统方法局限于单模态设计，对结合多种媒体格式的合成内容识别效果不佳，因此提出了BusterX++框架。

**Method:** 介绍了一种名为BusterX++的新框架，专用于合成媒体的跨模态检测和解释，并使用了先进的强化学习(RL)后训练策略来消除冷启动问题。BusterX++通过多阶段训练、思考奖励和混合推理实现了稳定和显著的性能改进。

**Result:** 实验显示了BusterX++框架在检测合成媒体方面的有效性与泛化能力。

**Conclusion:** BusterX++在跨模态合成媒体检测中展示了显著的性能提升，同时提出了GenBuster++基准，用于进行全面的评估。

**Abstract:** Recent advances in generative AI have dramatically improved image and video
synthesis capabilities, significantly increasing the risk of misinformation
through sophisticated fake content. In response, detection methods have evolved
from traditional approaches to multimodal large language models (MLLMs),
offering enhanced transparency and interpretability in identifying synthetic
media. However, current detection systems remain fundamentally limited by their
single-modality design. These approaches analyze images or videos separately,
making them ineffective against synthetic content that combines multiple media
formats. To address these challenges, we introduce \textbf{BusterX++}, a novel
framework designed specifically for cross-modal detection and explanation of
synthetic media. Our approach incorporates an advanced reinforcement learning
(RL) post-training strategy that eliminates cold-start. Through Multi-stage
Training, Thinking Reward, and Hybrid Reasoning, BusterX++ achieves stable and
substantial performance improvements. To enable comprehensive evaluation, we
also present \textbf{GenBuster++}, a cross-modal benchmark leveraging
state-of-the-art image and video generation techniques. This benchmark
comprises 4,000 images and video clips, meticulously curated by human experts
using a novel filtering methodology to ensure high quality, diversity, and
real-world applicability. Extensive experiments demonstrate the effectiveness
and generalizability of our approach.

</details>


### [74] [Multispectral State-Space Feature Fusion: Bridging Shared and Cross-Parametric Interactions for Object Detection](https://arxiv.org/abs/2507.14643)
*Jifeng Shen,Haibo Zhan,Shaohua Dong,Xin Zuo,Wankou Yang,Haibin Ling*

Main category: cs.CV

> 提出了基于状态空间模型的MS2Fusion框架，通过双路径参数交互机制有效地解决了多光谱特征融合在目标检测中的局限性，并在多个基准测试中显示出优越性。

<details>
  <summary>Details</summary>

**Motivation:** 现有的多光谱特征融合方法在对象检测中存在偏好局部互补特征而忽视跨模式共享语义以及感受野大小和计算复杂度之间的权衡问题。

**Method:** 设计了一个基于状态空间模型的新的双路径参数交互机制，其中第一个跨参数交互分支通过跨注意力挖掘互补信息，第二个共享参数分支通过跨模式对齐获取共享语义特征。

**Result:** 实验结果表明，MS2Fusion在像FLIR、M3FD和LLVIP等多个主流基准测试中显著优于其他最先进的多光谱目标检测方法。

**Conclusion:** MS2Fusion框架不仅在目标检测中表现出色，还展示了其在RGB-T语义分割和RGBT显著物体检测中的广泛适用性和通用性。

**Abstract:** Modern multispectral feature fusion for object detection faces two critical
limitations: (1) Excessive preference for local complementary features over
cross-modal shared semantics adversely affects generalization performance; and
(2) The trade-off between the receptive field size and computational complexity
present critical bottlenecks for scalable feature modeling. Addressing these
issues, a novel Multispectral State-Space Feature Fusion framework, dubbed
MS2Fusion, is proposed based on the state space model (SSM), achieving
efficient and effective fusion through a dual-path parametric interaction
mechanism. More specifically, the first cross-parameter interaction branch
inherits the advantage of cross-attention in mining complementary information
with cross-modal hidden state decoding in SSM. The second shared-parameter
branch explores cross-modal alignment with joint embedding to obtain
cross-modal similar semantic features and structures through parameter sharing
in SSM. Finally, these two paths are jointly optimized with SSM for fusing
multispectral features in a unified framework, allowing our MS2Fusion to enjoy
both functional complementarity and shared semantic space. In our extensive
experiments on mainstream benchmarks including FLIR, M3FD and LLVIP, our
MS2Fusion significantly outperforms other state-of-the-art multispectral object
detection methods, evidencing its superiority. Moreover, MS2Fusion is general
and applicable to other multispectral perception tasks. We show that, even
without specific design, MS2Fusion achieves state-of-the-art results on RGB-T
semantic segmentation and RGBT salient object detection, showing its
generality. The source code will be available at
https://github.com/61s61min/MS2Fusion.git.

</details>


### [75] [AI-Powered Precision in Sport Taekwondo: Enhancing Fairness, Speed, and Trust in Competition (FST.ai)](https://arxiv.org/abs/2507.14657)
*Keivan Shariatmadar,Ahmad Osman*

Main category: cs.CV

> FST.ai is an AI-powered framework for enhancing sports officiating, initially targeted at Sport Taekwondo to improve fairness and trust through automated, real-time decision-making.

<details>
  <summary>Details</summary>

**Motivation:** To enhance officiating in Sport Taekwondo and address the issues of latency, subjectivity, and inconsistent enforcement in traditional manual systems.

**Method:** Leveraging computer vision, deep learning, and edge inference, the system automates the identification and classification of key actions.

**Result:** Significantly reducing decision time from minutes to seconds while improving consistency and transparency in the complex task of real-time head kick detection and scoring.

**Conclusion:** By demonstrating robustness and scalability with head kick scoring in Taekwondo, FST.ai showcases its potential to transform officiating standards across various sports disciplines.

**Abstract:** The integration of Artificial Intelligence (AI) into sports officiating
represents a paradigm shift in how decisions are made in competitive
environments. Traditional manual systems, even when supported by Instant Video
Replay (IVR), often suffer from latency, subjectivity, and inconsistent
enforcement, undermining fairness and athlete trust. This paper introduces
FST.ai, a novel AI-powered framework designed to enhance officiating in Sport
Taekwondo, particularly focusing on the complex task of real-time head kick
detection and scoring. Leveraging computer vision, deep learning, and edge
inference, the system automates the identification and classification of key
actions, significantly reducing decision time from minutes to seconds while
improving consistency and transparency. Importantly, the methodology is not
limited to Taekwondo. The underlying framework -- based on pose estimation,
motion classification, and impact analysis -- can be adapted to a wide range of
sports requiring action detection, such as judo, karate, fencing, or even team
sports like football and basketball, where foul recognition or performance
tracking is critical. By addressing one of Taekwondo's most challenging
scenarios -- head kick scoring -- we demonstrate the robustness, scalability,
and sport-agnostic potential of FST.ai to transform officiating standards
across multiple disciplines.

</details>


### [76] [Artificial Intelligence in the Food Industry: Food Waste Estimation based on Computer Vision, a Brief Case Study in a University Dining Hall](https://arxiv.org/abs/2507.14662)
*Shayan Rokhva,Babak Teimourpour*

Main category: cs.CV

> 研究提出了一种基于计算机视觉的方法来量化餐盘级别的食物浪费，使用RGB图像中的语义分割技术，并通过实际测试展示了该方法的有效性和实时性，为减少机构内食物浪费提供了可能的解决方案。

<details>
  <summary>Details</summary>

**Motivation:** 量化机构餐饮环境中餐后食物浪费对支持数据驱动的可持续性策略是必不可少的。这项研究的动机在于开发一种成本效益高的计算机视觉框架，以解决这一问题。

**Method:** 本研究提出了一种基于计算机视觉的框架，利用RGB图像的语义分割来估算餐盘级别的食物浪费。为了实现这一目标，训练了四个全监督模型（U-Net、U-Net++及其轻量级变体）。这些模型使用了受限动态反频率损失和AdamW优化器，并通过多个评估指标（包括像素准确率、Dice、IoU和一个特定任务定制的分布像素协议（DPA））进行了评估。

**Result:** 所有模型都表现出了令人满意的效果，对于每种食物类型，至少有一个模型的DPA达到了或接近90%，表明像素级别的比例估算与实际情况高度一致。轻量级模型由于参数更少，提供了更快的推理速度，可以在NVIDIA T4 GPU上实现实时吞吐量。进一步分析表明，对于干燥且硬的成分（如米饭和炸薯条）的分割表现更好，而对于更复杂、碎片化或粘稠的菜肴（如炖菜），则在餐后表现较差。

**Conclusion:** 尽管存在一些限制，如依赖二维成像、食物品种限制和手动数据收集，这项研究提出的方法是一个开创性的解决方案，代表了一种可以连续监控食物消费的可扩展、非接触式方法。研究为大型餐饮服务环境中的自动实时废物跟踪系统奠定了基础，并为管理人员和政策制定者提供了减少机构食物浪费的实际见解和可行的未来方向。

**Abstract:** Quantifying post-consumer food waste in institutional dining settings is
essential for supporting data-driven sustainability strategies. This study
presents a cost-effective computer vision framework that estimates plate-level
food waste by utilizing semantic segmentation of RGB images taken before and
after meal consumption across five Iranian dishes. Four fully supervised models
(U-Net, U-Net++, and their lightweight variants) were trained using a capped
dynamic inverse-frequency loss and AdamW optimizer, then evaluated through a
comprehensive set of metrics, including Pixel Accuracy, Dice, IoU, and a
custom-defined Distributional Pixel Agreement (DPA) metric tailored to the
task. All models achieved satisfying performance, and for each food type, at
least one model approached or surpassed 90% DPA, demonstrating strong alignment
in pixel-wise proportion estimates. Lighter models with reduced parameter
counts offered faster inference, achieving real-time throughput on an NVIDIA T4
GPU. Further analysis showed superior segmentation performance for dry and more
rigid components (e.g., rice and fries), while more complex, fragmented, or
viscous dishes, such as stews, showed reduced performance, specifically
post-consumption. Despite limitations such as reliance on 2D imaging,
constrained food variety, and manual data collection, the proposed framework is
pioneering and represents a scalable, contactless solution for continuous
monitoring of food consumption. This research lays foundational groundwork for
automated, real-time waste tracking systems in large-scale food service
environments and offers actionable insights and outlines feasible future
directions for dining hall management and policymakers aiming to reduce
institutional food waste.

</details>


### [77] [Gene-DML: Dual-Pathway Multi-Level Discrimination for Gene Expression Prediction from Histopathology Images](https://arxiv.org/abs/2507.14670)
*Yaxuan Song,Jianan Fan,Hang Chang,Weidong Cai*

Main category: cs.CV

> 本研究提出Gene-DML框架来增强组织学图像和基因表达谱之间的跨模态对齐，通过多尺度和跨层次的判别路径，提高了基因表达预测的性能和泛化能力。

<details>
  <summary>Details</summary>

**Motivation:** 该研究旨在解决现有方法在跨模态表示对齐上的不足，特别是组织学图像和基因表达谱之间多个表示层面的不足，进而提高预测性能。通过准确预测基因表达，可以从组织病理图像中获得可扩展且非侵入性的分子分析方法，对精准医疗和计算病理学具有重要意义。

**Method:** 本论文提出了Gene-DML框架，通过Dual-pathway Multi-Level discrimination来构建潜在空间，以增强组织学图像和基因表达谱之间的对应关系。多尺度实例级判别路径通过在局部、邻域和全局级别对组织学表示进行对齐，捕获了尺度感知的形态转录关系。同时，跨层次实例组别判别路径强化了个体（图像/基因）实例和模态跨跃（基因/图像）组之间的结构一致性，从而增强了跨模态对齐。

**Result:** 实验结果表明，Gene-DML在公共空间转录组学数据集上的基因表达预测达到了最先进的性能水平。

**Conclusion:** Gene-DML框架通过联合建模细粒度和结构层次的判别性，学习了稳健的跨模态表示，从而提高了基因表达预测的准确性。

**Abstract:** Accurately predicting gene expression from histopathology images offers a
scalable and non-invasive approach to molecular profiling, with significant
implications for precision medicine and computational pathology. However,
existing methods often underutilize the cross-modal representation alignment
between histopathology images and gene expression profiles across multiple
representational levels, thereby limiting their prediction performance. To
address this, we propose Gene-DML, a unified framework that structures latent
space through Dual-pathway Multi-Level discrimination to enhance correspondence
between morphological and transcriptional modalities. The multi-scale
instance-level discrimination pathway aligns hierarchical histopathology
representations extracted at local, neighbor, and global levels with gene
expression profiles, capturing scale-aware morphological-transcriptional
relationships. In parallel, the cross-level instance-group discrimination
pathway enforces structural consistency between individual (image/gene)
instances and modality-crossed (gene/image, respectively) groups, strengthening
the alignment across modalities. By jointly modelling fine-grained and
structural-level discrimination, Gene-DML is able to learn robust cross-modal
representations, enhancing both predictive accuracy and generalization across
diverse biological contexts. Extensive experiments on public spatial
transcriptomics datasets demonstrate that Gene-DML achieves state-of-the-art
performance in gene expression prediction. The code and checkpoints will be
released soon.

</details>


### [78] [Docopilot: Improving Multimodal Models for Document-Level Understanding](https://arxiv.org/abs/2507.14675)
*Yuchen Duan,Zhe Chen,Yusong Hu,Weiyun Wang,Shenglong Ye,Botian Shi,Lewei Lu,Qibin Hou,Tong Lu,Hongsheng Li,Jifeng Dai,Wenhai Wang*

Main category: cs.CV

> 研究提出了一套高质量的多模态文档理解解决方案，包括Doc-750K数据集和Docopilot模型，解决了现有模型在文档级理解方面的不足。

<details>
  <summary>Details</summary>

**Motivation:** 尽管多模态大型语言模型在多模态理解方面取得了显著进展，但它们在复杂、多页文档理解上的表现仍欠佳，主要因为缺少高质量的文档级数据集。现有RAG方法虽部分解决问题，但存在片段化检索上下文、多阶段错误累积和额外检索时间成本等问题。

**Method:** 本研究提出了一个高质量的文档级数据集Doc-750K，用以支持多模态文档的深入理解。该数据集包含了多样的文档结构、广泛的跨页依赖关系，以及源自原始文档的实际问答对。同时，基于此数据集开发了一个原生的多模态模型Docopilot，该模型能准确处理文档级依赖关系，无需依赖检索增强生成(RAG)方法。

**Result:** 实验表明，Docopilot在文档理解任务和多轮交互中实现了更高的连贯性、准确性和效率，为文档级多模态理解设定了新的底线。

**Conclusion:** 本研究通过Doc-750K数据集和Docopilot模型，为解决文档级多模态理解问题提供了有效的方案。

**Abstract:** Despite significant progress in multimodal large language models (MLLMs),
their performance on complex, multi-page document comprehension remains
inadequate, largely due to the lack of high-quality, document-level datasets.
While current retrieval-augmented generation (RAG) methods offer partial
solutions, they suffer from issues, such as fragmented retrieval contexts,
multi-stage error accumulation, and extra time costs of retrieval. In this
work, we present a high-quality document-level dataset, Doc-750K, designed to
support in-depth understanding of multimodal documents. This dataset includes
diverse document structures, extensive cross-page dependencies, and real
question-answer pairs derived from the original documents. Building on the
dataset, we develop a native multimodal model, Docopilot, which can accurately
handle document-level dependencies without relying on RAG. Experiments
demonstrate that Docopilot achieves superior coherence, accuracy, and
efficiency in document understanding tasks and multi-turn interactions, setting
a new baseline for document-level multimodal understanding. Data, code, and
models are released at https://github.com/OpenGVLab/Docopilot

</details>


### [79] [WSI-Agents: A Collaborative Multi-Agent System for Multi-Modal Whole Slide Image Analysis](https://arxiv.org/abs/2507.14680)
*Xinheng Lyu,Yuci Liang,Wenting Chen,Meidan Ding,Jiaqi Yang,Guolin Huang,Daokun Zhang,Xiangjian He,Linlin Shen*

Main category: cs.CV

> 本研究提出了一种名为WSI-Agents的协作多智能体系统，用于多模态的全玻片影像分析。该系统整合了专用功能智能体，结合了任务分配模块、验证机制和总结模块，实现在多种病理任务中的高精度和多功能性。实验结果表明，WSI-Agents在多模态全玻片影像基准测试中优于现有的MLLM和医疗智能体框架。

<details>
  <summary>Details</summary>

**Motivation:** 现有的多模态大规模语言模型在分析全玻片影像时，表现往往不如专门针对特定任务的模型，并且，在病理学特定领域的协作多智能体系统的潜力还未得到充分探索。

**Method:** WSI-Agents系统融合了专用功能智能体，并配备了任务分配、验证和总结三大功能模块，以此增强特定任务的准确性和多功能性。

**Result:** 实验显示，WSI-Agents在多种病理任务中，实现了优于现有全玻片影像多模态大规模语言模型和医疗智能体框架的表现。

**Conclusion:** WSI-Agents系统通过协作多智能体的概念，实现了全玻片影像的多模态分析，对于提升数字病理学的分析能力和准确性具有重要意义。

**Abstract:** Whole slide images (WSIs) are vital in digital pathology, enabling gigapixel
tissue analysis across various pathological tasks. While recent advancements in
multi-modal large language models (MLLMs) allow multi-task WSI analysis through
natural language, they often underperform compared to task-specific models.
Collaborative multi-agent systems have emerged as a promising solution to
balance versatility and accuracy in healthcare, yet their potential remains
underexplored in pathology-specific domains. To address these issues, we
propose WSI-Agents, a novel collaborative multi-agent system for multi-modal
WSI analysis. WSI-Agents integrates specialized functional agents with robust
task allocation and verification mechanisms to enhance both task-specific
accuracy and multi-task versatility through three components: (1) a task
allocation module assigning tasks to expert agents using a model zoo of patch
and WSI level MLLMs, (2) a verification mechanism ensuring accuracy through
internal consistency checks and external validation using pathology knowledge
bases and domain-specific models, and (3) a summary module synthesizing the
final summary with visual interpretation maps. Extensive experiments on
multi-modal WSI benchmarks show WSI-Agents's superiority to current WSI MLLMs
and medical agent frameworks across diverse tasks.

</details>


### [80] [From Semantics, Scene to Instance-awareness: Distilling Foundation Model for Open-vocabulary Situation Recognition](https://arxiv.org/abs/2507.14686)
*Chen Cai,Tianyi Liu,Jianjun Gao,Wenyang Liu,Kejun Wu,Ruoyu Wang,Yi Wang,Soo Chin Liew*

Main category: cs.CV

> Error

<details>
  <summary>Details</summary>

**Motivation:** Error

**Method:** Error

**Result:** Error

**Conclusion:** Error

**Abstract:** Recent Multimodal Large Language Models (MLLMs) exhibit strong zero-shot
abilities but struggle with complex Grounded Situation Recognition (GSR) and
are resource-intensive for edge device deployment. Meanwhile, conventional GSR
models often lack generalization ability, falling short in recognizing unseen
and rare situations. In this paper, we exploit transferring knowledge from a
teacher MLLM to a small GSR model to enhance its generalization and zero-shot
abilities, thereby introducing the task of Open-vocabulary Grounded Situation
Recognition (Ov-GSR). To achieve this, we propose Multimodal Interactive Prompt
Distillation (MIPD), a novel framework that distills enriched multimodal
knowledge from the foundation model, enabling the student Ov-GSR model to
recognize unseen situations and be better aware of rare situations.
Specifically, the MIPD framework first leverages the LLM-based Judgmental
Rationales Generator (JRG) to construct positive and negative glimpse and gaze
rationales enriched with contextual semantic information. The proposed
scene-aware and instance-perception prompts are then introduced to align
rationales with visual information from the MLLM teacher via the
Negative-Guided Multimodal Prompting Alignment (NMPA) module, effectively
capturing holistic and perceptual multimodal knowledge. Finally, the aligned
multimodal knowledge is distilled into the student Ov-GSR model, providing a
stronger foundation for generalization that enhances situation understanding,
bridges the gap between seen and unseen scenarios, and mitigates prediction
bias in rare cases. We evaluate MIPD on the refined Ov-SWiG dataset, achieving
superior performance on seen, rare, and unseen situations, and further
demonstrate improved unseen detection on the HICO-DET dataset.

</details>


### [81] [GTPBD: A Fine-Grained Global Terraced Parcel and Boundary Dataset](https://arxiv.org/abs/2507.14697)
*Zhiwei Zhang,Zi Ye,Yibin Wen,Shuai Yuan,Haohuan Fu,Jianxi Huang,Juepeng Zheng*

Main category: cs.CV

> 本文介绍了全球梯田地块及边界数据集GTPBD，该数据集包含了超过200,000个复杂的梯田地块，适合用于语义分割、边界检测、梯田地块提取和无监督领域自适应等多种任务，并为这些任务设置了基准测试。

<details>
  <summary>Details</summary>

**Motivation:** 为了填补现有农业地块提取研究中对复杂梯田地块数据不足的空白，本研究旨在提供一个全新的高精度全球梯田数据集。

**Method:** 构建了全球梯田地块数据集GTPBD，包含高分辨率图像，具有像素级边界标签、掩码标签和地块标签。

**Result:** GTPBD涵盖了七个地理区域，适合四种不同任务，并为此设置了基准测试，结果展示了数据集的独特挑战与适用性。

**Conclusion:** GTPBD数据集为细粒度农业地貌分析和跨场景知识迁移提供了基础工具，填补了梯田遥感研究中的关键空白。

**Abstract:** Agricultural parcels serve as basic units for conducting agricultural
practices and applications, which is vital for land ownership registration,
food security assessment, soil erosion monitoring, etc. However, existing
agriculture parcel extraction studies only focus on mid-resolution mapping or
regular plain farmlands while lacking representation of complex terraced
terrains due to the demands of precision agriculture.In this paper, we
introduce a more fine-grained terraced parcel dataset named GTPBD (Global
Terraced Parcel and Boundary Dataset), which is the first fine-grained dataset
covering major worldwide terraced regions with more than 200,000 complex
terraced parcels with manual annotation. GTPBD comprises 47,537 high-resolution
images with three-level labels, including pixel-level boundary labels, mask
labels, and parcel labels. It covers seven major geographic zones in China and
transcontinental climatic regions around the world.Compared to the existing
datasets, the GTPBD dataset brings considerable challenges due to the: (1)
terrain diversity; (2) complex and irregular parcel objects; and (3) multiple
domain styles. Our proposed GTPBD dataset is suitable for four different tasks,
including semantic segmentation, edge detection, terraced parcel extraction,
and unsupervised domain adaptation (UDA) tasks.Accordingly, we benchmark the
GTPBD dataset on eight semantic segmentation methods, four edge extraction
methods, three parcel extraction methods, and five UDA methods, along with a
multi-dimensional evaluation framework integrating pixel-level and object-level
metrics. GTPBD fills a critical gap in terraced remote sensing research,
providing a basic infrastructure for fine-grained agricultural terrain analysis
and cross-scenario knowledge transfer.

</details>


### [82] [MultiRetNet: A Multimodal Vision Model and Deferral System for Staging Diabetic Retinopathy](https://arxiv.org/abs/2507.14738)
*Jeannie She,Katie Spivakovsky*

Main category: cs.CV

> 我们提出了一种叫做MultiRetNet的新系统，它结合了视网膜成像、经济因素和共病概况以提高诊断准确性，并利用对比学习减少医疗差异，特别是在低收入社区中。这有助于提高早期检出率和医疗服务公平。

<details>
  <summary>Details</summary>

**Motivation:** 糖尿病视网膜病变（DR）是全球可预防失明的主要原因之一，影响着超过1亿的人。在美国，低收入社区的个体由于缺乏筛查而面临更高的风险，病情会在诊断前发展到晚期。我们提出了一种新系统来提高诊断准确性，并解决未充分服务的人群的健康差异问题。

**Method:** 我们提出了MultiRetNet，这是一种新颖的流水线，结合了视网膜成像、社会经济因素以及共病概况来提高糖尿病视网膜病变（DR）分期的准确性。我们实验了三种多模态融合方法，发现通过全连接层的融合是迄今为止最灵活的方法。此外，我们生成了对抗的、低质量的图像，并利用对比学习训练了一种分诊系统，指导模型识别出需要医生审查的样本。

**Result:** 研究中的方法通过维持在次优图像上的诊断准确性并整合关键健康数据，我们的系统可以提高早期检测率。这有助于减少医疗成本，提高早期检出率，并解决医疗服务获得的不平等问题，促进医疗服务公平。

**Conclusion:** 通过结合社会经济因素和共病概况，MultiRetNet是一种创新性工具，可以提高DR的检出率，并减少在未充分服务人群中的医疗差异。

**Abstract:** Diabetic retinopathy (DR) is a leading cause of preventable blindness,
affecting over 100 million people worldwide. In the United States, individuals
from lower-income communities face a higher risk of progressing to advanced
stages before diagnosis, largely due to limited access to screening. Comorbid
conditions further accelerate disease progression. We propose MultiRetNet, a
novel pipeline combining retinal imaging, socioeconomic factors, and
comorbidity profiles to improve DR staging accuracy, integrated with a clinical
deferral system for a clinical human-in-the-loop implementation. We experiment
with three multimodal fusion methods and identify fusion through a fully
connected layer as the most versatile methodology. We synthesize adversarial,
low-quality images and use contrastive learning to train the deferral system,
guiding the model to identify out-of-distribution samples that warrant
clinician review. By maintaining diagnostic accuracy on suboptimal images and
integrating critical health data, our system can improve early detection,
particularly in underserved populations where advanced DR is often first
identified. This approach may reduce healthcare costs, increase early detection
rates, and address disparities in access to care, promoting healthcare equity.

</details>


### [83] [InterAct-Video: Reasoning-Rich Video QA for Urban Traffic](https://arxiv.org/abs/2507.14743)
*Joseph Raj Vishal,Rutuja Patil,Manas Srinivas Gowda,Katha Naik,Yezhou Yang,Bharatesh Chakravarthi*

Main category: cs.CV

> 本文介绍了InterAct VideoQA数据集，用于交通监控任务，旨在改进VideoQA模型，特别是在处理复杂交通场景时的性能。

<details>
  <summary>Details</summary>

**Motivation:** 视频问答（VideoQA）模型在交通监控方面有显著进展，但现有的VideoQA模型在处理真实世界复杂交通场景时遇到了困难。该论文的动机是解决这些挑战。

**Method:** 本研究提出了InterAct VideoQA数据集，旨在为交通监控任务提供基准并改进VideoQA模型。该数据集包含了8小时的真实世界交通视频片段，每段10秒，总共有超过25000个问题回答对，涵盖时空动态、车辆互动、事故检测等关键交通属性。

**Result:** 对现有的最佳VideoQA模型进行了评估，并展示了在复杂交通场景中的细粒度时空依赖推理挑战。通过在InterAct VideoQA数据集上的微调，得到了显著的性能改进。

**Conclusion:** 实验表明，使用现有最佳VideoQA模型在InterAct VideoQA数据集上训练可以显著提高性能，强调了领域特定数据集对于VideoQA研究的重要性，该数据集公开提供以支持未来研究。

**Abstract:** Traffic monitoring is crucial for urban mobility, road safety, and
intelligent transportation systems (ITS). Deep learning has advanced
video-based traffic monitoring through video question answering (VideoQA)
models, enabling structured insight extraction from traffic videos. However,
existing VideoQA models struggle with the complexity of real-world traffic
scenes, where multiple concurrent events unfold across spatiotemporal
dimensions. To address these challenges, this paper introduces \textbf{InterAct
VideoQA}, a curated dataset designed to benchmark and enhance VideoQA models
for traffic monitoring tasks. The InterAct VideoQA dataset comprises 8 hours of
real-world traffic footage collected from diverse intersections, segmented into
10-second video clips, with over 25,000 question-answer (QA) pairs covering
spatiotemporal dynamics, vehicle interactions, incident detection, and other
critical traffic attributes. State-of-the-art VideoQA models are evaluated on
InterAct VideoQA, exposing challenges in reasoning over fine-grained
spatiotemporal dependencies within complex traffic scenarios. Additionally,
fine-tuning these models on InterAct VideoQA yields notable performance
improvements, demonstrating the necessity of domain-specific datasets for
VideoQA. InterAct VideoQA is publicly available as a benchmark dataset to
facilitate future research in real-world deployable VideoQA models for
intelligent transportation systems. GitHub Repo:
https://github.com/joe-rabbit/InterAct_VideoQA

</details>
