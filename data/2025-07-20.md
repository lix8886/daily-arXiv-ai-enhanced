<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 26]
- [cs.CV](#cs.CV) [Total: 22]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Modeling Open-World Cognition as On-Demand Synthesis of Probabilistic Models](https://arxiv.org/abs/2507.12547)
*Lionel Wong,Katherine M. Collins,Lance Ying,Cedegao E. Zhang,Adrian Weller,Tobias Gersternberg,Timothy O'Donnell,Alexander K. Lew,Jacob D. Andreas,Joshua B. Tenenbaum,Tyler Brooke-Wilson*

Main category: cs.CL

> 提出一种结合分布式和符号表征的MSA架构，用于处理开放性推理任务，展示了优于传统语言模型的方法。

<details>
  <summary>Details</summary>

**Motivation:** 探讨人们如何在面对新情境时，综合广泛的背景知识和相关信息并进行连贯的推理。

**Method:** 作者提出了一个名为`模型合成架构`（MSA）的计算实现方法，该方法使用语言模型来实现全局相关性检索和模型合成，并使用概率程序来实现定制的、连贯的世界模型。

**Result:** 在Model Olympics数据集上，MSA方法在直接生成和链式思考生成模式下，都比仅使用语言模型的基准方法更好地捕捉了人类的判断。

**Conclusion:** MSA可以以模仿人类处理全局相关变量能力的方式实现，为理解和复制人类在开放性领域中的推理提供了路径。

**Abstract:** When faced with novel situations, people are able to marshal relevant
considerations from a wide range of background knowledge and put these to use
in inferences and predictions. What permits us to draw in globally relevant
information and reason over it coherently? Here, we explore the hypothesis that
people use a combination of distributed and symbolic representations to
construct bespoke mental models tailored to novel situations. We propose a
computational implementation of this idea -- a ``Model Synthesis Architecture''
(MSA) -- using language models to implement global relevance-based retrieval
and model synthesis and probabilistic programs to implement bespoke, coherent
world models. We evaluate our MSA as a model of human judgments on a novel
reasoning dataset. The dataset -- built around a `Model Olympics` domain of
sports vignettes -- tests models' capacity for human-like, open-ended reasoning
by requiring (i) judgments about novel causal structures described in language;
(ii) drawing on large bodies of background knowledge; and (iii) doing both in
light of observations that introduce arbitrary novel variables. Our MSA
approach captures human judgments better than language model-only baselines,
under both direct and chain-of-thought generations from the LM that supports
model synthesis. These results suggest that MSAs can be implemented in a way
that mirrors people's ability to deliver locally coherent reasoning over
globally relevant variables, offering a path to understanding and replicating
human reasoning in open-ended domains.

</details>


### [2] [Is This Just Fantasy? Language Model Representations Reflect Human Judgments of Event Plausibility](https://arxiv.org/abs/2507.12553)
*Michael A. Lepori,Jennifer Hu,Ishita Dasgupta,Roma Patel,Thomas Serre,Ellie Pavlick*

Main category: cs.CL

> 研究中通过识别模态差向量揭示了语言模型在模态分类中比以往认为的更为可靠的能力，并且这种向量的分析为理解人类的模态分类行为提供新的视角。

<details>
  <summary>Details</summary>

**Motivation:** 探讨语言模型在模态分类中的表现，纠正此前关于语言模型无法可靠地进行模态分类的说法。

**Method:** 通过分析线性表示来区分语言模型中的模态类别，这些线性表示被称为模态差向量。

**Result:** 发现模态差向量可以提供比之前研究更可靠的模态分类判断，并且随着模型变得更加专业，这些矢量在一致性顺序中出现。

**Conclusion:** 模态差向量不仅有助于理解语言模型的模态分类能力，还能帮助解释人类如何区分模态类别。

**Abstract:** Language models (LMs) are used for a diverse range of tasks, from question
answering to writing fantastical stories. In order to reliably accomplish these
tasks, LMs must be able to discern the modal category of a sentence (i.e.,
whether it describes something that is possible, impossible, completely
nonsensical, etc.). However, recent studies have called into question the
ability of LMs to categorize sentences according to modality (Michaelov et al.,
2025; Kauf et al., 2023). In this work, we identify linear representations that
discriminate between modal categories within a variety of LMs, or modal
difference vectors. Analysis of modal difference vectors reveals that LMs have
access to more reliable modal categorization judgments than previously
reported. Furthermore, we find that modal difference vectors emerge in a
consistent order as models become more competent (i.e., through training steps,
layers, and parameter count). Notably, we find that modal difference vectors
identified within LM activations can be used to model fine-grained human
categorization behavior. This potentially provides a novel view into how human
participants distinguish between modal categories, which we explore by
correlating projections along modal difference vectors with human participants'
ratings of interpretable features. In summary, we derive new insights into LM
modal categorization using techniques from mechanistic interpretability, with
the potential to inform our understanding of modal categorization in humans.

</details>


### [3] [The first open machine translation system for the Chechen language](https://arxiv.org/abs/2507.12672)
*Abu-Viskhan A. Umishov,Vladislav A. Grigorian*

Main category: cs.CL

> 本文介绍了首个多语言翻译开源模型，专门用于车臣语与俄语之间的翻译，同时也评估了在NLLB-200模型上进行新语言微调的能力。

<details>
  <summary>Details</summary>

**Motivation:** 本研究旨在为脆弱的车臣语言提供开源翻译模型和数据集，同时也探索在已有的大型语言模型上进行新语言微调的方法。

**Method:** 我们介绍了首个用于车臣语与俄语之间翻译的开源模型及所收集的训练和评估数据集。我们探讨了将新语言纳入大型语言模型系统NLLB-200进行多语言翻译微调的能力。

**Result:** 评测结果显示，模型的BLEU和ChrF++得分分别为8.34/34.69（俄语到车臣语）和20.89/44.55（车臣语到俄语）。

**Conclusion:** 此次发布不仅包括了翻译模型，还包括了平行词汇、短语和句子语料库，以及适应车臣语的多语言句子编码器。

**Abstract:** We introduce the first open-source model for translation between the
vulnerable Chechen language and Russian, and the dataset collected to train and
evaluate it. We explore fine-tuning capabilities for including a new language
into a large language model system for multilingual translation NLLB-200. The
BLEU / ChrF++ scores for our model are 8.34 / 34.69 and 20.89 / 44.55 for
translation from Russian to Chechen and reverse direction, respectively. The
release of the translation models is accompanied by the distribution of
parallel words, phrases and sentences corpora and multilingual sentence encoder
adapted to the Chechen language.

</details>


### [4] [Improving Drug Identification in Overdose Death Surveillance using Large Language Models](https://arxiv.org/abs/2507.12679)
*Arthur J. Funnell,Panayiotis Petousis,Fabrice Harel-Canada,Ruby Romero,Alex A. T. Bui,Adam Koncsol,Hritika Chaturvedi,Chelsea Shover,David Goodman-Meza*

Main category: cs.CL

> 研究评估了多种NLP模型对药物相关死亡证书文本的分类能力，发现微调后的BioClinicalBERT表现最佳，提供了一种高精度的过量服药死亡分类方法。

<details>
  <summary>Details</summary>

**Motivation:** 提升与药物有关的死亡事件监测的及时性和准确性，解决因为手动将数据编码到ICD-10分类而导致的延迟和信息丢失问题。

**Method:** 使用了多种自然语言处理方法来分类特定药物涉及情况，包括传统的单标签和多标签分类器，以及微调后的BioClinicalBERT和Qwen 3等大语言模型。

**Result:** 微调后的BioClinicalBERT模型在内部测试集上的宏观F1分数达到0.998，在外部验证集上也达到了0.966的宏观F1分数。

**Conclusion:** NLP模型，尤其是微调后的BioClinicalBERT，能提供一种精准且可扩展的方案，用于分类药物过量死亡，从而加速公共卫生监测流程。

**Abstract:** The rising rate of drug-related deaths in the United States, largely driven
by fentanyl, requires timely and accurate surveillance. However, critical
overdose data are often buried in free-text coroner reports, leading to delays
and information loss when coded into ICD (International Classification of
Disease)-10 classifications. Natural language processing (NLP) models may
automate and enhance overdose surveillance, but prior applications have been
limited. A dataset of 35,433 death records from multiple U.S. jurisdictions in
2020 was used for model training and internal testing. External validation was
conducted using a novel separate dataset of 3,335 records from 2023-2024.
Multiple NLP approaches were evaluated for classifying specific drug
involvement from unstructured death certificate text. These included
traditional single- and multi-label classifiers, as well as fine-tuned
encoder-only language models such as Bidirectional Encoder Representations from
Transformers (BERT) and BioClinicalBERT, and contemporary decoder-only large
language models such as Qwen 3 and Llama 3. Model performance was assessed
using macro-averaged F1 scores, and 95% confidence intervals were calculated to
quantify uncertainty. Fine-tuned BioClinicalBERT models achieved near-perfect
performance, with macro F1 scores >=0.998 on the internal test set. External
validation confirmed robustness (macro F1=0.966), outperforming conventional
machine learning, general-domain BERT models, and various decoder-only large
language models. NLP models, particularly fine-tuned clinical variants like
BioClinicalBERT, offer a highly accurate and scalable solution for overdose
death classification from free-text reports. These methods can significantly
accelerate surveillance workflows, overcoming the limitations of manual ICD-10
coding and supporting near real-time detection of emerging substance use
trends.

</details>


### [5] [AdaptiSent: Context-Aware Adaptive Attention for Multimodal Aspect-Based Sentiment Analysis](https://arxiv.org/abs/2507.12695)
*S M Rafiuddin,Sadia Kamal,Mohammed Rakib,Arunkumar Bagavathi,Atriya Sen*

Main category: cs.CL

> AdaptiSent是一个用于多模态基于方面的意见分析的新框架，它利用自适应的跨模态注意力机制来提高从文本和图像中提取情感分类和方面词的能力。实验证明，AdaptiSent超越了现有方法，在精确度、召回率和F1分数方面有显著提高。

<details>
  <summary>Details</summary>

**Motivation:** 目标是解决多模态情感分析中的挑战，提升在处理复杂多模态信息时的理解和分析能力，特别是要提高情感分类和方面词提取的精度。

**Method:** AdaptiSent通过集成动态模态权重和上下文自适应注意力机制，专注于文本线索和视觉上下文之间的相互作用来强化情感和方面信息的提取。

**Result:** 在标准的Twitter数据集上测试，AdaptiSent在多个指标上表现优于传统文本模型和其他多模态方法，尤其在识别微妙的跨模态关系上特别有效。

**Conclusion:** AdaptiSent由于其动态调整注意力焦点的能力，在多种多模态数据集上的深度和准确性分析中取得了显著效果，为基于多模态的意见分析设定了新的标准。

**Abstract:** We introduce AdaptiSent, a new framework for Multimodal Aspect-Based
Sentiment Analysis (MABSA) that uses adaptive cross-modal attention mechanisms
to improve sentiment classification and aspect term extraction from both text
and images. Our model integrates dynamic modality weighting and
context-adaptive attention, enhancing the extraction of sentiment and
aspect-related information by focusing on how textual cues and visual context
interact. We tested our approach against several baselines, including
traditional text-based models and other multimodal methods. Results from
standard Twitter datasets show that AdaptiSent surpasses existing models in
precision, recall, and F1 score, and is particularly effective in identifying
nuanced inter-modal relationships that are crucial for accurate sentiment and
aspect term extraction. This effectiveness comes from the model's ability to
adjust its focus dynamically based on the context's relevance, improving the
depth and accuracy of sentiment analysis across various multimodal data sets.
AdaptiSent sets a new standard for MABSA, significantly outperforming current
methods, especially in understanding complex multimodal information.

</details>


### [6] [AudioJudge: Understanding What Works in Large Audio Model Based Speech Evaluation](https://arxiv.org/abs/2507.12705)
*Potsawee Manakul,Woody Haosheng Gan,Michael J. Ryan,Ali Sartaz Khan,Warit Sirichotedumrong,Kunat Pipatanakul,William Held,Diyi Yang*

Main category: cs.CL

> 本文探讨了一种统一的评估框架—AudioJudge，利用大型音频模型来提升音频特征检测和人类偏好模拟任务的性能，同时提出解决LAM的冗余和位置偏倚问题的方法。

<details>
  <summary>Details</summary>

**Motivation:** 研究的动机在于解决当前语音评估中两个关键问题：设计专门针对个体音频特征的系统的需要和难度，以及自动评估方法与人类偏好之间的差强人意的相关性。

**Method:** 研究采用了不同的提示工程策略，发现音频连接结合上下文学习可以显著提升音频特征检测和人类偏好模拟任务的表现。同时还介绍了将语音评估分解为专门针对词汇内容、语音质量和超音段特征的评估者的集成方法。

**Result:** 研究探索了AudioJudge在包括发音、说话速率、说话人识别和语音质量在内的多个音频特征检测任务，并通过系统级的人类偏好模拟进行自动化基准化测试。

**Conclusion:** 通过引入多方面集成的AudioJudge，该研究实现了高达0.91的斯皮尔曼相关性，与人类偏好在系统排名基准测试中的匹配度显著提高。尽管LAM在存在噪声条件下性能稳健，但需要解决冗余和位置偏倚的问题。

**Abstract:** Current speech evaluation suffers from two critical limitations: the need and
difficulty of designing specialized systems targeting individual audio
characteristics, and poor correlation between automatic evaluation methods and
human preferences. This work presents a systematic study of Large Audio Model
(LAM) as a Judge, AudioJudge, investigating whether it can provide a unified
evaluation framework that addresses both challenges. We systematically explore
AudioJudge across audio characteristic detection tasks, including
pronunciation, speaking rate, speaker identification and speech quality, and
system-level human preference simulation for automated benchmarking. We
investigate different prompt engineering strategies, finding that audio
concatenation combined with in-context learning significantly improves
performance across both audio characteristic detection and human preference
simulation tasks. We further introduce a multi-aspect ensemble AudioJudge to
enable general-purpose multi-aspect audio evaluation. This method decomposes
speech assessment into specialized judges for lexical content, speech quality,
and paralinguistic features, achieving up to 0.91 Spearman correlation with
human preferences on our system ranking benchmark. Robustness analysis reveals
that while LAMs maintain strong performance under acoustic noise, they exhibit
significant verbosity and positional biases that require careful mitigation.

</details>


### [7] [FLEXITOKENS: Flexible Tokenization for Evolving Language Models](https://arxiv.org/abs/2507.12720)
*Abraham Toluase Owodunni,Orevaoghene Ahia,Sachin Kumar*

Main category: cs.CL

> 研究开发了一种更为灵活的语言模型FLEXITOKENS，实验证明其能够在多种任务上避免分词过度碎片化，并提升总体性能。

<details>
  <summary>Details</summary>

**Motivation:** 现有的语言模型通过简单的微调难以适应新的数据分布，问题主要在于其子词分词器的僵化，无法学习新的语言或脚本的分词方式。

**Method:** 开发了一种名为FLEXITOKENS的方法，利用预测输入字节序列边界的小模块，将输入编码成可变长度的片段，相比现有方法更为灵活。

**Result:** 通过开发可学习的字节级语言模型及其自适应分词器，实现了在多种任务和语料库下的性能提升，有效减少分词碎片化问题，相比传统子词分词器，性能提升可达10%。

**Conclusion:** 提出了一种名为FLEXITOKENS的训练目标，能够有效提升语言模型的适应性和性能，改进了传统方法中分词器固定的限制。

**Abstract:** Language models (LMs) are challenging to adapt to new data distributions by
simple finetuning. This is due to the rigidity of their subword tokenizers,
which typically remain unchanged during adaptation. This inflexibility often
leads to inefficient tokenization, causing overfragmentation of
out-of-distribution domains, unseen languages, or scripts. In this work, we
develop byte-level LMs with learnable tokenizers to make tokenization adaptive.
Our models include a submodule that learns to predict boundaries between the
input byte sequence, encoding it into variable-length segments. Existing
tokenizer-free methods train this boundary predictor using an auxiliary loss
that enforces a fixed compression rate across the training corpus, introducing
a new kind of rigidity. We propose FLEXITOKENS, a simplified training objective
that enables significantly greater flexibility during adaptation. Evaluating
across multiple multilingual benchmarks, morphologically diverse tasks, and
domains, we demonstrate that FLEXITOKENS consistently reduces token
over-fragmentation and achieves up to 10\% improvements on downstream task
performance compared to subword and other gradient-based tokenizers. Code and
data for our experiments will be released at
https://github.com/owos/flexitokens

</details>


### [8] [TransEvalnia: Reasoning-based Evaluation and Ranking of Translations](https://arxiv.org/abs/2507.12724)
*Richard Sproat,Tianyu Zhao,Llion Jones*

Main category: cs.CL

> TransEvalnia改进了翻译评估技术，优于或至少达到了当前最佳系统MT-Ranker的水平，并解决了位置偏见问题。

<details>
  <summary>Details</summary>

**Motivation:** 为了提供一个精细的翻译评估工具，可以与现有的最佳系统MT-Ranker进行竞争或超过其性能，并证明其评估结果能够被人类评分者高度接受。

**Method:** 基于提示的翻译评估和排名系统（TransEvalnia），该系统通过推理来进行评估和排名，使用了Multidimensional Quality Metrics的子集来进行评估，并返回其认为最佳翻译的评估及各个维度和整体翻译的数值分数。

**Result:** TransEvalnia在英语-日语数据以及来自WMT共享任务的几种语言对上，表现优于或至少与MT-Ranker持平。使用Claude-3.5-Sonnet和Qwen-2.5-72B-Instruct作为评估大模型，显示出较高的人类评分者接受度，且其评分与人类评分高度相关。

**Conclusion:** TransEvalnia系统提供了一种新的、有效的翻译评估方法，适用于多种语言对，并且提议的方法可解决系统敏感性问题。所有数据和代码均已公开。

**Abstract:** We present TransEvalnia, a prompting-based translation evaluation and ranking
system that uses reasoning in performing its evaluations and ranking. This
system presents fine-grained evaluations based on a subset of the
Multidimensional Quality Metrics (https://themqm.org/), returns an assessment
of which translation it deems the best, and provides numerical scores for the
various dimensions and for the overall translation. We show that TransEvalnia
performs as well as or better than the state-of-the-art MT-Ranker (Moosa et al.
2024) on our own English-Japanese data as well as several language pairs from
various WMT shared tasks. Using Anthropic's Claude-3.5-Sonnet and
Qwen-2.5-72B-Instruct as the evaluation LLMs, we show that the evaluations
returned are deemed highly acceptable to human raters, and that the scores
assigned to the translations by Sonnet, as well as other LLMs, correlate well
with scores assigned by the human raters. We also note the sensitivity of our
system -- as well as MT-Ranker -- to the order in which the translations are
presented, and we propose methods to address this position bias. All data,
including the system's evaluation and reasoning, human assessments, as well as
code is released.

</details>


### [9] [Strategy Adaptation in Large Language Model Werewolf Agents](https://arxiv.org/abs/2507.12732)
*Fuya Nakamori,Yin Jou Huang,Fei Cheng*

Main category: cs.CL

> A new method for Werewolf agents allows them to switch between predefined strategies based on the game context and player attitudes, resulting in better performance compared to agents with fixed or implicit strategies.

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to improve the performance of Werewolf agents by enabling them to adapt their strategies based on game context and estimated roles of other players, which previous methods could not do effectively.

**Method:** The method involves switching between predefined strategies for Werewolf agents based on the attitudes of other players and context, contrasting with previous prompt engineering approaches which used implicit strategy definitions, unable to adapt to dynamic gaming situations.

**Result:** The study verifies the effectiveness of the proposed method by comparing strategy-adapting Werewolf agents with baseline agents using implicit or fixed strategies.

**Conclusion:** The conclusion is that the explicitly selected strategy based on game context and estimated roles of other players outperforms the implicit or fixed strategy approaches in Werewolf agents.

**Abstract:** This study proposes a method to improve the performance of Werewolf agents by
switching between predefined strategies based on the attitudes of other players
and the context of conversations. While prior works of Werewolf agents using
prompt engineering have employed methods where effective strategies are
implicitly defined, they cannot adapt to changing situations. In this research,
we propose a method that explicitly selects an appropriate strategy based on
the game context and the estimated roles of other players. We compare the
strategy adaptation Werewolf agents with baseline agents using implicit or
fixed strategies and verify the effectiveness of our proposed method.

</details>


### [10] [Logit Arithmetic Elicits Long Reasoning Capabilities Without Training](https://arxiv.org/abs/2507.12759)
*Yunxiang Zhang,Muhammad Khalifa,Lechen Zhang,Xin Liu,Ayoung Lee,Xinliang Frederick Zhang,Farima Fatahi Bayat,Lu Wang*

Main category: cs.CL

> 本文提出ThinkLogit及ThinkLogit-DPO方法提升大型语言模型的长链推理能力，实验显示相比Qwen2.5-32B模型显著提升通过率。

<details>
  <summary>Details</summary>

**Motivation:** 研究如何在不进行额外训练的情况下激发大型语言模型的长链推理能力，包括回溯和自我修正等认知策略。

**Method:** ThinkLogit和ThinkLogit-DPO两种方法用于增强大型语言模型的长链推理能力。ThinkLogit使用较小的引导模型通过logits算术微调大型语言模型，而ThinkLogit-DPO通过偏好优化训练引导模型以进一步提升性能。

**Result:** 实验表明，ThinkLogit和ThinkLogit-DPO方法分别在四个数学数据集上相比Qwen2.5-32B模型提升了26%和29%的通过率。此外，它还能将通过强化学习获得的长期推理技能迁移到其他模型。

**Conclusion:** 提出了一个计算效率高的方法，可以在不进行或只需要最少额外训练的情况下，显著提高大型语言模型进行长链推理的能力。

**Abstract:** Large reasoning models (LRMs) can do complex reasoning via long
chain-of-thought (CoT) involving cognitive strategies such as backtracking and
self-correction. Recent studies suggest that some models inherently possess
these long reasoning abilities, which may be unlocked via extra training. Our
work first investigates whether we can elicit such behavior without any
training. To this end, we propose a decoding-time approach, ThinkLogit, which
utilizes logits arithmetic (Liu et al., 2024) to tune a target large LM for
long reasoning using a substantially smaller model as guider. We then show that
we can further boost performance by training the guider model with preference
optimization over correct/incorrect reasoning pairs sampled from both the
target and guider model -- a setup we refer to as ThinkLogit-DPO. Our
experiments demonstrate that ThinkLogit and ThinkLogit-DPO achieve a relative
improvement in pass@1 by 26% and 29%, respectively, over four mathematical
datasets using the Qwen2.5-32B when guided by R1-Distill-Qwen-1.5B -- a model
21x smaller. Lastly, we show that ThinkLogit can transfer long reasoning skills
acquired through reinforcement learning, improving pass@1 by 13% relative
compared to the Qwen2.5-32B base model. Our work presents a
computationally-efficient method to elicit long reasoning in large models with
minimal or no additional training.

</details>


### [11] [Synergy: End-to-end Concept Model](https://arxiv.org/abs/2507.12769)
*Keli Zheng,Zerong Xie*

Main category: cs.CL

> Synergy语言模型通过学习的路由机制在端到端的方式中连接不同的语法抽象层次，无需传统的分词器而自动进行字节分词，性能优于Llama3。

<details>
  <summary>Details</summary>

**Motivation:** 研究的动机是为了创建一个能够无缝连接不同抽象层次的语言模型，并减少对预设分词方案的依赖。

**Method:** 提出了Synergy，一个通过学习路由机制在端到端的方式连接不同抽象层次的语言模型。模型以字节级语言模型的形式训练，自动学习对字节进行分词，生成的概念标记数量少于BBPE分词器，同时保持相似性能。

**Result:** 相比Llama3，在相同的模型规模和训练数据量下，Synergy表现出优势。研究发现模型中间部分（更高抽象层次）在移除位置编码后表现更佳，表明位置无关概念的出现。

**Conclusion:** 这些结果证明了无分词器架构的可行性，为更健壯和灵活的管道铺平了道路。

**Abstract:** In this paper, we present Synergy, a language model that bridges different
levels of abstraction in an end-to-end fashion through a learned routing
mechanism. Focusing on low-level linguistic abstraction, we trained our model
as a byte-level language model. Our model spontaneously learns to tokenize
bytes, producing fewer concept tokens than Byte-level Byte Pair Encoder (BBPE)
tokenizers while keeping comparable performance. By comparing with Llama3, we
observed an advantage of Synergy under the same model scale and training
dataset size. Further studies show that the middle part (the higher abstraction
part) of our model performs better when positional encodings are removed,
suggesting the emergence of position-independent concepts. These findings
demonstrate the feasibility of tokenizer-free architectures, paving the way for
more robust and flexible pipelines.

</details>


### [12] [Learning Robust Negation Text Representations](https://arxiv.org/abs/2507.12782)
*Thinh Hung Truong,Karin Verspoor,Trevor Cohn,Timothy Baldwin*

Main category: cs.CL

> 提出了一种改进文本编码器中否定稳健性的策略，通过使用多样化否定和保留模式从大型语言模型中蒸馏数据，极大地提高了否定理解能力。

<details>
  <summary>Details</summary>

**Motivation:** 虽然自回归大型语言模型快速普及，但在需要丰富上下文化表示的文本理解任务中，较小的文本编码器仍然扮演重要角色。然而，这些方法仍然未能恰当地捕捉否定，这影响到了许多依赖文本嵌入的下游应用。

**Method:** 通过多样化否定和保留模式从大型语言模型蒸馏数据来提高文本编码器的否定稳健性策略。采用标准对比学习策略微调强大的基于BERT的模型。

**Result:** 所提出的方法在提高否定理解能力的同时，保持了在通用基准测试上的竞争力。此外，该方法也可以适应大型语言模型，从而在否定基准测试上取得更好的性能。

**Conclusion:** 研究展示了一种能够提升文本编码器对于否定理解能力的方法，并且能够保持在通用基准测试上的竞争性能。方法还可用于大型语言模型，使得在否定基准测试上表现更高。

**Abstract:** Despite rapid adoption of autoregressive large language models, smaller text
encoders still play an important role in text understanding tasks that require
rich contextualized representations. Negation is an important semantic function
that is still not properly captured by such methods, affecting many downstream
applications relying on text embeddings. We propose a strategy to improve
negation robustness of text encoders, by distilling data from large language
models using diverse patterns of negation and hedging. We adopt a standard
contrastive learning strategy to finetune a strong BERT-based model, and
observe large improvement in negation understanding capabilities while
maintaining competitive performance on general benchmarks. In addition, we also
show that our method can be adapted to LLMs, leading to improved performance on
negation benchmarks.

</details>


### [13] [Large Language Models' Internal Perception of Symbolic Music](https://arxiv.org/abs/2507.12808)
*Andrew Shin,Kunitake Kaneko*

Main category: cs.CL

> 研究发现，大型语言模型可以在没有明确音乐训练的情况下，根据文本提示生成MIDI文件，并且可以进行音乐风格和类型的分类及旋律补全任务，显示出其对音乐模式的潜在隐式编码能力，同时也指出了其由于缺乏明确音乐背景所存在的局限性。

<details>
  <summary>Details</summary>

**Motivation:** 研究旨在探索大型语言模型在没有显式音乐训练的情况下，是否能够隐式地理解和生成音乐，特别是在理解和生成符号音乐方面的能力。

**Method:** 通过使用大型语言模型生成描述不同音乐风格和流派组合的文本提示，生成MIDI文件，然后训练神经网络对其进行风格和类型分类及旋律补全任务。

**Result:** 结果表明大型语言模型可以从文本中推断出基本的音乐结构和时间关系，但是也存在一定的限制，这归因于其缺乏明确的音乐上下文。

**Conclusion:** 大型语言模型显示出潜在的隐式编码音乐模式的能力，对于生成音乐内容具有一定的能力，但同时也表明需要更多关于音乐的明确训练才能提升其准确性。

**Abstract:** Large language models (LLMs) excel at modeling relationships between strings
in natural language and have shown promise in extending to other symbolic
domains like coding or mathematics. However, the extent to which they
implicitly model symbolic music remains underexplored. This paper investigates
how LLMs represent musical concepts by generating symbolic music data from
textual prompts describing combinations of genres and styles, and evaluating
their utility through recognition and generation tasks. We produce a dataset of
LLM-generated MIDI files without relying on explicit musical training. We then
train neural networks entirely on this LLM-generated MIDI dataset and perform
genre and style classification as well as melody completion, benchmarking their
performance against established models. Our results demonstrate that LLMs can
infer rudimentary musical structures and temporal relationships from text,
highlighting both their potential to implicitly encode musical patterns and
their limitations due to a lack of explicit musical context, shedding light on
their generative capabilities for symbolic music.

</details>


### [14] [Are Knowledge and Reference in Multilingual Language Models Cross-Lingually Consistent?](https://arxiv.org/abs/2507.12838)
*Xi Ai,Mahardika Krisna Ihsani,Min-Yen Kan*

Main category: cs.CL

> 研究分析了跨语言知识一致性，在考察了跨语言环境下模型的行为后发现，虽然普遍提升多语言性能的方法对增强知识一致性作用有限，但跨语言对齐监督和代码切换训练可显著提高跨语言一致性和多语言性能。

<details>
  <summary>Details</summary>

**Motivation:** 跨语言一致性对于评估跨语言传输能力、维护模型知识的事实性以及保持语言模型性能的平等性至关重要。研究的目标是为了分析、评估和解释跨语言知识的一致性。

**Method:** 通过分析跨语言环境中模型的行为来研究跨语言知识的一致性，特别是考查了混杂语言的共指陈述如何在不同的语言中传达相同的知识。研究使用了一些可解释性方法来发现多语言模型在不同语言家族、语言因素以及特定层面上的一致性水平。

**Result:** 发现仅靠提高多语言性能的常见策略并不一定能改善知识一致性。然而，代码切换训练和跨语言词对齐的目标显示出最令人鼓舞的结果，表明跨语言对齐监督和代码切换训练对于提高跨语言一致性和多语言性能都有重要意义。

**Conclusion:** 研究结果强调了跨语言对齐监督和代码切换训练对于维持知识跨语言一致性和提高多语言性能的重要性。尽管在多数情况下知识一致性并未得到保证，但研究指出某些特定策略对于提升跨语言一致性有显著帮助。

**Abstract:** Cross-lingual consistency should be considered to assess cross-lingual
transferability, maintain the factuality of the model knowledge across
languages, and preserve the parity of language model performance. We are thus
interested in analyzing, evaluating, and interpreting cross-lingual consistency
for factual knowledge. We examine code-mixed coreferential statements conveyed
identical knowledge across languages to study cross-lingual knowledge
consistency. We use some interpretability approaches to analyze the behavior of
a model in cross-lingual contexts, discovering that multilingual models show
different levels of consistency, subject to language families, linguistic
factors, and a bottleneck in cross-lingual consistency on a particular layer.
In addition, we evaluate common strategies aimed at improving multilingual
performance to observe whether these strategies can improve knowledge
consistency at the same time. While knowledge is not cross-lingual consistency
in many cases, code-switching training and cross-lingual word alignment
objectives show the most promising results, emphasizing the noteworthiness of
cross-lingual alignment supervision and code-switching training for both
multilingual performance and cross-lingual consistency enhancement.

</details>


### [15] [Making Language Model a Hierarchical Classifier and Generator](https://arxiv.org/abs/2507.12930)
*Yihong Wang,Zhonglin Jiang,Ningyuan Xi,Yue Zhao,Qingqing Gu,Xiyuan Chen,Hao Wu,Sheng Xu,Hange Zhou,Yong Chen,Luo Ji*

Main category: cs.CL

> 通过构建层次化解码器，改进预训练语言模型的性能，实现在多个任务上的优异效果。

<details>
  <summary>Details</summary>

**Motivation:** 模仿人类的层次化思维能力，提高语言模型的理解和生成能力。

**Method:** 将预训练模型的最后一层语言头复制到选定的中间层，并针对特定任务输入进行微调。

**Result:** 这项研究通过构建层次解码器架构，将预训练语言模型的不同层同时解码文本，来模拟人类的层次化思维能力。通过选择性地将最后一层的语言头复制到不同的中间层并进行微调，该方法在多个任务中表现出色，包括层次化文本分类、分类引导生成和层次化文本生成。研究结果表明，这种方法能够生成有意义的内容，并且可能为预训练通用层次化推理器提供可能性。

**Conclusion:** 该研究方法验证了层次化解码器的优势，并暗示了预训练通用层次化推理器的可能性。

**Abstract:** Decoder-only language models, such as GPT and LLaMA, generally decode on the
last layer. Motivated by human's hierarchical thinking capability, we propose
that a hierarchical decoder architecture could be built with different layers
decoding texts simultaneously. Due to limited time and computationally
resources, we choose to adapt a pretrained language model into this form of
hierarchical decoder. Language heads of the last layer are copied to different
selected intermediate layers, and fine-tuned with different task inputs. By
thorough experiments, we validate that these selective intermediate layers
could be adapted to speak meaningful and reasonable contents, and this paradigm
of hierarchical decoder can obtain state-of-the-art performances on multiple
tasks such as hierarchical text classification, classification-guided
generation, and hierarchical text generation. This study suggests the
possibility of a generalized hierarchical reasoner, pretraining from scratch.

</details>


### [16] [MRT at IberLEF-2025 PRESTA Task: Maximizing Recovery from Tables with Multiple Steps](https://arxiv.org/abs/2507.12981)
*Maximiliano Hormazábal Lagos,Álvaro Bueno Sáez,Héctor Cerezo-Costas,Pedro Alonso Doval,Jorge Alcalde Vesteiro*

Main category: cs.CL

> 本文提出了一种利用LLM生成Python代码来处理表格数据以回答问题的方法，在IberLEF 2025任务PRESTA中取得了85%的准确性。

<details>
  <summary>Details</summary>

**Motivation:** 本文的方法目的是参与IberLEF 2025任务PRESTA，即关于西班牙语表格的问题与答案。

**Method:** 通过LLM实现Python代码生成，用以筛选和处理表格内容，从而获得问题的答案。这个方法是基于MRT在SemEval 2025相关任务中的实现而发展的。该过程包括多个步骤：分析和理解表格内容，选择有用的列，生成自然语言指令，将这些指令翻译成代码，运行代码，并处理潜在的错误或异常。这些步骤使用开源语言模型和针对每个步骤优化的细粒度提示。

**Result:** 采用这个方法，我们在任务中达到了85%的准确性分数。

**Conclusion:** 通过LLM生成Python代码处理表格数据的方法取得了85%的准确率，展示了一种有效解决表格数据中问答问题的技术路径。

**Abstract:** This paper presents our approach for the IberLEF 2025 Task PRESTA: Preguntas
y Respuestas sobre Tablas en Espa\~nol (Questions and Answers about Tables in
Spanish). Our solution obtains answers to the questions by implementing Python
code generation with LLMs that is used to filter and process the table. This
solution evolves from the MRT implementation for the Semeval 2025 related task.
The process consists of multiple steps: analyzing and understanding the content
of the table, selecting the useful columns, generating instructions in natural
language, translating these instructions to code, running it, and handling
potential errors or exceptions. These steps use open-source LLMs and
fine-grained optimized prompts for each step. With this approach, we achieved
an accuracy score of 85\% in the task.

</details>


### [17] [Formalizing Attack Scenario Description: A Proposed Model](https://arxiv.org/abs/2507.13076)
*Quentin Goux,Nadira Lammari*

Main category: cs.CL

> The paper proposes a novel UML-based formal model for attack scenarios to support automation in cybersecurity processes, such as attack simulation and training script generation.

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to formalize input data for cybersecurity processes in an evolving threat landscape, facilitating process automation and improved security.

**Method:** A UML class model formalizes the context and scenario of attacks, enabling its use in attack analysis and automatic script generation.

**Result:** The model is capable of supporting upstream attack analysis and the automatic generation of attack scripts for training.

**Conclusion:** The paper highlights the importance and feasibility of formalizing attack scenarios to enhance cybersecurity automation and effectiveness.

**Abstract:** Organizations face an ever-changing threat landscape. They must continuously
dedicate significant efforts to protect their assets, making their adoption of
increased cybersecurity automation inevitable. However, process automation
requires formalization of input data. Through this paper, we address this need
for processes that use attack scenarios as input. Among these processes, one
can mention both the generation of scripts for attack simulation and training
purposes, as well as the analysis of attacks. Therefore, the paper's main
research contribution is a novel formal model that encompasses the attack's
context description and its scenario. It is abstracted using UML class model.
Once the description of our model done, we will show how it could serve an
upstream attack analysis process. We will show also its use for an automatic
generation of attack scripts in the context of cybersecurity training. These
two uses cases constitute the second contribution of this present research
work.

</details>


### [18] [SemCSE: Semantic Contrastive Sentence Embeddings Using LLM-Generated Summaries For Scientific Abstracts](https://arxiv.org/abs/2507.13105)
*Marc Brinner,Sina Zarriess*

Main category: cs.CL

> SemCSE is an unsupervised method for learning semantic embeddings of scientific texts using LLM-generated summaries to capture true semantic content, as shown by performance on various benchmarks.

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind this method is to improve upon traditional citation-based approaches that do not necessarily reflect the semantic similarity of texts. SemCSE aims to capture true semantic content of a text.

**Method:** Our method is called SemCSE, an unsupervised approach for learning semantic embeddings of scientific texts. It uses LLM-generated summaries of scientific abstracts to train a model, positioning semantically similar summaries closer together in the embedding space.

**Result:** The proposed method demonstrates strong semantic separation within the embedding space on a novel benchmark designed to assess semantic understanding of scientific texts. It also achieves state-of-the-art performance on the established SciRepEval benchmark.

**Conclusion:** The conclusion from the results is that focusing on semantic content in the training of embeddings could significantly enhance their performance and semantic representation capability.

**Abstract:** We introduce SemCSE, an unsupervised method for learning semantic embeddings
of scientific texts. Building on recent advances in contrastive learning for
text embeddings, our approach leverages LLM-generated summaries of scientific
abstracts to train a model that positions semantically related summaries closer
together in the embedding space. This resulting objective ensures that the
model captures the true semantic content of a text, in contrast to traditional
citation-based approaches that do not necessarily reflect semantic similarity.
To validate this, we propose a novel benchmark designed to assess a model's
ability to understand and encode the semantic content of scientific texts,
demonstrating that our method enforces a stronger semantic separation within
the embedding space. Additionally, we evaluate SemCSE on the comprehensive
SciRepEval benchmark for scientific text embeddings, where it achieves
state-of-the-art performance among models of its size, thus highlighting the
benefits of a semantically focused training approach.

</details>


### [19] [A Computational Framework to Identify Self-Aspects in Text](https://arxiv.org/abs/2507.13115)
*Jaya Caporusso,Matthew Purver,Senja Pollak*

Main category: cs.CL

> 该博士论文提案旨在开发一个计算框架识别文本中的自我方面，引入自我方面的本体论和黄金标准标注数据集，评估不同类型的模型，并应用于实际案例研究。

<details>
  <summary>Details</summary>

**Motivation:** 自我是一个多面的概念，在语言中有所体现。虽然它在认知科学和现象学等领域有所描述，但在自然语言处理（NLP）中尚未得到充分探索。许多自我方面的内容与心理健康等领域已有研究的现象相一致，强调了系统性NLP分析的必要性。

**Method:** 本研究计划开发一个计算框架来识别文本中的自我方面。将引入自我方面的本体论和一个黄金标准的标注数据集作为基础。将开发并评估传统的判别模型、生成型大型语言模型以及基于嵌入的检索方法，并根据可解释性、真实数据一致性、准确性以及计算效率四项主要标准进行评价。

**Result:** 此研究将开发一个自我方面的计算识别框架，评估不同的模型，并应用于心理健康和经验现象学的案例研究中。

**Conclusion:** 该研究将推动NLP领域自我识别的研究，并为心理健康和经验现象学提供有价值的工具和见解。

**Abstract:** This Ph.D. proposal introduces a plan to develop a computational framework to
identify Self-aspects in text. The Self is a multifaceted construct and it is
reflected in language. While it is described across disciplines like cognitive
science and phenomenology, it remains underexplored in natural language
processing (NLP). Many of the aspects of the Self align with psychological and
other well-researched phenomena (e.g., those related to mental health),
highlighting the need for systematic NLP-based analysis. In line with this, we
plan to introduce an ontology of Self-aspects and a gold-standard annotated
dataset. Using this foundation, we will develop and evaluate conventional
discriminative models, generative large language models, and embedding-based
retrieval approaches against four main criteria: interpretability, ground-truth
adherence, accuracy, and computational efficiency. Top-performing models will
be applied in case studies in mental health and empirical phenomenology.

</details>


### [20] [Assessing the Reliability of LLMs Annotations in the Context of Demographic Bias and Model Explanation](https://arxiv.org/abs/2507.13138)
*Hadi Mohammadi,Tina Shahedi,Pablo Mosteiro,Massimo Poesio,Ayoub Bagheri,Anastasia Giachanou*

Main category: cs.CL

> 该研究通过广义线性混合模型分析发现，尽管人口统计特征对标注有一定影响，但影响甚微，主要决定因素仍然是文本内容。尝试引导AI模型使用人口统计特征进行注释整体未能提升效果。

<details>
  <summary>Details</summary>

**Motivation:** 研究的主要动机是探讨如何在自然语言处理系统中实现更公平的性别歧视检测任务，尤其是通过理解注释者的人口统计特征和文本内容对标注决策的影响。这种研究对于消除标注过程中的偏差、提高模型在现实情况中的公平性具有重要意义。

**Method:** 该研究使用了一种统计模型——广义线性混合模型来量化注释者的人口统计特征和文本内容对标注决策的相对影响。还评估了生成AI模型在被具体人口统计特征指导时的表现，并使用了解释性AI技术来深入分析模型预测的关键依赖因素。

**Result:** {"tldr": "该研究探讨了注释者的人口统计特征和文本内容对其标注决策的影响，发现文本内容是主要决定因素，人口统计特征解释的变异性极小。此外，研究评估了引导生成AI模型使用人口统计特征作为注释者的效果，发现这种方法通常没有提升效果，有时反而会降低性能。", "motivation": "理解标注中的变异性来源对于开发公平的自然语言处理系统至关重要，尤其是在像性别歧视检测这样的任务中，人口统计偏差是一个需要关注的问题。", "method": "使用广义线性混合模型量化人口统计特征与文本内容对标注决策的影响，并使用解释性AI技术揭示模型预测的依赖因素。", "result": "发现人口统计特征仅解释了一小部分观察到的变异性（8%），并且引导生成AI模型使用人口统计特征并没有提升其与人类判断的一致性，有时反而降低了其性能。", "conclusion": "研究建议，应重点依靠内容驱动的解释和稳健的标注协议来实现公平性，而不是模拟人口统计特征。"}

**Conclusion:** 该研究指出，注释过程中的变异性和公平性问题主要源于文本内容，而非注释者的人口统计特征。此外，尝试通过引导AI模型使用人口统计特征来提升其注释效果的方法并不总是有效，有时反而降低了它们的表现。这些发现强调了在开发自然语言处理系统时应更多依赖于内容驱动的解释与标注协议来确保更好的公平性。

**Abstract:** Understanding the sources of variability in annotations is crucial for
developing fair NLP systems, especially for tasks like sexism detection where
demographic bias is a concern. This study investigates the extent to which
annotator demographic features influence labeling decisions compared to text
content. Using a Generalized Linear Mixed Model, we quantify this inf luence,
finding that while statistically present, demographic factors account for a
minor fraction ( 8%) of the observed variance, with tweet content being the
dominant factor. We then assess the reliability of Generative AI (GenAI) models
as annotators, specifically evaluating if guiding them with demographic
personas improves alignment with human judgments. Our results indicate that
simplistic persona prompting often fails to enhance, and sometimes degrades,
performance compared to baseline models. Furthermore, explainable AI (XAI)
techniques reveal that model predictions rely heavily on content-specific
tokens related to sexism, rather than correlates of demographic
characteristics. We argue that focusing on content-driven explanations and
robust annotation protocols offers a more reliable path towards fairness than
potentially persona simulation.

</details>


### [21] [Feature-based analysis of oral narratives from Afrikaans and isiXhosa children](https://arxiv.org/abs/2507.13164)
*Emma Sharratt,Annelien Smith,Retief Louw,Daleen Klop,Febe de Wet,Herman Kamper*

Main category: cs.CL

> 这项研究分析了被认定需要干预的四至五岁儿童的口头叙事，发现在南非语和 isiXhosa 两种语言中，词汇多样性和基于长度的特征是正常的言语发展的标志；并且特定动词和助动词的使用可以降低需要干预的可能性，这对多语言环境中早期评估有重要意义。

<details>
  <summary>Details</summary>

**Motivation:** 研究动机在于探索口头叙事的特征，特别是那些被专家认定需要干预的儿童。

**Method:** 本文采用简单的机器学习方法分析了四至五岁说南非语和 isiXhosa 的儿童所讲述的故事录音。

**Result:** 研究发现词汇多样性（独特的词汇）和基于长度的特征（平均言话语句长度）是正常发展的指标，而发音速率则不太具有信息量。特定动词和助动词的使用与降低需要干预的可能性相关。

**Conclusion:** 本研究揭示了两种语言中的语言特有和共同的叙事能力预测指标，这对多语言环境中的早期评估具有重要意义。

**Abstract:** Oral narrative skills are strong predictors of later literacy development.
This study examines the features of oral narratives from children who were
identified by experts as requiring intervention. Using simple machine learning
methods, we analyse recorded stories from four- and five-year-old Afrikaans-
and isiXhosa-speaking children. Consistent with prior research, we identify
lexical diversity (unique words) and length-based features (mean utterance
length) as indicators of typical development, but features like articulation
rate prove less informative. Despite cross-linguistic variation in
part-of-speech patterns, the use of specific verbs and auxiliaries associated
with goal-directed storytelling is correlated with a reduced likelihood of
requiring intervention. Our analysis of two linguistically distinct languages
reveals both language-specific and shared predictors of narrative proficiency,
with implications for early assessment in multilingual contexts.

</details>


### [22] [GEMMAS: Graph-based Evaluation Metrics for Multi Agent Systems](https://arxiv.org/abs/2507.13190)
*Jisoo Lee,Raeyoung Chang,Dongwook Kwon,Harmanpreet Singh,Nikhil Verma*

Main category: cs.CL

> 本研究提出了GEMMAS框架，用于评估多智能体系统内部的协作过程，揭示了仅依赖结果评估的不足，并强调了过程诊断的重要性。

<details>
  <summary>Details</summary>

**Motivation:** 现有的评估只关注最终输出的正确性，忽视了低效的通信和不良的协调如何导致重复推理和更高的计算成本。

**Method:** 引入GEMMAS，这是一种基于图的评估框架，通过将代理交互建模为有向无环图来分析内部协作过程。提出了两个过程级别的度量标准：信息多样性分数（IDS），用于测量代理间消息的语义变化；无用路径比率（UPR），用于量化冗余推理路径的数量。

**Result:** 在GSM8K基准测试中，两个系统准确率仅相差2.1%，但在IDS和UPR上的差异分别为12.8%和80%，说明内部协作的质量存在显著差异。

**Conclusion:** 结果显示，仅依赖结果的度量标准不足以评估多智能体系统的性能，强调了设计更可解释和资源高效的合作AI系统时过程级诊断的重要性。

**Abstract:** Multi-agent systems built on language models have shown strong performance on
collaborative reasoning tasks. However, existing evaluations focus only on the
correctness of the final output, overlooking how inefficient communication and
poor coordination contribute to redundant reasoning and higher computational
costs. We introduce GEMMAS, a graph-based evaluation framework that analyzes
the internal collaboration process by modeling agent interactions as a directed
acyclic graph. To capture collaboration quality, we propose two process-level
metrics: Information Diversity Score (IDS) to measure semantic variation in
inter-agent messages, and Unnecessary Path Ratio (UPR) to quantify redundant
reasoning paths. We evaluate GEMMAS across five benchmarks and highlight
results on GSM8K, where systems with only a 2.1% difference in accuracy differ
by 12.8% in IDS and 80% in UPR, revealing substantial variation in internal
collaboration. These findings demonstrate that outcome-only metrics are
insufficient for evaluating multi-agent performance and highlight the
importance of process-level diagnostics in designing more interpretable and
resource-efficient collaborative AI systems.

</details>


### [23] [Automatically assessing oral narratives of Afrikaans and isiXhosa children](https://arxiv.org/abs/2507.13205)
*R. Louw,E. Sharratt,F. de Wet,C. Jacobs,A. Smith,H. Kamper*

Main category: cs.CL

> 研究开发了一个自动评估学前儿童口头叙述能力的系统，该系统在预测分数方面优于线性模型，并可与人类专家匹配以识别需要干预的儿童。

<details>
  <summary>Details</summary>

**Motivation:** 研究旨在帮助教师准确识别需要干预的学前儿童，解决大规模幼儿园教室中的教学难题。

**Method:** 该系统使用自动语音识别技术，然后通过机器学习评分模型来预测叙述和理解分数。该研究比较了线性模型和大型语言模型（LLM）的评分预测效果。

**Result:** 大型语言模型（LLM）系统在大多数情况下优于线性模型，能与人类专家一样准确地识别需要干预的儿童。

**Conclusion:** 研究为教室中的自动口头评估奠定了基础，使教师有更多能力提供个性化的学习支持。

**Abstract:** Developing narrative and comprehension skills in early childhood is critical
for later literacy. However, teachers in large preschool classrooms struggle to
accurately identify students who require intervention. We present a system for
automatically assessing oral narratives of preschool children in Afrikaans and
isiXhosa. The system uses automatic speech recognition followed by a machine
learning scoring model to predict narrative and comprehension scores. For
scoring predicted transcripts, we compare a linear model to a large language
model (LLM). The LLM-based system outperforms the linear model in most cases,
but the linear system is competitive despite its simplicity. The LLM-based
system is comparable to a human expert in flagging children who require
intervention. We lay the foundation for automatic oral assessments in
classrooms, giving teachers extra capacity to focus on personalised support for
children's learning.

</details>


### [24] [Enhancing Cross-task Transfer of Large Language Models via Activation Steering](https://arxiv.org/abs/2507.13236)
*Xinyu Tang,Zhihao Lv,Xiaoxue Cheng,Junyi Li,Wayne Xin Zhao,Zujie Wen,Zhiqiang Zhang,Jun Zhou*

Main category: cs.CL

> 研究团队提出了一种名为CAST的新方法，通过分析和操作大型语言模型的潜在空间中的激活模式，来有效地实现跨任务的知识转移，并展示了在跨领域和跨语言场景中超过现有方法的效果。

<details>
  <summary>Details</summary>

**Motivation:** 大型语言模型（LLMs）虽然在通过提示来激发预训练知识方面表现出色，但在未见任务，特别是在数据稀缺的情况下，往往存在困难。尽管跨任务上下文学习为任务间的知识转移提供了直接解决方案，但该方法在鲁棒性、可扩展性和效率方面仍面临巨大挑战。

**Method:** 通过分析大型语言模型（LLMs）在任务之间进行跨任务传输时的潜在空间中的激活模式，研究团队观察到由同场景示例诱导的增强激活在不同任务中具有持续的模式。基于这些发现，他们提出了一种名为CAST的新方法，即跨任务激活引导传输框架，通过操作模型的内部激活状态来实现有效的传输。该方法首先从高资源任务中选择具有影响力且多样性的样本，然后利用这些样本的对比表示增强激活来适应低资源任务中的LLMs。

**Result:** 实验结果显示，该方法在跨领域和跨语言传输设置下表现优于竞争基线，展示了优良的可扩展性和更低的计算成本。

**Conclusion:** 研究结果表明，通过利用高资源任务的样本并通过操作模型内部的激活状态来适应低资源任务，可以在不进行参数更新或输入扩增的情况下实现高效、可靠的跨任务传输。这种方法相对于其他方法而言，不仅能提供更好的性能，还表现出更强的可扩展性和较低的计算代价。

**Abstract:** Large language models (LLMs) have shown impressive abilities in leveraging
pretrained knowledge through prompting, but they often struggle with unseen
tasks, particularly in data-scarce scenarios. While cross-task in-context
learning offers a direct solution for transferring knowledge across tasks, it
still faces critical challenges in terms of robustness, scalability, and
efficiency. In this paper, we investigate whether cross-task transfer can be
achieved via latent space steering without parameter updates or input
expansion. Through an analysis of activation patterns in the latent space of
LLMs, we observe that the enhanced activations induced by in-context examples
have consistent patterns across different tasks. Inspired by these findings, we
propose CAST, a novel Cross-task Activation Steering Transfer framework that
enables effective transfer by manipulating the model's internal activation
states. Our approach first selects influential and diverse samples from
high-resource tasks, then utilizes their contrastive representation-enhanced
activations to adapt LLMs to low-resource tasks. Extensive experiments across
both cross-domain and cross-lingual transfer settings show that our method
outperforms competitive baselines and demonstrates superior scalability and
lower computational costs.

</details>


### [25] [HATS: Hindi Analogy Test Set for Evaluating Reasoning in Large Language Models](https://arxiv.org/abs/2507.13238)
*Ashray Gupta,Rohan Joseph,Sunny Rai*

Main category: cs.CL

> 本文提出了一个用于评估多语言大语言模型（LLM）在印地语中推理能力的新测试集（HATS），并引入了一种新的基于认知理论的类比推理思维链方法。实验显示，无论提示策略如何，英文提示都使模型表现最佳。

<details>
  <summary>Details</summary>

**Motivation:** 大语言模型（LLMs）在英语推理能力方面被广泛评估，但在印度语言中的推理能力研究不足，影响了我们对这些模型跨语言泛化能力的理解。为此，本论文致力于填补这一空白。

**Method:** 引入了一个新的印地语类比测试集（HATS），包含405个多选题，这些题目源自印度政府考试。通过各种提示策略评估了最先进的多语言大语言模型，并引入了一种基于认知理论的类比推理思维链方法，以提高模型在印地语类比题目上的性能。

**Result:** 实验表明，使用英文提示时，模型在印地语类比题目的表现最好，无论采用哪种提示策略。

**Conclusion:** 本论文通过引入HATS解决了缺乏关键资源来评估LLM在印地语中的推理能力的问题，提出的方法改善了模型在印地语类比题目上的表现。

**Abstract:** Analogies test a model's ability to infer implicit relationships between
concepts, making them a key benchmark for evaluating reasoning capabilities.
While large language models (LLMs) are widely evaluated for reasoning in
English, their abilities in Indic languages remain understudied, limiting our
understanding of whether these models generalize across languages. To address
this gap, we introduce a new Hindi Analogy Test Set (HATS), comprising 405
multiple-choice questions sourced from Indian government exams. We benchmark
state-of-the-art multilingual LLMs using various prompting strategies and
introduce a grounded Chain of Thought approach that leverages cognitive
theories of analogical reasoning. This approach improves model performance on
Hindi analogy questions. Our experiments show that models perform best with
English prompts, irrespective of the prompting strategy. Our test set addresses
the lack of a critical resource to evaluate LLM reasoning capabilities in
Hindi.

</details>


### [26] [Automating Steering for Safe Multimodal Large Language Models](https://arxiv.org/abs/2507.13255)
*Lyucheng Wu,Mengru Wang,Ziwen Xu,Tri Cao,Nay Oo,Bryan Hooi,Shumin Deng*

Main category: cs.CL

> 该研究介绍了一种名为AutoSteer的推理时间干预技术，用于提高多模态大型语言模型的安全性，同时保持其一般能力。实验显示，AutoSteer显著降低了文本、视觉和跨模态威胁的攻击成功率。

<details>
  <summary>Details</summary>

**Motivation:** 由于多模态大型语言模型（MLLMs）在面对对抗性多模态输入时提出了新的安全问题，该研究旨在通过引入AutoSteer技术提高MLLMs在推理过程中的安全性，而无需对底层模型进行微调。

**Method:** AutoSteer 技术包含三个核心组件：一种新型的安全意识评分（SAS），自适应安全探测器，以及轻量级拒绝头。这些组件协同工作，以在检测到安全风险时调节生成过程。

**Result:** 实验表明，AutoSteer在减少了文本、视觉和跨模态威胁的攻击成功率的同时，还保持了模型的一般能力。

**Conclusion:** 该研究提出的方法可作为一个实用、可解释且有效的框架，用于多模态AI系统的安全部署。

**Abstract:** Recent progress in Multimodal Large Language Models (MLLMs) has unlocked
powerful cross-modal reasoning abilities, but also raised new safety concerns,
particularly when faced with adversarial multimodal inputs. To improve the
safety of MLLMs during inference, we introduce a modular and adaptive
inference-time intervention technology, AutoSteer, without requiring any
fine-tuning of the underlying model. AutoSteer incorporates three core
components: (1) a novel Safety Awareness Score (SAS) that automatically
identifies the most safety-relevant distinctions among the model's internal
layers; (2) an adaptive safety prober trained to estimate the likelihood of
toxic outputs from intermediate representations; and (3) a lightweight Refusal
Head that selectively intervenes to modulate generation when safety risks are
detected. Experiments on LLaVA-OV and Chameleon across diverse safety-critical
benchmarks demonstrate that AutoSteer significantly reduces the Attack Success
Rate (ASR) for textual, visual, and cross-modal threats, while maintaining
general abilities. These findings position AutoSteer as a practical,
interpretable, and effective framework for safer deployment of multimodal AI
systems.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [27] [Spatially Grounded Explanations in Vision Language Models for Document Visual Question Answering](https://arxiv.org/abs/2507.12490)
*Maximiliano Hormazábal Lagos,Héctor Cerezo-Costas,Dimosthenis Karatzas*

Main category: cs.CV

> 本文提出了一种无需训练的方法EaGERS，来在没有任何额外训练的情况下增强DocVQA任务的表现，并提供更高透明度和可重复性。

<details>
  <summary>Details</summary>

**Motivation:** 作者旨在开发一个不需要训练且能够增加解释性的方法，用于从文档图像中提取信息的任务，即DocVQA任务。

**Method:** 本文提出了一种名为EaGERS的全训练免费且模型无关的框架，该框架通过视觉语言模型生成自然语言解释，通过计算多模态嵌入相似度并采用多数投票方法将这些解释定位到空间子区域，并仅从被选中的遮罩图像区域生成响应。

**Result:** 实验结果表明，该方法在DocVQA数据集上不仅提高了基础模型在精确匹配准确率和平均归一化Levenshtein相似度指标上的表现，还提高了DocVQA任务的透明性和可重复性，并且无需额外的模型微调。

**Conclusion:** 通过在DocVQA基准上对比性能，本文证明了EaGERS在提高任务准确性和增强方法透明度方面的有效性，而不需要额外的训练步骤。

**Abstract:** We introduce EaGERS, a fully training-free and model-agnostic pipeline that
(1) generates natural language rationales via a vision language model, (2)
grounds these rationales to spatial sub-regions by computing multimodal
embedding similarities over a configurable grid with majority voting, and (3)
restricts the generation of responses only from the relevant regions selected
in the masked image. Experiments on the DocVQA dataset demonstrate that our
best configuration not only outperforms the base model on exact match accuracy
and Average Normalized Levenshtein Similarity metrics but also enhances
transparency and reproducibility in DocVQA without additional model
fine-tuning.

</details>


### [28] [MindJourney: Test-Time Scaling with World Models for Spatial Reasoning](https://arxiv.org/abs/2507.12508)
*Yuncong Yang,Jiageng Liu,Zheyuan Zhang,Siyuan Zhou,Reuben Tan,Jianwei Yang,Yilun Du,Chuang Gan*

Main category: cs.CV

> MindJourney couples a vision-language model with a controllable world model to enhance 3D spatial reasoning, achieving significant performance improvements on the SAT benchmark without fine-tuning.

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind MindJourney is the lack of 3D spatial understanding in VLMs, which limits their performance in embodied tasks like navigation and manipulation. This innovation aims to augment VLMs with a capability for understanding 3D dynamics, thus improving their performance in spatial tasks.

**Method:** Spatial reasoning in 3D space is often a challenge for vision-language models (VLMs) due to their 2D perception. The paper introduces MindJourney, a test-time scaling framework. This framework couples a VLM with a controllable world model based on video diffusion. The VLM sketches a camera trajectory and the world model generates corresponding views at each step, enabling the VLM to reason over multi-view evidence.

**Result:** Without fine-tuning, MindJourney achieves a significant performance boost of over 8% average on the SAT spatial reasoning benchmark. It also outperforms test-time inference methods in VLMs trained through reinforcement learning, highlighting the robustness of 3D reasoning through test-time scaling.

**Conclusion:** The paper concludes that combining VLMs with world models via test-time scaling can significantly improve performance in 3D reasoning tasks, showcasing the potential of this approach for enhancing VLMs without the need for additional training.

**Abstract:** Spatial reasoning in 3D space is central to human cognition and indispensable
for embodied tasks such as navigation and manipulation. However,
state-of-the-art vision-language models (VLMs) struggle frequently with tasks
as simple as anticipating how a scene will look after an egocentric motion:
they perceive 2D images but lack an internal model of 3D dynamics. We therefore
propose MindJourney, a test-time scaling framework that grants a VLM with this
missing capability by coupling it to a controllable world model based on video
diffusion. The VLM iteratively sketches a concise camera trajectory, while the
world model synthesizes the corresponding view at each step. The VLM then
reasons over this multi-view evidence gathered during the interactive
exploration. Without any fine-tuning, our MindJourney achieves over an average
8% performance boost on the representative spatial reasoning benchmark SAT,
showing that pairing VLMs with world models for test-time scaling offers a
simple, plug-and-play route to robust 3D reasoning. Meanwhile, our method also
improves upon the test-time inference VLMs trained through reinforcement
learning, which demonstrates the potential of our method that utilizes world
models for test-time scaling.

</details>


### [29] [Mono-InternVL-1.5: Towards Cheaper and Faster Monolithic Multimodal Large Language Models](https://arxiv.org/abs/2507.12566)
*Gen Luo,Wenhan Dou,Wenhao Li,Zhaokai Wang,Xue Yang,Changyao Tian,Hao Li,Weiyun Wang,Wenhai Wang,Xizhou Zhu,Yu Qiao,Jifeng Dai*

Main category: cs.CV

> 本文提出了一种处理单体多模态大语言模型优化不稳定问题的方法，并通过其改进版本实现了更低的数据成本和推理延迟。

<details>
  <summary>Details</summary>

**Motivation:** 本文旨在解决现有单体多模态大语言模型优化不稳定和灾难性遗忘的问题，同时降低数据成本，以实现更好的视觉能力和更高的运行效率。

**Method:** 本文提出了一种名为Mono-InternVL的单体多模态大语言模型，该模型在现有的大型语言模型基础上嵌入了新的视觉参数空间，通过增量调优学习来自嘈杂数据的视觉知识。为了进一步提高模型性能并降低成本，还引入了Mono-InternVL-1.5，它通过改进的内生视觉预训练（EViP++）和优化的推理过程实现了这些目标。

**Result:** 实验结果表明，Mono-InternVL在15个基准测试中的12个上优于现有的单体多模态大语言模型，例如在OCR-BENCH上相对Emu3提高了114个点的性能。与模块化对应版本相比，Mono-InternVL-1.5实现了相似的多模态性能，同时将首次响应延迟降低了69%。

**Conclusion:** 本文提出的方法Mono-InternVL及其改进版本Mono-InternVL-1.5在实现高效视觉知识学习的同时，显著降低了数据成本和推理延迟，并在多个基准测试中展示了竞争性能。

**Abstract:** This paper focuses on monolithic Multimodal Large Language Models (MLLMs),
which integrate visual encoding and language decoding into a single model.
Existing structures and pre-training strategies for monolithic MLLMs often
suffer from unstable optimization and catastrophic forgetting. To address these
challenges, our key idea is to embed a new visual parameter space into a
pre-trained LLM, enabling stable learning of visual knowledge from noisy data
via delta tuning. Based on this principle, we first introduce Mono-InternVL, an
advanced monolithic MLLM that incorporates a set of visual experts through a
multimodal mixture-of-experts architecture. In addition, we design an
innovative Endogenous Visual Pre-training (EViP) for Mono-InternVL to maximize
its visual capabilities via progressive learning. Mono-InternVL achieves
competitive performance against existing MLLMs but also leads to relatively
expensive data cost. Therefore, we further present Mono-InternVL-1.5, a cheaper
and stronger monolithic MLLM equipped with an improved EViP (EViP++). EViP++
introduces additional visual attention experts to Mono-InternVL-1.5 and
re-organizes the pre-training process in an efficient manner. During inference,
it includes a fused CUDA kernel to speed up its MoE operations. With these
designs, Mono-InternVL-1.5 significantly reduces training and inference costs,
while still maintaining competitive performance with Mono-InternVL. To evaluate
our approach, we conduct extensive experiments across 15 benchmarks. Results
demonstrate that Mono-InternVL outperforms existing monolithic MLLMs on 12 out
of 15 benchmarks, e.g., +114-point improvement over Emu3 on OCRBench. Compared
to its modular counterpart, i.e., InternVL-1.5, Mono-InternVL-1.5 achieves
similar multimodal performance while reducing first-token latency by up to 69%.
Code and models are released at https://github.com/OpenGVLab/Mono-InternVL.

</details>


### [30] [Best Practices for Large-Scale, Pixel-Wise Crop Mapping and Transfer Learning Workflows](https://arxiv.org/abs/2507.12590)
*Judy Long,Tao Liu,Sean Alexander Woznicki,Miljana Marković,Oskar Marko,Molly Sears*

Main category: cs.CV

> 该研究对大规模像素级农作物映射工作流程进行了全面回顾，涵盖了传统监督方法和新兴的迁移学习方法。通过系统实验，研究确定了最佳的预处理方法及分类模型，并展示了迁移学习技术在不同领域转移情况下的有效性。

<details>
  <summary>Details</summary>

**Motivation:** 推动大规模像素级农作物映射技术的进步，以便更准确地识别和分类作物类型。

**Method:** 系统地比较了六种卫星图像预处理方法以及十一种监督像素分类模型，还评估了不同训练样本尺寸和变量组合的影响。

**Result:** 精细间隔预处理及Transformer模型在监督和迁移工作流程中表现最佳。随机森林在传统监督学习和直接迁移相似域中表现出快速训练和竞争力。迁移学习显著提高了流程适应性。

**Conclusion:** 监督训练在有足够的标签样本时通常结果更准确和泛化能力强；样本数量不足时，匹配领域转移级别的迁移学习是一个可行的替代方案。

**Abstract:** Crop mapping involves identifying and classifying crop types using spatial
data, primarily derived from remote sensing imagery. This study presents the
first comprehensive review of large-scale, pixel-wise crop mapping workflows,
encompassing both conventional supervised methods and emerging transfer
learning approaches. To identify the optimal supervised crop mapping workflows,
we conducted systematic experiments, comparing six widely adopted satellite
image-based preprocessing methods, alongside eleven supervised pixel-wise
classification models. Additionally, we assessed the synergistic impact of
varied training sample sizes and variable combinations. Moreover, we identified
optimal transfer learning techniques for different magnitudes of domain shift.
The evaluation of best methods was conducted across five diverse agricultural
sites. Landsat 8 served as the primary satellite data source. Labels come from
CDL trusted pixels and field surveys.
  Our findings reveal three key insights. First, fine-scale interval
preprocessing paired with Transformer models consistently delivered optimal
performance for both supervised and transferable workflows. RF offered rapid
training and competitive performance in conventional supervised learning and
direct transfer to similar domains. Second, transfer learning techniques
enhanced workflow adaptability, with UDA being effective for homogeneous crop
classes while fine-tuning remains robust across diverse scenarios. Finally,
workflow choice depends heavily on the availability of labeled samples. With a
sufficient sample size, supervised training typically delivers more accurate
and generalizable results. Below a certain threshold, transfer learning that
matches the level of domain shift is a viable alternative to achieve crop
mapping. Repository:
Best-Practices-for-Large-Scale-Pixel-Wise-Crop-Mapping-and-Transfer-Learning-Workflows

</details>


### [31] [CT-ScanGaze: A Dataset and Baselines for 3D Volumetric Scanpath Modeling](https://arxiv.org/abs/2507.12591)
*Trong-Thang Pham,Akash Awasthi,Saba Khan,Esteban Duran Marti,Tien-Phat Nguyen,Khoa Vo,Minh Tran,Ngoc Son Nguyen,Cuong Tran Van,Yuki Ikebe,Anh Totti Nguyen,Anh Nguyen,Zhigang Deng,Carol C. Wu,Hien Van Nguyen,Ngan Le*

Main category: cs.CV

> Error

<details>
  <summary>Details</summary>

**Motivation:** Error

**Method:** Error

**Result:** Error

**Conclusion:** Error

**Abstract:** Understanding radiologists' eye movement during Computed Tomography (CT)
reading is crucial for developing effective interpretable computer-aided
diagnosis systems. However, CT research in this area has been limited by the
lack of publicly available eye-tracking datasets and the three-dimensional
complexity of CT volumes. To address these challenges, we present the first
publicly available eye gaze dataset on CT, called CT-ScanGaze. Then, we
introduce CT-Searcher, a novel 3D scanpath predictor designed specifically to
process CT volumes and generate radiologist-like 3D fixation sequences,
overcoming the limitations of current scanpath predictors that only handle 2D
inputs. Since deep learning models benefit from a pretraining step, we develop
a pipeline that converts existing 2D gaze datasets into 3D gaze data to
pretrain CT-Searcher. Through both qualitative and quantitative evaluations on
CT-ScanGaze, we demonstrate the effectiveness of our approach and provide a
comprehensive assessment framework for 3D scanpath prediction in medical
imaging.

</details>


### [32] [MS-DGCNN++: A Multi-Scale Fusion Dynamic Graph Neural Network with Biological Knowledge Integration for LiDAR Tree Species Classification](https://arxiv.org/abs/2507.12602)
*Said Ohamouddou,Abdellatif El Afia,Hanaa El Afia,Raddouane Chiheb*

Main category: cs.CV

> 本文介绍了MS-DGCNN++，一种分层多尺度融合动态图卷积网络方法，用于从地面LiDAR点云中进行树种分类，该方法在多种数据集上实现了高精度并优于现有的方法。

<details>
  <summary>Details</summary>

**Motivation:** 当前的方法使用多尺度动态图卷积网络（MS-DGCNN），但在捕捉树木结构层次间的语义关系方面存在不足。

**Method:** 通过多尺度具体特征工程，并在局部、枝干和树冠尺度上进行跨尺度信息传播，该论文提出了MS-DGCNN++，以更接近树自然结构的语义化表示替代了统一并行处理。

**Result:** 在STPCTLS数据集上，MS-DGCNN++达到了94.96%的分类精度，优于DGCNN、MS-DGCNN和最先进模型PPT；在FOR-species20K上也有不错的表现，准确率提升了6.1%；标准3D对象识别方面，该模型在ModelNet40上达到了93.15%，在ModelNet10上达到了94.05%，均优于DGCNN和MS-DGCNN。

**Conclusion:** 该方法不仅适用于树木分类，还可以扩展到标准3D对象识别，作为一个多种点云处理应用的多功能解决方案。

**Abstract:** Tree species classification from terrestrial LiDAR point clouds is
challenging because of the complex multi-scale geometric structures in forest
environments. Existing approaches using multi-scale dynamic graph convolutional
neural networks (MS-DGCNN) employ parallel multi-scale processing, which fails
to capture the semantic relationships between the hierarchical levels of the
tree architecture. We present MS-DGCNN++, a hierarchical multiscale fusion
dynamic graph convolutional network that uses semantically meaningful feature
extraction at local, branch, and canopy scales with cross-scale information
propagation. Our method employs scale-specific feature engineering, including
standard geometric features for the local scale, normalized relative vectors
for the branch scale, and distance information for the canopy scale. This
hierarchical approach replaces uniform parallel processing with semantically
differentiated representations that are aligned with the natural tree
structure. Under the same proposed tree species data augmentation strategy for
all experiments, MS-DGCNN++ achieved an accuracy of 94.96 \% on STPCTLS,
outperforming DGCNN, MS-DGCNN, and the state-of-the-art model PPT. On
FOR-species20K, it achieves 67.25\% accuracy (6.1\% improvement compared to
MS-DGCNN). For standard 3D object recognition, our method outperformed DGCNN
and MS-DGCNN with overall accuracies of 93.15\% on ModelNet40 and 94.05\% on
ModelNet10. With lower parameters and reduced complexity compared to
state-of-the-art transformer approaches, our method is suitable for
resource-constrained applications while maintaining a competitive accuracy.
Beyond tree classification, the method generalizes to standard 3D object
recognition, establishing it as a versatile solution for diverse point cloud
processing applications. The implementation code is publicly available at
https://github.com/said-ohamouddou/MS-DGCNN2.

</details>


### [33] [Predicting Soccer Penalty Kick Direction Using Human Action Recognition](https://arxiv.org/abs/2507.12617)
*David Freire-Obregón,Oliverio J. Santana,Javier Lorenzo-Navarro,Daniel Hernández-Sosa,Modesto Castrillón-Santana*

Main category: cs.CV

> 本研究发布了一个新的手动标注的足球点球数据集，用于预测射门方向。研究采用深度学习分类器进行评估，模型在预测射门方向上的精度高达63.9%，优于实际守门员的判断。

<details>
  <summary>Details</summary>

**Motivation:** 尽管人类行为识别中的动作预测在体育场景中的应用受到合适标注数据集的限制，本研究旨在通过构建一个新的足球点球数据集来突破这一限制。

**Method:** 本研究构建了一个手动标注的足球点球数据集，用于根据射门前的球员动作预测射门方向。提出了一种结合人类行为识别特征嵌入与上下文元数据的深度学习分类器来评估该数据集。

**Result:** 通过对七个架构家族中二十二个基础模型的评估，所提模型在预测射门方向（左或右）的准确率达到63.9%，优于实际守门员的表现。

**Conclusion:** 研究结果表明所构建的数据集对于预测性动作识别具有重要价值，并验证了所提模型作为体育领域预测任务通用化方法的潜力。

**Abstract:** Action anticipation has become a prominent topic in Human Action Recognition
(HAR). However, its application to real-world sports scenarios remains limited
by the availability of suitable annotated datasets. This work presents a novel
dataset of manually annotated soccer penalty kicks to predict shot direction
based on pre-kick player movements. We propose a deep learning classifier to
benchmark this dataset that integrates HAR-based feature embeddings with
contextual metadata. We evaluate twenty-two backbone models across seven
architecture families (MViTv2, MViTv1, SlowFast, Slow, X3D, I3D, C2D),
achieving up to 63.9% accuracy in predicting shot direction (left or right),
outperforming the real goalkeepers' decisions. These results demonstrate the
dataset's value for anticipatory action recognition and validate our model's
potential as a generalizable approach for sports-based predictive tasks.

</details>


### [34] [Funnel-HOI: Top-Down Perception for Zero-Shot HOI Detection](https://arxiv.org/abs/2507.12628)
*Sandipan Sarma,Agney Talwarr,Arijit Sur*

Main category: cs.CV

> 本文提出Funnel-HOI框架，通过编码阶段的特征挖掘实现更好的人-物交互检测，尤其是在零样本学习场景中，取得了显著性能提升。

<details>
  <summary>Details</summary>

**Motivation:** 现有的HOI框架主要集中在改进解码器设计上，以学习纠缠或非纠缠的交互解释。作者认为，为了获得更强的场景解释能力，HOI特有的线索应该在编码阶段就被预测。因此，提出了本文的创新框架和方法。

**Method:** 提出了一种名为Funnel-HOI的自顶向下的框架，首先探测图像中的物体（明确定义的概念），然后再探查与它们相关的动作（抽象的概念）。使用新颖的不对称共同注意机制利用多模态信息（包含零样本学习能力）来挖掘这些线索，并在编码阶段产生更强的交互表示。同时还设计了一种新的损失函数，考虑了对象-动作相关性，并比现有损失函数更好地调节分类错误惩罚，以指导交互分类器。

**Result:** 在HICO-DET和V-COCO数据集上进行的广泛实验，涵盖了全监督和六种零样本设置，显示了本文方法达到了当前最先进的性能，分别为未见过和罕见的HOI类别提升了12.4%和8.4%。

**Conclusion:** 本研究展示了通过在HOI预测中引入编码阶段的HOI特定线索来获得更强的场景解释能力，并通过实验验证了方法的有效性。

**Abstract:** Human-object interaction detection (HOID) refers to localizing interactive
human-object pairs in images and identifying the interactions. Since there
could be an exponential number of object-action combinations, labeled data is
limited - leading to a long-tail distribution problem. Recently, zero-shot
learning emerged as a solution, with end-to-end transformer-based object
detectors adapted for HOID becoming successful frameworks. However, their
primary focus is designing improved decoders for learning entangled or
disentangled interpretations of interactions. We advocate that HOI-specific
cues must be anticipated at the encoder stage itself to obtain a stronger scene
interpretation. Consequently, we build a top-down framework named Funnel-HOI
inspired by the human tendency to grasp well-defined concepts first and then
associate them with abstract concepts during scene understanding. We first
probe an image for the presence of objects (well-defined concepts) and then
probe for actions (abstract concepts) associated with them. A novel asymmetric
co-attention mechanism mines these cues utilizing multimodal information
(incorporating zero-shot capabilities) and yields stronger interaction
representations at the encoder level. Furthermore, a novel loss is devised that
considers objectaction relatedness and regulates misclassification penalty
better than existing loss functions for guiding the interaction classifier.
Extensive experiments on the HICO-DET and V-COCO datasets across
fully-supervised and six zero-shot settings reveal our state-of-the-art
performance, with up to 12.4% and 8.4% gains for unseen and rare HOI
categories, respectively.

</details>


### [35] [Reconstruct, Inpaint, Finetune: Dynamic Novel-view Synthesis from Monocular Videos](https://arxiv.org/abs/2507.12646)
*Kaihua Chen,Tarasha Khurana,Deva Ramanan*

Main category: cs.CV

> This paper presents a new approach, CogNVS, which synthesizes novel views of dynamic scenes from monocular videos using a combination of dynamic 3D reconstructions and a self-supervised 2D video diffusion model for inpainting.

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind this paper is to address the limitations of prior approaches, which either rely on costly test-time optimization of 4D representations or fail to preserve scene geometry when trained in a feed-forward manner for novel-view synthesis of dynamic scenes from monocular videos.

**Method:** Our approach involves three key steps: firstly, it reconstructs the dynamic 3D scene using covisible pixels visible in both input and target views. Secondly, it uses a feed-forward 2D video diffusion model to 'inpaint' hidden pixels in novel views. Lastly, our video inpainting diffusion model, CogNVS, can be self-supervised from 2D videos and is applied zero-shot to novel test videos through test-time finetuning.

**Result:** The empirical results show that CogNVS outperforms nearly all prior methods for synthesizing novel views of dynamic scenes from monocular videos.

**Conclusion:** The conclusion is that the proposed method, CogNVS, achieves superior performance for novel-view synthesis of dynamic scenes from monocular videos compared to previous approaches and can be applied in a zero-shot manner.

**Abstract:** We explore novel-view synthesis for dynamic scenes from monocular videos.
Prior approaches rely on costly test-time optimization of 4D representations or
do not preserve scene geometry when trained in a feed-forward manner. Our
approach is based on three key insights: (1) covisible pixels (that are visible
in both the input and target views) can be rendered by first reconstructing the
dynamic 3D scene and rendering the reconstruction from the novel-views and (2)
hidden pixels in novel views can be "inpainted" with feed-forward 2D video
diffusion models. Notably, our video inpainting diffusion model (CogNVS) can be
self-supervised from 2D videos, allowing us to train it on a large corpus of
in-the-wild videos. This in turn allows for (3) CogNVS to be applied zero-shot
to novel test videos via test-time finetuning. We empirically verify that
CogNVS outperforms almost all prior art for novel-view synthesis of dynamic
scenes from monocular videos.

</details>


### [36] [Integrated Oculomics and Lipidomics Reveal Microvascular Metabolic Signatures Associated with Cardiovascular Health in a Healthy Cohort](https://arxiv.org/abs/2507.12663)
*Inamullah,Ernesto Elias Vidal Rosas,Imran Razzak,Shoaib Jameel*

Main category: cs.CV

> 研究通过结合深度学习提取的视网膜微血管特征与血清脂质组学数据，发现了心血管疾病（CVD）新的非侵入性早期生物标志物，有助于早期发现和个性化干预。

<details>
  <summary>Details</summary>

**Motivation:** 传统的风险分层方法通常无法发现早期亚临床的CVD改变，而本研究整合了视网膜微血管特点和全面的血清脂质组学。

**Method:** 采用创新的成像组学框架，结合深度学习图像处理提取的视网膜微血管特征与血清脂质组学数据，对健康人群进行了大规模、共变量调整和分层的相关性分析。

**Result:** 研究表明视网膜表型，例如平均动脉宽度，血管密度与脂质亚类（如三酰甘油、二酰甘油和神经酰胺）存在强相关性。

**Conclusion:** 研究填补了对早期CVD发病机制理解的空缺，为CVD的早期发现、定向预防和个性化医疗提供了重要机会。

**Abstract:** Cardiovascular disease (CVD) remains the leading global cause of mortality,
yet current risk stratification methods often fail to detect early, subclinical
changes. Previous studies have generally not integrated retinal
microvasculature characteristics with comprehensive serum lipidomic profiles as
potential indicators of CVD risk. In this study, an innovative imaging omics
framework was introduced, combining retinal microvascular traits derived
through deep learning based image processing with serum lipidomic data to
highlight asymptomatic biomarkers of cardiovascular risk beyond the
conventional lipid panel. This represents the first large scale, covariate
adjusted and stratified correlation analysis conducted in a healthy population,
which is essential for identifying early indicators of disease. Retinal
phenotypes were quantified using automated image analysis tools, while serum
lipid profiling was performed by Ultra High Performance Liquid Chromatography
Electrospray ionization High resolution mass spectrometry (UHPLC ESI HRMS).
Strong, age- and sex-independent correlations were established, particularly
between average artery width, vessel density, and lipid subclasses such as
triacylglycerols (TAGs), diacylglycerols (DAGs), and ceramides (Cers). These
associations suggest a converging mechanism of microvascular remodeling under
metabolic stress. By linking detailed
  vascular structural phenotypes to specific lipid species, this study fills a
critical gap in the understanding of early CVD pathogenesis. This integration
not only offers a novel perspective on microvascular metabolic associations but
also presents a significant opportunity for the identification of robust,
non-invasive biomarkers. Ultimately, these findings may support improved early
detection, targeted prevention, and personalized approaches in cardiovascular
healthcare.

</details>


### [37] [FORTRESS: Function-composition Optimized Real-Time Resilient Structural Segmentation via Kolmogorov-Arnold Enhanced Spatial Attention Networks](https://arxiv.org/abs/2507.12675)
*Christina Thrainer,Md Meftahul Ferdaus,Mahdi Abdelguerfi,Christian Guetl,Steven Sloan,Kendall N. Niles,Ken Pathak*

Main category: cs.CV

> FORTRESS是用于平衡基础设施自动化结构缺陷分割精度和速度的新架构，通过减少91%的参数和近91%的计算复杂度，同时将推理速度提高了3倍，达到了卓越的效率。

<details>
  <summary>Details</summary>

**Motivation:** 研究动机是解决自动化结构缺陷分割在基础设施中面临的挑战，即在实现高精度的同时保持计算效率，以实现实时部署。

**Method:** FORTRESS采用特殊方法结合深度可分离卷积和自适应Kolmogorov-Arnold网络集成，该架构包含三项关键创新：系统化的深度可分离卷积框架，实现了每层3.6倍的参数减少，自适应TiKAN集成选择性地应用函数组合变换，以及跨解码器级别的多尺度注意力融合结合空间、通道和KAN增强特征。

**Result:** 在基础设施基准数据集上的评估显示，FORTRESS达到了最先进的结果，F1得分为0.771，平均IoU为0.677，显著优于现有的U-Net、SA-UNet和U-KAN方法。

**Conclusion:** 双优化策略对于最佳性能至关重要，FORTRESS作为一个强大解决方案，在资源受限环境中实现精确定位和计算效率并重的结构缺陷分割。

**Abstract:** Automated structural defect segmentation in civil infrastructure faces a
critical challenge: achieving high accuracy while maintaining computational
efficiency for real-time deployment. This paper presents FORTRESS
(Function-composition Optimized Real-Time Resilient Structural Segmentation), a
new architecture that balances accuracy and speed by using a special method
that combines depthwise separable convolutions with adaptive Kolmogorov-Arnold
Network integration. FORTRESS incorporates three key innovations: a systematic
depthwise separable convolution framework achieving a 3.6x parameter reduction
per layer, adaptive TiKAN integration that selectively applies function
composition transformations only when computationally beneficial, and
multi-scale attention fusion combining spatial, channel, and KAN-enhanced
features across decoder levels. The architecture achieves remarkable efficiency
gains with 91% parameter reduction (31M to 2.9M), 91% computational complexity
reduction (13.7 to 1.17 GFLOPs), and 3x inference speed improvement while
delivering superior segmentation performance. Evaluation on benchmark
infrastructure datasets demonstrates state-of-the-art results with an F1- score
of 0.771 and a mean IoU of 0.677, significantly outperforming existing methods
including U-Net, SA-UNet, and U- KAN. The dual optimization strategy proves
essential for optimal performance, establishing FORTRESS as a robust solution
for practical structural defect segmentation in resource-constrained
environments where both accuracy and computational efficiency are paramount.
Comprehensive architectural specifications are provided in the Supplemental
Material. Source code is available at URL:
https://github.com/faeyelab/fortress-paper-code.

</details>


### [38] [NeuraLeaf: Neural Parametric Leaf Models with Shape and Deformation Disentanglement](https://arxiv.org/abs/2507.12714)
*Yang Yang,Dongni Mao,Hiroaki Santo,Yasuyuki Matsushita,Fumio Okura*

Main category: cs.CV

> 本文开发了一种名为NeuraLeaf的神经参数模型，用于植物叶片的3D建模和重建，该模型能够对叶片的基础形状和变形进行学习，并有助于农业和计算机图形学的研究。

<details>
  <summary>Details</summary>

**Motivation:** 现有的关于人类和动物的神经参数模型研究较为丰富，但叶片由于其形状多样和灵活变形特性，需要一个专门的模型来处理。我们的目标是开发一种适用于植物叶片建模和重建的神经参数模型，以推动农业和计算机图形学领域的相关研究。

**Method:** NeuraLeaf模型利用2D叶片图像数据集学习叶片的基础形状，并使用一种无骨架蒙皮方法来模拟3D变形，同时提出一个名为DeformLeaf的新3D叶片数据集用于模型训练和测试。

**Result:** 本文提出了一种用于3D叶片建模和重建的神经参数模型NeuraLeaf，这是农业和计算机图形学中的重要组成部分。NeuraLeaf利用叶片摊平后的形状可以近似为2D平面的特点，将叶片的几何形状分为2D基础形状和3D变形两个部分。该模型能够利用丰富的2D叶片图像数据集进行学习，并且可以同时学习与几何形状一致的纹理。为了解决3D变形问题，我们提出了一种无骨架蒙皮（skeleton-free skinning）模型，并创建了一个新的3D叶片数据集DeformLeaf。实验表明，NeuraLeaf能够成功生成多种不同形状的叶片，并且其生成的模型能够准确地拟合3D观测数据（如深度图和点云）。实验代码和数据集可以在https://neuraleaf-yang.github.io/获取。

**Conclusion:** 这种基于神经参数模型的新型叶片建模方法为农业研究和计算机图形学领域中的植物建模和重建提供了一种精确的技术手段。

**Abstract:** We develop a neural parametric model for 3D leaves for plant modeling and
reconstruction that are essential for agriculture and computer graphics. While
neural parametric models are actively studied for humans and animals, plant
leaves present unique challenges due to their diverse shapes and flexible
deformation. To this problem, we introduce a neural parametric model for
leaves, NeuraLeaf. Capitalizing on the fact that flattened leaf shapes can be
approximated as a 2D plane, NeuraLeaf disentangles the leaves' geometry into
their 2D base shapes and 3D deformations. This representation allows learning
from rich sources of 2D leaf image datasets for the base shapes, and also has
the advantage of simultaneously learning textures aligned with the geometry. To
model the 3D deformation, we propose a novel skeleton-free skinning model and
create a newly captured 3D leaf dataset called DeformLeaf. We show that
NeuraLeaf successfully generates a wide range of leaf shapes with deformation,
resulting in accurate model fitting to 3D observations like depth maps and
point clouds. Our implementation and dataset are available at
https://neuraleaf-yang.github.io/.

</details>


### [39] [SOD-YOLO: Enhancing YOLO-Based Detection of Small Objects in UAV Imagery](https://arxiv.org/abs/2507.12727)
*Peijun Wang,Jinhua Zhao*

Main category: cs.CV

> This paper introduces SOD-YOLO, an enhanced YOLOv8-based model, which shows substantial improvements in detecting small objects by integrating multi-scale features, a high-resolution detection layer, and refined NMS, resulting in better performance on the VisDrone2019-DET dataset.

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind this paper is the persistent challenge of achieving accurate small object detection in surveillance and UAV imagery, which traditional object detection models often struggle with. The aim is to improve detection performance specifically for small objects.

**Method:** This paper addresses the challenge of small object detection by proposing a model called SOD-YOLO, which is an enhanced version of YOLOv8. It includes an Adaptive Scale Fusion (ASF) mechanism in the neck to boost multi-scale feature fusion, introduces a Small Object Detection Layer (P2) for higher-resolution feature maps, and uses Soft-NMS to refine confidence scores and ensure the retention of true positives.

**Result:** The experimental results show a significant improvement in the detection performance of the proposed model. Compared to the baseline model, SOD-YOLO achieves a 36.1% increase in mAP$_{50:95}$ and a 20.6% increase in mAP$_{50}$ on the VisDrone2019-DET dataset, highlighting its effectiveness in small object detection.

**Conclusion:** The conclusions drawn from the paper are that the proposed SOD-YOLO model significantly enhances the detection performance for small objects in UAV imagery. This improvement makes the model more practical and efficient for real-world applications involving small object detection, such as in UAV surveillance systems.

**Abstract:** Small object detection remains a challenging problem in the field of object
detection. To address this challenge, we propose an enhanced YOLOv8-based
model, SOD-YOLO. This model integrates an ASF mechanism in the neck to enhance
multi-scale feature fusion, adds a Small Object Detection Layer (named P2) to
provide higher-resolution feature maps for better small object detection, and
employs Soft-NMS to refine confidence scores and retain true positives.
Experimental results demonstrate that SOD-YOLO significantly improves detection
performance, achieving a 36.1% increase in mAP$_{50:95}$ and 20.6% increase in
mAP$_{50}$ on the VisDrone2019-DET dataset compared to the baseline model.
These enhancements make SOD-YOLO a practical and efficient solution for small
object detection in UAV imagery. Our source code, hyper-parameters, and model
weights are available at https://github.com/iamwangxiaobai/SOD-YOLO.

</details>


### [40] [A Privacy-Preserving Semantic-Segmentation Method Using Domain-Adaptation Technique](https://arxiv.org/abs/2507.12730)
*Homare Sueyoshi,Kiyoshi Nishikawa,Hitoshi Kiya*

Main category: cs.CV

> A privacy-preserving semantic-segmentation method that uses perceptual encryption and domain-adaptation on ViT embedding structures, maintaining model accuracy as confirmed through experiments with the Segmentation Transformer.

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to preserve privacy while maintaining the accuracy of models used for semantic segmentation.

**Method:** We propose a privacy-preserving semantic-segmentation method that applies perceptual encryption to images used for model training and test images. This method maintains high accuracy by employing a domain-adaptation technique on the embedding structure of the Vision Transformer (ViT).

**Result:** The method was tested using the Segmentation Transformer, and it was found to significantly maintain the accuracy of semantic segmentation, demonstrating its effectiveness.

**Conclusion:** The proposed method successfully achieves privacy preservation without sacrificing the accuracy of models used for semantic segmentation tasks.

**Abstract:** We propose a privacy-preserving semantic-segmentation method for applying
perceptual encryption to images used for model training in addition to test
images. This method also provides almost the same accuracy as models without
any encryption. The above performance is achieved using a domain-adaptation
technique on the embedding structure of the Vision Transformer (ViT). The
effectiveness of the proposed method was experimentally confirmed in terms of
the accuracy of semantic segmentation when using a powerful
semantic-segmentation model with ViT called Segmentation Transformer.

</details>


### [41] [Transformer-based Spatial Grounding: A Comprehensive Survey](https://arxiv.org/abs/2507.12739)
*Ijazul Haq,Muhammad Saqib,Yingjie Zhang*

Main category: cs.CV

> 本文通过系统文献回顾，概述了2018年至2025年间基于变换器的空间定位的研究进展，提供了关键方法论趋势和最佳实践，为研究人员和从业者开发稳健的、可靠的以及面向工业的应用程序提供指导。

<details>
  <summary>Details</summary>

**Motivation:** 尽管基于变换器模型的空间定位技术在多模态表示和跨模态对齐方面取得了显著的进步，但缺乏对当前方法的综合概述，特别是关于数据集使用、评估指标和工业应用的实用性方面的信息。

**Method:** 本研究采用系统性的文献回顾方法，分析了2018年至2025年间基于变换器的空间定位方法，重点在于模型架构、数据集、评估指标以及方法论趋势和最佳实践的识别。

**Result:** 研究结果包括对主导模型架构、常用数据集和广泛采用的评估指标的识别，揭示了关键的方法论趋势和最佳实践。

**Conclusion:** 通过这项研究，研究者能够获得关于基于变换器的空间定位方法的重要洞察，得到结构化的指导，有助于开发出稳健且适用于工业环境的模型。

**Abstract:** Spatial grounding, the process of associating natural language expressions
with corresponding image regions, has rapidly advanced due to the introduction
of transformer-based models, significantly enhancing multimodal representation
and cross-modal alignment. Despite this progress, the field lacks a
comprehensive synthesis of current methodologies, dataset usage, evaluation
metrics, and industrial applicability. This paper presents a systematic
literature review of transformer-based spatial grounding approaches from 2018
to 2025. Our analysis identifies dominant model architectures, prevalent
datasets, and widely adopted evaluation metrics, alongside highlighting key
methodological trends and best practices. This study provides essential
insights and structured guidance for researchers and practitioners,
facilitating the development of robust, reliable, and industry-ready
transformer-based spatial grounding models.

</details>


### [42] [Domain-Enhanced Dual-Branch Model for Efficient and Interpretable Accident Anticipation](https://arxiv.org/abs/2507.12755)
*Yanchen Guan,Haicheng Liao,Chengyue Wang,Bonan Wang,Jiaxun Zhang,Jia Hu,Zhenning Li*

Main category: cs.CV

> 本论文提出了一种采用双分支架构的交通事故预测框架，该框架通过大型模型（GPT-4o, Long-CLIP）结合前视摄像头视频和事故报告中的结构化文本数据，实现了高效精确的交通事故预测，并通过实验验证了其最新技术水平的预测准确性、响应时间、计算效率和可解释性的优越性。

<details>
  <summary>Details</summary>

**Motivation:** 为了进一步推动当代自动驾驶技术的发展和损失预防，需要开发精确且计算高效的交通事故预测系统。

**Method:** Structure

**Result:** {"tldr": "本论文提出了一种采用双分支架构的交通事故预测框架，该框架通过大型模型（GPT-4o, Long-CLIP）结合前视摄像头视频和事故报告中的结构化文本数据，实现了高效精确的交通事故预测，并通过实验验证了其最新技术水平的预测准确性、响应时间、计算效率和可解释性的优越性。", "motivation": "为了进一步推动当代自动驾驶技术的发展和损失预防，需要开发精确且计算高效的交通事故预测系统。", "method": "架构包括两个分支：一个用于处理前视摄像头视频的视觉信息，另一个用于处理事故报告的结构化文本数据。模型通过大型AI模型实现多模态输入的有效结合，并辅以针对性的提示工程策略。", "result": "在DAD, CCD 和 A3D等基准数据集上的综合评估验证了该方法在预测准确性、响应速度、计算效率和解释性方面的优越性。", "conclusion": "该研究成功建立了新的交通事故预测系统的最新水平标准，证明了双分支架构和大型模型的优越性能。"}

**Conclusion:** 该研究成功建立了新的交通事故预测系统的最新水平标准，证明了双分支架构和大型模型的优越性能。

**Abstract:** Developing precise and computationally efficient traffic accident
anticipation system is crucial for contemporary autonomous driving
technologies, enabling timely intervention and loss prevention. In this paper,
we propose an accident anticipation framework employing a dual-branch
architecture that effectively integrates visual information from dashcam videos
with structured textual data derived from accident reports. Furthermore, we
introduce a feature aggregation method that facilitates seamless integration of
multimodal inputs through large models (GPT-4o, Long-CLIP), complemented by
targeted prompt engineering strategies to produce actionable feedback and
standardized accident archives. Comprehensive evaluations conducted on
benchmark datasets (DAD, CCD, and A3D) validate the superior predictive
accuracy, enhanced responsiveness, reduced computational overhead, and improved
interpretability of our approach, thus establishing a new benchmark for
state-of-the-art performance in traffic accident anticipation.

</details>


### [43] [HairShifter: Consistent and High-Fidelity Video Hair Transfer via Anchor-Guided Animation](https://arxiv.org/abs/2507.12758)
*Wangzheng Shi,Yinglin Zheng,Yuxin Lin,Jianmin Bao,Ming Zeng,Dong Chen*

Main category: cs.CV

> 本研究介绍HairShifter框架，这是一种将高质量图像头发转移与平滑一致的视频动画结合的新方法。HairShifter解决了视频头发转移中的时间一致性和空间精确度问题，实现了卓越的视觉质量和时间一致性。

<details>
  <summary>Details</summary>

**Motivation:** 虽然单图像头发转移已取得显著进展，但在视频中进行头发转移难度较大，因为它需要在时间一致性、空间精确度和动态适应性之间找到平衡。

**Method:** 在本研究中，提出了名为HairShifter的新框架，该框架结合了单帧图像头发转移模块和多尺度门控SPADE解码器，以实现高质量的空间融合和时间一致性。

**Result:** 大量的实验表明，HairShifter在视频发型转移中实现了最先进的性能，结合了卓越的视觉质量、时间一致性和可扩展性。

**Conclusion:** 这项工作的目的是为基于视频的发型转移打开新的方向，并为此领域建立一个强大的基线。

**Abstract:** Hair transfer is increasingly valuable across domains such as social media,
gaming, advertising, and entertainment. While significant progress has been
made in single-image hair transfer, video-based hair transfer remains
challenging due to the need for temporal consistency, spatial fidelity, and
dynamic adaptability. In this work, we propose HairShifter, a novel "Anchor
Frame + Animation" framework that unifies high-quality image hair transfer with
smooth and coherent video animation. At its core, HairShifter integrates a
Image Hair Transfer (IHT) module for precise per-frame transformation and a
Multi-Scale Gated SPADE Decoder to ensure seamless spatial blending and
temporal coherence. Our method maintains hairstyle fidelity across frames while
preserving non-hair regions. Extensive experiments demonstrate that HairShifter
achieves state-of-the-art performance in video hairstyle transfer, combining
superior visual quality, temporal consistency, and scalability. The code will
be publicly available. We believe this work will open new avenues for
video-based hairstyle transfer and establish a robust baseline in this field.

</details>


### [44] [Unified Medical Image Segmentation with State Space Modeling Snake](https://arxiv.org/abs/2507.12760)
*Ruicheng Zhang,Haowei Guo,Kanghui Tian,Jun Zhou,Mingliang Yan,Zeyu Zhang,Shen Zhao*

Main category: cs.CV

> Mamba Snake, a novel deep snake framework with state space modeling for UMIS, improves segmentation efficacy by incorporating Mamba Evolution Block, energy map shape priors, and a dual-classification synergy mechanism, showing 3% better Dice scores.

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to address the challenges in UMIS due to the multi-scale structural heterogeneity, which conventional pixel-based approaches cannot effectively handle due to the lack of object-level anatomical insight and inter-organ relational modeling.

**Method:** Mamba Snake, a novel deep snake framework enhanced by state space modeling, is proposed for Unified Medical Image Segmentation (UMIS). It includes a snake-specific vision state space module called Mamba Evolution Block (MEB) for adaptive refinement of complex morphologies, and uses energy map shape priors and a dual-classification synergy mechanism to optimize detection and segmentation.

**Result:** Extensive evaluations across five clinical datasets revealed Mamba Snake's superior performance in UMIS, demonstrating an average Dice improvement of 3% over state-of-the-art methods.

**Conclusion:** The introduction of Mamba Snake, with its innovative approach using state space modeling and various optimization mechanisms, marks a significant advancement in UMIS, enhancing the accuracy and comprehensive assessment of anatomical structures in medical images.

**Abstract:** Unified Medical Image Segmentation (UMIS) is critical for comprehensive
anatomical assessment but faces challenges due to multi-scale structural
heterogeneity. Conventional pixel-based approaches, lacking object-level
anatomical insight and inter-organ relational modeling, struggle with
morphological complexity and feature conflicts, limiting their efficacy in
UMIS. We propose Mamba Snake, a novel deep snake framework enhanced by state
space modeling for UMIS. Mamba Snake frames multi-contour evolution as a
hierarchical state space atlas, effectively modeling macroscopic inter-organ
topological relationships and microscopic contour refinements. We introduce a
snake-specific vision state space module, the Mamba Evolution Block (MEB),
which leverages effective spatiotemporal information aggregation for adaptive
refinement of complex morphologies. Energy map shape priors further ensure
robust long-range contour evolution in heterogeneous data. Additionally, a
dual-classification synergy mechanism is incorporated to concurrently optimize
detection and segmentation, mitigating under-segmentation of microstructures in
UMIS. Extensive evaluations across five clinical datasets reveal Mamba Snake's
superior performance, with an average Dice improvement of 3\% over
state-of-the-art methods.

</details>


### [45] [Think-Before-Draw: Decomposing Emotion Semantics & Fine-Grained Controllable Expressive Talking Head Generation](https://arxiv.org/abs/2507.12761)
*Hanlei Shi,Leyuan Qu,Yu Liu,Di Gao,Yuhua Zheng,Taihao Li*

Main category: cs.CV

> 研究提出了一种名为Think-Before-Draw的框架，通过引入思维链方法将抽象的情感标签转化为面部肌肉运动描述，并使用逐级引导降噪策略对其进行了优化，从而提高了生成情感说话头像的自然性和精细度。实验表明该方法在MEAD和HDTF基准测试中达到了最先进的性能。

<details>
  <summary>Details</summary>

**Motivation:** 当前文本驱动的情感对话头像生成方法依赖于预定义的情感标签文本，这简化了真实的面部肌肉运动复杂性，未能实现自然的情感表达。为了克服这一限制并提高生成的情感对话头像的自然性，该研究提出了这一框架。

**Method:** Think-Before-Draw框架通过引入思维链方法将抽象的情感标签转化为面部肌肉运动描述，并采用类似肖像画的过程提出了逐级引导降噪策略，通过“全局情感定位-局部肌肉控制”机制优化微表情的动态。

**Result:** 实验结果表明该方法在MEAD和HDTF基准上达到了最先进的性能水平，并通过一组肖像图片验证了此模型的零样本生成能力。

**Conclusion:** 该研究提出的方法通过深入语义解析情感和精细表达能力优化，提高了文本驱动情感对话头像生成的自然性和动态精细度。

**Abstract:** Emotional talking-head generation has emerged as a pivotal research area at
the intersection of computer vision and multimodal artificial intelligence,
with its core value lying in enhancing human-computer interaction through
immersive and empathetic engagement.With the advancement of multimodal large
language models, the driving signals for emotional talking-head generation has
shifted from audio and video to more flexible text. However, current
text-driven methods rely on predefined discrete emotion label texts,
oversimplifying the dynamic complexity of real facial muscle movements and thus
failing to achieve natural emotional expressiveness.This study proposes the
Think-Before-Draw framework to address two key challenges: (1) In-depth
semantic parsing of emotions--by innovatively introducing Chain-of-Thought
(CoT), abstract emotion labels are transformed into physiologically grounded
facial muscle movement descriptions, enabling the mapping from high-level
semantics to actionable motion features; and (2) Fine-grained expressiveness
optimization--inspired by artists' portrait painting process, a progressive
guidance denoising strategy is proposed, employing a "global emotion
localization--local muscle control" mechanism to refine micro-expression
dynamics in generated videos.Our experiments demonstrate that our approach
achieves state-of-the-art performance on widely-used benchmarks, including MEAD
and HDTF. Additionally, we collected a set of portrait images to evaluate our
model's zero-shot generation capability.

</details>


### [46] [World Model-Based End-to-End Scene Generation for Accident Anticipation in Autonomous Driving](https://arxiv.org/abs/2507.12762)
*Yanchen Guan,Haicheng Liao,Chengyue Wang,Xingcheng Liu,Jiaxun Zhang,Zhenning Li*

Main category: cs.CV

> 本文提出一种综合框架，通过生成增强学习和改进的时间推理技术，显著提升了自动驾驶系统的事故预测能力和时间，并发布了一个新的数据分析集。

<details>
  <summary>Details</summary>

**Motivation:** 当前的自动驾驶系统在交通事故预测方面面临两个根本挑战：高质量多样训练数据的稀缺性以及由于环境干扰或传感器缺陷导致的关键对象级别线索的缺失。本文旨在通过提出一种新的框架来解决这些问题。

**Method:** 本研究提出了一种结合生成场景增强与自适应时间推理的综合框架。首先，通过世界模型，基于领域引导提示，开发了一种视频生成流程，以创建高分辨率的、统计一致的驾驶场景，特别是丰富了边缘情况和复杂交互的覆盖。其次，构建了一个动态预测模型，通过加强的图卷积和膨胀的时间运算符编码时空关系，有效处理了数据不完整性和暂时的视觉噪声。此外，还发布了一个新的基准数据集，旨在更好地捕捉多样化的驾驶风险。

**Result:** 实验结果表明，本研究提出的框架在公共数据集和新发布数据集上大大提高了事故预测的准确性和预测时间。

**Conclusion:** 研究证明了所提出框架解决当前自动驾驶应用中数据和建模限制的能力，并提供了一种应对安全性关键应用的强健解决方案。

**Abstract:** Reliable anticipation of traffic accidents is essential for advancing
autonomous driving systems. However, this objective is limited by two
fundamental challenges: the scarcity of diverse, high-quality training data and
the frequent absence of crucial object-level cues due to environmental
disruptions or sensor deficiencies. To tackle these issues, we propose a
comprehensive framework combining generative scene augmentation with adaptive
temporal reasoning. Specifically, we develop a video generation pipeline that
utilizes a world model guided by domain-informed prompts to create
high-resolution, statistically consistent driving scenarios, particularly
enriching the coverage of edge cases and complex interactions. In parallel, we
construct a dynamic prediction model that encodes spatio-temporal relationships
through strengthened graph convolutions and dilated temporal operators,
effectively addressing data incompleteness and transient visual noise.
Furthermore, we release a new benchmark dataset designed to better capture
diverse real-world driving risks. Extensive experiments on public and newly
released datasets confirm that our framework enhances both the accuracy and
lead time of accident anticipation, offering a robust solution to current data
and modeling limitations in safety-critical autonomous driving applications.

</details>


### [47] [Continuous Marine Tracking via Autonomous UAV Handoff](https://arxiv.org/abs/2507.12763)
*Heegyeong Kim,Alice James,Avishkar Seth,Endrowednes Kuantama,Jane Williamson,Yimeng Feng,Richard Han*

Main category: cs.CV

> 该研究介绍了一种自主无人机视觉系统，专门针对在动态海洋环境中连续实时跟踪鲨鱼。系统成功展示了在复杂条件下的跟踪能力和无人机间的任务交接，为扩展的海洋动物跟踪提供了可能。

<details>
  <summary>Details</summary>

**Motivation:** 研究动机在于解决动态海洋环境中对鲨鱼进行连续实时跟踪的挑战，并扩展无人机的运行范围。这项研究为可扩展的自主监测奠定了基础，并证实了协同无人机操作用于延长海洋跟踪的可行性。

**Method:** 介绍了一种用于在动态海洋环境中连续实时跟踪海洋动物（特别是鲨鱼）的自主UAV视觉系统。该系统集成了机载计算机、稳定化的RGB-D摄像头和一个定制训练的OSTrack管道，在具有挑战性的光照、遮挡和海况条件下实现视觉识别。关键创新是无人机之间的跟踪任务交接协议，它可以在单个无人机的电池限制之外扩展操作范围。

**Result:** 在精心挑选的鲨鱼数据集（共5200帧）上进行了性能评估，实际飞行控制期间的跟踪成功率达到了81.9%，抗遮挡、光照变化和背景杂波的鲁棒性也得到了验证。此外，通过高置信度特征匹配实现目标转移，目标覆盖率达到了82.9%。

**Conclusion:** 该研究为通过协同无人机操作进行延长的海洋跟踪提供了可能性，并为可扩展的自主监测奠定了基础。

**Abstract:** This paper introduces an autonomous UAV vision system for continuous,
real-time tracking of marine animals, specifically sharks, in dynamic marine
environments. The system integrates an onboard computer with a stabilised RGB-D
camera and a custom-trained OSTrack pipeline, enabling visual identification
under challenging lighting, occlusion, and sea-state conditions. A key
innovation is the inter-UAV handoff protocol, which enables seamless transfer
of tracking responsibilities between drones, extending operational coverage
beyond single-drone battery limitations. Performance is evaluated on a curated
shark dataset of 5,200 frames, achieving a tracking success rate of 81.9\%
during real-time flight control at 100 Hz, and robustness to occlusion,
illumination variation, and background clutter. We present a seamless UAV
handoff framework, where target transfer is attempted via high-confidence
feature matching, achieving 82.9\% target coverage. These results confirm the
viability of coordinated UAV operations for extended marine tracking and lay
the groundwork for scalable, autonomous monitoring.

</details>


### [48] [AnyPos: Automated Task-Agnostic Actions for Bimanual Manipulation](https://arxiv.org/abs/2507.12768)
*Hengkai Tan,Yao Feng,Xinyi Mao,Shuhe Huang,Guodong Liu,Zhongkai Hao,Hang Su,Jun Zhu*

Main category: cs.CV

> 该研究提出了一种新的不依赖任务的动作范式来解决数据收集难题，提高了复杂操作任务中的表现。

<details>
  <summary>Details</summary>

**Motivation:** 传统视觉语言动作模型在复杂场景中表现出色，但其大量依赖于特定任务的人类演示，限制了泛化能力，并且增加了数据收集成本。因此，该研究旨在通过提出一个新的不依赖任务的动作范式来解决这些问题，提高效率并降低成本。

**Method:** 该研究提出了一个新的不依赖任务的动作范式ATARA（Automated Task-Agnostic Random Actions），该范式可以加速数据收集过程，同时引入了AnyPos，一个配备有肢解耦估计和方向感知解码器的逆动力学模型，以及一个基于视频条件的动作验证模块以验证学习策略的可行性。

**Result:** 实验结果显示，AnyPos- ATARA流水线在测试准确性上提高了51％，在抓取、放置和点击等下游任务的成功率上提高了30-40％。

**Conclusion:** 通过该新范式及其工具的使用，可以高效地从不依赖任务的数据中学习，从而提高在多种操作任务中的表现。

**Abstract:** Vision-language-action (VLA) models have shown promise on task-conditioned
control in complex settings such as bimanual manipulation. However, the heavy
reliance on task-specific human demonstrations limits their generalization and
incurs high data acquisition costs. In this work, we present a new notion of
task-agnostic action paradigm that decouples action execution from
task-specific conditioning, enhancing scalability, efficiency, and
cost-effectiveness. To address the data collection challenges posed by this
paradigm -- such as low coverage density, behavioral redundancy, and safety
risks -- we introduce ATARA (Automated Task-Agnostic Random Actions), a
scalable self-supervised framework that accelerates collection by over $
30\times $ compared to human teleoperation. To further enable effective
learning from task-agnostic data, which often suffers from distribution
mismatch and irrelevant trajectories, we propose AnyPos, an inverse dynamics
model equipped with Arm-Decoupled Estimation and a Direction-Aware Decoder
(DAD). We additionally integrate a video-conditioned action validation module
to verify the feasibility of learned policies across diverse manipulation
tasks. Extensive experiments show that the AnyPos-ATARA pipeline yields a 51%
improvement in test accuracy and achieves 30-40% higher success rates in
downstream tasks such as lifting, pick-and-place, and clicking, using
replay-based video validation. Project Page:
https://embodiedfoundation.github.io/vidar_anypos

</details>
