<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 35]
- [cs.CV](#cs.CV) [Total: 32]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [StreetMath: Study of LLMs' Approximation Behaviors](https://arxiv.org/abs/2510.25776)
*Chiung-Yi Tseng,Somshubhra Roy,Maisha Thasin,Danyang Zhang,Blessing Effiong*

Main category: cs.CL

> 研究提出了StreetMath基准测试，评估LLMs的近似推理能力，并指出这些模型在需要近似计算时倾向于进行精确计算。

<details>
  <summary>Details</summary>

**Motivation:** 大量文献探讨了大型语言模型的数学推理能力，特别是它们在自回归架构中的精确算术运算中的表现，但它们在非正式、快速数学运算中的近似推理能力却关注较少，尤其是在非自回归解码器模型中。

**Method:** 本研究提出了StreetMath基准测试，用于评估大语言模型在现实世界近似数学情景下的近似推理能力。研究在不同架构的大型语言模型中进行广泛的评估，并使用机制可解释性技术探查模型的内部计算状态。

**Result:** 分析显示，大型语言模型倾向于计算精确值或调用外部工具来处理近似计算任务，即使这些任务要求近似处理。虽然模型有时在早期层或步骤中得出正确答案，但在解决近似任务时仍消耗更多的标记。additional experiments indicate that exact and approximate arithmetic operations rely on largely separate neural components.

**Conclusion:** 基于认知心理学研究，我们提出LLMs在街头数学环境中并不表现出人类的那种认知吝啬。

**Abstract:** There is a substantial body of literature examining the mathematical
reasoning capabilities of large language models (LLMs), particularly their
performance on precise arithmetic operations in autoregressive architectures.
However, their ability to perform approximate reasoning in informal, fast-paced
mathematical operations has received far less attention, especially among
non-autoregressive decoder models. Our work addresses this gap by introducing
StreetMath, a benchmark designed to evaluate models' approximation abilities
under real-world approximation scenarios. We conduct extensive evaluations
across different LLM architectures: Qwen3-4B-Instruct-2507,
Qwen3-4B-Thinking-2507, Dream-v0-Instruct-7B, Falcon-Mamba-7B-Instruct, and
Mamba-GPT-3B. Furthermore, we apply mechanistic interpretability techniques to
probe their internal computational states. Our analysis reveals that LLMs
generally attempt to compute exact values or invoke external tools even in
tasks that call for approximation. Moreover, while models sometimes reach the
correct answer in early layers or steps, they still consume more tokens when
solving approximation tasks. Additional experiments indicate that exact and
approximate arithmetic operations rely on largely separate neural components.
Drawing upon research on cognitive psychology, we argue that LLMs do not
exhibit cognitive miserliness in the same way humans do in street math
settings. We open source our work https://github.com/ctseng777/StreetMath

</details>


### [2] [Review Based Entity Ranking using Fuzzy Logic Algorithmic Approach: Analysis](https://arxiv.org/abs/2510.25778)
*Pratik N. Kalamkar,Anupama G. Phakatkar*

Main category: cs.CL

> 本文提出了一种方法，通过结合描述感兴趣产品属性的意见词（如副词、形容词、名词和动词），并利用模糊逻辑算法和句法依赖解析，将意见词分类为不同类别（非常弱、弱、中等、非常强、强），从而根据评论的方向和强度以及用户的查询对实体进行排序。

<details>
  <summary>Details</summary>

**Motivation:** 整体词汇方法没有考虑每个意见的强度，仅仅关注了意见的方向。为了更准确地反映意见的强度，本文旨在改进意见挖掘的方法。

**Method:** 结合描述产品属性的意见词，使用模糊逻辑算法对意见词进行分类，通过句法依赖解析找到有关方面词汇的关系，以计算实体在评论中的分数。

**Result:** 通过结合意见词并根据其强度分类，使得对实体的评价更加细化和准确，可以帮助更精确地对实体进行排序。

**Conclusion:** 所提出的方法能够更细致地评估和排序实体，不仅考虑到意见的方向，也考虑到了意见的强度。

**Abstract:** Opinion mining, also called sentiment analysis, is the field of study that
analyzes people opinions, sentiments, evaluations, appraisals, attitudes, and
emotions towards entities such as products, services, organizations,
individuals, issues, events, topics, and their attributes. Holistic
lexicon-based approach does not consider the strength of each opinion, i.e.,
whether the opinion is very strongly negative (or positive), strongly negative
(or positive), moderate negative (or positive), very weakly negative (or
positive) and weakly negative (or positive). In this paper, we propose approach
to rank entities based on orientation and strength of the entity reviews and
user's queries by classifying them in granularity levels (i.e. very weak, weak,
moderate, very strong and strong) by combining opinion words (i.e. adverb,
adjective, noun and verb) that are related to aspect of interest of certain
product. We shall use fuzzy logic algorithmic approach in order to classify
opinion words into different category and syntactic dependency resolution to
find relations for desired aspect words. Opinion words related to certain
aspects of interest are considered to find the entity score for that aspect in
the review.

</details>


### [3] [LASTIST: LArge-Scale Target-Independent STance dataset](https://arxiv.org/abs/2510.25783)
*DongJae Kim,Yaejin Lee,Minsu Park,Eunil Park*

Main category: cs.CL

> 本文提供了一个名为LASTIST的大规模韩语无目标依存立场检测数据集，适用于多种立场检测任务，填补了低资源语言立场检测研究的空白。

<details>
  <summary>Details</summary>

**Motivation:** 当前立场检测研究大多集中在目标依存立场检测上，且大多数基准数据集基于英语，限制了在诸如韩语等低资源语言中模型的发展。为了应对这一挑战，本研究提出了一个新的数据集。

**Method:** 本文提出了一种名为LArge-Scale Target-Independent STance (LASTIST) 的大规模无目标依存立场数据集，填补了研究空白。该数据集是从韩国政党的新闻稿中收集的，包含563,299个标注好的韩语句子。

**Result:** LASTIST 数据集适用于各种立场检测任务，包括无目标依存立场检测和历时立场检测，并且已经在 https://anonymous.4open.science/r/LASTIST-3721/ 上部署。

**Conclusion:** 本研究通过提供LASTIST 数据集，为立场检测领域提供了重要资源，尤其对于韩语语言的立场检测研究具有推动意义。

**Abstract:** Stance detection has emerged as an area of research in the field of
artificial intelligence. However, most research is currently centered on the
target-dependent stance detection task, which is based on a person's stance in
favor of or against a specific target. Furthermore, most benchmark datasets are
based on English, making it difficult to develop models in low-resource
languages such as Korean, especially for an emerging field such as stance
detection. This study proposes the LArge-Scale Target-Independent STance
(LASTIST) dataset to fill this research gap. Collected from the press releases
of both parties on Korean political parties, the LASTIST dataset uses 563,299
labeled Korean sentences. We provide a detailed description of how we collected
and constructed the dataset and trained state-of-the-art deep learning and
stance detection models. Our LASTIST dataset is designed for various tasks in
stance detection, including target-independent stance detection and diachronic
evolution stance detection. We deploy our dataset on
https://anonymous.4open.science/r/LASTIST-3721/.

</details>


### [4] [zFLoRA: Zero-Latency Fused Low-Rank Adapters](https://arxiv.org/abs/2510.25784)
*Dhananjaya Gowda,Seoha Song,Harshith Goka,Junhyun Lee*

Main category: cs.CL

> 研究提出了一种无延迟融合低秩适配器（zFLoRA），大大降低了大语言模型在推理时的延迟开销。

<details>
  <summary>Details</summary>

**Motivation:** 随着大语言模型（LLMs）的应用，任务特定的适配器成为了常见配置。尽管适配器参数数量通常不足基础模型的 1%，但在推理时间上，其额外计算量可高达基础模型的 2.5 倍。本研究旨在降低这种延迟以提高推理效率。

**Method:** 本研究提出了一种新的无延迟融合低秩适配器（zFLoRA），旨在在基础模型之上引入零或可忽略的延迟开销。

**Result:** 实验结果显示，zFLoRA 在 1B、3B 和 7B 规模的大语言模型上，与流行的监督微调基准（包括低秩适配器 LoRA 以及全量微调 FFT）相比表现更优。通过在三种不同类别的 18 项任务（常识推理、数学推理和总结对话）上进行实验验证，并在 NPU（三星 Galaxy S25+）和 GPU（NVIDIA H100）平台上测量延迟，表明 zFLoRA 适配器引入几乎无延迟开销。

**Conclusion:** zFLoRA 的无延迟或可忽略延迟特性表明它是一个有潜力的适配器方案，可以在不牺牲模型性能的同时提高推理效率。

**Abstract:** Large language models (LLMs) are increasingly deployed with task-specific
adapters catering to multiple downstream applications. In such a scenario, the
additional compute associated with these apparently insignificant number of
adapter parameters (typically less than 1% of the base model) turns out to be
disproportionately significant during inference time (upto 2.5x times that of
the base model). In this paper, we propose a new zero-latency fused low-rank
adapter (zFLoRA) that introduces zero or negligible latency overhead on top of
the base model. Experimental results on LLMs of size 1B, 3B and 7B show that
zFLoRA compares favorably against the popular supervised fine-tuning benchmarks
including low-rank adapters (LoRA) as well as full fine-tuning (FFT).
Experiments are conducted on 18 different tasks across three different
categories namely commonsense reasoning, math reasoning and summary-dialogue.
Latency measurements made on NPU (Samsung Galaxy S25+) as well as GPU (NVIDIA
H100) platforms show that the proposed zFLoRA adapters introduce zero to
negligible latency overhead.

</details>


### [5] [BlackboxNLP-2025 MIB Shared Task: Improving Circuit Faithfulness via Better Edge Selection](https://arxiv.org/abs/2510.25786)
*Yaniv Nikankin,Dana Arad,Itay Itzhak,Anja Reusch,Adi Simhi,Gal Kesten-Pomeranz,Yonatan Belinkov*

Main category: cs.CL

> 通过三项关键改进提高电路发现过程，以更好地理解和解释模型的工作方式。

<details>
  <summary>Details</summary>

**Motivation:** 解决机制可解释性的主要挑战之一即电路发现，识别模型中执行给定任务的部分。

**Method:** 通过使用自助法来识别具有稳定归因分数的边，引入基于比率的选择策略以及使用整数线性规划取代标准贪婪选择，改进了回路发现过程。

**Result:** 新的方法得到了更忠实的电路，并且在多个MIB任务和模型中优于先前的方法。

**Conclusion:** 提出的改进方法能够更好地识别模型中的电路，提升了模型的可解释性和性能。

**Abstract:** One of the main challenges in mechanistic interpretability is circuit
discovery, determining which parts of a model perform a given task. We build on
the Mechanistic Interpretability Benchmark (MIB) and propose three key
improvements to circuit discovery. First, we use bootstrapping to identify
edges with consistent attribution scores. Second, we introduce a simple
ratio-based selection strategy to prioritize strong positive-scoring edges,
balancing performance and faithfulness. Third, we replace the standard greedy
selection with an integer linear programming formulation. Our methods yield
more faithful circuits and outperform prior approaches across multiple MIB
tasks and models. Our code is available at:
https://github.com/technion-cs-nlp/MIB-Shared-Task.

</details>


### [6] [LISTEN to Your Preferences: An LLM Framework for Multi-Objective Selection](https://arxiv.org/abs/2510.25799)
*Adam S. Jovine,Tinghan Ye,Francis Bahk,Jingjing Wang,David B. Shmoys,Peter I. Frazier*

Main category: cs.CL

> 本研究介绍了一种框架LISTEN，利用大型语言模型作为偏好预言机，以解决多目标决策中人类专家选择难题，提出了两种算法用于应对模型限制问题。

<details>
  <summary>Details</summary>

**Motivation:** 专家们常在选择一项最佳方案时遇到困难，特别是在面对多目标竞争的问题上，重点在于难以形式化复杂的、隐含的偏好。本研究旨在解决这一问题。

**Method:** 本研究提出了LISTEN框架，利用大型语言模型(LLM)作为零次学习偏好预言机，基于专家的高层次自然语言优先级进行指导。为了适应LLM的限制，如上下文窗口和推理成本，研究提出了两个迭代算法：LISTEN-U，该算法利用LLM来精炼一个参数化效用函数；LISTEN-T，则是一次对小批量解进行锦标赛式选择的非参数化方法。

**Result:** 研究在航班预订、购物和考试安排等多样化任务上进行了评估，结果显示，当偏好在参数上一致时，LISTEN-U表现更佳（这一特性通过一个新颖的符合度指标进行测量），而LISTEN-T在性能上更稳定。

**Conclusion:** 本研究探索了一种有前景的方向，即通过自然语言直接指导复杂的多目标决策，减少传统偏好激发的认知负担。

**Abstract:** Human experts often struggle to select the best option from a large set of
items with multiple competing objectives, a process bottlenecked by the
difficulty of formalizing complex, implicit preferences. To address this, we
introduce LISTEN, a framework that leverages a Large Language Model (LLM) as a
zero-shot preference oracle, guided only by an expert's high-level priorities
in natural language. To operate within LLM constraints like context windows and
inference costs, we propose two iterative algorithms: LISTEN-U, which uses the
LLM to refine a parametric utility function, and LISTEN-T, a non-parametric
method that performs tournament-style selections over small batches of
solutions. Evaluated on diverse tasks including flight booking, shopping, and
exam scheduling, our results show LISTEN-U excels when preferences are
parametrically aligned (a property we measure with a novel concordance metric),
while LISTEN-T offers more robust performance. This work explores a promising
direction for steering complex multi-objective decisions directly with natural
language, reducing the cognitive burden of traditional preference elicitation.

</details>


### [7] [Beyond Length: Quantifying Long-Range Information for Long-Context LLM Pretraining Data](https://arxiv.org/abs/2510.25804)
*Haoran Deng,Yingyu Lin,Zhenghao Lin,Xiao Liu,Yizhou Sun,Yi-An Ma,Yeyun Gong*

Main category: cs.CL

> LongFilter是一个精选长文本预训练数据的框架，通过对长距离依赖重要性的评估来挑选关键样本，从而改善模型在各种基准测试中的表现。

<details>
  <summary>Details</summary>

**Motivation:** 为解决长文本语言模型训练效率低下的问题，作者设计了LongFilter框架，专门筛选高质量训练数据。

**Method:** LongFilter通过对比长文本和短文本模型预测结果，挑选出含有关键长距离依赖的数据样本。

**Result:** {
  "tldr": "LongFilter is a framework designed to curate training data for long-context pretraining by identifying samples with essential long-range dependencies, improving model performance on various benchmarks.", 
  "motivation": "To address the inefficiency of training long-context language models on data lacking meaningful long-distance dependencies, the authors introduce LongFilter to filter and curate high-quality training data.", 
  "method": "LongFilter measures the gain of long context over short context in model predictions and selects samples where long-range dependencies are crucial for prediction.", 
  "result": "Experiments with LLaMA-3-8B showed that LongFilter efficiently selects data and improves performance on benchmarks such as HELMET, LongBench, and RULER.", 
  "conclusion": "The introduction of LongFilter for data curation leads to better and more efficient long-context language modeling training. This enhances the model's performance on reasoning, code generation, and document summarization tasks."]}

**Conclusion:** LongFilter有效改善了长文本训练数据的选择，提升了语言模型在推理、代码生成和文档总结任务上的性能。

**Abstract:** Long-context language models unlock advanced capabilities in reasoning, code
generation, and document summarization by leveraging dependencies across
extended spans of text. However, a significant portion of readily available
long-text data lacks meaningful long-distance dependencies; most spans can be
predicted using only local context. Training on such data is inefficient,
making careful data selection crucial. Therefore, we introduce LongFilter, a
framework for curating training data tailored to long-context pretraining.
LongFilter measures the information gain provided by extended context by
contrasting model predictions under long-context versus short-context settings,
thereby identifying samples where long-range dependencies are essential.
Experiments with LLaMA-3-8B, extending its context length from 8K to 64K, show
that LongFilter efficiently selects high-quality data and yields substantial
improvements on benchmarks such as HELMET, LongBench, and RULER.

</details>


### [8] [Ideology-Based LLMs for Content Moderation](https://arxiv.org/abs/2510.25805)
*Stefano Civelli,Pietro Bernardelle,Nardiena A. Pratama,Gianluca Demartini*

Main category: cs.CL

> 研究发现，不同的意识形态人格设定会对大型语言模型的有害内容分类产生微妙的影响，特别是意识形态一致性会随着模型的大小增加。这种影响可能会导致在中立名义下的AI系统在无意识中强化党派观点。

<details>
  <summary>Details</summary>

**Motivation:** 研究动机在于探讨在内容审核系统中使用大型语言模型时，保证公平性和中立性的重要性。这里特别关注不同的人格设定对LLM架构、模型大小以及内容模态（语言和视觉）的影响。

**Method:** 研究首先分析了不同意识形态的人格设定在有害内容分类上的倾向性。接着，通过一致性分析显示了模型（尤其是较大的模型）与相同意识形态的人格设定更加一致，导致在意识形态群体之间的差异增大。最后，通过一个政治针对性任务研究进一步验证了这些发现。

**Result:** 该研究通过分析不同的人格设定如何影响大型语言模型（LLMs）对有害内容分类的一致性和公平性，提出了一个新的视角。尽管表面上看来，人格设定对整体分类准确性影响不大，但深入分析却发现模型对有害内容的识别倾向随着人格设定的意识形态偏好而变化。特别是较大的模型，在与相同意识形态的人格设定下显示出更高的一致性，而与不同意识形态的人格设定下则呈现出更大的分歧。这表明，人格设定可能会引入微妙的意识形态偏见到LLM的输出中，从而引起对可能在中立名义下强化党派观点的AI系统的担忧。

**Conclusion:** 研究结论指出，尽管人格设定可能会对危害内容的分类准确性影响较小，但是他们从本质上给LLM带入了潜在的意识形态偏见。这一发现引发了关于在中立的名义下AIS系统可能强化党派观点的担忧。

**Abstract:** Large language models (LLMs) are increasingly used in content moderation
systems, where ensuring fairness and neutrality is essential. In this study, we
examine how persona adoption influences the consistency and fairness of harmful
content classification across different LLM architectures, model sizes, and
content modalities (language vs. vision). At first glance, headline performance
metrics suggest that personas have little impact on overall classification
accuracy. However, a closer analysis reveals important behavioral shifts.
Personas with different ideological leanings display distinct propensities to
label content as harmful, showing that the lens through which a model "views"
input can subtly shape its judgments. Further agreement analyses highlight that
models, particularly larger ones, tend to align more closely with personas from
the same political ideology, strengthening within-ideology consistency while
widening divergence across ideological groups. To show this effect more
directly, we conducted an additional study on a politically targeted task,
which confirmed that personas not only behave more coherently within their own
ideology but also exhibit a tendency to defend their perspective while
downplaying harmfulness in opposing views. Together, these findings highlight
how persona conditioning can introduce subtle ideological biases into LLM
outputs, raising concerns about the use of AI systems that may reinforce
partisan perspectives under the guise of neutrality.

</details>


### [9] [Beyond Long Context: When Semantics Matter More than Tokens](https://arxiv.org/abs/2510.25816)
*Tarun Kumar Chawdhury,Jon D. Duke*

Main category: cs.CL

> Lopez等人提出的CLEAR方法在临床自然语言处理中通过实体感知检索提高了效率和准确性，优于传统方法。

<details>
  <summary>Details</summary>

**Motivation:** 传统向量数据库方法往往忽视细微的临床关系，而电子健康记录(EHR)将临床文档作为base64编码附件存储，使其语义问答变得困难。

**Method:** CLEAR方法使用实体感知检索，相比基于嵌入的检索在F1分数上提高了0.04，同时减少了70%的令牌使用。开发了一个临床笔记QA评估平台来验证CLEAR方法。

**Result:** 在12份临床笔记上测试，CLEAR方法取得了58.3%的胜率，平均语义相似度为0.878，使用87%更少的令牌。对于超过65,000个令牌的文档，胜率为75%。

**Conclusion:** 实体感知检索改进了临床自然语言处理的效率和准确性，提供的评估框架有助于评估语义精度和计算效率关键的临床问答系统。

**Abstract:** Electronic Health Records (EHR) store clinical documentation as base64
encoded attachments in FHIR DocumentReference resources, which makes semantic
question answering difficult. Traditional vector database methods often miss
nuanced clinical relationships. The Clinical Entity Augmented Retrieval (CLEAR)
method, introduced by Lopez et al. 2025, uses entity aware retrieval and
achieved improved performance with an F1 score of 0.90 versus 0.86 for
embedding based retrieval, while using over 70 percent fewer tokens. We
developed a Clinical Notes QA Evaluation Platform to validate CLEAR against
zero shot large context inference and traditional chunk based retrieval
augmented generation. The platform was tested on 12 clinical notes ranging from
10,000 to 65,000 tokens representing realistic EHR content. CLEAR achieved a
58.3 percent win rate, an average semantic similarity of 0.878, and used 78
percent fewer tokens than wide context processing. The largest performance
gains occurred on long notes, with a 75 percent win rate for documents
exceeding 65,000 tokens. These findings confirm that entity aware retrieval
improves both efficiency and accuracy in clinical natural language processing.
The evaluation framework provides a reusable and transparent benchmark for
assessing clinical question answering systems where semantic precision and
computational efficiency are critical.

</details>


### [10] [A Survey on Efficient Large Language Model Training: From Data-centric Perspectives](https://arxiv.org/abs/2510.25817)
*Junyu Luo,Bohan Wu,Xiao Luo,Zhiping Xiao,Yiqiao Jin,Rong-Cheng Tu,Nan Yin,Yifan Wang,Jingyang Yuan,Wei Ju,Ming Zhang*

Main category: cs.CL

> 本文是首个从数据角度对数据高效的LLM后训练方法进行系统性研究的工作，涵盖了多种方法类别，并提出了未来的研究方向。

<details>
  <summary>Details</summary>

**Motivation:** 本文旨在解决当前LLM后训练中面临的数据挑战，例如人工标注的成本高昂以及数据规模递增带来的收益递减问题，提出数据高效的后训练方法成为关键的研究问题。

**Method:** 本篇论文采用系统性的视角对数据高效的LLM后训练方法进行了分类，涵盖了数据选择、数据质量提升、合成数据生成、数据蒸馏与压缩和自我演化的数据生态系统。

**Result:** 论文提供了一个数据高效的LLM后训练方法的分类法，并总结了每个类别中代表性方法，提出了未来的研究方向。

**Conclusion:** 通过对数据高效LLM后训练中的挑战进行分析，本文强调了公开的问题并提出了潜在的研究途径，激发了对大规模模型训练中数据利用最大化的进一步探索。

**Abstract:** Post-training of Large Language Models (LLMs) is crucial for unlocking their
task generalization potential and domain-specific capabilities. However, the
current LLM post-training paradigm faces significant data challenges, including
the high costs of manual annotation and diminishing marginal returns on data
scales. Therefore, achieving data-efficient post-training has become a key
research question. In this paper, we present the first systematic survey of
data-efficient LLM post-training from a data-centric perspective. We propose a
taxonomy of data-efficient LLM post-training methods, covering data selection,
data quality enhancement, synthetic data generation, data distillation and
compression, and self-evolving data ecosystems. We summarize representative
approaches in each category and outline future research directions. By
examining the challenges in data-efficient LLM post-training, we highlight open
problems and propose potential research avenues. We hope our work inspires
further exploration into maximizing the potential of data utilization in
large-scale model training. Paper List:
https://github.com/luo-junyu/Awesome-Data-Efficient-LLM

</details>


### [11] [Evaluating the Impact of LLM-Assisted Annotation in a Perspectivized Setting: the Case of FrameNet Annotation](https://arxiv.org/abs/2510.25904)
*Frederico Belcavello,Ely Matos,Arthur Lorenzi,Lisandra Bonoto,Lívia Ruiz,Luiz Fernando Pereira,Victor Herbst,Yulla Navarro,Helen de Andrade Abreu,Lívia Dutra,Tiago Timponi Torrent*

Main category: cs.CL

> The paper evaluates the use of a large language model for FrameNet-like semantic annotation, comparing manual, automatic, and semi-automatic methods. It finds that semi-automatic annotation is most effective, balancing diversity and coverage, with automatic annotation being quicker but less effective overall.

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to address the lack of comprehensive evaluation of LLM performance and impact on creating annotated datasets, particularly in the context of NLP research. The study aims to reduce the knowledge gap by providing insights into the effectiveness of (semi-)automatized semantic annotation.

**Method:** The paper evaluates the performance of a large language model (LLM)-based semantic role labeler in automating FrameNet-like semantic annotation. It compares three settings: manual, automatic, and semi-automatic annotation, assessing factors such as annotation time, coverage, and diversity of semantic frames.

**Result:** The results indicate that the semi-automatic annotation method increases frame diversity and maintains similar annotation coverage in comparison to purely manual annotation. The fully automatic approach, however, is less effective in most metrics except for reducing annotation time.

**Conclusion:** The study concludes that semi-automated annotation using LLMs can be an effective method for enhancing the diversity of semantic annotations while maintaining coverage levels typical of human annotation. The automatic approach, while faster, is less effective overall for creating high-quality annotated datasets.

**Abstract:** The use of LLM-based applications as a means to accelerate and/or substitute
human labor in the creation of language resources and dataset is a reality.
Nonetheless, despite the potential of such tools for linguistic research,
comprehensive evaluation of their performance and impact on the creation of
annotated datasets, especially under a perspectivized approach to NLP, is still
missing. This paper contributes to reduction of this gap by reporting on an
extensive evaluation of the (semi-)automatization of FrameNet-like semantic
annotation by the use of an LLM-based semantic role labeler. The methodology
employed compares annotation time, coverage and diversity in three experimental
settings: manual, automatic and semi-automatic annotation. Results show that
the hybrid, semi-automatic annotation setting leads to increased frame
diversity and similar annotation coverage, when compared to the human-only
setting, while the automatic setting performs considerably worse in all
metrics, except for annotation time.

</details>


### [12] [RECAP: Reproducing Copyrighted Data from LLMs Training with an Agentic Pipeline](https://arxiv.org/abs/2510.25941)
*André V. Duarte,Xuying li,Bin Zeng,Arlindo L. Oliveira,Lei Li,Zhuo Li*

Main category: cs.CL

> Error

<details>
  <summary>Details</summary>

**Motivation:** Error

**Method:** Error

**Result:** Error

**Conclusion:** Error

**Abstract:** If we cannot inspect the training data of a large language model (LLM), how
can we ever know what it has seen? We believe the most compelling evidence
arises when the model itself freely reproduces the target content. As such, we
propose RECAP, an agentic pipeline designed to elicit and verify memorized
training data from LLM outputs. At the heart of RECAP is a feedback-driven
loop, where an initial extraction attempt is evaluated by a secondary language
model, which compares the output against a reference passage and identifies
discrepancies. These are then translated into minimal correction hints, which
are fed back into the target model to guide subsequent generations. In
addition, to address alignment-induced refusals, RECAP includes a jailbreaking
module that detects and overcomes such barriers. We evaluate RECAP on
EchoTrace, a new benchmark spanning over 30 full books, and the results show
that RECAP leads to substantial gains over single-iteration approaches. For
instance, with GPT-4.1, the average ROUGE-L score for the copyrighted text
extraction improved from 0.38 to 0.47 - a nearly 24% increase.

</details>


### [13] [Revisiting Multilingual Data Mixtures in Language Model Pretraining](https://arxiv.org/abs/2510.25947)
*Negar Foroutan,Paul Teiletche,Ayush Kumar Tarun,Antoine Bosselut*

Main category: cs.CL

> 研究通过大量参数的语言模型及多种语言语料库的训练，挑战了多语言训练的常见假设，表明多语言数据在平衡得当的情况下可以提升模型性能。

<details>
  <summary>Details</summary>

**Motivation:** 研究旨在探讨多语言数据混合在预训练大型语言模型时语言覆盖范围与模型性能之间的潜在折衷问题，即所谓的'多语言诅咒'。

**Method:** 本研究通过训练参数规模为11亿和30亿的大型语言模型（LLMs），使用从25种到400种语言不同的多语言语料库，对多语言预训练的影响进行了调查。

**Result:** 研究结果表明，结合英语和其他多语言数据并不必然降低这些语言的性能，只要这些语言在预训练语料库中有足够的token。将英语作为中介语言可以给跨语言家族带来好处，并且选择某个特定家族的语言作为中介语言并不会一致地改善该家族语言的性能。随着训练语言数量的增加，模型并未显示出显著的'多语言诅咒'。

**Conclusion:** 研究发现，多语言数据，当适量平衡时，可以增强语言模型的能力而不损害性能，即使在低资源环境中也是如此。

**Abstract:** The impact of different multilingual data mixtures in pretraining large
language models (LLMs) has been a topic of ongoing debate, often raising
concerns about potential trade-offs between language coverage and model
performance (i.e., the curse of multilinguality). In this work, we investigate
these assumptions by training 1.1B and 3B parameter LLMs on diverse
multilingual corpora, varying the number of languages from 25 to 400. Our study
challenges common beliefs surrounding multilingual training. First, we find
that combining English and multilingual data does not necessarily degrade the
in-language performance of either group, provided that languages have a
sufficient number of tokens included in the pretraining corpus. Second, we
observe that using English as a pivot language (i.e., a high-resource language
that serves as a catalyst for multilingual generalization) yields benefits
across language families, and contrary to expectations, selecting a pivot
language from within a specific family does not consistently improve
performance for languages within that family. Lastly, we do not observe a
significant "curse of multilinguality" as the number of training languages
increases in models at this scale. Our findings suggest that multilingual data,
when balanced appropriately, can enhance language model capabilities without
compromising performance, even in low-resource settings

</details>


### [14] [Semantic Label Drift in Cross-Cultural Translation](https://arxiv.org/abs/2510.25967)
*Mohsinul Kabir,Tasnim Ahmed,Md Mezbaur Rahman,Polydoros Giannouris,Sophia Ananiadou*

Main category: cs.CL

> 文章探讨了文化差异对机器翻译中语义标签漂移的影响，并指出文化因素的忽视会导致标签保真度下降和潜在的文化冲突。

<details>
  <summary>Details</summary>

**Motivation:** 探讨机器翻译过程中，文化差异对语义标签漂移的影响，以及这种影响对最终翻译质量和文化适应性的重要性。

**Method:** 通过在文化和非文化敏感领域进行一系列实验，比较MT系统（包括现代大型语言模型）在翻译过程中语义标签的漂移情况。

**Result:** 发现MT系统在文化和非文化敏感领域翻译时会诱导标签漂移，大型语言模型因编码了文化知识，反而加剧了这个问题。

**Conclusion:** 文化相似度影响标签的保真度，忽视文化因素会使得下游应用面临解释错误和文化冲突的风险。

**Abstract:** Machine Translation (MT) is widely employed to address resource scarcity in
low-resource languages by generating synthetic data from high-resource
counterparts. While sentiment preservation in translation has long been
studied, a critical but underexplored factor is the role of cultural alignment
between source and target languages. In this paper, we hypothesize that
semantic labels are drifted or altered during MT due to cultural divergence.
Through a series of experiments across culturally sensitive and neutral
domains, we establish three key findings: (1) MT systems, including modern
Large Language Models (LLMs), induce label drift during translation,
particularly in culturally sensitive domains; (2) unlike earlier statistical MT
tools, LLMs encode cultural knowledge, and leveraging this knowledge can
amplify label drift; and (3) cultural similarity or dissimilarity between
source and target languages is a crucial determinant of label preservation. Our
findings highlight that neglecting cultural factors in MT not only undermines
label fidelity but also risks misinterpretation and cultural conflict in
downstream applications.

</details>


### [15] [SymCode: A Neurosymbolic Approach to Mathematical Reasoning via Verifiable Code Generation](https://arxiv.org/abs/2510.25975)
*Sina Bagheri Nezhad,Yao Li,Ameeta Agrawal*

Main category: cs.CL

> SymCode框架将数学问题求解视为使用SymPy库的可验证代码生成任务，显著提高了准确率，提升了AI在正式领域的准确性和可信度。

<details>
  <summary>Details</summary>

**Motivation:** 大型语言模型在数学推理方面往往表现不佳，其基于文本的生成方式常常导致未验证且算术上或对不对的答案。

**Method:** 引入了一个名为SymCode的神经符号框架，该框架将数学问题求解重新定义为使用SymPy库的可验证代码生成任务。

**Result:** 在MATH-500和OlympiadBench等具有挑战性的基准测试上，SymCode将准确率提高了最多13.6个百分点，比基线模型更高效。

**Conclusion:** 通过将大型语言模型的推理基于一个确定性的符号引擎，SymCode代表了更准确和可信赖的AI系统的一个关键步骤，特别是在正式的领域中。

**Abstract:** Large Language Models (LLMs) often struggle with complex mathematical
reasoning, where prose-based generation leads to unverified and arithmetically
unsound solutions. Current prompting strategies like Chain of Thought still
operate within this unreliable medium, lacking a mechanism for deterministic
verification. To address these limitations, we introduce SymCode, a
neurosymbolic framework that reframes mathematical problem-solving as a task of
verifiable code generation using the SymPy library. We evaluate SymCode on
challenging benchmarks, including MATH-500 and OlympiadBench, demonstrating
significant accuracy improvements of up to 13.6 percentage points over
baselines. Our analysis shows that SymCode is not only more token-efficient but
also fundamentally shifts model failures from opaque logical fallacies towards
transparent, programmatic errors. By grounding LLM reasoning in a deterministic
symbolic engine, SymCode represents a key step towards more accurate and
trustworthy AI in formal domains.

</details>


### [16] [NeuronMM: High-Performance Matrix Multiplication for LLM Inference on AWS Trainium](https://arxiv.org/abs/2510.25977)
*Dinghong Song,Jierui Xu,Weichu Yang,Pengfei Su,Dong Li*

Main category: cs.CL

> 本文通过优化矩阵乘法运算，提升了基于Trainium架构的大规模语言模型推理性能，获得了显著的速度提升。

<details>
  <summary>Details</summary>

**Motivation:** 通过优化矩阵乘法运算，降低数据搬运消耗、最大化SRAM带宽、避免矩阵转置操作，从而提高大规模语言模型在Trainium架构上的训练和推理性能。

**Method:** 研究设计了一种高效的矩阵乘法（matmul）算法，采用了核函数融合和创新的缓存策略，减少数据在内存层次之间的搬移，最大化SRAM带宽，并避免昂贵的矩阵转置操作。

**Result:** 本文通过设计适用于Trainium的高效矩阵乘法运算，显著提升了大规模语言模型（LLM）的推理性能，相较于AWS现有的方法，矩阵乘法运算的平均加速比达到1.35倍，最高可达2.22倍，最终使得整个LLM推理过程的平均加速比达到1.66倍，最高加速比为2.49倍。

**Conclusion:** 通过针对Trainium架构优化矩阵乘法算法，该研究显著提升了大规模语言模型推理的性能，展示了在特定硬件上优化算法的重要作用。

**Abstract:** AI accelerators, customized to AI workloads, provide cost-effective and
high-performance solutions for training and inference. Trainium, an AI
accelerator recently developed by Amazon Web Services (AWS), provides an
attractive option for LLM training and inference through its heterogeneous
architecture. However, leveraging Trainium architecture for high performance
can be challenging because of its systolic array architecture and special
requirement on data layout. In this paper, we design high-performance matrix
multiplication (matmul), a critical compute kernel, for LLM inference on
Trainium. We introduce a series of techniques customized to Trainium based on
kernel fusion and novel caching strategies to reduce data movement across the
software-managed memory hierarchy, maximize SRAM bandwidth, and avoid expensive
matrix transpose. Evaluating with nine datasets and four recent LLMs, we show
that our system largely outperforms the state-of-the-art matmul implemented by
AWS on Trainium: at the level of matmul kernel, it achieves an average 1.35x
speedup (up to 2.22x), which translates to an average 1.66x speedup (up to
2.49x) for end-to-end LLM inference.

</details>


### [17] [AttnCache: Accelerating Self-Attention Inference for LLM Prefill via Attention Cache](https://arxiv.org/abs/2510.25979)
*Dinghong Song,Yuan Feng,Yiwei Wang,Shangye Chen,Cyril Guyot,Filip Blagojevic,Hyeran Jeon,Pengfei Su,Dong Li*

Main category: cs.CL

> AttnCache accelerates the prefill stage of LLM inference by caching and reusing attention maps, achieving notable speedups on both CPU and GPU with minimal accuracy trade-off.

<details>
  <summary>Details</summary>

**Motivation:** To address the performance bottleneck caused by self-attention computation in the prefill stage of inference for large language models.

**Method:** AttnCache, a framework utilizing an attention map memorization database to cache and reuse attention maps during inference.

**Result:** AttnCache achieves an average of 1.2x end-to-end and 2x attention speedup on CPU, and 1.6x end-to-end and 3x attention speedup on GPU, with negligible accuracy degradation.

**Conclusion:** The proposed AttnCache framework effectively accelerates the prefill stage of LLM inference by reusing pre-cached attention maps, significantly improving performance without compromising accuracy.

**Abstract:** Large Language Models (LLMs) are widely used in generative applications such
as chatting, code generation, and reasoning. However, many realworld workloads
such as classification, question answering, recommendation, and text embedding
rely solely on the prefill stage of inference, where the model encodes input
sequences without performing autoregressive decoding. In these prefill only
scenarios, the self-attention computation becomes the primary performance
bottleneck due to its quadratic complexity with respect to sequence length. In
this paper, we observe that semantically different sentences often produce
similar attention maps across layers and heads. Building on this insight, we
propose AttnCache, a framework that accelerates the prefill stage of LLM
inference by retrieving and reusing similar attention maps. Based on an
attention map memorization database, AttnCache employs efficient caching and
similarity search techniques to identify and reuse pre-cached attention maps
during inference, thereby reducing the computational overhead of
self-attention. Experimental results show that AttnCache achieves an average of
1.2x end-to-end and 2x attention speedup on CPU, and 1.6x end-to-end and 3x
attention speedup on GPU, with negligible accuracy degradation.

</details>


### [18] [Supervised Reinforcement Learning: From Expert Trajectories to Step-wise Reasoning](https://arxiv.org/abs/2510.25992)
*Yihe Deng,I-Hung Hsu,Jun Yan,Zifeng Wang,Rujun Han,Gufeng Zhang,Yanfei Chen,Wei Wang,Tomas Pfister,Chen-Yu Lee*

Main category: cs.CL

> 提出了SRL框架，通过创新的训练方法解决多步推理问题，适用于小型开源模型，增强了其处理复杂问题的能力。

<details>
  <summary>Details</summary>

**Motivation:** 解决大语言模型在多步推理问题上的困难，特别是在小型开源模型中，强化学习（RLVR）在正确解法很难被采样的情况下失败，而监督微调（SFT）倾向于过度拟合到长的演示，通过僵化的按令牌模仿来实现。

**Method:** 提出了监督强化学习（SRL）框架，此框架将问题解决重新定义为生成逻辑“动作”序列的任务。SRL训练模型在提交每个动作之前先生成一种内部理性独白，根据从SFT数据集中按步骤提取的专家动作与模型动作的相似度提供平滑奖励。

**Result:** 该框架通过将问题解决重新定义为生成一系列逻辑“动作”的方式，使得模型可以生成一段内部推理独白，然后再执行每个动作。这种训练方法提供了基于模型动作与从SFT数据集中提取的专家动作相似度的平滑奖励，即便所有尝试都是错误的情况下也能提供丰富的学习信号，同时鼓励了灵活的推理，使小模型能够学习到以前无法学习的挑战性问题。初始训练使用SRL，并用RLVR进行优化，可以获得最佳的整体性能。此外，SRL在代理软件工程任务上表现良好，证明了它作为一个强大的和多功能的训练框架的实用性，适用于侧重推理的大型语言模型。

**Conclusion:** SRL框架不仅能够让小型模型学习到从前无法学到的困难问题，还展示了在代理软件工程任务中的有效推广。初步使用SRL训练，接着通过RLVR进行优化，可以获得最佳的整体性能。

**Abstract:** Large Language Models (LLMs) often struggle with problems that require
multi-step reasoning. For small-scale open-source models, Reinforcement
Learning with Verifiable Rewards (RLVR) fails when correct solutions are rarely
sampled even after many attempts, while Supervised Fine-Tuning (SFT) tends to
overfit long demonstrations through rigid token-by-token imitation. To address
this gap, we propose Supervised Reinforcement Learning (SRL), a framework that
reformulates problem solving as generating a sequence of logical "actions". SRL
trains the model to generate an internal reasoning monologue before committing
to each action. It provides smoother rewards based on the similarity between
the model's actions and expert actions extracted from the SFT dataset in a
step-wise manner. This supervision offers richer learning signals even when all
rollouts are incorrect, while encouraging flexible reasoning guided by expert
demonstrations. As a result, SRL enables small models to learn challenging
problems previously unlearnable by SFT or RLVR. Moreover, initializing training
with SRL before refining with RLVR yields the strongest overall performance.
Beyond reasoning benchmarks, SRL generalizes effectively to agentic software
engineering tasks, establishing it as a robust and versatile training framework
for reasoning-oriented LLMs.

</details>


### [19] [PORTool: Tool-Use LLM Training with Rewarded Tree](https://arxiv.org/abs/2510.26020)
*Feijie Wu,Weiwu Zhu,Yuxiang Zhang,Soumya Chatterjee,Jiarong Zhu,Fan Mo,Rodin Luo,Jing Gao*

Main category: cs.CL

> PORTool, a reinforcement learning method, enhances LLMs' tool-use capabilities in dynamic environments by encouraging exploration of diverse solution trajectories, leading to improved accuracy and efficiency.

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind this work is to address the limitations of current tool-use LLMs, which are trained on static datasets and thus fail to explore various solutions in a dynamic tool-call environment.

**Method:** This paper proposes PORTool, a reinforcement learning method that aims to enhance the performance of tool-use LLMs by encouraging them to explore a variety of trajectories leading to correct answers. The method involves generating multiple rollouts for a given query, assigning step-wise rewards based on the success of tool calls and the production of correct answers, and training the LLM using these rewards.

**Result:** Experiments using PORTool with 17 tools to address user queries demonstrate improvements in final accuracy and the number of tool-call steps compared to other training approaches.

**Conclusion:** The study concludes that PORTool effectively improves the performance of tool-use LLMs by providing a mechanism to explore and optimize various solution paths.

**Abstract:** Current tool-use large language models (LLMs) are trained on static datasets,
enabling them to interact with external tools and perform multi-step,
tool-integrated reasoning, which produces tool-call trajectories. However,
these models imitate how a query is resolved in a generic tool-call routine,
thereby failing to explore possible solutions and demonstrating limited
performance in an evolved, dynamic tool-call environment. In this work, we
propose PORTool, a reinforcement learning (RL) method that encourages a
tool-use LLM to explore various trajectories yielding the correct answer.
Specifically, this method starts with generating multiple rollouts for a given
query, and some of them share the first few tool-call steps, thereby forming a
tree-like structure. Next, we assign rewards to each step, based on its ability
to produce a correct answer and make successful tool calls. A shared step
across different trajectories receives the same reward, while different steps
under the same fork receive different rewards. Finally, these step-wise rewards
are used to calculate fork-relative advantages, blended with
trajectory-relative advantages, to train the LLM for tool use. The experiments
utilize 17 tools to address user queries, covering both time-sensitive and
time-invariant topics. We conduct ablation studies to systematically justify
the necessity and the design robustness of step-wise rewards. Furthermore, we
compare the proposed PORTool with other training approaches and demonstrate
significant improvements in final accuracy and the number of tool-call steps.

</details>


### [20] [Rethinking Cross-lingual Alignment: Balancing Transfer and Cultural Erasure in Multilingual LLMs](https://arxiv.org/abs/2510.26024)
*HyoJung Han,Sweta Agrawal,Eleftheria Briakou*

Main category: cs.CL

> 本研究发现跨语言对齐虽提升了事实知识的跨语言传输，却可能导致文化响应的本地化丧失。提出一种新方法——手术导向，可以在不同网络层调节事实传输和文化知识的平衡。

<details>
  <summary>Details</summary>

**Motivation:** 研究动机在于质疑跨语言对齐（CLA）在追求表征收敛的同时是否存在“文化擦除”的风险，即丧失提供基于查询语言的文化特定响应的能力。

**Method:** 本研究提出了一个全面的评估框架——传输-本地化平面，该框架量化了知识传输的理想效果和文化擦除的不良影响。通过这个框架，作者重新评估了最近的跨语言对齐方法，并发现这些方法在改善事实性知识传输的同时，会以文化本地化为代价。研究还揭示了普遍事实传输和文化特定知识在模型的不同层中是最优可调节的。基于这一发现，提出了手术导向方法，该方法通过在不同的层上应用定向激活引导，实现了两个竞争维度之间的更好平衡。

**Result:** 使用传输-本地化平面评估框架，作者发现所研究的六种语言中，最近的跨语言对齐方法普遍提高了事实传输知识，但以文化本地化为代价。

**Conclusion:** 提出了一种新颖的推理时间方法——手术导向，该方法通过在不同层应用定向激活引导，可以更好地平衡事实知识传输和文化本地化的竞争需求，克服了现有对齐技术的局限性。

**Abstract:** Cross-lingual alignment (CLA) aims to align multilingual representations,
enabling Large Language Models (LLMs) to seamlessly transfer knowledge across
languages. While intuitive, we hypothesize, this pursuit of representational
convergence can inadvertently cause "cultural erasure", the functional loss of
providing culturally-situated responses that should diverge based on the query
language. In this work, we systematically analyze this trade-off by introducing
a holistic evaluation framework, the transfer-localization plane, which
quantifies both desirable knowledge transfer and undesirable cultural erasure.
Using this framework, we re-evaluate recent CLA approaches and find that they
consistently improve factual transfer at the direct cost of cultural
localization across all six languages studied. Our investigation into the
internal representations of these models reveals a key insight: universal
factual transfer and culturally-specific knowledge are optimally steerable at
different model layers. Based on this finding, we propose Surgical Steering, a
novel inference-time method that disentangles these two objectives. By applying
targeted activation steering to distinct layers, our approach achieves a better
balance between the two competing dimensions, effectively overcoming the
limitations of current alignment techniques.

</details>


### [21] [Artificial Intelligence-Enabled Analysis of Radiology Reports: Epidemiology and Consequences of Incidental Thyroid Findings](https://arxiv.org/abs/2510.26032)
*Felipe Larios,Mariana Borras-Osorio,Yuqi Wu,Ana Gabriela Claros,David Toro-Tobon,Esteban Cabezas,Ricardo Loor-Torres,Maria Mateo Chavez,Kerly Guevara Maldonado,Luis Vilatuna Andrango,Maria Lizarazo Jimenez,Ivan Mateo Alzamora,Misk Al Zahidy,Marcelo Montero,Ana Cristina Proano,Cristian Soto Jacome,Jungwei W. Fan,Oscar J. Ponce-Ponte,Megan E. Branda,Naykky Singh Ospina,Juan P. Brito*

Main category: cs.CL

> 基于一种NLP管道开发、验证并部署以识别放射学报告中的非预期甲状腺发现（ITFs），结果显示ITFs发生率为7.8%，更常见于特定人群和成像方式中，并且与甲状腺结节、活检、手术和癌症的诊断有关，特别是之后检测到的癌症往往较大。这突显了规范化报告和选择性随访的必要性。

<details>
  <summary>Details</summary>

**Motivation:** 研究旨在评估非预期甲状腺发现（ITFs）在成人放射学报告中的发生率、特征和临床后果，探索开发和应用自然语言处理（NLP）技术在提升甲状腺疾病诊断与管理中的作用。

**Method:** 本研究采用一种基于变压器的自然语言处理（NLP）技术来识别放射学报告中的ITFs。研究对象是在2017年7月1日至2023年9月30日期间在Mayo Clinic接受甲状腺捕获成像的成人患者。NLP管道用于提取结节特征。

**Result:** 本次研究通过开发并验证基于变压器的自然语言处理（NLP）管道，识别放射学报告中的非预期甲状腺发现（ITFs），并分析其发生率、特征和临床后果。研究在无既往甲状腺疾病的成人患者中进行，通过不同成像方式发现ITFs的总体发生率为7.8%。ITFs常在女性、年龄较大、BMI较高以及由肿瘤学或内科开立影像检查的患者中出现。ITFs还被发现在颈部CT、PET和核医学扫描中的可能性更高。与其他患者相比，有ITFs的患者更有可能被诊断为甲状腺结节、进行活检、甲状腺手术和甲状腺癌诊断，特别是乳头状癌。这些发现强调了ITFs在甲状腺癌过度诊断中的作用，呼吁标准化报告和更有选择性的随访。

**Conclusion:** 非预期甲状腺发现（ITFs）的现象普遍，并与甲状腺疾病的诊断和治疗途径紧密相关。这些发现提示了ITFs在甲状腺癌过度诊断中的角色，强调了标准化报告和更审慎的随访措施的必要性。

**Abstract:** Importance Incidental thyroid findings (ITFs) are increasingly detected on
imaging performed for non-thyroid indications. Their prevalence, features, and
clinical consequences remain undefined. Objective To develop, validate, and
deploy a natural language processing (NLP) pipeline to identify ITFs in
radiology reports and assess their prevalence, features, and clinical outcomes.
Design, Setting, and Participants Retrospective cohort of adults without prior
thyroid disease undergoing thyroid-capturing imaging at Mayo Clinic sites from
July 1, 2017, to September 30, 2023. A transformer-based NLP pipeline
identified ITFs and extracted nodule characteristics from image reports from
multiple modalities and body regions. Main Outcomes and Measures Prevalence of
ITFs, downstream thyroid ultrasound, biopsy, thyroidectomy, and thyroid cancer
diagnosis. Logistic regression identified demographic and imaging-related
factors. Results Among 115,683 patients (mean age, 56.8 [SD 17.2] years; 52.9%
women), 9,077 (7.8%) had an ITF, of which 92.9% were nodules. ITFs were more
likely in women, older adults, those with higher BMI, and when imaging was
ordered by oncology or internal medicine. Compared with chest CT, ITFs were
more likely via neck CT, PET, and nuclear medicine scans. Nodule
characteristics were poorly documented, with size reported in 44% and other
features in fewer than 15% (e.g. calcifications). Compared with patients
without ITFs, those with ITFs had higher odds of thyroid nodule diagnosis,
biopsy, thyroidectomy and thyroid cancer diagnosis. Most cancers were
papillary, and larger when detected after ITFs vs no ITF. Conclusions ITFs were
common and strongly associated with cascades leading to the detection of small,
low-risk cancers. These findings underscore the role of ITFs in thyroid cancer
overdiagnosis and the need for standardized reporting and more selective
follow-up.

</details>


### [22] [QCoder Benchmark: Bridging Language Generation and Quantum Hardware through Simulator-Based Feedback](https://arxiv.org/abs/2510.26101)
*Taku Mikuriya,Tatsuya Ishigaki,Masayuki Kawarada,Shunya Minami,Tadashi Kadowaki,Yohichi Suzuki,Soshun Naito,Shunya Takata,Takumi Kato,Tamotsu Basseda,Reo Yamada,Hiroya Takamura*

Main category: cs.CL

> 本文介绍QCoder基准测试框架，用于评估大语言模型在量子编程中的表现，并发现尽管GPT-4的准确率只有18.97%，但基于推理的模型如o3可达到78%的准确率，超过了人类编写的代码平均成功率。

<details>
  <summary>Details</summary>

**Motivation:** 目前，大语言模型在需要与硬件设备交互的领域，如量子编程中应用较少。本研究旨在填补这一空白，研究大语言模型在量子编程中的应用。

**Method:** 通过引入QCoder基准测试框架来评估大语言模型在量子编程中的表现，该框架使用量子模拟器环境进行反馈评估，并结合来自真实编程比赛的人类编写的代码样本来进行定量和定性分析。

**Result:** 实验结果显示，即使像GPT-4这样的先进模型也仅能达到约18.97%的准确率；而基于推理的模型如o3则能达到78%的准确率，超过了人类编写的代码平均成功率（39.98%）。

**Conclusion:** 研究发现量子编程仍是一个挑战性高的领域，但基于推理的模型具有更好的性能。发布的QCoder基准测试集和公共评估API将支持进一步研究。

**Abstract:** Large language models (LLMs) have increasingly been applied to automatic
programming code generation. This task can be viewed as a language generation
task that bridges natural language, human knowledge, and programming logic.
However, it remains underexplored in domains that require interaction with
hardware devices, such as quantum programming, where human coders write Python
code that is executed on a quantum computer. To address this gap, we introduce
QCoder Benchmark, an evaluation framework that assesses LLMs on quantum
programming with feedback from simulated hardware devices. Our benchmark offers
two key features. First, it supports evaluation using a quantum simulator
environment beyond conventional Python execution, allowing feedback of
domain-specific metrics such as circuit depth, execution time, and error
classification, which can be used to guide better generation. Second, it
incorporates human-written code submissions collected from real programming
contests, enabling both quantitative comparisons and qualitative analyses of
LLM outputs against human-written codes. Our experiments reveal that even
advanced models like GPT-4o achieve only around 18.97% accuracy, highlighting
the difficulty of the benchmark. In contrast, reasoning-based models such as o3
reach up to 78% accuracy, outperforming averaged success rates of human-written
codes (39.98%). We release the QCoder Benchmark dataset and public evaluation
API to support further research.

</details>


### [23] [Reasoning Path Divergence: A New Metric and Curation Strategy to Unlock LLM Diverse Thinking](https://arxiv.org/abs/2510.26122)
*Feng Ju,Zeyu Qin,Rui Min,Zhitao He,Lingpeng Kong,Yi R. Fung*

Main category: cs.CL

> 提出了"一个问题，多种解决方案"（1PNS）训练范式以提高模型推理多样性，并引入了推理路径差异（RPD）来衡量多步思维链之间的语义差异。实验表明，与"一个问题，一个解决方案"（1P1S）基准相比，1PNS训练在通过率上有了显著提升。

<details>
  <summary>Details</summary>

**Motivation:** 旨在解决大语言模型推理能力提升的同时，输出多样性不足的问题，特别是由于"一个问题，一个解决方案"（1P1S）训练实践中提供的单一标准答案导致模式趋窄的问题。

**Method:** 提出了"一个问题，多种解决方案"（1PNS）方法，以增加有效的推理路径的多样性，并引入了推理路径差异（RPD），来评估多步链式思维之间的语义差异。利用RPD，为每个问题选择最大多样性的解决方案集合，并对基础模型Qwen3-4B-Base进行微调。

**Result:** 实验结果显示，使用RPD选择的训练方法能产生更多样化的输出，并在多个指标上提升了通过率，包括+2.80%的pass@16增益，以及在特定任务上+4.99%的增益。

**Conclusion:** 研究证明了"一个问题，多种解决方案"（1PNS）方法能进一步提升模型的测试时间扩展（TTS）的有效性，特别是在增加推理多样性和提高通过率方面。

**Abstract:** While Test-Time Scaling (TTS) has proven effective in improving the reasoning
ability of large language models (LLMs), low diversity in model outputs often
becomes a bottleneck; this is partly caused by the common "one problem, one
solution" (1P1S) training practice, which provides a single canonical answer
and can push models toward a narrow set of reasoning paths. To address this, we
propose a "one problem, multiple solutions" (1PNS) training paradigm that
exposes the model to a variety of valid reasoning trajectories and thus
increases inference diversity. A core challenge for 1PNS is reliably measuring
semantic differences between multi-step chains of thought, so we introduce
Reasoning Path Divergence (RPD), a step-level metric that aligns and scores
Long Chain-of-Thought solutions to capture differences in intermediate
reasoning. Using RPD, we curate maximally diverse solution sets per problem and
fine-tune Qwen3-4B-Base. Experiments show that RPD-selected training yields
more varied outputs and higher pass@k, with an average +2.80% gain in pass@16
over a strong 1P1S baseline and a +4.99% gain on AIME24, demonstrating that
1PNS further amplifies the effectiveness of TTS. Our code is available at
https://github.com/fengjujf/Reasoning-Path-Divergence .

</details>


### [24] [On the Influence of Discourse Relations in Persuasive Texts](https://arxiv.org/abs/2510.26124)
*Nawar Turk,Sevag Kaspar,Leila Kosseim*

Main category: cs.CL

> 本研究通过数据集重新标注和分类，探索了说服技巧和话语关系之间的联系，并发现某些话语关系对说服性文本有显著影响，这在检测在线宣传和错误信息方面有潜在应用。

<details>
  <summary>Details</summary>

**Motivation:** 研究的主要动机是探讨PTs和DRs之间的关系，并通过开发银色数据集来填补现有数据集的空白，这有助于检测在线宣传和错误信息，并提高我们对有效沟通的一般理解。

**Method:** 本研究利用了大语言模型（LLMs）和提示工程来探索说服技巧（PTs）和话语关系（DRs）之间的关系。由于不存在同时标注了PTs和DRs的数据集，研究者们以SemEval 2023任务3中包含19种PTs的数据集为基础，开发了基于LLMs的分类器来标注数据集中的每个实例与22种PDTB 3.0二级DRs中的一个。

**Result:** 研究评估了4个不同的LLMs，使用了10种不同的提示，最终产生了40个不同的DR分类器。通过使用各种多数投票策略的集成模型，创建了5个标注了说服技巧和PDTB二级意义的银色数据集，其规模从1,281个实例到204个实例不等，具体取决于使用的多数投票手段。统计分析显示，六种话语关系（即因果关系、目的、对比、因果+信念、让步、条件）在说服性文本中起着关键作用，特别是在使用夸张/淡化、重复和制造怀疑的说服手法时。

**Conclusion:** 该研究证明了某些话语关系在说服性文本中的重要性，特别是当这些文本使用特定说服技巧时，这些发现可以应用到在线宣传和错误信息的检测中，从而提高对有效沟通的理解。

**Abstract:** This paper investigates the relationship between Persuasion Techniques (PTs)
and Discourse Relations (DRs) by leveraging Large Language Models (LLMs) and
prompt engineering. Since no dataset annotated with both PTs and DRs exists, we
took the SemEval 2023 Task 3 dataset labelled with 19 PTs as a starting point
and developed LLM-based classifiers to label each instance of the dataset with
one of the 22 PDTB 3.0 level-2 DRs. In total, four LLMs were evaluated using 10
different prompts, resulting in 40 unique DR classifiers. Ensemble models using
different majority-pooling strategies were used to create 5 silver datasets of
instances labelled with both persuasion techniques and level-2 PDTB senses. The
silver dataset sizes vary from 1,281 instances to 204 instances, depending on
the majority pooling technique used. Statistical analysis of these silver
datasets shows that six discourse relations (namely Cause, Purpose, Contrast,
Cause+Belief, Concession, and Condition) play a crucial role in persuasive
texts, especially in the use of Loaded Language, Exaggeration/Minimisation,
Repetition and to cast Doubt. This insight can contribute to detecting online
propaganda and misinformation, as well as to our general understanding of
effective communication.

</details>


### [25] [MossNet: Mixture of State-Space Experts is a Multi-Head Attention](https://arxiv.org/abs/2510.26182)
*Shikhar Tuli,James Seale Smith,Haris Jeelani,Chi-Heng Lin,Abhishek Patel,Vasili Ramanishka,Yen-Chang Hsu,Hongxia Jin*

Main category: cs.CL

> Error

<details>
  <summary>Details</summary>

**Motivation:** Error

**Method:** Error

**Result:** Error

**Conclusion:** Error

**Abstract:** Large language models (LLMs) have significantly advanced generative
applications in natural language processing (NLP). Recent trends in model
architectures revolve around efficient variants of transformers or
state-space/gated-recurrent models (SSMs, GRMs). However, prevailing
SSM/GRM-based methods often emulate only a single attention head, potentially
limiting their expressiveness. In this work, we propose MossNet, a novel
mixture-of-state-space-experts architecture that emulates a linear multi-head
attention (MHA). MossNet leverages a mixture-of-experts (MoE) implementation
not only in channel-mixing multi-layered perceptron (MLP) blocks but also in
the time-mixing SSM kernels to realize multiple "attention heads." Extensive
experiments on language modeling and downstream evaluations show that MossNet
outperforms both transformer- and SSM-based architectures of similar model size
and data budgets. Larger variants of MossNet, trained on trillions of tokens,
further confirm its scalability and superior performance. In addition,
real-device profiling on a Samsung Galaxy S24 Ultra and an Nvidia A100 GPU
demonstrate favorable runtime speed and resource usage compared to similarly
sized baselines. Our results suggest that MossNet is a compelling new direction
for efficient, high-performing recurrent LLM architectures.

</details>


### [26] [Similarity-Distance-Magnitude Language Models](https://arxiv.org/abs/2510.26183)
*Allen Schmaltz*

Main category: cs.CL

> 通过监督微调方法，将现有Transformer语言模型转换为SDM语言模型，以提高其在指令跟随任务中的表现。

<details>
  <summary>Details</summary>

**Motivation:** 旨在提升语言模型在遵循指令时生成高质量文本的能力，特别是在高概率区域内保持校准。

**Method:** 提出了相似度-距离-幅度（SDM）语言模型，为指令跟随的二元分类使用最终层的SDM激活层，通过监督微调将现有的预训练解码器Transformer语言模型转换为SDM语言模型。

**Result:** 相比强大的监督基线模型，SDM语言模型减少了弃权情况（即提高了统计效率）。

**Conclusion:** 通过引入SDM语言模型，有效提高了语言模型在生成任务中的统计效率。

**Abstract:** We introduce Similarity-Distance-Magnitude (SDM) language models (LMs), which
are sequence prediction models fine-tuned to maximize the proportion of
generations in the well-calibrated, high-probability region partitioned by a
final-layer SDM activation layer used for binary classification of
instruction-following. We demonstrate that existing pre-trained decoder-only
Transformer LMs can be readily converted into SDM LMs via supervised
fine-tuning, using the final-layer SDM activation layer during training to
estimate a change-of-base for a supervised next-token loss over a contrastive
input encoding scheme, with additional hard negative examples generated online
during training. This results in reduced abstentions (i.e., improved
statistical efficiency) compared to strong supervised baselines.

</details>


### [27] [RCScore: Quantifying Response Consistency in Large Language Models](https://arxiv.org/abs/2510.26193)
*Dongjun Jang,Youngchae Ahn,Hyopil Shin*

Main category: cs.CL

> 提出RCScore评估LLM对指令风格敏感性的框架，揭示了其影响及自洽性的重要性。

<details>
  <summary>Details</summary>

**Motivation:** 现有的大型语言模型（LLM）评估通常依赖单一指令模板，忽视了指令风格对模型响应的影响，这在实际部署中是一个关键因素。希望通过RCScore提供一种评估指令鲁棒性的原则方法。

**Method:** 提出RCScore框架，该框架通过系统地将基准问题转化为多种指令风格，来量化指令形式如何影响模型的响应。此外，引入交叉响应相似性(CRS)方法来衡量风格上的自洽性。

**Result:** 实验显示了指令风格可以改变精度高达16.7个百分点的结果，建立风格上的自洽性与任务准确性的强相关关系。还发现确定性解码会产生风格上更稳定的输出，模型规模与跨风格一致性成正相关。

**Conclusion:** RCScore为评估LLM对指令风格的敏感性提供了一种量化框架，并通过实验展示其有效性，强调风格自洽性作为模型可靠性的一个有价值指标。

**Abstract:** Current LLM evaluations often rely on a single instruction template,
overlooking models' sensitivity to instruction style-a critical aspect for
real-world deployments. We present RCScore, a multi-dimensional framework
quantifying how instruction formulation affects model responses. By
systematically transforming benchmark problems into multiple instruction
styles, RCScore reveals performance variations undetected by conventional
metrics. Our experiments across ten LLMs on four reasoning benchmarks
demonstrate that instruction style can shift accuracy by up to 16.7% points. We
introduce Cross-Response Similarity (CRS), a method applying RCScore metrics to
measure stylistic self-consistency, and establish its strong correlation with
task accuracy, suggesting consistency as a valuable proxy for model
reliability. Additional findings show that deterministic decoding produces more
stylistically stable outputs, and model scale correlates positively with
cross-style consistency. RCScore offers a principled approach to assess
instruction robustness.

</details>


### [28] [Don't Let It Fade: Preserving Edits in Diffusion Language Models via Token Timestep Allocation](https://arxiv.org/abs/2510.26200)
*Woojin Kim,Jaeyoung Do*

Main category: cs.CL

> The paper presents TTA, a method to manage token refinement during text generation in DLMs, leading to improved control and coherence.

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to address the issue of "update forgetting" in diffusion language models (DLMs), which occurs due to uniform and context-agnostic updates that lead to token-level fluctuations and disrupt the refinement process.

**Method:** Token Timestep Allocation (TTA) is proposed. This approach schedules token refinement based on their importance, freezing critical tokens early and continuing to refine uncertain ones. TTA can be implemented as a fixed policy or an adaptive policy driven by task signals.

**Result:** TTA enhances controllability and fluency, achieving over 20% higher accuracy in sentiment control, reducing perplexity to less than half, and completing in less than one-fifth of the steps. In detoxification tasks, TTA reduces maximum toxicity and perplexity.

**Conclusion:** The results demonstrate that softened ordering through timestep allocation is crucial for mitigating update forgetting and achieving stable, controllable diffusion text generation.

**Abstract:** While diffusion language models (DLMs) enable fine-grained refinement, their
practical controllability remains fragile. We identify and formally
characterize a central failure mode called update forgetting, in which uniform
and context agnostic updates induce token level fluctuations across timesteps,
erasing earlier semantic edits and disrupting the cumulative refinement
process, thereby degrading fluency and coherence. As this failure originates in
uniform and context agnostic updates, effective control demands explicit token
ordering. We propose Token Timestep Allocation (TTA), which realizes soft and
semantic token ordering via per token timestep schedules: critical tokens are
frozen early, while uncertain tokens receive continued refinement. This
timestep based ordering can be instantiated as either a fixed policy or an
adaptive policy driven by task signals, thereby supporting a broad spectrum of
refinement strategies. Because it operates purely at inference time, it applies
uniformly across various DLMs and naturally extends to diverse supervision
sources. Empirically, TTA improves controllability and fluency: on sentiment
control, it yields more than 20 percent higher accuracy and nearly halves
perplexity using less than one fifth the steps; in detoxification, it lowers
maximum toxicity (12.2 versus 14.5) and perplexity (26.0 versus 32.0).
Together, these results demonstrate that softened ordering via timestep
allocation is the critical lever for mitigating update forgetting and achieving
stable and controllable diffusion text generation.

</details>


### [29] [What's In My Human Feedback? Learning Interpretable Descriptions of Preference Data](https://arxiv.org/abs/2510.26202)
*Rajiv Movva,Smitha Milli,Sewon Min,Emma Pierson*

Main category: cs.CL

> 通过WIMHF方法分析人类反馈数据揭示了偏好的多样性和上下文，且有助于提升数据整理的安全性和个性化预测。

<details>
  <summary>Details</summary>

**Motivation:** 由于实践者对反馈数据所编码的内容缺乏明确的理解，人类反馈可能导致语言模型发生不可预测和不希望的变化。本研究旨在无需预设假设的情况下自动提取相关特征。

**Method:** 本研究提出了名为What's In My Human Feedback? (WIMHF) 的方法，利用稀疏自编码器来解释反馈数据，既表征了数据集能够测量的偏好，也揭示了数据标注者实际表达的偏好。

**Result:** WIMHF 在7个数据集中确定了少数几个可解释的人类特征，这些特征解释了黑盒模型实现的一些偏好预测信号。特征揭示了人类偏好的多样性及数据集级别的上下文的角色。利用学到的特征可以有效进行数据整理以及个性化预测的提升。

**Conclusion:** WIMHF 提供了一种以人类为中心的分析方法，帮助从业者更好地理解和利用偏好数据。

**Abstract:** Human feedback can alter language models in unpredictable and undesirable
ways, as practitioners lack a clear understanding of what feedback data
encodes. While prior work studies preferences over certain attributes (e.g.,
length or sycophancy), automatically extracting relevant features without
pre-specifying hypotheses remains challenging. We introduce What's In My Human
Feedback? (WIMHF), a method to explain feedback data using sparse autoencoders.
WIMHF characterizes both (1) the preferences a dataset is capable of measuring
and (2) the preferences that the annotators actually express. Across 7
datasets, WIMHF identifies a small number of human-interpretable features that
account for the majority of the preference prediction signal achieved by
black-box models. These features reveal a wide diversity in what humans prefer,
and the role of dataset-level context: for example, users on Reddit prefer
informality and jokes, while annotators in HH-RLHF and PRISM disprefer them.
WIMHF also surfaces potentially unsafe preferences, such as that LMArena users
tend to vote against refusals, often in favor of toxic content. The learned
features enable effective data curation: re-labeling the harmful examples in
Arena yields large safety gains (+37%) with no cost to general performance.
They also allow fine-grained personalization: on the Community Alignment
dataset, we learn annotator-specific weights over subjective features that
improve preference prediction. WIMHF provides a human-centered analysis method
for practitioners to better understand and use preference data.

</details>


### [30] [Towards Global Retrieval Augmented Generation: A Benchmark for Corpus-Level Reasoning](https://arxiv.org/abs/2510.26205)
*Qi Luo,Xiaonan Li,Tingshuo Fan,Xinchi Chen,Xipeng Qiu*

Main category: cs.CL

> 提出GlobalQA，一个专为评估全局RAG能力设计的基准，涵盖四种类别的核心任务：计数、极值查询、排序和顶级提取。并提出了GlobalRAG框架，展示了显著的性能提升。

<details>
  <summary>Details</summary>

**Motivation:** 当前的RAG评估基准主要集中在局部RAG上，然而许多现实世界的应用需要一种完全不同的能力——全局RAG，涉及在整个文档集合中聚合和分析信息以得出语料库级别的见解。

**Method:** GlobalRAG，一种多工具协作框架，它通过片段级别的检索保持结构一致性，运用大语言模型驱动的智能过滤器消除噪声文档，并集成聚合模块以实现精确的符号计算。

**Result:** 在Qwen2.5-14B模型上，GlobalRAG在全局任务上的F1分数达到6.63，远超最强基线的1.51。

**Conclusion:** 提出的GlobalRAG框架通过多工具协作，有效提高了全局RAG任务的性能。

**Abstract:** Retrieval-augmented generation (RAG) has emerged as a leading approach to
reducing hallucinations in large language models (LLMs). Current RAG evaluation
benchmarks primarily focus on what we call local RAG: retrieving relevant
chunks from a small subset of documents to answer queries that require only
localized understanding within specific text chunks. However, many real-world
applications require a fundamentally different capability -- global RAG --
which involves aggregating and analyzing information across entire document
collections to derive corpus-level insights (for example, "What are the top 10
most cited papers in 2023?"). In this paper, we introduce GlobalQA -- the first
benchmark specifically designed to evaluate global RAG capabilities, covering
four core task types: counting, extremum queries, sorting, and top-k
extraction. Through systematic evaluation across different models and
baselines, we find that existing RAG methods perform poorly on global tasks,
with the strongest baseline achieving only 1.51 F1 score. To address these
challenges, we propose GlobalRAG, a multi-tool collaborative framework that
preserves structural coherence through chunk-level retrieval, incorporates
LLM-driven intelligent filters to eliminate noisy documents, and integrates
aggregation modules for precise symbolic computation. On the Qwen2.5-14B model,
GlobalRAG achieves 6.63 F1 compared to the strongest baseline's 1.51 F1,
validating the effectiveness of our method.

</details>


### [31] [Pragmatic Theories Enhance Understanding of Implied Meanings in LLMs](https://arxiv.org/abs/2510.26253)
*Takuma Sato,Seiya Kawano,Koichiro Yoshino*

Main category: cs.CL

> 研究展示了为语言模型提供语用理论作为提示的在上下文中学习方法在理解暗示意义任务上的有效性，该方法较基准方法能提高性能。

<details>
  <summary>Details</summary>

**Motivation:** 研究动机是展示为语言模型提供语用理论作为提示是一种有效的在上下文中学习的方法，可以帮助模型理解暗示意义。

**Method:** 本研究提出了一种方法，即将语用理论概述（如格赖斯语用学和相关理论）作为语言模型的提示，引导其进行逐步推理以得出最终解释。

**Result:** 实验结果表明，与不提供语用理论仅提供中间推理提示的基准方法相比，本研究的方法能够使语言模型在语用推理任务上的得分提高最多9.6%。

**Conclusion:** 研究得出的结论是，即使只是在提示中提到语用理论的名字，而不解释其细节，也能使较大规模的语言模型相较于基准方法取得一定的性能提升（约1%-3%）。

**Abstract:** The ability to accurately interpret implied meanings plays a crucial role in
human communication and language use, and language models are also expected to
possess this capability. This study demonstrates that providing language models
with pragmatic theories as prompts is an effective in-context learning approach
for tasks to understand implied meanings. Specifically, we propose an approach
in which an overview of pragmatic theories, such as Gricean pragmatics and
Relevance Theory, is presented as a prompt to the language model, guiding it
through a step-by-step reasoning process to derive a final interpretation.
Experimental results showed that, compared to the baseline, which prompts
intermediate reasoning without presenting pragmatic theories (0-shot
Chain-of-Thought), our methods enabled language models to achieve up to 9.6\%
higher scores on pragmatic reasoning tasks. Furthermore, we show that even
without explaining the details of pragmatic theories, merely mentioning their
names in the prompt leads to a certain performance improvement (around 1-3%) in
larger models compared to the baseline.

</details>


### [32] [Language Models Are Borrowing-Blind: A Multilingual Evaluation of Loanword Identification across 10 Languages](https://arxiv.org/abs/2510.26254)
*Mérilin Sousa Silva,Sina Ahmadi*

Main category: cs.CL

> 研究发现预训练语言模型在识别借词和本地词汇方面的能力较差，这对开发支持弱势语言的NLP工具具有重要意义。

<details>
  <summary>Details</summary>

**Motivation:** 研究动机在于探索预训练语言模型是否能像双语环境中的讲者一样识别借词，特别是在强势语言对弱势语言产生影响的情况下。

**Method:** 该研究评估了多个预训练语言模型（包括大型语言模型）在10种不同语言中识别借词的能力。

**Result:** 研究结果显示，即使提供明确指令和上下文信息，模型在区分借词和本地词汇方面表现不佳。

**Conclusion:** 该研究发现证实了现代NLP系统在借词识别上存在向借词偏倚的现象，这对开发针对弱势语言的NLP工具和支持这些语言的保存具有重要意义。

**Abstract:** Throughout language history, words are borrowed from one language to another
and gradually become integrated into the recipient's lexicon. Speakers can
often differentiate these loanwords from native vocabulary, particularly in
bilingual communities where a dominant language continuously imposes lexical
items on a minority language. This paper investigates whether pretrained
language models, including large language models, possess similar capabilities
for loanword identification. We evaluate multiple models across 10 languages.
Despite explicit instructions and contextual information, our results show that
models perform poorly in distinguishing loanwords from native ones. These
findings corroborate previous evidence that modern NLP systems exhibit a bias
toward loanwords rather than native equivalents. Our work has implications for
developing NLP tools for minority languages and supporting language
preservation in communities under lexical pressure from dominant languages.

</details>


### [33] [Distilling Multilingual Vision-Language Models: When Smaller Models Stay Multilingual](https://arxiv.org/abs/2510.26271)
*Sukrit Sriratanawilai,Jhayahgrit Thongwat,Romrawin Chumpu,Patomporn Payoungkhamdee,Sarana Nutanong,Peerat Limkonchotiwat*

Main category: cs.CL

> 论文通过实验研究了五种知识蒸馏方法在多语言视觉语言模型中的效果，发现尽管模型大小减半，某些配置仍能改善跨语言检索的稳定性，但也存在无法维持跨任务稳定性的配置。

<details>
  <summary>Details</summary>

**Motivation:** 文章的动机在于探索在多语言环境下，应用知识蒸馏技术于视觉语言模型时，能否在保证模型压缩的同时提升或维持其性能，特别关注跨语言理解和任务稳定性的表现。

**Method:** 本文通过在一个多语言的视觉语言模型上进行知识蒸馏实验，研究了五种不同的蒸馏方法对于跨语言表示一致性以及模型压缩后下游任务性能稳定性的影响。

**Result:** 研究发现，某些蒸馏配置即使在模型大小减半的情况下也能保持或改善多语言检索的鲁棒性，但其他配置则未能维持跨任务的稳定性，揭示了准确率之外的设计敏感性权衡。

**Conclusion:** 结论指出，知识蒸馏在不同配置下对多语言视觉语言模型的影响各不相同，某些设计可以提高压缩模型的性能和稳定性，但也存在权衡选择的必要。

**Abstract:** Vision-language models (VLMs) exhibit uneven performance across languages, a
problem that is often exacerbated when the model size is reduced. While
Knowledge distillation (KD) demonstrates promising results in transferring
knowledge from larger to smaller VLMs, applying KD in multilingualism is an
underexplored area. This paper presents a controlled empirical study of KD
behavior across five distillation approaches, isolating their effects on
cross-lingual representation consistency and downstream performance stability
under model compression. We study five distillation formulations across CLIP
and SigLIP2, and evaluate them on in-domain retrieval and out-of-domain visual
QA. We find that some configurations preserve or even improve multilingual
retrieval robustness despite halving model size, but others fail to maintain
cross-task stability, exposing design-sensitive trade-offs that aggregate
accuracy alone does not reveal.

</details>


### [34] [Do LLMs Signal When They're Right? Evidence from Neuron Agreement](https://arxiv.org/abs/2510.26277)
*Kang Chen,Yaoning Wang,Kai Xiong,Zhuoka Feng,Wenhe Sun,Haotian Chen,Yixin Cao*

Main category: cs.CL

> The paper investigates internal neuron activations in large language models for enhancing ensemble decoding and introduces Neuron Agreement Decoding (NAD), an innovative method that achieves higher accuracy and reduced token usage by focusing on internal dynamics for selection among generated responses.

<details>
  <summary>Details</summary>

**Motivation:** Addresses the inadequacies of using only external signals for decoders in large language models, which can be poorly calibrated post-training. Seeks to improve label-free reasoning gains by leveraging internal dynamics.

**Method:** Proposes Neuron Agreement Decoding (NAD), an unsupervised method that utilizes internal neuron activations of LLMs for selecting the best generated response among multiple candidates.

**Result:** NAD matches the performance of majority voting on benchmarks with verifiable answers and outperforms Avg@64 on coding tasks, while reducing token usage by 99%.

**Conclusion:** Internal signals within large language models can serve as a reliable, scalable, and efficient guide for label-free ensemble decoding, with NAD showcasing significant improvements over traditional methods.

**Abstract:** Large language models (LLMs) commonly boost reasoning via
sample-evaluate-ensemble decoders, achieving label free gains without ground
truth. However, prevailing strategies score candidates using only external
outputs such as token probabilities, entropies, or self evaluations, and these
signals can be poorly calibrated after post training. We instead analyze
internal behavior based on neuron activations and uncover three findings: (1)
external signals are low dimensional projections of richer internal dynamics;
(2) correct responses activate substantially fewer unique neurons than
incorrect ones throughout generation; and (3) activations from correct
responses exhibit stronger cross sample agreement, whereas incorrect ones
diverge. Motivated by these observations, we propose Neuron Agreement Decoding
(NAD), an unsupervised best-of-N method that selects candidates using
activation sparsity and cross sample neuron agreement, operating solely on
internal signals and without requiring comparable textual outputs. NAD enables
early correctness prediction within the first 32 generated tokens and supports
aggressive early stopping. Across math and science benchmarks with verifiable
answers, NAD matches majority voting; on open ended coding benchmarks where
majority voting is inapplicable, NAD consistently outperforms Avg@64. By
pruning unpromising trajectories early, NAD reduces token usage by 99% with
minimal loss in generation quality, showing that internal signals provide
reliable, scalable, and efficient guidance for label free ensemble decoding.

</details>


### [35] [Unravelling the Mechanisms of Manipulating Numbers in Language Models](https://arxiv.org/abs/2510.26285)
*Michal Štefánik,Timothee Mickus,Marek Kadlčík,Bertram Højer,Michal Spiegel,Raúl Vázquez,Aman Sinha,Josef Kuchař,Philipp Mondorf*

Main category: cs.CL

> 研究发现尽管大语言模型容易在处理数字信息时出错，但它们学习到了一致的、高度准确的数字表示，这些表示在模型的不同隐藏层和输入上下文中都是统一的。

<details>
  <summary>Details</summary>

**Motivation:** 解释大语言模型在输入嵌入表示中表现出的高度一致性和准确性与在处理数字信息时容易产生错误输出的现象之间的矛盾。

**Method:** 通过探讨语言模型如何处理数字信息并量化这些机制的准确性的下限来解释这种冲突。

**Result:** 发现不同语言模型学习到了彼此可交换的、系统性和高度准确的数字表示方法，允许创建针对每个模型的通用探测器并将信息追溯到特定层级。

**Conclusion:** 此项研究为进一步理解和改进大语言模型处理数字信息的方式提供了基础。

**Abstract:** Recent work has shown that different large language models (LLMs) converge to
similar and accurate input embedding representations for numbers. These
findings conflict with the documented propensity of LLMs to produce erroneous
outputs when dealing with numeric information. In this work, we aim to explain
this conflict by exploring how language models manipulate numbers and quantify
the lower bounds of accuracy of these mechanisms. We find that despite
surfacing errors, different language models learn interchangeable
representations of numbers that are systematic, highly accurate and universal
across their hidden states and the types of input contexts. This allows us to
create universal probes for each LLM and to trace information -- including the
causes of output errors -- to specific layers. Our results lay a fundamental
understanding of how pre-trained LLMs manipulate numbers and outline the
potential of more accurate probing techniques in addressed refinements of LLMs'
architectures.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [36] [Enhancing Underwater Object Detection through Spatio-Temporal Analysis and Spatial Attention Networks](https://arxiv.org/abs/2510.25797)
*Sai Likhith Karri,Ansh Saxena*

Main category: cs.CV

> 本研究评估了时空模型和深度学习中空间注意力机制在水下目标检测中的有效性。结果表明，T-YOLOv5在动态海洋环境下的检测精度优于YOLOv5，而加入CBAM的T-YOLOv5进一步提高了在具有挑战性场景中的性能。

<details>
  <summary>Details</summary>

**Motivation:** 研究动机在于探索在水下环境中使用改进的时空模型和注意力机制，以提高目标检测方法的性能和鲁棒性。

**Method:** 方法包括，在第一个阶段评估了T-YOLOv5对比标准YOLOv5的性能；在第二阶段，通过添加CBAM构建了增强版的T-YOLOv5。

**Result:** 研究结果显示YOLOv5的mAP@50-95得分为0.563，而T-YOLOv5和带有CBAM的T-YOLOv5的mAP@50-95得分分别为0.813和0.811。

**Conclusion:** 结论指出，T-YOLOv5通过时间建模显著增强了检测可靠性，而T-YOLOv5加上CBAM在具有挑战性的场景中进一步提高了性能，尽管在简单场景中存在准确度损失。

**Abstract:** This study examines the effectiveness of spatio-temporal modeling and the
integration of spatial attention mechanisms in deep learning models for
underwater object detection. Specifically, in the first phase, the performance
of temporal-enhanced YOLOv5 variant T-YOLOv5 is evaluated, in comparison with
the standard YOLOv5. For the second phase, an augmented version of T-YOLOv5 is
developed, through the addition of a Convolutional Block Attention Module
(CBAM). By examining the effectiveness of the already pre-existing YOLOv5 and
T-YOLOv5 models and of the newly developed T-YOLOv5 with CBAM. With CBAM, the
research highlights how temporal modeling improves detection accuracy in
dynamic marine environments, particularly under conditions of sudden movements,
partial occlusions, and gradual motion. The testing results showed that YOLOv5
achieved a mAP@50-95 of 0.563, while T-YOLOv5 and T-YOLOv5 with CBAM
outperformed with mAP@50-95 scores of 0.813 and 0.811, respectively,
highlighting their superior accuracy and generalization in detecting complex
objects. The findings demonstrate that T-YOLOv5 significantly enhances
detection reliability compared to the standard model, while T-YOLOv5 with CBAM
further improves performance in challenging scenarios, although there is a loss
of accuracy when it comes to simpler scenarios.

</details>


### [37] [MIRO: MultI-Reward cOnditioned pretraining improves T2I quality and efficiency](https://arxiv.org/abs/2510.25897)
*Nicolas Dufour,Lucas Degeorge,Arijit Ghosh,Vicky Kalogeiton,David Picard*

Main category: cs.CV

> We introduce a method called MIRO, which conditions text-to-image models on multiple reward models during training, improving the visual quality and efficiency of generated images.

<details>
  <summary>Details</summary>

**Motivation:** To address the issues of reduced diversity, semantic fidelity, and efficiency that arise from using single reward models for post-hoc selection of generated images.

**Method:** We propose to condition text-to-image generative models on multiple reward models during training to let the model learn user preferences directly, rather than using post-hoc selection methods.

**Result:** Our proposed method, called MIRO, achieves state-of-the-art performances on GenEval compositional benchmark and high user-preference scores using different reward models.

**Conclusion:** The MIRO method demonstrates superior performance in learning user preferences directly during training, leading to enhanced visual quality and faster training times compared to conventional methods.

**Abstract:** Current text-to-image generative models are trained on large uncurated
datasets to enable diverse generation capabilities. However, this does not
align well with user preferences. Recently, reward models have been
specifically designed to perform post-hoc selection of generated images and
align them to a reward, typically user preference. This discarding of
informative data together with the optimizing for a single reward tend to harm
diversity, semantic fidelity and efficiency. Instead of this post-processing,
we propose to condition the model on multiple reward models during training to
let the model learn user preferences directly. We show that this not only
dramatically improves the visual quality of the generated images but it also
significantly speeds up the training. Our proposed method, called MIRO,
achieves state-of-the-art performances on the GenEval compositional benchmark
and user-preference scores (PickAScore, ImageReward, HPSv2).

</details>


### [38] [BikeScenes: Online LiDAR Semantic Segmentation for Bicycles](https://arxiv.org/abs/2510.25901)
*Denniz Goren,Holger Caesar*

Main category: cs.CV

> 本文开发了一种适用于自行车的LiDAR分割技术，通过引入新的BikeScenes-lidarseg数据集来进行模型训练和评估，展示了特定领域训练的必要性和有效性。

<details>
  <summary>Details</summary>

**Motivation:** 由于电动自行车的流行，骑车人的脆弱性变得更加严重。这篇文章的动机是将汽车感知技术适应到自行车安全中。

**Method:** 我们使用多传感器'SenseBike'研究平台来开发和评估一种针对自行车的3D LiDAR分割方法。为了弥补从汽车到自行车领域的差距，我们引入了一个新的BikeScenes-lidarseg数据集，包含在代尔夫特理工大学校园周围连续的3021个LiDAR扫描，对29个动态和静态类进行了语义注释。

**Result:** 通过在BikeScenes数据集上进行微调，模型的表现大大提高，达到了63.6%的平均交并比（mIoU），远高于仅使用SemanticKITTI预训练所获得的13.8%。

**Conclusion:** 这篇文章指出自行车安装的感知系统中硬件受限的关键挑战，并建议BikeScenes数据集作为研究自行车中心LiDAR分割的一个资源。

**Abstract:** The vulnerability of cyclists, exacerbated by the rising popularity of faster
e-bikes, motivates adapting automotive perception technologies for bicycle
safety. We use our multi-sensor 'SenseBike' research platform to develop and
evaluate a 3D LiDAR segmentation approach tailored to bicycles. To bridge the
automotive-to-bicycle domain gap, we introduce the novel BikeScenes-lidarseg
Dataset, comprising 3021 consecutive LiDAR scans around the university campus
of the TU Delft, semantically annotated for 29 dynamic and static classes. By
evaluating model performance, we demonstrate that fine-tuning on our BikeScenes
dataset achieves a mean Intersection-over-Union (mIoU) of 63.6%, significantly
outperforming the 13.8% obtained with SemanticKITTI pre-training alone. This
result underscores the necessity and effectiveness of domain-specific training.
We highlight key challenges specific to bicycle-mounted, hardware-constrained
perception systems and contribute the BikeScenes dataset as a resource for
advancing research in cyclist-centric LiDAR segmentation.

</details>


### [39] [Generative Image Restoration and Super-Resolution using Physics-Informed Synthetic Data for Scanning Tunneling Microscopy](https://arxiv.org/abs/2510.25921)
*Nikola L. Kolev,Tommaso Rodani,Neil J. Curson,Taylor J. Z. Stock,Alberto Cazzaniga*

Main category: cs.CV

> 论文提出了一种机器学习方法，用于STM图像的修复和超分辨率，从而提高了STM的实验效率。

<details>
  <summary>Details</summary>

**Motivation:** 开发一种方法，解决STM的针尖退化和低数据采集群速度问题，以改善STM的成像效果并加快实验过程。

**Method:** 使用机器学习方法，通过分析36张Si(001):H的实验图像进行训练，其中包含流匹配和扩散模型。

**Result:** 此论文提出了一种机器学习方法，用于解决扫描隧道显微镜（STM）中的图像修复和超分辨率问题。基于36张Si(001):H的实验图像，使用物理信息合成数据生成管道训练了多种最先进的流匹配和扩散模型。实验结果表明，该模型可以有效恢复图像，并通过从稀疏采样数据中准确重建图像将图像采集时间减少两到四倍，从而提高STM实验的吞吐量。

**Conclusion:** 该框架具有巨大潜力，可以显著提高STM实验的总吞吐量，通过减少针尖调节的频率并提高现有高速STM系统的帧率。

**Abstract:** Scanning tunnelling microscopy (STM) enables atomic-resolution imaging and
atom manipulation, but its utility is often limited by tip degradation and slow
serial data acquisition. Fabrication adds another layer of complexity since the
tip is often subjected to large voltages, which may alter the shape of its
apex, requiring it to be conditioned. Here, we propose a machine learning (ML)
approach for image repair and super-resolution to alleviate both challenges.
Using a dataset of only 36 pristine experimental images of Si(001):H, we
demonstrate that a physics-informed synthetic data generation pipeline can be
used to train several state-of-the-art flow-matching and diffusion models.
Quantitative evaluation with metrics such as the CLIP Maximum Mean Discrepancy
(CMMD) score and structural similarity demonstrates that our models are able to
effectively restore images and offer a two- to fourfold reduction in image
acquisition time by accurately reconstructing images from sparsely sampled
data. Our framework has the potential to significantly increase STM
experimental throughput by offering a route to reducing the frequency of
tip-conditioning procedures and to enhancing frame rates in existing high-speed
STM systems.

</details>


### [40] [SplitFlow: Flow Decomposition for Inversion-Free Text-to-Image Editing](https://arxiv.org/abs/2510.25970)
*Sung-Hoon Yoon,Minghan Li,Gaspard Beaudouin,Congcong Wen,Muhammad Rafay Azhar,Mengyu Wang*

Main category: cs.CV

> 研究提出了一种新的基于流分解和聚合的图像编辑方法，通过无逆映射框架解决了现有流模型在图像编辑中的局限，展现了高质量的零样本编辑能力。

<details>
  <summary>Details</summary>

**Motivation:** 该研究的动机在于解决现有的流模型在图像编辑任务中的局限性，特别是准确度不高和编辑过程中梯度缠结的问题。

**Method:** 我们提出了一种基于分解和聚合的无逆映射框架，通过将目标提示语分解为多个子提示语，并为每个子提示语计算独立的流，然后聚合以形成统一的编辑轨迹来克服现有问题。我们的方法还包括设计的投影和软聚合机制，用于解决多任务学习中的梯度冲突问题，从而自适应地加权子目标速度场，减少语义冗余并强调不同的方向。

**Result:** 实验结果表明，我们的方法在语义保真度和属性解耦方面优于现有的零样本编辑方法。

**Conclusion:** 本研究提出了一种新的流分解和聚合框架，通过无逆映射的方式解决了图像编辑的关键限制。

**Abstract:** Rectified flow models have become a de facto standard in image generation due
to their stable sampling trajectories and high-fidelity outputs. Despite their
strong generative capabilities, they face critical limitations in image editing
tasks: inaccurate inversion processes for mapping real images back into the
latent space, and gradient entanglement issues during editing often result in
outputs that do not faithfully reflect the target prompt. Recent efforts have
attempted to directly map source and target distributions via ODE-based
approaches without inversion; however,these methods still yield suboptimal
editing quality. In this work, we propose a flow decomposition-and-aggregation
framework built upon an inversion-free formulation to address these
limitations. Specifically, we semantically decompose the target prompt into
multiple sub-prompts, compute an independent flow for each, and aggregate them
to form a unified editing trajectory. While we empirically observe that
decomposing the original flow enhances diversity in the target space,
generating semantically aligned outputs still requires consistent guidance
toward the full target prompt. To this end, we design a projection and
soft-aggregation mechanism for flow, inspired by gradient conflict resolution
in multi-task learning. This approach adaptively weights the sub-target
velocity fields, suppressing semantic redundancy while emphasizing distinct
directions, thereby preserving both diversity and consistency in the final
edited output. Experimental results demonstrate that our method outperforms
existing zero-shot editing approaches in terms of semantic fidelity and
attribute disentanglement. The code is available at
https://github.com/Harvard-AI-and-Robotics-Lab/SplitFlow.

</details>


### [41] [Brain-IT: Image Reconstruction from fMRI via Brain-Interaction Transformer](https://arxiv.org/abs/2510.25976)
*Roman Beliy,Amit Zalcher,Jonathan Kogman,Navve Wasserman,Michal Irani*

Main category: cs.CV

> The study introduces Brain-IT, a brain-inspired method using a Brain Interaction Transformer (BIT) to facilitate effective interactions between brain-voxel clusters for accurate image reconstruction from fMRI, surpassing state-of-the-art methods by visual and objective metrics, even with limited data.

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind this study is to tackle the challenge of producing faithful image reconstructions from fMRI brain recordings, as current methods often fall short in their accuracy, and to do so with less data than current approaches.

**Method:** Brain-IT method uses a Brain Interaction Transformer (BIT) to effectively interact between clusters of functionally-similar brain-voxels, guiding the image reconstruction process through high-level semantic and low-level structural features.

**Result:** The method achieves more faithful image reconstructions from fMRI data compared to state-of-the-art methods, as measured by visual fidelity and standard objective metrics. It also demonstrates comparable performance to current methods with significantly less training data.

**Conclusion:** BIT's approach to image reconstruction from fMRI data, employing functional-clusters interaction and guided by both high-level semantic and low-level structural features, outperforms current top methods and does so efficiently with limited data.

**Abstract:** Reconstructing images seen by people from their fMRI brain recordings
provides a non-invasive window into the human brain. Despite recent progress
enabled by diffusion models, current methods often lack faithfulness to the
actual seen images. We present "Brain-IT", a brain-inspired approach that
addresses this challenge through a Brain Interaction Transformer (BIT),
allowing effective interactions between clusters of functionally-similar
brain-voxels. These functional-clusters are shared by all subjects, serving as
building blocks for integrating information both within and across brains. All
model components are shared by all clusters & subjects, allowing efficient
training with a limited amount of data. To guide the image reconstruction, BIT
predicts two complementary localized patch-level image features: (i)high-level
semantic features which steer the diffusion model toward the correct semantic
content of the image; and (ii)low-level structural features which help to
initialize the diffusion process with the correct coarse layout of the image.
BIT's design enables direct flow of information from brain-voxel clusters to
localized image features. Through these principles, our method achieves image
reconstructions from fMRI that faithfully reconstruct the seen images, and
surpass current SotA approaches both visually and by standard objective
metrics. Moreover, with only 1-hour of fMRI data from a new subject, we achieve
results comparable to current methods trained on full 40-hour recordings.

</details>


### [42] [Fine-tuning Segment Anything for Real-Time Tumor Tracking in Cine-MRI](https://arxiv.org/abs/2510.25990)
*Valentin Boussot,Cédric Hémon,Jean-Claude Nunes,Jean-Louis Dillenseger*

Main category: cs.CV

> 在强数据稀缺条件下，通过基于提示的基础模型SAM 2.1进行可靠的实时肿瘤跟踪，取得了良好的Dice相似系数结果。

<details>
  <summary>Details</summary>

**Motivation:** 解决在胸部和腹部区域实时MRI序列中，在数据稀缺的条件下进行肿瘤跟踪的难题。

**Method:** 探讨了两种策略：(i) 使用IMPACT相似度度量的无监督注册；(ii) 基于SAM 2.1及其变体的提示式基础模型分割。最终选择了SAM-based方法作为解决方案。

**Result:** {BaseContext}.structure_result

**Conclusion:** 该方法在TrackRAD2025挑战中达到了0.8794的Dice得分，排名第六，证明了基础模型在MRI引导的放射治疗中肿瘤跟踪的应用潜力。

**Abstract:** In this work, we address the TrackRAD2025 challenge of real-time tumor
tracking in cine-MRI sequences of the thoracic and abdominal regions under
strong data scarcity constraints. Two complementary strategies were explored:
(i) unsupervised registration with the IMPACT similarity metric and (ii)
foundation model-based segmentation leveraging SAM 2.1 and its recent variants
through prompt-based interaction. Due to the one-second runtime constraint, the
SAM-based method was ultimately selected. The final configuration used SAM2.1
b+ with mask-based prompts from the first annotated slice, fine-tuned solely on
the small labeled subset from TrackRAD2025. Training was configured to minimize
overfitting, using 1024x1024 patches (batch size 1), standard augmentations,
and a balanced Dice + IoU loss. A low uniform learning rate (0.0001) was
applied to all modules (prompt encoder, decoder, Hiera backbone) to preserve
generalization while adapting to annotator-specific styles. Training lasted 300
epochs (~12h on RTX A6000, 48GB). The same inference strategy was consistently
applied across all anatomical sites and MRI field strengths. Test-time
augmentation was considered but ultimately discarded due to negligible
performance gains. The final model was selected based on the highest Dice
Similarity Coefficient achieved on the validation set after fine-tuning. On the
hidden test set, the model reached a Dice score of 0.8794, ranking 6th overall
in the TrackRAD2025 challenge. These results highlight the strong potential of
foundation models for accurate and real-time tumor tracking in MRI-guided
radiotherapy.

</details>


### [43] [Larger Hausdorff Dimension in Scanning Pattern Facilitates Mamba-Based Methods in Low-Light Image Enhancement](https://arxiv.org/abs/2510.26001)
*Xinhua Wang,Caibo Feng,Xiangjun Fu,Chunxiao Liu*

Main category: cs.CV

> 通过引入Hilbert Selective Scan机制提升Mamba框架的扫描模式的豪斯多夫维度，提高了细节捕捉能力和空间局部性，同时减少了计算资源消耗和推断时间，显著提升了低光图像增强的效果。

<details>
  <summary>Details</summary>

**Motivation:** 改进Mamba框架以更好地捕捉细节并提高空间局部性，同时优化计算资源使用和推断时间。

**Method:** 提出Hilbert Selective Scan机制提升扫描模式的豪斯多夫维度，改进Mamba框架。

**Result:** 在公开基准测试中展示了方法的有效性，提高了图像增强的质量和效率。

**Conclusion:** 该方法不仅增强了低光图像增强的效果，也为更广泛领域的Mamba技术应用提供了可能。

**Abstract:** We propose an innovative enhancement to the Mamba framework by increasing the
Hausdorff dimension of its scanning pattern through a novel Hilbert Selective
Scan mechanism. This mechanism explores the feature space more effectively,
capturing intricate fine-scale details and improving overall coverage. As a
result, it mitigates information inconsistencies while refining spatial
locality to better capture subtle local interactions without sacrificing the
model's ability to handle long-range dependencies. Extensive experiments on
publicly available benchmarks demonstrate that our approach significantly
improves both the quantitative metrics and qualitative visual fidelity of
existing Mamba-based low-light image enhancement methods, all while reducing
computational resource consumption and shortening inference time. We believe
that this refined strategy not only advances the state-of-the-art in low-light
image enhancement but also holds promise for broader applications in fields
that leverage Mamba-based techniques.

</details>


### [44] [CAVE: Detecting and Explaining Commonsense Anomalies in Visual Environments](https://arxiv.org/abs/2510.26006)
*Rishika Bhagwatkar,Syrielle Montariol,Angelika Romanou,Beatriz Borges,Irina Rish,Antoine Bosselut*

Main category: cs.CV

> 该论文提出了CAVE基准，专注于现实世界中视觉异常的检测与理解，揭示了现有模型在感知异常和常识推理上的不足。

<details>
  <summary>Details</summary>

**Motivation:** 现有的异常检测在计算机视觉领域局限于工业缺陷或合成生成的异常，无法捕捉现实世界异常的丰富性和不可预测性。这个基准测试旨在填补这一研究空白。

**Method:** 介绍了一个名为CAVE的新基准，专注于现实中视觉异常的描述、解释和证明三大开放式任务，并使用细粒度标注来辅助视觉定位和分类。

**Result:** 结果显示最先进的Vision-Language Models (VLMs)在感知视觉异常和进行常识推理方面存在问题，即便采用了高级的提示策略。

**Conclusion:** 通过提供一个现实且有认知基础的基准测试，CAVE作为推进VLMs在异常检测和常识推理方面研究的一个宝贵资源。

**Abstract:** Humans can naturally identify, reason about, and explain anomalies in their
environment. In computer vision, this long-standing challenge remains limited
to industrial defects or unrealistic, synthetically generated anomalies,
failing to capture the richness and unpredictability of real-world anomalies.
In this work, we introduce CAVE, the first benchmark of real-world visual
anomalies. CAVE supports three open-ended tasks: anomaly description,
explanation, and justification; with fine-grained annotations for visual
grounding and categorizing anomalies based on their visual manifestations,
their complexity, severity, and commonness. These annotations draw inspiration
from cognitive science research on how humans identify and resolve anomalies,
providing a comprehensive framework for evaluating Vision-Language Models
(VLMs) in detecting and understanding anomalies. We show that state-of-the-art
VLMs struggle with visual anomaly perception and commonsense reasoning, even
with advanced prompting strategies. By offering a realistic and cognitively
grounded benchmark, CAVE serves as a valuable resource for advancing research
in anomaly detection and commonsense reasoning in VLMs.

</details>


### [45] [Climate Adaptation-Aware Flood Prediction for Coastal Cities Using Deep Learning](https://arxiv.org/abs/2510.26017)
*Bilal Hassan,Areg Karapetyan,Aaron Chung Hin Chow,Samer Madanat*

Main category: cs.CV

> A lightweight CNN-based model for coastal flood prediction under various sea-level rise scenarios outperforms existing methods with a 20% reduction in mean absolute error.

<details>
  <summary>Details</summary>

**Motivation:** Traditional methods for predicting flood hazards are too computationally expensive for practical city-scale planning. This paper aims to provide an efficient and accurate alternative using deep learning.

**Method:** The paper introduces a vision-based, low-resource DL framework, leveraging CNN-based models to predict flooding under different sea-level rise and shoreline adaptation scenarios.

**Result:** The model demonstrates superior performance in predicting flood depth, with a nearly 20% decrease in MAE compared to state-of-the-art methods, validated across datasets from Abu Dhabi and San Francisco.

**Conclusion:** The proposed model is a scalable and practical tool for coastal flood management, offering a promising solution for decision-makers to implement effective climate change mitigation strategies.

**Abstract:** Climate change and sea-level rise (SLR) pose escalating threats to coastal
cities, intensifying the need for efficient and accurate methods to predict
potential flood hazards. Traditional physics-based hydrodynamic simulators,
although precise, are computationally expensive and impractical for city-scale
coastal planning applications. Deep Learning (DL) techniques offer promising
alternatives, however, they are often constrained by challenges such as data
scarcity and high-dimensional output requirements. Leveraging a recently
proposed vision-based, low-resource DL framework, we develop a novel,
lightweight Convolutional Neural Network (CNN)-based model designed to predict
coastal flooding under variable SLR projections and shoreline adaptation
scenarios. Furthermore, we demonstrate the ability of the model to generalize
across diverse geographical contexts by utilizing datasets from two distinct
regions: Abu Dhabi and San Francisco. Our findings demonstrate that the
proposed model significantly outperforms state-of-the-art methods, reducing the
mean absolute error (MAE) in predicted flood depth maps on average by nearly
20%. These results highlight the potential of our approach to serve as a
scalable and practical tool for coastal flood management, empowering
decision-makers to develop effective mitigation strategies in response to the
growing impacts of climate change. Project Page: https://caspiannet.github.io/

</details>


### [46] [Enhancing Temporal Understanding in Video-LLMs through Stacked Temporal Attention in Vision Encoders](https://arxiv.org/abs/2510.26027)
*Ali Rasekh,Erfan Bagheri Soula,Omid Daliran,Simon Gottschalk,Mohsen Fayyaz*

Main category: cs.CV

> 研究指出，新型Video-LLM通过增强时间注意力机制，在视频问答和动作识别任务上表现出色，提高了理解视频时间动态的能力。

<details>
  <summary>Details</summary>

**Motivation:** 当前的Video-LLM架构在理解视频中的时间动态方面存在局限性，特别是在需要详细理解动作序列和时间进展的任务上表现不佳。

**Method:** 本文提出了一种新的Video-LLM架构，通过在视觉编码器中引入堆叠的时间注意力模块，增强了对视频中时间动态的理解能力。

**Result:** 实验结果显示，本文的方法显著提升了时间推理能力，在视频问答任务上的表现优于现有模型，特别是在动作识别方面，提升了最多5.5%的性能。

**Conclusion:** 通过改进视觉编码器中的时间结构，本研究解决了Video-LLM在视频理解中的关键问题。

**Abstract:** Despite significant advances in Multimodal Large Language Models (MLLMs),
understanding complex temporal dynamics in videos remains a major challenge.
Our experiments show that current Video Large Language Model (Video-LLM)
architectures have critical limitations in temporal understanding, struggling
with tasks that require detailed comprehension of action sequences and temporal
progression. In this work, we propose a Video-LLM architecture that introduces
stacked temporal attention modules directly within the vision encoder. This
design incorporates a temporal attention in vision encoder, enabling the model
to better capture the progression of actions and the relationships between
frames before passing visual tokens to the LLM. Our results show that this
approach significantly improves temporal reasoning and outperforms existing
models in video question answering tasks, specifically in action recognition.
We improve on benchmarks including VITATECS, MVBench, and Video-MME by up to
+5.5%. By enhancing the vision encoder with temporal structure, we address a
critical gap in video understanding for Video-LLMs. Project page and code are
available at: https://alirasekh.github.io/STAVEQ2/.

</details>


### [47] [FlexICL: A Flexible Visual In-context Learning Framework for Elbow and Wrist Ultrasound Segmentation](https://arxiv.org/abs/2510.26049)
*Yuyue Zhou,Jessica Knight,Shrimanti Ghosh,Banafshe Felfeliyan,Jacob L. Jaremko,Abhilash R. Hareendranathan*

Main category: cs.CV

> 研究了全新的上下文学习框架FlexICL，通过少量标注样本实现超声图像的高效分割，显著提高儿童骨折诊断的准确性，减少了手动标注的需求。

<details>
  <summary>Details</summary>

**Motivation:** 儿童群体中肘部和腕部骨折是最常见的骨折类型。通过自动分割肌肉骨骼结构，超声诊断的准确性及治疗规划均可得到提升。但是，获取用于训练的像素级专家标注数据耗时费力。因此，此研究旨在通过减少标注需求来提高诊断效率并降低成本。

**Method:** 提出了一种新的灵活的上下文学习框架FlexICL，用于在超声图像中分割骨骼区域。此框架应用于视频内分割环境中，其中专家仅标注一小部分帧，而模型能够分割未见帧。研究了多种图像拼接技术和训练策略，并引入新的拼接方法，显著提升了模型在有限标签数据下的表现。通过集成多种增强策略，FlexICL仅需5%的训练图像即可实现稳健的分割性能。

**Result:** FlexICL在四个腕部和肘部超声数据集中实现了比最先进的视觉ICL模型（Painter, MAE-VQGAN）和传统分割模型（U-Net, TransUNet）更高的分割性能，Dice系数提高了1-27%，基于1,252次超声扫描结果。

**Conclusion:** FlexICL作为超声图像分割的一种有效和可扩展的解决方案，特别是在标记数据稀缺的医学成像使用案例中展现出巨大的潜力。

**Abstract:** Elbow and wrist fractures are the most common fractures in pediatric
populations. Automatic segmentation of musculoskeletal structures in ultrasound
(US) can improve diagnostic accuracy and treatment planning. Fractures appear
as cortical defects but require expert interpretation. Deep learning (DL) can
provide real-time feedback and highlight key structures, helping lightly
trained users perform exams more confidently. However, pixel-wise expert
annotations for training remain time-consuming and costly. To address this
challenge, we propose FlexICL, a novel and flexible in-context learning (ICL)
framework for segmenting bony regions in US images. We apply it to an
intra-video segmentation setting, where experts annotate only a small subset of
frames, and the model segments unseen frames. We systematically investigate
various image concatenation techniques and training strategies for visual ICL
and introduce novel concatenation methods that significantly enhance model
performance with limited labeled data. By integrating multiple augmentation
strategies, FlexICL achieves robust segmentation performance across four wrist
and elbow US datasets while requiring only 5% of the training images. It
outperforms state-of-the-art visual ICL models like Painter, MAE-VQGAN, and
conventional segmentation models like U-Net and TransUNet by 1-27% Dice
coefficient on 1,252 US sweeps. These initial results highlight the potential
of FlexICL as an efficient and scalable solution for US image segmentation well
suited for medical imaging use cases where labeled data is scarce.

</details>


### [48] [Dynamic VLM-Guided Negative Prompting for Diffusion Models](https://arxiv.org/abs/2510.26052)
*Hoyeon Chang,Seungjin Kim,Yoonseok Choi*

Main category: cs.CV

> 本文提出了一种创新的动态负提示方法，利用视觉-语言模型在扩散模型中自动生成适应性负提示，以改善图像生成的质量和文本-图像对齐。

<details>
  <summary>Details</summary>

**Motivation:** 为了提高扩散模型在生成图像时的灵活性和准确性，我们引入了一种新的方法来动态生成负提示，从而在去噪过程中提供更精细的控制。

**Method:** 我们提出了一种在扩散模型中使用视觉-语言模型（VLM）自适应地生成负提示的新型动态负提示方法。与传统的使用固定负提示的负提示方法不同，我们在特定的去噪步骤中生成中间图像预测，并查询VLM以产生与上下文相关的负提示。

**Result:** 我们在各种基准数据集上评估了我们的方法，并展示了负提示强度和文本-图像对齐之间的权衡。

**Conclusion:** 实验表明，这种方法可以在保持图像质量的同时，通过调整负提示的强度来改善文本-图像对齐的效果。

**Abstract:** We propose a novel approach for dynamic negative prompting in diffusion
models that leverages Vision-Language Models (VLMs) to adaptively generate
negative prompts during the denoising process. Unlike traditional Negative
Prompting methods that use fixed negative prompts, our method generates
intermediate image predictions at specific denoising steps and queries a VLM to
produce contextually appropriate negative prompts. We evaluate our approach on
various benchmark datasets and demonstrate the trade-offs between negative
guidance strength and text-image alignment.

</details>


### [49] [Security Risk of Misalignment between Text and Image in Multi-modal Model](https://arxiv.org/abs/2510.26105)
*Xiaosen Wang,Zhijin Ge,Shaokang Wang*

Main category: cs.CV

> 研究揭示了多模态扩散模型（如文本到图像模型）易受到对抗性输入的影响，并提出了一种新型攻击方法PReMA，通过修改输入图像来操纵生成内容，无需改变提示词，并证明了其在图像修复和风格转换任务中的有效性。

<details>
  <summary>Details</summary>

**Motivation:** 现有的多模态扩散模型在面对对抗性输入时的脆弱性未被充分探索，尤其是文本与图像模态对齐不足可能导致生成不适当或NSFW内容，威胁模型完整性。

**Method:** Structure

**Result:** PReMA攻击首次通过创建对抗性图像来操纵生成内容，证明了其在图像修复和风格转换任务中的高效性。

**Conclusion:** PReMA作为一种新的威胁形式，对多模态扩散模型的完整性和固定提示词下的图像编辑应用构成了新的挑战。

**Abstract:** Despite the notable advancements and versatility of multi-modal diffusion
models, such as text-to-image models, their susceptibility to adversarial
inputs remains underexplored. Contrary to expectations, our investigations
reveal that the alignment between textual and Image modalities in existing
diffusion models is inadequate. This misalignment presents significant risks,
especially in the generation of inappropriate or Not-Safe-For-Work (NSFW)
content. To this end, we propose a novel attack called Prompt-Restricted
Multi-modal Attack (PReMA) to manipulate the generated content by modifying the
input image in conjunction with any specified prompt, without altering the
prompt itself. PReMA is the first attack that manipulates model outputs by
solely creating adversarial images, distinguishing itself from prior methods
that primarily generate adversarial prompts to produce NSFW content.
Consequently, PReMA poses a novel threat to the integrity of multi-modal
diffusion models, particularly in image-editing applications that operate with
fixed prompts. Comprehensive evaluations conducted on image inpainting and
style transfer tasks across various models confirm the potent efficacy of
PReMA.

</details>


### [50] [EgoExo-Con: Exploring View-Invariant Video Temporal Understanding](https://arxiv.org/abs/2510.26113)
*Minjoon Jung,Junbin Xiao,Junghyun Kim,Byoung-Tak Zhang,Angela Yao*

Main category: cs.CV

> 研究视频大模型在不同视角视频中的一致时间理解能力，提出了一种新的强化学习框架，提高了跨视角的一致性。

<details>
  <summary>Details</summary>

**Motivation:** 为了研究视频大模型在从不同视角捕捉同一事件的视频时能否实现一致的时间理解，引入了EgoExo-Con这一基准。

**Method:** 提出了一种新的强化学习框架View-GRPO，旨在增强针对不同视角的时间推理能力，并鼓励跨视角的一致性理解。

**Result:** 分析揭示了现有视频大模型的两个关键限制：难以保持一致性；以端到端的方式使用同步视频进行微调时，虽然一致性有所提高，但在某些情况下性能不如仅在一个视角上进行训练。

**Conclusion:** View-GRPO方法相对于简单的SFT和GRPO方法表现出了优势，特别是在改善跨视角一致性方面，并且所有资源将公开提供。

**Abstract:** Can Video-LLMs achieve consistent temporal understanding when videos capture
the same event from different viewpoints? To study this, we introduce
EgoExo-Con (Consistency), a benchmark of comprehensively synchronized
egocentric and exocentric video pairs with human-refined queries in natural
language. EgoExo-Con emphasizes two temporal understanding tasks: Temporal
Verification and Temporal Grounding. It evaluates not only correctness but
consistency across viewpoints. Our analysis reveals two critical limitations of
existing Video-LLMs: (1) models often fail to maintain consistency, with
results far worse than their single-view performances. (2) When naively
finetuned with synchronized videos of both viewpoints, the models show improved
consistency but often underperform those trained on a single view. For
improvements, we propose View-GRPO, a novel reinforcement learning framework
that effectively strengthens view-specific temporal reasoning while encouraging
consistent comprehension across viewpoints. Our method demonstrates its
superiority over naive SFT and GRPO, especially for improving cross-view
consistency. All resources will be made publicly available.

</details>


### [51] [OracleAgent: A Multimodal Reasoning Agent for Oracle Bone Script Research](https://arxiv.org/abs/2510.26114)
*Caoshuo Li,Zengmao Ding,Xiaobin Hu,Bang Li,Donghao Luo,Xu Peng,Taisong Jin,Yongge Liu,Shengwei Han,Jing Yang,Xiaoping He,Feng Gao,AndyPian Wu,SevenShu,Chaoyang Wang,Chengjie Wang*

Main category: cs.CV

> OracleAgent是首个针对甲骨文信息管理与检索设计的智能系统，集成了多种分析工具并能够利用大型语言模型来协同工作，构建了包含丰富资源的多模态知识库。实验表明其在多模态推理与生成任务中表现出色。

<details>
  <summary>Details</summary>

**Motivation:** 针对甲骨文释读中存在的复杂工作流程以及信息组织和检索效率低下的问题，设计了OracleAgent系统，以便更好地管理、检索与甲骨文相关的信息。

**Method:** 设计了一个名为OracleAgent的智能系统，该系统集成了多个甲骨文分析工具，并利用大型语言模型(如LLMs)灵活地协同这些工具。此外，构建了一个全面的专用多模态知识库，包含了超过140万个单字拓片图像和80000个释文文本，以支持OracleAgent的检索任务。

**Result:** 实验显示，OracleAgent在多模态推理和生成任务中表现出色，超越了现有的多模态大型语言模型如GPT-4，同时通过案例研究验证了其能显著减少甲骨文研究的时间成本。

**Conclusion:** OracleAgent为辅助甲骨文研究和自动释读系统的实际部署迈出了重要一步，展示了在未来研究中的潜力。

**Abstract:** As one of the earliest writing systems, Oracle Bone Script (OBS) preserves
the cultural and intellectual heritage of ancient civilizations. However,
current OBS research faces two major challenges: (1) the interpretation of OBS
involves a complex workflow comprising multiple serial and parallel sub-tasks,
and (2) the efficiency of OBS information organization and retrieval remains a
critical bottleneck, as scholars often spend substantial effort searching for,
compiling, and managing relevant resources. To address these challenges, we
present OracleAgent, the first agent system designed for the structured
management and retrieval of OBS-related information. OracleAgent seamlessly
integrates multiple OBS analysis tools, empowered by large language models
(LLMs), and can flexibly orchestrate these components. Additionally, we
construct a comprehensive domain-specific multimodal knowledge base for OBS,
which is built through a rigorous multi-year process of data collection,
cleaning, and expert annotation. The knowledge base comprises over 1.4M
single-character rubbing images and 80K interpretation texts. OracleAgent
leverages this resource through its multimodal tools to assist experts in
retrieval tasks of character, document, interpretation text, and rubbing image.
Extensive experiments demonstrate that OracleAgent achieves superior
performance across a range of multimodal reasoning and generation tasks,
surpassing leading mainstream multimodal large language models (MLLMs) (e.g.,
GPT-4o). Furthermore, our case study illustrates that OracleAgent can
effectively assist domain experts, significantly reducing the time cost of OBS
research. These results highlight OracleAgent as a significant step toward the
practical deployment of OBS-assisted research and automated interpretation
systems.

</details>


### [52] [JOGS: Joint Optimization of Pose Estimation and 3D Gaussian Splatting](https://arxiv.org/abs/2510.26117)
*Yuxuan Li,Tao Wang,Xianben Yang*

Main category: cs.CV

> 提出了一种新型框架用于改进传统视图合成方法，无需外部相机姿态估计工具，解决了计算瓶颈和错误传播。

<details>
  <summary>Details</summary>

**Motivation:** 传统的视图合成方法依赖于外部相机姿态估计工具，如COLMAP，这些工具往往引入计算瓶颈和错误传播。

**Method:** 该论文提出了一种统一的框架，该框架能够同时优化3D高斯点和相机姿态，而不需要预校准的输入。方法包括两个交替的过程：1) 使用固定姿态的可微渲染来更新3D高斯参数；2) 使用结合了几何和辐射约束的定制3D光流算法来细化相机姿态。

**Result:** 实验结果表明，该方法在重建质量上显著优于现有的无需COLMAP的方法，甚至超过了基于COLMAP的标准方法。

**Conclusion:** 该框架通过解耦联优化策略，分别对3D高斯参数进行可微渲染更新，随后使用定制的3D光流算法更新相机姿态，从而提高重建的准确性和姿态的精确性，这些问题在大的视角变化和稀疏特征分布场景中尤其突出。

**Abstract:** Traditional novel view synthesis methods heavily rely on external camera pose
estimation tools such as COLMAP, which often introduce computational
bottlenecks and propagate errors. To address these challenges, we propose a
unified framework that jointly optimizes 3D Gaussian points and camera poses
without requiring pre-calibrated inputs. Our approach iteratively refines 3D
Gaussian parameters and updates camera poses through a novel co-optimization
strategy, ensuring simultaneous improvements in scene reconstruction fidelity
and pose accuracy. The key innovation lies in decoupling the joint optimization
into two interleaved phases: first, updating 3D Gaussian parameters via
differentiable rendering with fixed poses, and second, refining camera poses
using a customized 3D optical flow algorithm that incorporates geometric and
photometric constraints. This formulation progressively reduces projection
errors, particularly in challenging scenarios with large viewpoint variations
and sparse feature distributions, where traditional methods struggle. Extensive
evaluations on multiple datasets demonstrate that our approach significantly
outperforms existing COLMAP-free techniques in reconstruction quality, and also
surpasses the standard COLMAP-based baseline in general.

</details>


### [53] [WOD-E2E: Waymo Open Dataset for End-to-End Driving in Challenging Long-tail Scenarios](https://arxiv.org/abs/2510.26125)
*Runsheng Xu,Hubert Lin,Wonseok Jeon,Hao Feng,Yuliang Zou,Liting Sun,John Gorman,Kate Tolstaya,Sarah Tang,Brandyn White,Ben Sapp,Mingxing Tan,Jyh-Jing Hwang,Drago Anguelov*

Main category: cs.CV

> 本文提出WOD-E2E数据集和RFS评估指标，以解决当前E2E自动驾驶基准和评估指标的问题，推动自动驾驶研究。

<details>
  <summary>Details</summary>

**Motivation:** 针对现有E2E驾驶基准测试主要是名义场景和开放环评估指标不完善的缺点，旨在促进能处理复杂现实情况的E2E自动驾驶研究。

**Method:** 介绍Waymo Open Dataset for End-to-End Driving (WOD-E2E)，包含4,021个驾驶片段，专注于罕见的长尾场景，提供高精度路由信息、车辆状态和8个周围摄像头的360度视角。提出Rater Feedback Score (RFS)用于评估E2E驾驶性能，该指标衡量预测轨迹与标注轨迹的匹配程度。

**Result:** 开发了WOD-E2E数据集和RFS评估指标，用于释放自动化驾驶在长尾场景中的潜力。

**Conclusion:** 通过WOD-E2E数据集和RFS评估指标，促进E2E自动驾驶研究的发展，使其更通用、更稳健、更安全。

**Abstract:** Vision-based end-to-end (E2E) driving has garnered significant interest in
the research community due to its scalability and synergy with multimodal large
language models (MLLMs). However, current E2E driving benchmarks primarily
feature nominal scenarios, failing to adequately test the true potential of
these systems. Furthermore, existing open-loop evaluation metrics often fall
short in capturing the multi-modal nature of driving or effectively evaluating
performance in long-tail scenarios. To address these gaps, we introduce the
Waymo Open Dataset for End-to-End Driving (WOD-E2E). WOD-E2E contains 4,021
driving segments (approximately 12 hours), specifically curated for challenging
long-tail scenarios that that are rare in daily life with an occurring
frequency of less than 0.03%. Concretely, each segment in WOD-E2E includes the
high-level routing information, ego states, and 360-degree camera views from 8
surrounding cameras. To evaluate the E2E driving performance on these long-tail
situations, we propose a novel open-loop evaluation metric: Rater Feedback
Score (RFS). Unlike conventional metrics that measure the distance between
predicted way points and the logs, RFS measures how closely the predicted
trajectory matches rater-annotated trajectory preference labels. We have
released rater preference labels for all WOD-E2E validation set segments, while
the held out test set labels have been used for the 2025 WOD-E2E Challenge.
Through our work, we aim to foster state of the art research into
generalizable, robust, and safe end-to-end autonomous driving agents capable of
handling complex real-world situations.

</details>


### [54] [Exploring Object-Aware Attention Guided Frame Association for RGB-D SLAM](https://arxiv.org/abs/2510.26131)
*Ali Caglayan,Nevrez Imamoglu,Oguzhan Guclu,Ali Osman Serhatoglu,Ahmet Burak Can,Ryosuke Nakamura*

Main category: cs.CV

> 本文提出了一种新的方法，通过将基于任务特定网络梯度的注意力信息集成到CNN特征表示中，以提高RGB-D室内SLAM任务的性能。

<details>
  <summary>Details</summary>

**Motivation:** 虽然可视化技术如类激活映射为理解CNN的推理过程提供了视觉上的洞察，但将基于梯度的注意力信息明确地集成到CNN表示中以提高语义对象理解的能力仍然有限。这种集成对于像同时定位与地图构建(SLAM)这样的视觉任务尤其有益。

**Method:** 本文提出了一种在RGB-D室内SLAM任务中利用任务特定网络注意力的方法。具体而言，本文将从网络梯度中提取的逐层注意力信息与CNN特征表示相结合，以提高帧关联性能。

**Result:** 实验结果表明，本文提出的方法在基准方法基础上提高了性能，特别是在大型环境中表现出色。

**Conclusion:** 这项工作展示了通过结合注意力机制和CNN特征来改善SLAM系统性能的有效性，为未来在大型环境中的视觉任务提供了新的解决方案。

**Abstract:** Attention models have recently emerged as a powerful approach, demonstrating
significant progress in various fields. Visualization techniques, such as class
activation mapping, provide visual insights into the reasoning of convolutional
neural networks (CNNs). Using network gradients, it is possible to identify
regions where the network pays attention during image recognition tasks.
Furthermore, these gradients can be combined with CNN features to localize more
generalizable, task-specific attentive (salient) regions within scenes.
However, explicit use of this gradient-based attention information integrated
directly into CNN representations for semantic object understanding remains
limited. Such integration is particularly beneficial for visual tasks like
simultaneous localization and mapping (SLAM), where CNN representations
enriched with spatially attentive object locations can enhance performance. In
this work, we propose utilizing task-specific network attention for RGB-D
indoor SLAM. Specifically, we integrate layer-wise attention information
derived from network gradients with CNN feature representations to improve
frame association performance. Experimental results indicate improved
performance compared to baseline methods, particularly for large environments.

</details>


### [55] [FullPart: Generating each 3D Part at Full Resolution](https://arxiv.org/abs/2510.26140)
*Lihe Ding,Shaocong Dong,Yaokun Li,Chenjian Gao,Xiao Chen,Rui Han,Yihao Kuang,Hong Zhang,Bo Huang,Zhanpeng Huang,Zibin Wang,Dan Xu,Tianfan Xue*

Main category: cs.CV

> 本文提出了FullPart框架，结合隐式和显式表示，解决了现有的3D部分生成器在几何细节和部分质量上的问题，通过引入中心点编码策略来保持全局一致性，并发布了PartVerse-XL数据集。

<details>
  <summary>Details</summary>

**Motivation:** 解决现有部分生成器在3D生成中几何细节不足的问题，以及在共享全局体素网格时，小部分因体素占用太少而导致质量下降的问题。

**Method:** FullPart提出了一种结合隐式和显式表示的新框架。它首先通过隐式框向量集扩散过程推导出边界框布局，这使得即使是小部分也能在完整分辨率的固定体素网格中生成，从而实现细节丰富的合成。此外，还引入了中心点编码策略以解决不同实际大小部分之间的信息交换对齐问题，以保持全局一致性。

**Result:** 实验结果表明，FullPart在3D部分生成中达到了最先进的结果，同时引入了PartVerse-XL，这是迄今为止最大的人类注释3D部分数据集，包含40K个对象和320K个部分。

**Conclusion:** FullPart框架有效解决了在3D部分生成中关于几何细节和部分质量的问题，其结果优于现有技术，并将发布所有代码、数据和模型以促进3D部分生成领域的未来研究。

**Abstract:** Part-based 3D generation holds great potential for various applications.
Previous part generators that represent parts using implicit vector-set tokens
often suffer from insufficient geometric details. Another line of work adopts
an explicit voxel representation but shares a global voxel grid among all
parts; this often causes small parts to occupy too few voxels, leading to
degraded quality. In this paper, we propose FullPart, a novel framework that
combines both implicit and explicit paradigms. It first derives the bounding
box layout through an implicit box vector-set diffusion process, a task that
implicit diffusion handles effectively since box tokens contain little
geometric detail. Then, it generates detailed parts, each within its own fixed
full-resolution voxel grid. Instead of sharing a global low-resolution space,
each part in our method - even small ones - is generated at full resolution,
enabling the synthesis of intricate details. We further introduce a
center-point encoding strategy to address the misalignment issue when
exchanging information between parts of different actual sizes, thereby
maintaining global coherence. Moreover, to tackle the scarcity of reliable part
data, we present PartVerse-XL, the largest human-annotated 3D part dataset to
date with 40K objects and 320K parts. Extensive experiments demonstrate that
FullPart achieves state-of-the-art results in 3D part generation. We will
release all code, data, and model to benefit future research in 3D part
generation.

</details>


### [56] [BasicAVSR: Arbitrary-Scale Video Super-Resolution via Image Priors and Enhanced Motion Compensation](https://arxiv.org/abs/2510.26149)
*Wei Shang,Wanying Zhang,Shuhang Gu,Pengfei Zhu,Qinghua Hu,Dongwei Ren*

Main category: cs.CV

> 本文介绍了一种新型的任意尺度视频超分辨率算法BasicAVSR，该算法在超分辨率质量、泛化能力和推理速度方面超越现有方法，并适用于不同场景。

<details>
  <summary>Details</summary>

**Motivation:** 任意尺度视频超分辨率技术以在不同比例因子下增强视频帧的分辨率为目标，这一过程面临着如何再现空间细节、维持时间一致性及处理计算复杂性等方面的挑战。本文旨在解决这些问题，提出一种有效的任意尺度视频超分辨率方法。

**Method:** 本文提出了一种新型的任意尺度视频超分辨率增强技术BasicAVSR，该技术结合了四个关键组件：1）来自图像拉普拉斯金字塔的自适应多尺度频率先验知识，2）流引导传播单元来聚合相邻帧的时空信息，3）第二阶运动补偿单元以更准确地对齐相邻帧，4）超上采样单元生成比例感知和内容无关的上采样核。此外，提供了三种传播变体，满足不同的应用场景需求，包括严格在线推理的单向RNN单元、带有限前瞻能力的单向RNN单元以及适用于计算资源较为充裕的离线任务的双向RNN单元。

**Result:** 实验结果表明，BasicAVSR在超分辨率质量、通用能力和推理速度方面都优于现有的方法。本文的研究不仅推动了任意尺度视频超分辨率领域的发展，也通过将核心技术扩展到多种框架中以适用于不同的场景。

**Conclusion:** 通过广泛的实验验证，BasicAVSR证明了其在任意尺度视频超分辨率技术上的优越性，不仅提升了超分辨率质量，也扩大了其在不同框架中的应用范围。

**Abstract:** Arbitrary-scale video super-resolution (AVSR) aims to enhance the resolution
of video frames, potentially at various scaling factors, which presents several
challenges regarding spatial detail reproduction, temporal consistency, and
computational complexity. In this paper, we propose a strong baseline BasicAVSR
for AVSR by integrating four key components: 1) adaptive multi-scale frequency
priors generated from image Laplacian pyramids, 2) a flow-guided propagation
unit to aggregate spatiotemporal information from adjacent frames, 3) a
second-order motion compensation unit for more accurate spatial alignment of
adjacent frames, and 4) a hyper-upsampling unit to generate scale-aware and
content-independent upsampling kernels. To meet diverse application demands, we
instantiate three propagation variants: (i) a unidirectional RNN unit for
strictly online inference, (ii) a unidirectional RNN unit empowered with a
limited lookahead that tolerates a small output delay, and (iii) a
bidirectional RNN unit designed for offline tasks where computational resources
are less constrained. Experimental results demonstrate the effectiveness and
adaptability of our model across these different scenarios. Through extensive
experiments, we show that BasicAVSR significantly outperforms existing methods
in terms of super-resolution quality, generalization ability, and inference
speed. Our work not only advances the state-of-the-art in AVSR but also extends
its core components to multiple frameworks for diverse scenarios. The code is
available at https://github.com/shangwei5/BasicAVSR.

</details>


### [57] [MV-MLM: Bridging Multi-View Mammography and Language for Breast Cancer Diagnosis and Risk Prediction](https://arxiv.org/abs/2510.26151)
*Shunjie-Fabian Zheng,Hyeonjun Lee,Thijs Kooi,Ali Diba*

Main category: cs.CV

> 本文介绍了一种基于多视角乳腺X光图像和合成放射学报告的视觉-语言模型（MV-MLM），用于乳腺癌分类和风险预测。通过跨模态自我监督学习和新颖的视觉-文本联合学习策略，在多个分类任务中展现了卓越的数据效率和性能。

<details>
  <summary>Details</summary>

**Motivation:** 解决获取带有详细标注的大规模医疗图像数据集的成本高昂和耗时的问题，并利用视觉-语言模型提高在医学成像任务中的稳健性和数据效率。

**Method:** 引入了多视角乳腺X光图像和合成放射学报告的数据集，提出了一种多视图视觉-语言模型（MV-MLM），通过跨模态自我监督学习和视觉-文本联合学习策略增强了模型的泛化能力和准确率。

**Result:** 评估了该方法在私人及公开数据集上的性能，实现了三种分类任务中的最先进性能：恶性肿瘤分类、亚型分类和基于图像的癌症风险预测。

**Conclusion:** 所提出的MV-MLM模型不仅展示了显著的数据效率，在利用合成文本报告而非实际放射学报告的情况下仍超越现有全监督或视觉-语言模型基线。

**Abstract:** Large annotated datasets are essential for training robust Computer-Aided
Diagnosis (CAD) models for breast cancer detection or risk prediction. However,
acquiring such datasets with fine-detailed annotation is both costly and
time-consuming. Vision-Language Models (VLMs), such as CLIP, which are
pre-trained on large image-text pairs, offer a promising solution by enhancing
robustness and data efficiency in medical imaging tasks. This paper introduces
a novel Multi-View Mammography and Language Model for breast cancer
classification and risk prediction, trained on a dataset of paired mammogram
images and synthetic radiology reports. Our MV-MLM leverages multi-view
supervision to learn rich representations from extensive radiology data by
employing cross-modal self-supervision across image-text pairs. This includes
multiple views and the corresponding pseudo-radiology reports. We propose a
novel joint visual-textual learning strategy to enhance generalization and
accuracy performance over different data types and tasks to distinguish breast
tissues or cancer characteristics(calcification, mass) and utilize these
patterns to understand mammography images and predict cancer risk. We evaluated
our method on both private and publicly available datasets, demonstrating that
the proposed model achieves state-of-the-art performance in three
classification tasks: (1) malignancy classification, (2) subtype
classification, and (3) image-based cancer risk prediction. Furthermore, the
model exhibits strong data efficiency, outperforming existing fully supervised
or VLM baselines while trained on synthetic text reports and without the need
for actual radiology reports.

</details>


### [58] [Detecting Unauthorized Vehicles using Deep Learning for Smart Cities: A Case Study on Bangladesh](https://arxiv.org/abs/2510.26154)
*Sudipto Das Sukanto,Diponker Roy,Fahim Shakil,Nirjhar Singha,Abdullah Asik,Aniket Joarder,Mridha Md Nafis Fuad,Muhammad Ibrahim*

Main category: cs.CV

> 研究提出了一个基于YOLOv8模型的实时机动三轮车自动检测方法，展示了在处理不同类型交通场景的有效性，并公布了数据集供进一步研究。

<details>
  <summary>Details</summary>

**Motivation:** 鉴于交通规则限制了机动三轮车的行驶路线，监测其运动非常必要。然而现有的监控系统由于机动三轮车与非机动三轮车等其他车辆的相似性，很难准确监测它们，人工视频分析又费时费力。因此需要一种自动化的解决方案。

**Method:** 本研究采用实时目标检测方法，使用YOLOv8模型自动检测交通图像中的机动三轮车。

**Result:** 实验结果表明，该模型在实时检测机动三轮车上表现良好，mAP50值达到83.447%，且二元准确率和召回率均超过78%，展示了其在处理密集和稀疏交通场景中的有效性。

**Conclusion:** 该研究提出的基于机器学习的解决方案能够有效自动检测照片中的机动三轮车，结果表明有很大潜力进一步应用和研究。

**Abstract:** Modes of transportation vary across countries depending on geographical
location and cultural context. In South Asian countries rickshaws are among the
most common means of local transport. Based on their mode of operation,
rickshaws in cities across Bangladesh can be broadly classified into non-auto
(pedal-powered) and auto-rickshaws (motorized). Monitoring the movement of
auto-rickshaws is necessary as traffic rules often restrict auto-rickshaws from
accessing certain routes. However, existing surveillance systems make it quite
difficult to monitor them due to their similarity to other vehicles, especially
non-auto rickshaws whereas manual video analysis is too time-consuming. This
paper presents a machine learning-based approach to automatically detect
auto-rickshaws in traffic images. In this system, we used real-time object
detection using the YOLOv8 model. For training purposes, we prepared a set of
1,730 annotated images that were captured under various traffic conditions. The
results show that our proposed model performs well in real-time auto-rickshaw
detection and offers an mAP50 of 83.447% and binary precision and recall values
above 78%, demonstrating its effectiveness in handling both dense and sparse
traffic scenarios. The dataset has been publicly released for further research.

</details>


### [59] [CRAG-MM: Multi-modal Multi-turn Comprehensive RAG Benchmark](https://arxiv.org/abs/2510.26160)
*Jiaqi Wang,Xiao Yang,Kai Sun,Parth Suresh,Sanat Sharma,Adam Czyzewski,Derek Andersen,Surya Appini,Arkav Banerjee,Sajal Choudhary,Shervin Ghasemlou,Ziqiang Guan,Akil Iyer,Haidar Khan,Lingkun Kong,Roy Luo,Tiffany Ma,Zhen Qiao,David Tran,Wenfang Xu,Skyler Yeatman,Chen Zhou,Gunveer Gujral,Yinglong Xia,Shane Moon,Nicolas Scheffer,Nirav Shah,Eun Chang,Yue Liu,Florian Metze,Tammy Stark,Zhaleh Feizollahi,Andrea Jessee,Mangesh Pujari,Ahmed Aly,Babak Damavandi,Rakesh Wanga,Anuj Kumar,Rohit Patel,Wen-tau Yih,Xin Luna Dong*

Main category: cs.CV

> 提出了CRAG-MM，一个多模态、多轮次对话的全面基准，旨在填补关于可穿戴设备场景下多模态检索增强生成（MM-RAG）研究的空白。

<details>
  <summary>Details</summary>

**Motivation:** 弥补现有研究中缺乏全面评估多模态多轮次对话尤其是穿戴设备场景下信息检索和生成任务的基准。

**Method:** 构建了一个含有6.5K(图像、问题、答案)三元组和2K视觉多轮次对话的基准，涵盖了包括来自可穿戴设备拍摄的6.2K第一人称图像等13个领域的数据集。设计了单/多数据源增强和多轮次对话三种任务。

**Result:** 评估显示，直接的RAG方法在单/多轮次问答上的真实度分别为32%和43%，而先进的行业解决方案效果类似（32%/45%），表明有大量改进空间；CRAG-MM在KDD Cup 2025比赛中引领了领域进步。

**Conclusion:** CRAG-MM提供了一个全面的基准，能够评估和推进MM-RAG技术在穿戴设备场景下的应用能力。

**Abstract:** Wearable devices such as smart glasses are transforming the way people
interact with their surroundings, enabling users to seek information regarding
entities in their view. Multi-Modal Retrieval-Augmented Generation (MM-RAG)
plays a key role in supporting such questions, yet there is still no
comprehensive benchmark for this task, especially regarding wearables
scenarios. To fill this gap, we present CRAG-MM -- a Comprehensive RAG
benchmark for Multi-modal Multi-turn conversations. CRAG-MM contains a diverse
set of 6.5K (image, question, answer) triplets and 2K visual-based multi-turn
conversations across 13 domains, including 6.2K egocentric images designed to
mimic captures from wearable devices. We carefully constructed the questions to
reflect real-world scenarios and challenges, including five types of
image-quality issues, six question types, varying entity popularity, differing
information dynamism, and different conversation turns. We design three tasks:
single-source augmentation, multi-source augmentation, and multi-turn
conversations -- each paired with an associated retrieval corpus and APIs for
both image-KG retrieval and webpage retrieval. Our evaluation shows that
straightforward RAG approaches achieve only 32% and 43% truthfulness on CRAG-MM
single- and multi-turn QA, respectively, whereas state-of-the-art industry
solutions have similar quality (32%/45%), underscoring ample room for
improvement. The benchmark has hosted KDD Cup 2025, attracting about 1K
participants and 5K submissions, with winning solutions improving baseline
performance by 28%, highlighting its early impact on advancing the field.

</details>


### [60] [MoTDiff: High-resolution Motion Trajectory estimation from a single blurred image using Diffusion models](https://arxiv.org/abs/2510.26173)
*Wontae Choi,Jaelin Lee,Hyung Sup Yun,Byeungwoo Jeon,Il Yong Chun*

Main category: cs.CV

> 该论文介绍了MoTDiff，一种新的高分辨率运动轨迹估计框架，通过扩散模型从单个运动模糊图像中估计高质量的运动轨迹，实验表明该方法优于现有技术。

<details>
  <summary>Details</summary>

**Motivation:** 目前的运动表示方法存在质量和精度问题，难以准确捕捉单个模糊图像中的详细运动信息。为此，研究人员提出了高质量的高分辨率（HR）运动轨迹估计框架MoTDiff，以解决现有表示方法的不足。

**Method:** MoTDiff 包括两个关键组成部分：1）一个使用从单个模糊图像提取的多尺度特征图作为条件的新条件扩散框架；2）一种能够促进细粒度运动轨迹精确识别、整体形状和位置一致估计以及沿着运动轨迹的像素连通性训练的新方法。

**Result:** 实验表明，所提出的MoTDiff在盲目图像去模糊和编码曝光摄影应用中能够超越现有的最先进技术。

**Conclusion:** 该研究成功实现了从单个模糊图像中估计高质量高分辨率运动轨迹的目标，为计算成像和计算机视觉领域的多种应用提供了更精确的运动信息。

**Abstract:** Accurate estimation of motion information is crucial in diverse computational
imaging and computer vision applications. Researchers have investigated various
methods to extract motion information from a single blurred image, including
blur kernels and optical flow. However, existing motion representations are
often of low quality, i.e., coarse-grained and inaccurate. In this paper, we
propose the first high-resolution (HR) Motion Trajectory estimation framework
using Diffusion models (MoTDiff). Different from existing motion
representations, we aim to estimate an HR motion trajectory with high-quality
from a single motion-blurred image. The proposed MoTDiff consists of two key
components: 1) a new conditional diffusion framework that uses multi-scale
feature maps extracted from a single blurred image as a condition, and 2) a new
training method that can promote precise identification of a fine-grained
motion trajectory, consistent estimation of overall shape and position of a
motion path, and pixel connectivity along a motion trajectory. Our experiments
demonstrate that the proposed MoTDiff can outperform state-of-the-art methods
in both blind image deblurring and coded exposure photography applications.

</details>


### [61] [ConceptScope: Characterizing Dataset Bias via Disentangled Visual Concepts](https://arxiv.org/abs/2510.26186)
*Jinho Choi,Hyesu Lim,Steffen Schneider,Jaegul Choo*

Main category: cs.CV

> ConceptScope是一个分析视觉数据集的可扩展自动框架，用于发现和量化人类可解释的视觉概念。它能够有效地识别数据集偏差并可视化语义有意义的图像区域。

<details>
  <summary>Details</summary>

**Motivation:** 系统地识别数据集偏差具有挑战性，特别是在缺乏详细的属性标注的情况下。ConceptScope旨在解决这一挑战，为分析视觉数据集提供工具。

**Method:** 使用稀疏自编码器训练视觉基础模型的表示，以发现和量化人类可解释的概念。ConceptScope将这些概念分类为目标、上下文和偏见类型，基于它们对类别标签的语义相关性和统计相关性，从而使数据集级别表征、偏见识别和通过基于概念的子组进行鲁棒性评估成为可能。

**Result:** 验证了ConceptScope能够捕捉到广泛视觉概念，并通过空间属性标注证明了所识别概念与其在图像中对应区域的语义意义高度一致。此外，它可靠地检测出了已知偏见和未标注偏见。

**Conclusion:** 通过利用视觉基础模型的表示和稀疏自编码器，ConceptScope提供了一种有效的工具，用于数据集审计和模型诊断，特别是在识别视觉偏见方面有着极高的实用价值。

**Abstract:** Dataset bias, where data points are skewed to certain concepts, is ubiquitous
in machine learning datasets. Yet, systematically identifying these biases is
challenging without costly, fine-grained attribute annotations. We present
ConceptScope, a scalable and automated framework for analyzing visual datasets
by discovering and quantifying human-interpretable concepts using Sparse
Autoencoders trained on representations from vision foundation models.
ConceptScope categorizes concepts into target, context, and bias types based on
their semantic relevance and statistical correlation to class labels, enabling
class-level dataset characterization, bias identification, and robustness
evaluation through concept-based subgrouping. We validate that ConceptScope
captures a wide range of visual concepts, including objects, textures,
backgrounds, facial attributes, emotions, and actions, through comparisons with
annotated datasets. Furthermore, we show that concept activations produce
spatial attributions that align with semantically meaningful image regions.
ConceptScope reliably detects known biases (e.g., background bias in
Waterbirds) and uncovers previously unannotated ones (e.g, co-occurring objects
in ImageNet), offering a practical tool for dataset auditing and model
diagnostics.

</details>


### [62] [Sketch2PoseNet: Efficient and Generalized Sketch to 3D Human Pose Prediction](https://arxiv.org/abs/2510.26196)
*Li Wang,Yiyu Zhuang,Yanwen Wang,Xun Cao,Chuan Guo,Xinxin Zuo,Hao Zhu*

Main category: cs.CV

> A novel 'learn from synthesis' approach is used to create a synthetic dataset and framework for accurately and efficiently estimating 3D human poses from sketch images, improving upon previous methods.

<details>
  <summary>Details</summary>

**Motivation:** To overcome the limitations of previous methods in sketch-to-pose estimation, such as time-consumption and limited generalizability, due to the lack of large-scale sketch-3D pose annotations.

**Method:** First, a diffusion model synthesizes sketch images from 2D poses projected from 3D human poses. Then, an end-to-end data-driven framework is introduced, combining 2D pose detectors, generative diffusion priors for feature extraction, and a feed-forward neural network for 2D pose estimation.

**Result:** Qualitative, quantitative, and subjective evaluations show that the model surpasses previous ones in both accuracy and speed for sketch-to-pose tasks.

**Conclusion:** The proposed method, which leverages synthetic datasets and advanced models, is more efficient and accurate in deriving 3D human poses from sketches.

**Abstract:** 3D human pose estimation from sketches has broad applications in computer
animation and film production. Unlike traditional human pose estimation, this
task presents unique challenges due to the abstract and disproportionate nature
of sketches. Previous sketch-to-pose methods, constrained by the lack of
large-scale sketch-3D pose annotations, primarily relied on optimization with
heuristic rules-an approach that is both time-consuming and limited in
generalizability. To address these challenges, we propose a novel approach
leveraging a "learn from synthesis" strategy. First, a diffusion model is
trained to synthesize sketch images from 2D poses projected from 3D human
poses, mimicking disproportionate human structures in sketches. This process
enables the creation of a synthetic dataset, SKEP-120K, consisting of 120k
accurate sketch-3D pose annotation pairs across various sketch styles. Building
on this synthetic dataset, we introduce an end-to-end data-driven framework for
estimating human poses and shapes from diverse sketch styles. Our framework
combines existing 2D pose detectors and generative diffusion priors for sketch
feature extraction with a feed-forward neural network for efficient 2D pose
estimation. Multiple heuristic loss functions are incorporated to guarantee
geometric coherence between the derived 3D poses and the detected 2D poses
while preserving accurate self-contacts. Qualitative, quantitative, and
subjective evaluations collectively show that our model substantially surpasses
previous ones in both estimation accuracy and speed for sketch-to-pose tasks.

</details>


### [63] [Developing a Multi-task Ensemble Geometric Deep Network for Supply Chain Sustainability and Risk Management](https://arxiv.org/abs/2510.26203)
*Mehdi Khaleghi,Nastaran Khaleghi,Sobhan Sheykhivand,Sebelan Danishvar*

Main category: cs.CV

> 该研究提出了一种新的几何深度网络——Chebyshev集成几何网络(Ch-EGN)，用于分析供应链数据集，提高供应链的可持续性和风险管理。通过两个不同数据库验证了该方法的有效性，结果表明该方法比最先进的方法更具优势。

<details>
  <summary>Details</summary>

**Motivation:** 供应链中风险管理和产品的正确分类对于提高供应链的可持续性和性能至关重要。现有方法在供应链风险管理中表现并不出色，因此需要更有效的方法来提高这一领域的性能。

**Method:** 研究提出了一种新的几何深度网络——Chebyshev集成几何网络(Ch-EGN)，集成卷积和几何深度学习技术，旨在利用供应链中的信息依赖性，以揭示数据库中的隐形状态。

**Result:** 研究通过SupplyGraph数据库和DataCo数据库验证了方法的有效性。基于DataCo数据库的供应链风险管理和基于SupplyGraph数据库的产品分类与边分类实验表明，平均准确率分别为98.95%、100%和98.07%，对于25家公司关系分类准确率为92.37%，显著优于现有方法。

**Conclusion:** 研究证明了所提出的方法在提高供应链的可持续性和风险管理方面的有效性和优越性，能够显著提升供应链的风险管理准确率和可持续性。

**Abstract:** The sustainability of supply chain plays a key role in achieving optimal
performance in controlling the supply chain. The management of risks that occur
in a supply chain is a fundamental problem for the purpose of developing the
sustainability of the network and elevating the performance efficiency of the
supply chain. The correct classification of products is another essential
element in a sustainable supply chain. Acknowledging recent breakthroughs in
the context of deep networks, several architectural options have been deployed
to analyze supply chain datasets. A novel geometric deep network is used to
propose an ensemble deep network. The proposed Chebyshev ensemble geometric
network (Ch-EGN) is a hybrid convolutional and geometric deep learning. This
network is proposed to leverage the information dependencies in supply chain to
derive invisible states of samples in the database. The functionality of the
proposed deep network is assessed on the two different databases. The
SupplyGraph Dataset and DataCo are considered in this research. The prediction
of delivery status of DataCo supply chain is done for risk administration. The
product classification and edge classification are performed using the
SupplyGraph database to enhance the sustainability of the supply network. An
average accuracy of 98.95% is obtained for the ensemble network for risk
management. The average accuracy of 100% and 98.07% are obtained for
sustainable supply chain in terms of 5 product group classification and 4
product relation classification, respectively. The average accuracy of 92.37%
is attained for 25 company relation classification. The results confirm an
average improvement and efficiency of the proposed method compared to the
state-of-the-art approaches.

</details>


### [64] [OmniLayout: Enabling Coarse-to-Fine Learning with LLMs for Universal Document Layout Generation](https://arxiv.org/abs/2510.26213)
*Hengrui Kang,Zhuangcheng Gu,Zhiyuan Zhao,Zichen Wen,Bin Wang,Weijia Li,Conghui He*

Main category: cs.CV

> 研究构建了第一个百万级别的多样化文档布局数据集OmniLayout-1M，并提出基于该数据集的两阶段学习范式的OmniLayout-LLM模型，显著提升了文档布局生成的效果，并将在多个领域中展现优越性能。

<details>
  <summary>Details</summary>

**Motivation:** 现有的文档布局生成研究主要集中在学术论文等具有Manhattan风格结构的文档上，而报纸和杂志等开放世界的文档类型代表性严重不足。为了解决这一问题，研究人员构建了包含六种常见文档类型的OmniLayout-1M数据集，旨在解决现有方法在复杂领域中表现不佳的问题。

**Method:** OmniLayout-LLM采用两阶段粗细学习范式：首先，通过OmniLayout-1M数据集中的粗略分类定义学习普遍的布局原则；其次，将知识转移到具有细粒度标注的具体领域中。

**Result:** 实验结果表明，该方法在M$^{6}$Doc数据集的多个领域中表现优异，显著优于现有的布局生成专家和一些最新的通用大模型。

**Conclusion:** 研究开发了OmniLayout-LLM模型，通过两阶段的学习方法实现了在多样文档类型中的强大的布局生成能力，该成果将在代码、模型和数据集上公开。

**Abstract:** Document AI has advanced rapidly and is attracting increasing attention. Yet,
while most efforts have focused on document layout analysis (DLA), its
generative counterpart, document layout generation, remains underexplored. A
major obstacle lies in the scarcity of diverse layouts: academic papers with
Manhattan-style structures dominate existing studies, while open-world genres
such as newspapers and magazines remain severely underrepresented. To address
this gap, we curate OmniLayout-1M, the first million-scale dataset of diverse
document layouts, covering six common document types and comprising
contemporary layouts collected from multiple sources. Moreover, since existing
methods struggle in complex domains and often fail to arrange long sequences
coherently, we introduce OmniLayout-LLM, a 0.5B model with designed two-stage
Coarse-to-Fine learning paradigm: 1) learning universal layout principles from
OmniLayout-1M with coarse category definitions, and 2) transferring the
knowledge to a specific domain with fine-grained annotations. Extensive
experiments demonstrate that our approach achieves strong performance on
multiple domains in M$^{6}$Doc dataset, substantially surpassing both existing
layout generation experts and several latest general-purpose LLMs. Our code,
models, and dataset will be publicly released.

</details>


### [65] [Which Way Does Time Flow? A Psychophysics-Grounded Evaluation for Vision-Language Models](https://arxiv.org/abs/2510.26241)
*Shiho Matta,Lis Kanashiro Pereira,Peitao Han,Fei Cheng,Shigeru Kitazawa*

Main category: cs.CV

> 研究通过判断视频片段播放方向的测试揭示了视觉语言模型在处理视频时间信息方面的不足。

<details>
  <summary>Details</summary>

**Motivation:** 当前的VLMs虽然在许多多模态任务上表现出色，但在处理视频中的时间信息方面仍然薄弱且评估不足。这项研究旨在探讨这一不足，并通过极简但具有启发性的挑战来揭示这种能力的缺失。

**Method:** 通过判断视频片段的时间箭头（AoT）——即判断片段是正向播放还是反向播放——来测试多模态视觉语言模型（VLMs）在视频中的时间信息处理能力。为此，引入了AoT-PsyPhyBENCH基准测试，该测试使用经过心理物理验证的刺激和人类的行为基线来评估VLMs能否在自然视频中推断时间方向。

**Result:** 对开源和专有、推理和非推理VLMs的全面评估显示，大多数模型的性能接近随机水平，且明显的滞后于人类对于物理不可逆过程及因果动作的识别准确性。

**Conclusion:** 大多数VLMs在AoT-PsyPhyBENCH基准测试上的表现接近随机水平，即使表现最好的模型在人类几乎可以瞬间识别的物理不可逆过程和因果手动动作（例如自由落体、扩散/爆炸、分割/加法）上也远远落后于人类准确性。结果强调了当前多模态系统在时间连续性和因果理解上缺乏所需的归纳偏见。

**Abstract:** Modern vision-language models (VLMs) excel at many multimodal tasks, yet
their grasp of temporal information in video remains weak and, crucially,
under-evaluated. We probe this gap with a deceptively simple but revealing
challenge: judging the arrow of time (AoT)-whether a short clip is played
forward or backward. We introduce AoT-PsyPhyBENCH, a psychophysically validated
benchmark that tests whether VLMs can infer temporal direction in natural
videos using the same stimuli and behavioral baselines established for humans.
Our comprehensive evaluation of open-weight and proprietary, reasoning and
non-reasoning VLMs reveals that most models perform near chance, and even the
best lag far behind human accuracy on physically irreversible processes (e.g.,
free fall, diffusion/explosion) and causal manual actions (division/addition)
that humans recognize almost instantly. These results highlight a fundamental
gap in current multimodal systems: while they capture rich visual-semantic
correlations, they lack the inductive biases required for temporal continuity
and causal understanding. We release the code and data for AoT-PsyPhyBENCH to
encourage further progress in the physical and temporal reasoning capabilities
of VLMs.

</details>


### [66] [Revisiting Generative Infrared and Visible Image Fusion Based on Human Cognitive Laws](https://arxiv.org/abs/2510.26268)
*Lin Guo,Xiaoqing Luo,Wei Xie,Zhancheng Zhang,Hui Li,Rui Wang,Zhenhua Feng,Xiaoning Song*

Main category: cs.CV

> HCLFuse 是一种受人类认知定律启发的红外和可见光图像融合方法，通过多尺度掩模调节变分瓶颈编码器和时间变化的物理引导机制，实现了高质量的数据结构感知和细节生成，提升了图像融合的表现。

<details>
  <summary>Details</summary>

**Motivation:** 现有方法在模态信息平衡、生成能力和选择解释性方面存在不足，影响了复杂场景下的可靠性和一致性。HCLFuse旨在改进这些问题，提供更加一致和清晰的图像结构细节。

**Method:** HCLFuse通过多尺度掩模调节变分瓶颈编码器提取精确的低层次模态信息并生成高质量的结构细节，结合扩散模型的生成能力和物理定律形成自适应调节机制。

**Result:** 实验结果表明HCLFuse在多数据集的定性和定量评估中表现出优秀融合性能，并显著提高了语义分割指标。

**Conclusion:** 这种方法展示了人类认知启发的生成图像融合方法在提高结构一致性和细节质量方面的优势。

**Abstract:** Existing infrared and visible image fusion methods often face the dilemma of
balancing modal information. Generative fusion methods reconstruct fused images
by learning from data distributions, but their generative capabilities remain
limited. Moreover, the lack of interpretability in modal information selection
further affects the reliability and consistency of fusion results in complex
scenarios. This manuscript revisits the essence of generative image fusion
under the inspiration of human cognitive laws and proposes a novel infrared and
visible image fusion method, termed HCLFuse. First, HCLFuse investigates the
quantification theory of information mapping in unsupervised fusion networks,
which leads to the design of a multi-scale mask-regulated variational
bottleneck encoder. This encoder applies posterior probability modeling and
information decomposition to extract accurate and concise low-level modal
information, thereby supporting the generation of high-fidelity structural
details. Furthermore, the probabilistic generative capability of the diffusion
model is integrated with physical laws, forming a time-varying physical
guidance mechanism that adaptively regulates the generation process at
different stages, thereby enhancing the ability of the model to perceive the
intrinsic structure of data and reducing dependence on data quality.
Experimental results show that the proposed method achieves state-of-the-art
fusion performance in qualitative and quantitative evaluations across multiple
datasets and significantly improves semantic segmentation metrics. This fully
demonstrates the advantages of this generative image fusion method, drawing
inspiration from human cognition, in enhancing structural consistency and
detail quality.

</details>


### [67] [Exploring Complementarity and Explainability in CNNs for Periocular Verification Across Acquisition Distances](https://arxiv.org/abs/2510.26282)
*Fernando Alonso-Fernandez,Kevin Hernandez Diaz,Jose M. Buades,Kiran Raja,Josef Bigun*

Main category: cs.CV

> Three CNN architectures trained on VGGFace2 eye crops are compared and fused for periocular verification on UBIPr, achieving state-of-the-art performance with significant gains especially when networks are fused.

<details>
  <summary>Details</summary>

**Motivation:** The goal is to study the complementarity of different CNNs for periocular verification at varying distances using the UBIPr database.

**Method:** We train three architectures of increasing complexity (SqueezeNet, MobileNetv2, and ResNet50) on a large set of eye crops from VGGFace2. We analyze performance with cosine and chi2 metrics, compare different network initializations, and apply score-level fusion via logistic regression. LIME heatmaps and Jensen-Shannon divergence are utilized to compare the attention patterns of the CNNs.

**Result:** ResNet50 performs the best individually, but the fusion of all three networks provides even greater performance gains. The networks tend to focus on different regions of the image, explaining their complementarity.

**Conclusion:** The proposed method outperforms previous works on the UBIPr database, achieving state-of-the-art results.

**Abstract:** We study the complementarity of different CNNs for periocular verification at
different distances on the UBIPr database. We train three architectures of
increasing complexity (SqueezeNet, MobileNetv2, and ResNet50) on a large set of
eye crops from VGGFace2. We analyse performance with cosine and chi2 metrics,
compare different network initialisations, and apply score-level fusion via
logistic regression. In addition, we use LIME heatmaps and Jensen-Shannon
divergence to compare attention patterns of the CNNs. While ResNet50
consistently performs best individually, the fusion provides substantial gains,
especially when combining all three networks. Heatmaps show that networks
usually focus on distinct regions of a given image, which explains their
complementarity. Our method significantly outperforms previous works on UBIPr,
achieving a new state-of-the-art.

</details>
