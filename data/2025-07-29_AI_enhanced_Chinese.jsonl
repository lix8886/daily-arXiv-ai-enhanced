{"id": "2507.19511", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.19511", "abs": "https://arxiv.org/abs/2507.19511", "authors": ["Khalid Hasan", "Jamil Saquer", "Mukulika Ghosh"], "title": "Advancing Mental Disorder Detection: A Comparative Evaluation of Transformer and LSTM Architectures on Social Media", "comment": "The 49th IEEE International Conference on Computers, Software, and\n  Applications (COMPSAC 2025) (camera-ready)", "summary": "The rising prevalence of mental health disorders necessitates the development\nof robust, automated tools for early detection and monitoring. Recent advances\nin Natural Language Processing (NLP), particularly transformer-based\narchitectures, have demonstrated significant potential in text analysis. This\nstudy provides a comprehensive evaluation of state-of-the-art transformer\nmodels (BERT, RoBERTa, DistilBERT, ALBERT, and ELECTRA) against Long Short-Term\nMemory (LSTM) based approaches using different text embedding techniques for\nmental health disorder classification on Reddit. We construct a large annotated\ndataset, validating its reliability through statistical judgmental analysis and\ntopic modeling. Experimental results demonstrate the superior performance of\ntransformer models over traditional deep-learning approaches. RoBERTa achieved\nthe highest classification performance, with a 99.54% F1 score on the hold-out\ntest set and a 96.05% F1 score on the external test set. Notably, LSTM models\naugmented with BERT embeddings proved highly competitive, achieving F1 scores\nexceeding 94% on the external dataset while requiring significantly fewer\ncomputational resources. These findings highlight the effectiveness of\ntransformer-based models for real-time, scalable mental health monitoring. We\ndiscuss the implications for clinical applications and digital mental health\ninterventions, offering insights into the capabilities and limitations of\nstate-of-the-art NLP methodologies in mental disorder detection.", "AI": {"tldr": "研究比较了变压器模型和LSTM模型在Reddit心理健康障碍文本分类上的表现，数据显示变压器模型更为出色，特别是RoBERTa模型。LSTM模型搭配BERT嵌入也表现良好但计算需求更低，这对临床应用和数字心理健康干预有重要影响。", "motivation": "鉴于心理健康障碍的普遍性和自动工具在早期检测和监控中的必要性，这项研究利用自然语言处理的最新进展来评估不同模型在心理健康障碍分类上的表现。", "method": "该研究对比了基于变压器架构的先进模型（如BERT、RoBERTa、DistilBERT、ALBERT、ELECTRA）和基于LSTM的方法，并使用了不同的文本嵌入技术来分类Reddit上的心理健康障碍。研究还构建了一个大型标注数据集，并通过统计判决分析和主题建模验证了数据集的可靠性。", "result": "实验结果表明，变压器模型的表现优于传统的深度学习方法。RoBERTa达到了最高的分类表现，其在保留测试集上F1分数为99.54%，在外部测试集上F1分数为96.05%。增强BERT嵌入的LSTM模型同样表现优秀，虽然计算资源需求较低，但在外部数据集上的F1分数超过94%。", "conclusion": "研究结果指出了基于变压器的模型在实时、可扩展的心理健康监控中的有效性。讨论了这些发现对临床应用及数字化心理健康干预的意义，提供对于先进自然语言处理方法在心理障碍检测中的能力与局限性的见解。"}}
{"id": "2507.19521", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.19521", "abs": "https://arxiv.org/abs/2507.19521", "authors": ["Vishakh Padmakumar", "Joseph Chee Chang", "Kyle Lo", "Doug Downey", "Aakanksha Naik"], "title": "Setting The Table with Intent: Intent-aware Schema Generation and Editing for Literature Review Tables", "comment": null, "summary": "The increasing volume of academic literature makes it essential for\nresearchers to organize, compare, and contrast collections of documents. Large\nlanguage models (LLMs) can support this process by generating schemas defining\nshared aspects along which to compare papers. However, progress on schema\ngeneration has been slow due to: (i) ambiguity in reference-based evaluations,\nand (ii) lack of editing/refinement methods. Our work is the first to address\nboth issues. First, we present an approach for augmenting unannotated table\ncorpora with synthesized intents and apply it to create a dataset for studying\nschema generation conditioned on a given information need, thus reducing\nambiguity. With this dataset, we show how incorporating table intents\nsignificantly improves baseline performance in reconstructing reference\nschemas. Next, we propose several LLM-based schema editing techniques. We start\nby comprehensively benchmarking several single-shot schema generation methods,\nincluding prompted LLM workflows and fine-tuned models, showing that smaller,\nopen-weight models can be fine-tuned to be competitive with state-of-the-art\nprompted LLMs. Then we demonstrate that our editing techniques can further\nimprove schemas generated by these methods.", "AI": {"tldr": "本文解决了模式生成中的两个主要问题:参考评估的模糊性和缺乏编辑方法,通过合成意图增强表格语料库和提出基于大语言模型的编辑技术,提高模式生成的质量。", "motivation": "由于学术文献数量的增加,研究人员需要组织、比较和对比文献集。虽然大型语言模型可以帮助生成比较论文的共同方面的模式,模式生成的进步缓慢,主要由于参考评估的模糊性和缺乏编辑/细化方法。", "method": "提出了一种方法来增强未标注的表格语料库与合成意图,并使用该数据集展示了如何通过结合表格意图显著提高基线性能,重新构造参考模式。此外,还提出了几种基于大语言模型的模式编辑技术,并展示了通过编辑技术可以进一步改善这些方法生成的模式。", "result": "展示了结合表格意图显著提高了基线性能,重新构造参考模式。此外,展示了基于大语言模型的编辑技术可以进一步改善生成的模式。", "conclusion": "本文首次解决了模式生成中的两个关键问题:参考评估的模糊性和缺乏编辑方法。通过提出新的数据集和编辑技术,证明了可以提高模式生成的质量。"}}
{"id": "2507.19537", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.19537", "abs": "https://arxiv.org/abs/2507.19537", "authors": ["Felix Kraus", "Nicolas Blumenröhr", "Danah Tonne", "Achim Streit"], "title": "Mind the Language Gap in Digital Humanities: LLM-Aided Translation of SKOS Thesauri", "comment": null, "summary": "We introduce WOKIE, an open-source, modular, and ready-to-use pipeline for\nthe automated translation of SKOS thesauri. This work addresses a critical need\nin the Digital Humanities (DH), where language diversity can limit access,\nreuse, and semantic interoperability of knowledge resources. WOKIE combines\nexternal translation services with targeted refinement using Large Language\nModels (LLMs), balancing translation quality, scalability, and cost. Designed\nto run on everyday hardware and be easily extended, the application requires no\nprior expertise in machine translation or LLMs. We evaluate WOKIE across\nseveral DH thesauri in 15 languages with different parameters, translation\nservices and LLMs, systematically analysing translation quality, performance,\nand ontology matching improvements. Our results show that WOKIE is suitable to\nenhance the accessibility, reuse, and cross-lingual interoperability of\nthesauri by hurdle-free automated translation and improved ontology matching\nperformance, supporting more inclusive and multilingual research\ninfrastructures.", "AI": {"tldr": "介绍了WOKIE，该工具是一个开源、模块化、可直接使用的管线，用于SKOS词典的自动化翻译，解决了数字人文中语言多样性的问题，促进了知识资源的访问和互操作性。", "motivation": "解决数字人文领域的语言多样性问题，该问题限制了知识资源的访问、重用及语义互操作性。", "method": "采用了一种结合外部翻译服务与大型语言模型（LLMs）进行针对性优化的方法，以平衡翻译的质量、可扩展性与成本。WOKIE设计用于日常硬件上运行且易于扩展，无需专门的机器翻译或LLMs知识。", "result": "通过多个数字人文词典的实验，在15种不同语言中，使用不同参数、翻译服务及大型语言模型进行系统分析，结果表明WOKIE适合进行无障碍自动化翻译，并提升了本体匹配的性能。", "conclusion": "WOKIE能够增强词典的可访问性、重用率及跨语言互操作性，从而支持更加包容的多语言研究基础设施。"}}
{"id": "2507.19586", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.19586", "abs": "https://arxiv.org/abs/2507.19586", "authors": ["Shengyuan Wang", "Jie Feng", "Tianhui Liu", "Dan Pei", "Yong Li"], "title": "Mitigating Geospatial Knowledge Hallucination in Large Language Models: Benchmarking and Dynamic Factuality Aligning", "comment": "19 pages, 9 figures", "summary": "Large language models (LLMs) possess extensive world knowledge, including\ngeospatial knowledge, which has been successfully applied to various geospatial\ntasks such as mobility prediction and social indicator prediction. However,\nLLMs often generate inaccurate geospatial knowledge, leading to geospatial\nhallucinations (incorrect or inconsistent representations of geospatial\ninformation) that compromise their reliability. While the phenomenon of general\nknowledge hallucination in LLMs has been widely studied, the systematic\nevaluation and mitigation of geospatial hallucinations remain largely\nunexplored. To address this gap, we propose a comprehensive evaluation\nframework for geospatial hallucinations, leveraging structured geospatial\nknowledge graphs for controlled assessment. Through extensive evaluation across\n20 advanced LLMs, we uncover the hallucinations in their geospatial knowledge.\nBuilding on these insights, we introduce a dynamic factuality aligning method\nbased on Kahneman-Tversky Optimization (KTO) to mitigate geospatial\nhallucinations in LLMs, leading to a performance improvement of over 29.6% on\nthe proposed benchmark. Extensive experimental results demonstrate the\neffectiveness of our benchmark and learning algorithm in enhancing the\ntrustworthiness of LLMs in geospatial knowledge and reasoning tasks.", "AI": {"tldr": "The study presents an evaluation framework and mitigation strategy for geospatial hallucinations in LLMs, significantly enhancing their performance in geospatial knowledge tasks.", "motivation": "The aim is to address the issue of geospatial hallucinations (inaccuracies in geospatial information) in large language models, as the phenomenon has been less explored compared to general knowledge hallucination.", "method": "The paper proposes a comprehensive evaluation framework for geospatial hallucinations in LLMs using structured geospatial knowledge graphs and introduces a dynamic factuality aligning method based on Kahneman-Tversky Optimization (KTO) to mitigate geospatial hallucinations.", "result": "The proposed method demonstrates a performance improvement of over 29.6% on the benchmark, enhancing the trustworthiness of LLMs in geospatial knowledge and reasoning tasks.", "conclusion": "The research shows the effectiveness of the evaluation framework and the factuality aligning method in mitigating geospatial hallucinations, thereby improving the reliability of LLMs for geospatial tasks."}}
{"id": "2507.19574", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.19574", "abs": "https://arxiv.org/abs/2507.19574", "authors": ["Ghufran Abualhail Alhamzawi", "Ali Saeed Alfoudi", "Ali Hakem Alsaeedi", "Suha Mohammed Hadi", "Amjed Abbas Ahmed", "Md. Riad Hassan", "Nurhizam Safie Mohd Satar", "Waeel Yahya Yasseen"], "title": "Tuning adaptive gamma correction (TAGC) for enhancing images in low ligh", "comment": null, "summary": "Enhancing images in low-light conditions is an important challenge in\ncomputer vision. Insufficient illumination negatively affects the quality of\nimages, resulting in low contrast, intensive noise, and blurred details. This\npaper presents a model for enhancing low-light images called tuning adaptive\ngamma correction (TAGC). The model is based on analyzing the color luminance of\nthe low-light image and calculating the average color to determine the adaptive\ngamma coefficient. The gamma value is calculated automatically and adaptively\nat different illumination levels suitable for the image without human\nintervention or manual adjustment. Based on qualitative and quantitative\nevaluation, tuning adaptive gamma correction model has effectively improved\nlow-light images while maintaining details, natural contrast, and correct color\ndistribution. It also provides natural visual quality. It can be considered a\nmore efficient solution for processing low-light images in multiple\napplications such as night surveillance, improving the quality of medical\nimages, and photography in low-light environments.", "AI": {"tldr": "The paper presents a model named TAGC for enhancing images in low-light conditions, which effectively improves low-light images while keeping details, contrast, and color natural.", "motivation": "The motivation arises from addressing the challenges of low-light images such as low contrast, noise, and blurred details which are caused by insufficient illumination. The need for an effective method to enhance the quality of these images is highlighted.", "method": "The method involves analyzing the color luminance of the low-light image and calculating the average color to determine the adaptive gamma coefficient. The gamma value is automatically and adaptively calculated according to different levels of illumination suitable for the image, requiring no human intervention or manual adjustment.", "result": "The model has been found to be effective in enhancing low-light images, maintaining natural visual quality, and is suitable for various applications.", "conclusion": "The tuning adaptive gamma correction (TAGC) model is a more efficient solution for processing low-light images in applications such as night surveillance, improving medical images, and photography in low-light settings."}}
{"id": "2507.19595", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.19595", "abs": "https://arxiv.org/abs/2507.19595", "authors": ["Yutao Sun", "Zhenyu Li", "Yike Zhang", "Tengyu Pan", "Bowen Dong", "Yuyi Guo", "Jianyong Wang"], "title": "Efficient Attention Mechanisms for Large Language Models: A Survey", "comment": "work in progress", "summary": "Transformer-based architectures have become the prevailing backbone of large\nlanguage models. However, the quadratic time and memory complexity of\nself-attention remains a fundamental obstacle to efficient long-context\nmodeling. To address this limitation, recent research has introduced two\nprincipal categories of efficient attention mechanisms. Linear attention\nmethods achieve linear complexity through kernel approximations, recurrent\nformulations, or fastweight dynamics, thereby enabling scalable inference with\nreduced computational overhead. Sparse attention techniques, in contrast, limit\nattention computation to selected subsets of tokens based on fixed patterns,\nblock-wise routing, or clustering strategies, enhancing efficiency while\npreserving contextual coverage. This survey provides a systematic and\ncomprehensive overview of these developments, integrating both algorithmic\ninnovations and hardware-level considerations. In addition, we analyze the\nincorporation of efficient attention into largescale pre-trained language\nmodels, including both architectures built entirely on efficient attention and\nhybrid designs that combine local and global components. By aligning\ntheoretical foundations with practical deployment strategies, this work aims to\nserve as a foundational reference for advancing the design of scalable and\nefficient language models.", "AI": {"tldr": "本文综述了用于解决Transformer架构中自注意力机制效率问题的线性注意力和稀疏注意力机制，并讨论了它们在大规模预训练语言模型中的应用。", "motivation": "解决Transformer中自注意力机制的二次复杂度问题，使其适用于长上下文的高效建模。", "method": "Transformer架构以其强大的语言建模能力而占据了主导地位，但自注意力机制的时间和内存复杂度为二次方，成为高效长上下文建模的主要障碍。本文综述了两种主要的高效注意力机制类别：线性注意力机制和稀疏注意力机制，并探讨了它们在大规模预训练语言模型中的应用。", "result": "通过系统全面地综述线性注意力和稀疏注意力机制的开发，本文为设计高效可扩展的语言模型提供了理论和实践上的指导。", "conclusion": "综述表明，通过采用线性注意力和稀疏注意力机制，可以在保持上下文覆盖率的同时提高Transformer模型的效率，为设计高效的长上下文语言模型指明了方向。"}}
{"id": "2507.19575", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.19575", "abs": "https://arxiv.org/abs/2507.19575", "authors": ["Ayush Roy", "Samin Enam", "Jun Xia", "Vishnu Suresh Lokhande", "Won Hwa Kim"], "title": "Is Exchangeability better than I.I.D to handle Data Distribution Shifts while Pooling Data for Data-scarce Medical image segmentation?", "comment": null, "summary": "Data scarcity is a major challenge in medical imaging, particularly for deep\nlearning models. While data pooling (combining datasets from multiple sources)\nand data addition (adding more data from a new dataset) have been shown to\nenhance model performance, they are not without complications. Specifically,\nincreasing the size of the training dataset through pooling or addition can\ninduce distributional shifts, negatively affecting downstream model\nperformance, a phenomenon known as the \"Data Addition Dilemma\". While the\ntraditional i.i.d. assumption may not hold in multi-source contexts, assuming\nexchangeability across datasets provides a more practical framework for data\npooling. In this work, we investigate medical image segmentation under these\nconditions, drawing insights from causal frameworks to propose a method for\ncontrolling foreground-background feature discrepancies across all layers of\ndeep networks. This approach improves feature representations, which are\ncrucial in data-addition scenarios. Our method achieves state-of-the-art\nsegmentation performance on histopathology and ultrasound images across five\ndatasets, including a novel ultrasound dataset that we have curated and\ncontributed. Qualitative results demonstrate more refined and accurate\nsegmentation maps compared to prominent baselines across three model\narchitectures. The code will be available on Github.", "AI": {"tldr": "This paper introduces a method to improve feature representations in deep networks for medical image segmentation, achieving state-of-the-art performance and overcoming distributional shifts associated with data pooling or addition.", "motivation": "The motivation is to address the 'Data Addition Dilemma' in medical imaging, where pooling or adding more data can lead to distributional shifts that decrease model performance. This is particularly relevant as data scarcity poses a significant challenge for deep learning models in this domain.", "method": "The paper proposes a method inspired by causal frameworks to control foreground-background feature discrepancies across all layers of deep networks, which improves feature representation crucial for data addition scenarios.", "result": "The method achieves state-of-the-art performance in medical image segmentation tasks on histopathology and ultrasound images across five datasets, showcasing more refined and accurate segmentation maps than prominent baseline models.", "conclusion": "The study concludes that their proposed method effectively mitigates the negative impact of distribution shifts caused by data pooling or addition by improving the foreground-background feature representations in deep networks, thereby enhancing segmentation performance in medical imaging tasks."}}
{"id": "2507.19598", "categories": ["cs.CL", "cs.AI", "cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.19598", "abs": "https://arxiv.org/abs/2507.19598", "authors": ["Muntasir Wahed", "Xiaona Zhou", "Kiet A. Nguyen", "Tianjiao Yu", "Nirav Diwan", "Gang Wang", "Dilek Hakkani-Tür", "Ismini Lourentzou"], "title": "MOCHA: Are Code Language Models Robust Against Multi-Turn Malicious Coding Prompts?", "comment": "Winner Defender Team at Amazon Nova AI Challenge 2025", "summary": "Recent advancements in Large Language Models (LLMs) have significantly\nenhanced their code generation capabilities. However, their robustness against\nadversarial misuse, particularly through multi-turn malicious coding prompts,\nremains underexplored. In this work, we introduce code decomposition attacks,\nwhere a malicious coding task is broken down into a series of seemingly benign\nsubtasks across multiple conversational turns to evade safety filters. To\nfacilitate systematic evaluation, we introduce \\benchmarkname{}, a large-scale\nbenchmark designed to evaluate the robustness of code LLMs against both\nsingle-turn and multi-turn malicious prompts. Empirical results across open-\nand closed-source models reveal persistent vulnerabilities, especially under\nmulti-turn scenarios. Fine-tuning on MOCHA improves rejection rates while\npreserving coding ability, and importantly, enhances robustness on external\nadversarial datasets with up to 32.4% increase in rejection rates without any\nadditional supervision.", "AI": {"tldr": "本文介绍了code decomposition attacks，这是一种通过将恶意任务分解为多个看似无害的子任务来绕过安全检查的方法，并提出了一个大规模基准测试\benchmarkname{}，用于评估代码生成LLMs的稳健性。实验结果表明了这些模型在多轮次恶意提示下的薄弱点。", "motivation": "尽管大型语言模型在代码生成能力上取得了显著进展，但它们对于多轮恶意编程提示的稳健性问题仍然尚未充分探讨，这就是本文的研究动机。", "method": "本文提出了code decomposition attacks的概念，即将恶意编程任务分解为一系列看似无害的子任务，以逃避安全过滤器的检测。为系统性评估大模型的鲁棒性，作者还提出了一个大规模基准测试\benchmarkname{}，用于评估代码LLMs对单轮和多轮恶意提示的鲁棒性。", "result": "实验结果表明，开放和封闭源代码模型在多轮攻击场景中存在持续的脆弱性。针对MOCHA进行微调可以提高拒绝率，同时保持代码生成能力，并在没有额外监督的情况下提高对外部对抗性数据集的鲁棒性，拒绝率增加高达32.4%。", "conclusion": "研究表明，提出的方法能够有效地检测和缓解code decomposition attacks，并通过微调增强了模型的对外敌对攻击的鲁棒性。"}}
{"id": "2507.19590", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.19590", "abs": "https://arxiv.org/abs/2507.19590", "authors": ["Chandravardhan Singh Raghaw", "Jasmer Singh Sanjotra", "Mohammad Zia Ur Rehman", "Shubhi Bansal", "Shahid Shafi Dar", "Nagendra Kumar"], "title": "T-MPEDNet: Unveiling the Synergy of Transformer-aware Multiscale Progressive Encoder-Decoder Network with Feature Recalibration for Tumor and Liver Segmentation", "comment": null, "summary": "Precise and automated segmentation of the liver and its tumor within CT scans\nplays a pivotal role in swift diagnosis and the development of optimal\ntreatment plans for individuals with liver diseases and malignancies. However,\nautomated liver and tumor segmentation faces significant hurdles arising from\nthe inherent heterogeneity of tumors and the diverse visual characteristics of\nlivers across a broad spectrum of patients. Aiming to address these challenges,\nwe present a novel Transformer-aware Multiscale Progressive Encoder-Decoder\nNetwork (T-MPEDNet) for automated segmentation of tumor and liver. T-MPEDNet\nleverages a deep adaptive features backbone through a progressive\nencoder-decoder structure, enhanced by skip connections for recalibrating\nchannel-wise features while preserving spatial integrity. A\nTransformer-inspired dynamic attention mechanism captures long-range contextual\nrelationships within the spatial domain, further enhanced by multi-scale\nfeature utilization for refined local details, leading to accurate prediction.\nMorphological boundary refinement is then employed to address indistinct\nboundaries with neighboring organs, capturing finer details and yielding\nprecise boundary labels. The efficacy of T-MPEDNet is comprehensively assessed\non two widely utilized public benchmark datasets, LiTS and 3DIRCADb. Extensive\nquantitative and qualitative analyses demonstrate the superiority of T-MPEDNet\ncompared to twelve state-of-the-art methods. On LiTS, T-MPEDNet achieves\noutstanding Dice Similarity Coefficients (DSC) of 97.6% and 89.1% for liver and\ntumor segmentation, respectively. Similar performance is observed on 3DIRCADb,\nwith DSCs of 98.3% and 83.3% for liver and tumor segmentation, respectively.\nOur findings prove that T-MPEDNet is an efficacious and reliable framework for\nautomated segmentation of the liver and its tumor in CT scans.", "AI": {"tldr": "研究提出了一种新的网络T-MPEDNet，用于提高CT扫描中肝和肿瘤的自动分割精度，并在两个基准数据集上验证了其有效性。", "motivation": "肝和肿瘤的自动分割在CT扫描中面临巨大挑战，如肿瘤的异质性和肝脏的视觉特征多样性。此研究旨在解决这些挑战，提供高效的解决方案。", "method": "提出了一种名为T-MPEDNet的新网络，用于CT扫描中肝和肿瘤的自动分割。该网络采用了具有跳过连接的渐进式编解码器结构，融合了Transformer启发的动态注意机制和多尺度特征利用。此外，还应用了形态边界细化以提高精确度。", "result": "实验结果表明，T-MPEDNet在LiTS和3DIRCADb两个公共基准数据集上，分别达到了97.6%和89.1%（肝和肿瘤分割）的Dice相似系数，在3DIRCADb数据集上分别达到了98.3%和83.3%。", "conclusion": "T-MPEDNet在CT扫描中对于肝和肿瘤的自动分割展示出了优越性和可靠性。"}}
{"id": "2507.19616", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.19616", "abs": "https://arxiv.org/abs/2507.19616", "authors": ["Xuchen Wei", "Yangxin Wu", "Yaoyin Zhang", "Henglyu Liu", "Kehai Chen", "Xuefeng Bai", "Min Zhang"], "title": "HITSZ's End-To-End Speech Translation Systems Combining Sequence-to-Sequence Auto Speech Recognition Model and Indic Large Language Model for IWSLT 2025 in Indic Track", "comment": "7 pages, 1 figure, submitted to IWSLT 2025", "summary": "This paper presents HITSZ's submission for the IWSLT 2025 Indic track,\nfocusing on speech-to-text translation (ST) for English-to-Indic and\nIndic-to-English language pairs. To enhance translation quality in this\nlow-resource scenario, we propose an end-to-end system integrating the\npre-trained Whisper automated speech recognition (ASR) model with Krutrim, an\nIndic-specialized large language model (LLM). Experimental results demonstrate\nthat our end-to-end system achieved average BLEU scores of $28.88$ for\nEnglish-to-Indic directions and $27.86$ for Indic-to-English directions.\nFurthermore, we investigated the Chain-of-Thought (CoT) method. While this\nmethod showed potential for significant translation quality improvements on\nsuccessfully parsed outputs (e.g. a $13.84$ BLEU increase for\nTamil-to-English), we observed challenges in ensuring the model consistently\nadheres to the required CoT output format.", "AI": {"tldr": "HITSZ 提交了 IWSLT 2025 Indic 轨道的成果，该成果应对英语和印度语之间的语音转文字翻译。通过结合预训练的 Whisper ASR 和 Indic 专用Krutm LLM，平均 BLEU 分数分别达到 28.88（英语转印度语）和 27.86（印度语转英语）。同时，Chain-of-Thought 方法显示出潜力但存在输出格式一致性的挑战。", "motivation": "为了改善低资源情况下的翻译质量，HITSZ 提出了一个将预训练的 ASR 模型与 Indic 专用大语言模型结合的端到端系统。", "method": "端到端的系统将预训练的 Whisper ASR 模型与 Krutrim 语言模型相结合，应用于英语和印度语的语音转文字翻译。同时，还研究了 Chain-of-Thought 方法的可能性。", "result": "实验结果显示，此系统的平均 BLEU 分数分别为：从英语到印度语为 28.88，从印度语到英语为 27.86。Chain-of-Thought 方法显示出了通过成功解析的输出增加翻译质量的潜力，如在泰米尔语到英语翻译中，有 13.84 的 BLEU 提升。然而，也发现难以确保模型始终遵守所需的 Chain-of-Thought 输出格式。", "conclusion": "虽然 Chain-of-Thought 方法证明了提升翻译质量的可能，但在格式一致性上面临挑战。结合集成的预训练 Whispher ASR 和 Krutrim，模型在低资源环境中的翻译效果有了显著成效。同时，研究了进一步提高翻译质量的可能性。"}}
{"id": "2507.19592", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.19592", "abs": "https://arxiv.org/abs/2507.19592", "authors": ["Meng Wei", "Charlie Budd", "Oluwatosin Alabi", "Miaojing Shi", "Tom Vercauteren"], "title": "SurgPIS: Surgical-instrument-level Instances and Part-level Semantics for Weakly-supervised Part-aware Instance Segmentation", "comment": null, "summary": "Consistent surgical instrument segmentation is critical for automation in\nrobot-assisted surgery. Yet, existing methods only treat instrument-level\ninstance segmentation (IIS) or part-level semantic segmentation (PSS)\nseparately, without interaction between these tasks. In this work, we formulate\na surgical tool segmentation as a unified part-aware instance segmentation\n(PIS) problem and introduce SurgPIS, the first PIS model for surgical\ninstruments. Our method adopts a transformer-based mask classification approach\nand introduces part-specific queries derived from instrument-level object\nqueries, explicitly linking parts to their parent instrument instances. In\norder to address the lack of large-scale datasets with both instance- and\npart-level labels, we propose a weakly-supervised learning strategy for SurgPIS\nto learn from disjoint datasets labelled for either IIS or PSS purposes. During\ntraining, we aggregate our PIS predictions into IIS or PSS masks, thereby\nallowing us to compute a loss against partially labelled datasets. A\nstudent-teacher approach is developed to maintain prediction consistency for\nmissing PIS information in the partially labelled data, e.g., parts of the IIS\nlabelled data. Extensive experiments across multiple datasets validate the\neffectiveness of SurgPIS, achieving state-of-the-art performance in PIS as well\nas IIS, PSS, and instrument-level semantic segmentation.", "AI": {"tldr": "本文提出了一种新的手术工具分割方法SurgPIS，它通过将任务视为统一的部件感知实例分割（PIS）问题，并采用弱监督学习来解决大型数据集缺乏的问题。实验结果显示，SurgPIS在PIS、IIS、PSS及仪器级语义分割方面均实现了最先进的性能。", "motivation": "目前的方法仅处理仪器级实例分割（IIS）或部件级语义分割（PSS）任务，但缺乏两者之间的互动。本研究旨在通过将手术工具分割作为一个统一的部件感知实例分割（PIS）问题，解决这一不足。", "method": "我们的方法采用基于变压器的掩码分类方法，并引入从仪器级对象查询派生的部分特定查询，明确地将零件与其父级仪器实例链接起来。为了应对缺乏同时具有实例级和零件级标签的大规模数据集的问题，我们提出了一种弱监督学习策略，使SurgPIS能够从分别标记为IIS或PSS目的的数据集中学习。在训练过程中，我们将PIS预测组合成IIS或PSS掩码，允许我们使用部分标记的数据集计算损失。我们开发了一种学生-教师策略来保持在部分标记数据中丢失PIS信息（如IIS标记数据的部分）的预测一致性。", "result": "实验结果显示，SurgPIS在PIS、IIS、PSS及仪器级语义分割方面实现了最先进的性能。", "conclusion": "实验表明，所提出的方法能够在多种数据集上有效工作，展示出在PIS（以及IIS、PSS和仪器级语义分割）方面实现了最先进性能的SurgPIS的有效性。"}}
{"id": "2507.19634", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.SD"], "pdf": "https://arxiv.org/pdf/2507.19634", "abs": "https://arxiv.org/abs/2507.19634", "authors": ["Sara Papi", "Maike Züfle", "Marco Gaido", "Beatrice Savoldi", "Danni Liu", "Ioannis Douros", "Luisa Bentivogli", "Jan Niehues"], "title": "MCIF: Multimodal Crosslingual Instruction-Following Benchmark from Scientific Talks", "comment": "Work in progress", "summary": "Recent advances in large language models have catalyzed the development of\nmultimodal LLMs (MLLMs) that integrate text, speech, and vision within unified\nframeworks. As MLLMs evolve from narrow, monolingual, task-specific systems to\ngeneral-purpose instruction-following models, a key frontier lies in evaluating\ntheir multilingual and multimodal capabilities over both long and short\ncontexts. However, existing benchmarks fall short in evaluating these\ndimensions jointly: they are often limited to English, mostly focus on one\nsingle modality at a time, rely on short-form contexts, or lack human\nannotations -- hindering comprehensive assessment of model performance across\nlanguages, modalities, and task complexity. To address these gaps, we introduce\nMCIF (Multimodal Crosslingual Instruction Following), the first multilingual\nhuman-annotated benchmark based on scientific talks that is designed to\nevaluate instruction-following in crosslingual, multimodal settings over both\nshort- and long-form inputs. MCIF spans three core modalities -- speech,\nvision, and text -- and four diverse languages (English, German, Italian, and\nChinese), enabling a comprehensive evaluation of MLLMs' abilities to interpret\ninstructions across languages and combine them with multimodal contextual\ninformation. MCIF is released under a CC-BY 4.0 license to encourage open\nresearch and progress in MLLMs development.", "AI": {"tldr": "介绍MCIF，该基准测试旨在评估多模态大规模语言模型（MLLMs）的多语言和多模态能力，解决了现有基准测试在这些方面评估不足的问题。", "motivation": "现有的基准测试在多语言和多模态能力方面的评估不足：它们往往是单一语言的，主要集中在某一模态，依赖于短上下文，或者缺乏人工注释，这阻碍了模型在跨语言、多模态和任务复杂性方面表现的全面评估。", "method": "提出MCIF (Multimodal Crosslingual Instruction Following)，这是一个多语言的人工注释基准，基于科学演讲，旨在评估跨语言、多模态环境中的指令跟随能力，涵盖短文本和长文本输入。MCIF跨越了三个核心模态（语音、视觉和文本）和四种不同的语言（英语、德语、意大利语和中文）。", "result": "未提供具体实验结果。", "conclusion": "MCIF基准测试发布在CC-BY 4.0许可下，鼓励开放研究和MLLMs的发展。"}}
{"id": "2507.19599", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.19599", "abs": "https://arxiv.org/abs/2507.19599", "authors": ["Haochen Wang", "Qirui Chen", "Cilin Yan", "Jiayin Cai", "Xiaolong Jiang", "Yao Hu", "Weidi Xie", "Stratis Gavves"], "title": "Object-centric Video Question Answering with Visual Grounding and Referring", "comment": null, "summary": "Video Large Language Models (VideoLLMs) have recently demonstrated remarkable\nprogress in general video understanding. However, existing models primarily\nfocus on high-level comprehension and are limited to text-only responses,\nrestricting the flexibility for object-centric, multiround interactions. In\nthis paper, we make three contributions: (i) we address these limitations by\nintroducing a VideoLLM model, capable of performing both object referring for\ninput and grounding for output in video reasoning tasks, i.e., allowing users\nto interact with videos using both textual and visual prompts; (ii) we propose\nSTOM (Spatial-Temporal Overlay Module), a novel approach that propagates\narbitrary visual prompts input at any single timestamp to the remaining frames\nwithin a video; (iii) we present VideoInfer, a manually curated object-centric\nvideo instruction dataset featuring questionanswering pairs that require\nreasoning. We conduct comprehensive experiments on VideoInfer and other\nexisting benchmarks across video question answering and referring object\nsegmentation. The results on 12 benchmarks of 6 tasks show that our proposed\nmodel consistently outperforms baselines in both video question answering and\nsegmentation, underscoring its robustness in multimodal, object-centric video\nand image understanding. Project page:\nhttps://qirui-chen.github.io/RGA3-release/.", "AI": {"tldr": "本文研究了一种新的VideoLLM模型，提升现有视频大型语言模型的灵活性，支持使用文本和视觉提示与视频进行交互。", "motivation": "现有的视频理解模型主要关注高层次的理解，并且局限于文本响应，这限制了面向对象的多轮交互的灵活性。我们的研究旨在通过引入新的模型方法来解决这些限制，增强用户与视频交互的灵活性和多样性。", "method": "我们提出了一个名为VideoLLM的模型，它可以进行输入对象引用和输出定位的视频推理任务，允许用户通过文本和视觉提示与视频进行交互。此外，我们设计了一个新的方法STOM（时空叠加模块），该方法能够将任意单一时段的视觉提示传播到整个视频的其余帧。还构建了一个名为VideoInfer的手工标注的以对象为中心的视频指令数据集，其中包括需要推理的问题和答案对。", "result": "在VideoInfer和其它现有的基准测试中进行的全面试验发现，所提出的模型在视频问答和分割中均超过了基线模型，显示了其在多模态对象中心视频和图像理解中的鲁棒性。", "conclusion": "实验结果验证了所提模型在视频问答和分割任务上的优越性，表明模型在多模态、以对象为中心的视频和图像理解中发挥了重要的作用。"}}
{"id": "2507.19666", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.19666", "abs": "https://arxiv.org/abs/2507.19666", "authors": ["Andrei Vlad Man", "Răzvan-Alexandru Smădu", "Cristian-George Craciun", "Dumitru-Clementin Cercel", "Florin Pop", "Mihaela-Claudia Cercel"], "title": "RoD-TAL: A Benchmark for Answering Questions in Romanian Driving License Exams", "comment": "49 pages, 52 figures", "summary": "The intersection of AI and legal systems presents a growing need for tools\nthat support legal education, particularly in under-resourced languages such as\nRomanian. In this work, we aim to evaluate the capabilities of Large Language\nModels (LLMs) and Vision-Language Models (VLMs) in understanding and reasoning\nabout Romanian driving law through textual and visual question-answering tasks.\nTo facilitate this, we introduce RoD-TAL, a novel multimodal dataset comprising\nRomanian driving test questions, text-based and image-based, alongside\nannotated legal references and human explanations. We implement and assess\nretrieval-augmented generation (RAG) pipelines, dense retrievers, and\nreasoning-optimized models across tasks including Information Retrieval (IR),\nQuestion Answering (QA), Visual IR, and Visual QA. Our experiments demonstrate\nthat domain-specific fine-tuning significantly enhances retrieval performance.\nAt the same time, chain-of-thought prompting and specialized reasoning models\nimprove QA accuracy, surpassing the minimum grades required to pass driving\nexams. However, visual reasoning remains challenging, highlighting the\npotential and the limitations of applying LLMs and VLMs to legal education.", "AI": {"tldr": "研究了LLMs和VLMs对罗马尼亚交通法的理解与推理能力，通过RoD-TAL数据集测试了多个模型。发现领域特定微调和特定提示可以优化性能，但在视觉推理上挑战仍存。", "motivation": "随着AI与法律系统的交汇，尤其在像罗马尼亚语这样的资源不足的语言中，需要支持法律教育的工具。因此，评估LLMs和VLMs在理解与推理罗马尼亚交通法方面的能力变得十分重要。", "method": "我们通过引入RoD-TAL这一多模态数据集，包含了罗马尼亚驾驶测试中的文本及图像问题、标注的法律参考和人类解释，来评估LLMs和VLMs对罗马尼亚交通法的理解与推理能力。我们实现了和评估了包含信息检索、问答、视觉信息检索和视觉问答等任务的检索增强生成（RAG）流水线、密集检索器和推理优化模型。", "result": "实验表明，针对特定领域的微调可以显著提高检索性能，而链式思路的提示和专门的推理模型能提高问答准确率，超过了通过驾驶考试所需的最低分数。然而，视觉推理仍然是一个挑战。", "conclusion": "LLMs和VLMs在特定领域的法律教育中有显著的应用潜力，特别是在问答任务中，但它们在处理视觉推理问题上仍具有局限性，显示了这些模型在应用中的潜在和限制。"}}
{"id": "2507.19621", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.19621", "abs": "https://arxiv.org/abs/2507.19621", "authors": ["Sheethal Bhat", "Bogdan Georgescu", "Adarsh Bhandary Panambur", "Mathias Zinnen", "Tri-Thien Nguyen", "Awais Mansoor", "Karim Khalifa Elbarbary", "Siming Bayer", "Florin-Cristian Ghesu", "Sasa Grbic", "Andreas Maier"], "title": "Exemplar Med-DETR: Toward Generalized and Robust Lesion Detection in Mammogram Images and beyond", "comment": null, "summary": "Detecting abnormalities in medical images poses unique challenges due to\ndifferences in feature representations and the intricate relationship between\nanatomical structures and abnormalities. This is especially evident in\nmammography, where dense breast tissue can obscure lesions, complicating\nradiological interpretation. Despite leveraging anatomical and semantic\ncontext, existing detection methods struggle to learn effective class-specific\nfeatures, limiting their applicability across different tasks and imaging\nmodalities. In this work, we introduce Exemplar Med-DETR, a novel multi-modal\ncontrastive detector that enables feature-based detection. It employs\ncross-attention with inherently derived, intuitive class-specific exemplar\nfeatures and is trained with an iterative strategy. We achieve state-of-the-art\nperformance across three distinct imaging modalities from four public datasets.\nOn Vietnamese dense breast mammograms, we attain an mAP of 0.7 for mass\ndetection and 0.55 for calcifications, yielding an absolute improvement of 16\npercentage points. Additionally, a radiologist-supported evaluation of 100\nmammograms from an out-of-distribution Chinese cohort demonstrates a twofold\ngain in lesion detection performance. For chest X-rays and angiography, we\nachieve an mAP of 0.25 for mass and 0.37 for stenosis detection, improving\nresults by 4 and 7 percentage points, respectively. These results highlight the\npotential of our approach to advance robust and generalizable detection systems\nfor medical imaging.", "AI": {"tldr": "本文介绍了一种新的多模态对比检测器Exemplar Med-DETR，解决了已有方法在医学图像异常检测中的不足，达到了前沿性能。", "motivation": "由于医学图像中的异常检测面临特征表现差异及解剖结构和异常间复杂关系的挑战，尤其是在乳腺X线摄影中，密集乳腺组织可能会掩盖病灶，使放射学解释变得复杂。已有检测方法利用解剖和语义上下文学习有效的类别特定特征的能力有限，限制了其在不同任务和成像模式中的应用。因此提出了本研究以解决这一问题。", "method": "引入了Exemplar Med-DETR，这是一种新的多模态对比检测器，能够实现基于特征的检测。它采用通过迭代策略训练的交叉注意力机制，包含内在生成的直观的类别特定示例特征。", "result": "在三种不同的成像模态来自四个公共数据集上，本方法达成了前沿性能。在越南密集乳腺X光影像中，本方法获得了积斑检测0.7的mAP和钙化物0.55的mAP，与现有方法相比绝对提高了16个百分点。在来自不同分布的中国人群中的100个乳腺X光影中的放射科医师支持评估显示病灶检测性能提升了两倍。此外在胸部X光影像和动脉造影中，分别达到了积斑检测0.25和狭窄检测0.37的mAP，分别提高了4和7个百分点。", "conclusion": "此方法有望促进医学影像中健壮且可泛化的检测系统的发展。"}}
{"id": "2507.19699", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.19699", "abs": "https://arxiv.org/abs/2507.19699", "authors": ["Maitha Alshehhi", "Ahmed Sharshar", "Mohsen Guizani"], "title": "Towards Inclusive NLP: Assessing Compressed Multilingual Transformers across Diverse Language Benchmarks", "comment": "Published in the 3rd International Workshop on Generalizing from\n  Limited Resources in the Open World. Workshop at International Joint\n  Conference on Artificial Intelligence (IJCAI) 2025", "summary": "Although LLMs have attained significant success in high-resource languages,\ntheir capacity in low-resource linguistic environments like Kannada and Arabic\nis not yet fully understood. This work benchmarking the performance of\nmultilingual and monolingual Large Language Models (LLMs) across Arabic,\nEnglish, and Indic languages, with particular emphasis on the effects of model\ncompression strategies such as pruning and quantization. Findings shows\nsignificant performance differences driven by linguistic diversity and resource\navailability on SOTA LLMS as BLOOMZ, AceGPT, Jais, LLaMA-2, XGLM, and AraGPT2.\nWe find that multilingual versions of the model outperform their\nlanguage-specific counterparts across the board, indicating substantial\ncross-lingual transfer benefits. Quantization (4-bit and 8-bit) is effective in\nmaintaining model accuracy while promoting efficiency, but aggressive pruning\nsignificantly compromises performance, especially in bigger models. Our\nfindings pinpoint key strategies to construct scalable and fair multilingual\nNLP solutions and underscore the need for interventions to address\nhallucination and generalization errors in the low-resource setting.", "AI": {"tldr": "研究比较了多种语言大语言模型在资源丰富的语言和资源匮乏的语言中的表现，发现多语言模型性能更优，量化有效但剪枝可能减少性能。", "motivation": "尽管大型语言模型在资源丰富的语言上取得了显著成功，但它们在资源匮乏的语言环境中的表现尚不明确。本研究旨在探索这一领域。", "method": "该研究通过评估多语言和单语言大语言模型（LLMs）在阿拉伯语、英语和印度语环境中的表现，重点考察模型压缩策略（如剪枝和量化）的影响。", "result": "研究发现，多语言模型在所有语言环境中均优于其特定语言的版本，表明跨语言迁移具有显著优势。量化技术（4位和8位）能有效保持模型准确率并提高效率，但激进的剪枝会严重影响大型模型的性能。", "conclusion": "研究指出了构建可扩展和公平的多语言NLP解决方案的关键策略，并强调了在资源匮乏环境下解决幻觉和泛化错误问题的必要性。"}}
{"id": "2507.19626", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.19626", "abs": "https://arxiv.org/abs/2507.19626", "authors": ["Adrian Celaya", "Tucker Netherton", "Dawid Schellingerhout", "Caroline Chung", "Beatrice Riviere", "David Fuentes"], "title": "Pre- and Post-Treatment Glioma Segmentation with the Medical Imaging Segmentation Toolkit", "comment": null, "summary": "Medical image segmentation continues to advance rapidly, yet rigorous\ncomparison between methods remains challenging due to a lack of standardized\nand customizable tooling. In this work, we present the current state of the\nMedical Imaging Segmentation Toolkit (MIST), with a particular focus on its\nflexible and modular postprocessing framework designed for the BraTS 2025 pre-\nand post-treatment glioma segmentation challenge. Since its debut in the 2024\nBraTS adult glioma post-treatment segmentation challenge, MIST's postprocessing\nmodule has been significantly extended to support a wide range of transforms,\nincluding removal or replacement of small objects, extraction of the largest\nconnected components, and morphological operations such as hole filling and\nclosing. These transforms can be composed into user-defined strategies,\nenabling fine-grained control over the final segmentation output. We evaluate\nthree such strategies - ranging from simple small-object removal to more\ncomplex, class-specific pipelines - and rank their performance using the BraTS\nranking protocol. Our results highlight how MIST facilitates rapid\nexperimentation and targeted refinement, ultimately producing high-quality\nsegmentations for the BraTS 2025 challenge. MIST remains open source and\nextensible, supporting reproducible and scalable research in medical image\nsegmentation.", "AI": {"tldr": "This paper details the enhanced postprocessing capabilities of the Medical Imaging Segmentation Toolkit (MIST), aimed at improving medical image segmentation, especially for the BraTS 2025 challenge, through customizable strategies.", "motivation": "The motivation behind this work is the need for standardized and customizable tooling to rigorously compare medical image segmentation methods, addressing the current lack thereof.", "method": "The paper introduces advancements in the Medical Imaging Segmentation Toolkit (MIST) with a focus on its enhanced postprocessing module that supports various morphological operations and custom strategies for medical image segmentation.", "result": "The evaluation of three different postprocessing strategies using the BraTS ranking protocol showed that MIST facilitates the production of high-quality segmentations by enabling rapid experimentation and targeted refinement.", "conclusion": "MIST, being an open source and extensible toolkit, supports reproducible and scalable research in medical image segmentation, particularly for the BraTS 2025 glioma segmentation challenge."}}
{"id": "2507.19710", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.19710", "abs": "https://arxiv.org/abs/2507.19710", "authors": ["Ronak Upasham", "Tathagata Dey", "Pushpak Bhattacharyya"], "title": "Ta-G-T: Subjectivity Capture in Table to Text Generation via RDF Graphs", "comment": null, "summary": "In Table-to-Text (T2T) generation, existing approaches predominantly focus on\nproviding objective descriptions of tabular data. However, generating text that\nincorporates subjectivity, where subjectivity refers to interpretations beyond\nraw numerical data, remains underexplored. To address this, we introduce a\nnovel pipeline that leverages intermediate representations to generate both\nobjective and subjective text from tables. Our three-stage pipeline consists\nof: 1) extraction of Resource Description Framework (RDF) triples, 2)\naggregation of text into coherent narratives, and 3) infusion of subjectivity\nto enrich the generated text. By incorporating RDFs, our approach enhances\nfactual accuracy while maintaining interpretability. Unlike large language\nmodels (LLMs) such as GPT-3.5, Mistral-7B, and Llama-2, our pipeline employs\nsmaller, fine-tuned T5 models while achieving comparable performance to GPT-3.5\nand outperforming Mistral-7B and Llama-2 in several metrics. We evaluate our\napproach through quantitative and qualitative analyses, demonstrating its\neffectiveness in balancing factual accuracy with subjective interpretation. To\nthe best of our knowledge, this is the first work to propose a structured\npipeline for T2T generation that integrates intermediate representations to\nenhance both factual correctness and subjectivity.", "AI": {"tldr": "本文提出了一种新的Table-to-Text生成管道，通过RDF三元组中间表示来生成既包含客观又包含主观解释的文本，并通过小规模微调的T5模型实现了与大规模语言模型相当的性能，甚至在某些指标上优于它们。", "motivation": "现有的Table-to-Text生成方法主要集中于提供表格数据的客观描述，对于需要主观解释的文本生成研究不足。因此，该研究旨在填补这一空白，提出一种结合中间表示的方法来增强生成文本的主观性和客观性。", "method": "Structure", "result": "{\n  \"tldr\": \"本文提出了一种新的Table-to-Text生成管道，通过RDF三元组中间表示来生成既包含客观又包含主观解释的文本，并通过小规模微调的T5模型实现了与大规模语言模型相当的性能，甚至在某些指标上优于它们。\",\n  \"motivation\": \"现有的Table-to-Text生成方法主要集中于提供表格数据的客观描述，对于需要主观解释的文本生成研究不足。因此，该研究旨在填补这一空白，提出一种结合中间表示的方法来增强生成文本的主观性和客观性。\",\n  \"method\": \"文中的三阶段管道方法包括：1) 提取资源描述框架（RDF）三元组；2) 将文本聚合为连贯的故事；3) 向生成的文本中加入主观性解释，以丰富其内容。文中使用的模型是小规模微调的T5模型。\",\n  \"result\": \"通过定量和定性分析，验证了方法在保持事实准确性的同时，能够在主观解释上进行有效平衡，并且证明了此方法在性能上可以与大规模的语言模型相匹敌，甚至在某些指标上超越它们。\",\n  \"conclusion\": \"这项工作是首次提出的将中间表示集成到Table-to-Text生成中的结构化管道，其创新在于通过小模型实现高性能的同时，还能生成包含主观性解释的文本，是对现有T2T生成流程的一个突破。\")", "conclusion": "这项工作是首次提出的将中间表示集成到Table-to-Text生成中的结构化管道，其创新在于通过小模型实现高性能的同时，还能生成包含主观性解释的文本，是对现有T2T生成流程的一个突破。"}}
{"id": "2507.19673", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.19673", "abs": "https://arxiv.org/abs/2507.19673", "authors": ["Babak Taati", "Muhammad Muzammil", "Yasamin Zarghami", "Abhishek Moturu", "Airhossein Kazerouni", "Hailey Reimer", "Alex Mihailidis", "Thomas Hadjistavropoulos"], "title": "SynPAIN: A Synthetic Dataset of Pain and Non-Pain Facial Expressions", "comment": "10 pages, 4 figures, submitted to IEEE JBHI", "summary": "Accurate pain assessment in patients with limited ability to communicate,\nsuch as older adults with dementia, represents a critical healthcare challenge.\nRobust automated systems of pain detection may facilitate such assessments.\nExisting pain detection datasets, however, suffer from limited ethnic/racial\ndiversity, privacy constraints, and underrepresentation of older adults who are\nthe primary target population for clinical deployment. We present SynPAIN, a\nlarge-scale synthetic dataset containing 10,710 facial expression images (5,355\nneutral/expressive pairs) across five ethnicities/races, two age groups (young:\n20-35, old: 75+), and two genders. Using commercial generative AI tools, we\ncreated demographically balanced synthetic identities with clinically\nmeaningful pain expressions. Our validation demonstrates that synthetic pain\nexpressions exhibit expected pain patterns, scoring significantly higher than\nneutral and non-pain expressions using clinically validated pain assessment\ntools based on facial action unit analysis. We experimentally demonstrate\nSynPAIN's utility in identifying algorithmic bias in existing pain detection\nmodels. Through comprehensive bias evaluation, we reveal substantial\nperformance disparities across demographic characteristics. These performance\ndisparities were previously undetectable with smaller, less diverse datasets.\nFurthermore, we demonstrate that age-matched synthetic data augmentation\nimproves pain detection performance on real clinical data, achieving a 7.0%\nimprovement in average precision. SynPAIN addresses critical gaps in pain\nassessment research by providing the first publicly available, demographically\ndiverse synthetic dataset specifically designed for older adult pain detection,\nwhile establishing a framework for measuring and mitigating algorithmic bias.\nThe dataset is available at https://doi.org/10.5683/SP3/WCXMAP", "AI": {"tldr": "该研究创建了一个大规模合成面部表情数据集SynPAIN，包含10,710张面部表情图像，旨在用于解决现有疼痛检测数据集中存在的多样性不足、隐私限制和老年人代表性不足的问题。", "motivation": "为了针对疼痛检测数据集中存在的问题，如种族/民族多样性不足、隐私限制和目标群体老年人的代表性不足，开发了SynPAIN数据集。", "method": "使用商业生成AI工具创建了种族/民族、年龄和性别平衡的合成身份，并加入了具有临床意义的疼痛表情。", "result": "验证发现合成疼痛表情存在预期的疼痛模式，并且年龄匹配的合成数据扩充在真实临床数据上的疼痛检测性能提高了7.0%的平均精度。", "conclusion": "SynPAIN数据集为首次公开的、具有多样化人口统计学特征的老年人疼痛检测合成数据集，同时建立了衡量和减轻算法偏见的框架。"}}
{"id": "2507.19741", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.19741", "abs": "https://arxiv.org/abs/2507.19741", "authors": ["Zhi Zhou", "Sirui Miao", "Xiangyu Duan", "Hao Yang", "Min Zhang"], "title": "Basic Reading Distillation", "comment": null, "summary": "Large language models (LLMs) have demonstrated remarkable abilities in\nvarious natural language processing areas, but they demand high computation\nresources which limits their deployment in real-world. Distillation is one\ntechnique to solve this problem through either knowledge distillation or task\ndistillation. Both distillation approaches train small models to imitate\nspecific features of LLMs, but they all neglect basic reading education for\nsmall models on generic texts that are \\emph{unrelated} to downstream tasks. In\nthis paper, we propose basic reading distillation (BRD) which educates a small\nmodel to imitate LLMs basic reading behaviors, such as named entity\nrecognition, question raising and answering, on each sentence. After such basic\neducation, we apply the small model on various tasks including language\ninference benchmarks and BIG-bench tasks. It shows that the small model can\noutperform or perform comparable to over 20x bigger LLMs. Analysis reveals that\nBRD effectively influences the probability distribution of the small model, and\nhas orthogonality to either knowledge distillation or task distillation.", "AI": {"tldr": "本文提出了基础阅读蒸馏（BRD）的方法，用于训练小模型模仿大型模型的基础阅读行为，以增强其在各种任务上的表现。", "motivation": "大型语言模型（LLMs）在自然语言处理领域表现出色，但计算资源需求高，限制了实际部署。通过知识蒸馏或任务蒸馏技术将大型模型的知识转移到小模型上，但这些方法忽略了对小模型进行通用文本基础阅读教育的需求。", "method": "提出基础阅读蒸馏（BRD）方法，通过模仿大模型在无关下游任务的一般文本上的基础阅读行为，如命名实体识别、问题提出与回答，对小模型进行训练。", "result": "在语言推理基准测试和BIG-bench任务上，接受BRD训练的小模型性能可以优于或接近20倍大的LLMs。", "conclusion": "BRD能够有效地影响小模型的概率分布，并且与知识蒸馏或任务蒸馏具有正交性，即BRD可以与这两种方法结合使用以进一步提高小模型的性能。"}}
{"id": "2507.19679", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.19679", "abs": "https://arxiv.org/abs/2507.19679", "authors": ["Mandar Kulkarni"], "title": "Efficient Learning for Product Attributes with Compact Multimodal Models", "comment": null, "summary": "Image-based product attribute prediction in e-commerce is a crucial task with\nnumerous applications. The supervised fine-tuning of Vision Language Models\n(VLMs) faces significant scale challenges due to the cost of manual or API\nbased annotation. In this paper, we investigate label-efficient semi-supervised\nfine-tuning strategies for compact VLMs (2B-3B parameters) that leverage\nunlabeled product listings through Direct Preference Optimization (DPO).\nBeginning with a small, API-based, annotated, and labeled set, we first employ\nPEFT to train low-rank adapter modules. To update the adapter weights with\nunlabeled data, we generate multiple reasoning-and-answer chains per unlabeled\nsample and segregate these chains into preferred and dispreferred based on\nself-consistency. We then fine-tune the model with DPO loss and use the updated\nmodel for the next iteration. By using PEFT fine-tuning with DPO, our method\nachieves efficient convergence with minimal compute overhead. On a dataset\nspanning twelve e-commerce verticals, DPO-based fine-tuning, which utilizes\nonly unlabeled data, demonstrates a significant improvement over the supervised\nmodel. Moreover, experiments demonstrate that accuracy with DPO training\nimproves with more unlabeled data, indicating that a large pool of unlabeled\nsamples can be effectively leveraged to improve performance.", "AI": {"tldr": "本文提出了一种用于视觉语言模型的标签高效半监督微调策略，该策略通过直接偏好优化和少量标注数据，有效利用大量未标注数据，显著提高了模型在电商平台产品属性预测任务上的性能。", "motivation": "本文的动机在于解决视觉语言模型的监督微调面临的数据标注成本高且规模挑战大的问题。", "method": "本文研究了用于紧凑型视觉语言模型（20亿到30亿参数）的标签高效半监督微调策略，这些策略通过直接偏好优化（DPO）利用未标注的产品列表。该方法首先使用基于API的小标注数据集进行PEFT训练，生成低秩适配器模块，然后通过生成每个未标注样本的多个推理和回答链，根据自洽性将其分为偏好和非偏好，最后使用DPO损失进行模型微调，并使用更新后的模型进行下一轮迭代。", "result": "在涵盖十二个电子商务垂直领域的数据集上，基于DPO的微调方法仅使用未标注数据显著优于监督模型，并且实验表明，随着未标注数据量的增加，DPO训练的准确率会提升，表明可以有效地利用大量未标注样本提高性能。", "conclusion": "研究结果表明，通过PEFT与DPO组合使用，可以实现高效收敛并最小化计算开销，仅使用未标注数据进行DPO训练的方法在多个电子商务类别上显著优于监督学习，并且随着未标注数据量的增加，DPO训练的性能表现也逐步提升。"}}
{"id": "2507.19748", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.19748", "abs": "https://arxiv.org/abs/2507.19748", "authors": ["Yifan Hao", "Fangning Chao", "Yaqian Hao", "Zhaojun Cui", "Huan Bai", "Haiyu Zhang", "Yankai Liu", "Chao Deng", "Junlan Feng"], "title": "JT-Math: A Multi-Stage Framework for Advanced Mathematical Reasoning in Large Language Models", "comment": null, "summary": "Mathematical reasoning is a cornerstone of artificial general intelligence\nand a primary benchmark for evaluating the capabilities of Large Language\nModels (LLMs). While state-of-the-art models show promise, they often falter\nwhen faced with complex problems that demand deep conceptual understanding and\nintricate, multi-step deliberation. To address this challenge, we introduce\nJT-Math-8B, a series of open-source models comprising base, instruct, and\nthinking versions, built upon a systematic, multi-stage optimization framework.\nOur pre-training corpus is a high-quality, 210B-token dataset curated through a\ndedicated data pipeline that uses model-based validation to ensure quality and\ndiversity. The Instruct Model is optimized for direct, concise answers through\nSupervised Fine-Tuning (SFT) and a GRPO-based reinforcement learning (RL)\nmethod. The Thinking Model is trained for complex problem-solving using a Long\nChain-of-Thought (Long CoT) approach, combining SFT with a novel, multi-stage\nRL curriculum that progressively increases task difficulty and context length\nup to 32K tokens. JT-Math-8B achieves state-of-the-art results among\nopen-source models of similar size, surpassing prominent models like OpenAI's\nO1-mini and GPT-4o , and demonstrating superior performance on\ncompetition-level mathematics.", "AI": {"tldr": "介绍了JT-Math-8B系列开放源代码模型，用于提高解决复杂数学问题的能力，在同类模型中表现优异。", "motivation": "解决当前顶尖模型在处理需要深入概念理解和复杂的多步骤推理的复杂问题时表现不佳的挑战。", "method": "通过一个系统化的多阶段优化框架，介绍了一个开放源代码模型系列JT-Math-8B，包括基础模型、指令模型和思考模型。预训练语料库是一个高质量的210B-token数据集，通过专门的数据管道，利用基于模型的验证以确保数据的质量和多样性。指令模型通过监督微调和基于GRPO的强化学习方法进行优化，以获得直接、简洁的答案。思考模型通过结合长链条思维方法和新型多阶段强化学习课程进行训练，以解决复杂问题，逐步增加任务难度和上下文长度高达32K token。", "result": "JT-Math-8B在同类开放源代码模型中取得了最先进的成果，超越了如OpenAI的O1-mini和GPT-4o等著名模型，并在竞赛级别的数学上表现出色。", "conclusion": "JT-Math-8B证明了其在解决问题能力方面的优越性，特别是在处理复杂数学逻辑方面，为改进大型语言模型的数学推理能力提供了新的思路。"}}
{"id": "2507.19682", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.19682", "abs": "https://arxiv.org/abs/2507.19682", "authors": ["Matthew Drexler", "Benjamin Risk", "James J Lah", "Suprateek Kundu", "Deqiang Qiu"], "title": "DeepJIVE: Learning Joint and Individual Variation Explained from Multimodal Data Using Deep Learning", "comment": "26 pages, 10 figures", "summary": "Conventional multimodal data integration methods provide a comprehensive\nassessment of the shared or unique structure within each individual data type\nbut suffer from several limitations such as the inability to handle\nhigh-dimensional data and identify nonlinear structures. In this paper, we\nintroduce DeepJIVE, a deep-learning approach to performing Joint and Individual\nVariance Explained (JIVE). We perform mathematical derivation and experimental\nvalidations using both synthetic and real-world 1D, 2D, and 3D datasets.\nDifferent strategies of achieving the identity and orthogonality constraints\nfor DeepJIVE were explored, resulting in three viable loss functions. We found\nthat DeepJIVE can successfully uncover joint and individual variations of\nmultimodal datasets. Our application of DeepJIVE to the Alzheimer's Disease\nNeuroimaging Initiative (ADNI) also identified biologically plausible\ncovariation patterns between the amyloid positron emission tomography (PET) and\nmagnetic resonance (MR) images. In conclusion, the proposed DeepJIVE can be a\nuseful tool for multimodal data analysis.", "AI": {"tldr": "我们介绍了一种新的深度学习方法DeepJIVE，用于进行联合与个体方差解释，能够成功揭示多模态数据中的重要关系，并有助于识别疾病相关的生物标志物。", "motivation": "传统的多模态数据集成方法虽能提供对每个数据类型的共享或独特结构的全面评估，但存在着无法处理高维数据和识别非线性结构的问题，因此我们提出了DeepJIVE方法来解决这些问题。", "method": "我们提出了一种名为DeepJIVE的深度学习方法，用于执行联合与个体方差解释(JIVE)，并探讨了三种实现DeepJIVE身份和正交性约束的策略及其对应的损失函数。", "result": "实验结果表明，DeepJIVE能够成功地揭示多模态数据集中的联合与个体变化。DeepJIVE应用于阿尔茨海默病神经影像学计划(ADNI)时，也识别出了淀粉样正电子发射断层扫描(PET)和磁共振(MR)成像之间的生物学上合理的共变模式。", "conclusion": "结论部分指出，所提出的DeepJIVE可以成为多模态数据分析的有用工具。"}}
{"id": "2507.19756", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.19756", "abs": "https://arxiv.org/abs/2507.19756", "authors": ["Rebecca M. M. Hicke", "Brian Haggard", "Mia Ferrante", "Rayhan Khanna", "David Mimno"], "title": "Are You There God? Lightweight Narrative Annotation of Christian Fiction with LMs", "comment": null, "summary": "In addition to its more widely studied political activities, the American\nEvangelical movement has a well-developed but less externally visible cultural\nand literary side. Christian Fiction, however, has been little studied, and\nwhat scholarly attention there is has focused on the explosively popular Left\nBehind series. In this work, we use computational tools to provide both a broad\ntopical overview of Christian Fiction as a genre and a more directed\nexploration of how its authors depict divine acts. Working with human\nannotators we first developed definitions and a codebook for \"acts of God.\" We\nthen adapted those instructions designed for human annotators for use by a\nrecent, lightweight LM with the assistance of a much larger model. The\nlaptop-scale LM is capable of matching human annotations, even when the task is\nsubtle and challenging. Using these annotations, we show that significant and\nmeaningful differences exist between the Left Behind books and Christian\nFiction more broadly and between books by male and female authors.", "AI": {"tldr": "研究通过计算工具分析了美国福音派文化下的基督教小说，聚焦神迹主题，揭示了《创世记》系列与广泛基督教小说以及男性和女性作者作品间的差异。", "motivation": "尽管美国福音派政治活动备受瞩目，但其文化与文学方面，尤其是基督教小说，却较少受到学术界的关注。本研究旨在通过计算方法对这一文学类别进行深入探讨。", "method": "研究团队利用人类注释者开发定义和编码手册，并通过调整使其适用于轻量级语言模型，用以分析《创世记》系列和其他基督教小说中的神迹描述。", "result": "该研究主要关注美国福音派文化中的基督教小说，并运用计算工具对其进行了广泛的主题概述及神迹描绘的具体分析。研究团队首先开发了用于人类注释者的定义和编码手册，并将其转换为适合由轻量级语言模型使用的指令。结果显示，与广受欢迎的《创世记》系列相比，基督教小说中存在显著且有意义的差异。此外，还发现男性和女性作者的作品之间存在差异。", "conclusion": "研究结果表明，使用计算工具可以很好地分析基督教小说的不同方面，揭示不同类型作品之间以及不同性别作者作品之间的显著差异。"}}
{"id": "2507.19691", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.19691", "abs": "https://arxiv.org/abs/2507.19691", "authors": ["Haichuan Li", "Tomi Westerlund"], "title": "Co-Win: Joint Object Detection and Instance Segmentation in LiDAR Point Clouds via Collaborative Window Processing", "comment": null, "summary": "Accurate perception and scene understanding in complex urban environments is\na critical challenge for ensuring safe and efficient autonomous navigation. In\nthis paper, we present Co-Win, a novel bird's eye view (BEV) perception\nframework that integrates point cloud encoding with efficient parallel\nwindow-based feature extraction to address the multi-modality inherent in\nenvironmental understanding. Our method employs a hierarchical architecture\ncomprising a specialized encoder, a window-based backbone, and a query-based\ndecoder head to effectively capture diverse spatial features and object\nrelationships. Unlike prior approaches that treat perception as a simple\nregression task, our framework incorporates a variational approach with\nmask-based instance segmentation, enabling fine-grained scene decomposition and\nunderstanding. The Co-Win architecture processes point cloud data through\nprogressive feature extraction stages, ensuring that predicted masks are both\ndata-consistent and contextually relevant. Furthermore, our method produces\ninterpretable and diverse instance predictions, enabling enhanced downstream\ndecision-making and planning in autonomous driving systems.", "AI": {"tldr": "本文提出了一种新颖的BEV感知框架Co-Win，用于增强自动驾驶系统中的场景理解和下游决策。", "motivation": "本文旨在解决复杂城市环境中准确感知和场景理解的难题，这对于确保自动驾驶的安全和效率至关重要。", "method": "本文提出了一种名为Co-Win的新颖的鸟瞰视图（BEV）感知框架，该框架结合了点云编码与高效的并行窗口特征提取，以应对环境理解中的多模态性挑战。该方法采用分层架构，包括专门的编码器、窗口主干和基于查询的解码头，以有效捕捉多样化的空间特征和对象关系。", "result": "Co-Win架构通过逐步特征提取步骤处理点云数据，既可以确保预测的掩膜是数据一致的，又能保持上下文的相关性。", "conclusion": "Co-Win 架构能够生成高质量的实例预测，这对于增强自动驾驶系统中的下游决策和规划至关重要。"}}
{"id": "2507.19766", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.19766", "abs": "https://arxiv.org/abs/2507.19766", "authors": ["Dong Du", "Shulin Liu", "Tao Yang", "Shaohua Chen", "Yang Li"], "title": "UloRL:An Ultra-Long Output Reinforcement Learning Approach for Advancing Large Language Models' Reasoning Abilities", "comment": "12 pages", "summary": "Recent advances in large language models (LLMs) have highlighted the\npotential of reinforcement learning with verifiable rewards (RLVR) to enhance\nreasoning capabilities through extended output sequences. However, traditional\nRL frameworks face inefficiencies when handling ultra-long outputs due to\nlong-tail sequence distributions and entropy collapse during training. To\naddress these challenges, we propose an Ultra-Long Output Reinforcement\nLearning (UloRL) approach for advancing large language models' reasoning\nabilities. Specifically, we divide ultra long output decoding into short\nsegments, enabling efficient training by mitigating delays caused by long-tail\nsamples. Additionally, we introduce dynamic masking of well-Mastered Positive\nTokens (MPTs) to prevent entropy collapse. Experimental results demonstrate the\neffectiveness of our approach. On the Qwen3-30B-A3B model, RL with segment\nrollout achieved 2.06x increase in training speed, while RL training with\n128k-token outputs improves the model's performance on AIME2025 from 70.9\\% to\n85.1\\% and on BeyondAIME from 50.7\\% to 61.9\\%, even surpassing Qwen3-235B-A22B\nwith remarkable gains. These findings underscore the potential of our methods\nto advance the reasoning capabilities of LLMs with ultra-long sequence\ngeneration. We will release our code and model for further use by the\ncommunity.", "AI": {"tldr": "提出了一种超长输出强化学习方法UloRL，通过分割输出和动态屏蔽技术提高大语言模型的训练效率和推理能力。", "motivation": "传统的强化学习框架在处理超长输出时面临效率问题，由于长尾序列分布和训练过程中的熵崩溃。为了应对这些挑战，提高大语言模型的推理能力。", "method": "将超长输出解码分割成短片段，通过减少长尾样本导致的延迟来实现高效训练。同时，引入动态屏蔽掌握良好的正令牌（MPTs）以防止熵崩溃。", "result": "实验结果表明，分段回放的RL实现了2.06倍的训练速度提升，而使用128k-token输出的RL培训将Qwen3-30B-A3B模型在AIME2025上的表现从70.9%提高到85.1%，在BeyondAIME上的表现从50.7%提高到61.9%。", "conclusion": "研究结果强调了该方法在提高超长序列生成的大型语言模型推理能力方面的潜力。研究团队会公开代码和模型供社区进一步使用。"}}
{"id": "2507.19705", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.19705", "abs": "https://arxiv.org/abs/2507.19705", "authors": ["Asmae Lamsaf", "Lucia Cascone", "Hugo Proença", "João Neves"], "title": "Bias Analysis for Synthetic Face Detection: A Case Study of the Impact of Facial Attribute", "comment": null, "summary": "Bias analysis for synthetic face detection is bound to become a critical\ntopic in the coming years. Although many detection models have been developed\nand several datasets have been released to reliably identify synthetic content,\none crucial aspect has been largely overlooked: these models and training\ndatasets can be biased, leading to failures in detection for certain\ndemographic groups and raising significant social, legal, and ethical issues.\nIn this work, we introduce an evaluation framework to contribute to the\nanalysis of bias of synthetic face detectors with respect to several facial\nattributes. This framework exploits synthetic data generation, with evenly\ndistributed attribute labels, for mitigating any skew in the data that could\notherwise influence the outcomes of bias analysis. We build on the proposed\nframework to provide an extensive case study of the bias level of five\nstate-of-the-art detectors in synthetic datasets with 25 controlled facial\nattributes. While the results confirm that, in general, synthetic face\ndetectors are biased towards the presence/absence of specific facial\nattributes, our study also sheds light on the origins of the observed bias\nthrough the analysis of the correlations with the balancing of facial\nattributes in the training sets of the detectors, and the analysis of detectors\nactivation maps in image pairs with controlled attribute modifications.", "AI": {"tldr": "研究分析了合成人脸检测器的偏见，并通过一个评估框架来评估多个面部属性的检测偏差，揭示了检测器偏见的根源。", "motivation": "尽管已经开发了许多检测模型并发布了几个数据集来可靠地识别合成内容，但这些模型和训练数据可能存在偏见，导致某些人口群体的检测失败，并引发重大的社会、法律和伦理问题。", "method": "本研究提出了一套评估框架，用于分析合成人脸检测器在多种面部属性上的偏见。该框架利用了属性标签均匀分布的合成数据生成方法，以减少数据偏斜对偏见分析的影响。", "result": "研究表明，总体而言，合成脸检测器具有对特定面部特征存在与否的偏见。通过分析检测器训练集中面部属性的平衡性以及对图像配对控制属性修改的检测器激活图，研究揭示了观察到的偏见的起源。", "conclusion": "研究为理解和分析合成人脸检测器的偏见提供了重要见解，并强调了在开发检测器和生成训练数据时需要关注的公平性和伦理问题。"}}
{"id": "2507.19786", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.19786", "abs": "https://arxiv.org/abs/2507.19786", "authors": ["Tianxiang Chen", "Zhentao Tan", "Xiaofan Bo", "Yue Wu", "Tao Gong", "Qi Chu", "Jieping Ye", "Nenghai Yu"], "title": "Flora: Effortless Context Construction to Arbitrary Length and Scale", "comment": null, "summary": "Effectively handling long contexts is challenging for Large Language Models\n(LLMs) due to the rarity of long texts, high computational demands, and\nsubstantial forgetting of short-context abilities. Recent approaches have\nattempted to construct long contexts for instruction tuning, but these methods\noften require LLMs or human interventions, which are both costly and limited in\nlength and diversity. Also, the drop in short-context performances of present\nlong-context LLMs remains significant. In this paper, we introduce Flora, an\neffortless (human/LLM-free) long-context construction strategy. Flora can\nmarkedly enhance the long-context performance of LLMs by arbitrarily assembling\nshort instructions based on categories and instructing LLMs to generate\nresponses based on long-context meta-instructions. This enables Flora to\nproduce contexts of arbitrary length and scale with rich diversity, while only\nslightly compromising short-context performance. Experiments on\nLlama3-8B-Instruct and QwQ-32B show that LLMs enhanced by Flora excel in three\nlong-context benchmarks while maintaining strong performances in short-context\ntasks. Our data-construction code is available at\n\\href{https://github.com/txchen-USTC/Flora}{https://github.com/txchen-USTC/Flora}.", "AI": {"tldr": "本文介绍的Flora通过一种无需大型语言模型或人类介入的策略，构建了多样且任意长度的长上下文，有效提升了大型语言模型在长上下文下的性能，同时保持良好的短期上下文性能。", "motivation": "由于长文本的稀有性，高计算需求以及遗忘短期能力的问题，处理长上下文对于大型语言模型来说是一个挑战。现有的方法通常需要大型语言模型或人为干预，成本高昂且在长度和多样性上有限，同时现有长上下文大型语言模型在短期上下文性能上的下降仍然相当显著。", "method": "Flora采用了一种简单（无需人类或大型语言模型介入）的长上下文构建策略。Flora通过任意组合基于类别的短期指令，指示大型语言模型根据长上下文元指令生成响应，从而显著提升大型语言模型的长上下文性能。这使Flora能够生成任意长度和丰富多样性上下文，同时仅轻微影响短期上下文性能。", "result": "实验结果证明，经Flora增强后的Llama3-8B-Instruct和QwQ-32B在三个长上下文基准测试中表现出色，同时在短期上下文任务上保持强大性能。", "conclusion": "Flora展示了其在构建长上下文方面的优势，提供了丰富多样且任意长度的上下文，并且在不显著影响短期性能的情况下提升了大型语言模型的长上下文能力。"}}
{"id": "2507.19730", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.19730", "abs": "https://arxiv.org/abs/2507.19730", "authors": ["Liyang Wang", "Shiqian Wu", "Shun Fang", "Qile Zhu", "Jiaxin Wu", "Sos Again"], "title": "Quaternion-Based Robust PCA for Efficient Moving Target Detection and Background Recovery in Color Videos", "comment": null, "summary": "Moving target detection is a challenging computer vision task aimed at\ngenerating accurate segmentation maps in diverse in-the-wild color videos\ncaptured by static cameras. If backgrounds and targets can be simultaneously\nextracted and recombined, such synthetic data can significantly enrich\nannotated in-the-wild datasets and enhance the generalization ability of deep\nmodels. Quaternion-based RPCA (QRPCA) is a promising unsupervised paradigm for\ncolor image processing. However, in color video processing, Quaternion Singular\nValue Decomposition (QSVD) incurs high computational costs, and rank-1\nquaternion matrix fails to yield rank-1 color channels. In this paper, we\nreduce the computational complexity of QSVD to o(1) by utilizing a quaternion\nRiemannian manifold. Furthermor, we propose the universal QRPCA (uQRPCA)\nframework, which achieves a balance in simultaneously segmenting targets and\nrecovering backgrounds from color videos. Moreover, we expand to uQRPCA+ by\nintroducing the Color Rank-1 Batch (CR1B) method to further process and obtain\nthe ideal low-rank background across color channels. Experiments demonstrate\nour uQRPCA+ achieves State Of The Art (SOTA) performance on moving target\ndetection and background recovery tasks compared to existing open-source\nmethods. Our implementation is publicly available on GitHub at\nhttps://github.com/Ruchtech/uQRPCA", "AI": {"tldr": "This paper introduces the uQRPCA+ framework for moving target detection in color videos, offering state-of-the-art performance in both target segmentation and background recovery by reducing computational complexity and proposing novel methods for color channel processing.", "motivation": "The motivation behind the paper is to address the limitations of Quaternion-based RPCA (QRPCA) by reducing its computational complexity and improving its ability to process color channels effectively. The ultimate goal is to enhance the creation of synthetic data that can enrich datasets and improve the generalization ability of deep learning models for moving target detection in color videos.", "method": "The paper employs a quaternion Riemannian manifold to minimize the computational complexity of Quaternion Singular Value Decomposition (QSVD) to o(1). Additionally, the universal QRPCA (uQRPCA) framework is proposed, followed by the uQRPCA+ method that includes the Color Rank-1 Batch (CR1B) technique to improve background recovery.", "result": "The experimental results indicate that the uQRPCA+ method outperforms existing open-source approaches in moving target detection and background recovery tasks.", "conclusion": "The uQRPCA+ framework successfully enhances moving target detection by optimizing the processing of color videos and offering state-of-the-art performance. The work signifies progress towards automated generation of labeled datasets and improving deep model performance in diverse video scenarios."}}
{"id": "2507.19823", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.19823", "abs": "https://arxiv.org/abs/2507.19823", "authors": ["Dongquan Yang", "Yifan Yang", "Xiaotian Yu", "Xianbiao Qi", "Rong Xiao"], "title": "HCAttention: Extreme KV Cache Compression via Heterogeneous Attention Computing for LLMs", "comment": null, "summary": "Processing long-context inputs with large language models presents a\nsignificant challenge due to the enormous memory requirements of the Key-Value\n(KV) cache during inference. Existing KV cache compression methods exhibit\nnoticeable performance degradation when memory is reduced by more than 85%.\nAdditionally, strategies that leverage GPU-CPU collaboration for approximate\nattention remain underexplored in this setting. We propose HCAttention, a\nheterogeneous attention computation framework that integrates key quantization,\nvalue offloading, and dynamic KV eviction to enable efficient inference under\nextreme memory constraints. The method is compatible with existing transformer\narchitectures and does not require model fine-tuning. Experimental results on\nthe LongBench benchmark demonstrate that our approach preserves the accuracy of\nfull-attention model while shrinking the KV cache memory footprint to 25% of\nits original size. Remarkably, it stays competitive with only 12.5% of the\ncache, setting a new state-of-the-art in LLM KV cache compression. To the best\nof our knowledge, HCAttention is the first to extend the Llama-3-8B model to\nprocess 4 million tokens on a single A100 GPU with 80GB memory.", "AI": {"tldr": "本文提出HCAttention方法，该方法结合键量化、值卸载和动态KV缓存淘汰技术，能够在极端内存限制下进行高效的推理，显著减少了KV缓存的内存占用，并且在长输入处理上达到了最先进的水平。", "motivation": "现有的KV缓存压缩方法在内存减少超过85%时性能会显著下降，并且在近似注意力计算中利用GPU-CPU协作的策略还处于探索阶段。", "method": "HCAttention, 一种异构注意力计算框架，结合了键量化、值卸载和动态KV缓存淘汰，以在极端内存约束下实现高效的推理。该方法与现有的transformer架构兼容，且不需要模型微调。", "result": "实验结果表明，该方法在LongBench基准上能够保留全注意力模型的准确性，同时将KV缓存内存占用减少到原尺寸的25%。当缓存占用仅为12.5%时该方法仍然具有竞争力，并且在KV缓存压缩中达到了新的最先进水平。", "conclusion": "HCAttention首次将Llama-3-8B模型扩展到单个80GB内存的A100 GPU上处理400万个token。"}}
{"id": "2507.19738", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.19738", "abs": "https://arxiv.org/abs/2507.19738", "authors": ["Jinsu Yoo", "Sooyoung Jeon", "Zanming Huang", "Tai-Yu Pan", "Wei-Lun Chao"], "title": "Leveraging Sparse LiDAR for RAFT-Stereo: A Depth Pre-Fill Perspective", "comment": null, "summary": "We investigate LiDAR guidance within the RAFT-Stereo framework, aiming to\nimprove stereo matching accuracy by injecting precise LiDAR depth into the\ninitial disparity map. We find that the effectiveness of LiDAR guidance\ndrastically degrades when the LiDAR points become sparse (e.g., a few hundred\npoints per frame), and we offer a novel explanation from a signal processing\nperspective. This insight leads to a surprisingly simple solution that enables\nLiDAR-guided RAFT-Stereo to thrive: pre-filling the sparse initial disparity\nmap with interpolation. Interestingly, we find that pre-filling is also\neffective when injecting LiDAR depth into image features via early fusion, but\nfor a fundamentally different reason, necessitating a distinct pre-filling\napproach. By combining both solutions, the proposed Guided RAFT-Stereo\n(GRAFT-Stereo) significantly outperforms existing LiDAR-guided methods under\nsparse LiDAR conditions across various datasets. We hope this study inspires\nmore effective LiDAR-guided stereo methods.", "AI": {"tldr": "通过分析LiDAR指导在RAFT-Stereo框架中的应用，本文提出了一种新的解决方案，即通过预填充稀疏的视差图来改进LiDAR指导的效果，并且通过结合不同的预填充方法，使提出的GRAFT-Stereo在不同数据集下优于现有的LiDAR指导方法。", "motivation": "本文的目标是探索如何利用稀疏的LiDAR点云来提高立体匹配算法的性能，并发现了当LiDAR点稀疏时，立体匹配效果会显著下降，从而寻求解决方案。", "method": "我们探讨了在RAFT-Stereo框架中使用LiDAR进行指导，以通过向初始视差图注入精确的LiDAR深度来提高立体匹配的准确性。我们发现，当LiDAR点稀疏时（例如，每帧只有几百个点），LiDAR指导的效果会急剧下降，并从信号处理的角度给出了一种新颖的解释。这一见解导致了一个出人意料的简单解决方案，使得LiDAR指导的RAFT-Stereo能够成功：预先通过插值填充稀疏的初始视差图。有趣的是，我们发现，在通过早期融合将LiDAR深度注入图像特征时，预先填充同样有效，但这背后原因不同，需要采用不同的预填充方法。", "result": "通过结合这两种方案，提出的Guided RAFT-Stereo (GRAFT-Stereo)在各种数据集下的稀疏LiDAR条件下显著优于现有的LiDAR指导方法。", "conclusion": "我们期望这项研究能够激励更多高效利用LiDAR指导的立体匹配方法。"}}
{"id": "2507.19867", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.19867", "abs": "https://arxiv.org/abs/2507.19867", "authors": ["Anshul Chavda", "M Jagadeesh", "Chintalapalli Raja Kullayappa", "B Jayaprakash", "Medchalimi Sruthi", "Pushpak Bhattacharyya"], "title": "DRIVE: Disfluency-Rich Synthetic Dialog Data Generation Framework for Intelligent Vehicle Environments", "comment": null, "summary": "In-car conversational AI is becoming increasingly critical as autonomous\nvehicles and smart assistants gain widespread adoption. Yet, existing datasets\nfail to capture the spontaneous disfluencies such as hesitations, false starts,\nrepetitions, and self-corrections that characterize real driver-AI dialogs. To\naddress this, we introduce DiscoDrive, a synthetic corpus of 3500 multi-turn\ndialogs across seven automotive domains, generated using a two-stage,\nprompt-driven pipeline that dynamically integrates disfluencies during\nsynthesis. We show that DiscoDrive is effective both as a training resource,\nenabling DialoGPT-Medium and T5-Base to match or exceed KVRET-trained models on\nthe MultiWOZ 2.2 and Schema-Guided Dialogue (SGD) relevant test sets (BLEU-4\nimprovements of 0.26 to 0.61; METEOR +2.10; ROUGE-L +3.48; BERTScore F1\nimprovements of 1.35 to 3.48), and as a data augmentation resource in\nlow-resource scenarios, delivering additional gains of up to BLEU-4 +0.38,\nMETEOR +1.95, ROUGE-L +2.87, and BERTScore F1 +4.00 when combined with 10\npercent of KVRET. Human evaluations further confirm that dialogs sampled from\nDiscoDrive are rated higher than KVRET's human-collected dialogs in naturalness\n(3.8 vs 3.6) and coherence (4.1 vs 4.0), and are perceived as more\ncontext-appropriate than leading post-hoc methods (such as LARD), without\ncompromising clarity. DiscoDrive fills a critical gap in existing resources and\nserves as a versatile corpus for both training and augmenting conversational\nAI, enabling robust handling of real-world, disfluent in-car interactions.", "AI": {"tldr": "DiscoDrive 是一个包含 3500 个多轮对话的合成语料库，专注于汽车领域的对话，包含真实对话中的自发性不流利特征，提高了训练和数据增强的效果，提升了自然性和连贯性。", "motivation": "为解决现有数据集未能捕捉到驾驶员与AI对话中的真实不流利特征，引入了DiscoDrive，以改善基于对话的AI在车载场景中的表现。", "method": "采用两阶段、提示驱动的管道，在合成过程中动态集成不流利特征，生成了DiscoDrive语料库。", "result": "DiscoDrive作为训练资源及数据增强资源在低资源场景下的表现优越，提升了模型的多个评价指标，并且在人类评估中，表现出更高的自然性和连贯性。", "conclusion": "DiscoDrive填补了现有资源中的空白，为训练和增强车载对话AI提供了一个多功能的语料库，能够处理更真实的车内对话情境。"}}
{"id": "2507.19754", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.19754", "abs": "https://arxiv.org/abs/2507.19754", "authors": ["Seunghun Lee", "Jiwan Seo", "Minwoo Choi", "Kiljoon Han", "Jaehoon Jeong", "Zane Durante", "Ehsan Adeli", "Sang Hyun Park", "Sunghoon Im"], "title": "Latest Object Memory Management for Temporally Consistent Video Instance Segmentation", "comment": "ICCV 2025. Code: https://github.com/Seung-Hun-Lee/LOMM", "summary": "In this paper, we present Latest Object Memory Management (LOMM) for\ntemporally consistent video instance segmentation that significantly improves\nlong-term instance tracking. At the core of our method is Latest Object Memory\n(LOM), which robustly tracks and continuously updates the latest states of\nobjects by explicitly modeling their presence in each frame. This enables\nconsistent tracking and accurate identity management across frames, enhancing\nboth performance and reliability through the VIS process. Moreover, we\nintroduce Decoupled Object Association (DOA), a strategy that separately\nhandles newly appearing and already existing objects. By leveraging our memory\nsystem, DOA accurately assigns object indices, improving matching accuracy and\nensuring stable identity consistency, even in dynamic scenes where objects\nfrequently appear and disappear. Extensive experiments and ablation studies\ndemonstrate the superiority of our method over traditional approaches, setting\na new benchmark in VIS. Notably, our LOMM achieves state-of-the-art AP score of\n54.0 on YouTube-VIS 2022, a dataset known for its challenging long videos.\nProject page: https://seung-hun-lee.github.io/projects/LOMM/", "AI": {"tldr": "本文提出了一种名为Latest Object Memory Management (LOMM)的方法，显著改善了视频实例分割中的长时间一致性问题，通过Latest Object Memory (LOM)和Decoupled Object Association (DOA)策略，提高了匹配精度和身份一致性，在YouTube-VIS 2022数据集上达到了54.0的AP得分。", "motivation": "本文旨在解决视频实例分割中的长时间一致性问题，通过改进长时间实例跟踪来提高性能和可靠性。", "method": "Latest Object Memory Management (LOMM)采用Latest Object Memory (LOM)来处理长时间实例跟踪，通过明确建模每个帧中的对象存在情况，不断更新对象的最新状态以实现一致的跟踪和准确的身份管理。同时引入了Decoupled Object Association (DOA)策略，分别处理新出现和已存在的对象，以提高匹配精度和身份的一致性。", "result": "广泛的实验和消融研究证明，所提出的方法优于传统方法，达到了新的基准。在YouTube-VIS 2022数据集上，其AP得分为54.0。", "conclusion": "该研究证明了Latest Object Memory Management (LOMM)能显著改进长时间实例跟踪，并在视频实例分割领域取得了最优结果。"}}
{"id": "2507.19869", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.19869", "abs": "https://arxiv.org/abs/2507.19869", "authors": ["Danil Fokin", "Monika Płużyczka", "Grigory Golovin"], "title": "The Polish Vocabulary Size Test: A Novel Adaptive Test for Receptive Vocabulary Assessment", "comment": null, "summary": "We present the Polish Vocabulary Size Test (PVST), a novel tool for assessing\nthe receptive vocabulary size of both native and non-native Polish speakers.\nBased on Item Response Theory and Computerized Adaptive Testing, PVST\ndynamically adjusts to each test-taker's proficiency level, ensuring high\naccuracy while keeping the test duration short. To validate the test, a pilot\nstudy was conducted with 1.475 participants. Native Polish speakers\ndemonstrated significantly larger vocabularies compared to non-native speakers.\nFor native speakers, vocabulary size showed a strong positive correlation with\nage. The PVST is available online at myvocab.info/pl.", "AI": {"tldr": "开发并验证了一种基于计算机自适应测试技术的波兰语词汇量评估工具-PVST。", "motivation": "当前缺乏一种能够准确、高效地评估波兰语词汇量的工具，特别是在母语者和非母语者之间进行比较时。因此，研究开发了波兰词汇量测试（PVST）以填补这一空白。", "method": "研究采用了项目反应理论和计算机自适应测试技术开发测试工具，并进行了一项包含1475名参与者的初步研究来验证测试的有效性。", "result": "研究开发了波兰词汇量测试（PVST），一项用于评估波兰语母语者和非母语者的接受词汇量的新工具。该测试基于项目反应理论和计算机自适应测试技术，可以根据每个测试者的水平动态调整难度，既保证了测试的准确性，又缩短了测试时间。通过一项包含1475名参与者的初步研究验证了该测试的有效性。研究表明，母语者相比非母语者拥有更大的词汇量，并且对于母语者而言，词汇量随年龄增长而增加。PVST在线测试地址为myvocab.info/pl。", "conclusion": "该研究成功开发了一个能够高效且准确评估波兰语词汇量的工具，对于语言教育和研究有重要应用价值。"}}
{"id": "2507.19770", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.19770", "abs": "https://arxiv.org/abs/2507.19770", "authors": ["Jiaxin Liu", "Qichao Ying", "Zhenxing Qian", "Sheng Li", "Runqi Zhang", "Jian Liu", "Xinpeng Zhang"], "title": "MoFRR: Mixture of Diffusion Models for Face Retouching Restoration", "comment": null, "summary": "The widespread use of face retouching on social media platforms raises\nconcerns about the authenticity of face images. While existing methods focus on\ndetecting face retouching, how to accurately recover the original faces from\nthe retouched ones has yet to be answered. This paper introduces Face\nRetouching Restoration (FRR), a novel computer vision task aimed at restoring\noriginal faces from their retouched counterparts. FRR differs from traditional\nimage restoration tasks by addressing the complex retouching operations with\nvarious types and degrees, which focuses more on the restoration of the\nlow-frequency information of the faces. To tackle this challenge, we propose\nMoFRR, Mixture of Diffusion Models for FRR. Inspired by DeepSeek's expert\nisolation strategy, the MoFRR uses sparse activation of specialized experts\nhandling distinct retouching types and the engagement of a shared expert\ndealing with universal retouching traces. Each specialized expert follows a\ndual-branch structure with a DDIM-based low-frequency branch guided by an\nIterative Distortion Evaluation Module (IDEM) and a Cross-Attention-based\nHigh-Frequency branch (HFCAM) for detail refinement. Extensive experiments on a\nnewly constructed face retouching dataset, RetouchingFFHQ++, demonstrate the\neffectiveness of MoFRR for FRR.", "AI": {"tldr": "The paper introduces FRR and MoFRR to address the challenge of restoring original faces from retouched ones on social media images, using specialized experts and a dual-branch structure.", "motivation": "Existing methods focus on detecting face retouching, but there is a lack of methods to accurately recover the original faces from retouched ones, which motivates this research.", "method": "Face Retouching Restoration (FRR) is introduced, focusing on restoring original faces from retouched ones. The proposed solution, MoFRR, uses a mixture of diffusion models, with specialized experts for distinct retouching types and a shared expert for universal retouching, using a dual-branch structure.", "result": "Experiments on RetouchingFFHQ++ show the effectiveness of MoFRR in FRR.", "conclusion": "The paper contributes to the field by proposing a novel task and solution for restoring original faces from retouched images, demonstrating its effectiveness on a new dataset."}}
{"id": "2507.19885", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.19885", "abs": "https://arxiv.org/abs/2507.19885", "authors": ["Cesar Augusto Madid Truyts", "Amanda Gomes Rabelo", "Gabriel Mesquita de Souza", "Daniel Scaldaferri Lages", "Adriano Jose Pereira", "Uri Adrian Prync Flato", "Eduardo Pontes dos Reis", "Joaquim Edson Vieira", "Paulo Sergio Panse Silveira", "Edson Amaro Junior"], "title": "Zero-shot Performance of Generative AI in Brazilian Portuguese Medical Exam", "comment": null, "summary": "Artificial intelligence (AI) has shown the potential to revolutionize\nhealthcare by improving diagnostic accuracy, optimizing workflows, and\npersonalizing treatment plans. Large Language Models (LLMs) and Multimodal\nLarge Language Models (MLLMs) have achieved notable advancements in natural\nlanguage processing and medical applications. However, the evaluation of these\nmodels has focused predominantly on the English language, leading to potential\nbiases in their performance across different languages.\n  This study investigates the capability of six LLMs (GPT-4.0 Turbo,\nLLaMA-3-8B, LLaMA-3-70B, Mixtral 8x7B Instruct, Titan Text G1-Express, and\nCommand R+) and four MLLMs (Claude-3.5-Sonnet, Claude-3-Opus, Claude-3-Sonnet,\nand Claude-3-Haiku) to answer questions written in Brazilian spoken portuguese\nfrom the medical residency entrance exam of the Hospital das Cl\\'inicas da\nFaculdade de Medicina da Universidade de S\\~ao Paulo (HCFMUSP) - the largest\nhealth complex in South America. The performance of the models was benchmarked\nagainst human candidates, analyzing accuracy, processing time, and coherence of\nthe generated explanations.\n  The results show that while some models, particularly Claude-3.5-Sonnet and\nClaude-3-Opus, achieved accuracy levels comparable to human candidates,\nperformance gaps persist, particularly in multimodal questions requiring image\ninterpretation. Furthermore, the study highlights language disparities,\nemphasizing the need for further fine-tuning and data set augmentation for\nnon-English medical AI applications.\n  Our findings reinforce the importance of evaluating generative AI in various\nlinguistic and clinical settings to ensure a fair and reliable deployment in\nhealthcare. Future research should explore improved training methodologies,\nimproved multimodal reasoning, and real-world clinical integration of AI-driven\nmedical assistance.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2507.19773", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.19773", "abs": "https://arxiv.org/abs/2507.19773", "authors": ["Jeongwoo Shin", "Inseo Lee", "Junho Lee", "Joonseok Lee"], "title": "Self-Guided Masked Autoencoder", "comment": null, "summary": "Masked Autoencoder (MAE) is a self-supervised approach for representation\nlearning, widely applicable to a variety of downstream tasks in computer\nvision. In spite of its success, it is still not fully uncovered what and how\nMAE exactly learns. In this paper, with an in-depth analysis, we discover that\nMAE intrinsically learns pattern-based patch-level clustering from surprisingly\nearly stages of pretraining. Upon this understanding, we propose self-guided\nmasked autoencoder, which internally generates informed mask by utilizing its\nprogress in patch clustering, substituting the naive random masking of the\nvanilla MAE. Our approach significantly boosts its learning process without\nrelying on any external models or supplementary information, keeping the\nbenefit of self-supervised nature of MAE intact. Comprehensive experiments on\nvarious downstream tasks verify the effectiveness of the proposed method.", "AI": {"tldr": "研究揭示掩码自编码器学习模式，并提出了一种自引导掩码自编码器方法，提高学习效率的同时保持了自监督特性。", "motivation": "尽管掩码自编码器（Masked Autoencoder，简称MAE）在计算机视觉的多个下游任务上取得了成功，但其内部学习机制尚未被完全理解。本文旨在通过深入分析揭示MAE的学习机制，并在其基础上提出改进方法。", "method": "文章提出了一种名为自引导掩码自编码器（self-guided masked autoencoder）的方法。该方法在预训练的早期阶段内部生成基于其在块聚类中的进展的智能掩码，用以替代传统的随机掩码策略。", "result": "实验结果表明，该方法在多项下游任务上表现出了显著的性能提升，且无需依赖任何外部模型或额外信息，保持了自监督学习的本质。", "conclusion": "研究表明，MAE在预训练的早期阶段就学会了基于模式的块级别聚类。通过引入自引导机制，这种方法巩固了MAE的学习效果，且无需引入外部模型或额外的数据信息。"}}
{"id": "2507.19899", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.19899", "abs": "https://arxiv.org/abs/2507.19899", "authors": ["Prajval Bolegave", "Pushpak Bhattacharya"], "title": "A Gold Standard Dataset and Evaluation Framework for Depression Detection and Explanation in Social Media using LLMs", "comment": null, "summary": "Early detection of depression from online social media posts holds promise\nfor providing timely mental health interventions. In this work, we present a\nhigh-quality, expert-annotated dataset of 1,017 social media posts labeled with\ndepressive spans and mapped to 12 depression symptom categories. Unlike prior\ndatasets that primarily offer coarse post-level labels\n\\cite{cohan-etal-2018-smhd}, our dataset enables fine-grained evaluation of\nboth model predictions and generated explanations.\n  We develop an evaluation framework that leverages this clinically grounded\ndataset to assess the faithfulness and quality of natural language explanations\ngenerated by large language models (LLMs). Through carefully designed prompting\nstrategies, including zero-shot and few-shot approaches with domain-adapted\nexamples, we evaluate state-of-the-art proprietary LLMs including GPT-4.1,\nGemini 2.5 Pro, and Claude 3.7 Sonnet.\n  Our comprehensive empirical analysis reveals significant differences in how\nthese models perform on clinical explanation tasks, with zero-shot and few-shot\nprompting. Our findings underscore the value of human expertise in guiding LLM\nbehavior and offer a step toward safer, more transparent AI systems for\npsychological well-being.", "AI": {"tldr": "研究开发了一个高质量的数据集用于评估语言模型在抑郁症症状识别中的表现，并采用定制化的提示策略对比了几种先进模型的表现。结果表明，先进模型在心理健康任务中表现出显著差异，强调了人类专业知识指导模型行为的重要性。", "motivation": "本研究旨在通过分析社交媒体帖子实现抑郁症的早期检测，以便及时提供心理健康干预。", "method": "构建了一个高质量、专家标注的数据集，包含1,017条社交媒体帖子，这些帖子被标记了抑郁片段并映射到12个抑郁症状类别。该研究评估了先进语言模型（LLM）生成的自然语言解释的可靠性和质量，采用了零样本和少样本提示策略。", "result": "通过综合实证分析，不同模型在心理健康的解释任务中的表现存在显著差异。", "conclusion": "研究结果强调了人类专业知识在指导LLM行为中的价值，并为构建更安全、透明的人工智能系统以促进心理健康提供了一个步骤。"}}
