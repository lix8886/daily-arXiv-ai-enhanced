{"id": "2508.06495", "categories": ["cs.CL", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2508.06495", "abs": "https://arxiv.org/abs/2508.06495", "authors": ["Juliana Resplande Sant'anna Gomes", "Arlindo Rodrigues Galvão Filho"], "title": "Semi-automated Fact-checking in Portuguese: Corpora Enrichment using Retrieval with Claim extraction", "comment": "Master Thesis in Computer Science at Federal University on Goias\n  (UFG). Written in Portuguese", "summary": "The accelerated dissemination of disinformation often outpaces the capacity\nfor manual fact-checking, highlighting the urgent need for Semi-Automated\nFact-Checking (SAFC) systems. Within the Portuguese language context, there is\na noted scarcity of publicly available datasets that integrate external\nevidence, an essential component for developing robust AFC systems, as many\nexisting resources focus solely on classification based on intrinsic text\nfeatures. This dissertation addresses this gap by developing, applying, and\nanalyzing a methodology to enrich Portuguese news corpora (Fake.Br, COVID19.BR,\nMuMiN-PT) with external evidence. The approach simulates a user's verification\nprocess, employing Large Language Models (LLMs, specifically Gemini 1.5 Flash)\nto extract the main claim from texts and search engine APIs (Google Search API,\nGoogle FactCheck Claims Search API) to retrieve relevant external documents\n(evidence). Additionally, a data validation and preprocessing framework,\nincluding near-duplicate detection, is introduced to enhance the quality of the\nbase corpora.", "AI": {"tldr": "论文提出了一个在葡萄牙语背景下使用大型语言模型和搜索引擎API提取外部证据来增强新闻语料库的方法，以解决现有资源仅基于文本特征分类的问题，从而推动半自动事实检查系统的发展。", "motivation": "因为手动事实核查的能力赶不上虚假信息的快速传播，特别在葡萄牙语环境中缺乏整合外部证据的公开数据集，需要开发有效的半自动事实核查系统。", "method": "采用大型语言模型(Gemini 1.5 Flash)从文本中提取主要声明，同时使用搜索引擎API(如Google Search API)检索相关的外部文档作为证据，并引入一个数据验证和预处理框架来提高基础语料库的质量。", "result": "本研究开发了一种方法论，用于通过模拟用户的验证过程来增强葡萄牙语新闻语料库(Fake.Br, COVID19.BR, MuMiN-PT)，整合外部证据以支持更加强大的事实核查系统开发。", "conclusion": "研究开发的方法有效地解决了葡萄牙语新闻语料库缺乏外部证据这一问题，为建立健全的半自动事实核查系统奠定了基础。"}}
{"id": "2508.06504", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06504", "abs": "https://arxiv.org/abs/2508.06504", "authors": ["Yao Ge", "Sudeshna Das", "Yuting Guo", "Abeed Sarker"], "title": "Retrieval augmented generation based dynamic prompting for few-shot biomedical named entity recognition using large language models", "comment": "31 pages, 4 figures, 15 tables", "summary": "Biomedical named entity recognition (NER) is a high-utility natural language\nprocessing (NLP) task, and large language models (LLMs) show promise\nparticularly in few-shot settings (i.e., limited training data). In this\narticle, we address the performance challenges of LLMs for few-shot biomedical\nNER by investigating a dynamic prompting strategy involving retrieval-augmented\ngeneration (RAG). In our approach, the annotated in-context learning examples\nare selected based on their similarities with the input texts, and the prompt\nis dynamically updated for each instance during inference. We implemented and\noptimized static and dynamic prompt engineering techniques and evaluated them\non five biomedical NER datasets. Static prompting with structured components\nincreased average F1-scores by 12% for GPT-4, and 11% for GPT-3.5 and LLaMA\n3-70B, relative to basic static prompting. Dynamic prompting further improved\nperformance, with TF-IDF and SBERT retrieval methods yielding the best results,\nimproving average F1-scores by 7.3% and 5.6% in 5-shot and 10-shot settings,\nrespectively. These findings highlight the utility of contextually adaptive\nprompts via RAG for biomedical NER.", "AI": {"tldr": "本文研究通过检索增强生成的动态提示策略来提高大型语言模型在少样本生物医学命名实体识别中的性能。", "motivation": "生物医学命名实体识别在自然语言处理中是一项非常有用的NLP任务，大型语言模型在少样本场景中显示出潜力。然而，这些模型在少样本生物医学命名实体识别中的性能仍有待提高。", "method": "通过研究涉及检索增强生成（RAG）的动态提示策略来解决LLM在少样本生物医学命名实体识别中的性能挑战。在该方法中，基于与输入文本的相似度选择标注的上下文学习示例，并在推理过程中为每个实例动态更新提示。实现了静态和动态提示工程技术，并在五个生物医学命名实体识别数据集上进行了评估。", "result": "结构化组件的静态提示法使GPT-4和GPT-3.5及LLaMA 3-70B的平均F1分数分别提高了12%和11%。动态提示法进一步提升了性能，TF-IDF和SBERT检索方法分别在5-shot和10-shot设置中使平均F1分数提高了7.3%和5.6%。", "conclusion": "这些发现突显了通过RAG实现的上下文适应性提示在生物医学命名实体识别中的效用。"}}
{"id": "2508.06524", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.06524", "abs": "https://arxiv.org/abs/2508.06524", "authors": ["Lei Jiang", "Fan Chen"], "title": "CarbonScaling: Extending Neural Scaling Laws for Carbon Footprint in Large Language Models", "comment": "8 pages", "summary": "Neural scaling laws have driven the development of increasingly large\nlanguage models (LLMs) by linking accuracy improvements to growth in parameter\ncount, dataset size, and compute. However, these laws overlook the carbon\nemissions that scale exponentially with LLM size. This paper presents\n\\textit{CarbonScaling}, an analytical framework that extends neural scaling\nlaws to incorporate both operational and embodied carbon in LLM training. By\nintegrating models for neural scaling, GPU hardware evolution, parallelism\noptimization, and carbon estimation, \\textit{CarbonScaling} quantitatively\nconnects model accuracy to carbon footprint. Results show that while a\npower-law relationship between accuracy and carbon holds, real-world\ninefficiencies significantly increase the scaling factor. Hardware technology\nscaling reduces carbon emissions for small to mid-sized models, but offers\ndiminishing returns for extremely large LLMs due to communication overhead and\nunderutilized GPUs. Training optimizations-especially aggressive critical batch\nsize scaling-help alleviate this inefficiency. \\textit{CarbonScaling} offers\nkey insights for training more sustainable and carbon-efficient LLMs.", "AI": {"tldr": "本文提出了CarbonScaling框架，该框架将神经缩放定律应用于考虑LLM训练中的碳排放，量化了模型准确性与碳足迹之间的关系。", "motivation": "鉴于当前的神经缩放定律仅关注准确性改进与参数量、数据集大小和计算能力的增长之间的关联，但忽略了碳排放的显著增加问题，因此作者提出该框架以填补这一研究空白。", "method": "本论文提出了一种名为CarbonScaling的分析框架，该框架扩展了神经缩放定律，将操作碳和LLM训练中的物质化碳纳入考虑。通过整合神经缩放模型、GPU硬件进化、并行优化及碳估算模型，CarbonScaling定量地将模型准确性与碳足迹联系起来。", "result": "研究表明，尽管准确性与碳之间存在幂律关系，但实际存在的低效会显著增加碳排放。对于极大规模的LLM，硬件技术缩放减少碳排放的效果逐渐减弱，主要是由于通信开销和未充分利用的GPU。", "conclusion": "CarbonScaling框架为训练更加可持续和碳效率更高的LLM提供了关键洞见。"}}
{"id": "2508.06533", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06533", "abs": "https://arxiv.org/abs/2508.06533", "authors": ["Aamod Thakur", "Ajay Nagpal", "Atharva Savarkar", "Kundeshwar Pundalik", "Siddhesh Dosi", "Piyush Sawarkar", "Viraj Thakur", "Rohit Saluja", "Maunendra Sankar Desarkar", "Ganesh Ramakrishnan"], "title": "The Art of Breaking Words: Rethinking Multilingual Tokenizer Design", "comment": null, "summary": "While model architecture and training objectives are well-studied,\ntokenization, particularly in multilingual contexts, remains a relatively\nneglected aspect of Large Language Model (LLM) development. Existing tokenizers\noften exhibit high token-to-word ratios, inefficient use of context length, and\nslower inference. We present a systematic study that links vocabulary size,\npre-tokenization rules, and training-corpus composition to both token-to-word\nefficiency and model quality. To ground our analysis in a linguistically\ndiverse context, we conduct extensive experiments on Indic scripts, which\npresent unique challenges due to their high script diversity and orthographic\ncomplexity. Drawing on the insights from these analyses, we propose a novel\nalgorithm for data composition that balances multilingual data for tokenizer\ntraining. Our observations on pretokenization strategies significantly improve\nmodel performance, and our data composition algorithm reduces the average\ntoken-to-word ratio by approximately 6% with respect to the conventional data\nrandomization approach. Our tokenizer achieves more than 40% improvement on\naverage token-to-word ratio against stateof-the-art multilingual Indic models.\nThis improvement yields measurable gains in both model performance and\ninference speed. This highlights tokenization alongside architecture and\ntraining objectives as a critical lever for building efficient, scalable\nmultilingual LLMs", "AI": {"tldr": "本研究侧重于多语言语境内的分词技术，提出了一种新的数据组合算法来平衡用于训练分词器的多语言数据，改善了分词效率和模型性能，降低了推理时间。", "motivation": "由于现有的分词器往往会导致较高的分词到词汇比率，不高效的上下文长度使用以及较慢的推理速度，本论文旨在研究分词化，特别是多语言背景下的分词化，作为大规模语言模型开发中的一个较未被重视的方面。", "method": "本论文提出了一种新的数据组合算法，该算法在训练分词器时平衡多语言数据，并探讨了词汇大小、预分词规则和训练语料库组成如何影响分词到词汇的效率和模型质量。", "result": "经过研究，论文提出的数据组合算法与传统随机化方法相比，将平均分词到词汇比率降低了约6%，并在多语言印度语模型中平均分词到词汇比率方面实现了40%以上的改进。", "conclusion": "研究结果表明，分词化是一种有效的手段，能够改进训练目标和模型架构，用于构建高效、可扩展的多语言大规模语言模型。"}}
{"id": "2508.06496", "categories": ["cs.CV", "cs.MA"], "pdf": "https://arxiv.org/pdf/2508.06496", "abs": "https://arxiv.org/abs/2508.06496", "authors": ["Rakesh Raj Madavan", "Akshat Kaimal", "Hashim Faisal", "Chandrakala S"], "title": "Med-GRIM: Enhanced Zero-Shot Medical VQA using prompt-embedded Multimodal Graph RAG", "comment": null, "summary": "An ensemble of trained multimodal encoders and vision-language models (VLMs)\nhas become a standard approach for visual question answering (VQA) tasks.\nHowever, such models often fail to produce responses with the detailed\nprecision necessary for complex, domain-specific applications such as medical\nVQA. Our representation model, BIND: BLIVA Integrated with Dense Encoding,\nextends prior multimodal work by refining the joint embedding space through\ndense, query-token-based encodings inspired by contrastive pretraining\ntechniques. This refined encoder powers Med-GRIM, a model designed for medical\nVQA tasks that leverages graph-based retrieval and prompt engineering to\nintegrate domain-specific knowledge. Rather than relying on compute-heavy\nfine-tuning of vision and language models on specific datasets, Med-GRIM\napplies a low-compute, modular workflow with small language models (SLMs) for\nefficiency. Med-GRIM employs prompt-based retrieval to dynamically inject\nrelevant knowledge, ensuring both accuracy and robustness in its responses. By\nassigning distinct roles to each agent within the VQA system, Med-GRIM achieves\nlarge language model performance at a fraction of the computational cost.\nAdditionally, to support scalable research in zero-shot multimodal medical\napplications, we introduce DermaGraph, a novel Graph-RAG dataset comprising\ndiverse dermatological conditions. This dataset facilitates both multimodal and\nunimodal querying. The code and dataset are available at:\nhttps://github.com/Rakesh-123-cryp/Med-GRIM.git", "AI": {"tldr": "BIND通过改进联合嵌入空间，Med-GRIM采用小语言模型和基于提示的检索实现高效、准确的医学问答，同时提出了医学图像数据集DermaGraph。", "motivation": "现有的多模态模型在复杂的、特定领域应用如医疗问答中难以生成所需的详细而精确的回答，故提出改进模型。", "method": "BIND：一种扩展了先前多模态工作的表示模型，通过密集聚合查询令牌编码改进联合嵌入空间，受到对比预训练技术的启发。Med-GRIM：该模型专门针对医学问答任务，通过图形查询和提示工程整合领域特定知识。采用小语言模型实现低计算量且模块化的管线，利用基于提示的检索动态注入相关知识，确保其响应的准确性和鲁棒性。", "result": "Med-GRIM以远低于大型语言模型的计算成本实现高效性能，且DermaGraph数据集支撑了在零样本多模态医疗应用中的可扩展研究。", "conclusion": "新的模型和数据集为零样本多模态医疗应用的研究提供了支持。"}}
{"id": "2508.06548", "categories": ["cs.CL", "cs.AI", "cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2508.06548", "abs": "https://arxiv.org/abs/2508.06548", "authors": ["Zhanye Luo", "Yuefeng Han", "Xiufan Yu"], "title": "Factor Augmented Supervised Learning with Text Embeddings", "comment": null, "summary": "Large language models (LLMs) generate text embeddings from text data,\nproducing vector representations that capture the semantic meaning and\ncontextual relationships of words. However, the high dimensionality of these\nembeddings often impedes efficiency and drives up computational cost in\ndownstream tasks. To address this, we propose AutoEncoder-Augmented Learning\nwith Text (AEALT), a supervised, factor-augmented framework that incorporates\ndimension reduction directly into pre-trained LLM workflows. First, we extract\nembeddings from text documents; next, we pass them through a supervised\naugmented autoencoder to learn low-dimensional, task-relevant latent factors.\nBy modeling the nonlinear structure of complex embeddings, AEALT outperforms\nconventional deep-learning approaches that rely on raw embeddings. We validate\nits broad applicability with extensive experiments on classification, anomaly\ndetection, and prediction tasks using multiple real-world public datasets.\nNumerical results demonstrate that AEALT yields substantial gains over both\nvanilla embeddings and several standard dimension reduction methods.", "AI": {"tldr": "提出了一种新的框架AEALT，它直接在预训练语言模型中集成了维度减少，通过学习低维、任务相关的潜在因子来提高模型在不同类型任务上的性能。", "motivation": "大型语言模型生成的文本嵌入具有高维度，这阻碍了下游任务的效率并导致计算成本增加。为解决此问题，提出AEALT框架。", "method": "我们提出了一种名为AutoEncoder-Augmented Learning with Text (AEALT)的监督框架，该框架在预训练语言模型的流程中直接集成了维度减少。该方法先从文本文档中提取嵌入，然后通过监督增强的自编码器学习低维、任务相关的潜在因子。", "result": "实验验证了AEALT在分类、异常检测和预测等多种任务上的广泛适用性，结果表明AEALT相比原始嵌入和几种标准维度减少方法有显著优势。", "conclusion": "AEALT通过高效地使用监督增强的自编码器在任务相关维度上进行学习，在多种任务上展现出了优于传统方法的性能。"}}
{"id": "2508.06511", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06511", "abs": "https://arxiv.org/abs/2508.06511", "authors": ["He Feng", "Yongjia Ma", "Donglin Di", "Lei Fan", "Tonghua Su", "Xiangqian Wu"], "title": "DiTalker: A Unified DiT-based Framework for High-Quality and Speaking Styles Controllable Portrait Animation", "comment": null, "summary": "Portrait animation aims to synthesize talking videos from a static reference\nface, conditioned on audio and style frame cues (e.g., emotion and head poses),\nwhile ensuring precise lip synchronization and faithful reproduction of\nspeaking styles. Existing diffusion-based portrait animation methods primarily\nfocus on lip synchronization or static emotion transformation, often\noverlooking dynamic styles such as head movements. Moreover, most of these\nmethods rely on a dual U-Net architecture, which preserves identity consistency\nbut incurs additional computational overhead. To this end, we propose DiTalker,\na unified DiT-based framework for speaking style-controllable portrait\nanimation. We design a Style-Emotion Encoding Module that employs two separate\nbranches: a style branch extracting identity-specific style information (e.g.,\nhead poses and movements), and an emotion branch extracting identity-agnostic\nemotion features. We further introduce an Audio-Style Fusion Module that\ndecouples audio and speaking styles via two parallel cross-attention layers,\nusing these features to guide the animation process. To enhance the quality of\nresults, we adopt and modify two optimization constraints: one to improve lip\nsynchronization and the other to preserve fine-grained identity and background\ndetails. Extensive experiments demonstrate the superiority of DiTalker in terms\nof lip synchronization and speaking style controllability. Project Page:\nhttps://thenameishope.github.io/DiTalker/", "AI": {"tldr": "提出了一种新的框架DiTalker，用于生成说话风格可控的肖像动画，该框架能够处理动态风格，同时提高了唇同步效果，并在大量的实验中展示了优越性。", "motivation": "现有的基于扩散模型的肖像动画方法主要关注唇部同步或静态情绪转换，往往忽略了像头部移动这样的动态风格。此外，大多数这些方法依赖于双U-Net架构，这虽然保持了身份的一致性，但也带来了额外的计算开销。因此，我们提出了DiTalker，以解决上述问题。", "method": "我们提出了一种基于DiT的统一框架DiTalker，用于支持说话风格可控的肖像动画。该框架设计了一个风格情感编码模块，其中包括两个不同的分支——用于提取身份特定风格信息（如头部姿势和动作）的风格分支和用于提取身份无关情感特征的情感分支。此外，我们还引入了一个音频风格融合模块，通过两个并行的交叉注意力层来分离音频和说话风格，并使用这些特征引导动画过程。为了提高结果质量，我们采用了两种优化约束：一种用于提高唇同步效果，另一种用于保持精细的身份和背景细节。", "result": "大量的实验表明，DiTalker在唇部同步和说话风格可控性上都表现出色。", "conclusion": "总之，DiTalker提供了一种创新的方法来生成高质量且风格可控的肖像动画，极大地改善了唇同步效果和动态风格的再现。"}}
{"id": "2508.06583", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06583", "abs": "https://arxiv.org/abs/2508.06583", "authors": ["Ying Liu", "Can Li", "Ting Zhang", "Mei Wang", "Qiannan Zhu", "Jian Li", "Hua Huang"], "title": "Discerning minds or generic tutors? Evaluating instructional guidance capabilities in Socratic LLMs", "comment": null, "summary": "The conversational capabilities of large language models hold significant\npromise for enabling scalable and interactive tutoring. While prior research\nhas primarily examined their capacity for Socratic questioning, it often\noverlooks a critical dimension: adaptively guiding learners based on their\ncognitive states. This study shifts focus from mere question generation to the\nbroader instructional guidance capability. We ask: Can LLMs emulate expert\ntutors who dynamically adjust strategies in response to learners'\nunderstanding? To investigate this, we propose GuideEval, a benchmark grounded\nin authentic educational dialogues that evaluates pedagogical guidance through\na three-phase behavioral framework: (1) Perception, inferring learner states;\n(2) Orchestration, adapting instructional strategies; and (3) Elicitation,\nstimulating proper reflections. Empirical findings reveal that existing LLMs\nfrequently fail to provide effective adaptive scaffolding when learners exhibit\nconfusion or require redirection. Furthermore, we introduce a behavior-guided\nfinetuning strategy that leverages behavior-prompted instructional dialogues,\nsignificantly enhancing guidance performance. By shifting the focus from\nisolated content evaluation to learner-centered interaction, our work advocates\na more dialogic paradigm for evaluating Socratic LLMs.", "AI": {"tldr": "研究探讨了大型语言模型作为适应性导师的能力，强调了根据学习者的认知状态提供指导的重要性，并提出了一种新的评估基准GuideEval和一种提升性能的微调策略。", "motivation": "以往的研究主要集中在大型语言模型的苏格拉底提问能力上，而忽略了根据学习者的认知状态进行适应性引导这一关键方面。这项研究旨在探讨语言模型是否能像专家导师那样根据学习者的理解动态调整策略。", "method": "该研究提出了GuideEval基准，该基准基于真实的教育对话，通过三阶段行为框架评估教学指导能力：感知（推断学习者状态）、编排（调整教学策略）和激发（促进适当反思）。研究采用行为引导的微调策略，利用行为提示的教学对话，显著提高了指导性能。", "result": "实证结果表明，现有的大型语言模型在学习者表现出困惑或需要引导时，常常无法提供有效的适应性支持。通过引入基于行为的微调策略，显著增强了语言模型的指导性能。", "conclusion": "通过从单独的内容评估转向以学习者为中心的互动，这项工作推广了一种更对话式的评价苏格拉底语言模型的范式，强调了适应性教学指导的重要性。"}}
{"id": "2508.06515", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06515", "abs": "https://arxiv.org/abs/2508.06515", "authors": ["Minh Duc Chu", "Kshitij Pawar", "Zihao He", "Roxanna Sharifi", "Ross Sonnenblick", "Magdalayna Curry", "Laura D'Adamo", "Lindsay Young", "Stuart B Murray", "Kristina Lerman"], "title": "BigTokDetect: A Clinically-Informed Vision-Language Model Framework for Detecting Pro-Bigorexia Videos on TikTok", "comment": null, "summary": "Social media platforms increasingly struggle to detect harmful content that\npromotes muscle dysmorphic behaviors, particularly pro-bigorexia content that\ndisproportionately affects adolescent males. Unlike traditional eating disorder\ndetection focused on the \"thin ideal,\" pro-bigorexia material masquerades as\nlegitimate fitness content through complex multimodal combinations of visual\ndisplays, coded language, and motivational messaging that evade text-based\ndetection systems. We address this challenge by developing BigTokDetect, a\nclinically-informed detection framework for identifying pro-bigorexia content\non TikTok. We introduce BigTok, the first expert-annotated multimodal dataset\nof over 2,200 TikTok videos labeled by clinical psychologists and psychiatrists\nacross five primary categories spanning body image, nutrition, exercise,\nsupplements, and masculinity. Through a comprehensive evaluation of\nstate-of-the-art vision language models, we achieve 0.829% accuracy on primary\ncategory classification and 0.690% on subcategory detection via domain-specific\nfinetuning. Our ablation studies demonstrate that multimodal fusion improves\nperformance by 5-10% over text-only approaches, with video features providing\nthe most discriminative signals. These findings establish new benchmarks for\nmultimodal harmful content detection and provide both the computational tools\nand methodological framework needed for scalable content moderation in\nspecialized mental health domains.", "AI": {"tldr": "提出BigTokDetect框架和BigTok数据集，用于检测TikTok上影响青少年男性的大肌肉畸形症内容，借助多模态处理，实现高精度的类目分类。", "motivation": "由于现有的文本基础检测系统无法有效检测大肌肉畸形症内容，该研究旨在解决这个问题，这些内容常常伪装成合法的健身内容，主要影响青少年男性。", "method": "提出了一种名为BigTokDetect的临床知情检测框架，用于识别TikTok上的大肌肉畸形症内容。该框架使用了一个称为BigTok的独特专家注释的多模态数据集，包括超过2,200个TikTok视频，这些视频由临床心理学家和精神病学家根据五个主要类别（包括身体形象、营养、锻炼、补充剂和男子气概）进行标注。", "result": "通过全面评估最先进的视觉语言模型，实现了主要类目分类82.9%的精度以及子类目检测69.0%的精度。实验还表明，多模态融合比文本方法提高了5-10%的性能，尤其是视频特征，提供了最多的判别信号。", "conclusion": "研究结果建立了一个新的多模态有害内容检测基准，并提供了计算工具和方法论框架，以实现具有专门心理健康领域针对性的可扩展内容审核。"}}
{"id": "2508.06595", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.06595", "abs": "https://arxiv.org/abs/2508.06595", "authors": ["Xiaoyuan Zhu", "Muru Zhang", "Ollie Liu", "Robin Jia", "Willie Neiswanger"], "title": "LLM Unlearning Without an Expert Curated Dataset", "comment": null, "summary": "Modern large language models often encode sensitive, harmful, or copyrighted\nknowledge, raising the need for post-hoc unlearning-the ability to remove\nspecific domains of knowledge from a model without full retraining. A major\nbottleneck in current unlearning pipelines is constructing effective forget\nsets-datasets that approximate the target domain and guide the model to forget\nit. In this work, we introduce a scalable, automated approach to generate\nhigh-quality forget sets using language models themselves. Our method\nsynthesizes textbook-style data through a structured prompting pipeline,\nrequiring only a domain name as input. Through experiments on unlearning\nbiosecurity, cybersecurity, and Harry Potter novels, we show that our synthetic\ndatasets consistently outperform the baseline synthetic alternatives and are\ncomparable to the expert-curated ones. Additionally, ablation studies reveal\nthat the multi-step generation pipeline significantly boosts data diversity,\nwhich in turn improves unlearning utility. Overall, our findings suggest that\nsynthetic datasets offer a promising path toward practical, scalable unlearning\nfor a wide range of emerging domains without the need for manual intervention.\nWe release our code and dataset at\nhttps://github.com/xyzhu123/Synthetic_Textbook.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2508.06517", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06517", "abs": "https://arxiv.org/abs/2508.06517", "authors": ["Haoran Xi", "Chen Liu", "Xiaolin Li"], "title": "Frequency Prior Guided Matching: A Data Augmentation Approach for Generalizable Semi-Supervised Polyp Segmentation", "comment": "19 pages, 8 figures, 6 tables", "summary": "Automated polyp segmentation is essential for early diagnosis of colorectal\ncancer, yet developing robust models remains challenging due to limited\nannotated data and significant performance degradation under domain shift.\nAlthough semi-supervised learning (SSL) reduces annotation requirements,\nexisting methods rely on generic augmentations that ignore polyp-specific\nstructural properties, resulting in poor generalization to new imaging centers\nand devices. To address this, we introduce Frequency Prior Guided Matching\n(FPGM), a novel augmentation framework built on a key discovery: polyp edges\nexhibit a remarkably consistent frequency signature across diverse datasets.\nFPGM leverages this intrinsic regularity in a two-stage process. It first\nlearns a domain-invariant frequency prior from the edge regions of labeled\npolyps. Then, it performs principled spectral perturbations on unlabeled\nimages, aligning their amplitude spectra with this learned prior while\npreserving phase information to maintain structural integrity. This targeted\nalignment normalizes domain-specific textural variations, thereby compelling\nthe model to learn the underlying, generalizable anatomical structure.\nValidated on six public datasets, FPGM establishes a new state-of-the-art\nagainst ten competing methods. It demonstrates exceptional zero-shot\ngeneralization capabilities, achieving over 10% absolute gain in Dice score in\ndata-scarce scenarios. By significantly enhancing cross-domain robustness, FPGM\npresents a powerful solution for clinically deployable polyp segmentation under\nlimited supervision.", "AI": {"tldr": "FPGM improves generalization and robustness in polyp segmentation by leveraging the consistent frequency signature of polyp edges, outperforming existing methods in a semi-supervised learning setting with limited data and achieving state-of-the-art results.", "motivation": "The motivation is to improve generalization in automated polyp segmentation across different imaging centers and devices, leveraging semi-supervised learning while addressing the issues of limited annotated data and poor generalization under domain shift.", "method": "Frequency Prior Guided Matching (FPGM) is introduced as a novel augmentation framework for polyp segmentation. It relies on the discovery that polyp edges have a consistent frequency signature. FPGM operates in two stages: firstly, it learns a domain-invariant frequency prior from labeled polyp edges. Secondly, it applies spectral perturbations to unlabeled images to align their amplitude spectra with the learned prior, while preserving the phase to maintain structural integrity.", "result": "FPGM establishes a new state-of-the-art in polyp segmentation on six public datasets, outperforming ten competing methods. It shows exceptional zero-shot generalization capabilities and achieves over 10% absolute gain in Dice score in scenarios with limited data.", "conclusion": "FPGM provides enhanced cross-domain robustness for polyp segmentation, making it a suitable solution for clinical deployment under limited supervision."}}
{"id": "2508.06600", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2508.06600", "abs": "https://arxiv.org/abs/2508.06600", "authors": ["Zijian Chen", "Xueguang Ma", "Shengyao Zhuang", "Ping Nie", "Kai Zou", "Andrew Liu", "Joshua Green", "Kshama Patel", "Ruoxi Meng", "Mingyi Su", "Sahel Sharifymoghaddam", "Yanxi Li", "Haoran Hong", "Xinyu Shi", "Xuye Liu", "Nandan Thakur", "Crystina Zhang", "Luyu Gao", "Wenhu Chen", "Jimmy Lin"], "title": "BrowseComp-Plus: A More Fair and Transparent Evaluation Benchmark of Deep-Research Agent", "comment": null, "summary": "Deep-Research agents, which integrate large language models (LLMs) with\nsearch tools, have shown success in improving the effectiveness of handling\ncomplex queries that require iterative search planning and reasoning over\nsearch results. Evaluations on current benchmarks like BrowseComp relies on\nblack-box live web search APIs, have notable limitations in (1) fairness:\ndynamic and opaque web APIs hinder fair comparisons and reproducibility of deep\nresearch methods; (2) transparency: lack of control over the document corpus\nmakes it difficult to isolate retriever contributions. In other words, the\ncurrent evaluations may compare a complete deep research system at a given\ntime, but they do not foster well-controlled experiments to provide insights\ninto the capability of underlying deep research LLMs. To address these\nchallenges, we introduce BrowseComp-Plus, a benchmark derived from BrowseComp,\nemploying a fixed, carefully curated corpus. Each query in BrowseComp-Plus\nincludes human-verified supporting documents and mined challenging negatives,\nenabling controlled experimentation. The benchmark is shown to be effective in\ndistinguishing the performance of deep research systems. For instance, the\nopen-source model Search-R1, when paired with the BM25 retriever, achieves\n3.86% accuracy, whereas the GPT-5 achieves 55.9%. Integrating the GPT-5 with\nthe Qwen3-Embedding-8B retriever further enhances its accuracy to 70.1% with\nfewer search calls. This benchmark allows comprehensive evaluation and\ndisentangled analysis of deep research agents and retrieval methods, fostering\ninsights into retrieval effectiveness, citation accuracy, and context\nengineering in Deep-Research system.", "AI": {"tldr": "该论文介绍了一个新的基准测试BrowseComp-Plus，它利用静态且仔细策划的语料库，以解决先前基准测试的局限性，提高对深度研究系统性能和组成的分析能力。", "motivation": "当前的基准测试存在公平性和透明度的局限性，这阻碍了对深度研究方法公平比较和其可重复性。因此，该研究旨在设计一个更有效的基准测试，提供对底层深度研究LLM能力的深入见解。", "method": "该论文提出了一个名为BrowseComp-Plus的新基准，它是从BrowseComp衍生而来的，使用了一个静态且仔细策划的语料库，以解决以前基准测试的局限性。这个新基准包含人工验证的支持文档和挖掘具有挑战性的负样本，从而支持更受控的实验。", "result": "实验结果显示，BrowseComp-Plus能够有效地区分深度研究系统的性能。例如，开源模型Search-R1与BM25检索器结合时，准确率仅为3.86%，而GPT-5的准确率为55.9%，与Qwen3-Embedding-8B检索器相结合时，准确率提升至70.1%，同时减少了搜索调用次数。", "conclusion": "BrowseComp-Plus基准测试证明了它可以更好地控制实验，从而提供深入的见解，包括检索有效性、引文准确性以及深度研究系统中的上下文工程。这为评估和分析深研究代理和检索方法提供了新的途径。"}}
{"id": "2508.06525", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06525", "abs": "https://arxiv.org/abs/2508.06525", "authors": ["Guoyuan An", "JaeYoon Kim", "SungEui Yoon"], "title": "Large Language Models Facilitate Vision Reflection in Image Classification", "comment": null, "summary": "This paper presents several novel findings on the explainability of vision\nreflection in large multimodal models (LMMs). First, we show that prompting an\nLMM to verify the prediction of a specialized vision model can improve\nrecognition accuracy, even on benchmarks like ImageNet, despite prior evidence\nthat LMMs typically underperform dedicated vision encoders. Second, we analyze\nthe internal behavior of vision reflection and find that the vision-language\nconnector maps visual features into explicit textual concepts, allowing the\nlanguage model to reason about prediction plausibility using commonsense\nknowledge. We further observe that replacing a large number of vision tokens\nwith only a few text tokens still enables LLaVA to generate similar answers,\nsuggesting that LMMs may rely primarily on a compact set of distilled textual\nrepresentations rather than raw vision features. Third, we show that a\ntraining-free connector can enhance LMM performance in fine-grained recognition\ntasks, without extensive feature-alignment training. Together, these findings\noffer new insights into the explainability of vision-language models and\nsuggest that vision reflection is a promising strategy for achieving robust and\ninterpretable visual recognition.", "AI": {"tldr": "研究发现，在没有使用大规模视觉模型的情况下，通过视觉反射机制提升了LMMs在图像识别任务上的准确率，揭示了视觉语言模型的工作原理和可解释性。", "motivation": "研究动机是探索在多模态模型中视觉反思的潜在能力，特别是在不需要进行大量训练的情况下提升识别准确性和增强模型的解释性。", "method": "此论文展示了通过一种名为视觉反思的方法提升大型多模态模型（LMMs）预测准确性的可能性。研究分为三个方面：1）将LMM用于验证专门的视觉模型预测，以提高图像识别准确率；2）分析视觉语言连接器如何将视觉特征转换为文本概念，从而增强常识推理能力；3）无需训练即可通过连接器提升LMM在细粒度识别任务上的性能。", "result": "研究发现LMMs可以通过视觉反思提高在ImageNet等基准测试上的表现，即使其原始性能通常低于专业的视觉编码器；发现视觉-语言连接器将视觉特征转化为明确的文本概念，可以使语言模型通过常识进行预测合理性判断；少量文本标记替换大量视觉标记也能使模型产生接近的答案，表明LMMs可能主要依赖于一套精简的文本表示而不是原始视觉特征；无需广泛特征对齐训练即可增强模型在细粒度识别任务上的表现。", "conclusion": "该研究提供了对视觉-语言模型可解释性的新见解，并表明视觉反思是实现稳健和可解释视觉识别的有前途的策略。"}}
{"id": "2508.06621", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.06621", "abs": "https://arxiv.org/abs/2508.06621", "authors": ["Tomohiro Sawada", "Kartik Goyal"], "title": "Train It and Forget It: Merge Lists are Unnecessary for BPE Inference in Language Models", "comment": "Submitted to EMNLP", "summary": "Standard Byte-Pair Encoding (BPE) tokenization compresses text by pairing a\nlearned token vocabulary with a detailed merge list. Recent work has shown that\nthis merge list exposes a potential attack surface for extracting information\nabout language model's training data. In this paper, we explore the downstream\nimpact of BPE inference algorithms that do not rely on this merge list at all,\nand hence differ from the encoding process during BPE training. To address this\nquestion, we investigate two broad classes of BPE inference schemes that differ\nfrom BPE application during training: a) targeted deviation from merge-lists\nincluding random merge orders, and various corruptions of merge list involving\ndeletion/truncation, and b) non-targeted BPE inference algorithms that do not\ndepend on the merge list but focus on compressing the text either greedily or\nexactly. Extensive experiments across diverse language modeling tasks like\naccuracy-based QA benchmarks, machine translation, and open-ended generation\nreveal that while targeted deviation from the merge lists exhibits significant\ndegradation in language model performance, the non-targeted merge-list-free\ninference algorithms result in minimal impact on downstream performance that is\noften much smaller than expected. These findings pave way for simpler and\npotentially more privacy-preserving tokenization schemes that do not\ncatastrophically compromise model performance.", "AI": {"tldr": "本文探讨了不依赖于BPE训练过程中使用的合并列表的BPE推理算法的下游影响，结果显示一些算法不影响性能，为更简单的分词方案提供了可能。", "motivation": "标准的字节对编码（BPE）分词通过学习的词汇表和详细的合并列表压缩文本。研究表明，合并列表存在潜在的攻击面，可能泄露语言模型训练数据的信息。本文旨在探讨不依赖于合并列表的BPE推理算法对下游任务的影响。", "method": "本文研究了两种BPE推断方案：一种是针对合并列表的偏差，包括随机合并顺序和删除/截断等合并列表的多种篡改；另一种是非针对性的BPE推理算法，它们不依赖于合并列表，而是专注于通过贪婪或精确的方式压缩文本。", "result": "实验结果显示，在准确性问答基准测试、机器翻译和开放生成等语言模型任务中，对合并列表的针对性偏差显著降低了语言模型的性能，而非针对性的不使用合并列表的推理算法对下游性能的影响最小，远低于预期。", "conclusion": "研究结果为可能更加隐私保护的简化分词方案铺平了道路，这些方案不会严重损害模型性能。"}}
{"id": "2508.06528", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06528", "abs": "https://arxiv.org/abs/2508.06528", "authors": ["Xiuliang Zhang", "Tadiwa Elisha Nyamasvisva", "Chuntao Liu"], "title": "A Framework Combining 3D CNN and Transformer for Video-Based Behavior Recognition", "comment": "9 pages,6 figures", "summary": "Video-based behavior recognition is essential in fields such as public\nsafety, intelligent surveillance, and human-computer interaction. Traditional\n3D Convolutional Neural Network (3D CNN) effectively capture local\nspatiotemporal features but struggle with modeling long-range dependencies.\nConversely, Transformers excel at learning global contextual information but\nface challenges with high computational costs. To address these limitations, we\npropose a hybrid framework combining 3D CNN and Transformer architectures. The\n3D CNN module extracts low-level spatiotemporal features, while the Transformer\nmodule captures long-range temporal dependencies, with a fusion mechanism\nintegrating both representations. Evaluated on benchmark datasets, the proposed\nmodel outperforms traditional 3D CNN and standalone Transformers, achieving\nhigher recognition accuracy with manageable complexity. Ablation studies\nfurther validate the complementary strengths of the two modules. This hybrid\nframework offers an effective and scalable solution for video-based behavior\nrecognition.", "AI": {"tldr": "This paper introduces a new hybrid framework that merges 3D CNN and Transformer architectures to improve video-based behavior recognition by combining their unique strengths, achieving better results and lower computational complexity compared to using either architecture alone.", "motivation": "The motivation behind this research is to enhance behavior recognition in videos by addressing the limitations of 3D CNNs in capturing long-range dependencies and the high computational demands of Transformers.", "method": "The paper proposes a hybrid framework that combines 3D CNN and Transformer architectures. The 3D CNN module captures low-level spatiotemporal features while the Transformer module focuses on long-range temporal dependencies. A fusion mechanism integrates both representations to enhance overall performance.", "result": "The proposed hybrid model outperforms traditional 3D CNNs and standalone Transformers in video-based behavior recognition tasks in terms of accuracy and computational efficiency.", "conclusion": "The study concludes that integrating 3D CNN and Transformer within a hybrid framework offers a scalable and effective solution for video-based behavior recognition, validated by ablation studies that confirm the added value of each module."}}
{"id": "2508.06649", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.06649", "abs": "https://arxiv.org/abs/2508.06649", "authors": ["Daniel Wang", "Eli Brignac", "Minjia Mao", "Xiao Fang"], "title": "Measuring Stereotype and Deviation Biases in Large Language Models", "comment": null, "summary": "Large language models (LLMs) are widely applied across diverse domains,\nraising concerns about their limitations and potential risks. In this study, we\ninvestigate two types of bias that LLMs may display: stereotype bias and\ndeviation bias. Stereotype bias refers to when LLMs consistently associate\nspecific traits with a particular demographic group. Deviation bias reflects\nthe disparity between the demographic distributions extracted from\nLLM-generated content and real-world demographic distributions. By asking four\nadvanced LLMs to generate profiles of individuals, we examine the associations\nbetween each demographic group and attributes such as political affiliation,\nreligion, and sexual orientation. Our experimental results show that all\nexamined LLMs exhibit both significant stereotype bias and deviation bias\ntowards multiple groups. Our findings uncover the biases that occur when LLMs\ninfer user attributes and shed light on the potential harms of LLM-generated\noutputs.", "AI": {"tldr": "研究发现，四款高级语言模型在生成内容时表现出显著的刻板印象和偏差偏差，表明语言模型在推断用户属性时存在偏见，可能产生负面后果。", "motivation": "大规模语言模型（LLMs）在各个领域得到广泛应用，引发了对其局限性和潜在风险的担忧。研究目的是调查语言模型可能表现出的两种偏差：刻板印象偏差和偏差偏差。", "method": "本研究通过让四款高级语言模型（LLMs）生成个人资料，来考察不同人口统计群体与政治倾向、宗教信仰和性取向等属性之间的关联。", "result": "实验结果表明，所有被检查的语言模型都展现在多种群体上的显著刻板印象偏差和偏差偏差。", "conclusion": "研究发现揭示了LLMs在推断用户属性时出现的偏差，指出了LLMs生成内容可能带来的潜在危害。"}}
{"id": "2508.06529", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.06529", "abs": "https://arxiv.org/abs/2508.06529", "authors": ["Jiayuan Wang", "Q. M. Jonathan Wu", "Katsuya Suto", "Ning Zhang"], "title": "RMT-PPAD: Real-time Multi-task Learning for Panoptic Perception in Autonomous Driving", "comment": null, "summary": "Autonomous driving systems rely on panoptic driving perception that requires\nboth precision and real-time performance. In this work, we propose RMT-PPAD, a\nreal-time, transformer-based multi-task model that jointly performs object\ndetection, drivable area segmentation, and lane line segmentation. We introduce\na lightweight module, a gate control with an adapter to adaptively fuse shared\nand task-specific features, effectively alleviating negative transfer between\ntasks. Additionally, we design an adaptive segmentation decoder to learn the\nweights over multi-scale features automatically during the training stage. This\navoids the manual design of task-specific structures for different segmentation\ntasks. We also identify and resolve the inconsistency between training and\ntesting labels in lane line segmentation. This allows fairer evaluation.\nExperiments on the BDD100K dataset demonstrate that RMT-PPAD achieves\nstate-of-the-art results with mAP50 of 84.9% and Recall of 95.4% for object\ndetection, mIoU of 92.6% for drivable area segmentation, and IoU of 56.8% and\naccuracy of 84.7% for lane line segmentation. The inference speed reaches 32.6\nFPS. Moreover, we introduce real-world scenarios to evaluate RMT-PPAD\nperformance in practice. The results show that RMT-PPAD consistently delivers\nstable performance. The source codes and pre-trained models are released at\nhttps://github.com/JiayuanWang-JW/RMT-PPAD.", "AI": {"tldr": "该研究提出了RMT-PPAD，一个实时且基于变压器的多任务模型，用于对象检测、可行驶区域分割以及车道线分割，其结构和模块设计致力于提高性能并解决任务训练和测试的标签一致性问题，实验证明其在BDD100K数据集上表现出色，具有实时性高、精度高的优点。", "motivation": "作者旨在开发一个能够同时执行对象检测、可行驶区域分割及车道线分割并达到实时性能和高精度的模型，从而改进自动驾驶系统的全景驾驶感知。", "method": "RMT-PPAD模型利用了一个轻量级模块进行特征融合，并设计了自适应分割解码器来自动学习多尺度特征的权重，减少任务特定结构的手动设计需求。同时解决车道线分割任务中的训练与测试标签不一致问题。", "result": "实验结果表明，RMT-PPAD在BDD100K数据集上表现出色，提供了高精度的结果（对象检测mAP50达84.9%、召回率为95.4%；可行驶区域分割mIoU达92.6%；车道线分割IoU为56.8%，准确率为84.7%）并实现了32.6 FPS的推理速度。", "conclusion": "基于实验和实际场景测试的表现，RMT-PPAD可稳定地提供高性能结果，适合用于改进自动驾驶系统的全景驾驶感知能力，促进其实际应用。"}}
{"id": "2508.06665", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.06665", "abs": "https://arxiv.org/abs/2508.06665", "authors": ["Jonathan Shaw", "Dillon Mee", "Timothy Khouw", "Zackary Leech", "Daniel Wilson"], "title": "Testing the Limits of Machine Translation from One Book", "comment": null, "summary": "Current state-of-the-art models demonstrate capacity to leverage in-context\nlearning to translate into previously unseen language contexts. Tanzer et al.\n[2024] utilize language materials (e.g. a grammar) to improve translation\nquality for Kalamang using large language models (LLMs). We focus on Kanuri, a\nlanguage that, despite having substantial speaker population, has minimal\ndigital resources. We design two datasets for evaluation: one focused on health\nand humanitarian terms, and another containing generalized terminology,\ninvestigating how domain-specific tasks impact LLM translation quality.\n  By providing different combinations of language resources (grammar,\ndictionary, and parallel sentences), we measure LLM translation effectiveness,\ncomparing results to native speaker translations and human linguist\nperformance. We evaluate using both automatic metrics and native speaker\nassessments of fluency and accuracy.\n  Results demonstrate that parallel sentences remain the most effective data\nsource, outperforming other methods in human evaluations and automatic metrics.\nWhile incorporating grammar improves over zero-shot translation, it fails as an\neffective standalone data source. Human evaluations reveal that LLMs achieve\naccuracy (meaning) more effectively than fluency (grammaticality).\n  These findings suggest LLM translation evaluation benefits from\nmultidimensional assessment beyond simple accuracy metrics, and that grammar\nalone, without parallel sentences, does not provide sufficient context for\neffective domain-specific translation.", "AI": {"tldr": "研究通过提供不同组合的语言资源来评估大语言模型（LLMs）在Kanuri语言上的翻译效果，发现双语句子是最有效资源，且仅提供语法规则不足以产生优质的领域特定翻译。", "motivation": "一些具有大量使用者的语言，因为数字资源匮乏，面临翻译挑战。研究主要集中在Kanuri语言上，探究特定领域的任务如何影响LLMs的翻译质量。", "method": "通过提供不同组合的语言资源（如语法规则、词典和双语句子）来评估大语言模型（LLMs）的翻译效果，并将结果与母语者的翻译和语言学家的表现进行对比。评估方法包括自动度量和母语者对流利度和准确性的评估。", "result": "双语句子是最有效的数据来源，在人类评估和自动度量中都优于其他方法。虽然使用语法规则可以提升非零样本的翻译效果，但它单独使用并不能达到效果。人类评估表明，LLMs在提高翻译准确性上优于流利度。", "conclusion": "对于大语言模型的翻译评估，除了简单的准确性度量外，还需要多维度的评估方法，且单凭语法规则无法为有效领域特定翻译提供足够的上下文。"}}
{"id": "2508.06530", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.06530", "abs": "https://arxiv.org/abs/2508.06530", "authors": ["Ming-Kun Xie", "Jia-Hao Xiao", "Gang Niu", "Lei Feng", "Zhiqiang Kou", "Min-Ling Zhang", "Masashi Sugiyama"], "title": "What Makes \"Good\" Distractors for Object Hallucination Evaluation in Large Vision-Language Models?", "comment": null, "summary": "Large Vision-Language Models (LVLMs), empowered by the success of Large\nLanguage Models (LLMs), have achieved impressive performance across domains.\nDespite the great advances in LVLMs, they still suffer from the unavailable\nobject hallucination issue, which tends to generate objects inconsistent with\nthe image content. The most commonly used Polling-based Object Probing\nEvaluation (POPE) benchmark evaluates this issue by sampling negative\ncategories according to category-level statistics, \\textit{e.g.}, category\nfrequencies and co-occurrence. However, with the continuous advancement of\nLVLMs, the POPE benchmark has shown diminishing effectiveness in assessing\nobject hallucination, as it employs a simplistic sampling strategy that\noverlooks image-specific information and restricts distractors to negative\nobject categories only. In this paper, we introduce the Hallucination\nsearching-based Object Probing Evaluation (HOPE) benchmark, aiming to generate\nthe most misleading distractors (\\textit{i.e.}, non-existent objects or\nincorrect image descriptions) that can trigger hallucination in LVLMs, which\nserves as a means to more rigorously assess their immunity to hallucination. To\nexplore the image-specific information, the content-aware hallucination\nsearching leverages Contrastive Language-Image Pre-Training (CLIP) to\napproximate the predictive behavior of LVLMs by selecting negative objects with\nthe highest predicted likelihood as distractors. To expand the scope of\nhallucination assessment, the description-based hallucination searching\nconstructs highly misleading distractors by pairing true objects with false\ndescriptions. Experimental results show that HOPE leads to a precision drop of\nat least 9\\% and up to 23\\% across various state-of-the-art LVLMs,\nsignificantly outperforming POPE in exposing hallucination vulnerabilities. The\ncode is available at https://github.com/xiemk/HOPE.", "AI": {"tldr": "本文提出了HOPE基准测试，用于更严格评估LVLMs的幻觉免疫力，通过更精细的干扰项生成方法，实验结果显示HOPE在揭示幻觉漏洞方面显著优于POPE。", "motivation": "现有基于抽样的对象探测评估（POPE）基准测试已显示出评估对象幻觉的有效性在下降，原因是使用了忽视图像特定信息的简单抽样策略。", "method": "提出了基于幻觉搜索的对象探测评估（HOPE）基准测试，通过利用内容感知幻觉搜索和基于描述的幻觉搜索来生成更具误导性的干扰项，以更严格地评估LVLMs的幻觉免疫力。内容感知幻觉搜索使用CLIP来选择预测可能性最高的负对象作为干扰项，而基于描述的幻觉搜索则通过将真实对象与错误描述配对来构造高度误导的干扰项。", "result": "实验结果表明，HOPE导致各种最先进的LVLMs的精度下降至少9%，最高达23%，显著优于POPE在揭示幻觉漏洞方面的表现。", "conclusion": "HOPE基准测试通过引入更精确的干扰项生成方法，显著提升了对LVLMs幻觉问题检测的严格性和有效性。"}}
{"id": "2508.06671", "categories": ["cs.CL", "cs.AI", "I.2.7"], "pdf": "https://arxiv.org/pdf/2508.06671", "abs": "https://arxiv.org/abs/2508.06671", "authors": ["Swati Rajwal", "Shivank Garg", "Reem Abdel-Salam", "Abdelrahman Zayed"], "title": "Do Biased Models Have Biased Thoughts?", "comment": "Accepted at main track of the Second Conference on Language Modeling\n  (COLM 2025)", "summary": "The impressive performance of language models is undeniable. However, the\npresence of biases based on gender, race, socio-economic status, physical\nappearance, and sexual orientation makes the deployment of language models\nchallenging. This paper studies the effect of chain-of-thought prompting, a\nrecent approach that studies the steps followed by the model before it\nresponds, on fairness. More specifically, we ask the following question:\n\\textit{Do biased models have biased thoughts}? To answer our question, we\nconduct experiments on $5$ popular large language models using fairness metrics\nto quantify $11$ different biases in the model's thoughts and output. Our\nresults show that the bias in the thinking steps is not highly correlated with\nthe output bias (less than $0.6$ correlation with a $p$-value smaller than\n$0.001$ in most cases). In other words, unlike human beings, the tested models\nwith biased decisions do not always possess biased thoughts.", "AI": {"tldr": "研究表明，尽管语言模型可能输出带有偏见的结果，但是模型的思考过程并不总是带有偏见。", "motivation": "由于语言模型存在性别、种族、社会经济地位、身体特征和性取向等方面的偏见，这使得部署语言模型变得具有挑战性。研究目的是探讨带有偏见的语言模型的思考过程中是否存在偏见。", "method": "该研究使用了链式思考提示方法，对5个流行的大型语言模型进行了实验，以量化模型思考过程和输出中的11种不同偏见。", "result": "研究结果表明，思考过程中的偏见与输出偏见之间的相关性并不很高（大多数情况下相关系数小于0.6，p值小于0.001）。", "conclusion": "与人类不同，实验测试的具有偏见决策的模型并不总是存在偏见思考。"}}
{"id": "2508.06535", "categories": ["eess.IV", "cs.CV", "cs.LG", "F.2.2; I.2.7"], "pdf": "https://arxiv.org/pdf/2508.06535", "abs": "https://arxiv.org/abs/2508.06535", "authors": ["Faisal Ahmed"], "title": "Transfer Learning with EfficientNet for Accurate Leukemia Cell Classification", "comment": "8 pages, 1 figure", "summary": "Accurate classification of Acute Lymphoblastic Leukemia (ALL) from peripheral\nblood smear images is essential for early diagnosis and effective treatment\nplanning. This study investigates the use of transfer learning with pretrained\nconvolutional neural networks (CNNs) to improve diagnostic performance. To\naddress the class imbalance in the dataset of 3,631 Hematologic and 7,644 ALL\nimages, we applied extensive data augmentation techniques to create a balanced\ntraining set of 10,000 images per class. We evaluated several models, including\nResNet50, ResNet101, and EfficientNet variants B0, B1, and B3. EfficientNet-B3\nachieved the best results, with an F1-score of 94.30%, accuracy of 92.02%,\nandAUCof94.79%,outperformingpreviouslyreported methods in the C-NMCChallenge.\nThesefindings demonstrate the effectiveness of combining data augmentation with\nadvanced transfer learning models, particularly EfficientNet-B3, in developing\naccurate and robust diagnostic tools for hematologic malignancy detection.", "AI": {"tldr": "The study utilizes transfer learning and data augmentation to improve ALL classification accuracy, with EfficientNet-B3 delivering top performance.", "motivation": "The motivation is to improve the diagnostic performance of Acute Lymphoblastic Leukemia (ALL) classification from peripheral blood smear images for early diagnosis and effective treatment planning.", "method": "This study uses transfer learning with pre-trained CNNs and applies extensive data augmentation to address class imbalance in the dataset, evaluating models such as ResNet50, ResNet101, and EfficientNet variants B0, B1, and B3.", "result": "EfficientNet-B3 achieved the best results with an F1-score of 94.30%, accuracy of 92.02%, and AUC of 94.79%, outperforming previously reported methods in the C-NMC Challenge.", "conclusion": "The findings demonstrate the effectiveness of combining data augmentation with advanced transfer learning models, especially EfficientNet-B3, in developing accurate and robust diagnostic tools for hematologic malignancy detection."}}
{"id": "2508.06709", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06709", "abs": "https://arxiv.org/abs/2508.06709", "authors": ["Evangelia Spiliopoulou", "Riccardo Fogliato", "Hanna Burnsky", "Tamer Soliman", "Jie Ma", "Graham Horwood", "Miguel Ballesteros"], "title": "Play Favorites: A Statistical Method to Measure Self-Bias in LLM-as-a-Judge", "comment": null, "summary": "Large language models (LLMs) can serve as judges that offer rapid and\nreliable assessments of other LLM outputs. However, models may systematically\nassign overly favorable ratings to their own outputs, a phenomenon known as\nself-bias, which can distort evaluations of true model performance. Previous\nstudies often conflate genuine differences in model quality with bias or\nincorrectly assume that evaluations from LLMs and humans follow the same rating\ndistributions. In this work, we present a statistical framework that explicitly\nformalizes assumptions under which self-bias can be identified and estimated.\nOur method models the difference in the scoring distribution that\nLLM-as-a-judge assigns to its own completions compared to other models, while\naccounting for the underlying quality of the completions provided by an\nindependent, third-party judge (e.g., humans). Our method reliably isolates and\nquantifies self-bias, even when models vary in ability, ensuring that genuine\nperformance differences are not mistaken for self-bias. We conduct an empirical\nanalysis of self-bias on a large dataset (>5000 prompt-completion pairs)\nconsisting of expert human annotations and judgments from nine different LLM\njudges. We find that some models, such as GPT-4o and Claude 3.5 Sonnet,\nsystematically assign higher scores to their own outputs. These models also\ndisplay family-bias; systematically assigning higher ratings to outputs\nproduced by other models of the same family. Our findings highlight potential\npitfalls of using LLM judges and offer practical guidance to mitigate biases\nwhen interpreting automated evaluations.", "AI": {"tldr": "该研究通过统计框架识别和估计大型语言模型（LLMs）评分中的自我偏见，并通过独立第三方的评分进行调整，以区分真实性能差异和偏见。测试发现某些模型存在对自己和其他同族模型评分偏高的现象，提供了减少这种偏见的实际指导建议。", "motivation": "解决现有的LMM自我评分研究中混淆真正质量差异与偏见的问题，明确地形式化识别自我偏见的前提，并通过大量的数据集验证这些偏见现象。", "method": "提出一个统计框架，明确形式化了识别和估计自我偏见的假设；通过模型自身的评分和第三方评分（如人类评分）之间的差异来量化自我偏见，并在考虑不同模型能力差异的情况下隔离和量化这部分偏见。", "result": "在包含专家人工标注和9个不同LLM评分的大型数据集上进行实证分析，发现部分模型如GPT-4o和Claude 3.5 Sonnet会对自己的和同家族模型的输出给出较高评分，展示了在这种情况下使用LLM评分的潜在缺陷。", "conclusion": "研究结果揭示了利用LLM作为评分者时出现自我偏见和家族偏见的问题，并提供了减少这些偏见和更准确解读自动评估的实际建议。"}}
{"id": "2508.06537", "categories": ["cs.CV", "astro-ph.IM"], "pdf": "https://arxiv.org/pdf/2508.06537", "abs": "https://arxiv.org/abs/2508.06537", "authors": ["Shantanusinh Parmar"], "title": "Benchmarking Deep Learning-Based Object Detection Models on Feature Deficient Astrophotography Imagery Dataset", "comment": null, "summary": "Object detection models are typically trained on datasets like ImageNet,\nCOCO, and PASCAL VOC, which focus on everyday objects. However, these lack\nsignal sparsity found in non-commercial domains. MobilTelesco, a\nsmartphone-based astrophotography dataset, addresses this by providing sparse\nnight-sky images. We benchmark several detection models on it, highlighting\nchallenges under feature-deficient conditions.", "AI": {"tldr": "文章研究了目标检测模型在稀疏夜空图像上的表现，指出了其在特征不足条件下的局限性。", "motivation": "现有的目标检测模型训练数据集如ImageNet, COCO, 和 PASCAL VOC 主要聚焦于日常物体，缺乏非商业化领域的信号稀疏性。为了填补这一空缺，本文引入了针对稀疏夜空图像的MobilTelesco数据集。", "method": "本文通过使用MobilTelesco智能手机天文摄影数据集来评估几种目标检测模型，该数据集专注于稀疏的夜空图像，填补了现有数据集在非商业领域信号稀疏性的不足。", "result": "研究表明，在特征不足条件下，目标检测模型面临着显著的挑战。", "conclusion": "通过在MobilTelesco数据集上进行基准测试，本文阐明了在稀疏信号和特征不足的条件下，目标检测模型可能遇到的问题和挑战。"}}
{"id": "2508.06729", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06729", "abs": "https://arxiv.org/abs/2508.06729", "authors": ["Komala Subramanyam Cherukuri", "Pranav Abishai Moses", "Aisa Sakata", "Jiangping Chen", "Haihua Chen"], "title": "Large Language Models for Oral History Understanding with Text Classification and Sentiment Analysis", "comment": null, "summary": "Oral histories are vital records of lived experience, particularly within\ncommunities affected by systemic injustice and historical erasure. Effective\nand efficient analysis of their oral history archives can promote access and\nunderstanding of the oral histories. However, Large-scale analysis of these\narchives remains limited due to their unstructured format, emotional\ncomplexity, and high annotation costs. This paper presents a scalable framework\nto automate semantic and sentiment annotation for Japanese American\nIncarceration Oral History. Using LLMs, we construct a high-quality dataset,\nevaluate multiple models, and test prompt engineering strategies in\nhistorically sensitive contexts. Our multiphase approach combines expert\nannotation, prompt design, and LLM evaluation with ChatGPT, Llama, and Qwen. We\nlabeled 558 sentences from 15 narrators for sentiment and semantic\nclassification, then evaluated zero-shot, few-shot, and RAG strategies. For\nsemantic classification, ChatGPT achieved the highest F1 score (88.71%),\nfollowed by Llama (84.99%) and Qwen (83.72%). For sentiment analysis, Llama\nslightly outperformed Qwen (82.66%) and ChatGPT (82.29%), with all models\nshowing comparable results. The best prompt configurations were used to\nannotate 92,191 sentences from 1,002 interviews in the JAIOH collection. Our\nfindings show that LLMs can effectively perform semantic and sentiment\nannotation across large oral history collections when guided by well-designed\nprompts. This study provides a reusable annotation pipeline and practical\nguidance for applying LLMs in culturally sensitive archival analysis. By\nbridging archival ethics with scalable NLP techniques, this work lays the\ngroundwork for responsible use of artificial intelligence in digital humanities\nand preservation of collective memory. GitHub:\nhttps://github.com/kc6699c/LLM4OralHistoryAnalysis.", "AI": {"tldr": "本文提出了一种使用LLMs自动标注日本裔美国人监禁口述历史数据的情感和语义的新框架，展示了在大规模分析中有效应用。", "motivation": "由于大规模分析口述历史档案面临结构化差、情感复杂及标注成本高等困难，本文旨在展示如何通过LLMs进行有效的语义和情感标注，提升这些档案的访问和理解。", "method": "本文介绍了一个可扩展的框架，用于自动语义和情感注释日本裔美国人监禁口述历史。该框架使用了LLMs，通过专家标注、指令设计和LLM评估（使用ChatGPT、Llama和Qwen）来构造高质量的数据集，并评估多个模型及测试指令工程策略。", "result": "在语义分类中，ChatGPT获得了最高的F1分数为88.71%，其次是Llama的84.99%和Qwen的83.72%。在情感分析中，Llama略胜一筹，其次是Qwen和ChatGPT。最终，使用最佳指令配置标注了92,191个句子，来自1,002次访谈。", "conclusion": "研究证明，通过精心设计的指令，LLMs能够有效地对大型口述历史收藏进行语义和情感标注，这为在文化和伦理敏感的档案分析中应用LLMs提供了再利用注释流水线和实用指导。"}}
{"id": "2508.06543", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06543", "abs": "https://arxiv.org/abs/2508.06543", "authors": ["Jinghan Yu", "Zhiyuan Ma", "Yue Ma", "Kaiqi Liu", "Yuhan Wang", "Jianjun Li"], "title": "MILD: Multi-Layer Diffusion Strategy for Complex and Precise Multi-IP Aware Human Erasing", "comment": null, "summary": "Recent years have witnessed the success of diffusion models in\nimage-customized tasks. Prior works have achieved notable progress on\nhuman-oriented erasing using explicit mask guidance and semantic-aware\ninpainting. However, they struggle under complex multi-IP scenarios involving\nhuman-human occlusions, human-object entanglements, and background\ninterferences. These challenges are mainly due to: 1) Dataset limitations, as\nexisting datasets rarely cover dense occlusions, camouflaged backgrounds, and\ndiverse interactions; 2) Lack of spatial decoupling, where foreground instances\ncannot be effectively disentangled, limiting clean background restoration. In\nthis work, we introduce a high-quality multi-IP human erasing dataset with\ndiverse pose variations and complex backgrounds. We then propose Multi-Layer\nDiffusion (MILD), a novel strategy that decomposes generation into semantically\nseparated pathways for each instance and the background. To enhance\nhuman-centric understanding, we introduce Human Morphology Guidance,\nintegrating pose, parsing, and spatial relations. We further present\nSpatially-Modulated Attention to better guide attention flow. Extensive\nexperiments show that MILD outperforms state-of-the-art methods on challenging\nhuman erasing benchmarks.", "AI": {"tldr": "A new dataset and the Multi-Layer Diffusion (MILD) method are introduced to improve human erasing in complex multi-IP scenarios.", "motivation": "The motivation arises from the challenges faced by previous works when dealing with human-human occlusions, human-object entanglements, and background interferences. These issues are mainly due to dataset limitations and the inability to spatially decouple foreground instances from the background.", "method": "A high-quality multi-IP human erasing dataset is introduced with diverse pose variations and complex backgrounds. The Multi-Layer Diffusion (MILD) strategy is proposed, which separates generation into pathways for each instance and the background. The approach also includes Human Morphology Guidance for better human-centric understanding and Spatially-Modulated Attention for improved attention flow.", "result": "Extensive experiments demonstrate that MILD surpasses state-of-the-art methods on challenging human erasing benchmarks.", "conclusion": "The proposed approach effectively addresses the limitation of existing methods in handling complex occlusions and interactions, outperforming current state-of-the-art techniques in human erasing."}}
{"id": "2508.06755", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06755", "abs": "https://arxiv.org/abs/2508.06755", "authors": ["Xianjun Yang", "Liqiang Xiao", "Shiyang Li", "Faisal Ladhak", "Hyokun Yun", "Linda Ruth Petzold", "Yi Xu", "William Yang Wang"], "title": "Many-Turn Jailbreaking", "comment": null, "summary": "Current jailbreaking work on large language models (LLMs) aims to elicit\nunsafe outputs from given prompts. However, it only focuses on single-turn\njailbreaking targeting one specific query. On the contrary, the advanced LLMs\nare designed to handle extremely long contexts and can thus conduct multi-turn\nconversations. So, we propose exploring multi-turn jailbreaking, in which the\njailbroken LLMs are continuously tested on more than the first-turn\nconversation or a single target query. This is an even more serious threat\nbecause 1) it is common for users to continue asking relevant follow-up\nquestions to clarify certain jailbroken details, and 2) it is also possible\nthat the initial round of jailbreaking causes the LLMs to respond to additional\nirrelevant questions consistently. As the first step (First draft done at June\n2024) in exploring multi-turn jailbreaking, we construct a Multi-Turn Jailbreak\nBenchmark (MTJ-Bench) for benchmarking this setting on a series of open- and\nclosed-source models and provide novel insights into this new safety threat. By\nrevealing this new vulnerability, we aim to call for community efforts to build\nsafer LLMs and pave the way for a more in-depth understanding of jailbreaking\nLLMs.", "AI": {"tldr": "该研究构建了一个用于评估语言模型在多轮对话中越狱行为的基准测试MTJ-Bench，揭示了新安全威胁，强调了社区共建更安全语言模型的重要性。", "motivation": "现有的针对大语言模型的越狱研究仅关注单轮特定查询的不当输出，但先进的语言模型可以处理长上下文和多轮对话，这促使研究人员探索更为复杂的多轮越狱威胁。", "method": "提出多轮越狱（multi-turn jailbreaking）的概念，构建了一个名为Multi-Turn Jailbreak Benchmark (MTJ-Bench)的基准测试框架，用于评估开源和闭源模型在多轮对话中的越狱行为。", "result": "通过MTJ-Bench，揭示了一种新的安全威胁——用户在多轮对话中可能会不断追问越狱细节，或者违规的第一轮回答迫使语言模型在后续的无关问题中做出一致的违规响应。", "conclusion": "希望这一研究能够引起社群重视，促进更加安全的语言模型的开发，并推动对其变异和应对策略的深入理解。"}}
{"id": "2508.06546", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2508.06546", "abs": "https://arxiv.org/abs/2508.06546", "authors": ["Qi Xun Yeo", "Yanyan Li", "Gim Hee Lee"], "title": "Statistical Confidence Rescoring for Robust 3D Scene Graph Generation from Multi-View Images", "comment": "This paper has been accepted in ICCV 25", "summary": "Modern 3D semantic scene graph estimation methods utilize ground truth 3D\nannotations to accurately predict target objects, predicates, and\nrelationships. In the absence of given 3D ground truth representations, we\nexplore leveraging only multi-view RGB images to tackle this task. To attain\nrobust features for accurate scene graph estimation, we must overcome the noisy\nreconstructed pseudo point-based geometry from predicted depth maps and reduce\nthe amount of background noise present in multi-view image features. The key is\nto enrich node and edge features with accurate semantic and spatial information\nand through neighboring relations. We obtain semantic masks to guide feature\naggregation to filter background features and design a novel method to\nincorporate neighboring node information to aid robustness of our scene graph\nestimates. Furthermore, we leverage on explicit statistical priors calculated\nfrom the training summary statistics to refine node and edge predictions based\non their one-hop neighborhood. Our experiments show that our method outperforms\ncurrent methods purely using multi-view images as the initial input. Our\nproject page is available at https://qixun1.github.io/projects/SCRSSG.", "AI": {"tldr": "本文提出一种新方法，通过多视角图像分析，即使在没有3D注释的情况下，也能生成高质量的3D语义场景图。", "motivation": "当前3D语义场景图估计方法依赖于地面实况3D注释。本文探索仅使用多视角RGB图像进行3D语义场景图估计。", "method": "利用多视角RGB图像和从预测深度图中获得的伪点基几何结构来增强节点和边的特征，结合邻近节点信息和统计先验知识来提高场景图估计的鲁棒性。", "result": "实验表明我们的方法在仅使用多视角图像作为初始输入的情况下优于当前方法。", "conclusion": "研究展示了仅通过多视角图像输入，设计的方法能够在没有精确的3D注释的情况下，仍生成高质量的3D语义场景图。"}}
{"id": "2508.06803", "categories": ["cs.CL", "cs.MA"], "pdf": "https://arxiv.org/pdf/2508.06803", "abs": "https://arxiv.org/abs/2508.06803", "authors": ["Ziqi Liu", "Yangbin Chen", "Ziyang Zhou", "Yilin Li", "Mingxuan Hu", "Yushan Pan", "Zhijie Xu"], "title": "SEVADE: Self-Evolving Multi-Agent Analysis with Decoupled Evaluation for Hallucination-Resistant Irony Detection", "comment": null, "summary": "Sarcasm detection is a crucial yet challenging Natural Language Processing\ntask. Existing Large Language Model methods are often limited by\nsingle-perspective analysis, static reasoning pathways, and a susceptibility to\nhallucination when processing complex ironic rhetoric, which impacts their\naccuracy and reliability. To address these challenges, we propose **SEVADE**, a\nnovel **S**elf-**Ev**olving multi-agent **A**nalysis framework with\n**D**ecoupled **E**valuation for hallucination-resistant sarcasm detection. The\ncore of our framework is a Dynamic Agentive Reasoning Engine (DARE), which\nutilizes a team of specialized agents grounded in linguistic theory to perform\na multifaceted deconstruction of the text and generate a structured reasoning\nchain. Subsequently, a separate lightweight rationale adjudicator (RA) performs\nthe final classification based solely on this reasoning chain. This decoupled\narchitecture is designed to mitigate the risk of hallucination by separating\ncomplex reasoning from the final judgment. Extensive experiments on four\nbenchmark datasets demonstrate that our framework achieves state-of-the-art\nperformance, with average improvements of **6.75%** in Accuracy and **6.29%**\nin Macro-F1 score.", "AI": {"tldr": "The paper introduces SEVADE, a novel multi-agent analysis framework with decoupled evaluation for more accurate and reliable sarcasm detection in complex ironic rhetoric, achieving state-of-the-art results.", "motivation": "Existing models for sarcasm detection are often limited by single-perspective analysis, static reasoning pathways, and can produce incorrect outputs (hallucinations) when dealing with complex sarcasm, leading to reduced accuracy and reliability.", "method": "SEVADE, a multi-agent analysis framework with decoupled evaluation, is proposed. The core component is the Dynamic Agentive Reasoning Engine (DARE) which uses a team of linguistic theory-based agents to analyze the text and form a reasoning chain. A separate lightweight rationale adjudicator then uses this chain to classify the text, aiming to prevent hallucination.", "result": "Experiments on four benchmark datasets show that the SEVADE framework outperforms existing models with average improvements of 6.75% in Accuracy and 6.29% in Macro-F1 score.", "conclusion": "SEVADE is an effective approach to improving sarcasm detection by reducing the risk of hallucination and providing more structured reasoning pathways."}}
{"id": "2508.06551", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06551", "abs": "https://arxiv.org/abs/2508.06551", "authors": ["Ye Tao"], "title": "Slice or the Whole Pie? Utility Control for AI Models", "comment": null, "summary": "Training deep neural networks (DNNs) has become an increasingly\nresource-intensive task, requiring large volumes of labeled data, substantial\ncomputational power, and considerable fine-tuning efforts to achieve optimal\nperformance across diverse use cases. Although pre-trained models offer a\nuseful starting point, adapting them to meet specific user needs often demands\nextensive customization, and infrastructure overhead. This challenge grows when\na single model must support diverse appli-cations with differing requirements\nfor performance. Traditional solutions often involve training multiple model\nversions to meet varying requirements, which can be inefficient and difficult\nto maintain. In order to overcome this challenge, we propose NNObfuscator, a\nnovel utility control mechanism that enables AI models to dynamically modify\ntheir performance according to predefined conditions. It is different from\ntraditional methods that need separate models for each user. Instead,\nNNObfuscator allows a single model to be adapted in real time, giving you\ncontrolled access to multiple levels of performance. This mechanism enables\nmodel owners set up tiered access, ensuring that free-tier users receive a\nbaseline level of performance while premium users benefit from enhanced\ncapabilities. The approach improves resource allocation, reduces unnecessary\ncomputation, and supports sustainable business models in AI deployment. To\nvalidate our approach, we conducted experiments on multiple tasks, including\nimage classification, semantic segmentation, and text to image generation,\nusing well-established models such as ResNet, DeepLab, VGG16, FCN and Stable\nDiffusion. Experimental results show that NNObfuscator successfully makes model\nmore adaptable, so that a single trained model can handle a broad range of\ntasks without requiring a lot of changes.", "AI": {"tldr": "本文提出了一种新型的工具控制机制——NNObfuscator，允许AI模型根据预定义条件动态调整性能，从而支持单一模型的自适应性能层次结构，有利于优化资源分配，减少不必要的计算，并支持AI部署中的可持续商业模式。", "motivation": "训练深度神经网络已成为一个资源密集型任务，且需要大量的标注数据、显著计算能力和大量的调优努力才能在不同应用场景中取得最优性能。当单一模型需要支持具有不同性能要求的应用程序时，这种挑战会变得更加复杂。传统的解决方案是训练多个模型版本以满足不同需求，这通常是低效且难以维护的。", "method": "本研究提出了一种名为NNObfuscator的新工具控制机制，该机制可用于让AI模型根据预定义条件动态调整其性能，从而替代传统需要为每个用户训练不同模型的方法。", "result": "实验结果验证了NNObfuscator的有效性，其能够使单一训练的模型适应广泛的任务，而无需进行大量更改。", "conclusion": "NNObfuscator作为一种创新的方法，能够使AI模型的性能根据用户需求进行实时自适应调整，有效减少了资源消耗，提高了单一训练模型的适应性和效率，从而支持了AI部署中的可持续性商业模式。"}}
{"id": "2508.06810", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.06810", "abs": "https://arxiv.org/abs/2508.06810", "authors": ["Steven Coyne", "Diana Galvan-Sosa", "Ryan Spring", "Camélia Guerraoui", "Michael Zock", "Keisuke Sakaguchi", "Kentaro Inui"], "title": "Annotating Errors in English Learners' Written Language Production: Advancing Automated Written Feedback Systems", "comment": "Pre-review version of DOI 10.1007/978-3-031-98459-4_21, presented at\n  AIED 2025. All content is as of submission time except for de-anonymization,\n  ensuing layout fixes, use of the current code repository link, and BibTeX\n  fixes. Readers are encouraged to refer to the published version", "summary": "Recent advances in natural language processing (NLP) have contributed to the\ndevelopment of automated writing evaluation (AWE) systems that can correct\ngrammatical errors. However, while these systems are effective at improving\ntext, they are not optimally designed for language learning. They favor direct\nrevisions, often with a click-to-fix functionality that can be applied without\nconsidering the reason for the correction. Meanwhile, depending on the error\ntype, learners may benefit most from simple explanations and strategically\nindirect hints, especially on generalizable grammatical rules. To support the\ngeneration of such feedback, we introduce an annotation framework that models\neach error's error type and generalizability. For error type classification, we\nintroduce a typology focused on inferring learners' knowledge gaps by\nconnecting their errors to specific grammatical patterns. Following this\nframework, we collect a dataset of annotated learner errors and corresponding\nhuman-written feedback comments, each labeled as a direct correction or hint.\nWith this data, we evaluate keyword-guided, keyword-free, and template-guided\nmethods of generating feedback using large language models (LLMs). Human\nteachers examined each system's outputs, assessing them on grounds including\nrelevance, factuality, and comprehensibility. We report on the development of\nthe dataset and the comparative performance of the systems investigated.", "AI": {"tldr": "本文介绍了一种注释框架，用于为语言学习中的语法错误生成更适合学习者需求的反馈。该框架首先对错误进行分类并收集带有注释的数据集，然后评估了不同的反馈生成方法。", "motivation": "当前的自动写作评分(AWE)系统在纠正语法错误方面有效，但它们并非特别针对语言学习进行优化。这些系统通常提供直接纠正的功能，而忽视了考虑改正的原因。对于学习者来说，某些类型的错误可能更适合简单的解释或战略性间接提示。", "method": "本文提出了一种注释框架，用于根据错误类型和可推广性对错误进行建模。该框架包括错误类型分类和针对学习者知识缺口的特定语法模式的推断。在此基础上，收集了注释的错误数据集和相应的人类编写的反馈注释，每个注释被标记为直接修正或提示。", "result": "本研究收集了带有注释的学习者错误数据集与人类编写的反馈注释，并使用大型语言模型（LLMs）评估了关键词引导、非关键词引导和模板引导的反馈生成方法。教师对每个系统的输出进行了评估，包括相关性、事实性和可理解性。", "conclusion": "研究展示了发展此类数据集的进展以及所调查系统的性能对比。这有助于改善针对语言学习的自动写作评分系统，使其能提供更适合学习者需求的反馈。"}}
{"id": "2508.06552", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.06552", "abs": "https://arxiv.org/abs/2508.06552", "authors": ["Unisha Joshi"], "title": "Age-Diverse Deepfake Dataset: Bridging the Age Gap in Deepfake Detection", "comment": "11 pages, 4 figures, and 7 tables", "summary": "The challenges associated with deepfake detection are increasing\nsignificantly with the latest advancements in technology and the growing\npopularity of deepfake videos and images. Despite the presence of numerous\ndetection models, demographic bias in the deepfake dataset remains largely\nunaddressed. This paper focuses on the mitigation of age-specific bias in the\ndeepfake dataset by introducing an age-diverse deepfake dataset that will\nimprove fairness across age groups. The dataset is constructed through a\nmodular pipeline incorporating the existing deepfake datasets Celeb-DF,\nFaceForensics++, and UTKFace datasets, and the creation of synthetic data to\nfill the age distribution gaps. The effectiveness and generalizability of this\ndataset are evaluated using three deepfake detection models: XceptionNet,\nEfficientNet, and LipForensics. Evaluation metrics, including AUC, pAUC, and\nEER, revealed that models trained on the age-diverse dataset demonstrated\nfairer performance across age groups, improved overall accuracy, and higher\ngeneralization across datasets. This study contributes a reproducible,\nfairness-aware deepfake dataset and model pipeline that can serve as a\nfoundation for future research in fairer deepfake detection. The complete\ndataset and implementation code are available at\nhttps://github.com/unishajoshi/age-diverse-deepfake-detection.", "AI": {"tldr": "研究关注deepfake数据集中的年龄偏见问题，构建了年龄多样化数据集并改进了公平性的deepfake检测模型，提高了模型的准确性和泛化能力。", "motivation": "随着技术的进步和deepfake视频与图像的流行，deepfake检测面临挑战明显增加。尽管已存在大量检测模型，deepfake数据集中的人口统计性偏差（尤其是年龄偏见）仍未得到充分关注。为了提高年龄群体间的公平性，作者引入了年龄多样化的deepfake数据集。", "method": "通过融合Celeb-DF、FaceForensics++和UTKFace数据集，并生成合成数据来填补年龄分布缺口，构建了一个年龄多样化的deepfake数据集，旨在减轻deepfake数据集中的年龄偏向性问题。使用了三种deepfake检测模型进行评估：XceptionNet、EfficientNet和LipForensics。", "result": "评估结果表明，基于年龄多样化数据集训练的模型在各年龄群体中的表现更为公平，整体准确性提高，且在不同数据集间具有更高的泛化能力。", "conclusion": "本文贡献了一个可复现且具有公平意识的deepfake数据集和模型流水线，可作为未来公平deepfake检测研究的基础。相应的完整数据集和实现代码可见于https://github.com/unishajoshi/age-diverse-deepfake-detection。"}}
{"id": "2508.06870", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.06870", "abs": "https://arxiv.org/abs/2508.06870", "authors": ["Gangular Singh Irengbam", "Nirvash Singh Wahengbam", "Lanthoiba Meitei Khumanthem", "Paikhomba Oinam"], "title": "Text to Speech System for Meitei Mayek Script", "comment": null, "summary": "This paper presents the development of a Text-to-Speech (TTS) system for the\nManipuri language using the Meitei Mayek script. Leveraging Tacotron 2 and\nHiFi-GAN, we introduce a neural TTS architecture adapted to support tonal\nphonology and under-resourced linguistic environments. We develop a phoneme\nmapping for Meitei Mayek to ARPAbet, curate a single-speaker dataset, and\ndemonstrate intelligible and natural speech synthesis, validated through\nsubjective and objective metrics. This system lays the groundwork for\nlinguistic preservation and technological inclusion of Manipuri.", "AI": {"tldr": "本文推出了一个基于Meitei Mayek脚本的Manipuri语言的文本到语音系统，采用Tacotron 2和HiFi-GAN技术，为语言保护和技术包容提供了基础。", "motivation": "为了开发一种用于Manipuri语言的文本到语音系统，该系统使用Meitei Mayek脚本，并为语言保护和技术包容性奠定基础。", "method": "基于Tacotron 2和HiFi-GAN开发了一种适用于支持声调语音学和资源匮乏语言环境的神经文本到语音系统。为Meitei Mayek到ARPAbet开发了一个音素映射，创建了一个单说话人数据集，并通过主观和客观指标验证了语音合成的可理解和自然性。", "result": "该系统能够生成清晰和自然的语音，这已在主观和客观评估中得到验证。", "conclusion": "该神经TTS系统为Manipuri语言的技术包括和语言保护提供了坚实的基础。"}}
{"id": "2508.06553", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06553", "abs": "https://arxiv.org/abs/2508.06553", "authors": ["Jiahao Xiao", "Jianbo Zhang", "BoWen Yan", "Shengyu Guo", "Tongrui Ye", "Kaiwei Zhang", "Zicheng Zhang", "Xiaohong Liu", "Zhengxue Cheng", "Lei Fan", "Chuyi Li", "Guangtao Zhai"], "title": "Static and Plugged: Make Embodied Evaluation Simple", "comment": null, "summary": "Embodied intelligence is advancing rapidly, driving the need for efficient\nevaluation. Current benchmarks typically rely on interactive simulated\nenvironments or real-world setups, which are costly, fragmented, and hard to\nscale. To address this, we introduce StaticEmbodiedBench, a plug-and-play\nbenchmark that enables unified evaluation using static scene representations.\nCovering 42 diverse scenarios and 8 core dimensions, it supports scalable and\ncomprehensive assessment through a simple interface. Furthermore, we evaluate\n19 Vision-Language Models (VLMs) and 11 Vision-Language-Action models (VLAs),\nestablishing the first unified static leaderboard for Embodied intelligence.\nMoreover, we release a subset of 200 samples from our benchmark to accelerate\nthe development of embodied intelligence.", "AI": {"tldr": "文章介绍并评估了一个新的插拔式基准测试StaticEmbodiedBench，它通过使用静态场景表示实现了统一、可扩展且全面的评估。", "motivation": "当前的基准测试依赖于交互式模拟环境或现实世界设置，这些方法成本高、碎片化且难以扩展，因此提出了StaticEmbodiedBench这种插拔式基准测试来解决这些问题。", "method": "介绍了一种名为StaticEmbodiedBench的新基准测试，它使用静态场景表示来进行统一评估，覆盖了42个多样化的场景和8个核心维度，通过一个简单的接口支持可扩展性和全面性评估。", "result": "评估了19个视觉-语言模型（VLMs）和11个视觉-语言-动作模型（VLAs），建立了首个针对具身智能的统一静态排行榜，并发布了一个包含200个样本的子集以加速具身智能的发展。", "conclusion": "通过引入StaticEmbodiedBench，提供了对具身智能模型的统一、可扩展且全面的评估方法，并促进了该领域的研究与发展。"}}
{"id": "2508.06877", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06877", "abs": "https://arxiv.org/abs/2508.06877", "authors": ["Xiaobo Zhang", "Congqing He", "Ying He", "Jian Peng", "Dajie Fu", "Tien-Ping Tan"], "title": "ESNERA: Empirical and semantic named entity alignment for named entity dataset merging", "comment": "30 pages, 12 figures", "summary": "Named Entity Recognition (NER) is a fundamental task in natural language\nprocessing. It remains a research hotspot due to its wide applicability across\ndomains. Although recent advances in deep learning have significantly improved\nNER performance, they rely heavily on large, high-quality annotated datasets.\nHowever, building these datasets is expensive and time-consuming, posing a\nmajor bottleneck for further research. Current dataset merging approaches\nmainly focus on strategies like manual label mapping or constructing label\ngraphs, which lack interpretability and scalability. To address this, we\npropose an automatic label alignment method based on label similarity. The\nmethod combines empirical and semantic similarities, using a greedy pairwise\nmerging strategy to unify label spaces across different datasets. Experiments\nare conducted in two stages: first, merging three existing NER datasets into a\nunified corpus with minimal impact on NER performance; second, integrating this\ncorpus with a small-scale, self-built dataset in the financial domain. The\nresults show that our method enables effective dataset merging and enhances NER\nperformance in the low-resource financial domain. This study presents an\nefficient, interpretable, and scalable solution for integrating multi-source\nNER corpora.", "AI": {"tldr": "该文开发了一种自动化的标签对齐方法，用于合并多个NER数据集，同时提升了特定领域识别性能。", "motivation": "鉴于构建高质量标注数据集成本高昂且耗时，限制了进一步研究，而现有的数据集合并方法在可解释性和可扩展性方面不足，因此提议了一种新方法来解决这个问题。", "method": "该论文提出了一种基于标签相似性的自动标签对齐方法，结合了经验相似性和语义相似性，并采用了一种贪心的两两合并策略来统一不同数据集的标签空间。", "result": "实验分为两个阶段，第一阶段将三个现有的NER数据集合并成一个统一的语料库，对NER性能的影响极小；第二阶段将合并的语料库与一个小规模的自建金融领域数据集整合。结果显示，该方法能够实现有效的数据集合并，并提升了金融领域低资源环境下的NER性能。", "conclusion": "本研究提出了一种高效、可解释且可扩展的多源NER语料库整合方案。"}}
{"id": "2508.06555", "categories": ["cs.CV", "cs.CY", "cs.MA"], "pdf": "https://arxiv.org/pdf/2508.06555", "abs": "https://arxiv.org/abs/2508.06555", "authors": ["Hongbo Ma", "Fei Shen", "Hongbin Xu", "Xiaoce Wang", "Gang Xu", "Jinkai Zheng", "Liangqiong Qu", "Ming Li"], "title": "StyleTailor: Towards Personalized Fashion Styling via Hierarchical Negative Feedback", "comment": "24pages, 5 figures", "summary": "The advancement of intelligent agents has revolutionized problem-solving\nacross diverse domains, yet solutions for personalized fashion styling remain\nunderexplored, which holds immense promise for promoting shopping experiences.\nIn this work, we present StyleTailor, the first collaborative agent framework\nthat seamlessly unifies personalized apparel design, shopping recommendation,\nvirtual try-on, and systematic evaluation into a cohesive workflow. To this\nend, StyleTailor pioneers an iterative visual refinement paradigm driven by\nmulti-level negative feedback, enabling adaptive and precise user alignment.\nSpecifically, our framework features two core agents, i.e., Designer for\npersonalized garment selection and Consultant for virtual try-on, whose outputs\nare progressively refined via hierarchical vision-language model feedback\nspanning individual items, complete outfits, and try-on efficacy.\nCounterexamples are aggregated into negative prompts, forming a closed-loop\nmechanism that enhances recommendation quality.To assess the performance, we\nintroduce a comprehensive evaluation suite encompassing style consistency,\nvisual quality, face similarity, and artistic appraisal. Extensive experiments\ndemonstrate StyleTailor's superior performance in delivering personalized\ndesigns and recommendations, outperforming strong baselines without negative\nfeedback and establishing a new benchmark for intelligent fashion systems.", "AI": {"tldr": "本文探讨了时尚风格定制的智能解决方案，提出名为StyleTailor的协作代理框架，通过多层次负面反馈机制提升个性化服饰推荐的质量。", "motivation": "尽管智能代理在解决多个领域的问题上取得了进展，但个性化的时尚穿搭解决方案仍少有人问津，这充满潜力地促进了购物体验。", "method": "本文提出了StyleTailor，这是一种创新的协作代理框架，它将个性化服装设计、购物推荐、虚拟试穿和系统的评估整合为一个整体工作流。此框架引入了基于多层次负面反馈的迭代视觉细化范式，以实现对用户的自适应和精准对齐。", "result": "实验结果表明，StyleTailor在提供个性化设计和推荐方面表现出色，超越了没有负面反馈的强基线模型，确立了智能时尚系统的新的基准。", "conclusion": "通过引入多层次负面反馈机制和全面的评估套件，StyleTailor证明了其在智能时尚系统中的卓越性能，展现出其在消费者购物体验方面的巨大潜力。"}}
