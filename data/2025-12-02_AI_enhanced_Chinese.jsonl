{"id": "2512.00008", "categories": ["cs.CV", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2512.00008", "abs": "https://arxiv.org/abs/2512.00008", "authors": ["Veeramani Pugazhenthi", "Wei-Hsiang Chu", "Junwei Lu", "Jadyn N. Miyahira", "Soheil Salehi"], "title": "MOTION: ML-Assisted On-Device Low-Latency Motion Recognition", "comment": null, "summary": "The use of tiny devices capable of low-latency gesture recognition is gaining momentum in everyday human-computer interaction and especially in medical monitoring fields. Embedded solutions such as fall detection, rehabilitation tracking, and patient supervision require fast and efficient tracking of movements while avoiding unwanted false alarms. This study presents an efficient solution on how to build very efficient motion-based models only using triaxial accelerometer sensors. We explore the capability of the AutoML pipelines to extract the most important features from the data segments. This approach also involves training multiple lightweight machine learning algorithms using the extracted features. We use WeBe Band, a multi-sensor wearable device that is equipped with a powerful enough MCU to effectively perform gesture recognition entirely on the device. Of the models explored, we found that the neural network provided the best balance between accuracy, latency, and memory use. Our results also demonstrate that reliable real-time gesture recognition can be achieved in WeBe Band, with great potential for real-time medical monitoring solutions that require a secure and fast response time.", "AI": {"tldr": "本研究利用三轴加速度传感器和自动化机器学习管道开发了高效的基于运动的手势识别模型，特别适用于实时医疗监控，其中神经网络模型展现出了最佳的性能平衡。", "motivation": "为了在日常人机交互和医疗监测领域提供快速、高效且避免误报的手势识别功能。", "method": "使用自动机器学习管道从数据中提取最关键特征，并训练多个轻量级机器学习算法，基于WeBe Band多传感器可穿戴设备进行实验。", "result": "在WeBe Band上实现了可靠的实时手势识别，证明其在低延迟和低内存使用方面达到了良好平衡。", "conclusion": "实验表明神经网络模型在WeBe Band设备上的手势识别任务中表现出色，具有广泛应用于医疗监控的潜力。"}}
{"id": "2512.00042", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2512.00042", "abs": "https://arxiv.org/abs/2512.00042", "authors": ["Egemen Sert", "Şeyda Ertekin"], "title": "Closing the Gap: Data-Centric Fine-Tuning of Vision Language Models for the Standardized Exam Questions", "comment": null, "summary": "Multimodal reasoning has become a cornerstone of modern AI research. Standardized exam questions offer a uniquely rigorous testbed for such reasoning, providing structured visual contexts and verifiable answers. While recent progress has largely focused on algorithmic advances such as reinforcement learning (e.g., GRPO, DPO), the data centric foundations of vision language reasoning remain less explored.\n  We show that supervised fine-tuning (SFT) with high-quality data can rival proprietary approaches. To this end, we compile a 161.4 million token multimodal dataset combining textbook question-solution pairs, curriculum aligned diagrams, and contextual materials, and fine-tune Qwen-2.5VL-32B using an optimized reasoning syntax (QMSA). The resulting model achieves 78.6% accuracy, only 1.0% below Gemini 2.0 Flash, on our newly released benchmark YKSUniform, which standardizes 1,854 multimodal exam questions across 309 curriculum topics.\n  Our results reveal that data composition and representational syntax play a decisive role in multimodal reasoning. This work establishes a data centric framework for advancing open weight vision language models, demonstrating that carefully curated and curriculum-grounded multimodal data can elevate supervised fine-tuning to near state-of-the-art performance.", "AI": {"tldr": "研究展示了高质量数据对多模态推理的重要性，通过精美构建的数据集和优化推理语法的模型微调，取得了接近顶尖水平的性能。", "motivation": "该项工作的动机在于展示高质量数据在多模态视觉语言推理中的重要性，并构建一个以数据为中心的框架来提升开放权重的语言视觉模型。", "method": "本文提出了一种通过高质量数据的监督微调（SFT）来提升多模态推理的能力，构建了一个包含1.614亿个token的多模态数据集，集成了教科书的问题解答对、与课程对齐的图示以及上下文材料，并使用优化的推理语法（QMSA）对Qwen-2.5VL-32B模型进行微调。", "result": "实验结果表明，使用所构建的数据集和优化的推理语法微调后的模型在新发布的YKSUniform基准上的准确率达到了78.6%，仅比Gemini 2.0 Flash低1.0%。", "conclusion": "该研究揭示了数据构成和表示语法在多模态推理中的决定性作用，并证明了通过精心策划和基于课程的多模态数据可以将监督微调提升到接近当前最优性能。"}}
{"id": "2512.00060", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.00060", "abs": "https://arxiv.org/abs/2512.00060", "authors": ["Abdolazim Rezaei", "Mehdi Sookhak"], "title": "PEFT-DML: Parameter-Efficient Fine-Tuning Deep Metric Learning for Robust Multi-Modal 3D Object Detection in Autonomous Driving", "comment": null, "summary": "This study introduces PEFT-DML, a parameter-efficient deep metric learning framework for robust multi-modal 3D object detection in autonomous driving. Unlike conventional models that assume fixed sensor availability, PEFT-DML maps diverse modalities (LiDAR, radar, camera, IMU, GNSS) into a shared latent space, enabling reliable detection even under sensor dropout or unseen modality class combinations. By integrating Low-Rank Adaptation (LoRA) and adapter layers, PEFT-DML achieves significant training efficiency while enhancing robustness to fast motion, weather variability, and domain shifts. Experiments on benchmarks nuScenes demonstrate superior accuracy.", "AI": {"tldr": "PEFT-DML是一种高效的深度度量学习框架，它通过将多种传感器数据（如LiDAR、雷达、摄像机、IMU、GNSS）映射到共享的潜在空间，实现了在自动驾驶中鲁棒的多模态3D目标检测。", "motivation": "解决传统模型对固定传感器可用性的依赖，提高传感器故障或未见模态组合下的检测可靠性。", "method": "采用低秩适应(LoRA)和适配器层技术，PEFT-DML在提高训练效率的同时，增强了对快速运动、天气变化和域迁移的鲁棒性。", "result": "在nuScenes基准数据集上展示了优越的准确性。", "conclusion": "PEFT-DML在多模态3D物体检测方面表现出色，证明了它在复杂环境下的有效性和鲁棒性，特别是对于传感器多样性和不稳定网络环境。"}}
{"id": "2512.00046", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.00046", "abs": "https://arxiv.org/abs/2512.00046", "authors": ["Angelina Parfenova", "Andreas Marfurt", "Alexander Denzler", "Juergen Pfeffer"], "title": "Text Annotation via Inductive Coding: Comparing Human Experts to LLMs in Qualitative Data Analysis", "comment": null, "summary": "This paper investigates the automation of qualitative data analysis, focusing on inductive coding using large language models (LLMs). Unlike traditional approaches that rely on deductive methods with predefined labels, this research investigates the inductive process where labels emerge from the data. The study evaluates the performance of six open-source LLMs compared to human experts. As part of the evaluation, experts rated the perceived difficulty of the quotes they coded. The results reveal a peculiar dichotomy: human coders consistently perform well when labeling complex sentences but struggle with simpler ones, while LLMs exhibit the opposite trend. Additionally, the study explores systematic deviations in both human and LLM generated labels by comparing them to the golden standard from the test set. While human annotations may sometimes differ from the golden standard, they are often rated more favorably by other humans. In contrast, some LLMs demonstrate closer alignment with the true labels but receive lower evaluations from experts.", "AI": {"tldr": "This study compares the effectiveness of large language models (LLMs) and human experts in inductive coding for qualitative data analysis, revealing a dichotomy in their performance on complex versus simple sentences.", "motivation": "The research aims to automate qualitative data analysis, particularly the inductive coding process, and compare the outcomes of using LLMs to those generated by human experts.", "method": "The study compares six open-source large language models (LLMs) against human experts in the inductive coding process, which involves generating labels from qualitative data, rather than using predefined labels as in deductive methods. The human experts also rated the difficulty of coding various quotes.", "result": "The results show that human coders outperform LLMs on complex sentences but struggle with simpler ones, whereas LLMs perform better on simpler sentences but worse on complex ones. Despite sometimes aligning more closely with the golden standard, LLMs are often rated lower by human experts.", "conclusion": "The paper concludes that although LLMs can match or exceed human performance in certain aspects of qualitative data analysis, there are situations where human coders are still more effective, suggesting a need for further refinement and integration of AI in qualitative research."}}
{"id": "2512.00061", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.00061", "abs": "https://arxiv.org/abs/2512.00061", "authors": ["Pouya Shiri", "Amirali Baniasadi"], "title": "DL-CapsNet: A Deep and Light Capsule Network", "comment": null, "summary": "Capsule Network (CapsNet) is among the promising classifiers and a possible successor of the classifiers built based on Convolutional Neural Network (CNN). CapsNet is more accurate than CNNs in detecting images with overlapping categories and those with applied affine transformations. In this work, we propose a deep variant of CapsNet consisting of several capsule layers. In addition, we design the Capsule Summarization layer to reduce the complexity by reducing the number of parameters. DL-CapsNet, while being highly accurate, employs a small number of parameters and delivers faster training and inference. DL-CapsNet can process complex datasets with a high number of categories.", "AI": {"tldr": "提出了一种名为DL-CapsNet的深度胶囊网络变体，它在保持高精度的同时，减少了参数数量，实现了更快的训练和推理，适用于复杂数据集。", "motivation": "胶囊网络（CapsNet）被认为是比基于卷积神经网络（CNN）的分类器更准确的一种潜在替代方案，尤其是在检测具有重叠类别和应用仿射变换的图像方面。", "method": "本研究提出了一种深度变体的胶囊网络（CapsNet），包含多个胶囊层，并设计了一种胶囊总结层来降低参数复杂度。", "result": "DL-CapsNet在保持高准确率的同时，使用较少的参数，并提供更快的训练和推理速度。", "conclusion": "DL-CapsNet可以在保持高准确性的前提下，以较少的参数处理具有大量类别的复杂数据集。"}}
{"id": "2512.00047", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.00047", "abs": "https://arxiv.org/abs/2512.00047", "authors": ["Angelina Parfenova", "Alexander Denzler", "Juergen Pfeffer"], "title": "Emergent Convergence in Multi-Agent LLM Annotation", "comment": null, "summary": "Large language models (LLMs) are increasingly deployed in collaborative settings, yet little is known about how they coordinate when treated as black-box agents. We simulate 7500 multi-agent, multi-round discussions in an inductive coding task, generating over 125000 utterances that capture both final annotations and their interactional histories. We introduce process-level metrics: code stability, semantic self-consistency, and lexical confidence alongside sentiment and convergence measures, to track coordination dynamics. To probe deeper alignment signals, we analyze the evolving geometry of output embeddings, showing that intrinsic dimensionality declines over rounds, suggesting semantic compression. The results reveal that LLM groups converge lexically and semantically, develop asymmetric influence patterns, and exhibit negotiation-like behaviors despite the absence of explicit role prompting. This work demonstrates how black-box interaction analysis can surface emergent coordination strategies, offering a scalable complement to internal probe-based interpretability methods.", "AI": {"tldr": "本研究通过模拟多轮多代理讨论的场景，使用过程水平指标和情感收敛度量，揭示了语言模型在互动中展现出的不明确角色提示下的协调策略，表现为语义和词汇上的收敛及不对称影响力。", "motivation": "鉴于大型语言模型在协作场景中的部署越来越多，但关于这些被当作黑盒代理的语言模型如何进行协调的知识却很少，本研究旨在探究在没有明确角色提示的情况下，语言模型在互动中如何达成共识及发展出哪些策略。", "method": "本研究使用模拟的方法，设置7500个多方多轮的讨论场景，以归纳编码任务为例，引入过程水平的指标、情感和收敛度量来分析语言模型在互动中的协调策略，并通过考察输出嵌入的内在几何结构来进一步分析其语义特征的变化。", "result": "本研究通过模拟7500个多方多轮讨论，生成超过125000个实体，探讨了语言模型在归纳编码任务中的协作情况。研究引入了过程水平的指标如代码稳定性、语义自我一致性、词汇信心以及情感和收敛度量来跟踪协调动态。通过分析输出嵌入的内在几何结构，发现其内在维度在各轮次中下降，表明了语义压缩。结果显示，语言模型群体在词汇和语义上都有所收敛，形成了不对称影响力的模式，并表现出类似谈判的行为，尽管没有明确的角色提示。这项工作证明了黑盒互动分析可以揭示出新兴的协调策略，为内部探针式的可解释性方法提供了可扩展的补充。", "conclusion": "本研究揭示了语言模型群体在互动中具有词汇和语义上的收敛性，展现出不对称影响力的模式及类似谈判的行为，证明了黑盒互动分析可以适用于揭示新兴的协调策略，有助于深入理解语言模型的运作机制，并为可解释性方法提供了新的视角。"}}
{"id": "2512.00065", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.00065", "abs": "https://arxiv.org/abs/2512.00065", "authors": ["Sreesritha Sai", "Sai Venkata Suma Sreeja", "Deepthi", "Nikhil"], "title": "Satellite to Street : Disaster Impact Estimator", "comment": "11 pages,9 figures", "summary": "Accurate post-disaster damage assessment is of high importance for prioritizing emergency response; however, manual interpretation of satellite imagery is slow, subjective, and hard to scale. While deep-learning models for image segmentation, such as U-Net-based baselines and change-detection models, are useful baselines, they often struggle with subtle structural variations and severe class imbalance, yielding poor detection of highly damaged regions. The present work proposes a deep-learning framework that jointly processes pre- and post-disaster satellite images to obtain fine-grained pixel-level damage maps: Satellite-to-Street: Disaster Impact Estimator. The model uses a modified dual-input U-Net architecture with enhanced feature fusion to capture both the local structural changes as well as the broader contextual cues. Class-aware weighted loss functions are integrated in order to handle the dominance of undamaged pixels in real disaster datasets, thus enhancing sensitivity toward major and destroyed categories. Experimentation on publicly available disaster datasets shows improved localization and classification of structural damage when compared to traditional segmentation and baseline change-detection models. The resulting damage maps provide a rapid and consistent assessment mechanism to support and not replace expert decision-making, thus allowing more efficient, data-driven disaster management.", "AI": {"tldr": "本文介绍了一种基于改良双输入U-Net架构的深度学习框架Satellite-to-Street，对受灾区域的损毁程度进行更精准的评估。", "motivation": "准确的灾后损害评估对于优先考虑紧急响应具有重要意义，但利用卫星图像进行人工解释耗时、主观且难以扩展。现有的基于深度学习的图像分割模型通常在处理微妙的结构变化和严重的类别不平衡时表现不佳。", "method": "本研究提出了一种基于深度学习的框架，通过联合处理灾前和灾后卫星图像来获得精细的像素级损害地图。框架采用修改后的双输入U-Net架构，增强特征融合，以捕捉局部结构变化和更广泛的情境线索，并整合了类别感知加权损失函数，以解决实际灾难数据集中未受损像素占主导的问题。", "result": "实验在公开可用的灾难数据集上进行，结果显示相较于传统的分割和基线变动检测模型，该模型在结构损害的定位和分类方面表现更好。", "conclusion": "由此产生的损害地图提供了一种快速且一致的评估机制，以支持（且不取代）专家决策，从而能够更有效地进行基于数据的灾难管理。"}}
{"id": "2512.00204", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.00204", "abs": "https://arxiv.org/abs/2512.00204", "authors": ["Jason Lunder"], "title": "Tree Matching Networks for Natural Language Inference: Parameter-Efficient Semantic Understanding via Dependency Parse Trees", "comment": "16 pages, preprint", "summary": "In creating sentence embeddings for Natural Language Inference (NLI) tasks, using transformer-based models like BERT leads to high accuracy, but require hundreds of millions of parameters. These models take in sentences as a sequence of tokens, and learn to encode the meaning of the sequence into embeddings such that those embeddings can be used reliably for NLI tasks. Essentially, every word is considered against every other word in the sequence, and the transformer model is able to determine the relationships between them, entirely from scratch. However, a model that accepts explicit linguistic structures like dependency parse trees may be able to leverage prior encoded information about these relationships, without having to learn them from scratch, thus improving learning efficiency. To investigate this, we adapt Graph Matching Networks (GMN) to operate on dependency parse trees, creating Tree Matching Networks (TMN). We compare TMN to a BERT based model on the SNLI entailment task and on the SemEval similarity task. TMN is able to achieve significantly better results with a significantly reduced memory footprint and much less training time than the BERT based model on the SNLI task, while both models struggled to preform well on the SemEval. Explicit structural representations significantly outperform sequence-based models at comparable scales, but current aggregation methods limit scalability. We propose multi-headed attention aggregation to address this limitation.", "AI": {"tldr": "The paper proposes Tree Matching Networks (TMN) based on dependency parse trees for creating sentence embeddings. TMN outperforms BERT on SNLI with less resources but both struggle on SemEval, suggesting that structured representations enhance efficiency, although scaling is a challenge.", "motivation": "The motivation is to propose a model that can leverage prior encoded information about word relations from dependency parse trees, leading to a higher learning efficiency than a model like BERT which needs to learn relationships from scratch.", "method": "The paper adapts Graph Matching Networks (GMN) to operate on dependency parse trees, creating Tree Matching Networks (TMN). It compares TMN with a BERT-based model on the SNLI entailment task and the SemEval similarity task.", "result": "TMN achieves significantly better results with reduced memory use and shorter training time than the BERT-based model on the SNLI entailment task. Both models struggled on the SemEval task. Explicit structural representations significantly outperform sequence-based models at comparable scales.", "conclusion": "The study concludes that explicit structural representations outperform sequence-based models at comparable scales, but there are scalability issues with current aggregation methods, for which they propose a multi-headed attention aggregation as a solution."}}
{"id": "2512.00073", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.00073", "abs": "https://arxiv.org/abs/2512.00073", "authors": ["Aswinkumar Varathakumaran", "Nirmala Paramanandham"], "title": "ProvRain: Rain-Adaptive Denoising and Vehicle Detection via MobileNet-UNet and Faster R-CNN", "comment": null, "summary": "Provident vehicle detection has a lot of scope in the detection of vehicle during night time. The extraction of features other than the headlamps of vehicles allows us to detect oncoming vehicles before they appear directly on the camera. However, it faces multiple issues especially in the field of night vision, where a lot of noise caused due to weather conditions such as rain or snow as well as camera conditions. This paper focuses on creating a pipeline aimed at dealing with such noise while at the same time maintaining the accuracy of provident vehicular detection. The pipeline in this paper, ProvRain, uses a lightweight MobileNet-U-Net architecture tuned to generalize to robust weather conditions by using the concept of curricula training. A mix of synthetic as well as available data from the PVDN dataset is used for this. This pipeline is compared to the base Faster RCNN architecture trained on the PVDN dataset to see how much the addition of a denoising architecture helps increase the detection model's performance in rainy conditions. The system boasts an 8.94\\% increase in accuracy and a 10.25\\% increase in recall in the detection of vehicles in rainy night time frames. Similarly, the custom MobileNet-U-Net architecture that was trained also shows a 10-15\\% improvement in PSNR, a 5-6\\% increase in SSIM, and upto a 67\\% reduction in perceptual error (LPIPS) compared to other transformer approaches.", "AI": {"tldr": "该论文提出了一种名为ProvRain的框架，它利用MobileNet-U-Net架构处理夜间降雪或下雨时的噪声问题，提高了车辆检测的准确性、召回率和图像质量。", "motivation": "夜间车辆检测在没有其他特征只凭车灯的情况下存在困难，特别是在雨雪天气或相机条件不佳的情况下。该论文旨在解决这些问题，并改善夜间车辆检测的准确性。", "method": "ProvRain pipeline利用轻量级的MobileNet-U-Net架构结合课程学习的概念来处理夜间降雪或下雨等天气条件下的噪声问题，同时保持车辆检测的准确性。该方法结合了合成数据和PVDN数据集的可用数据。", "result": "相比仅使用PVDN数据集训练的Faster RCNN架构，ProvRain在雨夜检测的准确性上提高了8.94%，召回率提高了10.25%。定制的MobileNet-U-Net架构也显示出在PSNR上提高了10-15%，SSIM上提高了5-6%，以及在感知误差（LPIPS）上降低了67%。", "conclusion": "该研究证明了在夜间降雪或下雨等具有挑战性的条件下使用ProvRain进行车辆检测的有效性，该方法能够显著提高检测的准确性、召回率和图像质量评价指标。"}}
{"id": "2512.00214", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.00214", "abs": "https://arxiv.org/abs/2512.00214", "authors": ["Matej Klemen", "Tjaša Arčon", "Luka Terčon", "Marko Robnik-Šikonja", "Kaja Dobrovoljc"], "title": "Towards Corpus-Grounded Agentic LLMs for Multilingual Grammatical Analysis", "comment": "Pre-print, submission under review", "summary": "Empirical grammar research has become increasingly data-driven, but the systematic analysis of annotated corpora still requires substantial methodological and technical effort. We explore how agentic large language models (LLMs) can streamline this process by reasoning over annotated corpora and producing interpretable, data-grounded answers to linguistic questions. We introduce an agentic framework for corpus-grounded grammatical analysis that integrates concepts such as natural-language task interpretation, code generation, and data-driven reasoning. As a proof of concept, we apply it to Universal Dependencies (UD) corpora, testing it on multilingual grammatical tasks inspired by the World Atlas of Language Structures (WALS). The evaluation spans 13 word-order features and over 170 languages, assessing system performance across three complementary dimensions - dominant-order accuracy, order-coverage completeness, and distributional fidelity - which reflect how well the system generalizes, identifies, and quantifies word-order variations. The results demonstrate the feasibility of combining LLM reasoning with structured linguistic data, offering a first step toward interpretable, scalable automation of corpus-based grammatical inquiry.", "AI": {"tldr": "本文探讨了代理大型语言模型如何通过推理注释语料库并生成可解释的答案来简化语法分析过程。", "motivation": "尽管经验语法研究越来越数据驱动，但对注释语料库的系统分析仍然需要大量的方法论和技术投入。本文旨在探讨代理大型语言模型如何简化这一过程。", "method": "本文提出了一种基于代理的框架，用于语料库驱动的语法分析，该框架结合了自然语言任务解释、代码生成和数据驱动推理等概念，并将其应用于Universal Dependencies (UD)语料库中，测试了受WALS启发的多语言语法任务。", "result": "实验评估了13个词序特征和超过170种语言，从三个互补维度——主要词序准确性、词序覆盖率完整性和分布忠实度——衡量系统的性能。结果证明了将大型语言模型推理与结构化的语言数据相结合的可行性。", "conclusion": "研究结果表明了将LLM推理与结构化语言数据结合应用于语料库驱动语法研究的可行性，为实现可解释和可扩展的基于语料库的语法查询自动化提供了一步。"}}
{"id": "2512.00075", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2512.00075", "abs": "https://arxiv.org/abs/2512.00075", "authors": ["Jun Jia", "Hongyi Miao", "Yingjie Zhou", "Wangqiu Zhou", "Jianbo Zhang", "Linhan Cao", "Dandan Zhu", "Hua Yang", "Xiongkuo Min", "Wei Sun", "Guangtao Zhai"], "title": "Adapter Shield: A Unified Framework with Built-in Authentication for Preventing Unauthorized Zero-Shot Image-to-Image Generation", "comment": null, "summary": "With the rapid progress in diffusion models, image synthesis has advanced to the stage of zero-shot image-to-image generation, where high-fidelity replication of facial identities or artistic styles can be achieved using just one portrait or artwork, without modifying any model weights. Although these techniques significantly enhance creative possibilities, they also pose substantial risks related to intellectual property violations, including unauthorized identity cloning and stylistic imitation. To counter such threats, this work presents Adapter Shield, the first universal and authentication-integrated solution aimed at defending personal images from misuse in zero-shot generation scenarios. We first investigate how current zero-shot methods employ image encoders to extract embeddings from input images, which are subsequently fed into the UNet of diffusion models through cross-attention layers. Inspired by this mechanism, we construct a reversible encryption system that maps original embeddings into distinct encrypted representations according to different secret keys. The authorized users can restore the authentic embeddings via a decryption module and the correct key, enabling normal usage for authorized generation tasks. For protection purposes, we design a multi-target adversarial perturbation method that actively shifts the original embeddings toward designated encrypted patterns. Consequently, protected images are embedded with a defensive layer that ensures unauthorized users can only produce distorted or encrypted outputs. Extensive evaluations demonstrate that our method surpasses existing state-of-the-art defenses in blocking unauthorized zero-shot image synthesis, while supporting flexible and secure access control for verified users.", "AI": {"tldr": "The authors introduce Adapter Shield, a security framework for diffusion models that employs encryption and adversarial perturbation techniques to safeguard against unauthorized zero-shot image synthesis, allowing only authenticated users to generate images.", "motivation": "The main motivation is to address the growing concerns around the misuse of personal images and artistic works in the context of zero-shot image synthesis, by providing a universal and flexible way to authenticate and secure image usage through encryption.", "method": "The paper develops Adapter Shield, a novel security system for diffusion models that encrypts image embeddings, ensuring only authorized users with the correct key can decrypt and use the images for legitimate zero-shot generation tasks. The method uses a multi-target adversarial perturbation to protect the original embeddings by embedding them into encrypted patterns.", "result": "The experimental results suggest that Adapter Shield effectively prevents unauthorized generation of images, outperforming existing methods in defending image misuse while still allowing secure access for authorized users.", "conclusion": "The paper concludes that Adapter Shield is an effective defense mechanism against the misuse of personal images and artistic works in zero-shot diffusion models, delivering a secure and versatile authentication solution."}}
{"id": "2512.00219", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.00219", "abs": "https://arxiv.org/abs/2512.00219", "authors": ["Akhil Rajeev P"], "title": "Minimal-Edit Instruction Tuning for Low-Resource Indic GEC", "comment": "Submitted to AACL-IJCNLP Bhasha Workshop Shared Task1 :GEC", "summary": "Grammatical error correction for Indic languages faces limited supervision, diverse scripts, and rich morphology. We propose an augmentation-free setup that uses instruction-tuned large language models and conservative decoding. A 12B GEMMA 3 model is instruction-tuned in bnb 4-bit precision with parameter-efficient fine-tuning (PEFT) and Alpaca-style formatting. Decoding follows a deterministic, constraint-aware procedure with a lightweight normaliser that encourages minimal, meaning-preserving edits. We operationalise inference, subsequent to instruction fine-tuning (IFT), via a fixed, language-specific prompt directly synthesised from a deterministic error classifier's taxonomy, label distributions, and precedence ordering computed on the training corpus.\n  Under the official untuned GLEU evaluation, the system scores 92.41 on Malayalam, sixth overall, and 81.44 on Hindi, third overall. These results indicate that classifier-informed prompt design, adapter-based instruction tuning, and deterministic decoding provide a reproducible and a computationally efficient alternative to augmentation-centred pipelines for Indic GEC. The approach also motivates future work on stronger morphosyntactic constraints and human-centred evaluation of conservative edits.", "AI": {"tldr": "该研究提出了一种无需数据扩增的语法错误纠正方法，采用大规模语言模型和保守解码策略，结果表明这种方法在印度语系语言上的性能具有竞争优势。", "motivation": "研究动机在于解决印度语言的语法错误纠正问题，因为该领域存在监督不足、脚本多样和丰富的形态等问题。本研究旨在探索无需数据扩增的方法，以提高语法错误纠正任务的效果。", "method": "该研究提出了一种无需数据扩增的设置，利用经过指令调优的大语言模型和保守解码策略。研究使用经过bnb 4位精度参数高效微调（PEFT）的12B GEMMA 3模型，并采用Alpaca样式的格式化处理。解码过程遵循确定性的、考虑约束的程序，并采用一种轻量级归一化方法，鼓励进行最小而意义保持的编辑。", "result": "在官方未调整的GLEU评估中，该系统的马来alam分数为92.41，位列第六，Hindi分数为81.44，位列第三。这些结果表明，基于分类器的提示设计、基于适配器的指令调优以及确定性解码为印度语GEC提供了一种可重复且计算高效的替代方案。", "conclusion": "该研究结论表明，分类器知情的提示设计、基于适配器的指令调优和确定性解码提供了一个计算效率高且可重复的替代方案作为中心扩增管道。这也激发了未来在形态句法约束和人类中心化保守编辑评估方面的工作。"}}
{"id": "2512.00078", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.00078", "abs": "https://arxiv.org/abs/2512.00078", "authors": ["Mario de Jesus da Graca", "Jörg Dahlkemper", "Peer Stelldinger"], "title": "Diffusion-Based Synthetic Brightfield Microscopy Images for Enhanced Single Cell Detection", "comment": null, "summary": "Accurate single cell detection in brightfield microscopy is crucial for biological research, yet data scarcity and annotation bottlenecks limit the progress of deep learning methods. We investigate the use of unconditional models to generate synthetic brightfield microscopy images and evaluate their impact on object detection performance. A U-Net based diffusion model was trained and used to create datasets with varying ratios of synthetic and real images. Experiments with YOLOv8, YOLOv9 and RT-DETR reveal that training with synthetic data can achieve improved detection accuracies (at minimal costs). A human expert survey demonstrates the high realism of generated images, with experts not capable to distinguish them from real microscopy images (accuracy 50%). Our findings suggest that diffusion-based synthetic data generation is a promising avenue for augmenting real datasets in microscopy image analysis, reducing the reliance on extensive manual annotation and potentially improving the robustness of cell detection models.", "AI": {"tldr": "研究使用扩散模型生成合成明场显微镜图像，并用它们来增强真实数据集，改善了细胞检测模型的准确性，并展示了这种生成技术的潜力，降低了对精细手动注释的依赖。", "motivation": "由于数据稀缺和注释瓶颈，限制了深度学习方法在准确检测单细胞方面的进展。本研究旨在通过使用无条件模型生成合成明场显微镜图像，来解决这些问题，并评估这些图像对目标检测性能的影响。", "method": "本研究采用了一种基于U-Net的扩散模型来生成合成的明场显微镜图像，并测试了这种合成图像对目标检测性能的影响。通过YOLOv8、YOLOv9和RT-DETR等检测模型的实验，展示合成数据提高检测精度的效果。", "result": "实验结果表明，使用生成的合成数据进行训练可以以极低的成本提高检测精度。同时，专家调查证实了生成的图像具有极高的现实感，专家无法将它们与真实的显微镜图像区分开来（识别准确率为50%）。", "conclusion": "研究结论表明，基于扩散模型的合成数据生成是增强显微镜图像分析数据集的一个有前景的方法，可以减少对手动标注的依赖，并可能提高细胞检测模型的鲁棒性。"}}
{"id": "2512.00234", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.00234", "abs": "https://arxiv.org/abs/2512.00234", "authors": ["Sai Koneru", "Matthias Huck", "Jan Niehues"], "title": "OmniFusion: Simultaneous Multilingual Multimodal Translations via Modular Fusion", "comment": "Preprint for ACL 2026", "summary": "There has been significant progress in open-source text-only translation large language models (LLMs) with better language coverage and quality. However, these models can be only used in cascaded pipelines for speech translation (ST), performing automatic speech recognition first followed by translation. This introduces additional latency, which is particularly critical in simultaneous ST (SimulST), and prevents the model from exploiting multimodal context, such as images, which can aid disambiguation. Pretrained multimodal foundation models (MMFMs) already possess strong perception and reasoning capabilities across multiple modalities, but generally lack the multilingual coverage and specialized translation performance of dedicated translation LLMs. To build an effective multimodal translation system, we propose an end-to-end approach that fuses MMFMs with translation LLMs. We introduce a novel fusion strategy that connects hidden states from multiple layers of a pretrained MMFM to a translation LLM, enabling joint end-to-end training. The resulting model, OmniFusion, built on Omni 2.5-7B as the MMFM and SeedX PPO-7B as the translation LLM, can perform speech-to-text, speech-and-image-to-text, and text-and-image-to-text translation. Experiments demonstrate that OmniFusion effectively leverages both audio and visual inputs, achieves a 1-second latency reduction in SimulST compared to cascaded pipelines and also improves the overall translation quality\\footnote{Code is available at https://github.com/saikoneru/OmniFusion}.", "AI": {"tldr": "本文提出一种端到端方法，融合多模态基础模型（MMFM）与翻译大型语言模型（LLM），名为OmniFusion，能实现语音文本翻译、语音图像翻译和文本图像翻译，提高了翻译质量和减少了延迟。", "motivation": "当前开源文本翻译大型语言模型在级联管道中使用自动语音识别和翻译操作，引入了额外的延迟，并无法利用多模态上下文。而预训练MMFM已具备强大的跨模态感知和推理能力，但是通常缺乏多语言覆盖和专用翻译性能。", "method": "提出了一种将多模态基础模型（MMFM）与专用翻译大型语言模型（LLM）融合的端到端方法。该方法介绍了一种新的融合策略，将预训练MMFM多层的隐藏状态与翻译LLM连接起来，实现了共同的端到端训练。", "result": "实验表明，OmniFusion有效利用了音频和视觉输入，与级联管道相比，SimulST的延迟减少了1秒，并且提高了整体翻译质量。", "conclusion": "OmniFusion模型有效地结合了语音和视觉输入，降低了SimulST的延迟，并提高了整体翻译质量。"}}
{"id": "2512.00080", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2512.00080", "abs": "https://arxiv.org/abs/2512.00080", "authors": ["André Dehne", "Juri Zach", "Peer Stelldinger"], "title": "Conceptual Evaluation of Deep Visual Stereo Odometry for the MARWIN Radiation Monitoring Robot in Accelerator Tunnels", "comment": null, "summary": "The MARWIN robot operates at the European XFEL to perform autonomous radiation monitoring in long, monotonous accelerator tunnels where conventional localization approaches struggle. Its current navigation concept combines lidar-based edge detection, wheel/lidar odometry with periodic QR-code referencing, and fuzzy control of wall distance, rotation, and longitudinal position. While robust in predefined sections, this design lacks flexibility for unknown geometries and obstacles. This paper explores deep visual stereo odometry (DVSO) with 3D-geometric constraints as a focused alternative. DVSO is purely vision-based, leveraging stereo disparity, optical flow, and self-supervised learning to jointly estimate depth and ego-motion without labeled data. For global consistency, DVSO can subsequently be fused with absolute references (e.g., landmarks) or other sensors. We provide a conceptual evaluation for accelerator tunnel environments, using the European XFEL as a case study. Expected benefits include reduced scale drift via stereo, low-cost sensing, and scalable data collection, while challenges remain in low-texture surfaces, lighting variability, computational load, and robustness under radiation. The paper defines a research agenda toward enabling MARWIN to navigate more autonomously in constrained, safety-critical infrastructures.", "AI": {"tldr": "论文探讨了深度视觉立体里程计（DVSO）结合三维几何约束作为替代现有导航方案的方法，减少了尺度漂移，降低了成本，但也面临一些挑战。", "motivation": "MARWIN 机器人现有的导航概念虽然在预定义的部分可靠，但在未知几何形状和障碍物面前缺乏灵活性。该论文旨在寻求一种更灵活、更适应复杂环境的导航解决方案。", "method": "该论文探索了一种基于深度视觉立体里程计（DVSO）的方法，结合三维几何约束，作为替代现有导航方案的集中式方法。DVSO 采用纯视觉方式，利用立体视差、光流和自监督学习共同估计深度和自我运动而不依赖标签数据。为了获得全局一致性，可以将 DVSO 与绝对参考（如地标）或其他传感器融合。", "result": "论文的目标是通过使用 DVSO 技术，减少尺度漂移，降低成本，提高数据收集的可扩展性。但仍存在诸如低纹理表面、光照变化、计算负载和辐射条件下的鲁棒性等挑战。", "conclusion": "论文定义了一个研究议程，目的在于增强 MARWIN 在受限和安全关键基础设施中更自主地导航的能力。"}}
{"id": "2512.00274", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.00274", "abs": "https://arxiv.org/abs/2512.00274", "authors": ["Prakrithi Shivaprakash", "Diptadhi Mukherjee", "Lekhansh Shukla", "Animesh Mukherjee", "Prabhat Chand", "Pratima Murthy"], "title": "Lost without translation -- Can transformer (language models) understand mood states?", "comment": "33 pages, 3 figures, 2 tables", "summary": "Background: Large Language Models show promise in psychiatry but are English-centric. Their ability to understand mood states in other languages is unclear, as different languages have their own idioms of distress. Aim: To quantify the ability of language models to faithfully represent phrases (idioms of distress) of four distinct mood states (depression, euthymia, euphoric mania, dysphoric mania) expressed in Indian languages. Methods: We collected 247 unique phrases for the four mood states across 11 Indic languages. We tested seven experimental conditions, comparing k-means clustering performance on: (a) direct embeddings of native and Romanised scripts (using multilingual and Indic-specific models) and (b) embeddings of phrases translated to English and Chinese. Performance was measured using a composite score based on Adjusted Rand Index, Normalised Mutual Information, Homogeneity and Completeness. Results: Direct embedding of Indic languages failed to cluster mood states (Composite Score = 0.002). All translation-based approaches showed significant improvement. High performance was achieved using Gemini-translated English (Composite=0.60) and human-translated English (Composite=0.61) embedded with gemini-001. Surprisingly, human-translated English, further translated into Chinese and embedded with a Chinese model, performed best (Composite = 0.67). Specialised Indic models (IndicBERT and Sarvam-M) performed poorly. Conclusion: Current models cannot meaningfully represent mood states directly from Indic languages, posing a fundamental barrier to their psychiatric application for diagnostic or therapeutic purposes in India. While high-quality translation bridges this gap, reliance on proprietary models or complex translation pipelines is unsustainable. Models must first be built to understand diverse local languages to be effective in global mental health.", "AI": {"tldr": "研究发现直接使用印地语嵌入无法有效分类情绪状态，而通过高质量翻译可以显著改善性能，但依靠复杂的翻译管道不是长久之计。研究建议开发能理解多种本地语言的模型以支持全球心理健康的应用。", "motivation": "研究动机在于评估语言模型理解印度语言中情绪状态表达的能力，并探索直接嵌入和翻译嵌入的表现差异。", "method": "研究团队收集了11种印度语言中的247个独特短语，这些短语代表了四种情绪状态。研究采用了七种不同的实验条件，包括直接嵌入和翻译到英文和中文的嵌入，使用k-means聚类算法性能作为评价标准。", "result": "结果显示，直接嵌入印度语言的表现很差，而翻译到英语和中文的方法显示出显著改善。特别是，使用Gemini翻译到英语和人工翻译到英语并进一步翻译到中文的方法表现最佳。", "conclusion": "当前的模型无法有效地从印度语言中识别情绪状态，这对他们在印度的心理诊断和治疗应用上构成了瓶颈。该研究建议开发能够理解多种本地语言的模型以促进全球心理健康研究的应用。"}}
{"id": "2512.00082", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.00082", "abs": "https://arxiv.org/abs/2512.00082", "authors": ["Divendar Murtadak", "Yoon Kim", "Trilokya Akula"], "title": "Exploring Diagnostic Prompting Approach for Multimodal LLM-based Visual Complexity Assessment: A Case Study of Amazon Search Result Pages", "comment": "9 pages, 4 figures, 9 tables. Study on diagnostic prompting for multimodal LLM-based visual complexity assessment of Amazon search result pages", "summary": "This study investigates whether diagnostic prompting can improve Multimodal Large Language Model (MLLM) reliability for visual complexity assessment of Amazon Search Results Pages (SRP). We compare diagnostic prompting with standard gestalt principles-based prompting using 200 Amazon SRP pages and human expert annotations. Diagnostic prompting showed notable improvements in predicting human complexity judgments, with F1-score increasing from 0.031 to 0.297 (+858\\% relative improvement), though absolute performance remains modest (Cohen's $κ$ = 0.071). The decision tree revealed that models prioritize visual design elements (badge clutter: 38.6\\% importance) while humans emphasize content similarity, suggesting partial alignment in reasoning patterns. Failure case analysis reveals persistent challenges in MLLM visual perception, particularly for product similarity and color intensity assessment. Our findings indicate that diagnostic prompting represents a promising initial step toward human-aligned MLLM-based evaluation, though failure cases with consistent human-MLLM disagreement require continued research and refinement in prompting approaches with larger ground truth datasets for reliable practical deployment.", "AI": {"tldr": "研究探讨了诊断提示是否能提升多模态大型语言模型（MLLM）在亚马逊搜索结果页面（SRP）视觉复杂性评估中的可靠性，并取得了显著的改进。", "motivation": "评估诊断提示与基于格式塔原则的提示在提高MLLM评估亚马逊SRP页面的视觉复杂性可靠性方面的差异。", "method": "比较诊断提示与标准格式塔原则提示的方法，使用200个亚马逊SRP页面和人类专家的注释进行评估。", "result": "使用诊断提示，F1-score从0.031提高到了0.297（相对提升了858%），尽管表现依旧偏低（Cohen's $\\kappa$ = 0.071）。", "conclusion": "研究发现，诊断提示是一个有前景的开始，但还需要通过更大规模的真实数据集研究和改进提示方法，以实现可靠的实践部署。"}}
{"id": "2512.00290", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.00290", "abs": "https://arxiv.org/abs/2512.00290", "authors": ["Guoqing Ma", "Jia Zhu", "Hanghui Guo", "Weijie Shi", "Yue Cui", "Jiawei Shen", "Zilong Li", "Yidan Liang"], "title": "EduEval: A Hierarchical Cognitive Benchmark for Evaluating Large Language Models in Chinese Education", "comment": null, "summary": "Large language models (LLMs) demonstrate significant potential for educational applications. However, their unscrutinized deployment poses risks to educational standards, underscoring the need for rigorous evaluation. We introduce EduEval, a comprehensive hierarchical benchmark for evaluating LLMs in Chinese K-12 education. This benchmark makes three key contributions: (1) Cognitive Framework: We propose the EduAbility Taxonomy, which unifies Bloom's Taxonomy and Webb's Depth of Knowledge to organize tasks across six cognitive dimensions including Memorization, Understanding, Application, Reasoning, Creativity, and Ethics. (2) Authenticity: Our benchmark integrates real exam questions, classroom conversation, student essays, and expert-designed prompts to reflect genuine educational challenges; (3) Scale: EduEval comprises 24 distinct task types with over 11,000 questions spanning primary to high school levels. We evaluate 14 leading LLMs under both zero-shot and few-shot settings, revealing that while models perform well on factual tasks, they struggle with classroom dialogue classification and exhibit inconsistent results in creative content generation. Interestingly, several open source models outperform proprietary systems on complex educational reasoning. Few-shot prompting shows varying effectiveness across cognitive dimensions, suggesting that different educational objectives require tailored approaches. These findings provide targeted benchmarking metrics for developing LLMs specifically optimized for diverse Chinese educational tasks.", "AI": {"tldr": "A hierarchical benchmark named EduEval is developed for assessing LLMs in Chinese K-12 education, focusing on cognitive abilities, educational authenticity and large-scale question pools. It reveals that models do well on factual tasks but struggle with classroom dialogue and creative content, indicating a need for more specialized approaches.", "motivation": "The motivation appears to evaluate the readiness and performance of large language models (LLMs) in Chinese K-12 educational contexts, emphasizing the necessity of a structured assessment approach to address potential risks and mitigate issues.", "method": "Paper analysis of the introduction and methodology section of the provided content is required to extract key analysis points.", "result": "Evaluation of 14 LLMs found strengths in factual knowledge tasks but noted limitations in dialogue and creative tasks, highlighting the variability in performance across different cognitive dimensions and tasks.", "conclusion": "The study underscores the need for developing distinct benchmarking standards for optimizing LLMs for educational tasks in Chinese K-12 educational settings, with open-source models surprisingly outperforming proprietary ones in complex reasoning tasks."}}
{"id": "2512.00084", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.00084", "abs": "https://arxiv.org/abs/2512.00084", "authors": ["Venkata Siddharth Dhara", "Pawan Kumar"], "title": "A Fast and Efficient Modern BERT based Text-Conditioned Diffusion Model for Medical Image Segmentation", "comment": "15 pages, 3 figures, Accepted in Slide 3 10th International Conference on Computer Vision & Image Processing (CVIP 2026)", "summary": "In recent times, denoising diffusion probabilistic models (DPMs) have proven effective for medical image generation and denoising, and as representation learners for downstream segmentation. However, segmentation performance is limited by the need for dense pixel-wise labels, which are expensive, time-consuming, and require expert knowledge. We propose FastTextDiff, a label-efficient diffusion-based segmentation model that integrates medical text annotations to enhance semantic representations. Our approach uses ModernBERT, a transformer capable of processing long clinical notes, to tightly link textual annotations with semantic content in medical images. Trained on MIMIC-III and MIMIC-IV, ModernBERT encodes clinical knowledge that guides cross-modal attention between visual and textual features. This study validates ModernBERT as a fast, scalable alternative to Clinical BioBERT in diffusion-based segmentation pipelines and highlights the promise of multi-modal techniques for medical image analysis. By replacing Clinical BioBERT with ModernBERT, FastTextDiff benefits from FlashAttention 2, an alternating attention mechanism, and a 2-trillion-token corpus, improving both segmentation accuracy and training efficiency over traditional diffusion-based models.", "AI": {"tldr": "FastTextDiff是一種標簽高效的分割模型，通過將ModernBERT與醫學圖像緊密結合來改進語義表示，提升了分割精度和訓練效率。", "motivation": "動機在于改善基于分塊標記的分段性能限制，通過使用標簽高效的FastTextDiff解決標記成本高昂且耗時的問題。", "method": "FastTextDiff 方法集成了醫學文本注釋以改進語義表示，利用ModernBERT處理長臨床記錄，將文本注釋緊密與醫學圖像的語義內容聯系起來。", "result": "ModernBERT證明了它可以作為擴散分段管道中Clinical BioBERT的快速可擴展的替代方案，並且通過替代Clinical BioBERT，FastTextDiff在分段精度和訓練效率方面有所提升。", "conclusion": "研究結果表明，使用多模態技術提高醫學圖像分析的潛力，以及FastTextDiff在提高分割真實性和訓練效率方面的有效性。"}}
{"id": "2512.00323", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.00323", "abs": "https://arxiv.org/abs/2512.00323", "authors": ["Muhammad Muneeb", "David B. Ascher", "Ahsan Baidar Bakht"], "title": "Comparative Analysis of 47 Context-Based Question Answer Models Across 8 Diverse Datasets", "comment": null, "summary": "Context-based question answering (CBQA) models provide more accurate and relevant answers by considering the contextual information. They effectively extract specific information given a context, making them functional in various applications involving user support, information retrieval, and educational platforms. In this manuscript, we benchmarked the performance of 47 CBQA models from Hugging Face on eight different datasets. This study aims to identify the best-performing model across diverse datasets without additional fine-tuning. It is valuable for practical applications where the need to retrain models for specific datasets is minimized, streamlining the implementation of these models in various contexts. The best-performing models were trained on the SQuAD v2 or SQuAD v1 datasets. The best-performing model was ahotrod/electra_large_discriminator_squad2_512, which yielded 43\\% accuracy across all datasets. We observed that the computation time of all models depends on the context length and the model size. The model's performance usually decreases with an increase in the answer length. Moreover, the model's performance depends on the context complexity. We also used the Genetic algorithm to improve the overall accuracy by integrating responses from other models. ahotrod/electra_large_discriminator_squad2_512 generated the best results for bioasq10b-factoid (65.92\\%), biomedical\\_cpgQA (96.45\\%), QuAC (11.13\\%), and Question Answer Dataset (41.6\\%). Bert-large-uncased-whole-word-masking-finetuned-squad achieved an accuracy of 82\\% on the IELTS dataset.", "AI": {"tldr": "研究评估了47个CBQA模型在八个数据集上的性能，指出ahotrod/electra_large_discriminator_squad2_512模型整体表现最佳，同时指出模型性能受上下文及答案长度影响。", "motivation": "这项研究为企业节约训练时间，加快这些模型在不同场景中的部署提供了便利。此外，研究还分析了模型性能与上下文长度、模型大小、答案长度和上下文复杂度之间的关系。", "method": "本文通过对Hugging Face上的47个基于上下文的问题回答(CBQA)模型进行基准测试，评估了这些模型在八个不同数据集上的表现。研究重点是没有额外微调的情况下，确定在各种数据集中表现最佳的模型。", "result": "最佳模型为ahotrod/electra_large_discriminator_squad2_512，在所有数据集中达到43%的准确率。该模型在bioasq10b-factoid、biomedical_cpgQA、QuAC和Question Answer Dataset数据集上分别获得了65.92%、96.45%、11.13%和41.6%的准确率。通过遗传算法整合其他模型的响应，进一步提高了整体准确率。", "conclusion": "研究展示了对不同数据集上CBQA模型性能的深入理解，突出了ahotrod/electra_large_discriminator_squad2_512模型的优势，并指出BERT-Base和BERT-Large模型在特定任务上的卓越表现。"}}
{"id": "2512.00086", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.00086", "abs": "https://arxiv.org/abs/2512.00086", "authors": ["Davide Nadalini", "Manuele Rusci", "Elia Cereda", "Luca Benini", "Francesco Conti", "Daniele Palossi"], "title": "Multi-modal On-Device Learning for Monocular Depth Estimation on Ultra-low-power MCUs", "comment": "14 pages, 9 figures, 3 tables. Associated open-source release available at: https://github.com/dnadalini/ondevice_learning_for_monocular_depth_estimation", "summary": "Monocular depth estimation (MDE) plays a crucial role in enabling spatially-aware applications in Ultra-low-power (ULP) Internet-of-Things (IoT) platforms. However, the limited number of parameters of Deep Neural Networks for the MDE task, designed for IoT nodes, results in severe accuracy drops when the sensor data observed in the field shifts significantly from the training dataset. To address this domain shift problem, we present a multi-modal On-Device Learning (ODL) technique, deployed on an IoT device integrating a Greenwaves GAP9 MicroController Unit (MCU), a 80 mW monocular camera and a 8 x 8 pixel depth sensor, consuming $\\approx$300mW. In its normal operation, this setup feeds a tiny 107 k-parameter $μ$PyD-Net model with monocular images for inference. The depth sensor, usually deactivated to minimize energy consumption, is only activated alongside the camera to collect pseudo-labels when the system is placed in a new environment. Then, the fine-tuning task is performed entirely on the MCU, using the new data. To optimize our backpropagation-based on-device training, we introduce a novel memory-driven sparse update scheme, which minimizes the fine-tuning memory to 1.2 MB, 2.2x less than a full update, while preserving accuracy (i.e., only 2% and 1.5% drops on the KITTI and NYUv2 datasets). Our in-field tests demonstrate, for the first time, that ODL for MDE can be performed in 17.8 minutes on the IoT node, reducing the root mean squared error from 4.9 to 0.6m with only 3 k self-labeled samples, collected in a real-life deployment scenario.", "AI": {"tldr": "本研究提出了一种应用于IoT设备的多模态设备端学习技术，用于解决单目深度估计任务中的领域变换问题，展示了在低功耗下保持高质量的推理精度的潜力。", "motivation": "鉴于IoT设备上的单目深度估计任务面临传感器观测数据领域变换的问题，导致准确性显著下降，本研究旨在通过设备端学习技术解决这一问题，以期在保持高效能耗的同时提高模型的准确性。", "method": "本研究采用了一种多模态的设备端学习（ODL）技术，集成在使用Greenwaves GAP9微控制器单元（MCU）的IoT设备上，包含一个80毫瓦的单目相机和一个8x8像素的深度传感器。此设备在正常使用时，向一个小型的107千参数的$\\mu$PyD-Net模型提供单目图像数据以进行推理。当设备置于新环境时，会激活深度传感器来收集伪标签，然后在MCU上完成重新训练任务。研究引入了一种基于内存驱动的稀疏更新方案，大大减少了重新训练的记忆需求，降低了内存消耗。", "result": "实验结果显示，首次在IoT设备上实现了单目深度估计的设备端学习，整个过程在17.8分钟内完成，使用3千个自标记样本，将实际部署场景下的均方根误差从4.9降低到了0.6米，减少了9%至12%的误差。", "conclusion": "通过对一种优化的内存驱动稀疏更新方案的应用，本研究成功在低功耗的IoT设备上实现了高效的单目深度估计模型的设备端学习，显著提高了在新环境下的模型精度，为IoT设备的空间感知应用打开了新的可能性。"}}
{"id": "2512.00329", "categories": ["cs.CL", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2512.00329", "abs": "https://arxiv.org/abs/2512.00329", "authors": ["Ashish Thanga", "Vibhu Dixit", "Abhilash Shankarampeta", "Vivek Gupta"], "title": "Evidence-Guided Schema Normalization for Temporal Tabular Reasoning", "comment": null, "summary": "Temporal reasoning over evolving semi-structured tables poses a challenge to current QA systems. We propose a SQL-based approach that involves (1) generating a 3NF schema from Wikipedia infoboxes, (2) generating SQL queries, and (3) query execution. Our central finding challenges model scaling assumptions: the quality of schema design has a greater impact on QA precision than model capacity. We establish three evidence-based principles: normalization that preserves context, semantic naming that reduces ambiguity, and consistent temporal anchoring. Our best configuration (Gemini 2.5 Flash schema + Gemini-2.0-Flash queries) achieves 80.39 EM, a 16.8\\% improvement over the baseline (68.89 EM).", "AI": {"tldr": "本文通过SQL方法处理随时间变化的半结构化数据的问答任务，强调了模式设计的重要性，并提出了三个改进问答系统精确性的原则。", "motivation": "目前的QA系统在处理随时间变化的半结构化表格时存在挑战，因此提出该方法。", "method": "提出了一种基于SQL的方法，该方法包括：(1)从维基百科信息框生成3NF (第三范式)模式，(2)生成SQL查询，(3)执行查询。", "result": "最好的配置（Gemini 2.5 Flash schema + Gemini-2.0-Flash queries）实现了80.39的精确匹配得分，比基础模型提高了16.8%（基础模型得分为68.89）。", "conclusion": "研究发现，在问答系统的准确性方面，模式设计的质量比模型容量的影响更大。且研究提出了三个基于证据的原则：保留上下文的规范化、减少歧义的语义命名、以及一致的时间锚定。"}}
{"id": "2512.00087", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.00087", "abs": "https://arxiv.org/abs/2512.00087", "authors": ["Ivo Bueno", "Ruikun Hou", "Babette Bühler", "Tim Fütterer", "James Drimalla", "Jonathan Kyle Foster", "Peter Youngs", "Peter Gerjets", "Ulrich Trautwein", "Enkelejda Kasneci"], "title": "Exploring Automated Recognition of Instructional Activity and Discourse from Multimodal Classroom Data", "comment": "This article has been accepted for publication in the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV) 2026", "summary": "Observation of classroom interactions can provide concrete feedback to teachers, but current methods rely on manual annotation, which is resource-intensive and hard to scale. This work explores AI-driven analysis of classroom recordings, focusing on multimodal instructional activity and discourse recognition as a foundation for actionable feedback. Using a densely annotated dataset of 164 hours of video and 68 lesson transcripts, we design parallel, modality-specific pipelines. For video, we evaluate zero-shot multimodal LLMs, fine-tuned vision-language models, and self-supervised video transformers on 24 activity labels. For transcripts, we fine-tune a transformer-based classifier with contextualized inputs and compare it against prompting-based LLMs on 19 discourse labels. To handle class imbalance and multi-label complexity, we apply per-label thresholding, context windows, and imbalance-aware loss functions. The results show that fine-tuned models consistently outperform prompting-based approaches, achieving macro-F1 scores of 0.577 for video and 0.460 for transcripts. These results demonstrate the feasibility of automated classroom analysis and establish a foundation for scalable teacher feedback systems.", "AI": {"tldr": "本研究通过使用密集标注的数据集，设计了用于视频和文本转录的平行、模态特定管道，表明AI驱动的课堂活动识别比手动标注更有效，为开发自动化的教师反馈系统提供了重要参考。", "motivation": "研究人员提出了用AI驱动的课堂记录分析方法，以观察课堂教学互动，为教师提供具体的反馈，并解决当前方法依赖于手动标注，资源强度大、难以扩展的问题。", "method": "此研究设计了并行的模态特定管道，用于处理视频和文本转录。视频处理部分使用了零样本多模态LLM、微调的视觉语言模型和自监督视频变压器。文本处理部分使用了基于变压器的分类器微调和提示驱动的LLM。为了处理类别不平衡和多标签复杂度，研究应用了每个标签的阈值调整、上下文窗口和不平衡感知的损失函数。", "result": "实验结果表明，微调的模型在视频和文本转录上分别达到了0.577和0.460的宏F1分数，优于提示驱动的方法。", "conclusion": "研究证明了自动课堂分析的可行性，并为可扩展的教师反馈系统奠定了基础。"}}
{"id": "2512.00332", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.00332", "abs": "https://arxiv.org/abs/2512.00332", "authors": ["Daud Waqas", "Aaryamaan Golthi", "Erika Hayashida", "Huanzhi Mao"], "title": "Assertion-Conditioned Compliance: A Provenance-Aware Vulnerability in Multi-Turn Tool-Calling Agents", "comment": "15 pages (incl. Appendix), 2 figures, 7 tables", "summary": "Multi-turn tool-calling LLMs (models capable of invoking external APIs or tools across several user turns) have emerged as a key feature in modern AI assistants, enabling extended dialogues from benign tasks to critical business, medical, and financial operations. Yet implementing multi-turn pipelines remains difficult for many safety-critical industries due to ongoing concerns regarding model resilience. While standardized benchmarks such as the Berkeley Function-Calling Leaderboard (BFCL) have underpinned confidence concerning advanced function-calling models (like Salesforce's xLAM V2), there is still a lack of visibility into multi-turn conversation-level robustness, especially given their exposure to real-world systems. In this paper, we introduce Assertion-Conditioned Compliance (A-CC), a novel evaluation paradigm for multi-turn function-calling dialogues. A-CC provides holistic metrics that evaluate a model's behavior when confronted with misleading assertions originating from two distinct vectors: (1) user-sourced assertions (USAs), which measure sycophancy toward plausible but misinformed user beliefs, and (2) function-sourced assertions (FSAs), which measure compliance with plausible but contradictory system policies (e.g., stale hints from unmaintained tools). Our results show that models are highly vulnerable to both USA sycophancy and FSA policy conflicts, confirming A-CC as a critical, latent vulnerability in deployed agents.", "AI": {"tldr": "文章提出了一种新的评估方法A-CC，用于评估多轮对话中的模型是否易于受到错误用户信息和系统政策冲突的影响。研究结果揭示了模型在这种情况下存在的高脆弱性。", "motivation": "由于缺乏多轮对话级别稳健性的可见性，安全关键行业面临实施多轮管道的挑战。因此，此研究旨在评估多轮对话中模型的潜在脆弱性。", "method": "引入了Assertion-Conditioned Compliance（A-CC），这是一个用于评估多轮函数调用对话的新颖评估范式。A-CC 提供了全面的指标来评估模型在面对用户来源的断言（USAs）和函数来源的断言（FSAs）时的行为。该方法通过两种途径来衡量模型的鲁棒性：一是用户可能存在的错误信息的影响，二是系统政策可能出现的冲突。", "result": "研究结果显示模型对USAs的奉承行为以及FSA中的政策冲突高度敏感，这验证了A-CC作为一种关键的潜在脆弱性在已部署代理中的有效性。", "conclusion": "A-CC作为评估多轮函数调用对话的新范式，揭示了现有模型在面对用户错误信息和系统政策冲突时的脆弱性，其结果表明了这种评估方法的重要性。"}}
{"id": "2512.00088", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.00088", "abs": "https://arxiv.org/abs/2512.00088", "authors": ["Mohammad Zare"], "title": "SemImage: Semantic Image Representation for Text, a Novel Framework for Embedding Disentangled Linguistic Features", "comment": null, "summary": "We propose SemImage, a novel method for representing a text document as a two-dimensional semantic image to be processed by convolutional neural networks (CNNs). In a SemImage, each word is represented as a pixel in a 2D image: rows correspond to sentences and an additional boundary row is inserted between sentences to mark semantic transitions. Each pixel is not a typical RGB value but a vector in a disentangled HSV color space, encoding different linguistic features: the Hue with two components H_cos and H_sin to account for circularity encodes the topic, Saturation encodes the sentiment, and Value encodes intensity or certainty. We enforce this disentanglement via a multi-task learning framework: a ColorMapper network maps each word embedding to the HSV space, and auxiliary supervision is applied to the Hue and Saturation channels to predict topic and sentiment labels, alongside the main task objective. The insertion of dynamically computed boundary rows between sentences yields sharp visual boundaries in the image when consecutive sentences are semantically dissimilar, effectively making paragraph breaks salient. We integrate SemImage with standard 2D CNNs (e.g., ResNet) for document classification. Experiments on multi-label datasets (with both topic and sentiment annotations) and single-label benchmarks demonstrate that SemImage can achieve competitive or better accuracy than strong text classification baselines (including BERT and hierarchical attention networks) while offering enhanced interpretability. An ablation study confirms the importance of the multi-channel HSV representation and the dynamic boundary rows. Finally, we present visualizations of SemImage that qualitatively reveal clear patterns corresponding to topic shifts and sentiment changes in the generated image, suggesting that our representation makes these linguistic features visible to both humans and machines.", "AI": {"tldr": "SemImage represents text documents as 2D images for CNN processing, using a disentangled HSV color space to encode linguistic features, achieving competitive classification accuracy with enhanced interpretability.", "motivation": "The aim is to enhance interpretability and efficiency of text classification while achieving competitive accuracy. The approach makes linguistic features visible in the image representation, which helps in understanding topic shifts and sentiment changes.", "method": "We propose SemImage, a novel method for representing text documents as 2D semantic images for processing with CNNs. Each word is a pixel, and sentences are separated by boundary rows to mark semantic transitions. HSV color space vectors represent different linguistic features, disentangled via multi-task learning, integrating the model with standard 2D CNNs for document classification.", "result": "Experiments on multi-label datasets and single-label benchmarks show that SemImage can achieve competitive or better accuracy compared to strong baselines like BERT and hierarchical attention networks. An ablation study validates the importance of the multi-channel HSV representation and dynamic boundary rows.", "conclusion": "SemImage effectively represents texts as visual images, providing a competitive method for document classification that not only matches but often surpasses existing models in terms of accuracy and provides clearer visualization of linguistic features for human and machine interpretable analysis."}}
{"id": "2512.00333", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.00333", "abs": "https://arxiv.org/abs/2512.00333", "authors": ["Ayush Maheshwari", "Kaushal Sharma", "Vivek Patel", "Aditya Maheshwari"], "title": "IndicParam: Benchmark to evaluate LLMs on low-resource Indic Languages", "comment": null, "summary": "While large language models excel on high-resource multilingual tasks, low- and extremely low-resource Indic languages remain severely under-evaluated. We present IndicParam, a human-curated benchmark of over 13,000 multiple-choice questions covering 11 such languages (Nepali, Gujarati, Marathi, Odia as low-resource; Dogri, Maithili, Rajasthani, Sanskrit, Bodo, Santali, Konkani as extremely low-resource) plus Sanskrit-English code-mixed set. We evaluated 19 LLMs, both proprietary and open-weights, which reveals that even the top-performing GPT-5 reaches only 45.0% average accuracy, followed by DeepSeek-3.2 (43.1) and Claude-4.5 (42.7). We additionally label each question as knowledge-oriented or purely linguistic to discriminate factual recall from grammatical proficiency. Further, we assess the ability of LLMs to handle diverse question formats-such as list-based matching, assertion-reason pairs, and sequence ordering-alongside conventional multiple-choice questions. IndicParam provides insights into limitations of cross-lingual transfer and establishes a challenging benchmark for Indic languages. The dataset is available at https://huggingface.co/datasets/bharatgenai/IndicParam. Scripts to run benchmark are present at https://github.com/ayushbits/IndicParam.", "AI": {"tldr": "介绍了IndicParam，这是一个为11种低资源和极低资源的印度语言设计的多选题基准测试，包含超过13,000个问题。研究表明，即使是顶级的GPT-5也只能达到平均45.0%的准确率。", "motivation": "研究低资源和极低资源的印度语言上的大规模语言模型的表现，以及这些模型在不同类型的题目上的表现。", "method": "创建了一个涵盖11种印度语言的多选题基准测试，问题被标记为知识导向或纯语言导向，评估包括GPT-5在内的19种大规模语言模型的表现。", "result": "即使是表现最好的GPT-5也只能达到45.0%的平均准确率。测试还进一步分析了模型处理不同类型题目的能力。", "conclusion": "IndicParam揭示了大规模语言模型在跨语言迁移上的限制，并为印度语言的能力测试设立了具有挑战性的基准。"}}
{"id": "2512.00089", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.00089", "abs": "https://arxiv.org/abs/2512.00089", "authors": ["Ioannis Prapas", "Nikolaos Papadopoulos", "Nikolaos-Ioannis Bountos", "Dimitrios Michail", "Gustau Camps-Valls", "Ioannis Papoutsis"], "title": "TeleViT1.0: Teleconnection-aware Vision Transformers for Subseasonal to Seasonal Wildfire Pattern Forecasts", "comment": "Under review", "summary": "Forecasting wildfires weeks to months in advance is difficult, yet crucial for planning fuel treatments and allocating resources. While short-term predictions typically rely on local weather conditions, long-term forecasting requires accounting for the Earth's interconnectedness, including global patterns and teleconnections. We introduce TeleViT, a Teleconnection-aware Vision Transformer that integrates (i) fine-scale local fire drivers, (ii) coarsened global fields, and (iii) teleconnection indices. This multi-scale fusion is achieved through an asymmetric tokenization strategy that produces heterogeneous tokens processed jointly by a transformer encoder, followed by a decoder that preserves spatial structure by mapping local tokens to their corresponding prediction patches.\n  Using the global SeasFire dataset (2001-2021, 8-day resolution), TeleViT improves AUPRC performance over U-Net++, ViT, and climatology across all lead times, including horizons up to four months. At zero lead, TeleViT with indices and global inputs reaches AUPRC 0.630 (ViT 0.617, U-Net 0.620), at 16x8day lead (around 4 months), TeleViT variants using global input maintain 0.601-0.603 (ViT 0.582, U-Net 0.578), while surpassing the climatology (0.572) at all lead times. Regional results show the highest skill in seasonally consistent fire regimes, such as African savannas, and lower skill in boreal and arid regions. Attention and attribution analyses indicate that predictions rely mainly on local tokens, with global fields and indices contributing coarse contextual information. These findings suggest that architectures explicitly encoding large-scale Earth-system context can extend wildfire predictability on subseasonal-to-seasonal timescales.", "AI": {"tldr": "本文提出TeleViT，一种基于远距离相互连接意识的视觉转换器，结合了细粒度的局部条件、全球数据以及远程连接指数，展示了预测野火在数周至数月间发生的优越性。相较于其他模型，TeleViT可以在长周期预报中保持良好表现。", "motivation": "考虑到提前数周到数月预测野火对于安排燃料处理和资源分配极其重要，但短时间预测通常依赖于本地天气条件，而长时间预测需要考虑地球系统的复杂性。我们需要开发一个考虑全球关联性的预测模型，尤其是全球模式和长途相互连接。", "method": "本文介绍了TeleViT，一种考虑地球系统中长途相互连接的预测模型，它结合了细粒度的局部火灾驱动因素、全球环境的泛化以及远程连接指数。该模型通过一种非对称令牌策略，实现了多尺度的信息融合，接着用Transformer编码器处理异构令牌，最后通过解码器将局部令牌映射到预测补丁，保持空间结构。", "result": "在使用全球SeasFire数据集（2001-2021，8天分辨率）进行训练和测试时，TeleViT在所有提前预测时间上均表现优于U-Net++，ViT和气候模型。例如在零滞后时，TeleViT可以达到AUPRC 0.630（ViT为0.617，U-Net为0.620），在16x8天的前置时间（大约4个月）后，TeleViT保留了0.601-0.603 AUPRC（ViT为0.582，U-Net为0.578），同时在所有提前预测时间内均超越气候模型（0.572）。在区域层面，模型在具有季节性一致性火灾模式的区域，如非洲草原上表现最佳，而在北部森林带和干旱地区表现较差。", "conclusion": "建筑上明确编码大规模地球系统环境，有可能延长季节内到季节性的野火预测。注意力和归因分析表明，预测主要依赖于局部令牌，而全球环境和指数为预测提供粗略的语境信息。这表明复杂的大规模环境信息能提升预测能力。然而，模型在具有一致季节性火灾规律的区域，如非洲草原上的表现优于在北极沿岸和干旱地区的表现。总的来说，该模型为长期预测野火提供了一种新的方案。"}}
{"id": "2512.00360", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.00360", "abs": "https://arxiv.org/abs/2512.00360", "authors": ["Vsevolod Kovalev", "Parteek Kumar"], "title": "CourseTimeQA: A Lecture-Video Benchmark and a Latency-Constrained Cross-Modal Fusion Method for Timestamped QA", "comment": "5 figures, 8 tables", "summary": "We study timestamped question answering over educational lecture videos under a single-GPU latency/memory budget. Given a natural-language query, the system retrieves relevant timestamped segments and synthesizes a grounded answer. We present CourseTimeQA (52.3 h, 902 queries across six courses) and a lightweight, latency-constrained cross-modal retriever (CrossFusion-RAG) that combines frozen encoders, a learned 512->768 vision projection, shallow query-agnostic cross-attention over ASR and frames with a temporal-consistency regularizer, and a small cross-attentive reranker. On CourseTimeQA, CrossFusion-RAG improves nDCG@10 by 0.10 and MRR by 0.08 over a strong BLIP-2 retriever while achieving approximately 1.55 s median end-to-end latency on a single A100. Closest comparators (zero-shot CLIP multi-frame pooling; CLIP + cross-encoder reranker + MMR; learned late-fusion gating; text-only hybrid with cross-encoder reranking and its MMR variant; caption-augmented text retrieval; non-learned temporal smoothing) are evaluated under matched hardware and indexing. We report robustness across ASR noise (WER quartiles), diagnostics for temporal localization, and full training/tuning details to support reproducible comparison.", "AI": {"tldr": "本文提出了CrossFusion-RAG模型，在教育视频问答任务上进行改进，并在CourseTimeQA数据集上进行了性能评估。", "motivation": "本文旨在研究在单一GPU延迟/内存预算下针对教育讲座视频的带时间戳问题回答。", "method": "本文提出了一种轻量级、延迟受约束的跨模态检索器（CrossFusion-RAG），该检索器结合了冻结编码器、学习的512->768视觉投影、浅层查询无关的ASR和帧之间的交叉注意力、以及一个小的交叉注意力重新排序器。", "result": "在CourseTimeQA数据集上，CrossFusion-RAG模型在nDCG@10和MRR指标上分别提高了0.10和0.08，同时保持了约1.55秒的中位端到端延迟。", "conclusion": "实验表明，CrossFusion-RAG在教育讲座视频的时间戳问题回答任务上取得了较好的性能，为未来的研究提供了可复现的对比基准。"}}
{"id": "2512.00091", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.00091", "abs": "https://arxiv.org/abs/2512.00091", "authors": ["Karam Mawas", "Mehdi Maboudi", "Pedro Achanccaray", "Markus Gerke"], "title": "Deep Filament Extraction for 3D Concrete Printing", "comment": null, "summary": "The architecture, engineering and construction (AEC) industry is constantly evolving to meet the demand for sustainable and effective design and construction of the built environment. In the literature, two primary deposition techniques for large-scale 3D concrete printing (3DCP) have been described, namely extrusion-based (Contour Crafting-CC) and shotcrete 3D printing (SC3DP) methods. The deposition methods use a digitally controlled nozzle to print material layer by layer. The continuous flow of concrete material used to create the printed structure is called a filament or layer. As these filaments are the essential structure defining the printed object, the filaments' geometry quality control is crucial. This paper presents an automated procedure for quality control (QC) of filaments in extrusion-based and SC3DP printing methods. The paper also describes a workflow that is independent of the sensor used for data acquisition, such as a camera, a structured light system (SLS) or a terrestrial laser scanner (TLS). This method can be used with materials in either the fresh or cured state. Thus, it can be used for online and post-printing QC.", "AI": {"tldr": "An automated quality control method for filaments used in large-scale 3D concrete printing is described, applicable to both fresh and cured materials and independent of the sensor type used for data acquisition.", "motivation": "The motivation behind this paper is to address the challenge of ensuring the consistent high quality of printed filaments in large-scale 3D concrete printing, which is critical for the structural integrity of the built environment.", "method": "This paper presents an automated procedure for quality control (QC) of filaments in extrusion-based and shotcrete 3D printing (SC3DP) methods. The method is independent of the type of sensor used for data acquisition and can be applied to both fresh and cured concrete materials, allowing for both online and post-printing quality control.", "result": "The paper does not explicitly outline a result section within the abstract; thus, detailed experimental outcomes and comparative performance are not provided.", "conclusion": "While the paper does not provide a clear conclusion in its abstract, the implication is that the proposed method effectively supports quality control in both extrusion-based and SC3DP in 3D concrete printing, improving construction quality and possibly efficiency."}}
{"id": "2512.00390", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2512.00390", "abs": "https://arxiv.org/abs/2512.00390", "authors": ["Nuo Chen", "Hanpei Fang", "Jiqun Liu", "Wilson Wei", "Tetsuya Sakai", "Xiao-Ming Wu"], "title": "Mitigating the Threshold Priming Effect in Large Language Model-Based Relevance Judgments via Personality Infusing", "comment": null, "summary": "Recent research has explored LLMs as scalable tools for relevance labeling, but studies indicate they are susceptible to priming effects, where prior relevance judgments influence later ones. Although psychological theories link personality traits to such biases, it is unclear whether simulated personalities in LLMs exhibit similar effects. We investigate how Big Five personality profiles in LLMs influence priming in relevance labeling, using multiple LLMs on TREC 2021 and 2022 Deep Learning Track datasets. Our results show that certain profiles, such as High Openness and Low Neuroticism, consistently reduce priming susceptibility. Additionally, the most effective personality in mitigating priming may vary across models and task types. Based on these findings, we propose personality prompting as a method to mitigate threshold priming, connecting psychological evidence with LLM-based evaluation practices.", "AI": {"tldr": "研究探讨了LLMs中的大五人格特质如何影响相关性标注中的启动效应，并提出人格提示作为缓解启动效应的方法。", "motivation": "研究动机在于探索LLMs中的大五人格特质是否会导致如人体一样的启动效应，以及如何通过调节这些特质来减少这种效应。", "method": "研究采用不同的LLM模型，在TREC 2021和2022的深度学习赛道数据集上进行实验，探讨大五人格特质如何影响启动效应。", "result": "结果表明，具有高开放性和低神经质人格特质的LLM模型表现出较低的启动效应。有效的性格配置可能因模型和任务类型的不同而不同。", "conclusion": "研究建议使用人格提示作为减轻启动效应的方法，将心理学证据与基于LLM的评估实践相结合。"}}
{"id": "2512.00103", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.00103", "abs": "https://arxiv.org/abs/2512.00103", "authors": ["Ifeanyi Okala"], "title": "Comparative Analysis of Vision Transformer, Convolutional, and Hybrid Architectures for Mental Health Classification Using Actigraphy-Derived Images", "comment": null, "summary": "This work examines how three different image-based methods, VGG16, ViT-B/16, and CoAtNet-Tiny, perform in identifying depression, schizophrenia, and healthy controls using daily actigraphy records. Wrist-worn activity signals from the Psykose and Depresjon datasets were converted into 30 by 48 images and evaluated through a three-fold subject-wise split. Although all methods fitted the training data well, their behaviour on unseen data differed. VGG16 improved steadily but often settled at lower accuracy. ViT-B/16 reached strong results in some runs, but its performance shifted noticeably from fold to fold. CoAtNet-Tiny stood out as the most reliable, recording the highest average accuracy and the most stable curves across folds. It also produced the strongest precision, recall, and F1-scores, particularly for the underrepresented depression and schizophrenia classes. Overall, the findings indicate that CoAtNet-Tiny performed most consistently on the actigraphy images, while VGG16 and ViT-B/16 yielded mixed results. These observations suggest that certain hybrid designs may be especially suited for mental-health work that relies on actigraphy-derived images.", "AI": {"tldr": "The study uses actigraphy images and compares three image-based models for mental health conditions, with CoAtNet-Tiny showing the most consistent results.", "motivation": "The motivation is to explore and compare different image-based methods' effectiveness in identifying mental health conditions such as depression and schizophrenia using actigraphy data.", "method": "This work uses three different image-based methods: VGG16, ViT-B/16, and CoAtNet-Tiny to identify depression, schizophrenia, and healthy controls using daily actigraphy records converted into 30x48 images.", "result": "CoAtNet-Tiny performed most consistently, achieving the highest average accuracy and stability across different folds, and strong precision, recall, and F1-scores, especially for depression and schizophrenia classes. VGG16 and ViT-B/16 showed mixed results.", "conclusion": "CoAtNet-Tiny is suggested as particularly suited for mental health analysis using actigraphy-derived images, indicating the potential of certain hybrid designs in this field."}}
{"id": "2512.00392", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.00392", "abs": "https://arxiv.org/abs/2512.00392", "authors": ["Damian Heywood", "Joseph Andrew Carrier", "Kyu-Hong Hwang"], "title": "A Taxonomy of Errors in English as she is spoke: Toward an AI-Based Method of Error Analysis for EFL Writing Instruction", "comment": "Metadata at \"Replication Data for: A Taxonomy of Errors in English as she is spoke: An AI-Based System for Error Analysis for EFL Writing Instruction\", https://doi.org/10.7910/DVN/N5O7C4, Harvard Dataverse, V1", "summary": "This study describes the development of an AI-assisted error analysis system designed to identify, categorize, and correct writing errors in English. Utilizing Large Language Models (LLMs) like Claude 3.5 Sonnet and DeepSeek R1, the system employs a detailed taxonomy grounded in linguistic theories from Corder (1967), Richards (1971), and James (1998). Errors are classified at both word and sentence levels, covering spelling, grammar, and punctuation. Implemented through Python-coded API calls, the system provides granular feedback beyond traditional rubric-based assessments. Initial testing on isolated errors refined the taxonomy, addressing challenges like overlapping categories. Final testing used \"English as she is spoke\" by Jose da Fonseca (1855), a text rich with authentic linguistic errors, to evaluate the system's capacity for handling complex, multi-layered analysis. The AI successfully identified diverse error types but showed limitations in contextual understanding and occasionally generated new error categories when encountering uncoded errors. This research demonstrates AI's potential to transform EFL instruction by automating detailed error analysis and feedback. While promising, further development is needed to improve contextual accuracy and expand the taxonomy to stylistic and discourse-level errors.", "AI": {"tldr": "研究开发了一个AI辅助系统，利用大型语言模型识别和分类英语写作中的错误，并通过API接口实现自动反馈。初步结果显示出系统的潜力，但也强调了进一步完善的需求。", "motivation": "该研究旨在通过AI技术自动化英语写作中的细致错误分析和反馈，以辅助英语作为外语（EFL）的教学活动，进一步提高该领域的教学效果。", "method": "该研究使用大型语言模型（如Claude 3.5 Sonnet和DeepSeek R1）开发了一个AI辅助的错误分析系统，用于识别、分类和纠正英语写作中的错误。系统采用基于Corder (1967), Richards (1971)和James (1998)的详细分类法，在单词和句子层面上对错误进行分类，涵盖了拼写、语法和标点。系统是通过Python编写的API调用来实现，并通过传统的评分标准评估进行详尽反馈的。", "result": "初始测试验证了分类法的有效性。通过使用含有丰富真实语言错误的文本，最终测试验证了系统在处理复杂、多层次分析方面的能力。虽然AI成功识别了各种错误类型，但其在上下文理解方面存在局限性，并且在遇到未编码的错误时会生成新的错误类别。", "conclusion": "研究展示了AI在自动化EFL错误分析与反馈方面的潜力，但同时也指出了在上下文准确性改进以及扩展分类法以包括更多样性和文本层次错误方面的未来改进方向。"}}
{"id": "2512.00117", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2512.00117", "abs": "https://arxiv.org/abs/2512.00117", "authors": ["Ishwaryah Pandiarajan", "Mohamed Mansoor Roomi Sindha", "Uma Maheswari Pandyan", "Sharafia N"], "title": "TinyViT: Field Deployable Transformer Pipeline for Solar Panel Surface Fault and Severity Screening", "comment": "3pages, 2figures,ICGVIP 2025", "summary": "Sustained operation of solar photovoltaic assets hinges on accurate detection and prioritization of surface faults across vast, geographically distributed modules. While multi modal imaging strategies are popular, they introduce logistical and economic barriers for routine farm level deployment. This work demonstrates that deep learning and classical machine learning may be judiciously combined to achieve robust surface anomaly categorization and severity estimation from planar visible band imagery alone. We introduce TinyViT which is a compact pipeline integrating Transformer based segmentation, spectral-spatial feature engineering, and ensemble regression. The system ingests consumer grade color camera mosaics of PV panels, classifies seven nuanced surface faults, and generates actionable severity grades for maintenance triage. By eliminating reliance on electroluminescence or IR sensors, our method enables affordable, scalable upkeep for resource limited installations, and advances the state of solar health monitoring toward universal field accessibility. Experiments on real public world datasets validate both classification and regression sub modules, achieving accuracy and interpretability competitive with specialized approaches.", "AI": {"tldr": "该研究提出了一种结合深度学习和经典机器学习的方法，用于仅通过平面可见光带图像实现光伏板表面故障分类和严重程度估计，消除了对电致发光或红外传感器的依赖，为资源有限的太阳能装置提供了经济实惠的维护解决方案。", "motivation": "该研究旨在通过提出一种仅依赖于平面可见光带图像的故障检测方法，解决了多模成像策略带来的后勤和经济障碍，从而实现在大规模分布式光伏模块中准确检测和优先处理表面故障。", "method": "该研究结合了深度学习和经典机器学习方法，提出了一种名为TinyViT的紧凑型流水线，该流水线通过整合Transformer基分割、光谱-空间特征工程和集成回归，仅利用平面可见光带图像实现了鲁棒的表面异常分类和严重程度估计。系统使用消费者级彩色相机拍摄的光伏板马赛克图像，将七个微妙的表面故障进行分类，并生成维护优先级的操作严重程度等级。", "result": "实验结果表明，该方法在分类和回归子模块上与专门的方法具有竞争力，验证了其精确性和可解释性。这种方法通过消除对电致发光或红外传感器的依赖，使资源有限的安装能够进行经济实惠、可扩展的维护，推动了太阳能健康监测向通用现场可用性发展。", "conclusion": "该研究证明了其方法在资源有限安装中的适用性，通过提供经济实惠、可扩展的维护方案，推动太阳能健康监测领域的创新，实现了分类准确性和可解释性与专门方法相媲美的结果。"}}
{"id": "2512.00417", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.00417", "abs": "https://arxiv.org/abs/2512.00417", "authors": ["Jiacheng Guo", "Suozhi Huang", "Zixin Yao", "Yifan Zhang", "Yifu Lu", "Jiashuo Liu", "Zihao Li", "Yanyan Deng", "Qixin Xiao", "Jia Tian", "Kanghong Zhan", "Tianyi Li", "Xiaochen Liu", "Jason Ge", "Chaoyang He", "Kaixuan Huang", "Lin Yang", "Wenhao Huang", "Mengdi Wang"], "title": "CryptoBench: A Dynamic Benchmark for Expert-Level Evaluation of LLM Agents in Cryptocurrency", "comment": null, "summary": "This paper introduces CryptoBench, the first expert-curated, dynamic benchmark designed to rigorously evaluate the real-world capabilities of Large Language Model (LLM) agents in the uniquely demanding and fast-paced cryptocurrency domain. Unlike general-purpose agent benchmarks for search and prediction, professional crypto analysis presents specific challenges: \\emph{extreme time-sensitivity}, \\emph{a highly adversarial information environment}, and the critical need to synthesize data from \\emph{diverse, specialized sources}, such as on-chain intelligence platforms and real-time Decentralized Finance (DeFi) dashboards. CryptoBench thus serves as a much more challenging and valuable scenario for LLM agent assessment. To address these challenges, we constructed a live, dynamic benchmark featuring 50 questions per month, expertly designed by crypto-native professionals to mirror actual analyst workflows. These tasks are rigorously categorized within a four-quadrant system: Simple Retrieval, Complex Retrieval, Simple Prediction, and Complex Prediction. This granular categorization enables a precise assessment of an LLM agent's foundational data-gathering capabilities alongside its advanced analytical and forecasting skills.\n  Our evaluation of ten LLMs, both directly and within an agentic framework, reveals a performance hierarchy and uncovers a failure mode. We observe a \\textit{retrieval-prediction imbalance}, where many leading models, despite being proficient at data retrieval, demonstrate a pronounced weakness in tasks requiring predictive analysis. This highlights a problematic tendency for agents to appear factually grounded while lacking the deeper analytical capabilities to synthesize information.", "AI": {"tldr": "本论文引入了CryptoBench，一个为评估LLM在加密货币领域的具体挑战而设计的基准测试，揭示了模型在预测分析方面的弱点。", "motivation": "加密货币分析领域面临独特的挑战，包括对时间的极端敏感性、高度对抗性的信息环境，以及需要从多元化的专门来源中综合信息的需求。这些挑战导致CryptoBench成为一个更加严格且有价值的LLM评估场景。", "method": "本文介绍了CryptoBench，这是一个专门为评估大型语言模型（LLM）在快节奏且充满挑战的加密货币领域的实际能力而设计的第一动态基准测试。CryptoBench由专家策划，每月包含50个问题，覆盖了从简单检索到复杂预测的四大类别，旨在反映实际的分析师工作流程。", "result": "通过对十个LLM模型的评估，研究发现了它们的性能层级，并揭示了一个问题，即许多领先模型在数据检索方面表现良好，但在预测分析任务中则表现不足，显示出一种事实检索与预测分析能力不平衡的状况。", "conclusion": "本研究展示了CryptoBench作为评估加密货币领域LLM性能的有效工具，同时揭示了需要提高这些模型的预测分析能力的重要性。"}}
{"id": "2512.00125", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.00125", "abs": "https://arxiv.org/abs/2512.00125", "authors": ["Ruo-Syuan Mei", "Sixian Jia", "Guangze Li", "Soo Yeon Lee", "Brian Musser", "William Keller", "Sreten Zakula", "Jorge Arinez", "Chenhui Shao"], "title": "Hybrid Synthetic Data Generation with Domain Randomization Enables Zero-Shot Vision-Based Part Inspection Under Extreme Class Imbalance", "comment": "Submitted to the NAMRC 54", "summary": "Machine learning, particularly deep learning, is transforming industrial quality inspection. Yet, training robust machine learning models typically requires large volumes of high-quality labeled data, which are expensive, time-consuming, and labor-intensive to obtain in manufacturing. Moreover, defective samples are intrinsically rare, leading to severe class imbalance that degrades model performance. These data constraints hinder the widespread adoption of machine learning-based quality inspection methods in real production environments. Synthetic data generation (SDG) offers a promising solution by enabling the creation of large, balanced, and fully annotated datasets in an efficient, cost-effective, and scalable manner. This paper presents a hybrid SDG framework that integrates simulation-based rendering, domain randomization, and real background compositing to enable zero-shot learning for computer vision-based industrial part inspection without manual annotation. The SDG pipeline generates 12,960 labeled images in one hour by varying part geometry, lighting, and surface properties, and then compositing synthetic parts onto real image backgrounds. A two-stage architecture utilizing a YOLOv8n backbone for object detection and MobileNetV3-small for quality classification is trained exclusively on synthetic data and evaluated on 300 real industrial parts. The proposed approach achieves an mAP@0.5 of 0.995 for detection, 96% classification accuracy, and 90.1% balanced accuracy. Comparative evaluation against few-shot real-data baseline approaches demonstrates significant improvement. The proposed SDG-based approach achieves 90-91% balanced accuracy under severe class imbalance, while the baselines reach only 50% accuracy. These results demonstrate that the proposed method enables annotation-free, scalable, and robust quality inspection for real-world manufacturing applications.", "AI": {"tldr": "The paper introduces a hybrid SDG framework that generates large, balanced datasets for industrial part inspections through simulation-based rendering, domain randomization, and real background compositing, achieving high mAP and balanced accuracy scores using only synthetic data.", "motivation": "The motivation is to tackle the challenge of obtaining large volumes of high-quality labeled data for training robust machine learning models, especially considering the scarcity of defective samples, leading to severe class imbalance. This hinders the efficiency and cost-effectiveness required for widespread adoption in real production environments.", "method": "The method is a hybrid synthetic data generation (SDG) framework that creates large, annotated datasets efficiently through simulation rendering, domain randomization, and real background compositing. It generates a large number of labeled images in a short time by varying part geometries, lighting, and surface properties and composites onto real backgrounds.", "result": "The method achieves an mAP@0.5 of 0.995 for detection, 96% classification accuracy, and 90.1% balanced accuracy using a two-stage architecture. Under severe class imbalance, the SDG-based approach outperforms few-shot real-data baseline approaches, reaching 90-91% balanced accuracy, while baselines achieve only 50%.", "conclusion": "The proposed SDG framework enables zero-shot learning and annotation-free, scalable, and robust quality inspection for real-world manufacturing applications, offering significant improvements over traditional few-shot real-data training methods."}}
{"id": "2512.00466", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.00466", "abs": "https://arxiv.org/abs/2512.00466", "authors": ["Yang Xiao", "Chunpu Xu", "Ruifeng Yuan", "Jiashuo Wang", "Wenjie Li", "Pengfei Liu"], "title": "SCALE: Selective Resource Allocation for Overcoming Performance Bottlenecks in Mathematical Test-time Scaling", "comment": "accepted by AAAI 2026", "summary": "Test-time compute scaling has emerged as a powerful paradigm for enhancing mathematical reasoning in large language models (LLMs) by allocating additional computational resources during inference. However, current methods employ uniform resource distribution across all reasoning sub-problems, creating fundamental bottlenecks where challenging sub-problems receive insufficient attention while routine operations consume disproportionate resources. This uniform allocation creates performance bottlenecks where additional computational resources yield diminishing returns. Inspired by dual-process theory, we propose \\textbf{SCALE} (Selective Resource Allocation), a framework that selectively allocates computational resources based on sub-problem difficulty. SCALE operates through four stages: (1) problem decomposition into sequential reasoning sub-problems, (2) difficulty assessment of each sub-problem to distinguish between routine operations and computationally challenging sub-problems, (3) selective processing mode assignment between System 1 for simple sub-problems and System 2 for complex ones, and (4) sequential execution with context propagation. By concentrating resources on challenging sub-problems while processing routine operations efficiently, SCALE achieves substantial performance improvements with superior resource utilization. Extensive experiments demonstrate that SCALE significantly outperforms uniform scaling baselines, achieving accuracy improvements of up to 13.75 percentage points (57.50% to 71.25% on AIME25) while reducing computational costs by 33%-53%, representing a major advance in test-time scaling that addresses fundamental limitations of current approaches.", "AI": {"tldr": "针对在大语言模型中均匀分配资源出现的效率问题，提出选择性资源分配框架SCALE，提高推理性能并减少计算成本。", "motivation": "现有方法在推理过程中均匀分配计算资源至所有推理子问题，导致资源瓶颈。一些子问题挑战性高却得到的计算资源不足，而常规操作则消耗了过多资源。这导致了额外计算资源带来的性能提升有限。", "method": "SCALE框架通过四个阶段操作：(1)将问题分解为顺序推理子问题；(2)评估每个子问题的难度来区分常规操作和计算挑战性子问题；(3)将处理模式分配给简单子问题的System 1和复杂子问题的System 2；(4)顺序执行并传播上下文。这个方法能够集中资源处理具有挑战性的子问题，同时高效处理常规操作。", "result": "实验表明，SCALE与均匀缩放基线相比显著提高了性能，准确率提高了最高13.75个百分点（AIME25从57.50%提高到71.25%），同时减少了33%-53%的计算成本。", "conclusion": "SCALE通过选择性资源分配极大地提高了问题解决中的计算效率和效果，这代表了测试时间缩放的重大进展，解决了现有方法的基本局限性。"}}
{"id": "2512.00129", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.00129", "abs": "https://arxiv.org/abs/2512.00129", "authors": ["Jayan Adhikari", "Prativa Joshi", "Susish Baral"], "title": "Analysis of Incursive Breast Cancer in Mammograms Using YOLO, Explainability, and Domain Adaptation", "comment": null, "summary": "Deep learning models for breast cancer detection from mammographic images have significant reliability problems when presented with Out-of-Distribution (OOD) inputs such as other imaging modalities (CT, MRI, X-ray) or equipment variations, leading to unreliable detection and misdiagnosis. The current research mitigates the fundamental OOD issue through a comprehensive approach integrating ResNet50-based OOD filtering with YOLO architectures (YOLOv8, YOLOv11, YOLOv12) for accurate detection of breast cancer. Our strategy establishes an in-domain gallery via cosine similarity to rigidly reject non-mammographic inputs prior to processing, ensuring that only domain-associated images supply the detection pipeline. The OOD detection component achieves 99.77\\% general accuracy with immaculate 100\\% accuracy on OOD test sets, effectively eliminating irrelevant imaging modalities. ResNet50 was selected as the optimum backbone after 12 CNN architecture searches. The joint framework unites OOD robustness with high detection performance (mAP@0.5: 0.947) and enhanced interpretability through Grad-CAM visualizations. Experimental validation establishes that OOD filtering significantly improves system reliability by preventing false alarms on out-of-distribution inputs while maintaining higher detection accuracy on mammographic data. The present study offers a fundamental foundation for the deployment of reliable AI-based breast cancer detection systems in diverse clinical environments with inherent data heterogeneity.", "AI": {"tldr": "研究人员开发了一种结合ResNet50 OOD过滤器和YOLO架构的检测框架，解决了乳腺癌检测中非分布数据导致的可靠性问题，获得了优秀的检测性能和可靠性。", "motivation": "目前的深度学习模型在处理非分布输入数据（如其他成像模式或设备差异）时，乳腺癌检测的可靠性存在问题，容易导致误诊。研究人员旨在提供一个可靠的基于AI的乳房癌检测系统，以克服这些问题。", "method": "该研究通过结合ResNet50为基础的OOD过滤器和YOLO架构（YOLOv8, YOLOv11, YOLOv12）来解决深度学习模型在乳腺癌检测中因非分布数据而引起的可靠性问题。使用余弦相似性建立了一个域内图库来刚性拒绝非乳腺图象输入，从而确保只有与域相关的图像提供给检测管道。", "result": "该方法的OOD检测组件达到了99.77%的整体准确率和100%的非分布测试集上的准确率，显著提升了系统的可靠性，检测性能（mAP@0.5: 0.947）也很高。实验验证表明这种方法在保持高检测准确率的同时也提高了系统可靠性。", "conclusion": "实验结果表明，通过OOD过滤可以显著提高系统的可靠性，避免对非分布输入发出误报警，同时保持在乳腺图象数据上的高检测精度。研究为在具有数据异质性的多样临床环境中部署基于AI的乳腺癌检测系统提供了基础。"}}
{"id": "2512.00496", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.00496", "abs": "https://arxiv.org/abs/2512.00496", "authors": ["Diego A. B. Moreira", "Alef I. Ferreira", "Jhessica Silva", "Gabriel O. dos Santos", "Gustavo Bonil", "João Gondim", "Marina dos Santos", "Helena Maia", "Simone Hashiguti", "Nádia da Silva", "Carolina Scarton", "Helio Pedrini", "Sandra Avila"], "title": "CACARA: Cross-Modal Alignment Leveraging a Text-Centric Approach for Cost-Effective Multimodal and Multilingual Learning", "comment": "25 pages, 12 tables, 5 figures", "summary": "As deep learning models evolve, new applications and challenges are rapidly emerging. Tasks that once relied on a single modality, such as text, images, or audio, are now enriched by seamless interactions between multimodal data. These connections bridge information gaps: an image can visually materialize a text, while audio can add context to an image. Researchers have developed numerous multimodal models, but most rely on resource-intensive training across multiple modalities. Similarly, extending these models to new languages often follows the same resource-heavy training strategy. In this work, we propose a multimodal and multilingual architecture, CACARA, trained through emergent alignment learning, enabling the seamless integration of new modalities into an existing bimodal/multimodal model without requiring full retraining. This work breaks new ground by demonstrating that this emergent alignment paradigm can unlock multilingual capabilities from monolingual training. By fine-tuning the newly incorporated modality only on data aligned with the English language, our model develops support for over 100 languages without explicit multilingual pretraining or tuning of the text encoder. Such emergent multimodal and multilingual properties are gained efficiently, preserving previously learned knowledge at a training cost comparable to that of a monolingual model. Our strategy achieves up to a 14.24 percentage points improvement in R@1 audio-to-text retrieval, outperforming state-of-the-art multimodal models -- all without the heavy computational cost of retraining across every modality and language.", "AI": {"tldr": "本文提出了CACARA架构，通过新颖的训练策略显著改善了音频到文本检索的效果，并且在没有跨每个模态和语言的重新训练成本的情况下使模型具备多语言支持。", "motivation": "传统的多模态模型在训练过程中需要大量的资源，而且将其扩展到新的语言时往往也需要资源密集的训练策略。本文旨在改变这一现状。", "method": "本文提出了一种新的多模态和多语言架构CACARA，通过涌现对齐学习，实现了在不进行全面重新训练的情况下将新的模态无缝地整合到现有的双模态或多模态模型中。", "result": "该模型仅通过在与英语对齐的数据上对新加入的模态进行微调，就发展出了对超过100种语言的支持，极大地改善了音频到文本检索的性能。", "conclusion": "研究表明，通过本文的方法，不仅可以实现多语言能力，而且可以在训练成本与单语模型相当的情况下获得高效的、涌现的多模态与多语言特性，大幅度减少了计算成本。"}}
{"id": "2512.00130", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.00130", "abs": "https://arxiv.org/abs/2512.00130", "authors": ["Fadi Dornaika", "Danyang Sun"], "title": "Local and Global Context-and-Object-part-Aware Superpixel-based Data Augmentation for Deep Visual Recognition", "comment": null, "summary": "Cutmix-based data augmentation, which uses a cut-and-paste strategy, has shown remarkable generalization capabilities in deep learning. However, existing methods primarily consider global semantics with image-level constraints, which excessively reduces attention to the discriminative local context of the class and leads to a performance improvement bottleneck. Moreover, existing methods for generating augmented samples usually involve cutting and pasting rectangular or square regions, resulting in a loss of object part information. To mitigate the problem of inconsistency between the augmented image and the generated mixed label, existing methods usually require double forward propagation or rely on an external pre-trained network for object centering, which is inefficient. To overcome the above limitations, we propose LGCOAMix, an efficient context-aware and object-part-aware superpixel-based grid blending method for data augmentation. To the best of our knowledge, this is the first time that a label mixing strategy using a superpixel attention approach has been proposed for cutmix-based data augmentation. It is the first instance of learning local features from discriminative superpixel-wise regions and cross-image superpixel contrasts. Extensive experiments on various benchmark datasets show that LGCOAMix outperforms state-of-the-art cutmix-based data augmentation methods on classification tasks, {and weakly supervised object location on CUB200-2011.} We have demonstrated the effectiveness of LGCOAMix not only for CNN networks, but also for Transformer networks. Source codes are available at https://github.com/DanielaPlusPlus/LGCOAMix.", "AI": {"tldr": "LGCOAMix 提出了一种基于超级像素的网格混合方法，用于数据增强，旨在解决现有Cutmix方法中存在的局部上下文信息不足和对象部分信息丢失的问题。实验表明，LGCOAMix 在多个数据集上优于现有方法，并且对于 CNN 和 Transformer 网络均有效。", "motivation": "现有 Cutmix 方法主要关注全局语义，忽略了局部上下文和对象部分信息。此外，这些方法中的混合图像和生成的混合标签之间的一致性问题也影响了效率。因此，提出了 LGCOAMix 以克服这些问题。", "method": "Structure", "result": "{\n  \"tldr\": \"LGCOAMix 提出了一种基于超级像素的网格混合方法，用于数据增强，旨在解决现有Cutmix方法中存在的局部上下文信息不足和对象部分信息丢失的问题。实验表明，LGCOAMix 在多个数据集上优于现有方法，并且对于 CNN 和 Transformer 网络均有效。\", \n  \"motivation\": \"现有 Cutmix 方法主要关注全局语义，忽略了局部上下文和对象部分信息。此外，这些方法中的混合图像和生成的混合标签之间的一致性问题也影响了效率。因此，提出了 LGCOAMix 以克服这些问题。\", \n  \"method\": \"LGCOAMix 使用超级像素注意力方法来混合标签，利用了局部特征从辨别性超级像素区域和跨图像超级像素对比来学习。该方法是首次应用在这种数据增强方法中的。\", \n  \"result\": \"通过在多种基准数据集上的大量实验，LGCOAMix 在分类任务上优于现有的 Cutmix 方法，并且在 CUB200-2011 数据集上的弱监督对象定位任务中也表现出色。\", \n  \"conclusion\": \"LGCOAMix 显示出了优秀的性能，并且它的有效性不仅限于传统 CNN 网络，还包括 Transformer 网络。在未来的研究中，可以进一步探索该方法在其他任务上的应用。\"}\n}", "conclusion": "LGCOAMix 显示出了优秀的性能，并且它的有效性不仅限于传统 CNN 网络，还包括 Transformer 网络。在未来的研究中，可以进一步探索该方法在其他任务上的应用。"}}
{"id": "2512.00504", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.00504", "abs": "https://arxiv.org/abs/2512.00504", "authors": ["Mengqi Liao", "Lu Wang", "Chaoyun Zhang", "Zekai Shen", "Xiaowei Mao", "Si Qin", "Qingwei Lin", "Saravan Rajmohan", "Dongmei Zhang", "Huaiyu Wan"], "title": "G-KV: Decoding-Time KV Cache Eviction with Global Attention", "comment": null, "summary": "Recent reasoning large language models (LLMs) excel in complex tasks but encounter significant computational and memory challenges due to long sequence lengths. KV cache compression has emerged as an effective approach to greatly enhance the efficiency of reasoning. However, existing methods often focus on prompt compression or token eviction with local attention score, overlooking the long-term importance of tokens. We propose G-KV, a KV cache eviction method that employs a global scoring mechanism, combining local and historical attention scores to more accurately assess token importance. Additionally, we introduce post-training techniques, including reinforcement learning and distillation, to optimize models for compressed KV cache settings. The code of this paper is available on: https://github.com/microsoft/G-KV.", "AI": {"tldr": "G-KV improves LLM efficiency by using a global scoring mechanism and incorporating post-training techniques.", "motivation": "Existing KV cache compression methods often fail to consider the long-term importance of tokens, leading to less effective token eviction and model optimization for long sequence reasoning tasks.", "method": "We propose G-KV, a KV cache eviction method that uses a global scoring mechanism combining local and historical attention scores. Additionally, post-training techniques like reinforcement learning and distillation are introduced to optimize models.", "result": "G-KV improves the efficiency of reasoning LLMs by accurately assessing the importance of tokens and optimizing models for compressed KV cache settings through post-training techniques.", "conclusion": "The proposed G-KV method and post-training techniques enhance the performance of reasoning LLMs, mitigating computational and memory issues associated with long sequence processing."}}
{"id": "2512.00179", "categories": ["cs.CV", "cs.AI", "cs.LG", "eess.IV"], "pdf": "https://arxiv.org/pdf/2512.00179", "abs": "https://arxiv.org/abs/2512.00179", "authors": ["Mohamed Abdallah Salem", "Nourhan Zein Diab"], "title": "Efficient Edge-Compatible CNN for Speckle-Based Material Recognition in Laser Cutting Systems", "comment": "Copyright 2025 IEEE. This is the author's version of the work that has been Accepted for publication in the Proceedings of the 2025 IEEE The 35th International Conference on Computer Theory and Applications (ICCTA 2025). Final published version will be available on IEEE Xplore", "summary": "Accurate material recognition is critical for safe and effective laser cutting, as misidentification can lead to poor cut quality, machine damage, or the release of hazardous fumes. Laser speckle sensing has recently emerged as a low-cost and non-destructive modality for material classification; however, prior work has either relied on computationally expensive backbone networks or addressed only limited subsets of materials. In this study, A lightweight convolutional neural network (CNN) tailored for speckle patterns is proposed, designed to minimize parameters while maintaining high discriminative power. Using the complete SensiCut dataset of 59 material classes spanning woods, acrylics, composites, textiles, metals, and paper-based products, the proposed model achieves 95.05% test accuracy, with macro and weighted F1-scores of 0.951. The network contains only 341k trainable parameters (~1.3 MB) -- over 70X fewer than ResNet-50 -- and achieves an inference speed of 295 images per second, enabling deployment on Raspberry Pi and Jetson-class devices. Furthermore, when materials are regrouped into nine and five practical families, recall exceeds 98% and approaches 100%, directly supporting power and speed preset selection in laser cutters. These results demonstrate that compact, domain-specific CNNs can outperform large backbones for speckle-based material classification, advancing the feasibility of material-aware, edge-deployable laser cutting systems.", "AI": {"tldr": "研究提出了一种轻量级CNN，用于激光散斑材料分类，表现良好且适合边缘部署。", "motivation": "本文旨在解决现有基于激光散斑的材料分类方法计算成本高或材料范围有限的问题，提出了一种新的分类方法，以提高激光切割的安全性和有效性。", "method": "本文提出了一种针对激光散斑模式的轻量级卷积神经网络（CNN），旨在在减少参数的同时保持较强的辨别力。", "result": "该模型在SensiCut数据集上达到了95.05%的测试准确率，模型仅包含341k个参数，比ResNet-50少70倍以上，推断速度为每秒295张图像。", "conclusion": "结果证明，针对特定领域的紧凑CNN能够超越大型基础网络在基于散斑模式的材料分类中的表现，推动了材料感知的边缘可部署激光切割系统的可行性。"}}
{"id": "2512.00515", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.00515", "abs": "https://arxiv.org/abs/2512.00515", "authors": ["Cem Rifki Aydin"], "title": "Developing a Comprehensive Framework for Sentiment Analysis in Turkish", "comment": "Ph.D. Thesis, Bogazici University, 2020", "summary": "In this thesis, we developed a comprehensive framework for sentiment analysis that takes its many aspects into account mainly for Turkish. We have also proposed several approaches specific to sentiment analysis in English only. We have accordingly made five major and three minor contributions. We generated a novel and effective feature set by combining unsupervised, semi-supervised, and supervised metrics. We then fed them as input into classical machine learning methods, and outperformed neural network models for datasets of different genres in both Turkish and English. We created a polarity lexicon with a semi-supervised domain-specific method, which has been the first approach applied for corpora in Turkish. We performed a fine morphological analysis for the sentiment classification task in Turkish by determining the polarities of morphemes. This can be adapted to other morphologically-rich or agglutinative languages as well. We have built a novel neural network architecture, which combines recurrent and recursive neural network models for English. We built novel word embeddings that exploit sentiment, syntactic, semantic, and lexical characteristics for both Turkish and English. We also redefined context windows as subclauses in modelling word representations in English. This can also be applied to other linguistic fields and natural language processing tasks. We have achieved state-of-the-art and significant results for all these original approaches. Our minor contributions include methods related to aspect-based sentiment in Turkish, parameter redefinition in the semi-supervised approach, and aspect term extraction techniques for English. This thesis can be considered the most detailed and comprehensive study made on sentiment analysis in Turkish as of July, 2020. Our work has also contributed to the opinion classification problem in English.", "AI": {"tldr": "本论文提出了一种全面的 sentiment analysis 框架，主要针对土耳其语，并为英语提出了一些特定的方法。通过结合无监督、半监督和有监督的指标生成了一系列新颖且有效的特征集，结合传统机器学习方法，优于神经网络模型。此外，还提出了几种创新的方法和技术，如结合循环和递归神经网络模型的新型神经网络架构等，达到了最先进的结果。这项工作也对英语的意见分类问题有所贡献。", "motivation": "论文的动机在于解决 sentiment analysis 中涉及的多个方面，特别是在土耳其语的情感分析上进行深入研究，同时提出适用于英语的新方法。旨在为自然语言处理任务，特别是情感分类提供新的视角和技术手段。", "method": "论文提出的方法包括生成特征集、使用传统机器学习方法进行情感分类、创建半监督领域特定的语义词典、针对土耳其语情感分类进行详细的形态分析、以及建立结合循环和递归神经网络模型的新型神经网络架构等。还为土耳其语和英语构建了利用情感、句法、语义和词汇特性的新词嵌入。", "result": "采用了本文提出的方法与技术，在土耳其语和英语的不同体裁的数据集上超越了神经网络模型。结果表明这些原创方法取得了最先进的成效。其中，土耳其语词典首次应用了半监督领域特定方法，为后续的土耳其语自然语言处理研究奠定了基础。", "conclusion": "该研究是截至2020年7月为止对土耳其语进行情感分析最为详尽和全面的研究。对英语意见分类问题也有显著贡献。"}}
{"id": "2512.00194", "categories": ["cs.CV", "cs.LG", "eess.IV", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2512.00194", "abs": "https://arxiv.org/abs/2512.00194", "authors": ["Zag ElSayed", "Grace Westerkamp", "Gavin Gammoh", "Yanchen Liu", "Peyton Siekierski", "Craig Erickson", "Ernest Pedapati"], "title": "AutocleanEEG ICVision: Automated ICA Artifact Classification Using Vision-Language AI", "comment": "6 pages, 8 figures", "summary": "We introduce EEG Autoclean Vision Language AI (ICVision) a first-of-its-kind system that emulates expert-level EEG ICA component classification through AI-agent vision and natural language reasoning. Unlike conventional classifiers such as ICLabel, which rely on handcrafted features, ICVision directly interprets ICA dashboard visualizations topography, time series, power spectra, and ERP plots, using a multimodal large language model (GPT-4 Vision). This allows the AI to see and explain EEG components the way trained neurologists do, making it the first scientific implementation of AI-agent visual cognition in neurophysiology. ICVision classifies each component into one of six canonical categories (brain, eye, heart, muscle, channel noise, and other noise), returning both a confidence score and a human-like explanation. Evaluated on 3,168 ICA components from 124 EEG datasets, ICVision achieved k = 0.677 agreement with expert consensus, surpassing MNE ICLabel, while also preserving clinically relevant brain signals in ambiguous cases. Over 97% of its outputs were rated as interpretable and actionable by expert reviewers. As a core module of the open-source EEG Autoclean platform, ICVision signals a paradigm shift in scientific AI, where models do not just classify, but see, reason, and communicate. It opens the door to globally scalable, explainable, and reproducible EEG workflows, marking the emergence of AI agents capable of expert-level visual decision-making in brain science and beyond.", "AI": {"tldr": "ICVision 使用 GPT-4 Vision 模型来分类 EEG ICA 组件，直接解释各种可视化，并提供解释性和可操作性强的输出，表明了科学 AI 的范式转变。", "motivation": "为了模仿专家级别的 EEG ICA 组件分类，ICVision 提供了一种基于 AI 代理视觉和自然语言推理的方法，超越了依赖手工制作特征的传统分类器。", "method": "ICVision 使用多模态大语言模型（GPT-4 Vision）直接解释 ICA 仪表板可视化，包括地形图、时间序列、功率谱和 ERP 图。它将每个组件分类为六个规范类别之一（脑、眼、心脏、肌肉、通道噪声和其他噪声），提供置信度分数和人类理解的解释。", "result": "在 3,168 个 ICA 组件上评估 ICVision，这些组件来自 124 个 EEG 数据集，结果显示其与专家共识的 k = 0.677 的一致率超过了 MNE ICLabel。此外，其超过 97% 的输出被专家评审员认定为可解释和可操作。", "conclusion": "ICVision 标志着科学 AI 的一个分水岭，其作为一个核心模块在开放式 EEG 自动清理平台中，展示出基于多模态大语言模型的 AI 代理可以达到专家级别的视觉决策。"}}
{"id": "2512.00552", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.00552", "abs": "https://arxiv.org/abs/2512.00552", "authors": ["Subramanyam Sahoo", "Vinija Jain", "Saanidhya Vats", "Siddharth Mohapatra", "Rui Min", "Aman Chadha", "Divya Chaudhary"], "title": "Catch Me If You Can: How Smaller Reasoning Models Pretend to Reason with Mathematical Fidelity", "comment": "8 pages, 5 figures. A preprint. Initial Work", "summary": "Current evaluation of mathematical reasoning in language models relies primarily on answer accuracy, potentially masking fundamental failures in logical computation. We introduce a diagnostic framework that distinguishes genuine mathematical reasoning from superficial pattern matching through four complementary axes: forward-backward consistency, transitivity coverage, counterfactual sensitivity, and perturbation robustness. Through a case study applying this framework to Qwen3-0.6B on the MenatQA dataset, we reveal a striking disconnect between surface performance and reasoning fidelity. While the model achieves reasonable answer accuracy (70%+), it demonstrates poor backward consistency (15%), limited transitivity coverage (32.2%), and brittle sensitivity to perturbations. Our diagnostics expose reasoning failures invisible to traditional accuracy metrics, suggesting that this small model relies heavily on pattern matching rather than genuine logical computation. While our empirical findings are based on a single 600M-parameter model, the diagnostic framework itself is model-agnostic and generalizable. We release our evaluation protocols to enable the research community to assess reasoning fidelity across different model scales and architectures, moving beyond surface-level accuracy toward verifiable mathematical reasoning.", "AI": {"tldr": "本文提出一种新的诊断框架，发现模型在数学推理上虽然在准确性上表现不错，但在逻辑计算上存在严重不足，揭示了模式匹配和真实逻辑推理的差距。", "motivation": "现有的语言模型在数学推理上的评估主要依赖于答案的准确性，这可能会掩盖逻辑计算中的根本性失败。为了揭示表面性能和推理准确度之间的脱节，提出了一种新的诊断方法。", "method": "提出了一个诊断框架，该框架通过四个互补的轴——前后一致性、传递性覆盖、反事实敏感性和扰动鲁棒性——来区分真实的数学推理和表面的模式匹配。", "result": "Qwen3-0.6B在MenatQA数据集上的试验显示，尽管模型的准确性达到70%以上，但在前后一致性（15%）、传递性覆盖（32.2%）等多个指标上表现差，表明该模型更多依赖于模式匹配。", "conclusion": "提出的诊断框架揭示了模型在数学推理上的具体缺陷，表明即使是在6亿参数量的小模型上，也能反映出依赖表面模式匹配而非真实逻辑推理的问题。这一框架适用于不同规模和架构的模型评估。"}}
{"id": "2512.00198", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.00198", "abs": "https://arxiv.org/abs/2512.00198", "authors": ["Shantanu Ghosh", "Vedant Parthesh Joshi", "Rayan Syed", "Aya Kassem", "Abhishek Varshney", "Payel Basak", "Weicheng Dai", "Judy Wawira Gichoya", "Hari M. Trivedi", "Imon Banerjee", "Shyam Visweswaran", "Clare B. Poynton", "Kayhan Batmanghelich"], "title": "Mammo-FM: Breast-specific foundational model for Integrated Mammographic Diagnosis, Prognosis, and Reporting", "comment": null, "summary": "Breast cancer is one of the leading causes of death among women worldwide. We introduce Mammo-FM, the first foundation model specifically for mammography, pretrained on the largest and most diverse dataset to date - 140,677 patients (821,326 mammograms) across four U.S. institutions. Mammo-FM provides a unified foundation for core clinical tasks in breast imaging, including cancer diagnosis, pathology localization, structured report generation, and cancer risk prognosis within a single framework. Its alignment between images and text enables both visual and textual interpretability, improving transparency and clinical auditability, which are essential for real-world adoption. We rigorously evaluate Mammo-FM across diagnosis, prognosis, and report-generation tasks in in- and out-of-distribution datasets. Despite operating on native-resolution mammograms and using only one-third of the parameters of state-of-the-art generalist FMs, Mammo-FM consistently outperforms them across multiple public and private benchmarks. These results highlight the efficiency and value of domain-specific foundation models designed around the full spectrum of tasks within a clinical domain and emphasize the importance of rigorous, domain-aligned evaluation.", "AI": {"tldr": "本研究介绍了Mammo-FM，一种专为乳腺摄影设计的基础模型，基于大规模数据集训练。该模型能够执行多项关键临床任务，并且优于常用的基础模型。", "motivation": "乳腺癌是全球女性死亡的主要原因之一。本文旨在介绍Mammo-FM，一款专为乳腺摄影领域设计的基础模型，以改善乳腺癌诊断和治疗过程中的数据分析与决策支持。", "method": "我们介绍了Mammo-FM，首个专门用于乳腺摄影的预训练基础模型，该模型基于迄今为止最大且最多样化的数据集进行训练——来自四个美国机构的140,677名患者（821,326张乳腺摄影图像）。Mammo-FM为乳腺成像的核心临床任务提供了统一的基础，包括癌症诊断、病理定位、结构化报告生成以及癌症风险预测。其图像与文本的对齐功能增强了可视和文本的解释性，提高了透明度和临床审查能力，这是实现实际应用的重要因素。", "result": "我们对Mammo-FM进行了严格的评估，测试了其在诊断、预测和报告生成任务上的性能，包括在内分布数据集和外分布数据集上的表现。尽管处理的是原始分辨率的乳腺摄影图像并且使用的是最先进的通用基础模型参数量的三分之一，Mammo-FM在多个公共和私有的基准测试中持续表现出色。", "conclusion": "研究结果显示，专门为一个临床领域设计的、涵盖该领域全范围任务的域特定基础模型不仅效率高，而且有很高的应用价值。这些结果也强调了严格、以领域为导向的评估的重要性。"}}
{"id": "2512.00579", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.00579", "abs": "https://arxiv.org/abs/2512.00579", "authors": ["Miroslav Blšták"], "title": "Slovak Conceptual Dictionary", "comment": "7 pages, 2 figures", "summary": "When solving tasks in the field of natural language processing, we sometimes need dictionary tools, such as lexicons, word form dictionaries or knowledge bases. However, the availability of dictionary data is insufficient in many languages, especially in the case of low resourced languages. In this article, we introduce a new conceptual dictionary for the Slovak language as the first linguistic tool of this kind. Since Slovak language is a language with limited linguistic resources and there are currently not available any machine-readable linguistic data sources with a sufficiently large volume of data, many tasks which require automated processing of Slovak text achieve weaker results compared to other languages and are almost impossible to solve.", "AI": {"tldr": "A new conceptual dictionary for the Slovak language is introduced to overcome the insufficiency of linguistic data and improve NLP task performance.", "motivation": "To tackle the scarcity of dictionary data, particularly for low-resourced languages like Slovak, and to boost the performance of text processing tasks.", "method": "We introduce a novel conceptual dictionary for the Slovak language to address the lack of linguistic resources and improve text processing tasks.", "result": "The new dictionary is the first substantial linguistic tool for Slovak, enhancing the potential for effective natural language processing tasks.", "conclusion": "The creation of a machine-readable dictionary for the Slovak language is a significant step towards advancing natural language processing capabilities for languages with limited resources."}}
{"id": "2512.00208", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.00208", "abs": "https://arxiv.org/abs/2512.00208", "authors": ["Hajra Anwar Beg", "Baptiste Chopin", "Hao Tang", "Mohamed Daoudi"], "title": "ReactionMamba: Generating Short &Long Human Reaction Sequences", "comment": null, "summary": "We present ReactionMamba, a novel framework for generating long 3D human reaction motions. Reaction-Mamba integrates a motion VAE for efficient motion encoding with Mamba-based state-space models to decode temporally consistent reactions. This design enables ReactionMamba to generate both short sequences of simple motions and long sequences of complex motions, such as dance and martial arts. We evaluate ReactionMamba on three datasets--NTU120-AS, Lindy Hop, and InterX--and demonstrate competitive performance in terms of realism, diversity, and long-sequence generation compared to previous methods, including InterFormer, ReMoS, and Ready-to-React, while achieving substantial improvements in inference speed.", "AI": {"tldr": "ReactionMamba是用于生成3D人类反应动作的新框架，它结合了VAE和Mamba模型，提高了动作生成效率和长序列动作的真实感，优于以前的方法。", "motivation": "创建一种能够生成高效且一致的3D人类反应动作框架，以克服现有方法在生成复杂长时间动作序列时的表现和效率问题。", "method": "ReactionMamba结合了运动变分自编码器（VAE）和Mamba状态空间模型来生成高效的3D人体反应动作。", "result": "在NTU120-AS、Lindy Hop和InterX三个数据集上的评估显示，ReactionMamba在真实感、多样性和长序列生成方面表现出色，并且在推理速度上相比以前的方法有显著提升。", "conclusion": "ReactionMamba不仅能够生成简单动作，也能生成复杂的舞蹈和武术动作，同时在几个关键指标上优于已有的方法。"}}
{"id": "2512.00590", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.00590", "abs": "https://arxiv.org/abs/2512.00590", "authors": ["Alla Chepurova", "Aydar Bulatov", "Yuri Kuratov", "Mikhail Burtsev"], "title": "Wikontic: Constructing Wikidata-Aligned, Ontology-Aware Knowledge Graphs with Large Language Models", "comment": null, "summary": "Knowledge graphs (KGs) provide structured, verifiable grounding for large language models (LLMs), but current LLM-based systems commonly use KGs as auxiliary structures for text retrieval, leaving their intrinsic quality underexplored. In this work, we propose Wikontic, a multi-stage pipeline that constructs KGs from open-domain text by extracting candidate triplets with qualifiers, enforcing Wikidata-based type and relation constraints, and normalizing entities to reduce duplication. The resulting KGs are compact, ontology-consistent, and well-connected; on MuSiQue, the correct answer entity appears in 96% of generated triplets. On HotpotQA, our triplets-only setup achieves 76.0 F1, and on MuSiQue 59.8 F1, matching or surpassing several retrieval-augmented generation baselines that still require textual context. In addition, Wikontic attains state-of-the-art information-retention performance on the MINE-1 benchmark (86%), outperforming prior KG construction methods. Wikontic is also efficient at build time: KG construction uses less than 1,000 output tokens, about 3$\\times$ fewer than AriGraph and $<$1/20 of GraphRAG. The proposed pipeline enhances the quality of the generated KG and offers a scalable solution for leveraging structured knowledge in LLMs.", "AI": {"tldr": "This paper presents Wikontic, a pipeline for constructing high-quality KGs from open-domain texts, which performs well on benchmarks and demonstrates efficiency and scalability in KG construction for LLMs.", "motivation": "The motivation is to explore and improve the intrinsic quality of KGs used in LLMs, moving beyond their current usage as auxiliary structures for text retrieval.", "method": "The paper introduces Wikontic, a multi-stage KG construction pipeline from open-domain texts. It extracts triplets with qualifiers, imposes type and relation constraints based on Wikidata, and normalizes entities for redundancy reduction.", "result": "The constructed KGs achieve high performance on various benchmarks such as MuSiQue and HotpotQA, outperforming retrieval-augmented generation baselines. Wikontic also shows efficiency in terms of output tokens used for KG construction.", "conclusion": "The proposed method offers a scalable and efficient way to construct high-quality KGs, enhancing LLMs' capabilities by providing well-structured and actionable knowledge."}}
{"id": "2512.00226", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.00226", "abs": "https://arxiv.org/abs/2512.00226", "authors": ["Zirui Wang", "Tao Zhang"], "title": "DenseScan: Advancing 3D Scene Understanding with 2D Dense Annotation", "comment": "Workshop on Space in Vision, Language, and Embodied AI at NeurIPS 2025", "summary": "3D understanding is a key capability for real-world AI assistance. High-quality data plays an important role in driving the development of the 3D understanding community. Current 3D scene understanding datasets often provide geometric and instance-level information, yet they lack the rich semantic annotations necessary for nuanced visual-language tasks.In this work, we introduce DenseScan, a novel dataset with detailed multi-level descriptions generated by an automated pipeline leveraging multi-view 2D images and multimodal large language models (MLLMs). Our approach enables dense captioning of scene elements, ensuring comprehensive object-level descriptions that capture context-sensitive details. Furthermore, we extend these annotations through scenario-based question generation, producing high-level queries that integrate object properties, spatial relationships, and scene context. By coupling geometric detail with semantic richness, DenseScan broadens the range of downstream tasks, from detailed visual-language navigation to interactive question answering. Experimental results demonstrate that our method significantly enhances object-level understanding and question-answering performance in 3D environments compared to traditional annotation pipelines. We release both the annotated dataset and our annotation pipeline to facilitate future research and applications in robotics, augmented reality, and beyond. Through DenseScan, we aim to catalyze new avenues in 3D scene understanding, allowing researchers and practitioners to tackle the complexities of real-world environments with richer, more contextually aware annotations.", "AI": {"tldr": "介绍了一种创新的数据集DenseScan，它利用多视角2D图像和多模态语言模型生成详细多级描述，显著提高了3D环境下的对象级理解和问答性能。", "motivation": "现有的3D场景理解数据集通常提供几何和实例级信息，但缺乏丰富的语义标注，这对于复杂的视觉语言任务是必要的。通过DenseScan，旨在促进未来在机器人、增强现实等领域的研究和应用。", "method": "通过利用多视角2D图像和多模态大型语言模型(MLLMs)的自动化管道，DenseScan实现了场景元素的密集描述，能够提供详尽的对象级描述，捕捉上下文敏感的细节。此外，通过基于场景的问题生成，扩展这些注释，产生整合对象属性、空间关系和场景背景的高层次查询。", "result": "通过实验结果表明，与传统的注释流水线相比，该方法在3D环境中的对象级理解和问答性能上显著提升。", "conclusion": "通过结合几何细节与语义丰富性，DenseScan拓宽了下游任务的范围，从详细的视觉语言导航到互动问答。这为理解和处理现实世界环境的复杂性提供了更丰富、更上下文感知的注释。"}}
{"id": "2512.00611", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.00611", "abs": "https://arxiv.org/abs/2512.00611", "authors": ["Franck Binard", "Vanja Kljajevic"], "title": "Prism: A Minimal Compositional Metalanguage for Specifying Agent Behavior", "comment": null, "summary": "Prism is a small, compositional metalanguage for specifying the behaviour of tool-using software agents. Rather than introducing ad hoc control constructs, Prism is built around a fixed core context, Core1, which provides a minimal background grammar of categories numbers, strings, user prompts, tools together with abstract combinators for booleans, predicates, pairs, and lists. Agent policies are written as ordinary expressions using a single abstraction operator so that conditionals appear as selections between alternatives instead of imperative if-else blocks. Domains extend the core by defining their own context-mini-grammars that introduce new categories, predicates, and external tools while reusing the same compositional machinery. We illustrate this with worked examples from thermostat control, home security, e-commerce recommendation, and medical monitoring, showing how natural language decision rules can be mapped to inspectable, executable policies. From a linguistic perspective, Prism enforces a clear separation between a reusable grammar-like core and domain specific lexicons and treats tools as bridges between internal policy representations and the external world. From an engineering perspective, it offers a compact interface language for agent control, making the space of possible actions explicit and amenable to analysis, verification, and safety constraints.", "AI": {"tldr": "Prism是一种用于指定和分析软件代理行为的元语言，它提供了一个固定的核心背景，并允许通过组合来扩展。这种方法在多个应用领域如家庭监控和电子商务中，展示了将自然语言规则转换为可检视和可执行策略的能力。", "motivation": "Prism的动机在于提供一种更规范、可分析的控制手段，用于构建和分析软件代理的行为逻辑。通过分离核心语法和领域特定词汇来增强对代理策略执行的可检验性。", "method": "Prism是一种小型的组合元语言，用于指定使用工具的软件代理行为。Prism的核心是Core1，它提供了一个最小的背景语法规则，包括数字、字符串、用户提示、工具等抽象组合子。代理策略被写成普通的表达式，使用单一的抽象操作符，使得条件表达式出现在选项之间而不是if-else块中。域通过定义自己的上下文-小语法来扩展核心，引入新的类别、谓词和外部工具，同时复用相同组合机制。", "result": "通过使用Prism，可以将以自然语言描述的决策规则映射到可检视的、可执行的策略中。这一方法在智能温控、家庭安全、电子商务推荐和医疗监控等领域中得到了验证。", "conclusion": "从语言学角度来看，Prism提供了核心可复用语法和领域特定词汇之间的明确分离，并将工具视为内部策略表示和外部世界之间的桥梁。从工程角度来看，它提供了一个紧凑的代理控制接口语言，使其具有明确和便于分析的潜在行动，并便于安全约束。"}}
{"id": "2512.00255", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.00255", "abs": "https://arxiv.org/abs/2512.00255", "authors": ["Kunwar Maheep Singh", "Jianchun Chen", "Vladislav Golyanik", "Stephan J. Garbin", "Thabo Beeler", "Rishabh Dabral", "Marc Habermann", "Christian Theobalt"], "title": "Relightable Holoported Characters: Capturing and Relighting Dynamic Human Performance from Sparse Views", "comment": null, "summary": "We present Relightable Holoported Characters (RHC), a novel person-specific method for free-view rendering and relighting of full-body and highly dynamic humans solely observed from sparse-view RGB videos at inference. In contrast to classical one-light-at-a-time (OLAT)-based human relighting, our transformer-based RelightNet predicts relit appearance within a single network pass, avoiding costly OLAT-basis capture and generation. For training such a model, we introduce a new capture strategy and dataset recorded in a multi-view lightstage, where we alternate frames lit by random environment maps with uniformly lit tracking frames, simultaneously enabling accurate motion tracking and diverse illumination as well as dynamics coverage. Inspired by the rendering equation, we derive physics-informed features that encode geometry, albedo, shading, and the virtual camera view from a coarse human mesh proxy and the input views. Our RelightNet then takes these features as input and cross-attends them with a novel lighting condition, and regresses the relit appearance in the form of texel-aligned 3D Gaussian splats attached to the coarse mesh proxy. Consequently, our RelightNet implicitly learns to efficiently compute the rendering equation for novel lighting conditions within a single feed-forward pass. Experiments demonstrate our method's superior visual fidelity and lighting reproduction compared to state-of-the-art approaches. Project page: https://vcai.mpi-inf.mpg.de/projects/RHC/", "AI": {"tldr": "The paper introduces Relightable Holoported Characters (RHC), utilizing a novel transformer-based method, RelightNet, for the real-time relighting of human characters under different lighting conditions, achieved through sparse RGB video inputs and physics-informed features.", "motivation": "The motivation is to develop a method for free-view rendering and relighting of full-body and highly dynamic humans using sparse-view RGB videos, improving upon the classical one-light-at-a-time approach by predicting relit appearance in a single network pass.", "method": "RelightNet, a transformer-based model, predicts the appearance of full-body dynamic humans under novel lighting conditions using sparse-view RGB videos. It encodes physics-informed features derived from a coarse human mesh proxy and input views, and then regresses the relit appearance as 3D Gaussian splats attached to the mesh.", "result": "Experiments showed that the proposed method provides superior visual fidelity and lighting reproduction compared to existing state-of-the-art approaches.", "conclusion": "RelightNet achieves efficient computation of the rendering equation for novel lighting conditions in a single feed-forward pass, marking an improvement in the field of virtual human rendering and relighting."}}
{"id": "2512.00617", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.00617", "abs": "https://arxiv.org/abs/2512.00617", "authors": ["Omer Jauhar Khan"], "title": "ART: Adaptive Response Tuning Framework -- A Multi-Agent Tournament-Based Approach to LLM Response Optimization", "comment": "8 pages, 5 figures, 5 tables. Conference-style paper", "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in natural language understanding and generation. However, single-model responses often exhibit inconsistencies, hallucinations, and varying quality across different query domains. This paper presents ART (Adaptive Response Tuning), a novel framework that employs tournament-style ELO ranking and multi-agent reasoning to systematically optimize LLM outputs. By enabling multiple LLM agents to compete, critique, and collaborate through structured tournament workflows, ART produces consensus responses that outperform individual model outputs. Our framework introduces configurable tournament parameters, dynamic agent selection, and multiple consensus fusion strategies. Experimental evaluations demonstrate significant improvements in response accuracy, coherence, and reliability compared to baseline single-model approaches. The ART framework provides a scalable, production-ready solution for applications requiring high-quality, vetted LLM responses, achieving an 8.4% improvement in overall quality metrics and R22 values exceeding 0.96 in ELO rating convergence.", "AI": {"tldr": "ART利用锦标赛风格的ELO排名和多智能体推理来提升LLM的性能。", "motivation": "解决LLM响应中的不一致、幻觉及跨不同查询域的质量变化问题。", "method": "Structure", "result": "{\"tldr\": \"ART is a framework using tournament-style ELO ranking and multi-agent reasoning to improve the performance of LLMs.\", \"motivation\": \"To address inconsistencies, hallucinations, and varying quality in LLM responses across different query domains.\", \"method\": \"Employs multiple LLM agents to compete, critique, and collaborate in structured workflows for improved output.\", \"result\": \"Significant improvements in response accuracy, coherence, and reliability over single-model approaches. An 8.4% improvement in overall quality metrics and R22 values above 0.96 in ELO rating convergence.\", \"conclusion\": \"ART provides a scalable and production-ready solution for producing high-quality, vetted LLM responses across various applications.\"}", "conclusion": "ART提供了一个可扩展且生产就绪的解决方案，用于生成高质量、验证过的LLM响应，适用于多种应用。"}}
{"id": "2512.00261", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.00261", "abs": "https://arxiv.org/abs/2512.00261", "authors": ["Yuzhen Hu", "Saurabh Prasad"], "title": "UniDiff: Parameter-Efficient Adaptation of Diffusion Models for Land Cover Classification with Multi-Modal Remotely Sensed Imagery and Sparse Annotations", "comment": "Camera-ready for WACV 2026", "summary": "Sparse annotations fundamentally constrain multimodal remote sensing: even recent state-of-the-art supervised methods such as MSFMamba are limited by the availability of labeled data, restricting their practical deployment despite architectural advances. ImageNet-pretrained models provide rich visual representations, but adapting them to heterogeneous modalities such as hyperspectral imaging (HSI) and synthetic aperture radar (SAR) without large labeled datasets remains challenging. We propose UniDiff, a parameter-efficient framework that adapts a single ImageNet-pretrained diffusion model to multiple sensing modalities using only target-domain data. UniDiff combines FiLM-based timestep-modality conditioning, parameter-efficient adaptation of approximately 5% of parameters, and pseudo-RGB anchoring to preserve pre-trained representations and prevent catastrophic forgetting. This design enables effective feature extraction from remote sensing data under sparse annotations. Our results with two established multi-modal benchmarking datasets demonstrate that unsupervised adaptation of a pre-trained diffusion model effectively mitigates annotation constraints and achieves effective fusion of multi-modal remotely sensed data.", "AI": {"tldr": "UniDiff adapts a pre-trained diffusion model to different remote sensing data with minimal parameters, enhancing feature extraction under limited annotations.", "motivation": "Sparse annotations limit the practical deployment of state-of-the-art supervised methods in multimodal remote sensing, even with architectural advances.", "method": "UniDiff, a parameter-efficient framework that adapts a single ImageNet-pretrained diffusion model to multiple sensing modalities using only target-domain data.", "result": "The proposed method effectively mitigates annotation constraints and achieves effective fusion of multi-modal remotely sensed data as demonstrated by results with two established multi-modal benchmarking datasets.", "conclusion": "The unsupervised adaptation of a pre-trained diffusion model using only target-domain data can effectively address the challenge of sparse annotations in remote sensing tasks."}}
{"id": "2512.00656", "categories": ["cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2512.00656", "abs": "https://arxiv.org/abs/2512.00656", "authors": ["Jan Batzner", "Volker Stocker", "Stefan Schmid", "Gjergji Kasneci"], "title": "Sycophancy Claims about Language Models: The Missing Human-in-the-Loop", "comment": "NeurIPS 2025 Workshop on LLM Evaluation and ICLR 2025 Workshop on Bi-Directional Human-AI Alignment", "summary": "Sycophantic response patterns in Large Language Models (LLMs) have been increasingly claimed in the literature. We review methodological challenges in measuring LLM sycophancy and identify five core operationalizations. Despite sycophancy being inherently human-centric, current research does not evaluate human perception. Our analysis highlights the difficulties in distinguishing sycophantic responses from related concepts in AI alignment and offers actionable recommendations for future research.", "AI": {"tldr": "本文回顾了在大型语言模型中测量谄媚反应模式的方法论挑战，并提出五个核心的操作化方法。强调了在AI对齐研究中识别谄媚反应与其他概念的区别及未来研究方向的建议。", "motivation": "由于现有研究忽视了人类对谄媚反应的感知，本文旨在解决在评估大型语言模型中的谄媚反应时面临的方法论难题。", "method": "Structure", "result": "{\n  \"tldr\": \"本文回顾了在大型语言模型中测量谄媚反应模式的方法论挑战，并提出五个核心的操作化方法。强调了在AI对齐研究中识别谄媚反应与其他概念的区别及未来研究方向的建议。\", \n  \"motivation\": \"由于现有研究忽视了人类对谄媚反应的感知，本文旨在解决在评估大型语言模型中的谄媚反应时面临的方法论难题。\", \n  \"method\": \"通过回顾文献，识别并归纳了谄媚反应的五个核心操作化定义，分析了谄媚反应与其他概念在AI对齐中的区别。\", \n  \"result\": \"本文指出了区分大型语言模型中的谄媚反应与其他概念的困难，并提出了未来研究的方向和操作建议。\", \n  \"conclusion\": \"文章强调了在大模型评估中加入人类感知评估的必要性及其对改进AI行为的重要性。建议未来研究应更多地关注这一方面。\"}\n}", "conclusion": "文章强调了在大模型评估中加入人类感知评估的必要性及其对改进AI行为的重要性。建议未来研究应更多地关注这一方面。"}}
{"id": "2512.00264", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.00264", "abs": "https://arxiv.org/abs/2512.00264", "authors": ["Zhengda Ma", "Abhirup Banerjee"], "title": "HeartFormer: Semantic-Aware Dual-Structure Transformers for 3D Four-Chamber Cardiac Point Cloud Reconstruction", "comment": null, "summary": "We present the first geometric deep learning framework based on point cloud representation for 3D four-chamber cardiac reconstruction from cine MRI data. This work addresses a long-standing limitation in conventional cine MRI, which typically provides only 2D slice images of the heart, thereby restricting a comprehensive understanding of cardiac morphology and physiological mechanisms in both healthy and pathological conditions. To overcome this, we propose \\textbf{HeartFormer}, a novel point cloud completion network that extends traditional single-class point cloud completion to the multi-class. HeartFormer consists of two key components: a Semantic-Aware Dual-Structure Transformer Network (SA-DSTNet) and a Semantic-Aware Geometry Feature Refinement Transformer Network (SA-GFRTNet). SA-DSTNet generates an initial coarse point cloud with both global geometry features and substructure geometry features. Guided by these semantic-geometry representations, SA-GFRTNet progressively refines the coarse output, effectively leveraging both global and substructure geometric priors to produce high-fidelity and geometrically consistent reconstructions. We further construct \\textbf{HeartCompv1}, the first publicly available large-scale dataset with 17,000 high-resolution 3D multi-class cardiac meshes and point-clouds, to establish a general benchmark for this emerging research direction. Extensive cross-domain experiments on HeartCompv1 and UK Biobank demonstrate that HeartFormer achieves robust, accurate, and generalizable performance, consistently surpassing state-of-the-art (SOTA) methods. Code and dataset will be released upon acceptance at: https://github.com/10Darren/HeartFormer.", "AI": {"tldr": "提出基于点云的几何深度学习框架HeartFormer，用于从心脏磁共振成像（MRI）数据重构3D四腔心脏模型，以及开发了一个大型公开数据集HeartCompv1进行性能评估。", "motivation": "现有的MRI技术通常只能提供心脏的二维切片图像，限制了对心脏形态学和生理机制的全面理解。HeartFormer旨在改进这一点，通过提供高质量的三维心脏重构。", "method": "HeartFormer由两部分组成：一是生成初始粗点云的SA-DSTNet，二是基于这些特征逐步细化的SA-GFRTNet，共同实现高保真度的几何一致重建。", "result": "通过在HeartCompv1和UK Biobank进行的跨领域实验，证明HeartFormer在性能上超过现有最新方法，表现强大、准确且可泛化。", "conclusion": "HeartFormer是首个面向多类点云完成任务的网络，结合新颖的几何深度学习框架，能够实现高质量的心脏3D重构，并且公开了大规模数据集作为新的基准测试数据集。"}}
{"id": "2512.00663", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.00663", "abs": "https://arxiv.org/abs/2512.00663", "authors": ["Tanmay Agrawal"], "title": "Graphing the Truth: Structured Visualizations for Automated Hallucination Detection in LLMs", "comment": null, "summary": "Large Language Models have rapidly advanced in their ability to interpret and generate natural language. In enterprise settings, they are frequently augmented with closed-source domain knowledge to deliver more contextually informed responses. However, operational constraints such as limited context windows and inconsistencies between pre-training data and supplied knowledge often lead to hallucinations, some of which appear highly credible and escape routine human review. Current mitigation strategies either depend on costly, large-scale gold-standard Q\\&A curation or rely on secondary model verification, neither of which offers deterministic assurance. This paper introduces a framework that organizes proprietary knowledge and model-generated content into interactive visual knowledge graphs. The objective is to provide end users with a clear, intuitive view of potential hallucination zones by linking model assertions to underlying sources of truth and indicating confidence levels. Through this visual interface, users can diagnose inconsistencies, identify weak reasoning chains, and supply corrective feedback. The resulting human-in-the-loop workflow creates a structured feedback loop that can enhance model reliability and continuously improve response quality.", "AI": {"tldr": "本文提出一个框架，通过将专有知识和模型生成的内容组织成交互式的知识图谱，帮助用户识别潜在的幻觉区域，并提供纠正反馈，从而改进模型的可靠性。", "motivation": "大型语言模型虽然在理解和生成自然语言方面取得了快速进步，但在企业环境中，由于操作限制和预训练数据与提供的知识之间的一致性问题，容易产生难以察觉的幻觉。现有的减轻策略成本高昂或不能提供确定性保障。", "method": "该论文介绍了一个框架，通过将专有知识和模型生成的内容转换成交互式的视觉知识图谱，使得用户能够直观地查看模型断言与基础真实源之间的关联，并指明可信度水平。", "result": "通过这个视觉交互界面，用户可以诊断不一致之处，识别薄弱的推理链，并提供纠正反馈。这创造了一个结构化的反馈循环，可以增强模型的可靠性，并持续改进响应质量。", "conclusion": "该框架提供一种新的减轻大型语言模型幻觉问题的方法，这种方式更加直观、成本效益高，能够促进模型的持续改进。"}}
{"id": "2512.00269", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.00269", "abs": "https://arxiv.org/abs/2512.00269", "authors": ["Jun Wang", "Peirong Liu"], "title": "USB: Unified Synthetic Brain Framework for Bidirectional Pathology-Healthy Generation and Editing", "comment": "16 pages, 17 figures", "summary": "Understanding the relationship between pathological and healthy brain structures is fundamental to neuroimaging, connecting disease diagnosis and detection with modeling, prediction, and treatment planning. However, paired pathological-healthy data are extremely difficult to obtain, as they rely on pre- and post-treatment imaging, constrained by clinical outcomes and longitudinal data availability. Consequently, most existing brain image generation and editing methods focus on visual quality yet remain domain-specific, treating pathological and healthy image modeling independently. We introduce USB (Unified Synthetic Brain), the first end-to-end framework that unifies bidirectional generation and editing of pathological and healthy brain images. USB models the joint distribution of lesions and brain anatomy through a paired diffusion mechanism and achieves both pathological and healthy image generation. A consistency guidance algorithm further preserves anatomical consistency and lesion correspondence during bidirectional pathology-healthy editing. Extensive experiments on six public brain MRI datasets including healthy controls, stroke, and Alzheimer's patients, demonstrate USB's ability to produce diverse and realistic results. By establishing the first unified benchmark for brain image generation and editing, USB opens opportunities for scalable dataset creation and robust neuroimaging analysis. Code is available at https://github.com/jhuldr/USB.", "AI": {"tldr": "本文提出了一种名为USB的端到端框架，以统一生成和编辑病理性和健康的大脑图像，提供了解剖一致性，并建立了第一个统一的基准，可用于生成可扩展的数据集和稳健的神经成像分析。", "motivation": "理解病理和健康大脑结构之间的关系是神经成像的基础，而两两匹配的病理-健康数据很难获得。现有的大多数大脑图像生成和编辑方法专注于视觉质量却缺乏领域通用性，并且独立处理病理性和健康图像建模。", "method": "介绍了一个名为USB（统一合成大脑）的端到端框架，该框架能够统一双向生成和编辑病理性和健康的大脑图像。通过配对扩散机制来建模损伤和大脑解剖结构的联合分布，并通过一致性的指导算法保证在双向病理-健康编辑中的解剖一致性。", "result": "在六个公共大脑MRI数据集上的广泛实验，包括健康控制、中风和阿尔茨海默病患者，表明USB能够产生多样化和逼真的结果。", "conclusion": "通过USB框架的实现，为大脑图像生成和编辑建立了第一个统一的基准，这开创了创建大规模数据集和进行稳健神经成像分析的可能性。"}}
{"id": "2512.00673", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.00673", "abs": "https://arxiv.org/abs/2512.00673", "authors": ["Breanna E. Green", "Ashley L. Shea", "Pengfei Zhao", "Drew B. Margolin"], "title": "A Comparison of Human and ChatGPT Classification Performance on Complex Social Media Data", "comment": "About 15 pages, draft version of accepted conference full paper. Published paper to follow", "summary": "Generative artificial intelligence tools, like ChatGPT, are an increasingly utilized resource among computational social scientists. Nevertheless, there remains space for improved understanding of the performance of ChatGPT in complex tasks such as classifying and annotating datasets containing nuanced language. Method. In this paper, we measure the performance of GPT-4 on one such task and compare results to human annotators. We investigate ChatGPT versions 3.5, 4, and 4o to examine performance given rapid changes in technological advancement of large language models. We craft four prompt styles as input and evaluate precision, recall, and F1 scores. Both quantitative and qualitative evaluations of results demonstrate that while including label definitions in prompts may help performance, overall GPT-4 has difficulty classifying nuanced language. Qualitative analysis reveals four specific findings. Our results suggest the use of ChatGPT in classification tasks involving nuanced language should be conducted with prudence.", "AI": {"tldr": "The paper investigates the performance of ChatGPT versions in classifying nuanced language datasets, finding that while label definitions can improve performance, GPT-4 still struggles with this task.", "motivation": "To better understand the effectiveness and limitations of ChatGPT models in handling complex tasks that involve nuanced and complex language.", "method": "The authors measure the performance of GPT-4, GPT-3.5, and GPT-4o on a classification task, using four different prompt styles and comparing the results to human annotators. Precision, recall, and F1 scores are used for quantitative evaluation.", "result": "Quantitative and qualitative evaluations reveal that while the inclusion of label definitions in prompts can improve performance, GPT-4 encounters significant challenges in classifying nuanced language. Four specific qualitative findings are highlighted.", "conclusion": "The paper suggests that the use of ChatGPT in classification tasks involving nuanced language should be carried out with caution due to the model's limitations in accurately classifying such data."}}
{"id": "2512.00275", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.00275", "abs": "https://arxiv.org/abs/2512.00275", "authors": ["Yi Liu", "Yi Wan", "Xinyi Liu", "Qiong Wu", "Panwang Xia", "Xuejun Huang", "Yongjun Zhang"], "title": "HIMOSA: Efficient Remote Sensing Image Super-Resolution with Hierarchical Mixture of Sparse Attention", "comment": null, "summary": "In remote sensing applications, such as disaster detection and response, real-time efficiency and model lightweighting are of critical importance. Consequently, existing remote sensing image super-resolution methods often face a trade-off between model performance and computational efficiency. In this paper, we propose a lightweight super-resolution framework for remote sensing imagery, named HIMOSA. Specifically, HIMOSA leverages the inherent redundancy in remote sensing imagery and introduces a content-aware sparse attention mechanism, enabling the model to achieve fast inference while maintaining strong reconstruction performance. Furthermore, to effectively leverage the multi-scale repetitive patterns found in remote sensing imagery, we introduce a hierarchical window expansion and reduce the computational complexity by adjusting the sparsity of the attention. Extensive experiments on multiple remote sensing datasets demonstrate that our method achieves state-of-the-art performance while maintaining computational efficiency.", "AI": {"tldr": "本文提出了一种名为HIMOSA的遥感图像轻量级超分辨率框架，在保持重建性能的同时实现了快速推理。", "motivation": "在诸如灾害检测与响应等遥感应用中，实时效率和模型轻量化至关重要。现有遥感图像超分辨率方法通常面临着模型性能和计算效率之间的权衡。", "method": "本文提出了一种针对遥感图像的轻量级超分辨率框架HIMOSA。该框架利用遥感图像中的固有冗余，并引入了基于内容感知的稀疏注意力机制，使得模型在保持强大重建性能的同时实现快速推理。此外，为了有效利用遥感图像中的多尺度重复模式，提出了分层窗口扩展技术，并通过调整注意力的稀疏性来降低计算复杂度。", "result": "在多个遥感数据集上的广泛实验表明，本文提出的方法在保持计算效率的同时达到了最先进的性能。", "conclusion": "实验结果表明，本文提出的方法在多个遥感数据集上展示了强大的性能，同时保持了良好的计算效率，达到了最先进的水平。"}}
{"id": "2512.00745", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.00745", "abs": "https://arxiv.org/abs/2512.00745", "authors": ["Md Abdullah Al Kafi", "Sumit Kumar Banshal"], "title": "FastPOS: Language-Agnostic Scalable POS Tagging Framework Low-Resource Use Case", "comment": null, "summary": "This study proposes a language-agnostic transformer-based POS tagging framework designed for low-resource languages, using Bangla and Hindi as case studies. With only three lines of framework-specific code, the model was adapted from Bangla to Hindi, demonstrating effective portability with minimal modification. The framework achieves 96.85 percent and 97 percent token-level accuracy across POS categories in Bangla and Hindi while sustaining strong F1 scores despite dataset imbalance and linguistic overlap. A performance discrepancy in a specific POS category underscores ongoing challenges in dataset curation. The strong results stem from the underlying transformer architecture, which can be replaced with limited code adjustments. Its modular and open-source design enables rapid cross-lingual adaptation while reducing model design and tuning overhead, allowing researchers to focus on linguistic preprocessing and dataset refinement, which are essential for advancing NLP in underrepresented languages.", "AI": {"tldr": "本研究提出了一种针对低资源语言的通用Transformer词性标注框架，并以孟加拉语和印地语为案例研究。该框架只需少量代码即可在两种语言之间转换，展示了极高的可移植性。框架在孟加拉语和印地语中分别达到了96.85%和97%的标签准确率，并且在面对数据不平衡和语言重叠的情况下仍能保持优秀的F1分数。部分词性类别的性能差异则指出了数据集构建的挑战。该框架的模块化和开源设计允许快速跨语言调整，降低模型设计和调整的工作量，使研究人员可以专注于语言预处理和数据集优化，从而促进在未充分代表的语言中的NLP发展。", "motivation": "为了提高低资源语言的自然语言处理性能，尤其是词性标注，且减少开发过程中的调整和设计工作量，团队设计了这种新的框架。", "method": "研究采用Transformer架构，进行词性标注，框架提供简易接口使模型能在不同语言之间转换。", "result": "框架在孟加拉语和印地语上实现了高准确率和优秀的F1分数，尽管存在数据集不平衡的问题。部分词性标签的性能差异也暴露出数据集构造中的挑战。", "conclusion": "该框架通过模块化设计支持跨语言的简便适应，并通过开放源代码减少了模型设计上的负担，为进一步改善未充分代表语言的自然语言处理提供了新的机会。"}}
{"id": "2512.00281", "categories": ["cs.CV", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2512.00281", "abs": "https://arxiv.org/abs/2512.00281", "authors": ["Sylvain Bodard", "Pierre Baudot", "Benjamin Renoust", "Charles Voyton", "Gwendoline De Bie", "Ezequiel Geremia", "Van-Khoa Le", "Danny Francis", "Pierre-Henri Siot", "Yousra Haddou", "Vincent Bobin", "Jean-Christophe Brisset", "Carey C. Thomson", "Valerie Bourdes", "Benoit Huet"], "title": "Rethinking Lung Cancer Screening: AI Nodule Detection and Diagnosis Outperforms Radiologists, Leading Models, and Standards Beyond Size and Growth", "comment": "25 pages, 8 figures, with supplementary information containing 11 figures", "summary": "Early detection of malignant lung nodules is critical, but its dependence on size and growth in screening inherently delays diagnosis. We present an AI system that redefines lung cancer screening by performing both detection and malignancy diagnosis directly at the nodule level on low-dose CT scans. To address limitations in dataset scale and explainability, we designed an ensemble of shallow deep learning and feature-based specialized models. Trained and evaluated on 25,709 scans with 69,449 annotated nodules, the system outperforms radiologists, Lung-RADS, and leading AI models (Sybil, Brock, Google, Kaggle). It achieves an area under the receiver operating characteristic curve (AUC) of 0.98 internally and 0.945 on an independent cohort. With 0.5 false positives per scan at 99.3\\% sensitivity, it addresses key barriers to AI adoption. Critically, it outperforms radiologists across all nodule sizes and stages, excelling in stage 1 cancers, and all growth-based metrics, including the least accurate: Volume-Doubling Time. It also surpasses radiologists by up to one year in diagnosing indeterminate and slow-growing nodules.", "AI": {"tldr": "设计了一种在低剂量CT扫描中直接检测和诊断恶性肺结节的AI系统，该系统优于人类专家和其他现有AI模型，特别是在早期和缓慢生长的癌变诊断方面表现出色。", "motivation": "早期发现恶性肺结节至关重要，但目前的方法依赖于结节的大小和增长速度，这会推迟诊断。这项研究旨在设计一种能够直接在低剂量CT扫描中进行结节检测和恶性诊断的AI系统，从而重新定义肺癌筛查。", "method": "我们设计了一个由浅层深度学习和基于特征的专用模型组成的模型组合，以解决数据集规模小和解释性的问题。", "result": "该系统在内部测试中达到了0.98的AUC，在独立队列中达到了0.945的AUC，显示出优于放射科医生、Lung-RADS和当前领先AI模型（包括Sybil、Brock、Google和Kaggle）的性能。", "conclusion": "该AI系统在所有结节大小和癌变阶段的诊断中均优于放射科医生，尤其擅长早期（阶段1）癌症的诊断。此外，该系统还提前一年诊断出不确定性和生长缓慢的结节。"}}
{"id": "2512.00789", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.00789", "abs": "https://arxiv.org/abs/2512.00789", "authors": ["Xiaodong Cai", "Hai Lin", "Shaoxiong Zhan", "Weiqi Luo", "Hong-Gee Kim", "Hongyan Hao", "Yu Yang", "Hai-Tao Zheng"], "title": "Auxiliary-Hyperparameter-Free Sampling: Entropy Equilibrium for Text Generation", "comment": null, "summary": "Token sampling strategies critically influence text generation quality in large language models (LLMs). However, existing methods introduce additional hyperparameters, requiring extensive tuning and complicating deployment. We present Entropy Equilibrium Sampling (EES), an auxiliary hyperparameter-free approach inspired by information theory that can dynamically adjust candidate sets by balancing normalized entropy with probability mass. We evaluate EES on both reasoning and generation tasks across a range of model architectures. Our results show that EES consistently performs well across temperature settings, delivering competitive accuracy and coherence while maintaining diversity. By eliminating the need for hyperparameter tuning, EES greatly simplifies deployment while improving performance. Code is available at https://github.com/shuanncai/EES", "AI": {"tldr": "本文提出了一种无需额外超参数调优的熵平衡采样（EES）方法，该方法提高文本生成的质量并简化了大语言模型的部署。", "motivation": "现有的抽样方法引入了额外的超参数，需要大量的调优并且增加了部署的复杂性。本研究旨在提出一个简化部署同时提高性能的方法。", "method": "介绍了一种名为熵平衡采样（EES）的方法，该方法基于信息论思想动态调整候选集，通过平衡归一化熵和概率质量来工作，无需额外的超参数调优。", "result": "在不同模型架构上的推理和生成任务中进行了评估，结果表明EES在不同的温度设置下表现良好，提供了竞争力的准确性和连贯性，同时保持多样性。", "conclusion": "EES方法通过消除超参数调优的需要，简化了部署过程并且提高了性能。"}}
{"id": "2512.00294", "categories": ["cs.CV", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2512.00294", "abs": "https://arxiv.org/abs/2512.00294", "authors": ["Lixing Guo", "Tobias Höllerer"], "title": "Words into World: A Task-Adaptive Agent for Language-Guided Spatial Retrieval in AR", "comment": null, "summary": "Traditional augmented reality (AR) systems predominantly rely on fixed class detectors or fiducial markers, limiting their ability to interpret complex, open-vocabulary natural language queries. We present a modular AR agent system that integrates multimodal large language models (MLLMs) with grounded vision models to enable relational reasoning in space and language-conditioned spatial retrieval in physical environments. Our adaptive task agent coordinates MLLMs and coordinate-aware perception tools to address varying query complexities, ranging from simple object identification to multi-object relational reasoning, while returning meter-accurate 3D anchors. It constructs dynamic AR scene graphs encoding nine typed relations (spatial, structural-semantic, causal-functional), enabling MLLMs to understand not just what objects exist, but how they relate and interact in 3D space. Through task-adaptive region-of-interest highlighting and contextual spatial retrieval, the system guides human attention to information-dense areas while supporting human-in-the-loop refinement. The agent dynamically invokes coordinate-aware tools for complex queries-selection, measurement, comparison, and actuation-grounding language understanding in physical operations. The modular architecture supports plug-and-use vision-language models without retraining, establishing AR agents as intermediaries that augment MLLMs with real-world spatial intelligence for interactive scene understanding. We also introduce GroundedAR-Bench, an evaluation framework for language-driven real world localization and relation grounding across diverse environments.", "AI": {"tldr": "一种新的模块化AR系统，结合多模态大语言模型（MLLMs）和基于视觉的模型，以理解和处理复杂的自然语言查询，同时提供精确的3D锚点和面向任务的环境理解能力。", "motivation": "该论文旨在克服传统AR系统依赖固定类检测器或参考标记所导致的局限性，并使AR系统能够理解和解释复杂的开放式词汇自然语言查询。", "method": "该论文提出了一种结合大型多模态语言模型（MLLM）和基础视觉模型的模块化增强现实（AR）系统。该系统能够通过空间推理和语言条件下的空间检索来处理复杂的自然语言查询。系统中的自适应任务代理协调MLLMs和基于坐标的感知工具，处理从简单物体识别到多个对象关系推理的各种查询，同时提供准确的3D锚点。", "result": "该系统能够构建一个动态的AR场景图，编码九种类型的关联（空间、结构-语义、因果-功能性），并引导人类注意力集中在信息丰富区域，在人机交互场景理解中增强MLLMs与真实世界的空间智能。另外，论文还介绍了一种针对语言驱动的现实世界定位和关系确定评估框架GroundedAR-Bench。", "conclusion": "该论文为增强现实系统提供了一个新的方法，通过结合视觉和语言模型，能够在物理环境中提供准确的空间推理和复杂的自然语言处理能力。"}}
{"id": "2512.00829", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.00829", "abs": "https://arxiv.org/abs/2512.00829", "authors": ["Md Mehrab Hossain Opi", "Sumaiya Khan", "Moshammad Farzana Rahman"], "title": "Accelerating Bangla NLP Tasks with Automatic Mixed Precision: Resource-Efficient Training Preserving Model Efficacy", "comment": null, "summary": "Training models for Natural Language Processing (NLP) requires substantial computational resources and time, posing significant challenges, especially for NLP development in Bangla, where access to high-end hardware is often limited. In this work, we explore automatic mixed precision (AMP) training as a means to improve computational efficiency without sacrificing model performance. By leveraging a dynamic mix of 16-bit and 32-bit floating-point computations, AMP lowers GPU memory requirements and speeds up training without degrading model performance. We evaluate AMP across four standard Bangla NLP tasks, namely sentiment analysis, named entity recognition, error classification, and question answering, using four transformer-based models: BanglaBERT, BanglishBERT, XLM-R, and mBERT. Our results demonstrate that AMP accelerates training by 44.5% and reduces memory consumption by 17.6%, while maintaining F-1 score within 99.7% of the full-precision baselines. This empirical study highlights AMP's potential to democratize access to state-of-the-art NLP capabilities in hardware-constrained settings by lowering computational barriers.", "AI": {"tldr": "本研究探讨了自动混合精度训练（AMP）在孟加拉语NLP任务中的应用，结果显示AMP提升了训练效率，降低了计算资源需求，同时保持了良好的模型性能。", "motivation": "Natural Language Processing（NLP）模型的训练需耗费大量的计算资源和时间，对孟加拉语NLP开发尤其具挑战性，因为该地区常常缺乏高性能硬件。本研究旨在探讨自动混合精度（AMP）训练作为解决计算效率问题的方案。", "method": "我们采用了自动混合精度（AMP）训练方法来提升计算效率，同时不损害模型性能。AMP在GPU内存要求和训练速度上提供了优势，并且通过在训练过程中动态使用16位和32位浮点数计算来降低成本。", "result": "实验表明，AMP训练加速了44.5%的训练，并降低了17.6%的内存消耗，同时保持了与全精度模型相比F-1分数99.7%的一致性。这些结果在四项标准的孟加拉语NLP任务中得到验证。", "conclusion": "这项实证研究展示了AMP在降低计算门槛的同时，具有提高孟加拉语NLP任务性能的潜力，特别是对于硬件受限的环境。"}}
{"id": "2512.00300", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.00300", "abs": "https://arxiv.org/abs/2512.00300", "authors": ["Rui Qian", "Haozhi Cao", "Tianchen Deng", "Tianxin Hu", "Weixiang Guo", "Shenghai Yuan", "Lihua Xie"], "title": "TGSFormer: Scalable Temporal Gaussian Splatting for Embodied Semantic Scene Completion", "comment": "14 pages, 10 figures", "summary": "Embodied 3D Semantic Scene Completion (SSC) infers dense geometry and semantics from continuous egocentric observations. Most existing Gaussian-based methods rely on random initialization of many primitives within predefined spatial bounds, resulting in redundancy and poor scalability to unbounded scenes. Recent depth-guided approach alleviates this issue but remains local, suffering from latency and memory overhead as scale increases. To overcome these challenges, we propose TGSFormer, a scalable Temporal Gaussian Splatting framework for embodied SSC. It maintains a persistent Gaussian memory for temporal prediction, without relying on image coherence or frame caches. For temporal fusion, a Dual Temporal Encoder jointly processes current and historical Gaussian features through confidence-aware cross-attention. Subsequently, a Confidence-aware Voxel Fusion module merges overlapping primitives into voxel-aligned representations, regulating density and maintaining compactness. Extensive experiments demonstrate that TGSFormer achieves state-of-the-art results on both local and embodied SSC benchmarks, offering superior accuracy and scalability with significantly fewer primitives while maintaining consistent long-term scene integrity. The code will be released upon acceptance.", "AI": {"tldr": "提出TGSFormer，一种有竞争力的3D SSC方法，通过优化高斯基本图形使用，实现更好的准确性和可扩展性。", "motivation": "现有的基于高斯的方法依赖于在预定义的空间边界内随机初始化的许多基本图形，这导致冗余并导致对无界场景的可扩展性差。尽管最近的深度引导方法缓解了这个问题，但它仍然是局部的，随着比例的增加，仍然存在延迟和内存开销。为了克服这些挑战，提出了TGSFormer。", "method": "我们提出了一种名为TGSFormer的可扩展的时空高斯散射框架，用于具身3D语义场景补全。该方法通过维护一个持久的高斯内存来进行时间预测，不依赖于图像连贯性或帧缓存。双重时态编码器通过置信度感知交叉注意力处理当前和历史高斯特征。接下来，置信度感知体素融合模块将重叠的基本图形合并到体素对齐的表示中，调整密度并保持紧凑性。", "result": "实验结果表明，TGSFormer在局部和具身3D语义场景完成基准测试中达到了最先进的性能，提供了更好的准确性和可扩展性，同时使用了显著更少的基本图形，并保持一致的长期场景完整性。", "conclusion": "TGSFormer通过引入持久的高斯内存和双重时态编码器，解决了现有方法无法处理无界场景、冗余和高内存开销的问题，实现了更高准确度和更好扩展性的3D语义场景完成。"}}
{"id": "2512.00837", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.00837", "abs": "https://arxiv.org/abs/2512.00837", "authors": ["Yukang Lin", "Jiahao Shao", "Shuoran Jiang", "Wentao Zhu", "Bingjie Lu", "Xiangping Wu", "Joanna Siebert", "Qingcai Chen"], "title": "WaterSearch: A Quality-Aware Search-based Watermarking Framework for Large Language Models", "comment": null, "summary": "Watermarking acts as a critical safeguard in text generated by Large Language Models (LLMs). By embedding identifiable signals into model outputs, watermarking enables reliable attribution and enhances the security of machine-generated content. Existing approaches typically embed signals by manipulating token generation probabilities. Despite their effectiveness, these methods inherently face a trade-off between detectability and text quality: the signal strength and randomness required for robust watermarking tend to degrade the performance of downstream tasks.\n  In this paper, we design a novel embedding scheme that controls seed pools to facilitate diverse parallel generation of watermarked text. Based on that scheme, we propose WaterSearch, a sentence-level, search-based watermarking framework adaptable to a wide range of existing methods. WaterSearch enhances text quality by jointly optimizing two key aspects: 1) distribution fidelity and 2) watermark signal characteristics. Furthermore, WaterSearch is complemented by a sentence-level detection method with strong attack robustness. We evaluate our method on three popular LLMs across ten diverse tasks. Extensive experiments demonstrate that our method achieves an average performance improvement of 51.01\\% over state-of-the-art baselines at a watermark detectability strength of 95\\%. In challenging scenarios such as short text generation and low-entropy output generation, our method yields performance gains of 47.78\\% and 36.47\\%, respectively. Moreover, under different attack senarios including insertion, synonym substitution and paraphrase attasks, WaterSearch maintains high detectability, further validating its robust anti-attack capabilities. Our code is available at \\href{https://github.com/Yukang-Lin/WaterSearch}{https://github.com/Yukang-Lin/WaterSearch}.", "AI": {"tldr": "本文介绍了一种名为WaterSearch的基于搜索的水印框架，该框架能够更优化地兼顾文本质量和水印信号强度，极大地提升了现有的水印检测技术的性能和鲁棒性。", "motivation": "传统的文本水印技术在保证检测性的同时往往牺牲了文本的质量。为了解决这一难题，提出了一种新的水印技术方案。", "method": "设计了一种新颖的种子池控制机制来支持并行生成的带水印的文本。基于此机制提出WaterSearch，这是一个适应各种现有方法的句子级别的搜索型文本水印框架。", "result": "在三个流行的大型语言模型和十种多样性任务上，实验表明该方法相比目前最佳的基准模型，水印的检测强度达到95%时能平均提升性能51.01%。在生成短文本（提升47.78%）和低熵输出（提升36.47%）等具有挑战性的场景中，依然可以取得显著的性能提升。", "conclusion": "WaterSearch不仅提升了文本质量，而且显示了其显著的性能提升和对不同攻击的强鲁棒性，为大型语言模型生成的文本增加了检测性和安全性。"}}
{"id": "2512.00308", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.00308", "abs": "https://arxiv.org/abs/2512.00308", "authors": ["Xiao Cui", "Yulei Qin", "Wengang Zhou", "Hongsheng Li", "Houqiang Li"], "title": "Optimizing Distributional Geometry Alignment with Optimal Transport for Generative Dataset Distillation", "comment": "NeurIPS 2025", "summary": "Dataset distillation seeks to synthesize a compact distilled dataset, enabling models trained on it to achieve performance comparable to models trained on the full dataset. Recent methods for large-scale datasets focus on matching global distributional statistics (e.g., mean and variance), but overlook critical instance-level characteristics and intraclass variations, leading to suboptimal generalization. We address this limitation by reformulating dataset distillation as an Optimal Transport (OT) distance minimization problem, enabling fine-grained alignment at both global and instance levels throughout the pipeline. OT offers a geometrically faithful framework for distribution matching. It effectively preserves local modes, intra-class patterns, and fine-grained variations that characterize the geometry of complex, high-dimensional distributions. Our method comprises three components tailored for preserving distributional geometry: (1) OT-guided diffusion sampling, which aligns latent distributions of real and distilled images; (2) label-image-aligned soft relabeling, which adapts label distributions based on the complexity of distilled image distributions; and (3) OT-based logit matching, which aligns the output of student models with soft-label distributions. Extensive experiments across diverse architectures and large-scale datasets demonstrate that our method consistently outperforms state-of-the-art approaches in an efficient manner, achieving at least 4% accuracy improvement under IPC=10 settings for each architecture on ImageNet-1K.", "AI": {"tldr": "A new dataset distillation method using Optimal Transport outperforms existing approaches, achieving substantial accuracy improvements on ImageNet-1K.", "motivation": "The motivation is to overcome the limitations of existing dataset distillation methods which only focus on global distributional statistics, neglecting instance-level characteristics and intraclass variations.", "method": "Our method uses Optimal Transport to reformulate dataset distillation as an OT distance minimization problem. It includes three components: OT-guided diffusion sampling, label-image-aligned soft relabeling, and OT-based logit matching to preserve distributional geometry.", "result": "Experiments show that our method outperforms state-of-the-art approaches, achieving at least a 4% accuracy improvement on ImageNet-1K for each architecture under IPC=10 settings.", "conclusion": "The conclusion is that using Optimal Transport for dataset distillation effectively captures both global and local distributional characteristics, leading to improved model performance."}}
{"id": "2512.00878", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.00878", "abs": "https://arxiv.org/abs/2512.00878", "authors": ["Chunlin Tian", "Xuyang Wei", "Huanrong Liu", "Zhijiang Guo", "Li Li"], "title": "Less is More: Resource-Efficient Low-Rank Adaptation", "comment": "18 pages, 7 figures", "summary": "Low-Rank Adaptation (LoRA) is a widely adopted parameter-efficient fine-tuning (PEFT) method for Large Language Models (LLMs), but it still incurs notable overhead and suffers from parameter interference in complex datasets. While re- cent works decouple LoRA update matrices to exploit matrix-wise asymmetry, training costs remain high. We revisit LoRA from the perspective of inter-matrix and intra-layer parameter redundancy and propose Resource-Efficient Low-Rank Adaptation, EffiLoRA, a lightweight and generalizable approach for language, multimodal, and diffusion models. EffiLoRA employs a unified A matrix across all transformer layers and introduces a runtime selective B matrices up- date to dynamically trade-off the system resource budget and model performance. EffiLoRA consistently outperforms LoRA across diverse modalities, including commonsense reasoning, visual instruction tuning, and image generation, demon- strating improved efficiency and robustness.", "AI": {"tldr": "本文提出了EffiLoRA，这是一种轻量级且通用的方法，用于语言、多模态和扩散模型，能够在解决参数干扰的同时提高效率和鲁棒性。", "motivation": "尽管LoRA作为一种参数高效的微调方法已被广泛采用，但它仍会带来显著的开销，并且在复杂数据集中存在参数干扰问题。尽管最近有研究将LoRA更新矩阵解耦以利用矩阵的不对称性，但训练成本仍然很高。", "method": "Content提到，EffiLoRA方法通过跨所有变压器层的统一A矩阵和在运行时选择性更新B矩阵来动态权衡系统资源预算和模型性能，以此来解决LoRA带来的额外开销和参数干扰问题。", "result": "EffiLoRA在各种模态下始终优于LoRA，包括常识推理，视觉指令微调和图像生成，展示了改进的效率和鲁棒性。", "conclusion": "EffiLoRA方法通过跨层统一A矩阵和动态选择更新B矩阵的策略展示出了优于LoRA的性能，尤其在常识推理、视觉指令微调和图像生成领域，证明了其在不同模态下的优越性和资源效率。"}}
{"id": "2512.00310", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.00310", "abs": "https://arxiv.org/abs/2512.00310", "authors": ["Qinyi Cao", "Jianan Fan", "Weidong Cai"], "title": "ART-ASyn: Anatomy-aware Realistic Texture-based Anomaly Synthesis Framework for Chest X-Rays", "comment": "Accepted in WACV2026", "summary": "Unsupervised anomaly detection aims to identify anomalies without pixel-level annotations. Synthetic anomaly-based methods exhibit a unique capacity to introduce controllable irregularities with known masks, enabling explicit supervision during training. However, existing methods often produce synthetic anomalies that are visually distinct from real pathological patterns and ignore anatomical structure. This paper presents a novel Anatomy-aware Realistic Texture-based Anomaly Synthesis framework (ART-ASyn) for chest X-rays that generates realistic and anatomically consistent lung opacity related anomalies using texture-based augmentation guided by our proposed Progressive Binary Thresholding Segmentation method (PBTSeg) for lung segmentation. The generated paired samples of synthetic anomalies and their corresponding precise pixel-level anomaly mask for each normal sample enable explicit segmentation supervision. In contrast to prior work limited to one-class classification, ART-ASyn is further evaluated for zero-shot anomaly segmentation, demonstrating generalizability on an unseen dataset without target-domain annotations. Code availability is available at https://github.com/angelacao-hub/ART-ASyn.", "AI": {"tldr": "本文提出一种名为ART-ASyn的新框架，可以生成与解剖结构一致的真实异常，用于胸部X光片的异常检测。该方法不仅进行单类分类，在零样本异常分割上也表现出良好的泛化能力。", "motivation": "无监督异常检测旨在没有像素级标注的情况下识别异常，而现有的合成异常方法生成的异常在视觉上与真实的病理性异常差异显著，并且忽视了解剖结构。", "method": "提出了一种名为ART-ASyn的新框架，用于胸部X光片的肺部异常检测。该框架基于纹理增强技术生成与解剖结构一致的真实异常，并利用一种称为PBTSeg的逐步二值阈值分割方法进行肺部分割。", "result": "ART-ASyn不仅进行了单类分类，还在零样本情况下评价了其在异常分割上的泛化能力，并在未见数据集上表现出良好的效果。", "conclusion": "本文提出的方法能够在没有目标领域标注的情况下识别出新的异常样本，具有较好的泛化能力。"}}
{"id": "2512.00920", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.00920", "abs": "https://arxiv.org/abs/2512.00920", "authors": ["Jianxiang Zang", "Yongda Wei", "Ruxue Bai", "Shiyu Jiang", "Nijia Mo", "Binhong Li", "Qiang Sun", "Hui Liu"], "title": "Reward Auditor: Inference on Reward Modeling Suitability in Real-World Perturbed Scenarios", "comment": null, "summary": "Reliable reward models (RMs) are critical for ensuring the safe alignment of large language models (LLMs). However, current evaluation methods focus solely on preference perception accuracies in given specific scenarios, obscuring the critical vulnerabilities of RMs in real-world scenarios. We identify the true challenge lies in assessing a novel dimension: Suitability, defined as conditional reliability under specific real-world perturbations. To this end, we introduce Reward Auditor, a hypothesis-testing framework specifically designed for RM suitability inference. Rather than answering \"How accurate is the RM's preference perception for given samples?\", it employs scientific auditing to answer: \"Can we infer RMs exhibit systematic vulnerabilities in specific real-world scenarios?\". Under real-world perturbed scenarios, Reward Auditor quantifies statistical significance and effect size by auditing distribution degradation of RM preference perception confidence. This enables inference of both the certainty and severity of RM vulnerabilities across diverse real-world scenarios. This lays a solid foundation for building next-generation LLM alignment systems that are verifiably safe, more robust, and trustworthy.", "AI": {"tldr": "提出Reward Auditor框架评估奖励模型在现实扰动下的条件可靠性，为构建更加安全、可靠的大型语言模型对齐系统打下基础。", "motivation": "当前的评估方法仅集中在特定场景下的偏好感知准确性上，忽视了奖励模型在真实世界场景中的关键漏洞。研究的真实挑战在于评估一个新的维度：适用性，即在特定现实世界扰动下的条件可靠性。", "method": "通过引入Reward Auditor这一假设检验框架来专门评估奖励模型的适用性，即在特定现实世界扰动下的条件可靠性。Reward Auditor不是回答给定样本中偏好感知的准确性，而是通过科学审计来回答奖励模型在特定现实场景中是否存在系统性漏洞。", "result": "通过在现实世界扰动场景中审计奖励模型偏好感知信心的分布退化，Reward Auditor量化解释了其统计显著性和影响范围，这使我们能够推断奖励模型在各类现实场景中的确定性和严重性漏洞。", "conclusion": "为建立验证安全、更稳健和可信赖的下一代大型语言模型对齐系统奠定了坚实基础。"}}
{"id": "2512.00327", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.00327", "abs": "https://arxiv.org/abs/2512.00327", "authors": ["Chenqi Zhu", "Levi Burner", "Yiannis Aloimonos"], "title": "Odometry Without Correspondence from Inertially Constrained Ruled Surfaces", "comment": "14 pages, 13 figures, 5 tables", "summary": "Visual odometry techniques typically rely on feature extraction from a sequence of images and subsequent computation of optical flow. This point-to-point correspondence between two consecutive frames can be costly to compute and suffers from varying accuracy, which affects the odometry estimate's quality. Attempts have been made to bypass the difficulties originating from the correspondence problem by adopting line features and fusing other sensors (event camera, IMU) to improve performance, many of which still heavily rely on correspondence. If the camera observes a straight line as it moves, the image of the line sweeps a smooth surface in image-space time. It is a ruled surface and analyzing its shape gives information about odometry. Further, its estimation requires only differentially computed updates from point-to-line associations. Inspired by event cameras' propensity for edge detection, this research presents a novel algorithm to reconstruct 3D scenes and visual odometry from these ruled surfaces. By constraining the surfaces with the inertia measurements from an onboard IMU sensor, the dimensionality of the solution space is greatly reduced.", "AI": {"tldr": "提出了一种新的基于事件相机和IMU的视觉里程计算法，利用直线特征形成的规则表面进行三维重建和视觉里程计估计，降低了计算复杂度和依赖点对应关系的限制。", "motivation": "针对传统视觉里程计依赖于点对应关系计算耗时且精度易受影响的问题，设计了一种新的方法来改善性能。", "method": "研究利用了直线特征和事件相机的优势，通过分析直线在图像时间空间中的特征（规则表面），结合IMU数据，减少了解决方案的空间维度，从而进行有效的三维重建和视觉里程计估计。", "result": "无具体结果列出，但方法减少了对点对应计算的依赖，并可能提高了性能。", "conclusion": "该方法提供了一种创新的途径来进行视觉里程计和3D重建，通过利用直线特征和IMU数据减少了计算量和依赖点对的关系，提高了系统的性能。"}}
{"id": "2512.00931", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.00931", "abs": "https://arxiv.org/abs/2512.00931", "authors": ["Imane Jaaouine", "Ross D. King"], "title": "Mitigating Hallucinations in Zero-Shot Scientific Summarisation: A Pilot Study", "comment": null, "summary": "Large language models (LLMs) produce context inconsistency hallucinations, which are LLM generated outputs that are misaligned with the user prompt. This research project investigates whether prompt engineering (PE) methods can mitigate context inconsistency hallucinations in zero-shot LLM summarisation of scientific texts, where zero-shot indicates that the LLM relies purely on its pre-training data. Across eight yeast biotechnology research paper abstracts, six instruction-tuned LLMs were prompted with seven methods: a base- line prompt, two levels of increasing instruction complexity (PE-1 and PE-2), two levels of context repetition (CR-K1 and CR-K2), and two levels of random addition (RA-K1 and RA-K2). Context repetition involved the identification and repetition of K key sentences from the abstract, whereas random addition involved the repetition of K randomly selected sentences from the abstract, where K is 1 or 2. A total of 336 LLM-generated summaries were evaluated using six metrics: ROUGE-1, ROUGE-2, ROUGE-L, BERTScore, METEOR, and cosine similarity, which were used to compute the lexical and semantic alignment be- tween the summaries and the abstracts. Four hypotheses on the effects of prompt methods on summary alignment with the reference text were tested. Statistical analysis on 3744 collected datapoints was performed using bias-corrected and accelerated (BCa) bootstrap confidence intervals and Wilcoxon signed-rank tests with Bonferroni-Holm correction. The results demonstrated that CR and RA significantly improve the lexical alignment of LLM-generated summaries with the abstracts. These findings indicate that prompt engineering has the potential to impact hallucinations in zero-shot scientific summarisation tasks.", "AI": {"tldr": "本研究通过八篇关于酵母生物技术的研究论文摘要，测试了六种指令调整过的大型语言模型（LLMs），使用了基本提示、两种复杂度递增的指令工程（PE-1和PE-2）、两层上下文重复（CR-K1和CR-K2）和两层随机增加（RA-K1和RA-K2）的方法。通过六项评估指标，研究发现上下文重复（CR）和随机增加（RA）显著改善了LLMs生成摘要与原文的词汇对齐，表明指令工程可能影响零样本科学总结中的幻觉问题。", "motivation": "为了调查指令工程（PE）能否减少大型语言模型（LLMs）在零样本摘要生成中的上下文不一致性幻觉。", "method": "总共选择了八篇酵母生物技术研究论文摘要，六种指令调整过的LLMs，七种不同的提示方法，并通过六种摘要评估指标（ROUGE-1、ROUGE-2、ROUGE-L、BERTScore、METEOR和余弦相似度）对LLMs生成的336个摘要进行评估。采用了BCa自助法置信区间和Wilcoxon符号秩检验（Bonferroni-Holm校正）进行统计分析。", "result": "研究结果发现，通过使用上下文重复（CR）和随机添加（RA），LLMs摘要的词汇与原文本的对齐度得到了显著提升。", "conclusion": "这些研究发现表明命令工程能够影响零样本科学摘要生成中的幻觉问题。"}}
{"id": "2512.00336", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.00336", "abs": "https://arxiv.org/abs/2512.00336", "authors": ["Mengxue Hu", "Yunfeng Diao", "Changtao Miao", "Jianshu Li", "Zhe Li", "Joey Tianyi Zhou"], "title": "MVAD : A Comprehensive Multimodal Video-Audio Dataset for AIGC Detection", "comment": "7 pages,2 figures", "summary": "The rapid advancement of AI-generated multimodal video-audio content has raised significant concerns regarding information security and content authenticity. Existing synthetic video datasets predominantly focus on the visual modality alone, while the few incorporating audio are largely confined to facial deepfakes--a limitation that fails to address the expanding landscape of general multimodal AI-generated content and substantially impedes the development of trustworthy detection systems. To bridge this critical gap, we introduce the Multimodal Video-Audio Dataset (MVAD), the first comprehensive dataset specifically designed for detecting AI-generated multimodal video-audio content. Our dataset exhibits three key characteristics: (1) genuine multimodality with samples generated according to three realistic video-audio forgery patterns; (2) high perceptual quality achieved through diverse state-of-the-art generative models; and (3) comprehensive diversity spanning realistic and anime visual styles, four content categories (humans, animals, objects, and scenes), and four video-audio multimodal data types. Our dataset will be available at https://github.com/HuMengXue0104/MVAD.", "AI": {"tldr": "The paper addresses the lack of multimodal datasets for AI-generated content by introducing MVAD, a comprehensive multimodal video-audio dataset with high perceptual quality and diverse content types to enhance the authenticity and security of content.", "motivation": "The motivation behind the paper is to develop a dataset that addresses the increasing concerns regarding the authenticity and security of AI-generated content. Existing datasets are limited in scope and the paper aims to provide a comprehensive resource to help in creating more robust detection systems for multimodal content.", "method": "The paper introduces the Multimodal Video-Audio Dataset (MVAD) which is designed to address the limitation of existing datasets that focus only on the visual or audio modality in synthetic content. MVAD includes three realistic video-audio forgery patterns, generated using state-of-the-art generative models, to ensure high perceptual quality. The dataset covers a diverse range of visual styles and content categories: realistic and anime, humans, animals, objects, and scenes, and includes four types of video-audio multimodal data.", "result": "The result of this paper is the introduction and explanation of the MVAD dataset that serves as a benchmark for future research and development of multimodal content detection systems. MVAD provides high-quality, diverse multimodal forgeries which were lacking in previous datasets.", "conclusion": "The paper concludes by emphasizing the importance of comprehensive multimodal datasets for the detection of AI-generated content. The MVAD dataset introduced in this paper represents a step towards improving the security and authenticity of video-audio content by overcoming the limitations of previous datasets."}}
{"id": "2512.00938", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.00938", "abs": "https://arxiv.org/abs/2512.00938", "authors": ["Ahmed Mustafa Younes"], "title": "DeformAr: Rethinking NER Evaluation through Component Analysis and Visual Analytics", "comment": "PhD Thesis, University of Sussex, 2025. 311 pages, 140 figures, 32 tables. Submitted as a PDF-only. First supervisor: Julie Weeds. Second supervisor: David Weir", "summary": "Transformer models have significantly advanced Natural Language Processing (NLP), demonstrating strong performance in English. However, their effectiveness in Arabic, particularly for Named Entity Recognition (NER), remains limited, even with larger pre-trained models. This performance gap stems from multiple factors, including tokenisation, dataset quality, and annotation inconsistencies. Existing studies often analyze these issues in isolation, failing to capture their joint effect on system behaviour and performance.\n  We introduce DeformAr (Debugging and Evaluation Framework for Transformer-based NER Systems), a novel framework designed to investigate and explain the performance discrepancy between Arabic and English NER systems. DeformAr integrates a data extraction library and an interactive dashboard, supporting two modes of evaluation: cross-component analysis and behavioural analysis. The framework divides each language into dataset and model components to examine their interactions.\n  The analysis proceeds in two stages. First, cross-component analysis provides systematic diagnostic measures across data and model subcomponents, addressing the \"what,\" \"how,\" and \"why\" behind observed discrepancies. The second stage applies behavioural analysis by combining interpretability techniques with token-level metrics, interactive visualisations, and representation space analysis. These stages enable a component-aware diagnostic process that detects model behaviours and explains them by linking them to underlying representational patterns and data factors. DeformAr is the first Arabic-specific, component-based interpretability tool, offering a crucial resource for advancing model analysis in under-resourced languages.", "AI": {"tldr": "该研究提出了DeformAr框架，用于分析阿拉伯语和英语在命名实体识别任务上的性能差异，是首个针对阿拉伯语的组件式解释工具，提供了重要的模型分析资源。", "motivation": "虽然Transformer模型在英语NLP任务中表现出色，但在阿拉伯语命名实体识别(NER)任务上效果有限，即便是在更大规模的预训练模型下也是如此。现有研究通常孤立地分析这些问题，而未能捕捉这些因素对系统行为和性能的共同影响。因此，本研究开发了一种新的分析框架DeformAr，旨在调查和解释阿拉伯语和英语NER系统之间的性能差异。", "method": "本研究开发了DeformAr框架，用于调查并解释阿拉伯语和英语命名实体识别系统性能上的差异。该框架包含数据抽取库和交互式仪表板，支持跨组件分析和行为分析两种评估模式。为了分解和分析每个语言的数据集和模型组件，框架将分析分为两个阶段，一是系统诊断，包括数据与模型子组件之间的交互作用；二是行为分析，结合解释技术、词级别度量、交互式可视化和表示空间分析，以检测模型行为并解释其背后的表征模式和数据因素。", "result": "DeformAr是一种特定于阿拉伯语的组件解释工具，可以系统地分析各种子组件间的行为和交互方式，并提供重要的资源，有助于推进在资源不足语言中模型分析的发展。", "conclusion": "DeformAr作为一种特定于阿拉伯语的组件解释工具，为在资源不足的语言中推进模型分析提供了重要的资源。"}}
{"id": "2512.00343", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.00343", "abs": "https://arxiv.org/abs/2512.00343", "authors": ["Zhongqi Wang", "Jie Zhang", "Shiguang Shan", "Xilin Chen"], "title": "Assimilation Matters: Model-level Backdoor Detection in Vision-Language Pretrained Models", "comment": null, "summary": "Vision-language pretrained models (VLPs) such as CLIP have achieved remarkable success, but are also highly vulnerable to backdoor attacks. Given a model fine-tuned by an untrusted third party, determining whether the model has been injected with a backdoor is a critical and challenging problem. Existing detection methods usually rely on prior knowledge of training dataset, backdoor triggers and targets, or downstream classifiers, which may be impractical for real-world applications. To address this, To address this challenge, we introduce Assimilation Matters in DETection (AMDET), a novel model-level detection framework that operates without any such prior knowledge. Specifically, we first reveal the feature assimilation property in backdoored text encoders: the representations of all tokens within a backdoor sample exhibit a high similarity. Further analysis attributes this effect to the concentration of attention weights on the trigger token. Leveraging this insight, AMDET scans a model by performing gradient-based inversion on token embeddings to recover implicit features that capable of activating backdoor behaviors. Furthermore, we identify the natural backdoor feature in the OpenAI's official CLIP model, which are not intentionally injected but still exhibit backdoor-like behaviors. We then filter them out from real injected backdoor by analyzing their loss landscapes. Extensive experiments on 3,600 backdoored and benign-finetuned models with two attack paradigms and three VLP model structures show that AMDET detects backdoors with an F1 score of 89.90%. Besides, it achieves one complete detection in approximately 5 minutes on a RTX 4090 GPU and exhibits strong robustness against adaptive attacks. Code is available at: https://github.com/Robin-WZQ/AMDET", "AI": {"tldr": "提出了AMDET，一种无需先验知识的模型级后门检测框架，通过识别后门样本中的特征同化特性来扫描并检测模型中的后门。", "motivation": "目前检测后门的方法需要依赖训练数据集、后门触发器和目标或下游分类器等先验知识，这些在实际应用中可能不可行。我们的方法旨在提高无先验知识条件下的后门检测能力。", "method": "我们提出了AMDET，一种无需任何先验知识的模型级检测框架。AMDET 首先揭示了后门文本编码器中的特征同化特性：后门样本内所有标记的表示呈现出高相似度。进一步分析表明这种效应归因于注意力权重集中在触发标记上。AMDET 通过基于梯度的逆向操作来扫描模型，恢复能够激活后门行为的隐含特征。此外，我们还识别并过滤了OpenAI官方CLIP模型中的自然后门特征。", "result": "实验显示，AMDET在各种攻击模式和VLP模型结构的3,600个被植入后门和良性微调的模型上，检测后门的F1得分为89.90%。此外，它在RTX 4090 GPU上大约5分钟内完成一次完整检测，并且对自适应攻击具较强的鲁棒性。", "conclusion": "此研究提出了一种创新的方法（AMDET）来检测视觉语言预训练模型中的后门，从而填补了现有方法中缺乏先验知识条件下的检测解决方案，同时其高效性和鲁棒性也得到了验证。"}}
{"id": "2512.00946", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.00946", "abs": "https://arxiv.org/abs/2512.00946", "authors": ["Alvaro Paredes Amorin", "Andre Python", "Christoph Weisser"], "title": "Fine-tuning of lightweight large language models for sentiment classification on heterogeneous financial textual data", "comment": null, "summary": "Large language models (LLMs) play an increasingly important role in finan- cial markets analysis by capturing signals from complex and heterogeneous textual data sources, such as tweets, news articles, reports, and microblogs. However, their performance is dependent on large computational resources and proprietary datasets, which are costly, restricted, and therefore inacces- sible to many researchers and practitioners. To reflect realistic situations we investigate the ability of lightweight open-source LLMs - smaller and publicly available models designed to operate with limited computational resources - to generalize sentiment understanding from financial datasets of varying sizes, sources, formats, and languages. We compare the benchmark finance natural language processing (NLP) model, FinBERT, and three open-source lightweight LLMs, DeepSeek-LLM 7B, Llama3 8B Instruct, and Qwen3 8B on five publicly available datasets: FinancialPhraseBank, Financial Question Answering, Gold News Sentiment, Twitter Sentiment and Chinese Finance Sentiment. We find that LLMs, specially Qwen3 8B and Llama3 8B, perform best in most scenarios, even from using only 5% of the available training data. These results hold in zero-shot and few-shot learning scenarios. Our findings indicate that lightweight, open-source large language models (LLMs) consti- tute a cost-effective option, as they can achieve competitive performance on heterogeneous textual data even when trained on only a limited subset of the extensive annotated corpora that are typically deemed necessary.", "AI": {"tldr": "轻量级开源LLM（尤其是Qwen3 8B和Llama3 8B）在处理异构金融文本数据方面表现出色，甚至在仅使用5%训练数据的情况下也能保持良好的性能。它们是成本效益高的选择。", "motivation": "由于LLMs的运行依赖昂贵且受限的大规模计算资源和专有数据集，许多研究人员和实践者无法使用。因此，作者探索了轻量级开源LLM在理解和处理金融领域的异构文本数据方面的能力。", "method": "研究了轻量级开源LLM（DeepSeek-LLM 7B，Llama3 8B Instruct，Qwen3 8B）和FinBERT在五种公开数据集（FinancialPhraseBank，Financial Question Answering，Gold News Sentiment，Twitter Sentiment和Chinese Finance Sentiment）上的表现。", "result": "发现轻量级开源LLM（特别是Qwen3 8B和Llama3 8B）在多数场景下表现最好，甚至在零样本和少量样本学习中也能保持优秀表现。", "conclusion": "轻量级开源LLM可以在有限子集的大型注释语料库训练下得到较好的性能表现，成为处理异构金融文本数据的一种具有成本效益的选择。"}}
{"id": "2512.00345", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.00345", "abs": "https://arxiv.org/abs/2512.00345", "authors": ["Junqiao Fan", "Haocong Rao", "Jiarui Zhang", "Jianfei Yang", "Lihua Xie"], "title": "mmPred: Radar-based Human Motion Prediction in the Dark", "comment": "This paper is accepted by AAAI-2026", "summary": "Existing Human Motion Prediction (HMP) methods based on RGB-D cameras are sensitive to lighting conditions and raise privacy concerns, limiting their real-world applications such as firefighting and healthcare. Motivated by the robustness and privacy-preserving nature of millimeter-wave (mmWave) radar, this work introduces radar as a novel sensing modality for HMP, for the first time. Nevertheless, radar signals often suffer from specular reflections and multipath effects, resulting in noisy and temporally inconsistent measurements, such as body-part miss-detection. To address these radar-specific artifacts, we propose mmPred, the first diffusion-based framework tailored for radar-based HMP. mmPred introduces a dual-domain historical motion representation to guide the generation process, combining a Time-domain Pose Refinement (TPR) branch for learning fine-grained details and a Frequency-domain Dominant Motion (FDM) branch for capturing global motion trends and suppressing frame-level inconsistency. Furthermore, we design a Global Skeleton-relational Transformer (GST) as the diffusion backbone to model global inter-joint cooperation, enabling corrupted joints to dynamically aggregate information from others. Extensive experiments show that mmPred achieves state-of-the-art performance, outperforming existing methods by 8.6% on mmBody and 22% on mm-Fi.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2512.00947", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.00947", "abs": "https://arxiv.org/abs/2512.00947", "authors": ["Liyao Li", "Chao Ye", "Wentao Ye", "Yifei Sun", "Zhe Jiang", "Haobo Wang", "Jiaming Tian", "Yiming Zhang", "Ningtao Wang", "Xing Fu", "Gang Chen", "Junbo Zhao"], "title": "Table as a Modality for Large Language Models", "comment": "Accepted to NeurIPS 2025", "summary": "To migrate the remarkable successes of Large Language Models (LLMs), the community has made numerous efforts to generalize them to the table reasoning tasks for the widely deployed tabular data. Despite that, in this work, by showing a probing experiment on our proposed StructQA benchmark, we postulate that even the most advanced LLMs (such as GPTs) may still fall short of coping with tabular data. More specifically, the current scheme often simply relies on serializing the tabular data, together with the meta information, then inputting them through the LLMs. We argue that the loss of structural information is the root of this shortcoming. In this work, we further propose TAMO, which bears an ideology to treat the tables as an independent modality integrated with the text tokens. The resulting model in TAMO is a multimodal framework consisting of a hypergraph neural network as the global table encoder seamlessly integrated with the mainstream LLM. Empirical results on various benchmarking datasets, including HiTab, WikiTQ, WikiSQL, FeTaQA, and StructQA, have demonstrated significant improvements on generalization with an average relative gain of 42.65%.", "AI": {"tldr": "研究揭示了大型语言模型在处理表格推理任务中的局限性，并通过TAMO方法，利用超图神经网络和多模态框架，显著提高了处理表格数据的能力。", "motivation": "探究最先进的大语言模型（如GPTs）在处理表格数据方面的不足，并提出改进方案。研究发现这些模型在处理表格数据时往往会忽略结构信息。", "method": "提出了一种名为TAMO的方法，该方法将表格视为独立的模态并与文本标记集成。TAMO模型是一个多模态框架，包含一个超图神经网络作为全局表格编码器，无缝集成主流的大语言模型。", "result": "在包括HiTab、WikiTQ、WikiSQL、FeTaQA和StructQA在内的各种基准数据集上进行了实验，结果显示平均相对提升了42.65%。", "conclusion": "提出的方法TAMO能够更好地处理表格数据，显著提高了模型在表格推理任务上的泛化能力。"}}
{"id": "2512.00355", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.00355", "abs": "https://arxiv.org/abs/2512.00355", "authors": ["Junqiao Fan", "Pengfei Liu", "Haocong Rao"], "title": "SMamDiff: Spatial Mamba for Stochastic Human Motion Prediction", "comment": null, "summary": "With intelligent room-side sensing and service robots widely deployed, human motion prediction (HMP) is essential for safe, proactive assistance. However, many existing HMP methods either produce a single, deterministic forecast that ignores uncertainty or rely on probabilistic models that sacrifice kinematic plausibility. Diffusion models improve the accuracy-diversity trade-off but often depend on multi-stage pipelines that are costly for edge deployment. This work focuses on how to ensure spatial-temporal coherence within a single-stage diffusion model for HMP. We introduce SMamDiff, a Spatial Mamba-based Diffusion model with two novel designs: (i) a residual-DCT motion encoding that subtracts the last observed pose before a temporal DCT, reducing the first DC component ($f=0$) dominance and highlighting informative higher-frequency cues so the model learns how joints move rather than where they are; and (ii) a stickman-drawing spatial-mamba module that processes joints in an ordered, joint-by-joint manner, making later joints condition on earlier ones to induce long-range, cross-joint dependencies. On Human3.6M and HumanEva, these coherence mechanisms deliver state-of-the-art results among single-stage probabilistic HMP methods while using less latency and memory than multi-stage diffusion baselines.", "AI": {"tldr": "The paper presents SMamDiff, a single-stage diffusion model with novel motion encoding and joint processing mechanisms for effective human motion prediction, achieving state-of-the-art results with lower latency and memory use than multi-stage models.", "motivation": "The authors aim to address the limitations of existing human motion prediction methods that either ignore uncertainty or sacrifice kinematic plausibility, and overcome the inefficiencies of multi-stage diffusion models for edge deployment.", "method": "This paper introduces SMamDiff, which introduces two novel designs: a residual-DCT motion encoding that highlights higher-frequency cues and a stickman-drawing spatial-mamba module that induces long-range, cross-joint dependencies.", "result": "On the Human3.6M and HumanEva datasets, SMamDiff achieves state-of-the-art results among single-stage probabilistic human motion prediction methods with less latency and memory use compared to multi-stage diffusion baselines.", "conclusion": "The proposed SMamDiff model successfully ensures spatial-temporal coherence in human motion prediction, outperforming existing methods by providing a balance between accuracy and diversity while maintaining kinematic plausibility and computational efficiency."}}
{"id": "2512.00986", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.00986", "abs": "https://arxiv.org/abs/2512.00986", "authors": ["Zhihan Guo", "Feiyang Xu", "Yifan Li", "Muzhi Li", "Shuai Zou", "Jiele Wu", "Han Shi", "Haoli Bai", "Ho-fung Leung", "Irwin King"], "title": "Dr.Mi-Bench: A Modular-integrated Benchmark for Scientific Deep Research Agent", "comment": null, "summary": "The explosive growth in academic literature necessitates automated deep research (DR) agents, yet their evaluation remains a significant challenge. First, existing benchmarks often focus narrowly on retrieval while neglecting high-level planning and reasoning. Second, existing benchmarks favor general domains over the scientific domains that are the core application for DR agents. To address these gaps, we introduce Dr.Mi-Bench, a Modular-integrated benchmark for scientific DR agents. Grounded in academic literature, our benchmark uses a human-annotated dataset of 200 instances across 10 scientific domains, including both research and review papers. Besides, we also propose a Modular-integrated Evaluation Paradigm for DR Agents (Dr.Mi-Eval), a novel modular-integrated evaluation paradigm, which leverages the rich structure of academic papers to assess the core competencies of planning, retrieval, and reasoning through two complementary modes: an end-to-end evaluation for DR agents and an isolated evaluation for foundational LLMs as potential backbones. Experimental results reveal a fragmented performance landscape: agents exhibit specialized strengths but share critical weaknesses, most notably in performing the multi-source retrieval required for review-style tasks and performing consistently across diverse scientific fields. Moreover, improving high-level planning capability is the crucial factor for unlocking the reasoning potential of foundational LLMs as backbones. By exposing these actionable failure modes, Dr.Mi-Bench provides a diagnostic tool to guide the development of more reliable academic research assistants.", "AI": {"tldr": "研究提出了Dr.Mi-Bench和Dr.Mi-Eval，用于评估科学DR代理，发现代理存在多源检索和跨领域一致性的弱点。", "motivation": "现有的基准侧重于检索，忽略了高层规划和推理，且更倾向于通用领域而非科学领域。针对这些不足，研究提出了一种新的科学深度研究代理的评估基准。", "method": "提出Dr.Mi-Bench作为科学深度研究（DR）代理的模块化综合基准，并引入Dr.Mi-Eval，一种新型的模块化综合评估范式，结合学术论文的丰富结构来评估规划、检索和推理的核心能力。", "result": "实验结果显示，代理展现出专门的优势，但在多源检索和跨多个科学领域的一致性方面存在关键弱点。提高高级规划能力是解锁基础大语言模型推理潜力的关键因素。", "conclusion": "Dr.Mi-Bench作为诊断工具，揭示了开发更可靠学术研究助手的可行动失败模式。"}}
{"id": "2512.00363", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.00363", "abs": "https://arxiv.org/abs/2512.00363", "authors": ["Jianhong Han", "Yupei Wang", "Yuan Zhang", "Liang Chen"], "title": "MM-DETR: An Efficient Multimodal Detection Transformer with Mamba-Driven Dual-Granularity Fusion and Frequency-Aware Modality Adapters", "comment": "Manuscript submitted to IEEE Transactions on Geoscience and Remote Sensing", "summary": "Multimodal remote sensing object detection aims to achieve more accurate and robust perception under challenging conditions by fusing complementary information from different modalities. However, existing approaches that rely on attention-based or deformable convolution fusion blocks still struggle to balance performance and lightweight design. Beyond fusion complexity, extracting modality features with shared backbones yields suboptimal representations due to insufficient modality-specific modeling, whereas dual-stream architectures nearly double the parameter count, ultimately limiting practical deployment. To this end, we propose MM-DETR, a lightweight and efficient framework for multimodal object detection. Specifically, we propose a Mamba-based dual granularity fusion encoder that reformulates global interaction as channel-wise dynamic gating and leverages a 1D selective scan for efficient cross-modal modeling with linear complexity. Following this design, we further reinterpret multimodal fusion as a modality completion problem. A region-aware 2D selective scanning completion branch is introduced to recover modality-specific cues, supporting fine-grained fusion along a bidirectional pyramid pathway with minimal overhead. To further reduce parameter redundancy while retaining strong feature extraction capability, a lightweight frequency-aware modality adapter is inserted into the shared backbone. This adapter employs a spatial-frequency co-expert structure to capture modality-specific cues, while a pixel-wise router dynamically balances expert contributions for efficient spatial-frequency fusion. Extensive experiments conducted on four multimodal benchmark datasets demonstrate the effectiveness and generalization capability of the proposed method.", "AI": {"tldr": "MM-DETR, a lightweight multimodal object detection framework, uses a dual granularity fusion encoder and a frequency-aware modality adapter to improve perception accuracy and robustness in challenging conditions.", "motivation": "The motivation is to achieve more accurate and robust perception in multimodal remote sensing object detection, particularly in challenging conditions, by resolving the issues of existing approaches with complex fusion mechanisms and suboptimal modality-specific feature extraction.", "method": "The method involves proposing MM-DETR, a lightweight and efficient multimodal object detection framework. It introduces a Mamba-based dual granularity fusion encoder that uses channel-wise dynamic gating and 1D selective scanning for cross-modal modeling. A region-aware 2D selective scanning completion branch and a lightweight frequency-aware modality adapter are also introduced to recover modality-specific cues and reduce parameter redundancy.", "result": "Extensive experiments on four multimodal benchmark datasets show that the proposed method effectively reduces the model's parameter count without compromising performance, demonstrating its effectiveness and generalization capability.", "conclusion": "The conclusion highlights the success of MM-DETR in addressing the primary challenges of existing multimodal object detection approaches, providing a lightweight yet effective model for practical deployment."}}
{"id": "2512.00991", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.00991", "abs": "https://arxiv.org/abs/2512.00991", "authors": ["Nicole Favero", "Francesca Salute", "Daniel Hardt"], "title": "Advancing Academic Chatbots: Evaluation of Non Traditional Outputs", "comment": null, "summary": "Most evaluations of large language models focus on standard tasks such as factual question answering or short summarization. This research expands that scope in two directions: first, by comparing two retrieval strategies, Graph RAG, structured knowledge-graph based, and Advanced RAG, hybrid keyword-semantic search, for QA; and second, by evaluating whether LLMs can generate high quality non-traditional academic outputs, specifically slide decks and podcast scripts. We implemented a prototype combining Meta's LLaMA 3 70B open weight and OpenAI's GPT 4o mini API based. QA performance was evaluated using both human ratings across eleven quality dimensions and large language model judges for scalable cross validation. GPT 4o mini with Advanced RAG produced the most accurate responses. Graph RAG offered limited improvements and led to more hallucinations, partly due to its structural complexity and manual setup. Slide and podcast generation was tested with document grounded retrieval. GPT 4o mini again performed best, though LLaMA 3 showed promise in narrative coherence. Human reviewers were crucial for detecting layout and stylistic flaws, highlighting the need for combined human LLM evaluation in assessing emerging academic outputs.", "AI": {"tldr": "这项研究通过对比两种检索策略评估了大语言模型在问答以及生成非传统学术内容（如幻灯片和播客脚本）上的能力，结果表明GPT 4o mini结合Advanced RAG策略表现最佳。", "motivation": "研究动机在于扩大了对大型语言模型评估的范围，不仅限于传统的事实性问答或简短摘要等标准任务，同时探索模型在生成非传统学术输出上的能力。", "method": "研究通过对比Graph RAG（基于结构化知识图谱的检索策略）和Advanced RAG（混合关键词-语义检索策略）两种方法来评估大语言模型在问答任务中的表现，并进一步测试了模型生成非传统学术输出（如幻灯片和播客脚本）的能力。研究使用了Meta的LLaMA 3 70B权重和OpenAI的GPT 4o mini API原型。", "result": "GPT 4o mini结合Advanced RAG策略提供了最准确的回答。Graph RAG由于其结构复杂性和手动设置，提供了有限的改进并导致更多的幻觉现象。在幻灯片和播客生成上，GPT 4o mini表现最佳，而LLaMA 3在叙事连贯性上展现出潜力。", "conclusion": "大语言模型能够生成高质量的非传统学术输出，但还需要结合人类评估来检测布局和风格方面的缺陷，强调了人机结合在评估新兴学术输出中的重要性。"}}
{"id": "2512.00365", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.00365", "abs": "https://arxiv.org/abs/2512.00365", "authors": ["Andrey Gizdov", "Andrea Procopio", "Yichen Li", "Daniel Harari", "Tomer Ullman"], "title": "Towards aligned body representations in vision models", "comment": "Andrea Procopio and Andrey Gizdov have equal contributions", "summary": "Human physical reasoning relies on internal \"body\" representations - coarse, volumetric approximations that capture an object's extent and support intuitive predictions about motion and physics. While psychophysical evidence suggests humans use such coarse representations, their internal structure remains largely unknown. Here we test whether vision models trained for segmentation develop comparable representations. We adapt a psychophysical experiment conducted with 50 human participants to a semantic segmentation task and test a family of seven segmentation networks, varying in size. We find that smaller models naturally form human-like coarse body representations, whereas larger models tend toward overly detailed, fine-grain encodings. Our results demonstrate that coarse representations can emerge under limited computational resources, and that machine representations can provide a scalable path toward understanding the structure of physical reasoning in the brain.", "AI": {"tldr": "研究发现，小规模的模型能自然地形成类似人类的粗略身体表示，而大规模模型则倾向于过度详细的表示。这表明，在计算资源有限的情况下，粗糙表示是可以产生的。", "motivation": "人类在进行物理推理时依赖于内部“身体”表示，这是一种粗略的、体素化的近似表示，能够捕捉对象的范围并支持对运动和物理的直观预测。尽管有心理物理学证据表明人类使用这种粗糙表示，但我们对其内部结构知之甚少。这项研究旨在探索视觉模型是否也会发展出类似的表示。", "method": "通过改编一项对50名人类参与者进行的心理物理学实验，将其应用于语义分割任务中，并测试了七个不同大小的分割网络，以此来检验视觉模型是否像人类一样形成粗略的身体表示。", "result": "研究结果显示，小模型能够自然形成与人类相似的粗略身体表示，但大模型发展出的则是过于详细的表示。", "conclusion": "研究结果表明，粗略的表示可以在计算资源有限的情况下出现，并且机器的表示可以提供一种可扩展的方式来了解脑中的物理推理结构。"}}
{"id": "2512.01037", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.01037", "abs": "https://arxiv.org/abs/2512.01037", "authors": ["Riad Ahmed Anonto", "Md Labid Al Nahiyan", "Md Tanvir Hassan", "Ch. Md. Rakin Haider"], "title": "When Safety Blocks Sense: Measuring Semantic Confusion in LLM Refusals", "comment": null, "summary": "Safety-aligned language models often refuse prompts that are actually harmless. Current evaluations mostly report global rates such as false rejection or compliance. These scores treat each prompt alone and miss local inconsistency, where a model accepts one phrasing of an intent but rejects a close paraphrase. This gap limits diagnosis and tuning. We introduce \"semantic confusion,\" a failure mode that captures such local inconsistency, and a framework to measure it. We build ParaGuard, a 10k-prompt corpus of controlled paraphrase clusters that hold intent fixed while varying surface form. We then propose three model-agnostic metrics at the token level: Confusion Index, Confusion Rate, and Confusion Depth. These metrics compare each refusal to its nearest accepted neighbors and use token embeddings, next-token probabilities, and perplexity signals. Experiments across diverse model families and deployment guards show that global false-rejection rate hides critical structure. Our metrics reveal globally unstable boundaries in some settings, localized pockets of inconsistency in others, and cases where stricter refusal does not increase inconsistency. We also show how confusion-aware auditing separates how often a system refuses from how sensibly it refuses. This gives developers a practical signal to reduce false refusals while preserving safety.", "AI": {"tldr": "本文引入了“语义困惑”作为度量局部一致性的概念，并提出了ParaGuard语料库及困惑度量标准，以帮助调试语言模型，减少错误拒绝率。", "motivation": "安全对齐的语言模型常常拒绝实际上无害的提示。本论文试图诊断和调优模型，引入了语义困惑概念，以捕捉局部不一致性。", "method": "提出了\"语义困惑\"这一失效模式概念以及测量它的框架。构建了ParaGuard，一个包含10k提示的受控释义簇语料库。提出了三种无模型特异性的令牌级别度量标准：困惑指数、困惑率和困惑深度。", "result": "实验表明，全局虚假拒绝率掩盖了关键结构，度量标准揭示了一些设置中的全局不稳定边界，局部一致性的凹槽区，以及严格的拒绝并不总会增加不一致性。", "conclusion": "该度量标准使开发者实际减少了错误拒绝，同时保持了模型的安全性。这提供了一种实用信号，帮助减少错误拒绝，保持模型的安全性。"}}
{"id": "2512.00368", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.00368", "abs": "https://arxiv.org/abs/2512.00368", "authors": ["Jian Zhu"], "title": "THCRL: Trusted Hierarchical Contrastive Representation Learning for Multi-View Clustering", "comment": null, "summary": "Multi-View Clustering (MVC) has garnered increasing attention in recent years. It is capable of partitioning data samples into distinct groups by learning a consensus representation. However, a significant challenge remains: the problem of untrustworthy fusion. This problem primarily arises from two key factors: 1) Existing methods often ignore the presence of inherent noise within individual views; 2) In traditional MVC methods using Contrastive Learning (CL), similarity computations typically rely on different views of the same instance, while neglecting the structural information from nearest neighbors within the same cluster. Consequently, this leads to the wrong direction for multi-view fusion. To address this problem, we present a novel Trusted Hierarchical Contrastive Representation Learning (THCRL). It consists of two key modules. Specifically, we propose the Deep Symmetry Hierarchical Fusion (DSHF) module, which leverages the UNet architecture integrated with multiple denoising mechanisms to achieve trustworthy fusion of multi-view data. Furthermore, we present the Average K-Nearest Neighbors Contrastive Learning (AKCL) module to align the fused representation with the view-specific representation. Unlike conventional strategies, AKCL enhances representation similarity among samples belonging to the same cluster, rather than merely focusing on the same sample across views, thereby reinforcing the confidence of the fused representation. Extensive experiments demonstrate that THCRL achieves the state-of-the-art performance in deep MVC tasks.", "AI": {"tldr": "本文提出了THCRL方法，解决多视图聚类中不信任融合问题，借助DSHF和AKCL模块实现多视图数据的可信融合，实验验证了其在深度多视图聚类中的先进性能。", "motivation": "针对现有多视图聚类方法中存在的不信任融合问题，提出了一种新的可信层级对比表示学习方法，以解决传统方法忽视单个视图内在噪声和同一聚类内近邻的结构信息问题。", "method": "THCRL, 包含两个主要模块：1) DSHF模块，利用集成多种去噪机制的UNet架构实现多视图数据的可信融合；2) AKCL模块，通过增强同一聚类中的样本之间的表示相似性而非仅仅关注跨视图的同一样本，与视图特定的表示对齐，提高融合表示的可信度。", "result": "实验结果显示，THCRL在深度多视图聚类任务中达到了最先进的性能。", "conclusion": "THCRL通过可信层级对比表示学习提高了多视图聚类的质量，解决了现有方法中的不信任融合问题，显示出在聚类任务上的优越性能。"}}
{"id": "2512.01077", "categories": ["cs.CL", "cs.HC"], "pdf": "https://arxiv.org/pdf/2512.01077", "abs": "https://arxiv.org/abs/2512.01077", "authors": ["Neha Joshi", "Pamir Gogoi", "Aasim Mirza", "Aayush Jansari", "Aditya Yadavalli", "Ayushi Pandey", "Arunima Shukla", "Deepthi Sudharsan", "Kalika Bali", "Vivek Seshadri"], "title": "ELR-1000: A Community-Generated Dataset for Endangered Indic Indigenous Languages", "comment": "Accepted at AACL 2025 (Main)", "summary": "We present a culturally-grounded multimodal dataset of 1,060 traditional recipes crowdsourced from rural communities across remote regions of Eastern India, spanning 10 endangered languages. These recipes, rich in linguistic and cultural nuance, were collected using a mobile interface designed for contributors with low digital literacy. Endangered Language Recipes (ELR)-1000 -- captures not only culinary practices but also the socio-cultural context embedded in indigenous food traditions. We evaluate the performance of several state-of-the-art large language models (LLMs) on translating these recipes into English and find the following: despite the models' capabilities, they struggle with low-resource, culturally-specific language. However, we observe that providing targeted context -- including background information about the languages, translation examples, and guidelines for cultural preservation -- leads to significant improvements in translation quality. Our results underscore the need for benchmarks that cater to underrepresented languages and domains to advance equitable and culturally-aware language technologies. As part of this work, we release the ELR-1000 dataset to the NLP community, hoping it motivates the development of language technologies for endangered languages.", "AI": {"tldr": "本研究建立了一个多模态、以文化为基础的数据集（ELR-1000），包含了来自印度偏远农村地区的1,060份传统菜谱，用以评估大型语言模型在翻译濒危语言时的表现。结果显示，虽然这些模型无法有效翻译这类文本材料，但利用文化和语言背景信息能显著提升翻译效果。", "motivation": "研究的动机在于通过收集濒危语言中的食谱，来捕捉其中的文化和社会背景信息，并推动针对资源匮乏和文化特定语言的语言技术发展。", "method": "我们使用了一款专为低数字素养贡献者设计的移动界面，收集了来自印度东部偏远地区的农村社区的1,060种传统食谱，涵盖了10种濒危语言。", "result": "尽管最先进的大型语言模型在翻译这些食谱时遇到了困难，但通过提供针对性的文化背景信息和翻译示例，翻译质量得到了显著提升。", "conclusion": "研究结果强调了开发能够关注较少被代表的语言和领域基准的重要性，以推进更加公平和平等的文化意识语言技术的发展。"}}
{"id": "2512.00369", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.00369", "abs": "https://arxiv.org/abs/2512.00369", "authors": ["Wenshuo Chen", "Haosen Li", "Shaofeng Liang", "Lei Wang", "Haozhe Jia", "Kaishen Yuan", "Jieming Wu", "Bowen Tian", "Yutao Yue"], "title": "POLARIS: Projection-Orthogonal Least Squares for Robust and Adaptive Inversion in Diffusion Models", "comment": null, "summary": "The Inversion-Denoising Paradigm, which is based on diffusion models, excels in diverse image editing and restoration tasks. We revisit its mechanism and reveal a critical, overlooked factor in reconstruction degradation: the approximate noise error. This error stems from approximating the noise at step t with the prediction at step t-1, resulting in severe error accumulation throughout the inversion process. We introduce Projection-Orthogonal Least Squares for Robust and Adaptive Inversion (POLARIS), which reformulates inversion from an error-compensation problem into an error-origin problem. Rather than optimizing embeddings or latent codes to offset accumulated drift, POLARIS treats the guidance scale ω as a step-wise variable and derives a mathematically grounded formula to minimize inversion error at each step. Remarkably, POLARIS improves inversion latent quality with just one line of code. With negligible performance overhead, it substantially mitigates noise approximation errors and consistently improves the accuracy of downstream tasks.", "AI": {"tldr": "The paper introduces POLARIS, a method that improves the accuracy of inversion processes in image reconstruction by addressing noise approximation errors, which significantly enhances the quality of reconstructed images.", "motivation": "The motivation behind this paper is to address reconstruction degradation in the Inversion-Denoising Paradigm, specifically focusing on the issue of approximate noise error that accumulates during the inversion process.", "method": "POLARIS is introduced as a solution, which involves reformulating the inversion process to treat the guidance scale ω as a step-wise variable and deriving a formula to minimize inversion error at each step, thereby compensating for noise approximation errors.", "result": "The method provides substantial improvements in mitigating noise approximation errors and enhances the accuracy of downstream tasks, all with minimal performance overhead and a straightforward implementation requiring only one line of code.", "conclusion": "The conclusion is that POLARIS provides a robust and adaptive approach to improve the inversion process by addressing the critical issue of noise approximation errors, leading to better quality image reconstruction and restoration."}}
{"id": "2512.01109", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.01109", "abs": "https://arxiv.org/abs/2512.01109", "authors": ["Yaxuan Ren", "Krithika Ramesh", "Yaxing Yao", "Anjalie Field"], "title": "How do we measure privacy in text? A survey of text anonymization metrics", "comment": "13 pages, 1 figure, 1 table. To be published in Findings of the Association for Computational Linguistics (AACL-IJCNLP 2025). Related resources at: https://github.com/ryxGuo/privacy-metrics-survey", "summary": "In this work, we aim to clarify and reconcile metrics for evaluating privacy protection in text through a systematic survey. Although text anonymization is essential for enabling NLP research and model development in domains with sensitive data, evaluating whether anonymization methods sufficiently protect privacy remains an open challenge. In manually reviewing 47 papers that report privacy metrics, we identify and compare six distinct privacy notions, and analyze how the associated metrics capture different aspects of privacy risk. We then assess how well these notions align with legal privacy standards (HIPAA and GDPR), as well as user-centered expectations grounded in HCI studies. Our analysis offers practical guidance on navigating the landscape of privacy evaluation approaches further and highlights gaps in current practices. Ultimately, we aim to facilitate more robust, comparable, and legally aware privacy evaluations in text anonymization.", "AI": {"tldr": "研究系统性地调查了47篇关于文本匿名化的论文，识别并比较了六种隐私保护概念，评估其与法律标准和用户期望的吻合度，以促进更实用和合法的隐私保护评估。", "motivation": "澄清和统一文本隐私保护的评估指标，解决匿名化方法是否足够保护隐私这一开放性问题。", "method": "通过系统性调查，手动审查了47篇报告隐私度量的论文，识别并比较了六种不同的隐私概念，并分析了相关度量捕捉隐私风险的不同方面。", "result": "评估这些概念与法律隐私标准（HIPAA和GDPR）以及基于HCI研究的用户中心期望的吻合程度。", "conclusion": "为隐私评估方法的导航提供了实用指导，并突出了当前实践中的差距，以促进更强大、可比性和法律意识的隐私评估。"}}
{"id": "2512.00381", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.00381", "abs": "https://arxiv.org/abs/2512.00381", "authors": ["Dong Li", "HuaLiang Lin", "JiaYu Li"], "title": "Pore-scale Image Patch Dataset and A Comparative Evaluation of Pore-scale Facial Features", "comment": null, "summary": "The weak-texture nature of facial skin regions presents significant challenges for local descriptor matching in applications such as facial motion analysis and 3D face reconstruction. Although deep learning-based descriptors have demonstrated superior performance to traditional hand-crafted descriptors in many applications, the scarcity of pore-scale image patch datasets has hindered their further development in the facial domain. In this paper, we propose the PorePatch dataset, a high-quality pore-scale image patch dataset, and establish a rational evaluation benchmark. We introduce a Data-Model Co-Evolution (DMCE) framework to generate a progressively refined, high-quality dataset from high-resolution facial images. We then train existing SOTA models on our dataset and conduct extensive experiments. Our results show that the SOTA model achieves a FPR95 value of 1.91% on the matching task, outperforming PSIFT (22.41%) by a margin of 20.5%. However, its advantage is diminished in the 3D reconstruction task, where its overall performance is not significantly better than that of traditional descriptors. This indicates that deep learning descriptors still have limitations in addressing the challenges of facial weak-texture regions, and much work remains to be done in this field.", "AI": {"tldr": "本研究提出了PorePatch数据集和数据-模型协同进化（DMCE）框架，用于生成高质量的面部图像补丁。实验表明，最先进的模型在匹配任务上优于传统PSIFT方法，但在3D重建任务上优势不显著，显示深学习描述符在面部区域应用仍有限制。", "motivation": "由于面部皮肤区域缺乏纹理，传统的局部描述符匹配在面部运动分析和3D面部重建等应用中面临重大挑战。深学习描述符在许多应用中表现出色，但缺乏毛孔级图像补丁数据集阻碍了它们在面部领域的进一步发展。", "method": "本研究提出了PorePatch数据集，这是一个高质量的毛孔级图像补丁数据集。同时，研究建立了一个数据-模型协同进化（DMCE）框架，用于从高分辨率面部图像中生成逐步精炼的高质量数据集。", "result": "实验结果显示，在匹配任务中，SOTA模型在FPR95值上达到1.91%，优于PSIFT方法的22.41%。然而，在3D重建任务上，SOTA模型的表现并没有显著优于传统描述符。", "conclusion": "尽管研究中的深学习描述符在匹配任务中表现出色，但在处理面部弱纹理区域时仍有局限性。这表明该领域仍有许多工作需要完成。"}}
{"id": "2512.01174", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.01174", "abs": "https://arxiv.org/abs/2512.01174", "authors": ["Hyunjun Kim", "Sooyoung Ryu"], "title": "DrawingBench: Evaluating Spatial Reasoning and UI Interaction Capabilities of Large Language Models through Mouse-Based Drawing Tasks", "comment": "AAAI 2026 TrustAgent Workshop", "summary": "As agentic AI systems increasingly operate autonomously, establishing trust through verifiable evaluation becomes critical. Yet existing benchmarks lack the transparency and auditability needed to assess whether agents behave reliably. We present DrawingBench, a verification framework for evaluating the trustworthiness of agentic LLMs through spatial reasoning tasks that require generating sequences of low-level GUI actions. Unlike opaque evaluations, DrawingBench provides transparent, rule-based assessment: 8 objective criteria enable reproducible scoring, while action-level inspection allows stakeholders to audit agent behavior. Our framework comprises 250 diverse prompts across 20 categories and 4 difficulty levels, deterministic evaluation metrics, and an external oversight mechanism through multi-turn feedback that enables human control over agent refinement. Evaluating four state-of-the-art LLMs (Claude-4 Sonnet, GPT-4.1, GPT-4.1-mini, Gemini-2.5 Flash) across 1,000 tests, we establish both capabilities and limitations: models achieved 92.8% perfect performance with structured external feedback driving significant improvements (average +3.2%, up to +32.8% for complex scenes), but systematic error patterns emerged in tool state management and long-horizon planning. Notably, specification clarity proved more important than task complexity -- models achieved 100% perfect performance when given explicit, verifiable criteria. These findings demonstrate that transparent evaluation frameworks can establish trust in agentic systems, with external oversight proving more reliable than self-correction for guiding agent behavior. Our open-source framework provides a template for trustworthy agent assessment. Code and data: https://github.com/hyunjun1121/DrawingBench", "AI": {"tldr": "DrawingBench是一种用于评估代理人工智能系统可信性的框架，通过提供透明度和人类监管评估其行为。实验结果表明，外部反馈改善了模型性能，尤其是在复杂环境中。", "motivation": "鉴于现有的基准测试缺乏透明度和可审计性，难以评估代理是否可靠行为，因此开发DrawingBench框架来解决这一问题。", "method": "DrawingBench是一个评估代理LLMs可信度的框架，通过空间推理任务，要求生成一系列低级别的GUI动作。该框架提供了透明的、基于规则的评估标准，包含8个客观标准以及多回合反馈机制，允许人类在代理行为中发挥作用。", "result": "评估四种最先进的LLMs（Claude-4 Sonnet, GPT-4.1, GPT-4.1-mini, Gemini-2.5 Flash）结果显示，模型在提供明确的可验证标准的情况下可以达到100%的完美表现，外部反馈对于复杂场景的性能改进尤为显著。", "conclusion": "研究结论支持透明评估框架在建立信任度方面的作用，外部监管比自我更正更可靠地引导代理行为。"}}
{"id": "2512.00385", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.00385", "abs": "https://arxiv.org/abs/2512.00385", "authors": ["Louis Geist", "Loic Landrieu", "Damien Robert"], "title": "EZ-SP: Fast and Lightweight Superpoint-Based 3D Segmentation", "comment": null, "summary": "Superpoint-based pipelines provide an efficient alternative to point- or voxel-based 3D semantic segmentation, but are often bottlenecked by their CPU-bound partition step. We propose a learnable, fully GPU partitioning algorithm that generates geometrically and semantically coherent superpoints 13$\\times$ faster than prior methods. Our module is compact (under 60k parameters), trains in under 20 minutes with a differentiable surrogate loss, and requires no handcrafted features. Combine with a lightweight superpoint classifier, the full pipeline fits in $<$2 MB of VRAM, scales to multi-million-point scenes, and supports real-time inference. With 72$\\times$ faster inference and 120$\\times$ fewer parameters, EZ-SP matches the accuracy of point-based SOTA models across three domains: indoor scans (S3DIS), autonomous driving (KITTI-360), and aerial LiDAR (DALES). Code and pretrained models are accessible at github.com/drprojects/superpoint_transformer.", "AI": {"tldr": "一种新的基于GPU的方法（EZ-SP）在生成超级点的效率和参数数量上，大幅超越了现有技术，同时在不同场景中保持了高度的准确性。", "motivation": "基于超级点的管道在3D语义分割中提供了一种高效的替代方案，但仍受限于其基于CPU的分区步骤。通过提出一种更快速的分区算法，旨在解决这一瓶颈问题。", "method": "提出了一种可学习的、完全基于GPU的分区算法，该算法比以往的方法快13倍，能够生成几何和语义上连贯的超级点。该模块紧凑（参数少于60k），可以在20分钟内使用可微替代损失进行训练，并且不需要手工特征。结合轻量级超级点分类器，整个管道占用的VRAM少于2MB，可以扩展到数百万点的场景，并且支持实时推理。", "result": "与基于点的最先进模型相比，EZ-SP在三个领域中达到了同等的准确性：室内扫描（S3DIS）、自动驾驶（KITTI-360）和航空LiDAR（DALES）。", "conclusion": "EZ-SP通过使用新的分区算法实现了72倍的推理速度和120倍更少的参数，并能够在多个大型场景中实现实时推理，表现出了强大的性能。"}}
{"id": "2512.01183", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.01183", "abs": "https://arxiv.org/abs/2512.01183", "authors": ["Yongxin Zhou", "Philippe Mulhem", "Didier Schwab"], "title": "TempPerturb-Eval: On the Joint Effects of Internal Temperature and External Perturbations in RAG Robustness", "comment": null, "summary": "The evaluation of Retrieval-Augmented Generation (RAG) systems typically examines retrieval quality and generation parameters like temperature in isolation, overlooking their interaction. This work presents a systematic investigation of how text perturbations (simulating noisy retrieval) interact with temperature settings across multiple LLM runs. We propose a comprehensive RAG Perturbation-Temperature Analysis Framework that subjects retrieved documents to three distinct perturbation types across varying temperature settings. Through extensive experiments on HotpotQA with both open-source and proprietary LLMs, we demonstrate that performance degradation follows distinct patterns: high-temperature settings consistently amplify vulnerability to perturbations, while certain perturbation types exhibit non-linear sensitivity across the temperature range. Our work yields three key contributions: (1) a diagnostic benchmark for assessing RAG robustness, (2) an analytical framework for quantifying perturbation-temperature interactions, and (3) practical guidelines for model selection and parameter tuning under noisy retrieval conditions.", "AI": {"tldr": "本文提出一种分析框架，用来评估RAG系统在不同温度设置和文本扰动下的性能表现，发现高温设置下的系统对扰动更为敏感。", "motivation": "本文旨在系统地研究文本扰动与温度设定在LLM中的相互作用，因为现有的RAG系统评估通常独立考虑检索质量和生成参数，忽视了它们之间的相互关系。", "method": "本文提出一个全面的检索增强生成（RAG）系统扰动-温度分析框架，通过多种大型语言模型（LLM）在HotpotQA数据集上的实验，探讨了三种不同的扰动类型在不同温度设定下的效果。", "result": "研究结果表明性能下降遵循一定的模式：高温设置会一致地放大对扰动的脆弱性，而某些扰动类型在温度范围内表现出非线性敏感性。", "conclusion": "本文的工作主要贡献了三项内容：（1）用于评估RAG系统鲁棒性的诊断基准，（2）用于量化学扰动-温度相互作用的分析框架，（3）在噪声检索条件下的模型选择和参数调优实践指南。"}}
{"id": "2512.00387", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.00387", "abs": "https://arxiv.org/abs/2512.00387", "authors": ["Kaihang Pan", "Weile Chen", "Haiyi Qiu", "Qifan Yu", "Wendong Bu", "Zehan Wang", "Yun Zhu", "Juncheng Li", "Siliang Tang"], "title": "WiseEdit: Benchmarking Cognition- and Creativity-Informed Image Editing", "comment": "32 pages, 20 figures. Project Page: https://qnancy.github.io/wiseedit_project_page/", "summary": "Recent image editing models boast next-level intelligent capabilities, facilitating cognition- and creativity-informed image editing. Yet, existing benchmarks provide too narrow a scope for evaluation, failing to holistically assess these advanced abilities. To address this, we introduce WiseEdit, a knowledge-intensive benchmark for comprehensive evaluation of cognition- and creativity-informed image editing, featuring deep task depth and broad knowledge breadth. Drawing an analogy to human cognitive creation, WiseEdit decomposes image editing into three cascaded steps, i.e., Awareness, Interpretation, and Imagination, each corresponding to a task that poses a challenge for models to complete at the specific step. It also encompasses complex tasks, where none of the three steps can be finished easily. Furthermore, WiseEdit incorporates three fundamental types of knowledge: Declarative, Procedural, and Metacognitive knowledge. Ultimately, WiseEdit comprises 1,220 test cases, objectively revealing the limitations of SoTA image editing models in knowledge-based cognitive reasoning and creative composition capabilities. The benchmark, evaluation code, and the generated images of each model will be made publicly available soon. Project Page: https://qnancy.github.io/wiseedit_project_page/.", "AI": {"tldr": "WiseEdit是一个新的基准测试，专门用于全面评估具有高度认知和创造性的图像编辑模型，揭示了当前模型在知识驱动的认知推理和创造性组合方面的能力不足。", "motivation": "鉴于现有的基准测试无法全面评估具备先进认知和创造性能力的图像编辑模型，本研究提出了WiseEdit来弥补这一不足。", "method": "本研究引入了WiseEdit，一个知识密集型的基准测试，用于全面评估认知及创造性的图像编辑能力。WiseEdit借鉴人类认知创作的过程，将图像编辑分解为三个连续的阶段：Awareness(意识)、Interpretation(解读)、Imagination(想象)，每个阶段对应一个任务，这三个任务共同构成了编辑过程中的挑战。该基准还包括了一些复杂的任务，这些任务在三个步骤中都无法轻易完成。此外，WiseEdit还整合了三种基本类型的知识：陈述性知识、程序性知识和元认知知识。", "result": "WiseEdit包含1,220个测试案例，客观地揭示了当前最先进的图像编辑模型在基于知识的认知推理和创造性组合能力方面的局限性。", "conclusion": "本研究通过WiseEdit基准测试，旨在评估当前最先进的图像编辑模型的认知和创造性能力，并揭示它们在这些方面的局限性。"}}
{"id": "2512.01191", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.01191", "abs": "https://arxiv.org/abs/2512.01191", "authors": ["Krithik Vishwanath", "Mrigayu Ghosh", "Anton Alyakin", "Daniel Alexander Alber", "Yindalon Aphinyanaphongs", "Eric Karl Oermann"], "title": "Generalist Large Language Models Outperform Clinical Tools on Medical Benchmarks", "comment": "17 pages, 4 figures (2 regular, 2 supplemental)", "summary": "Specialized clinical AI assistants are rapidly entering medical practice, often framed as safer or more reliable than general-purpose large language models (LLMs). Yet, unlike frontier models, these clinical tools are rarely subjected to independent, quantitative evaluation, creating a critical evidence gap despite their growing influence on diagnosis, triage, and guideline interpretation. We assessed two widely deployed clinical AI systems (OpenEvidence and UpToDate Expert AI) against three state-of-the-art generalist LLMs (GPT-5, Gemini 3 Pro, and Claude Sonnet 4.5) using a 1,000-item mini-benchmark combining MedQA (medical knowledge) and HealthBench (clinician-alignment) tasks. Generalist models consistently outperformed clinical tools, with GPT-5 achieving the highest scores, while OpenEvidence and UpToDate demonstrated deficits in completeness, communication quality, context awareness, and systems-based safety reasoning. These findings reveal that tools marketed for clinical decision support may often lag behind frontier LLMs, underscoring the urgent need for transparent, independent evaluation before deployment in patient-facing workflows.", "AI": {"tldr": "研究发现，在诊所辅助决策中，通用LLMs的表现优于临床AI工具，建议在患者工作中部署前需进行透明、独立的评估。", "motivation": "尽管专门的临床AI助手正迅速进入医疗实践，由于它们很少受到独立且定量的评估，这导致了一种关键证据的缺失。这些工具在诊断、分诊和指导原则解释上的影响越来越大。此研究旨在填补这一证据缺口。", "method": "使用了一个包含1,000个项目的迷你基准测试，结合了MedQA（医学知识）和HealthBench（医生一致性）任务，评估了两个广泛部署的临床AI系统（OpenEvidence和UpToDate Expert AI）和三种前沿的通用LLM（GPT-5、Gemini 3 Pro和Claude Sonnet 4.5）之间的性能差距。", "result": "通用模型在总体上表现优于临床工具，GPT-5取得了最高的分数，而OpenEvidence和UpToDate在完整性、交流质量、情境意识和系统性安全推理方面存在缺陷。", "conclusion": "临床决策支持工具可能落在前沿通用LLM后的状况，强调在进入临床实践之前，需要进行透明和独立的评估。"}}
{"id": "2512.00395", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.00395", "abs": "https://arxiv.org/abs/2512.00395", "authors": ["Jiazhen Liu", "Mingkuan Feng", "Long Chen"], "title": "Better, Stronger, Faster: Tackling the Trilemma in MLLM-based Segmentation with Simultaneous Textual Mask Prediction", "comment": null, "summary": "Integrating segmentation into Multimodal Large Language Models (MLLMs) presents a core trilemma: simultaneously preserving dialogue ability, achieving high segmentation performance, and ensuring fast inference. Prevailing paradigms are forced into a compromise. Embedding prediction methods introduce a conflicting pixel-level objective that degrades the MLLM's general dialogue abilities. The alternative, next-token prediction, reframes segmentation as an autoregressive task, which preserves dialogue but forces a trade-off between poor segmentation performance with sparse outputs or prohibitive inference speeds with rich ones. We resolve this trilemma with all-mask prediction, a novel paradigm that decouples autoregressive dialogue generation from non-autoregressive mask prediction. We present STAMP: Simultaneous Textual All-Mask Prediction, an MLLM that embodies this paradigm. After generating a textual response, STAMP predicts an entire segmentation mask in a single forward pass by treating it as a parallel \"fill-in-the-blank\" task over image patches. This design maintains the MLLM's dialogue ability by avoiding conflicting objectives, enables high segmentation performance by leveraging rich, bidirectional spatial context for all mask tokens, and achieves exceptional speed. Extensive experiments show that STAMP significantly outperforms state-of-the-art methods across multiple segmentation benchmarks, providing a solution that excels in dialogue, segmentation, and speed without compromise.", "AI": {"tldr": "Introduces STAMP, an MLLM that uses all-mask prediction to maintain dialogue ability, achieve high segmentation performance, and improve inference speed", "motivation": "to resolve the trilemma in integrating segmentation into Multimodal Large Language Models (MLLMs) - preserving dialogue ability, achieving high segmentation performance, and ensuring fast inference", "method": "all-mask prediction, embodied by STAMP: Simultaneous Textual All-Mask Prediction, which predicts an entire segmentation mask in a single forward pass by treating it as a parallel 'fill-in-the-blank' task over image patches", "result": "STAMP outperforms state-of-the-art methods across multiple segmentation benchmarks", "conclusion": "STAMP provides a solution to the trilemma by excelling in dialogue, segmentation, and speed without compromise"}}
{"id": "2512.01198", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.01198", "abs": "https://arxiv.org/abs/2512.01198", "authors": ["Jiatong Han"], "title": "Conveying Imagistic Thinking in Traditional Chinese Medicine Translation: A Prompt Engineering and LLM-Based Evaluation Framework", "comment": "3 figures", "summary": "Traditional Chinese Medicine (TCM) theory is built on imagistic thinking, in which medical principles and diagnostic and therapeutic logic are structured through metaphor and metonymy. However, existing English translations largely rely on literal rendering, making it difficult for target-language readers to reconstruct the underlying conceptual networks and apply them in clinical practice. This study adopted a human-in-the-loop (HITL) framework and selected four passages from the medical canon Huangdi Neijing that are fundamental in theory. Through prompt-based cognitive scaffolding, DeepSeek V3.1 was guided to identify metaphor and metonymy in the source text and convey the theory in translation. In the evaluation stage, ChatGPT 5 Pro and Gemini 2.5 Pro were instructed by prompts to simulate three types of real-world readers. Human translations, baseline model translations, and prompt-adjusted translations were scored by the simulated readers across five cognitive dimensions, followed by structured interviews and Interpretative Phenomenological Analysis (IPA). Results show that the prompt-adjusted LLM translations perform best across all five dimensions, with high cross-model and cross-role consistency. The interview themes reveal differences between human and machine translation, effective strategies for metaphor and metonymy transfer, and readers' cognitive preferences. This study provides a cognitive, efficient, and replicable HITL methodological pathway for the translation of ancient, concept-dense texts such as TCM.", "AI": {"tldr": "本研究采用人类介入循环方法，使用DeepSeek V3.1对《黄帝内经》中的重要段落进行翻译，通过提示引导模型理解和传达隐喻与借代，并利用模拟读者评估翻译效果，发现调整提示后的翻译模型在多个认知维度上表现最好，提供了针对概念密集文本如中医翻译的一种认知、高效、可重复的方法论路径。", "motivation": "动机在于解决现有中医理论翻译难以传达隐喻与借代问题，难以帮助目标语言读者理解和应用传统中医理论。", "method": "采用人类介入循环框架，利用DeepSeek V3.1识别并翻译隐喻与借代。采用ChatGPT 5 Pro和Gemini 2.5 Pro模拟不同类型的读者进行评估。", "result": "结果表明，经过调整提示后的语言模型在所有五个认知维度上表现最好。", "conclusion": "本研究提供了一种认知上有效，且可以重复的方法，适用于翻译如中医这样的概念密集型传统文本。"}}
{"id": "2512.00408", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.00408", "abs": "https://arxiv.org/abs/2512.00408", "authors": ["Lingdong Wang", "Guan-Ming Su", "Divya Kothandaraman", "Tsung-Wei Huang", "Mohammad Hajiesmaili", "Ramesh K. Sitaraman"], "title": "Low-Bitrate Video Compression through Semantic-Conditioned Diffusion", "comment": null, "summary": "Traditional video codecs optimized for pixel fidelity collapse at ultra-low bitrates and produce severe artifacts. This failure arises from a fundamental misalignment between pixel accuracy and human perception. We propose a semantic video compression framework named DiSCo that transmits only the most meaningful information while relying on generative priors for detail synthesis. The source video is decomposed into three compact modalities: a textual description, a spatiotemporally degraded video, and optional sketches or poses that respectively capture semantic, appearance, and motion cues. A conditional video diffusion model then reconstructs high-quality, temporally coherent videos from these compact representations. Temporal forward filling, token interleaving, and modality-specific codecs are proposed to improve multimodal generation and modality compactness. Experiments show that our method outperforms baseline semantic and traditional codecs by 2-10X on perceptual metrics at low bitrates.", "AI": {"tldr": "研究提出了DiSCo框架，通过捕获视频语义、外观和运动线索来改进视频压缩，在低比特率下提高了感知质量和压缩效率。", "motivation": "由于传统视频编解码器在超低比特率下针对像素保真度的优化导致失败，本研究旨在解决像素准确性与人类感知之间根本性的不匹配问题，提出了一种新的语义视频压缩框架。", "method": "DiSCo框架将源视频分解为三种紧凑模式：文本描述、时空退化视频和可选草图或姿态，分别捕捉语义、外观和运动线索。然后使用条件视频扩散模型从这些紧凑表示重建高质量的时间连贯视频，并提出时间前向填充、令牌交错和模态特定编解码器以提高多模态生成和模态紧凑性。", "result": "提出的DiSCo框架在低比特率下表现优于传统编解码器和基线语义编解码器2-10倍，特别是在感知指标上。此框架通过传输最具意义的信息和利用生成先验进行细节合成来解决传统像素保真度优化在超低比特率下失效的问题。", "conclusion": "实验表明，在低比特率下，该方法在感知指标上优于基线语义编解码器和传统编解码器。DiSCo框架通过创新的多模态视频传输方式，实现了视频压缩性能的显著提升。"}}
{"id": "2512.01256", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.01256", "abs": "https://arxiv.org/abs/2512.01256", "authors": ["Ekha Morang", "Surhoni A. Ngullie", "Sashienla Longkumer", "Teisovi Angami"], "title": "Sentiment Analysis and Emotion Classification using Machine Learning Techniques for Nagamese Language - A Low-resource Language", "comment": "10 pages", "summary": "The Nagamese language, a.k.a Naga Pidgin, is an Assamese-lexified creole language developed primarily as a means of communication in trade between the people from Nagaland and people from Assam in the north-east India. Substantial amount of work in sentiment analysis has been done for resource-rich languages like English, Hindi, etc. However, no work has been done in Nagamese language. To the best of our knowledge, this is the first attempt on sentiment analysis and emotion classification for the Nagamese Language. The aim of this work is to detect sentiments in terms of polarity (positive, negative and neutral) and basic emotions contained in textual content of Nagamese language. We build sentiment polarity lexicon of 1,195 nagamese words and use these to build features along with additional features for supervised machine learning techniques using Na\"ive Bayes and Support Vector Machines.\n  Keywords: Nagamese, NLP, sentiment analysis, machine learning", "AI": {"tldr": "该研究首次尝试对Nagamese语言进行情感分析和情感分类，构建了包含1,195个Nagamese词汇的情感极性词典，并使用Naive Bayes和支持向量机进行监督机器学习。", "motivation": "尽管其他资源丰富的语言如英语和印地语已经进行了大量的情感分析工作，但Nagamese语言尚未进行相关研究，因此该研究填补了这一空白。", "method": "研究构建了一个包含1,195个Nagamese词汇的情感极性词典作为特征，并利用Naive Bayes和支持向量机进行监督机器学习。", "result": "具体结果细节未在摘要中提供，但研究实现了对Nagamese语言文本中的情感极性（积极、消极和中立）和基本情感进行检测。", "conclusion": "该研究标志着对Nagamese语言进行情感分析和分类的首次尝试，为未来研究在该领域的发展奠定了基础。"}}
{"id": "2512.00413", "categories": ["cs.CV", "cs.GR"], "pdf": "https://arxiv.org/pdf/2512.00413", "abs": "https://arxiv.org/abs/2512.00413", "authors": ["Ji Gan", "Lingxu Chen", "Jiaxu Leng", "Xinbo Gao"], "title": "SplatFont3D: Structure-Aware Text-to-3D Artistic Font Generation with Part-Level Style Control", "comment": null, "summary": "Artistic font generation (AFG) can assist human designers in creating innovative artistic fonts. However, most previous studies primarily focus on 2D artistic fonts in flat design, leaving personalized 3D-AFG largely underexplored. 3D-AFG not only enables applications in immersive 3D environments such as video games and animations, but also may enhance 2D-AFG by rendering 2D fonts of novel views. Moreover, unlike general 3D objects, 3D fonts exhibit precise semantics with strong structural constraints and also demand fine-grained part-level style control. To address these challenges, we propose SplatFont3D, a novel structure-aware text-to-3D AFG framework with 3D Gaussian splatting, which enables the creation of 3D artistic fonts from diverse style text prompts with precise part-level style control. Specifically, we first introduce a Glyph2Cloud module, which progressively enhances both the shapes and styles of 2D glyphs (or components) and produces their corresponding 3D point clouds for Gaussian initialization. The initialized 3D Gaussians are further optimized through interaction with a pretrained 2D diffusion model using score distillation sampling. To enable part-level control, we present a dynamic component assignment strategy that exploits the geometric priors of 3D Gaussians to partition components, while alleviating drift-induced entanglement during 3D Gaussian optimization. Our SplatFont3D provides more explicit and effective part-level style control than NeRF, attaining faster rendering efficiency. Experiments show that our SplatFont3D outperforms existing 3D models for 3D-AFG in style-text consistency, visual quality, and rendering efficiency.", "AI": {"tldr": "本论文提出了SplatFont3D，一种基于3D高斯喷射的结构感知文本到3D艺术字体生成框架，相比现有方法能更高效且有效地进行部分级样式控制，并在多个关键指标上表现出色。", "motivation": "艺术字体生成（AFG）可以协助人类设计师创造创新的艺术字体。然而，大多数以往的研究主要集中在平面设计中的2D艺术字体，而个性化的3D-AFG却几乎未得到探索。3D-AFG不仅能够应用于沉浸式3D环境如视频游戏和动画，还可以通过渲染2D艺术字体的新颖视角来增强2D-AFG效果。与一般3D对象不同，3D字体展示出具有强结构性约束的精确语义，并且需要细粒度的部分级样式控制。因此，研究团队尝试解决这些挑战。", "method": "我们提出了SplatFont3D，这是一种基于3D高斯喷射的新型结构感知文本到3D艺术字体生成框架，能够从多样化的风格文本提示中创建具有精确部分级风格控制的3D艺术字体。具体而言，我们首先引入了一个Glyph2Cloud模块，该模块逐步增强2D字符（或组件）的形状和风格，并生成相应的3D点云用于高斯初始化。初步的3D高斯点则通过与预训练2D扩散模型的得分蒸馏采样进一步优化。为了实现部分级控制，我们提出了一种动态组件分配策略，利用3D高斯的几何先验划分组件，同时在3D高斯优化过程中缓解因漂移导致的纠缠问题。", "result": "实验表明，SplatFont3D在3D-AFG领域超过了现有3D模型，在样式文本一致性、视觉质量及渲染效率等方面。", "conclusion": "综上，我们的研究不仅填补了个性化3D-AFG在学术研究中的空白，还提出了新颖的方法框架SplatFont3D，能够生成具有部分级样式控制的高质量3D艺术字体。这些创新成果为艺术字体设计提供了新的可能性，尤其是在沉浸式3D应用领域。"}}
{"id": "2512.01274", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.01274", "abs": "https://arxiv.org/abs/2512.01274", "authors": ["Zehua Zhao", "Zhixian Huang", "Junren Li", "Siyu Lin", "Junting Zhou", "Fengqi Cao", "Kun Zhou", "Rui Ge", "Tingting Long", "Yuexiang Zhu", "Yan Liu", "Jie Zheng", "Junnian Wei", "Rong Zhu", "Peng Zou", "Wenyu Li", "Zekai Cheng", "Tian Ding", "Yaxuan Wang", "Yizhao Yan", "Tingru Wei", "Haowei Ming", "Weijie Mao", "Chen Sun", "Yiming Liu", "Zichen Wang", "Zuo Zhang", "Tong Yang", "Hao Ma", "Zhen Gao", "Jian Pei"], "title": "SUPERChem: A Multimodal Reasoning Benchmark in Chemistry", "comment": "35 pages, 11 figures, 5 tables", "summary": "Current benchmarks for evaluating the chemical reasoning capabilities of Large Language Models (LLMs) are limited by oversimplified tasks, lack of process-level evaluation, and misalignment with expert-level chemistry skills. To address these issues, we introduce SUPERChem, a benchmark of 500 expert-curated reasoning-intensive chemistry problems, covering diverse subfields and provided in both multimodal and text-only formats. Original content and an iterative curation pipeline eliminate flawed items and mitigate data contamination. Each problem is paired with an expert-authored solution path, enabling Reasoning Path Fidelity (RPF) scoring to evaluate reasoning quality beyond final-answer accuracy. Evaluations against a human baseline of 40.3% accuracy show that even the best-performing model, GPT-5 (High), reaches only 38.5%, followed closely by Gemini 2.5 Pro (37.9%) and DeepSeek-V3.1-Think (37.3%). SUPERChem elicits multi-step, multimodal reasoning, reveals model-dependent effects of visual information, and distinguishes high-fidelity reasoners from heuristic ones. By providing a challenging benchmark and a reliable evaluation framework, SUPERChem aims to facilitate the advancement of LLMs toward expert-level chemical intelligence. The dataset of the benchmark is available at https://huggingface.co/datasets/ZehuaZhao/SUPERChem.", "AI": {"tldr": "SUPERChem是一个由专家筛选出的500个化学推理题的基准，用于评估大型语言模型的化学推理能力。最新模型的性能接近人类基线，但仍有提升空间。", "motivation": "当前的基准测试对大型语言模型的化学推理能力评估存在局限性，如任务过于简化、缺乏过程层面的评估以及与专家级别的化学技能不匹配。", "method": "通过引入SUPERChem基准测试来评估大型语言模型的化学推理能力，该基准包含500个由专家筛选的化学推理题，覆盖多个子领域，并以多模态和纯文本两种格式提供。", "result": "评估结果显示，即使性能最优的GPT-5 (High)模型也只有38.5%的准确率，接近人类基线40.3%的性能。这表明模型在多步推理和图形信息处理方面仍存在差距。", "conclusion": "SUPERChem提出了一个具有挑战性的基准测试，提供了一个可靠的评估框架，有助于推动大型语言模型向专家级别的化学智能发展。"}}
{"id": "2512.00422", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.00422", "abs": "https://arxiv.org/abs/2512.00422", "authors": ["Yingxuan You", "Chen Zhao", "Hantao Zhang", "Mingda Xu", "Pascal Fua"], "title": "PhysGen: Physically Grounded 3D Shape Generation for Industrial Design", "comment": "14 pages, 10 figures", "summary": "Existing generative models for 3D shapes can synthesize high-fidelity and visually plausible shapes. For certain classes of shapes that have undergone an engineering design process, the realism of the shape is tightly coupled with the underlying physical properties, e.g., aerodynamic efficiency for automobiles. Since existing methods lack knowledge of such physics, they are unable to use this knowledge to enhance the realism of shape generation. Motivated by this, we propose a unified physics-based 3D shape generation pipeline, with a focus on industrial design applications. Specifically, we introduce a new flow matching model with explicit physical guidance, consisting of an alternating update process. We iteratively perform a velocity-based update and a physics-based refinement, progressively adjusting the latent code to align with the desired 3D shapes and physical properties. We further strengthen physical validity by incorporating a physics-aware regularization term into the velocity-based update step. To support such physics-guided updates, we build a shape-and-physics variational autoencoder (SP-VAE) that jointly encodes shape and physics information into a unified latent space. The experiments on three benchmarks show that this synergistic formulation improves shape realism beyond mere visual plausibility.", "AI": {"tldr": "论文提出了一种基于物理的3D形状生成模型，其通过结合物理属性和形状信息，提高了模型生成3D形状的真实性和物理准确性。", "motivation": "现有的针对3D形状的生成模型可以生成具有高保真度和视觉上具有说服力的形状。然而，对于某些经历了工程设计过程的形状类别，形状的真实感与其潜在的物理属性紧密相关。而现有方法缺乏对这种物理的了解，因此无法利用这种知识来提升形状生成的真实性。因此，提出了这种基于物理的3D形状生成管道。", "method": "提出了一种统一的基于物理的3D形状生成管道，特别关注工业设计应用。具体来说，引入了一种新的流匹配模型，具有明确的物理指导，包括交替更新过程。迭代执行基于速度的更新和基于物理的细化，逐步调整潜在代码以与所需的3D形状和物理属性对齐。此外，通过在基于速度的更新步骤中加入物理感知的正则化项来增强物理有效性。为了支持这种物理引导的更新，构建了形状和物理变分自编码器（SP-VAE），将形状和物理信息联合编码到统一的潜在空间中。", "result": "在三个基准测试上的实验表明，这种协同公式增强了形状的真实性，而不仅仅是视觉上的可信度。", "conclusion": "随着基于物理属性的调整和正则化，提出了的方法能够在保持视觉真实性的同时，大幅提升3D形状的物理合理性。"}}
{"id": "2512.01282", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.01282", "abs": "https://arxiv.org/abs/2512.01282", "authors": ["Jiahao Yuan", "Zhiqing Cui", "Hanqing Wang", "Yuansheng Gao", "Yucheng Zhou", "Usman Naseem"], "title": "Kardia-R1: Unleashing LLMs to Reason toward Understanding and Empathy for Emotional Support via Rubric-as-Judge Reinforcement Learning", "comment": null, "summary": "As web platforms evolve towards greater personalization and emotional complexity, conversational agents must transcend superficial empathy to demonstrate identity-aware emotional reasoning. However, existing systems face two limitations: (1) reliance on situation-centric datasets lacking persistent user identity, which hampers the capture of personalized affective nuances; and (2) dependence on opaque, coarse reward signals that hinder development of verifiable empathetic reasoning. To address these gaps, we introduce KardiaBench, a large-scale user-grounded benchmark comprising 178,080 QA pairs across 22,080 multi-turn conversations anchored to 671 real-world profiles. The dataset is constructed via a model-in-the-loop pipeline with iterative rubric-guided refinement to ensure psychological plausibility and persona consistency. This progressive empathy pipeline that integrates user comprehension, contextual reasoning, and emotion perception into conversations, followed by iterative critique and rubric-based refinement to ensure psychological plausibility, emotional fidelity, and persona consistency. Building on this, we propose Kardia-R1, a framework that trains models for interpretable, stepwise empathetic cognition. Kardia-R1 leverages Rubric-as-Judge Empathetic Reinforcement Learning (Rubric-ERL), a GRPO-based method that uses explainable, human-aligned rubric rewards to tightly couple user understanding, emotional inference, and supportive response generation. Extensive experiments across four LLM backbones demonstrate that Kardia-R1 consistently outperforms othet methods in emotion accuracy, empathy, relevance, persona consistency, and safety. Our dataset and model will be released at https://github.com/JhCircle/Kardia-R1.", "AI": {"tldr": "该论文提出KardiaBench和Kardia-R1框架，旨在通过用户导向的数据集和解释性强的奖励机制来提升对话系统的个性化情感处理能力。", "motivation": "当前的对话系统在个性化情感理解和一致人格表示方面存在不足，本文旨在通过引入新的基准数据集和学习框架来解决这些问题。", "method": "构建了包含178,080对问答的大规模用户导向基准KardiaBench，并提出了一种基于解释性和人类对齐奖励的Kardia-R1框架，用于训练解释性情感认知模型。", "result": "在四个大型语言模型的基础上进行的实验表明，Kardia-R1框架在情绪准确性、同理心、相关性、人格一致性和安全性方面优于其他方法。", "conclusion": "实验结果表明，通过KardiaBench和Kardia-R1框架，可以显著提升对话系统的情感理解和响应质量。未来将公开源代码和数据集以供进一步研究。"}}
{"id": "2512.00424", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.00424", "abs": "https://arxiv.org/abs/2512.00424", "authors": ["Nthenya Kyatha", "Jay Taneja"], "title": "Recovering Origin Destination Flows from Bus CCTV: Early Results from Nairobi and Kigali", "comment": null, "summary": "Public transport in sub-Saharan Africa (SSA) often operates in overcrowded conditions where existing automated systems fail to capture reliable passenger flow data. Leveraging onboard CCTV already deployed for security, we present a baseline pipeline that combines YOLOv12 detection, BotSORT tracking, OSNet embeddings, OCR-based timestamping, and telematics-based stop classification to recover bus origin--destination (OD) flows. On annotated CCTV segments from Nairobi and Kigali buses, the system attains high counting accuracy under low-density, well-lit conditions (recall $\\approx$95\\%, precision $\\approx$91\\%, F1 $\\approx$93\\%). It produces OD matrices that closely match manual tallies. Under realistic stressors such as overcrowding, color-to-monochrome shifts, posture variation, and non-standard door use, performance degrades sharply (e.g., $\\sim$40\\% undercount in peak-hour boarding and a $\\sim$17 percentage-point drop in recall for monochrome segments), revealing deployment-specific failure modes and motivating more robust, deployment-focused Re-ID methods for SSA transit.", "AI": {"tldr": "研究提出利用已部署的车载CCTV来获取非洲次撒哈拉地区公共交通的可靠的乘客流量数据的方法，该方法虽然在低密度和光线充足条件下效果良好，但在拥挤等现实条件下效果显著下降。", "motivation": "由于现有自动化系统无法准确收集非洲次撒哈拉地区公共交通的乘客流动数据，同时鉴于该地区的公交运营常常处于过度拥挤的状态，研究旨在通过利用已有的车载CCTV来改进数据收集。", "method": "采用结合YOLOv12检测、BotSORT跟踪、OSNet嵌入、基于OCR的时间戳以及基于车载通信系统的站点分类来恢复公交OD流。", "result": "研究系统在注释过的内罗毕和基加利公交车的CCTV片段上，于低密度且光线充足条件下实现了较高的计数准确性（召回率约为95%，精确率约为91%，F1约为93%），其生成的OD矩阵与手动记录的相似。但在过度拥挤、色彩偏移等条件下，性能显著下降。", "conclusion": "通过对现有技术在次撒哈拉非洲地区公共交通上的应用分析，提出了存在特定部署失败模式的观点，并强调了进一步开发更强大且专注部署的识别方法的重要性。"}}
{"id": "2512.01316", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.01316", "abs": "https://arxiv.org/abs/2512.01316", "authors": ["Koki Natsumi", "Hiroyuki Deguchi", "Yusuke Sakai", "Hidetaka Kamigaito", "Taro Watanabe"], "title": "Agreement-Constrained Probabilistic Minimum Bayes Risk Decoding", "comment": "IJCNLP-AACL 2025 Main", "summary": "Minimum Bayes risk (MBR) decoding generates high-quality translations by maximizing the expected utility of output candidates, but it evaluates all pairwise scores over the candidate set; hence, it takes quadratic time with respect to the number of candidates. To reduce the number of utility function calls, probabilistic MBR (PMBR) decoding partially evaluates quality scores using sampled pairs of candidates and completes the missing scores with a matrix completion algorithm. Nevertheless, it degrades the translation quality as the number of utility function calls is reduced. Therefore, to improve the trade-off between quality and cost, we propose agreement-constrained PMBR (AC-PMBR) decoding, which leverages a knowledge distilled model to guide the completion of the score matrix. Our AC-PMBR decoding improved approximation errors of matrix completion by up to 3 times and achieved higher translation quality compared with PMBR decoding at a comparable computational cost on the WMT'23 En$\\leftrightarrow$De translation tasks.", "AI": {"tldr": "提出了AC-PMBR解码方法，提升了评分矩阵完成的精度，相比PMBR解码，在相近的计算成本下，翻译质量更高。", "motivation": "传统的MBR解码虽然能生成高质量的翻译，但其时间复杂度与候选数量的平方成正比。PMBR解码虽然减少了效用函数的调用次数，但降低了翻译质量。我们的目标是提升质量-成本的权衡。", "method": "我们提出了协议约束PMBR（AC-PMBR）解码，它使用知识蒸馏模型来指导评分矩阵的完成。", "result": "在WMT'23的英德翻译任务上，AC-PMBR解码法将评分矩阵的近似误差降低了最多3倍，且翻译质量也得到了提高。", "conclusion": "AC-PMBR方法既减少了计算成本，又提高了翻译质量，特别是在评分矩阵的近似完成上改善明显。"}}
{"id": "2512.00425", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.00425", "abs": "https://arxiv.org/abs/2512.00425", "authors": ["Minh-Quan Le", "Yuanzhi Zhu", "Vicky Kalogeiton", "Dimitris Samaras"], "title": "What about gravity in video generation? Post-Training Newton's Laws with Verifiable Rewards", "comment": "Project page: https://cvlab-stonybrook.github.io/NewtonRewards", "summary": "Recent video diffusion models can synthesize visually compelling clips, yet often violate basic physical laws-objects float, accelerations drift, and collisions behave inconsistently-revealing a persistent gap between visual realism and physical realism. We propose $\\texttt{NewtonRewards}$, the first physics-grounded post-training framework for video generation based on $\\textit{verifiable rewards}$. Instead of relying on human or VLM feedback, $\\texttt{NewtonRewards}$ extracts $\\textit{measurable proxies}$ from generated videos using frozen utility models: optical flow serves as a proxy for velocity, while high-level appearance features serve as a proxy for mass. These proxies enable explicit enforcement of Newtonian structure through two complementary rewards: a Newtonian kinematic constraint enforcing constant-acceleration dynamics, and a mass conservation reward preventing trivial, degenerate solutions. We evaluate $\\texttt{NewtonRewards}$ on five Newtonian Motion Primitives (free fall, horizontal/parabolic throw, and ramp sliding down/up) using our newly constructed large-scale benchmark, $\\texttt{NewtonBench-60K}$. Across all primitives in visual and physics metrics, $\\texttt{NewtonRewards}$ consistently improves physical plausibility, motion smoothness, and temporal coherence over prior post-training methods. It further maintains strong performance under out-of-distribution shifts in height, speed, and friction. Our results show that physics-grounded verifiable rewards offer a scalable path toward physics-aware video generation.", "AI": {"tldr": "提出了一个基于可验证奖励的物理基础视频生成后训练框架NewtonRewards，通过从生成的视频中提取可测量代理，提高了视频的物理合理性、运动流畅性和时间连贯性。", "motivation": "旨在解决现有的视频扩散模型生成的视频虽然在视觉上逼真，但在物理特性上存在不足的问题，提升物理上的合理性。", "method": "使用冻结的效用模型从生成的视频中提取可测量代理，用作牛顿动力学约束和质量守恒奖励的测量标准，以确保牛顿力学结构得到了显式执行。", "result": "在构造的NewtonBench-60K大规模基准测试中，针对五种牛顿运动模式进行评估，表明NewtonRewards在物理合理性、运动流畅性和时间连贯性方面优于先前的后训练方法。", "conclusion": "研究结果表明，基于物理基础的可验证奖励方法为物理感知的视频生成提供了一条可扩展的路径。"}}
{"id": "2512.01369", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.01369", "abs": "https://arxiv.org/abs/2512.01369", "authors": ["Md. Rafiul Biswas", "Firoj Alam", "Wajdi Zaghouani"], "title": "MARSAD: A Multi-Functional Tool for Real-Time Social Media Analysis", "comment": "6 pages, 4 figures", "summary": "MARSAD is a multifunctional natural language processing (NLP) platform designed for real-time social media monitoring and analysis, with a particular focus on the Arabic-speaking world. It enables researchers and non-technical users alike to examine both live and archived social media content, producing detailed visualizations and reports across various dimensions, including sentiment analysis, emotion analysis, propaganda detection, fact-checking, and hate speech detection. The platform also provides secure data-scraping capabilities through API keys for accessing public social media data. MARSAD's backend architecture integrates flexible document storage with structured data management, ensuring efficient processing of large and multimodal datasets. Its user-friendly frontend supports seamless data upload and interaction.", "AI": {"tldr": "MARSAD 是一个多用途自然语言处理平台，专注于实时社交媒体监测和分析，特别是在阿拉伯语环境中。它允许研究人员和非技术人员分析实时和存档的社交媒体内容，并提供详细的情感分析、情绪分析、宣传检测、事实核查和仇恨言论检测可视化和报告。", "motivation": "开发 MARSAD 是为了提供一个工具，使得研究人员和非技术用户能够分析社交媒体内容，尤其是在阿拉伯语环境中，以实现更有效的实时监测和分析。", "method": "MARSAD 平台的后端架构集成了灵活的文档存储和结构化数据管理，支持通过 API 密钥访问公共社交媒体数据的数据抓取功能，而其前端则提供了用户友好的界面，支持无缝数据上传和交互。", "result": "尚未进行具体实验结果的描述。但一般可以预期，MARSAD 能够有效地处理大规模和多模态的数据集，并生产详细的分析结果。", "conclusion": "MARSAD 为阿拉伯语社交媒体内容的实时监测和分析提供了一个强大的工具，具备情感分析、情绪分析、宣传检测、事实核查和仇恨言论检测等多功能。"}}
{"id": "2512.00428", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.00428", "abs": "https://arxiv.org/abs/2512.00428", "authors": ["Jiachuan Peng", "Kyle Lam", "Jianing Qiu"], "title": "Recognizing Pneumonia in Real-World Chest X-rays with a Classifier Trained with Images Synthetically Generated by Nano Banana", "comment": "9 pages", "summary": "We trained a classifier with synthetic chest X-ray (CXR) images generated by Nano Banana, the latest AI model for image generation and editing, released by Google. When directly applied to real-world CXRs having only been trained with synthetic data, the classifier achieved an AUROC of 0.923 (95% CI: 0.919 - 0.927), and an AUPR of 0.900 (95% CI: 0.894 - 0.907) in recognizing pneumonia in the 2018 RSNA Pneumonia Detection dataset (14,863 CXRs), and an AUROC of 0.824 (95% CI: 0.810 - 0.836), and an AUPR of 0.913 (95% CI: 0.904 - 0.922) in the Chest X-Ray dataset (5,856 CXRs). These external validation results on real-world data demonstrate the feasibility of this approach and suggest potential for synthetic data in medical AI development. Nonetheless, several limitations remain at present, including challenges in prompt design for controlling the diversity of synthetic CXR data and the requirement for post-processing to ensure alignment with real-world data. However, the growing sophistication and accessibility of medical intelligence will necessitate substantial validation, regulatory approval, and ethical oversight prior to clinical translation.", "AI": {"tldr": "研究使用合成胸部X光图像训练分类器进行肺炎检测，展示了在实际数据上良好的性能，同时也指出了在合成数据及其应用过程中存在的若干局限。", "motivation": "探索使用合成胸部X射线图像进行肺炎检测的可行性，以降低获取真实医疗图像的需求并加快医学AI的发展进程。", "method": "使用由Google发布的最新图像生成和编辑AI模型Nano Banana生成的合成胸部X光（CXR）图像来训练一个分类器。", "result": "直接应用于仅用合成数据训练而来的实际CXR时，分类器在2018年RSNA肺炎检测数据集(14,863张CXR)中识别肺炎的AUROC为0.923（95% CI: 0.919 - 0.927），AUPR为0.900（95% CI: 0.894 - 0.907）。同时，在胸部X光数据集(5,856张CXR)中的外部验证结果AUROC为0.824（95% CI: 0.810 - 0.836），AUPR为0.913（95% CI: 0.904 - 0.922）。", "conclusion": "这些基于实际数据的外部验证结果表明该方法的可行性，并暗示合成数据在医学AI发展中具有潜在价值。然而，仍有包括合成CXR数据多样性控制提示设计挑战、与实际数据对齐所需后处理要求在内的若干局限存在。进一步的临床转化还需要大量的验证、监管审批和伦理监督。"}}
{"id": "2512.01410", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.01410", "abs": "https://arxiv.org/abs/2512.01410", "authors": ["Ruohan Zhou", "Jiachen Yuan", "Churui Yang", "Wenzheng Huang", "Guoyan Zhang", "Shiyao Wei", "Jiazhen Hu", "Ning Xin", "Md Maruf Hasan"], "title": "DyFuLM: An Advanced Multimodal Framework for Sentiment Analysis", "comment": "8 pages, 6 figures, preprint. Under review for a suitable AI conference", "summary": "Understanding sentiment in complex textual expressions remains a fundamental challenge in affective computing. To address this, we propose a Dynamic Fusion Learning Model (DyFuLM), a multimodal framework designed to capture both hierarchical semantic representations and fine-grained emotional nuances. DyFuLM introduces two key moodules: a Hierarchical Dynamic Fusion module that adaptively integrates multi-level features, and a Gated Feature Aggregation module that regulates cross-layer information ffow to achieve balanced representation learning. Comprehensive experiments on multi-task sentiment datasets demonstrate that DyFuLM achieves 82.64% coarse-grained and 68.48% fine-grained accuracy, yielding the lowest regression errors (MAE = 0.0674, MSE = 0.0082) and the highest R^2 coefficient of determination (R^2= 0.6903). Furthermore, the ablation study validates the effectiveness of each module in DyFuLM. When all modules are removed, the accuracy drops by 0.91% for coarse-grained and 0.68% for fine-grained tasks. Keeping only the gated fusion module causes decreases of 0.75% and 0.55%, while removing the dynamic loss mechanism results in drops of 0.78% and 0.26% for coarse-grained and fine-grained sentiment classification, respectively. These results demonstrate that each module contributes significantly to feature interaction and task balance. Overall, the experimental findings further validate that DyFuLM enhances sentiment representation and overall performance through effective hierarchical feature fusion.", "AI": {"tldr": "本研究提出了一种名为DyFuLM的动态融合学习模型，用于捕捉复杂文本中的情感，展示了其在情感分析任务中的优越性。", "motivation": "理解复杂文本中的情感对于情感计算是基本的挑战，需要一种新的模型来解决该问题。", "method": "DyFuLM模型包括两个主要模块：层次动态融合模块和门控特征聚合模块，以适应性地集成多级特征并平衡表征学习。", "result": "该模型在多任务情感数据集上展示了优异的性能，细粒度情感准确性为68.48%，并解释了每个模块的重要性。", "conclusion": "实验结果证明DyFuLM通过有效的多层次特征融合提升了情感表征和整体性能。"}}
{"id": "2512.00438", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.00438", "abs": "https://arxiv.org/abs/2512.00438", "authors": ["Hang Xu", "Linjiang Huang", "Feng Zhao"], "title": "FR-TTS: Test-Time Scaling for NTP-based Image Generation with Effective Filling-based Reward Signal", "comment": null, "summary": "Test-time scaling (TTS) has become a prevalent technique in image generation, significantly boosting output quality by expanding the number of parallel samples and filtering them using pre-trained reward models. However, applying this powerful methodology to the next-token prediction (NTP) paradigm remains challenging. The primary obstacle is the low correlation between the reward of an image decoded from an intermediate token sequence and the reward of the fully generated image. Consequently, these incomplete intermediate representations prove to be poor indicators for guiding the pruning direction, a limitation that stems from their inherent incompleteness in scale or semantic content. To effectively address this critical issue, we introduce the Filling-Based Reward (FR). This novel design estimates the approximate future trajectory of an intermediate sample by finding and applying a reasonable filling scheme to complete the sequence. Both the correlation coefficient between rewards of intermediate samples and final samples, as well as multiple intrinsic signals like token confidence, indicate that the FR provides an excellent and reliable metric for accurately evaluating the quality of intermediate samples. Building upon this foundation, we propose FR-TTS, a sophisticated scaling strategy. FR-TTS efficiently searches for good filling schemes and incorporates a diversity reward with a dynamic weighting schedule to achieve a balanced and comprehensive evaluation of intermediate samples. We experimentally validate the superiority of FR-TTS over multiple established benchmarks and various reward models. Code is available at \\href{https://github.com/xuhang07/FR-TTS}{https://github.com/xuhang07/FR-TTS}.", "AI": {"tldr": "A novel Filling-Based Reward (FR) method is introduced to improve the evaluation of intermediate samples' quality in next-token prediction, and FR-TTS, a scaling strategy, significantly enhances performance over existing benchmarks and reward models.", "motivation": "The primary motivation is to overcome the limitation of test-time scaling (TTS) application in next-token prediction (NTP) due to the poor correlation between intermediate and final image rewards and to provide an effective method for guiding NTP.", "method": "Test-time scaling (TTS) is adopted in image generation by expanding parallel samples and filtering them using pre-trained reward models, but applying it to next-token prediction (NTP) is challenging due to low correlation between intermediate and final image rewards. To solve this, Filling-Based Reward (FR) is introduced to estimate future intermediate trajectories by finding a reasonable filling scheme and providing a reliable metric for evaluating intermediate sample quality. FR-TTS, a scaling strategy, efficiently searches for good filling schemes and incorporates a diversity reward with dynamic weighting to balance the evaluation.", "result": "Experimental validation shows that FR-TTS outperforms multiple established benchmarks and various reward models in the evaluation of intermediate sample quality.", "conclusion": "FR-TTS significantly improves the evaluation of intermediate samples' quality in next-token prediction by providing a reliable filling scheme to estimate future trajectories, indicating enhanced potential for broader applications in complex sequence generation tasks."}}
{"id": "2512.01420", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.01420", "abs": "https://arxiv.org/abs/2512.01420", "authors": ["Yaxuan Wang", "Quan Liu", "Zhenting Wang", "Zichao Li", "Wei Wei", "Yang Liu", "Yujia Bao"], "title": "PromptBridge: Cross-Model Prompt Transfer for Large Language Models", "comment": null, "summary": "Large language models (LLMs) underpin applications in code generation, mathematical reasoning, and agent-based workflows. In practice, systems access LLMs via commercial APIs or open-source deployments, and the model landscape (e.g., GPT, Claude, Llama) evolves rapidly. This rapid evolution forces frequent model switches driven by capability, cost, deployment constraints, and privacy. Yet prompts are highly model-sensitive: reusing a prompt engineered for one model on another often yields substantially worse performance than a prompt optimized for the target model. We term this phenomenon Model Drifting. Through extensive empirical analysis across diverse LLM configurations, we show that model drifting is both common and severe. To address this challenge, we introduce PromptBridge, a training-free framework that preserves prompt effectiveness under model switches, enabling cross-model prompt transfer without costly per-task or per-model re-optimization. PromptBridge requires only a small set of alignment tasks for calibration. It first applies Model-Adaptive Reflective Prompt Evolution (MAP-RPE) to obtain task- and model-specific optimal prompts via iterative reflective refinement and quantitative evaluation. Using the resulting calibrated prompt pairs for the source and target models, PromptBridge learns a cross-model prompt mapping. At test time, i.e., for an unseen task, given a source-model prompt, this mapping directly produces an optimized prompt for the target model. Experiments in single-agent and multi-agent settings show that PromptBridge consistently improves downstream accuracy while reducing migration effort. The code will be available soon.", "AI": {"tldr": "研究了模型漂移问题，即在不同模型之间切换时提示的有效性下降，提出了一种训练免费的框架PromptBridge来解决这个问题，能够有效转换提示以适应不同的模型，而不需要对每个任务或每个模型再次优化。", "motivation": "大规模语言模型（LLMs）的快速发展导致频繁的模型切换，而提示对模型的高度敏感性使得使用为一个模型设计的提示常常在另一个模型上表现不佳，我们称这种现象为模型漂移。", "method": "提出了PromptBridge框架，该框架在不进行训练的情况下保持了提示的有效性，通过模型自适应反射提示进化（MAP-RPE）在迭代和定量评估中获得特定任务和模型的最佳提示，然后学习跨模型提示映射，以在目标模型中生成优化的提示。", "result": "实验结果表明，PromptBridge框架在单个代理和多代理设置中一致地提高了下游准确性，同时减少了迁移工作量。", "conclusion": "PromptBridge提供了一种有效的解决方案来减少模型漂移的问题，并且能够在不重新优化的情况下，实现了跨模型的提示转换。"}}
{"id": "2512.00450", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.00450", "abs": "https://arxiv.org/abs/2512.00450", "authors": ["Amit Kumar Gupta", "Farhan Sheth", "Hammad Shaikh", "Dheeraj Kumar", "Angkul Puniya", "Deepak Panwar", "Sandeep Chaurasia", "Priya Mathur"], "title": "RecruitView: A Multimodal Dataset for Predicting Personality and Interview Performance for Human Resources Applications", "comment": "20 pages, 10 figures, 10 tables", "summary": "Automated personality and soft skill assessment from multimodal behavioral data remains challenging due to limited datasets and methods that fail to capture geometric structure inherent in human traits. We introduce RecruitView, a dataset of 2,011 naturalistic video interview clips from 300+ participants with 27,000 pairwise comparative judgments across 12 dimensions: Big Five personality traits, overall personality score, and six interview performance metrics. To leverage this data, we propose Cross-Modal Regression with Manifold Fusion (CRMF), a geometric deep learning framework that explicitly models behavioral representations across hyperbolic, spherical, and Euclidean manifolds. CRMF employs geometry-specific expert networks to capture hierarchical trait structures, directional behavioral patterns, and continuous performance variations simultaneously. An adaptive routing mechanism dynamically weights expert contributions based on input characteristics. Through principled tangent space fusion, CRMF achieves superior performance while training 40-50% fewer trainable parameters than large multimodal models. Extensive experiments demonstrate that CRMF substantially outperforms the selected baselines, achieving up to 11.4% improvement in Spearman correlation and 6.0% in concordance index. Our RecruitView dataset is publicly available at https://huggingface.co/datasets/AI4A-lab/RecruitView", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2512.01439", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.01439", "abs": "https://arxiv.org/abs/2512.01439", "authors": ["Bharatdeep Hazarika", "Arya Suneesh", "Prasanna Devadiga", "Pawan Kumar Rajpoot", "Anshuman B Suresh", "Ahmed Ifthaquar Hussain"], "title": "Multilingual Conversational AI for Financial Assistance: Bridging Language Barriers in Indian FinTech", "comment": null, "summary": "India's linguistic diversity presents both opportunities and challenges for fintech platforms. While the country has 31 major languages and over 100 minor ones, only 10\\% of the population understands English, creating barriers to financial inclusion. We present a multilingual conversational AI system for a financial assistance use case that supports code-mixed languages like Hinglish, enabling natural interactions for India's diverse user base. Our system employs a multi-agent architecture with language classification, function management, and multilingual response generation. Through comparative analysis of multiple language models and real-world deployment, we demonstrate significant improvements in user engagement while maintaining low latency overhead (4-8\\%). This work contributes to bridging the language gap in digital financial services for emerging markets.", "AI": {"tldr": "研究开发了一种多语言对话AI系统，以支持代码混搭语言如Hinglish，以增强在印度语言多样化环境中的用户交互，证明对于促进金融包容性具有重要作用。", "motivation": "印度的多元语言环境给金融科技平台带来了机会和挑战。由于只有10%的人口懂英语，语言成为金融包容性的障碍，鉴于此，研究团队创建了一种多语言的对话AI系统以适应印度的用户基础。", "method": "研究采用了一个多代理架构，包括语言分类，功能管理和多语言响应生成等几个关键模块来构建对话AI系统。系统支持包括Hinglish在内的混合语言，使用户能够进行自然互动。", "result": "研究结果表明，相较其他语言模型，该系统的用户参与度有所提高，保持了4-8%的低延迟，证实了该系统的可行性和优越性。", "conclusion": "该研究为新兴市场中的数字金融服务中的语言障碍提供了突破性解决方案。通过支持代码混搭语言（如Hinglish）的多语言对话AI系统，增强了用户交互性，同时保持了低延迟，对于促进印度及其他类似市场的金融包容性具有重要意义。"}}
{"id": "2512.00456", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.00456", "abs": "https://arxiv.org/abs/2512.00456", "authors": ["Guanyu Hu", "Tangzheng Lian", "Dimitrios Kollias", "Oya Celiktutan", "Xinyu Yang"], "title": "CausalAffect: Causal Discovery for Facial Affective Understanding", "comment": null, "summary": "Understanding human affect from facial behavior requires not only accurate recognition but also structured reasoning over the latent dependencies that drive muscle activations and their expressive outcomes. Although Action Units (AUs) have long served as the foundation of affective computing, existing approaches rarely address how to infer psychologically plausible causal relations between AUs and expressions directly from data. We propose CausalAffect, the first framework for causal graph discovery in facial affect analysis. CausalAffect models AU-AU and AU-Expression dependencies through a two-level polarity and direction aware causal hierarchy that integrates population-level regularities with sample-adaptive structures. A feature-level counterfactual intervention mechanism further enforces true causal effects while suppressing spurious correlations. Crucially, our approach requires neither jointly annotated datasets nor handcrafted causal priors, yet it recovers causal structures consistent with established psychological theories while revealing novel inhibitory and previously uncharacterized dependencies. Extensive experiments across six benchmarks demonstrate that CausalAffect advances the state of the art in both AU detection and expression recognition, establishing a principled connection between causal discovery and interpretable facial behavior. All trained models and source code will be released upon acceptance.", "AI": {"tldr": "由于前方空间的疑虑，CausalAffect 依托数据登录动作单元及表情间的因果条件，实现了因果图的平衡，通常表示了动作单元和行为状态之间的有意因果相关性，并改善了动作单元的计算和行为状态预测的特色出刊。一起编程和源代码其储存于签发后。", "motivation": "研究动机在于首次尝试在非情绪学习中引入因果图的发现，聚焦于现存的动作单元（自动单元）和表情之间的因果单元进行探讨。", "method": "CausalAffect 使用了两层级的极性与方向敏感的因果图，并通过二维动态系统支持特定因果条件，可以加强图中的真实因果关系。", "result": "{\n  \"tldr\": \"\\u7531\\u4e8e\\u524d\\u7f6e\\u7a7a\\u95f4\\u7684\\u539f\\u56e0\\uff0cCausalAffect\\u6839\\u636e\\u7531\\u6570\\u636e\\u767b\\u8bb0\\u6676\\u90e8\\u52a8\\u7ea7\\u53ca\\u793a\\u6001\\u4e4b\\u95f4\\u7684\\u539f\\u56e0\\u6761\\u4ef6\\uff0c\\u5b9e\\u73b0\\u4e86\\u4e00\\u4e2a\\u539f\\u56e0\\u56fe\\u7684\\u6d4b\\u5e73\\uff0c\\u4e00\\u822c\\u8868\\u793a\\u4e86\\u6676\\u90e8\\u52a8\\u4f5c\\u548c\\u884c\\u4e3a\\u72b6\\u6001\\u4e4b\\u95f4\\u7684\\u6709\\u610f\\u7684\\u539f\\u56e0\\u4e00\\u5b9a\\u6027\\u76f8\\u5173\\uff0c\\u5e76\\u6539\\u5584\\u4e86\\u6676\\u90e8\\u52a8\\u7ea7\\u7684\\u8ba1\\u6570\\u548c\\u884c\\u4e3a\\u72b6\\u6001\\u8ba1\\u8bba\\u7684\\u7279\\u6cd5\\u51fa\\u7248\\u3002\\u4e00\\u8d77\\u7f16\\u8def\\u548c\\u6e90\\u7801\\u5176\\u5b58\\u50a8\\u4e8e\\u7b7e\\u53d1\\u540e\\u3002\", \n  \"motivation\": \"\\u9996\\u6b21\\u5728\\u975e\\u8868\\u60c5\\u8bad\\u7ec3\\u4e2d\\u63a5\\u5165\\u539f\\u56e0\\u56fe\\u53d1\\u73b0\\uff0c\\u5bf9\\u7531\\u4e0a\\u5b58\\u7684\\u8ffd\\u5e94\\u5668\\u5143\\u5b50(\\u81ea\\u52a8\\u5355\\u5143\\uff09\\u548c\\u884c\\u4e3a\\u72b6\\u6001\\u7684\\u95f4\\u4e2d\\u52a8\\u4f5c\\u4e4b\\u95f4\\u7684\\u9002\\u5e38\\u7d20\\u5143\\u8fdb\\u884c\\u53eb\\u676d\\u8c03\\u7406\\u3002\", \n  \"method\": \"CausalAffect \\u4f7f\\u7528\\u4e86\\u4e24\\u7ea7\\u522b\\u7684\\u578b\\u5f62\\u91cf\\u548c\\u65b9\\u5411\\u4e66\\u7528\\u539f\\u56e0\\u5b50\\u6811\\uff0c\\u5e76\\u652f\\u63f4\\u4e86\\u52a8\\u6001\\u7cfb\\u7edf\\u7279\\u672c\\u7684\\u6709\\u5149\\u6761\\u4ef6\\uff0c\\u540e\\u65b9\\u53ef\\u4ee5\\u5f3a\\u5316\\u56fe\\u7684\\u771f\\u5b9e\\u539f\\u56e0\\u5173\\u7cfb\\u76f8\\u5173\\u3002\", \n  \"result\": \"\\u5728\\u4e00\\u7eb3\\u5ba2\\u5e08\\u4e4b\\u4e0a\\u5e76\\u6ca1\\u6709\\u505a\\u51fc\\u548c\\u624b\\u5de5\\u5b9a\\u4e49\\u539f\\u56e0\\u5b50\\u548c\\u76f8\\u540c\\u7279\\u5f81\\u7684\\u6570\\u636e\\u96c6\\u60c5\\u51b5\\u4e0b\\uff0cCausalAffect \\u53ef\\u4ee5\\u8ddd\\u79bb\\u652f\\u65ad\\u7279\\u5f81\\u51fa\\u4e00\\u7ea7\\u5bf9\\u4f5c\\u4e0d\\u5141\\u4e89\\u7684\\u6570\\u636e\\u5173\\u7cfb\\u3002\", \n  \"conclusion\": \"\\u5c06\\u4f7f\\u7528CausalAffect \\u4e8e\\u5143\\u4ef6\\u52a8\\u4f5c\\u548c\\u884c\\u4e3a\\u72b6\\u6001\\u7684\\u8ba1\\u6570\\u4e2d\\uff0c\\u5b83\\u66f4\\u5927\\u7a0b\\u5ea6\\u6709\\u6548\\u5728\\u6570\\u636e\\u96c6\\u4e0a\\uff0c\\u7f32\\u5c40\\u8ba1\\u7b97\\u65b9\\u5f0f\\u5e76\\u5f53\\u5e94\\u7528\\u4e8e\\u73b0\\u65c1\\u5f97\\u5206\\u5b50\\u57faseof\\u6570\\u636e\\u77e5\\u8bc6\\u8d44\\u6e90\\u4e0a\\u3002\"}\n}", "conclusion": "本研究通过提出CausalAffect框架，实现了从数据中推导出面部动作单元和表情间的因果关系，证明了该框架在没有特别标注数据和没有手工设定因果先验的情况下，依然可以发现与心理学理论一致的因果结构。与此同时，在多个公共数据集上取得了目前领先的表现。这标志着在解释面部行为方面，因果发现已经成为必需的连接。"}}
{"id": "2512.01443", "categories": ["cs.CL", "cs.LG", "cs.NE", "cs.SD"], "pdf": "https://arxiv.org/pdf/2512.01443", "abs": "https://arxiv.org/abs/2512.01443", "authors": ["Xabier de Zuazo", "Ibon Saratxaga", "Eva Navas"], "title": "MEGConformer: Conformer-Based MEG Decoder for Robust Speech and Phoneme Classification", "comment": "10 pages, 5 figures, 4 tables, LibriBrain Workshop, NeurIPS 2025", "summary": "We present Conformer-based decoders for the LibriBrain 2025 PNPL competition, targeting two foundational MEG tasks: Speech Detection and Phoneme Classification. Our approach adapts a compact Conformer to raw 306-channel MEG signals, with a lightweight convolutional projection layer and task-specific heads. For Speech Detection, a MEG-oriented SpecAugment provided a first exploration of MEG-specific augmentation. For Phoneme Classification, we used inverse-square-root class weighting and a dynamic grouping loader to handle 100-sample averaged examples. In addition, a simple instance-level normalization proved critical to mitigate distribution shifts on the holdout split. Using the official Standard track splits and F1-macro for model selection, our best systems achieved 88.9% (Speech) and 65.8% (Phoneme) on the leaderboard, surpassing the competition baselines and ranking within the top-10 in both tasks. For further implementation details, the technical documentation, source code, and checkpoints are available at https://github.com/neural2speech/libribrain-experiments.", "AI": {"tldr": "This paper introduces a Conformer-based decoder for MEG signals aiming at Speech Detection and Phoneme Classification tasks, achieving high F1-macro scores and ranking top-10 on the LibriBrain 2025 PNPL competition.", "motivation": "The goal is to develop Conformer-based decoders for the LibriBrain 2025 PNPL competition, focusing on Speech Detection and Phoneme Classification tasks using MEG data.", "method": "Our approach adapts a compact Conformer to raw 306-channel MEG signals, including a lightweight convolutional projection layer and task-specific heads. Speech Detection utilizes MEG-oriented SpecAugment, while Phoneme Classification employs inverse-square-root class weighting, dynamic grouping loader, and instance-level normalization.", "result": "The system achieved 88.9% for Speech Detection and 65.8% for Phoneme Classification on the leaderboard, both surpassing the competition baselines and ranking in the top-10.", "conclusion": "The proposed method demonstrated the effectiveness of Conformer-based decoders for raw MEG signals and achieved strong performance in the competition."}}
{"id": "2512.00473", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.00473", "abs": "https://arxiv.org/abs/2512.00473", "authors": ["Junyan Ye", "Leiqi Zhu", "Yuncheng Guo", "Dongzhi Jiang", "Zilong Huang", "Yifan Zhang", "Zhiyuan Yan", "Haohuan Fu", "Conghui He", "Weijia Li"], "title": "RealGen: Photorealistic Text-to-Image Generation via Detector-Guided Rewards", "comment": null, "summary": "With the continuous advancement of image generation technology, advanced models such as GPT-Image-1 and Qwen-Image have achieved remarkable text-to-image consistency and world knowledge However, these models still fall short in photorealistic image generation. Even on simple T2I tasks, they tend to produce \" fake\" images with distinct AI artifacts, often characterized by \"overly smooth skin\" and \"oily facial sheens\". To recapture the original goal of \"indistinguishable-from-reality\" generation, we propose RealGen, a photorealistic text-to-image framework. RealGen integrates an LLM component for prompt optimization and a diffusion model for realistic image generation. Inspired by adversarial generation, RealGen introduces a \"Detector Reward\" mechanism, which quantifies artifacts and assesses realism using both semantic-level and feature-level synthetic image detectors. We leverage this reward signal with the GRPO algorithm to optimize the entire generation pipeline, significantly enhancing image realism and detail. Furthermore, we propose RealBench, an automated evaluation benchmark employing Detector-Scoring and Arena-Scoring. It enables human-free photorealism assessment, yielding results that are more accurate and aligned with real user experience. Experiments demonstrate that RealGen significantly outperforms general models like GPT-Image-1 and Qwen-Image, as well as specialized photorealistic models like FLUX-Krea, in terms of realism, detail, and aesthetics. The code is available at https://github.com/yejy53/RealGen.", "AI": {"tldr": "The paper introduces RealGen, a text-to-image framework enhancing photorealism by integrating an LLM for prompt optimization and a diffusion model, along with a 'Detector Reward' mechanism to quantify and reduce AI artifacts.", "motivation": "To improve photorealistic image generation as existing models often produce images with AI artifacts, RealGen addresses the issue by integrating advanced methods and mechanisms.", "method": "RealGen uses an LLM for prompt optimization and a diffusion model for realistic image generation, incorporating a 'Detector Reward' mechanism to evaluate and minimize AI artifacts.", "result": "Experiments show that RealGen surpasses models like GPT-Image-1, Qwen-Image, and FLUX-Krea in photorealistic generation and detail.", "conclusion": "RealGen, equipped with its unique 'Detector Reward' mechanism and RealBench evaluation, effectively enhances photorealism in AI-generated images, marking a significant advancement in text-to-image generation technology."}}
{"id": "2512.01460", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.01460", "abs": "https://arxiv.org/abs/2512.01460", "authors": ["Jozef Kubík", "Marek Šuppa", "Martin Takáč"], "title": "Enhancing BERT Fine-Tuning for Sentiment Analysis in Lower-Resourced Languages", "comment": null, "summary": "Limited data for low-resource languages typically yield weaker language models (LMs). Since pre-training is compute-intensive, it is more pragmatic to target improvements during fine-tuning. In this work, we examine the use of Active Learning (AL) methods augmented by structured data selection strategies which we term 'Active Learning schedulers', to boost the fine-tuning process with a limited amount of training data. We connect the AL to data clustering and propose an integrated fine-tuning pipeline that systematically combines AL, clustering, and dynamic data selection schedulers to enhance model's performance. Experiments in the Slovak, Maltese, Icelandic and Turkish languages show that the use of clustering during the fine-tuning phase together with AL scheduling can simultaneously produce annotation savings up to 30% and performance improvements up to four F1 score points, while also providing better fine-tuning stability.", "AI": {"tldr": "通过主动学习方法结合结构化数据选择策略（即'主动学习调度器'）以增强低资源语言模型的微调过程，实现更高的模型性能、更高的标注效率和更好的微调稳定性。此项研究在斯洛伐克语、马耳他语、冰岛语和土耳其语上展示了显著效果。", "motivation": "针对低资源语言通常由于训练数据不足而导致语言模型性能较差的问题，提出了通过主动学习和数据调度策略优化微调过程的方法。", "method": "连接主动学习和数据聚类，提出一个集成微调流程，系统结合主动学习、聚类及动态数据选择调度器以提升模型性能。", "result": "研究表明，在斯洛伐克语、马耳他语、冰岛语和土耳其语上，通过在微调期间使用聚类和主动学习调度策略可以同时实现高达30%的标注节省和最高提高四个F1评分点，同时也大幅提升了微调稳定性。", "conclusion": "该研究证实了聚类和主动学习调度策略结合可以显著改善低资源语言模型的微调效果，提高了模型性能，减少了标注所需数据量，同时也提升了微调过程的稳定性。"}}
{"id": "2512.00475", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.00475", "abs": "https://arxiv.org/abs/2512.00475", "authors": ["Xin Gu", "Congcong Li", "Xinyao Wang", "Dexiang Hong", "Libo Zhang", "Tiejian Luo", "Longyin Wen", "Heng Fan"], "title": "Structured Context Learning for Generic Event Boundary Detection", "comment": null, "summary": "Generic Event Boundary Detection (GEBD) aims to identify moments in videos that humans perceive as event boundaries. This paper proposes a novel method for addressing this task, called Structured Context Learning, which introduces the Structured Partition of Sequence (SPoS) to provide a structured context for learning temporal information. Our approach is end-to-end trainable and flexible, not restricted to specific temporal models like GRU, LSTM, and Transformers. This flexibility enables our method to achieve a better speed-accuracy trade-off. Specifically, we apply SPoS to partition the input frame sequence and provide a structured context for the subsequent temporal model. Notably, SPoS's overall computational complexity is linear with respect to the video length. We next calculate group similarities to capture differences between frames, and a lightweight fully convolutional network is utilized to determine the event boundaries based on the grouped similarity maps. To remedy the ambiguities of boundary annotations, we adapt the Gaussian kernel to preprocess the ground-truth event boundaries. Our proposed method has been extensively evaluated on the challenging Kinetics-GEBD, TAPOS, and shot transition detection datasets, demonstrating its superiority over existing state-of-the-art methods.", "AI": {"tldr": "本文提出一种处理视频事件边界检测的新方法，名为结构化上下文学习，利用SPoS进行序列结构化划分，提高了灵活性和性能。", "motivation": "GEBD旨在识别人类所感知的视频事件边界。由于现有方法可能局限在使用特定时间模型，本研究开发了一种灵活的、端到端可训练的方法。", "method": "本研究提出了名为结构化上下文学习（Structured Context Learning）的方法，通过引入序列结构化划分（SPoS）来为学习时间信息提供结构化的上下文。该方法是端到端可训练的，并且不局限于特定的时间模型，如GRU、LSTM和Transformer。SPoS用于对输入帧序列进行划分，并为后续的时间模型提供结构化的上下文。", "result": "在Kinetics-GEBD、TAPOS和镜头转换检测数据集上进行了广泛的测试，结果证明该方法在现有的最先进方法中表现最佳。", "conclusion": "实验在多个基准数据集上展示了所提出方法的优越性，提供了更好的速度-精度权衡，并有效地处理了事件边界标注中的模糊问题。"}}
{"id": "2512.01512", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.01512", "abs": "https://arxiv.org/abs/2512.01512", "authors": ["Yexing Du", "Kaiyuan Liu", "Youcheng Pan", "Bo Yang", "Keqi Deng", "Xie Chen", "Yang Xiang", "Ming Liu", "Bin Qin", "YaoWei Wang"], "title": "MCAT: Scaling Many-to-Many Speech-to-Text Translation with MLLMs to 70 Languages", "comment": null, "summary": "Multimodal Large Language Models (MLLMs) have achieved great success in Speech-to-Text Translation (S2TT) tasks. However, current research is constrained by two key challenges: language coverage and efficiency. Most of the popular S2TT datasets are substantially English-centric, which restricts the scaling-up of MLLMs' many-to-many translation capabilities. Moreover, the inference speed of MLLMs degrades dramatically when the speech is converted into long sequences (e.g., 750 tokens). To address these limitations, we propose a Multilingual Cost-effective Accelerated Speech-to-Text Translator (MCAT) framework, which includes two innovations. First, a language scaling method that leverages curriculum learning and a data balancing strategy is introduced to extend the language coverage supported by MLLMs to 70 languages and achieve mutual translation among these languages. Second, an optimized speech adapter module is designed to reduce the length of the speech sequence to only 30 tokens. Extensive experiments were conducted on MLLMs of different scales (9B and 27B). The experimental results demonstrate that MCAT not only surpasses state-of-the-art end-to-end models on the FLEURS dataset across 70x69 directions but also enhances batch inference efficiency. This is achieved with only ~100M trainable parameters and by using only 10 hours of S2TT data per language. Furthermore, we have released MCAT as open-source to promote the development of MLLMs for robust S2TT capabilities. The code and models are released at https://github.com/yxduir/m2m-70.", "AI": {"tldr": "本文提出了MCAT框架，解决了多模态大型语言模型在Speech-to-Text翻译中语言覆盖不足和效率低下的问题。通过课程学习和数据平衡策略扩展了语言覆盖至70种语言，并设计了优化的语音适配器模块，减少了语音序列长度，提升了模型效率。实验表明，MCAT在FLEURS数据集上超过现有模型，并且仅用了100M的可训练参数和少量数据。", "motivation": "当前多模态语言模型在Speech-to-Text翻译任务中受限于语言覆盖不足和效率低下的问题，作者提出MCAT框架来解决这些问题。", "method": "作者提出了MCAT框架，包括了课程学习和数据平衡策略以扩大语言覆盖，设计了优化的语音适配器模块以减少序列长度。", "result": "实验结果显示MCAT在FLEURS数据集上超越了现有的模型，并提高了批量推理效率，而且只用了100M的可训练参数和少量数据。", "conclusion": "MCAT成功解决了多模态语言模型在Speech-to-Text翻译中语言覆盖和效率的问题，并作为开源项目发布以促进未来的发展。"}}
{"id": "2512.00489", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.00489", "abs": "https://arxiv.org/abs/2512.00489", "authors": ["Jingyu Guo", "Emir Konuk", "Fredrik Strand", "Christos Matsoukas", "Kevin Smith"], "title": "Learning What Helps: Task-Aligned Context Selection for Vision Tasks", "comment": null, "summary": "Humans often resolve visual uncertainty by comparing an image with relevant examples, but ViTs lack the ability to identify which examples would improve their predictions. We present Task-Aligned Context Selection (TACS), a framework that learns to select paired examples which truly improve task performance rather than those that merely appear similar. TACS jointly trains a selector network with the task model through a hybrid optimization scheme combining gradient-based supervision and reinforcement learning, making retrieval part of the learning objective. By aligning selection with task rewards, TACS enables discriminative models to discover which contextual examples genuinely help. Across 18 datasets covering fine-grained recognition, medical image classification, and medical image segmentation, TACS consistently outperforms similarity-based retrieval, particularly in challenging or data-limited settings.", "AI": {"tldr": "提出 TACS 框架，该框架能够学习选择真正有助于任务性能的上下文示例，优于基于相似度的检索方法。", "motivation": "解决视觉不确定性时，人类往往通过将图像与相关示例进行比较，但 ViTs 缺乏识别哪些示例会改善其预测的这种能力。", "method": "Task-Aligned Context Selection (TACS), 一个框架，它学习选择真正能提高任务表现的配对样例，而不是仅仅看起来相似的样例。TACS 通过结合基于梯度的监督和强化学习的混合优化方案，联合训练选择器网络和任务模型，使检索成为学习目标的一部分。", "result": "在 18 个涵盖细粒度识别、医学图像分类和医学图像分割的数据集上，TACS 一致优于基于相似度的检索方法，特别是在具有挑战性或数据量有限的环境中。", "conclusion": "通过将选择与任务奖励对齐，TACS 能够使辨别模型发现哪些上下文示例真正有助于性能的提升。"}}
{"id": "2512.01557", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.01557", "abs": "https://arxiv.org/abs/2512.01557", "authors": ["Edward Ajayi", "Eudoxie Umwari", "Mawuli Deku", "Prosper Singadi", "Jules Udahemuka", "Bekalu Tadele", "Chukuemeka Edeh"], "title": "Language Diversity: Evaluating Language Usage and AI Performance on African Languages in Digital Spaces", "comment": null, "summary": "This study examines the digital representation of African languages and the challenges this presents for current language detection tools. We evaluate their performance on Yoruba, Kinyarwanda, and Amharic. While these languages are spoken by millions, their online usage on conversational platforms is often sparse, heavily influenced by English, and not representative of the authentic, monolingual conversations prevalent among native speakers. This lack of readily available authentic data online creates a challenge of scarcity of conversational data for training language models. To investigate this, data was collected from subreddits and local news sources for each language. The analysis showed a stark contrast between the two sources. Reddit data was minimal and characterized by heavy code-switching. Conversely, local news media offered a robust source of clean, monolingual language data, which also prompted more user engagement in the local language on the news publishers social media pages. Language detection models, including the specialized AfroLID and a general LLM, performed with near-perfect accuracy on the clean news data but struggled with the code-switched Reddit posts. The study concludes that professionally curated news content is a more reliable and effective source for training context-rich AI models for African languages than data from conversational platforms. It also highlights the need for future models that can process clean and code-switched text to improve the detection accuracy for African languages.", "AI": {"tldr": "本研究评估了语言检测工具在识别非洲语言的表现，发现新闻数据作为训练数据源更有效，强调了未来模型需要处理干净和混杂语言数据的能力。", "motivation": "研究的动机在于探讨非洲语言的数字化表示以及这对现有语言检测工具所构成的挑战，尤其是这些语言在线上交流中的代表性稀缺问题。", "method": "本研究收集了来自三个非洲语言（约鲁巴语、卢旺达语和阿姆哈拉语）的子论坛和当地新闻来源的数据，以评估现有语言检测工具在识别这些语言方面的表现。", "result": "研究结果表明新闻媒体提供了大量干净、单语言的数据，对于训练语言模型是有用的资源，而Reddit上的数据则有限且大部分为混杂语言。", "conclusion": "研究结论指出，专业编辑的新闻内容是训练富含上下文的AI模型的更可靠来源，但仍需要开发能够处理干净及混杂语言数据的未来模型以提高准确性。"}}
{"id": "2512.00493", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.00493", "abs": "https://arxiv.org/abs/2512.00493", "authors": ["Boshi Tang", "Henry Zheng", "Rui Huang", "Gao Huang"], "title": "CC-FMO: Camera-Conditioned Zero-Shot Single Image to 3D Scene Generation with Foundation Model Orchestration", "comment": null, "summary": "High-quality 3D scene generation from a single image is crucial for AR/VR and embodied AI applications. Early approaches struggle to generalize due to reliance on specialized models trained on curated small datasets. While recent advancements in large-scale 3D foundation models have significantly enhanced instance-level generation, coherent scene generation remains a challenge, where performance is limited by inaccurate per-object pose estimations and spatial inconsistency. To this end, this paper introduces CC-FMO, a zero-shot, camera-conditioned pipeline for single-image to 3D scene generation that jointly conforms to the object layout in input image and preserves instance fidelity. CC-FMO employs a hybrid instance generator that combines semantics-aware vector-set representation with detail-rich structured latent representation, yielding object geometries that are both semantically plausible and high-quality. Furthermore, CC-FMO enables the application of foundational pose estimation models in the scene generation task via a simple yet effective camera-conditioned scale-solving algorithm, to enforce scene-level coherence. Extensive experiments demonstrate that CC-FMO consistently generates high-fidelity camera-aligned compositional scenes, outperforming all state-of-the-art methods.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2512.01603", "categories": ["cs.CL", "cs.MM"], "pdf": "https://arxiv.org/pdf/2512.01603", "abs": "https://arxiv.org/abs/2512.01603", "authors": ["Yuezhang Peng", "Chonghao Cai", "Ziang Liu", "Shuai Fan", "Sheng Jiang", "Hua Xu", "Yuxin Liu", "Qiguang Chen", "Kele Xu", "Yao Li", "Sheng Wang", "Libo Qin", "Xie Chen"], "title": "MAC-SLU: Multi-Intent Automotive Cabin Spoken Language Understanding Benchmark", "comment": null, "summary": "Spoken Language Understanding (SLU), which aims to extract user semantics to execute downstream tasks, is a crucial component of task-oriented dialog systems. Existing SLU datasets generally lack sufficient diversity and complexity, and there is an absence of a unified benchmark for the latest Large Language Models (LLMs) and Large Audio Language Models (LALMs). This work introduces MAC-SLU, a novel Multi-Intent Automotive Cabin Spoken Language Understanding Dataset, which increases the difficulty of the SLU task by incorporating authentic and complex multi-intent data. Based on MAC-SLU, we conducted a comprehensive benchmark of leading open-source LLMs and LALMs, covering methods like in-context learning, supervised fine-tuning (SFT), and end-to-end (E2E) and pipeline paradigms. Our experiments show that while LLMs and LALMs have the potential to complete SLU tasks through in-context learning, their performance still lags significantly behind SFT. Meanwhile, E2E LALMs demonstrate performance comparable to pipeline approaches and effectively avoid error propagation from speech recognition. Code\\footnote{https://github.com/Gatsby-web/MAC\\_SLU} and datasets\\footnote{huggingface.co/datasets/Gatsby1984/MAC\\_SLU} are released publicly.", "AI": {"tldr": "引入MAC-SLU数据集并对其进行了不同的方法测试，结果显示尽管大语言模型和大型音频语言模型能够通过上下文学习完成SLU任务，但其性能仍远低于监督微调。同时，端到端的大型音频语言模型表现出可与流水线方法媲美的性能，并能有效避免语音识别中的错误传播。", "motivation": "现有的SLU数据集普遍缺乏足够的多样性和复杂性，缺少针对最新大语言模型和大型音频语言模型的统一基准。旨在增加SLU任务的难度并提供一个用于最新语言模型的基准测试。", "method": "介绍了一个新的数据集MAC-SLU，该数据集提高了SLU任务的难度，通过加入真实且复杂的多意图数据。基于MAC-SLU，对领先的开源大语言模型和大型音频语言模型进行了全面的基准测试，涵盖上下文学习、监督微调和端到端、流水线范式等方法。", "result": "实验表明，尽管大语言模型和大型音频语言模型能够通过上下文学习完成SLU任务，但它们的性能仍然显著落后于监督微调。此外，端到端的大型音频语言模型表现出可与流水线方法相媲美的性能，并能有效避免来自语音识别的错误传播。", "conclusion": "通过引入MAC-SLU数据集，作者提供了一个新的基准测试方法，这对于未来的SLU研究和发展提供了一条新的路径。值得注意的是，在新数据集上进行的实验显示，在不同的方法下性能表现存在差异，对未来的优化有一定的指导意义。代码和数据集已公开发布。"}}
{"id": "2512.00514", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.00514", "abs": "https://arxiv.org/abs/2512.00514", "authors": ["Tanaka Nobuaki"], "title": "Terrain Sensing with Smartphone Structured Light: 2D Dynamic Time Warping for Grid Pattern Matching", "comment": null, "summary": "Low-cost mobile rovers often operate on uneven terrain where small bumps or tilts are difficult to perceive visually but can significantly affect locomotion stability. To address this problem, we explore a smartphone-based structured-light system that projects a grid pattern onto the ground and reconstructs local terrain unevenness from a single handheld device. The system is inspired by face-recognition projectors, but adapted for ground sensing. A key technical challenge is robustly matching the projected grid with its deformed observation under perspective distortion and partial occlusion. Conventional one-dimensional dynamic time warping (1D-DTW) is not directly applicable to such two-dimensional grid patterns. We therefore propose a topology-constrained two-dimensional dynamic time warping (2D-DTW) algorithm that performs column-wise alignment under a global grid consistency constraint. The proposed method is designed to be simple enough to run on resource limited platforms while preserving the grid structure required for accurate triangulation. We demonstrate that our 2D-DTW formulation can be used not only for terrain sensing but also as a general tool for matching structured grid patterns in image processing scenarios. This paper describes the overall system design as well as the 2D-DTW extension that emerged from this application.", "AI": {"tldr": "研究提出了一种基于智能手机的结构光系统，用于解决低成本移动机器人在不平坦地形上的运动稳定性问题。系统通过拓扑约束的二维动态时间规整算法（2D-DTW）解决了网格模式匹配的挑战，并展示了该方法在图像处理中的通用性。", "motivation": "移动机器人在不平坦地形上运行时，小的障碍或倾斜很难被视觉识别，但会对运动稳定性产生显著影响。因此，需要一种简单而有效的方法来感知和处理这种地形不平。", "method": "研究提出了一个基于智能手机的结构光系统，该系统可以发射网格图案到地面上，并重建局部地形不平。通过一种新的2D-DTW算法处理网格模式下的匹配，从而解决了在透视畸变和部分遮挡下网格和其变形观测的匹配问题。", "result": "所提出的方法简单且能在资源有限的平台上运行，同时保持了用于精确三角测量所需的网格结构。该研究还展示这种2D-DTW方式可以作为图像处理中匹配网格图案的通用工具。", "conclusion": "研究成功地提出了一个手持设备的结构光系统，以改善机器人动态机敏能力和运动稳定性。2D-DTW的创新提供了一种新的图像处理工具，展示了利用资源有限设备解决方案的潜力。"}}
{"id": "2512.01661", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.01661", "abs": "https://arxiv.org/abs/2512.01661", "authors": ["Dengyun Peng", "Qiguang Chen", "Bofei Liu", "Jiannan Guan", "Libo Qin", "Zheng Yan", "Jinhao Liu", "Jianshu Zhang", "Wanxiang Che"], "title": "Learning the Boundary of Solvability: Aligning LLMs to Detect Unsolvable Problems", "comment": "preprint", "summary": "Ensuring LLM reliability requires not only solving complex problems but also recognizing when a problem is unsolvable. Current models often struggle to distinguish objective unsolvability (inherent contradictions in the problem) from subjective capability limitations (problems beyond the model's competence), which leads to hallucinations and overconfidence. To address this, we propose UnsolvableQA and UnsolvableRL to solve feasible problems, detect inherent contradictions, and prudently refuse tasks beyond capability. Specifically, we construct UnsolvableQA, a dataset of paired solvable and unsolvable instances derived via a dual-track methodology: programmatic generation for logic puzzles and a novel \"Reverse Construction\" method that injects contradictions into valid reasoning chains for mathematics. Building on this dataset, we introduce UnsolvableRL, a reinforcement learning framework with three reward components jointly accounting for accuracy, unsolvability, and difficulty. Empirical results show that our approach achieves near-perfect unsolvability detection while also improving accuracy on solvable tasks. Crucially, we identify Capability Collapse, demonstrating that explicit exposure to unsolvable data is indispensable for preventing models from becoming systematically overconfident. Our code and data are available at https://github.com/sfasfaffa/unsolvableQA.", "AI": {"tldr": "本文提出了UnsolvableQA数据集和UnsolvableRL框架，以提高大语言模型在解决问题时的能力，同时准确识别不可解问题，避免过度自信。", "motivation": "当前的LLM模型难以区分客观不可解性（问题本身存在的内在矛盾）和主观能力限制（超出模型能力范围的问题），这导致了幻觉和过度自信。", "method": "我们提出了UnsolvableQA和UnsolvableRL，用于解决可解决问题、检测内在矛盾并谨慎拒绝超出能力范围的任务。特别地，UnsolvableQA数据集通过编程生成逻辑谜题和一种新的\"逆向构建\"方法来制造内在矛盾，后者将矛盾注入有效的推理链中以构建数学问题。UnsolvableRL是一个强化学习框架，包含三个奖励组成部分，分别对应准确性、不可解性和难度。", "result": "实验结果显示，我们的方法在检测不可解性方面接近完美，同时提高了可解决问题的准确性。关键在于，我们确定了能力崩溃现象，表明显式暴露于不可解数据对于防止模型系统性过度自信至关重要。", "conclusion": "我们的方法不仅能够提高不可解性检测的准确率，同时也能提高对可解决问题的处理能力，避免了模型在不可解任务上的过度自信。"}}
{"id": "2512.00532", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2512.00532", "abs": "https://arxiv.org/abs/2512.00532", "authors": ["Ye Pang"], "title": "Image Generation as a Visual Planner for Robotic Manipulation", "comment": "11 pages 9 figures Under review at CVPR 2026", "summary": "Generating realistic robotic manipulation videos is an important step toward unifying perception, planning, and action in embodied agents. While existing video diffusion models require large domain-specific datasets and struggle to generalize, recent image generation models trained on language-image corpora exhibit strong compositionality, including the ability to synthesize temporally coherent grid images. This suggests a latent capacity for video-like generation even without explicit temporal modeling.\n  We explore whether such models can serve as visual planners for robots when lightly adapted using LoRA finetuning. We propose a two-part framework that includes: (1) text-conditioned generation, which uses a language instruction and the first frame, and (2) trajectory-conditioned generation, which uses a 2D trajectory overlay and the same initial frame. Experiments on the Jaco Play dataset, Bridge V2, and the RT1 dataset show that both modes produce smooth, coherent robot videos aligned with their respective conditions.\n  Our findings indicate that pretrained image generators encode transferable temporal priors and can function as video-like robotic planners under minimal supervision. Code is released at \\href{https://github.com/pangye202264690373/Image-Generation-as-a-Visual-Planner-for-Robotic-Manipulation}{https://github.com/pangye202264690373/Image-Generation-as-a-Visual-Planner-for-Robotic-Manipulation}.", "AI": {"tldr": "The paper shows that by finetuning pretrained image generators with LoRA, smooth and coherent robot manipulation videos can be generated under text or trajectory conditions, indicating the potential of these models as visual planners for robots without extensive training.", "motivation": "The research is motivated by the need for realistic robotic manipulation video generation to unify perception, planning, and action in robots. Existing methods require large domain-specific datasets and have poor generalization capabilities, which this paper aims to improve.", "method": "The paper proposes a two-part framework for generating robot manipulation videos using pretrained image generators with LoRA finetuning. The first part generates videos based on text instructions and the initial frame, while the second part generates videos based on a 2D trajectory and the initial frame.", "result": "Experiments on different datasets (Jaco Play, Bridge V2, and the RT1 dataset) show that the proposed framework can produce smooth and coherent robot videos aligned with their conditions under minimal supervision.", "conclusion": "The conclusion suggests that pretrained image generators with minor adaptation can serve as effective video-like robotic planners, capable of generating temporally coherent robot manipulation videos."}}
{"id": "2512.01710", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2512.01710", "abs": "https://arxiv.org/abs/2512.01710", "authors": ["Stefano Zeppieri"], "title": "MMAG: Mixed Memory-Augmented Generation for Large Language Models Applications", "comment": null, "summary": "Large Language Models (LLMs) excel at generating coherent text within a single prompt but fall short in sustaining relevance, personalization, and continuity across extended interactions. Human communication, however, relies on multiple forms of memory, from recalling past conversations to adapting to personal traits and situational context. This paper introduces the Mixed Memory-Augmented Generation (MMAG) pattern, a framework that organizes memory for LLM-based agents into five interacting layers: conversational, long-term user, episodic and event-linked, sensory and context-aware, and short-term working memory. Drawing inspiration from cognitive psychology, we map these layers to technical components and outline strategies for coordination, prioritization, and conflict resolution. We demonstrate the approach through its implementation in the Heero conversational agent, where encrypted long-term bios and conversational history already improve engagement and retention. We further discuss implementation concerns around storage, retrieval, privacy, and latency, and highlight open challenges. MMAG provides a foundation for building memory-rich language agents that are more coherent, proactive, and aligned with human needs.", "AI": {"tldr": "论文提出MMAG模式，改善语言模型在长时间交互中的表现，让其更加连贯、个性化，且符合人类需求。", "motivation": "大语言模型在单次提示内生成连贯文本方面表现出色，但在长时间交互中维持相关性、个性化和连贯性方面却力不从心。该论文旨在完善这方面的能力，使语言模型的交互更符合人类沟通的方式。", "method": "该论文提出了一种混合记忆增强生成（MMAG）模式，它将记忆组织成五个交互层：对话记忆、长期用户记忆、情景和事件关联记忆、感官和环境感知记忆以及短期工作记忆，并描述了这些记忆层的技术实现和协调策略。", "result": "该论文通过将其实施在名为Heero的对话代理中来演示其方法，在此代理中，加密的长期生物数据和对话历史记录已经提高了用户参与度和留存率。", "conclusion": "MMAG提供了一个构架，用于构建富记忆的语言代理，这些代理更加连贯、有主动性，并且能够更好地满足人类需求。"}}
{"id": "2512.00534", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.00534", "abs": "https://arxiv.org/abs/2512.00534", "authors": ["Zeyuan An", "Yanghang Xiao", "Zhiying Leng", "Frederick W. B. Li", "Xiaohui Liang"], "title": "Cross-Temporal 3D Gaussian Splatting for Sparse-View Guided Scene Update", "comment": "AAAI2026 accepted", "summary": "Maintaining consistent 3D scene representations over time is a significant challenge in computer vision. Updating 3D scenes from sparse-view observations is crucial for various real-world applications, including urban planning, disaster assessment, and historical site preservation, where dense scans are often unavailable or impractical. In this paper, we propose Cross-Temporal 3D Gaussian Splatting (Cross-Temporal 3DGS), a novel framework for efficiently reconstructing and updating 3D scenes across different time periods, using sparse images and previously captured scene priors. Our approach comprises three stages: 1) Cross-temporal camera alignment for estimating and aligning camera poses across different timestamps; 2) Interference-based confidence initialization to identify unchanged regions between timestamps, thereby guiding updates; and 3) Progressive cross-temporal optimization, which iteratively integrates historical prior information into the 3D scene to enhance reconstruction quality. Our method supports non-continuous capture, enabling not only updates using new sparse views to refine existing scenes, but also recovering past scenes from limited data with the help of current captures. Furthermore, we demonstrate the potential of this approach to achieve temporal changes using only sparse images, which can later be reconstructed into detailed 3D representations as needed. Experimental results show significant improvements over baseline methods in reconstruction quality and data efficiency, making this approach a promising solution for scene versioning, cross-temporal digital twins, and long-term spatial documentation.", "AI": {"tldr": "本文提出了Cross-Temporal 3D Gaussian Splatting，一种新框架，用于跨不同时段高效地重建和更新3D场景，利用稀疏图像和先前捕获的场景先验。", "motivation": "维持3D场景表示的一致性是计算机视觉中的一个重要挑战。从稀疏视角更新3D场景对于包括城市规划、灾害评估和历史遗址保护在内的各种实际应用来说至关重要，而在这些应用中，稠密扫描通常是不可用或不切实际的。", "method": "我们的方法包括三个阶段：1) 跨时间相机对齐，用于估计并跨不同时段对齐相机姿态；2) 干扰式置信初始化，以识别时间段之间的未改变区域，从而指导更新；3) 进行跨时间优化，迭代性地将历史先验信息整合到3D场景中，以提升重建质量。", "result": "实验结果表明，我们的方法在重建质量和数据效率方面明显优于基线方法。", "conclusion": "这种方法对于场景版本控制、跨时间孪生体和长期空间文档记录是一种有前途的解决方案。"}}
{"id": "2512.01713", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.01713", "abs": "https://arxiv.org/abs/2512.01713", "authors": ["Tim Wientzek"], "title": "Self-Supervised Borrowing Detection on Multilingual Wordlists", "comment": "29 pages, 3 figures, 12 tables", "summary": "This paper presents a fully self-supervised approach to borrowing detection in multilingual wordlists. The method combines two sources of information: PMI similarities based on a global correspondence model and a lightweight contrastive component trained on phonetic feature vectors. It further includes an automatic procedure for selecting decision thresholds without requiring labeled data. Experiments on benchmark datasets show that PMI alone already improves over existing string similarity measures such as NED and SCA, and that the combined similarity performs on par with or better than supervised baselines. An ablation study highlights the importance of character encoding, temperature settings and augmentation strategies. The approach scales to datasets of different sizes, works without manual supervision and is provided with a command-line tool that allows researchers to conduct their own studies.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2512.00539", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.00539", "abs": "https://arxiv.org/abs/2512.00539", "authors": ["Yongkang Hu", "Yu Cheng", "Yushuo Zhang", "Yuan Xie", "Zhaoxia Yin"], "title": "SAIDO: Generalizable Detection of AI-Generated Images via Scene-Aware and Importance-Guided Dynamic Optimization in Continual Learning", "comment": "17 pages, 19 figures", "summary": "The widespread misuse of image generation technologies has raised security concerns, driving the development of AI-generated image detection methods. However, generalization has become a key challenge and open problem: existing approaches struggle to adapt to emerging generative methods and content types in real-world scenarios. To address this issue, we propose a Scene-Aware and Importance-Guided Dynamic Optimization detection framework with continual learning (SAIDO). Specifically, we design Scene-Awareness-Based Expert Module (SAEM) that dynamically identifies and incorporates new scenes using VLLMs. For each scene, independent expert modules are dynamically allocated, enabling the framework to capture scene-specific forgery features better and enhance cross-scene generalization. To mitigate catastrophic forgetting when learning from multiple image generative methods, we introduce Importance-Guided Dynamic Optimization Mechanism (IDOM), which optimizes each neuron through an importance-guided gradient projection strategy, thereby achieving an effective balance between model plasticity and stability. Extensive experiments on continual learning tasks demonstrate that our method outperforms the current SOTA method in both stability and plasticity, achieving 44.22\\% and 40.57\\% relative reductions in average detection error rate and forgetting rate, respectively. On open-world datasets, it improves the average detection accuracy by 9.47\\% compared to the current SOTA method.", "AI": {"tldr": "Proposes a new framework for detecting AI-generated images that adapts to new scenarios and maintains performance over time through continual learning.", "motivation": "To address the challenge of generalizing AI-generated image detection methods to new generative methods and content types that emerge in real-world scenarios.", "method": "SAIDO (Scene-Aware and Importance-Guided Dynamic Optimization) detection framework with continual learning, which includes the Scene-Awareness-Based Expert Module (SAEM) and the Importance-Guided Dynamic Optimization Mechanism (IDOM).", "result": "Outperforms current SOTA methods in both stability and plasticity, achieving significant relative reductions in average detection error rate and forgetting rate, and an improvement in average detection accuracy on open-world datasets.", "conclusion": "The proposed SAIDO framework demonstrates superior performance in detecting AI-generated images across various scenarios, showing promise for addressing the challenges of image generation technology misuse in real-world contexts."}}
{"id": "2512.01725", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.01725", "abs": "https://arxiv.org/abs/2512.01725", "authors": ["Jiannan Guan", "Qiguang Chen", "Libo Qin", "Dengyun Peng", "Jinhao Liu", "Liangyu Huo", "Jian Xie", "Wanxiang Che"], "title": "Beware of Reasoning Overconfidence: Pitfalls in the Reasoning Process for Multi-solution Tasks", "comment": null, "summary": "Large Language Models (LLMs) excel in reasoning tasks requiring a single correct answer, but they perform poorly in multi-solution tasks that require generating comprehensive and diverse answers. We attribute this limitation to \\textbf{reasoning overconfidence}: a tendency to express undue certainty in an incomplete solution set. To examine the effect, we introduce \\textit{MuSoBench}, a benchmark of multi-solution problems. Experiments show that the conventional short chain-of-thought (Short-CoT) prompting paradigm exhibits pronounced overconfidence, whereas the emerging long chain-of-thought (Long-CoT) approach mitigates it through iterative exploration and self-reflection. We further characterise observable behaviours and influential factors. To probe the underlying cause, we propose the \\textbf{cognitive-rigidity hypothesis}, which posits that overconfidence arises when the reasoning process prematurely converges on a narrow set of thought paths. An attention-entropy analysis offers preliminary support for this view. These findings provide tools for assessing the completeness of LLM reasoning and highlight the need to move evaluation beyond single-answer accuracy toward comprehensive exploration.", "AI": {"tldr": "研究揭示大型语言模型（LLMs）在处理多解决方案任务时过度自信，并提出长链思维（Long-CoT）和认知僵化假说作为改进方法。", "motivation": "解决大型语言模型（LLMs）在多解决方案任务中生成全面而多样化答案的能力不足的问题。", "method": "通过对多解决方案任务中的表现进行实验分析，并提出认知僵化假说来解释大型语言模型（LLMs）在多解决方案任务中过度自信的问题。", "result": "实验证明短链思维（Short-CoT）表现出明显的过度自信，而长链思维（Long-CoT）通过迭代探索和自我反思减轻了这个问题。", "conclusion": "这项研究提供了评估LLMs推理完整性的工具，并强调了评价不能仅仅基于单一答案的准确性。"}}
{"id": "2512.00547", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.00547", "abs": "https://arxiv.org/abs/2512.00547", "authors": ["Sandika Biswas", "Qianyi Wu", "Biplab Banerjee", "Hamid Rezatofighi"], "title": "Asset-Driven Sematic Reconstruction of Dynamic Scene with Multi-Human-Object Interactions", "comment": null, "summary": "Real-world human-built environments are highly dynamic, involving multiple humans and their complex interactions with surrounding objects. While 3D geometry modeling of such scenes is crucial for applications like AR/VR, gaming, and embodied AI, it remains underexplored due to challenges like diverse motion patterns and frequent occlusions. Beyond novel view rendering, 3D Gaussian Splatting (GS) has demonstrated remarkable progress in producing detailed, high-quality surface geometry with fast optimization of the underlying structure. However, very few GS-based methods address multihuman, multiobject scenarios, primarily due to the above-mentioned inherent challenges. In a monocular setup, these challenges are further amplified, as maintaining structural consistency under severe occlusion becomes difficult when the scene is optimized solely based on GS-based rendering loss. To tackle the challenges of such a multihuman, multiobject dynamic scene, we propose a hybrid approach that effectively combines the advantages of 1) 3D generative models for generating high-fidelity meshes of the scene elements, 2) Semantic-aware deformation, \\ie rigid transformation of the rigid objects and LBS-based deformation of the humans, and mapping of the deformed high-fidelity meshes in the dynamic scene, and 3) GS-based optimization of the individual elements for further refining their alignments in the scene. Such a hybrid approach helps maintain the object structures even under severe occlusion and can produce multiview and temporally consistent geometry. We choose HOI-M3 for evaluation, as, to the best of our knowledge, this is the only dataset featuring multihuman, multiobject interactions in a dynamic scene. Our method outperforms the state-of-the-art method in producing better surface reconstruction of such scenes.", "AI": {"tldr": "本文提出一种结合3D生成模型、语义感知变形和3D高斯扩散优化的混合方法，用于解决多角色多物体动态场景下的3D几何建模难题。这种方法在保持物体结构一致性上表现优异，尤其适用于存在严重遮挡情形。", "motivation": "现实世界的人造环境通常充满动态变化，涉及到多个角色和复杂交互。现有的3D建模方法难以有效处理这种复杂的场景，尤其是在存在多种运动模式和频繁遮挡的情况下。", "method": "本文提出一种混合策略，它结合了3D生成模型来创建高保真场景元素网格，语义感知变形进行柔性变换和刚体变换，以及基于3D高斯扩散的优化来进一步改善场景中各个元素的排列。", "result": "在HOI-M3数据集上的评估显示，本文方法优于最先进方法，能够更佳地重建多角色多物体动态场景的表面几何。", "conclusion": "这种混合方法能有效处理多角色多物体动态场景下的3D几何重建问题，即使是在存在严重遮挡的情况下，也能够确保物体结构的一致性和多视角的一致性。"}}
{"id": "2512.01728", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.01728", "abs": "https://arxiv.org/abs/2512.01728", "authors": ["Zhengjia Wang", "Danding Wang", "Qiang Sheng", "Jiaying Wu", "Juan Cao"], "title": "Reasoning About the Unsaid: Misinformation Detection with Omission-Aware Graph Inference", "comment": "AAAI 2026", "summary": "This paper investigates the detection of misinformation, which deceives readers by explicitly fabricating misleading content or implicitly omitting important information necessary for informed judgment. While the former has been extensively studied, omission-based deception remains largely overlooked, even though it can subtly guide readers toward false conclusions under the illusion of completeness. To pioneer in this direction, this paper presents OmiGraph, the first omission-aware framework for misinformation detection. Specifically, OmiGraph constructs an omission-aware graph for the target news by utilizing a contextual environment that captures complementary perspectives of the same event, thereby surfacing potentially omitted contents. Based on this graph, omission-oriented relation modeling is then proposed to identify the internal contextual dependencies, as well as the dynamic omission intents, formulating a comprehensive omission relation representation. Finally, to extract omission patterns for detection, OmiGraph introduces omission-aware message-passing and aggregation that establishes holistic deception perception by integrating the omission contents and relations. Experiments show that, by considering the omission perspective, our approach attains remarkable performance, achieving average improvements of +5.4% F1 and +5.3% ACC on two large-scale benchmarks.", "AI": {"tldr": "本文提出了OmiGraph，这是首个面向遗漏感知的误导信息检测框架，通过构建遗漏感知图和进行遗漏关系建模，在两个大规模基准测试中显著提升了误导信息检测的性能。", "motivation": "本文研究了误导信息的检测，特别是通过隐式遗漏重要信息来引导读者得出错误结论这一较少被研究的领域。由于这种欺骗可以微妙地引导读者在完整性幻觉下走向错误结论，因此有必要对其进行研究和检测。", "method": "OmiGraph方法构建了一个基于上下文环境的遗漏感知图，通过捕捉同一事件的互补视角来揭示可能被遗漏的内容。基于这个图，提出了面向遗漏的关系建模来识别内部上下文依赖和动态遗漏意图，形成全面的遗漏关系表示。最后，OmiGraph通过引入遗漏感知的消息传递和聚合来提取遗漏模式，实现全面的欺骗感知。", "result": "实验结果表明，通过考虑遗漏视角，本文的方法在两个大规模基准测试中取得了显著的性能提升，F1值和准确性分别平均提高了5.4%和5.3%。", "conclusion": "本文提出了OmiGraph，这是首个用于检测遗漏误导信息的框架，通过揭示遗漏内容并识别遗漏意图，实现对误导信息的全面建模和检测。实验验证了该方法的有效性。"}}
{"id": "2512.00557", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.00557", "abs": "https://arxiv.org/abs/2512.00557", "authors": ["Haomiao Chen", "Keith W Jamison", "Mert R. Sabuncu", "Amy Kuceyeski"], "title": "NeuroVolve: Evolving Visual Stimuli toward Programmable Neural Objectives", "comment": null, "summary": "What visual information is encoded in individual brain regions, and how do distributed patterns combine to create their neural representations? Prior work has used generative models to replicate known category selectivity in isolated regions (e.g., faces in FFA), but these approaches offer limited insight into how regions interact during complex, naturalistic vision. We introduce NeuroVolve, a generative framework that provides brain-guided image synthesis via optimization of a neural objective function in the embedding space of a pretrained vision-language model. Images are generated under the guidance of a programmable neural objective, i.e., activating or deactivating single regions or multiple regions together. NeuroVolve is validated by recovering known selectivity for individual brain regions, while expanding to synthesize coherent scenes that satisfy complex, multi-region constraints. By tracking optimization steps, it reveals semantic trajectories through embedding space, unifying brain-guided image editing and preferred stimulus generation in a single process. We show that NeuroVolve can generate both low-level and semantic feature-specific stimuli for single ROIs, as well as stimuli aligned to curated neural objectives. These include co-activation and decorrelation between regions, exposing cooperative and antagonistic tuning relationships. Notably, the framework captures subject-specific preferences, supporting personalized brain-driven synthesis and offering interpretable constraints for mapping, analyzing, and probing neural representations of visual information.", "AI": {"tldr": "NeuroVolve, a brain-guided image synthesis framework, generates images tailored to specific neural objectives, revealing cooperative and antagonistic relationships between brain regions and supporting personalized brain-driven synthesis.", "motivation": "To understand the interaction between brain regions during complex vision, as previous methods could only replicate known category selectivity in isolated regions.", "method": "We introduce NeuroVolve, a generative framework that creates images via optimization in the embedding space of a pretrained vision-language model, guided by a neural objective function. It can activate or deactivate individual or multiple brain regions.", "result": "NeuroVolve successfully recovers known selectivity for individual brain regions and extends to create coherent scenes that meet complex, multi-region constraints. It also uncovers cooperative and antagonistic relationships between brain regions.", "conclusion": "The framework provides a new approach to brain-guided image synthesis, enabling the generation of both low-level and semantic feature-specific stimuli, capturing subject-specific preferences for personalized brain-driven synthesis."}}
{"id": "2512.01822", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2512.01822", "abs": "https://arxiv.org/abs/2512.01822", "authors": ["Jintian Zhang", "Kewei Xu", "Jingsheng Zheng", "Zhuoyun Yu", "Yuqi Zhu", "Yujie Luo", "Lanning Wei", "Shuofei Qiao", "Lun Du", "Da Zheng", "Shumin Deng", "Huajun Chen", "Ningyu Zhang"], "title": "InnoGym: Benchmarking the Innovation Potential of AI Agents", "comment": "Work in progress", "summary": "LLMs and Agents have achieved impressive progress in code generation, mathematical reasoning, and scientific discovery. However, existing benchmarks primarily measure correctness, overlooking the diversity of methods behind solutions. True innovation depends not only on producing correct answers but also on the originality of the approach. We present InnoGym, the first benchmark and framework designed to systematically evaluate the innovation potential of AI agents. InnoGym introduces two complementary metrics: performance gain, which measures improvement over the best-known solutions, and novelty, which captures methodological differences from prior approaches. The benchmark includes 18 carefully curated tasks from real-world engineering and scientific domains, each standardized through resource filtering, evaluator validation, and solution collection. In addition, we provide iGym, a unified execution environment for reproducible and long-horizon evaluations. Extensive experiments show that while some agents produce novel approaches, their lack of robustness limits performance gains. These results highlight a key gap between creativity and effectiveness, underscoring the need for benchmarks that evaluate both.", "AI": {"tldr": "引出InnoGym，一个用于评估AI代理创新潜力的新基准，包括性能提升和新颖性两个指标，并提出了iGym用于执行环境。", "motivation": "虽然现有的基准主要衡量正确性，但它们忽视了解决方案背后的多样性。真正的创新不仅依赖于产生正确的答案，还依赖于方法的原创性。为了评估AI代理的创新潜力，我们开发了InnoGym。", "method": "我们提出了InnoGym，这是一个新的基准和框架，旨在系统地评估AI代理的创新潜力。InnoGym包括两个补充指标：性能提升，用于衡量与已知最佳解决方案的改进程度；新颖性，用于捕捉方法上的差异。此基准包含了18个精心策划的任务，涵盖了实际的工程和科学领域，每个任务都有标准化的过程，包括资源过滤、评估者验证和解决方案收集。此外，我们还提供了iGym，一个统一的执行环境，用于可重复的长期评估。", "result": "实验结果表明，一些代理虽然带来了新颖的方法，但其缺乏稳健性限制了性能提升的潜力，表明了创新和成效之间存在的重要差距。", "conclusion": "这些结果突显了创新和成效之间存在的关键差距，强调了需要评估两者兼备的新基准。"}}
{"id": "2512.00565", "categories": ["cs.CV", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2512.00565", "abs": "https://arxiv.org/abs/2512.00565", "authors": ["Nicolas Gorlo", "Lukas Schmid", "Luca Carlone"], "title": "Describe Anything Anywhere At Any Moment", "comment": "14 pages, 5 figures, 6 tables", "summary": "Computer vision and robotics applications ranging from augmented reality to robot autonomy in large-scale environments require spatio-temporal memory frameworks that capture both geometric structure for accurate language-grounding as well as semantic detail. Existing methods face a tradeoff, where producing rich open-vocabulary descriptions comes at the expense of real-time performance when these descriptions have to be grounded in 3D. To address these challenges, we propose Describe Anything, Anywhere, at Any Moment (DAAAM), a novel spatio-temporal memory framework for large-scale and real-time 4D scene understanding. DAAAM introduces a novel optimization-based frontend to infer detailed semantic descriptions from localized captioning models, such as the Describe Anything Model (DAM), leveraging batch processing to speed up inference by an order of magnitude for online processing. It leverages such semantic understanding to build a hierarchical 4D scene graph (SG), which acts as an effective globally spatially and temporally consistent memory representation. DAAAM constructs 4D SGs with detailed, geometrically grounded descriptions while maintaining real-time performance. We show that DAAAM's 4D SG interfaces well with a tool-calling agent for inference and reasoning.\n  We thoroughly evaluate DAAAM in the complex task of spatio-temporal question answering on the NaVQA benchmark and show its generalization capabilities for sequential task grounding on the SG3D benchmark. We further curate an extended OC-NaVQA benchmark for large-scale and long-time evaluations. DAAAM achieves state-of-the-art results in both tasks, improving OC-NaVQA question accuracy by 53.6%, position errors by 21.9%, temporal errors by 21.6%, and SG3D task grounding accuracy by 27.8% over the most competitive baselines, respectively. We release our data and code open-source.", "AI": {"tldr": "DAAAM framework improves spatio-temporal memory for computer vision and robotics by offering detailed, real-time descriptions in large environments.", "motivation": "To overcome the tradeoff between rich open-vocabulary descriptions and real-time performance in large-scale environments for applications such as augmented reality and robotics.", "method": "DAAAM, a novel spatio-temporal memory framework, optimizes a frontend to infer semantic descriptions and uses a hierarchical 4D scene graph to maintain real-time performance while providing detailed, geometrically grounded descriptions.", "result": "DAAAM demonstrates state-of-the-art performance in spatio-temporal question answering and sequential task grounding, with significant improvements over baselines.", "conclusion": "DAAAM provides an effective balance between real-time performance and detail, offering a practical solution for spatio-temporal memory in computer vision and robotics."}}
{"id": "2512.01848", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.01848", "abs": "https://arxiv.org/abs/2512.01848", "authors": ["Jinghan Jia", "Nathalie Baracaldo", "Sijia Liu"], "title": "Beyond SFT: Reinforcement Learning for Safer Large Reasoning Models with Better Reasoning Ability", "comment": null, "summary": "Large reasoning models (LRMs) extend large language models by generating explicit chain-of-thought (CoT) reasoning, significantly improving mathematical and logical problem solving. However, this explicit reasoning process also introduces new safety risks, as unsafe behaviors often emerge within intermediate reasoning trajectories, even when final answers appear harmless. Existing safety alignment approaches primarily rely on supervised fine-tuning (SFT) over safety-oriented long CoT datasets. While intuitive, we find that SFT produces inconsistent safety improvements, degrades reasoning ability, and generalizes poorly across model families. These limitations suggest that purely supervised approaches are insufficient for robust safety alignment in LRMs. To address this, we investigate reinforcement learning (RL) as a complementary optimization framework for LRM safety training. Unlike SFT, RL directly optimizes model policies with reward feedback, enabling more adaptive and stable alignment. Extensive experiments across multiple model families and benchmarks show that RL achieves stronger and more consistent safety gains while maintaining reasoning competence. Further analysis of reflection dynamics and token-level entropy reveals that RL suppresses unsafe exploratory reasoning while preserving reflective depth, leading to safer and more reliable reasoning processes.", "AI": {"tldr": "研究探讨了通过强化学习（RL）改进大型推理模型（LRMs）的安全性，指出强化学习比监督细调（SFT）更能稳定且一致地提升安全性，并保留推理能力。", "motivation": "大型推理模型在生成问题解决的推理链条时引入了新的安全风险。现有安全校准方法如监督细调不仅效果不一致，还会损害模型的推理能力。", "method": "Structure", "result": "{\n  \"tldr\": \"研究探讨了通过强化学习（RL）改进大型推理模型（LRMs）的安全性，指出强化学习比监督细调（SFT）更能稳定且一致地提升安全性，并保留推理能力。\", \n  \"motivation\": \"大型推理模型在生成问题解决的推理链条时引入了新的安全风险。现有安全校准方法如监督细调不仅效果不一致，还会损害模型的推理能力。\", \n  \"method\": \"对比研究了强化学习（RL）与监督细调（SFT）两种方法在改善大型推理模型安全性能上的效果。\", \n  \"result\": \"实验表明，强化学习方法相较于监督细调方法，在维持推理能力的同时取得了更强和更加一致的安全性能改进。\", \n  \"conclusion\": \"强化学习作为校准框架可以更好地抑制不安全的推理行为，同时也保持了模型的反思层次，从而确保安全性和可靠性。\"}\n", "conclusion": "强化学习作为校准框架可以更好地抑制不安全的推理行为，同时也保持了模型的反思层次，从而确保安全性和可靠性。"}}
{"id": "2512.00572", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.00572", "abs": "https://arxiv.org/abs/2512.00572", "authors": ["Mohammed Mohiuddin", "Syed Mohammod Minhaz Hossain", "Sumaiya Khanam", "Prionkar Barua", "Aparup Barua", "MD Tamim Hossain"], "title": "Integrating Skeleton Based Representations for Robust Yoga Pose Classification Using Deep Learning Models", "comment": null, "summary": "Yoga is a popular form of exercise worldwide due to its spiritual and physical health benefits, but incorrect postures can lead to injuries. Automated yoga pose classification has therefore gained importance to reduce reliance on expert practitioners. While human pose keypoint extraction models have shown high potential in action recognition, systematic benchmarking for yoga pose recognition remains limited, as prior works often focus solely on raw images or a single pose extraction model. In this study, we introduce a curated dataset, 'Yoga-16', which addresses limitations of existing datasets, and systematically evaluate three deep learning architectures (VGG16, ResNet50, and Xception) using three input modalities (direct images, MediaPipe Pose skeleton images, and YOLOv8 Pose skeleton images). Our experiments demonstrate that skeleton-based representations outperform raw image inputs, with the highest accuracy of 96.09% achieved by VGG16 with MediaPipe Pose skeleton input. Additionally, we provide interpretability analysis using Grad-CAM, offering insights into model decision-making for yoga pose classification with cross validation analysis.", "AI": {"tldr": "本研究建立了'Yoga-16'数据集，并通过多种深度学习模型和输入模态进行系统评估，证明了基于骨架的表示方法优于原始图像输入，其中VGG16模型结合MediaPipe Pose骨架输入达到了最高的96.09%的准确率。", "motivation": "该研究的动机是解决瑜伽姿势识别系统化基准测试的不足，并减少对专家的依赖，因为错误的瑜伽姿势会导致伤害，出于这个考虑，自动化的瑜伽姿势分类变得尤为重要。", "method": "本研究引入了一个精心策划的数据集'Yoga-16'，并通过三种深度学习架构（VGG16、ResNet50和Xception）和三种输入模态（直接图像、MediaPipe Pose骨架图像和YOLOv8 Pose骨架图像）对其进行了系统的评估。", "result": "实验表明基于骨架的表示方法优于原始图像输入，VGG16模型结合MediaPipe Pose骨架输入取得了最高的准确率96.09%，同时使用Grad-CAM提供了模型决策的解释性分析。", "conclusion": "研究得出的结论是，骨架图像作为输入模态比原始图像输入更有利于瑜伽姿势分类，而在骨架图像输入下VGG16模型是最有效的分类模型。"}}
{"id": "2512.01852", "categories": ["cs.CL", "cs.AI", "cs.ET"], "pdf": "https://arxiv.org/pdf/2512.01852", "abs": "https://arxiv.org/abs/2512.01852", "authors": ["Hrishikesh Terdalkar", "Kirtan Bhojani", "Aryan Dongare", "Omm Aditya Behera"], "title": "BHRAM-IL: A Benchmark for Hallucination Recognition and Assessment in Multiple Indian Languages", "comment": "Accepted at BHASHA Workshop @ IJCNLP/AACL 2025", "summary": "Large language models (LLMs) are increasingly deployed in multilingual applications but often generate plausible yet incorrect or misleading outputs, known as hallucinations. While hallucination detection has been studied extensively in English, under-resourced Indian languages remain largely unexplored. We present BHRAM-IL, a benchmark for hallucination recognition and assessment in multiple Indian languages, covering Hindi, Gujarati, Marathi, Odia, along with English. The benchmark comprises 36,047 curated questions across nine categories spanning factual, numerical, reasoning, and linguistic tasks. We evaluate 14 state-of-the-art multilingual LLMs on a benchmark subset of 10,265 questions, analyzing cross-lingual and factual hallucinations across languages, models, scales, categories, and domains using category-specific metrics normalized to (0,1) range. Aggregation over all categories and models yields a primary score of 0.23 and a language-corrected fuzzy score of 0.385, demonstrating the usefulness of BHRAM-IL for hallucination-focused evaluation. The dataset, and the code for generation and evaluation are available on GitHub (https://github.com/sambhashana/BHRAM-IL/) and HuggingFace (https://huggingface.co/datasets/sambhashana/BHRAM-IL/) to support future research in multilingual hallucination detection and mitigation.", "AI": {"tldr": "BHRAM-IL是识别和评估多种印度语言中幻觉的基准，其评估结果表明在多语言幻觉检测和缓解方面具有潜在的应用。", "motivation": "尽管幻觉检测在英语中得到了广泛研究，但资源不足的印度语言幻觉检测研究相对较少。我们的研究填补了这一空白，并强调了BHRAM-IL的实用价值用于幻觉评估。", "method": "我们提出BHRAM-IL，这是一个用于识别和评估多种印度语言中幻觉的基准，涵盖北印度语、古吉拉特语、马拉提语、奥里亚语以及英语。基准包括36,047个精心挑选的问题，涵盖九个类别的事实、数值、推理和语言任务。", "result": "评估了14个最先进的多语言语言模型，使用特定类别的度量标准在语言和模型规模、类别和领域上进行跨语言和事实性幻觉分析。汇总所有类别和模型得分为0.23和语言校正后的模糊分数为0.385。", "conclusion": "作为评估多语言幻觉检测和缓解的基准，BHRAM-IL在整个GitHub和HuggingFace上提供了数据和生成与评估的代码，支持未来的研发工作。"}}
{"id": "2512.00582", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.00582", "abs": "https://arxiv.org/abs/2512.00582", "authors": ["Yue Jiang", "Haiwei Xue", "Minghao Han", "Mingcheng Li", "Xiaolu Hou", "Dingkang Yang", "Lihua Zhang", "Xu Zheng"], "title": "SatireDecoder: Visual Cascaded Decoupling for Enhancing Satirical Image Comprehension", "comment": "Accepted by AAAI 2026", "summary": "Satire, a form of artistic expression combining humor with implicit critique, holds significant social value by illuminating societal issues. Despite its cultural and societal significance, satire comprehension, particularly in purely visual forms, remains a challenging task for current vision-language models. This task requires not only detecting satire but also deciphering its nuanced meaning and identifying the implicated entities. Existing models often fail to effectively integrate local entity relationships with global context, leading to misinterpretation, comprehension biases, and hallucinations. To address these limitations, we propose SatireDecoder, a training-free framework designed to enhance satirical image comprehension. Our approach proposes a multi-agent system performing visual cascaded decoupling to decompose images into fine-grained local and global semantic representations. In addition, we introduce a chain-of-thought reasoning strategy guided by uncertainty analysis, which breaks down the complex satire comprehension process into sequential subtasks with minimized uncertainty. Our method significantly improves interpretive accuracy while reducing hallucinations. Experimental results validate that SatireDecoder outperforms existing baselines in comprehending visual satire, offering a promising direction for vision-language reasoning in nuanced, high-level semantic tasks.", "AI": {"tldr": "本文介绍了一种无需训练的框架SatireDecoder，旨在提高对讽刺性图像的理解，它通过多代理系统进行视觉级联解耦，并采用逐步推理策略，以减少不确定性，提高理解准确性。实验表明，SatireDecoder优于现有基线模型，为视觉语言推理中的语义任务提供了新的研究方向。", "motivation": "讽刺作为一种结合幽默和隐含批评的艺术表达形式，在揭示社会问题方面具有重要的社会价值。然而，尤其是在纯视觉形式中，现有的视觉语言模型在理解讽刺时经常无法有效地将局部实体关系与全局语境结合，导致误解、理解偏差和幻觉现象。因此，提出SatireDecoder来解决这一挑战。", "method": "我们的方法名为SatireDecoder，是一个无需训练的框架，旨在提高对讽刺图像的理解能力。该方法提出了一种多代理系统，通过视觉级联解耦来将图像分解成细粒度的局部和全局语义表示。此外，我们引入了一种基于不确定性分析的逐步推理策略，可以将复杂的讽刺理解过程分解为具有最小不确定性的顺序子任务。", "result": "实验结果表明，我们的方法SatireDecoder在理解讽刺性图像方面超越了现有的基线模型，显著提高了理解和解释的准确性，同时减少了幻觉现象。", "conclusion": "我们的研究验证了SatireDecoder框架的优越性，它可以作为一个强大的工具来提高对复杂、高阶语义任务的理解。这为视觉语言推理的未来研究方向提供了有力的支持。"}}
{"id": "2512.01865", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.01865", "abs": "https://arxiv.org/abs/2512.01865", "authors": ["Adel Moumen", "Guangzhi Sun", "Philip C. Woodland"], "title": "Cross-Lingual Interleaving for Speech Language Models", "comment": null, "summary": "Spoken Language Models (SLMs) aim to learn linguistic competence directly from speech using discrete units, widening access to Natural Language Processing (NLP) technologies for languages with limited written resources. However, progress has been largely English-centric due to scarce spoken evaluation benchmarks and training data, making cross-lingual learning difficult. We present a cross-lingual interleaving method that mixes speech tokens across languages without textual supervision. We also release an EN-FR training dataset, TinyStories (~42k hours), together with EN-FR spoken StoryCloze and TopicCloze benchmarks for cross-lingual semantic evaluation, both synthetically generated using GPT-4. On 360M and 1B SLMs under matched training-token budgets, interleaving improves monolingual semantic accuracy, enables robust cross-lingual continuation, and strengthens cross-lingual hidden-state alignment. Taken together, these results indicate that cross-lingual interleaving is a simple, scalable route to building multilingual SLMs that understand and converse across languages. All resources will be made open-source to support reproducibility.", "AI": {"tldr": "我们提出了一种跨语言混合方法，使用没有文本监督的跨语言混合语音标记，以支持多种语言的SLMs开发。", "motivation": "SLMs旨在从语音中直接学习语言能力，使用离散单元，拓宽了对于具有有限书面资源的语言的NLP技术的访问。然而，由于缺乏语音评估基准和训练数据，这一进展大多以英语为中心，导致跨语言学习困难。", "method": "我们提出了一种跨语言混合方法，该方法可以在没有文本监督的情况下跨语言混合语音标记。我们还发布了一个EN-FR训练数据集TinyStories（约42000小时）以及EN-FR语音StoryCloze和TopicCloze基准，用于跨语言语义评估，这些基准都是使用GPT-4合成生成的。", "result": "在匹配的训练令牌预算下，对于3亿和10亿参数的SLMs，混合方法提高了单语语义准确性，使跨语言延续变得稳健，并加强了跨语言隐藏状态的对齐。", "conclusion": "总体来看，这些结果表明，跨语言混合是一种简单的、可扩展的方式来构建多语言的SLMs，使它们能够理解和跨越不同语言进行对话。所有资源都将作为开源发布以支持可重复性。"}}
{"id": "2512.00597", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.00597", "abs": "https://arxiv.org/abs/2512.00597", "authors": ["Thuraya Alzubaidi", "Farhad R. Nezami", "Muzammil Behzad"], "title": "Scaling Down to Scale Up: Towards Operationally-Efficient and Deployable Clinical Models via Cross-Modal Low-Rank Adaptation for Medical Vision-Language Models", "comment": null, "summary": "Foundation models trained via vision-language pretraining have demonstrated strong zero-shot capabilities across diverse image domains, yet their application to volumetric medical imaging remains limited. We introduce MedCT-VLM: Medical CT Vision-Language Model, a parameter-efficient vision-language framework designed to adapt large-scale CT foundation models for downstream clinical tasks. MedCT-VLM uses a parameter-efficient approach to adapt CT-CLIP, a contrastive vision-language model trained on 25,692 chest CT volumes, for multi-label pathology classification using Low-Rank Adaptation (LoRA). Rather than fine-tuning the model's 440 M parameters directly, we insert low-rank decomposition matrices into attention layers of both vision and text encoders, training only 1.67M parameters (0.38\\% of total). We evaluate on zero-shot classification across 18 thoracic pathologies, where the model must align CT embeddings with unseen text prompts at inference without task-specific training. LoRA fine-tuning improves mean AUROC from 61.3\\% to 68.9\\% (+7.6 pp), accuracy from 67.2\\% to 73.6\\% (+6.4 pp), and macro-F1 from 32.1\\% to 36.9\\% (+4.8 pp). These results demonstrate that parameter-efficient methods can effectively transfer large-scale pretraining to downstream medical imaging tasks, particularly for zero-shot scenarios where labeled data is scarce.", "AI": {"tldr": "本文提出了MedCT-VLM，一种低参数效率的视-语模型适配框架，用于医疗CT影像的零样本任务，并在零样本分类任务中展示了该方法的有效性。", "motivation": "传统的视觉语言预训练模型虽然展示了强大的零样本学习能力，但在体积医疗影像的应用较少。旨在通过一种低参数效率的方法适配大规模预训练的CT模型，以提升下游临床任务的表现。", "method": "MedCT-VLM采用了一种低参数调整方法（LoRA），通过在vision和text编码器的attention层中插入低秩分解矩阵，将在25,692个胸部CT体积上训练的对比视觉语言模型CT-CLIP适配于多标签病理分类。这种方法只需要训练1.67M的参数，仅占总参数量的0.38%。", "result": "在18种胸腔病理的零样本分类任务中，LoRA微调提高了平均AUROC从61.3%到68.9%，准确率从67.2%到73.6%，宏观F1值从32.1%到36.9%。", "conclusion": "结果表明，参数高效的方法可以有效地将大规模预训练应用到下游医疗影像任务上，尤其是在标签数据稀缺的零样本场景下更为有效。"}}
{"id": "2512.01892", "categories": ["cs.CL", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2512.01892", "abs": "https://arxiv.org/abs/2512.01892", "authors": ["Heloisa Candello", "Muneeza Azmat", "Uma Sushmitha Gunturi", "Raya Horesh", "Rogerio Abreu de Paula", "Heloisa Pimentel", "Marcelo Carpinette Grave", "Aminat Adebiyi", "Tiago Machado", "Maysa Malfiza Garcia de Macedo"], "title": "Exploring Human Perceptions of AI Responses: Insights from a Mixed-Methods Study on Risk Mitigation in Generative Models", "comment": "16 pages, 2 figures, 6 tables. Under review for publication", "summary": "With the rapid uptake of generative AI, investigating human perceptions of generated responses has become crucial. A major challenge is their `aptitude' for hallucinating and generating harmful contents. Despite major efforts for implementing guardrails, human perceptions of these mitigation strategies are largely unknown. We conducted a mixed-method experiment for evaluating the responses of a mitigation strategy across multiple-dimensions: faithfulness, fairness, harm-removal capacity, and relevance. In a within-subject study design, 57 participants assessed the responses under two conditions: harmful response plus its mitigation and solely mitigated response. Results revealed that participants' native language, AI work experience, and annotation familiarity significantly influenced evaluations. Participants showed high sensitivity to linguistic and contextual attributes, penalizing minor grammar errors while rewarding preserved semantic contexts. This contrasts with how language is often treated in the quantitative evaluation of LLMs. We also introduced new metrics for training and evaluating mitigation strategies and insights for human-AI evaluation studies.", "AI": {"tldr": "\\u7531\\u4e8e\\u751f\\u6210\\u5f71\\u54cdAI\\u7684\\u5ff6\\u8def\\u51b3\\u7b56\\u8bd5\\u9a8c\\u6210\\u7531\\uff0c\\u5b66\\u4e60\\u4e86\\u4eba\\u7c7b\\u5bf9\\u751f\\u6210\\u5904\\u7406\\u6743\\u7684\\u5145\\u5206\\u3002\\u751f\\u6210\\u5904\\u7406\\u6b63\\u786e\\u53ca\\u5360\\u7528\\u65bd\\u63a5\\u88c5\\u4e86\\u90e8\\u4efd\\uff0c\\u4f46\\u5bf9\\u4e4b\\u7684\\u4eba\\u7c7b\\u592a\\u5fc3\\u540e\\u6743\\u8fd8\\u5c11\\u77e5\\u7531\\u5b9e\\u9a8c\\u786e\\u5b9e\\uff0c\\u5219\\u5c06\\u6b63\\u5f0f\\u3002", "motivation": "\\u4e3b\\u8981\\u65b9\\u9762\\u6709\\uff1a\\n- \\u751f\\u6210AI\\u7684\\u843d\\u73b0\\u5bfc\\u81f4\\u5bf9\\u751f\\u6210\\u8d85\\u7ebf\\u4e00\\u533a\\u5206\\u548c\\u5371\\u62a4\\u5185\\u5bb9\\u7684\\u8d85\\u7ebf\\u5145\\u5206\\u5bf9\\u8d85\\u7ebf\\u5145\\u5206\\u548c\\u70ed\\u6b63\\u8bed\\u8a00\\u5904\\u7406\\u7684\\u6545\\u7269\\u8868\\u793a\\u7ebf\\u5b9e\\u5b9e\\u884c\\u9a8c\\u786e\\u5b9e\\u4ee5\\u6182\\u90e8\\u4eba\\u7c7b\\u5bf9\\u751f\\u6210AI\\u7684\\u540e\\u6743\\u3002", "method": "Structure", "result": "{\"tldr\": \"\\u7531\\u4e8e\\u751f\\u6210\\u5f71\\u54cdAI\\u7684\\u5ff6\\u8def\\u51b3\\u7b56\\u8bd5\\u9a8c\\u6210\\u7531\\uff0c\\u5b66\\u4e60\\u4e86\\u4eba\\u7c7b\\u5bf9\\u751f\\u6210\\u5904\\u7406\\u6743\\u7684\\u5145\\u5206\\u3002\\u751f\\u6210\\u5904\\u7406\\u6b63\\u786e\\u53ca\\u5360\\u7528\\u65bd\\u63a5\\u88c5\\u4e86\\u90e8\\u4efd\\uff0c\\u4f46\\u5bf9\\u4e4b\\u7684\\u4eba\\u7c7b\\u592a\\u5fc3\\u540e\\u6743\\u8fd8\\u5c11\\u77e5\\u7531\\u5b9e\\u9a8c\\u786e\\u5b9e\\uff0c\\u5219\\u5c06\\u6b63\\u5f0f\\u3002\", \"motivation\": \"\\u4e3b\\u8981\\u65b9\\u9762\\u6709\\uff1a\\n- \\u751f\\u6210AI\\u7684\\u843d\\u73b0\\u5bfc\\u81f4\\u5bf9\\u751f\\u6210\\u8d85\\u7ebf\\u4e00\\u533a\\u5206\\u548c\\u5371\\u62a4\\u5185\\u5bb9\\u7684\\u8d85\\u7ebf\\u5145\\u5206\\u5bf9\\u8d85\\u7ebf\\u5145\\u5206\\u548c\\u70ed\\u6b63\\u8bed\\u8a00\\u5904\\u7406\\u7684\\u6545\\u7269\\u8868\\u793a\\u7ebf\\u5b9e\\u5b9e\\u884c\\u9a8c\\u786e\\u5b9e\\u4ee5\\u6182\\u90e8\\u4eba\\u7c7b\\u5bf9\\u751f\\u6210AI\\u7684\\u540e\\u6743\\u3002\", \"method\": \"\\u8bd5\\u9a8c\\u4ee5\\u7528\\u4e00\\u79cd\\u591a\\u7ec4\\u6cd5\\u8868\\u793a\\uff0c\\u5148\\u79fb\\u52a8\\u94a5\\u7b49\\u5143\\u7d20\\u5224\\u7edad\\u3002\\u5728\\u8f83\\u5c0f\\u9762\\u5bf957\\u4e2a\\u53c2\\u4e0e\\u8005\\u7684\\u5185-\\u5185\\u6d4b\\u8bd5\\u7ed3\\u679c\\u4e2d\\uff0c\\u6bcf\\u4e2a\\u53c2\\u4e0e\\u8005\\u5728\\u4e24\\u79cd\\u6761\\u4ef6\\u4e0b\\u8d5b\\u4e0b\\u8d5b\\uff1a\\u5371\\u62a4\\u53cd\\u5e94\\u52a8\\u548c\\u5b8c\\u6574\\u5468\\u5c3d\\u6765\\u51b3\\u5b9a\\u751f\\u6210AI\\u7684\\u8d85\\u7ebf\\u5145\\u5206\\u5047\\u5b50\\u4e2d\\u7684\\u52a8\\u4f5c\\u4e0e\\u5b9e\\u9a8c\\u7684\\u65b9\\u6cd5\\u5e76\\u5e76\\u6765\\u8d5b\\u4e0a\\u6765\\u8d5b\\u4e0a\\u6765\\u6765\\uff1b\\u6765\\u8d5b\\u4e0a\\u6765\\u6765\\u6765\\u8d5b\\u4e0a\\u6765\\u8d5b\\u4e0a\\u7bea\\u7684\\uff0c\\u5e76\\u4e00\\u767d\\u884c\\u5b58\\u544a\\u5e94\\u5143\\u7d20\\uff0c\\u5e76\\u5e76\\u7ecf\\u8fc7\\u901a\\u8fc7\\u901a\\u8fc7\\u8f6c\\u8f6c\\u6362\\u6cd5\\u4e3a\\u7531\\u76f8\\u5e94\\u3002\", \"result\": \"\\u5b9e\\u9a8c\\u786e\\u5b9e\\uff1a\\n- \\u53c2\\u4e0e\\u8005\\u7684\\u677e\\u8bed\\u5e94\\u7528\\u3001AI\\u5de5\\u4f5c\\u7ecf\\u9a8c\\u548c\\u6807\\u4e4b\\u786e\\u5b9e\\u544a\\u88ab\\u5f71\\u54cd\\u8d70\\u6216\\u5c06\\u5bf9\\u7d27\\u60c5\\u5904\\u7406\\u6784\\u6587\\u5e94\\u7528\\u6b63\\u786e\\u6807\\u51c6\\u5ea6\\u8d85\\u8fc780%\\u7684\\u4e0d\\u4f46\\u5206\\u91cf\\u5728\\u51fa\\u73b0\\u5fae\\u5c0f\\u5360\\u4e66\\u3002\\n- \\u53c2\\u4e0e\\u8005\\u5728\\u8bed\\u8a00\\u6839\\u672c\\u5c31\\u6709\\u9ad8\\u7684\\u54c1\\u8d28\\u611b\\u610f\\uff0c\\u5fc5\\u987b\\u914d\\u5408\\u7684\\u6b63\\u786e\\u68ac\\u5f62\\u5e0c\\u5e0c\\u5e0c\\u5e0c\\u5c06\\u6563\\u81ea\\u5df1\\u8d85\\u7ebf\\u5145\\u5206\\u6838\\u5fc3\\u5e94\\u7528\\u7684\\u548c\\u4e0a\\u65b0\\u5e0c\\u5e0c\\u5e0c\\u5e0c\\u5e0c\\u5e0c\\u5e0c\\u5e0c\\u5e0c\\u5e0c\\u5e0c\\u5e0c\\u5e0c\\u5e0c\\u5904\\u7406\\u63aa\\u6784\\u4e0a\\u5347\\u9ad8\\u5143\\u7d20\\u6a21\\u578b\\u3002\", \"conclusion\": \"\\u5b66\\u7814\\u7528\\u4e86\\u65b0\\u7684\\u6bcf\\u6b65\\u94a5\\u548c\\u89e3\\u51b3\\u6cd5\\u6765\\u4e2d\\u901a\\u884c\\u5206\\u4e00\\u767d\\u5728\\u5206\\u5e73\\u4e2a\\u5e0c\\u5e0c\\u5e0c\\u5e0c\\u5e0c\\u5e0c\\u5e0c\\u5e0c\\u5e0c\\u591a\\u7684\\u8eda\\u540e\\u6784\\u6587\\uff0c\\u3002\"}", "conclusion": "\\u5b66\\u7814\\u7528\\u4e86\\u65b0\\u7684\\u6bcf\\u6b65\\u94a5\\u548c\\u89e3\\u51b3\\u6cd5\\u6765\\u4e2d\\u901a\\u884c\\u5206\\u4e00\\u767d\\u5728\\u5206\\u5e73\\u4e2a\\u5e0c\\u5e0c\\u5e0c\\u5e0c\\u5e0c\\u5e0c\\u5e0c\\u5e0c\\u5e0c\\u591a\\u7684\\u8eda\\u540e\\u6784\\u6587\\uff0c\\u3002"}}
{"id": "2512.00625", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.00625", "abs": "https://arxiv.org/abs/2512.00625", "authors": ["Tzu-I Liao", "Mahmoud Fakhry", "Jibin Yesudas Varghese"], "title": "Automatic Pith Detection in Tree Cross-Section Images Using Deep Learning", "comment": "8 pages, 7 figures", "summary": "Pith detection in tree cross-sections is essential for forestry and wood quality analysis but remains a manual, error-prone task. This study evaluates deep learning models -- YOLOv9, U-Net, Swin Transformer, DeepLabV3, and Mask R-CNN -- to automate the process efficiently. A dataset of 582 labeled images was dynamically augmented to improve generalization. Swin Transformer achieved the highest accuracy (0.94), excelling in fine segmentation. YOLOv9 performed well for bounding box detection but struggled with boundary precision. U-Net was effective for structured patterns, while DeepLabV3 captured multi-scale features with slight boundary imprecision. Mask R-CNN initially underperformed due to overlapping detections, but applying Non-Maximum Suppression (NMS) improved its IoU from 0.45 to 0.80. Generalizability was next tested using an oak dataset of 11 images from Oregon State University's Tree Ring Lab. Additionally, for exploratory analysis purposes, an additional dataset of 64 labeled tree cross-sections was used to train the worst-performing model to see if this would improve its performance generalizing to the unseen oak dataset. Key challenges included tensor mismatches and boundary inconsistencies, addressed through hyperparameter tuning and augmentation. Our results highlight deep learning's potential for tree cross-section pith detection, with model choice depending on dataset characteristics and application needs.", "AI": {"tldr": "本研究通过多种深度学习模型提升了木蕊检测的自动化和精度，强调了模型选择的重要性以及通过扩展训练数据来改善模型性能的方法。", "motivation": "树木横截面中的木蕊检测对于林业和木材质量分析至关重要，但传统的手动检测方法耗时且易出错。为了提高木蕊检测的准确性和效率，本研究旨在通过深度学习技术实现这一过程的自动化。", "method": "本研究通过评估YOLOv9、U-Net、Swin Transformer、DeepLabV3和Mask R-CNN这五种深度学习模型来实现树木横截面中木蕊检测的自动化。使用的数据集包含582张标注图像，并通过扩展数据集来提高模型的泛化能力。", "result": "Swin Transformer模型取得了最高精度为0.94，在精细分割方面表现出色。YOLOv9在边界检测上表现良好，但在边界精度上略有不足。U-Net适合结构化模式的检测，而DeepLabV3能够捕捉多尺度特征，但在边界上存在轻微的不精确。Mask R-CNN最初的表现不佳，由于重叠检测问题，但通过应用非极大值抑制(NMS)技术后，其IoU从0.45提高到了0.80。", "conclusion": "本研究表明，基于深度学习的模型在树木横截面木蕊检测方面有潜力，模型的选择需视数据集特性和应用需求而定。"}}
{"id": "2512.01896", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.01896", "abs": "https://arxiv.org/abs/2512.01896", "authors": ["Jinzheng Yu", "Yang Xu", "Haozhen Li", "Junqi Li", "Yifan Feng", "Ligu Zhu", "Hao Shen", "Lei Shi"], "title": "OPOR-Bench: Evaluating Large Language Models on Online Public Opinion Report Generation", "comment": "27 pages, accepted by CMC-Computers, Materials & Continua, 2025", "summary": "Online Public Opinion Reports consolidate news and social media for timely crisis management by governments and enterprises. While large language models have made automated report generation technically feasible, systematic research in this specific area remains notably absent, particularly lacking formal task definitions and corresponding benchmarks. To bridge this gap, we define the Automated Online Public Opinion Report Generation (OPOR-GEN) task and construct OPOR-BENCH, an event-centric dataset covering 463 crisis events with their corresponding news articles, social media posts, and a reference summary. To evaluate report quality, we propose OPOR-EVAL, a novel agent-based framework that simulates human expert evaluation by analyzing generated reports in context. Experiments with frontier models demonstrate that our framework achieves high correlation with human judgments. Our comprehensive task definition, benchmark dataset, and evaluation framework provide a solid foundation for future research in this critical domain.", "AI": {"tldr": "本文定义了自动在线公共意见报告生成任务，构建了涵盖463个危机事件的数据集（OPOR-BENCH），并提出了一种基于代理的评估框架（OPOR-EVAL），该框架模拟人类专家评估。实验表明，该框架与人类判断高度相关。", "motivation": "尽管大型语言模型使自动化报告生成在技术上成为可能，但在在线公共意见报告生成这一特定领域，系统性研究明显不足，特别缺乏任务定义和相应的基准。为了填补这一空白，本文进行了相关研究。", "method": "本文定义了自动在线公共意见报告生成（OPOR-GEN）任务，并构建了一个涵盖463个危机事件的数据集OPOR-BENCH，该数据集包括相关新闻文章、社交媒体帖子以及参考摘要。为了评估报告质量，作者提出了一种基于代理的评估框架OPOR-EVAL，该框架通过在上下文中分析生成的报告来模拟人类专家评估。", "result": "实验表明，该框架与人类判断具有高度的相关性。", "conclusion": "本文的任务定义、基准数据集和评估框架为这一关键领域未来的研究提供了坚实的基础。"}}
{"id": "2512.00626", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.00626", "abs": "https://arxiv.org/abs/2512.00626", "authors": ["Kim Gerard A. Villanueva", "Priyanka Kumar"], "title": "XAI-Driven Skin Disease Classification: Leveraging GANs to Augment ResNet-50 Performance", "comment": null, "summary": "Accurate and timely diagnosis of multi-class skin lesions is hampered by subjective methods, inherent data imbalance in datasets like HAM10000, and the \"black box\" nature of Deep Learning (DL) models. This study proposes a trustworthy and highly accurate Computer-Aided Diagnosis (CAD) system to overcome these limitations. The approach utilizes Deep Convolutional Generative Adversarial Networks (DCGANs) for per class data augmentation to resolve the critical class imbalance problem. A fine-tuned ResNet-50 classifier is then trained on the augmented dataset to classify seven skin disease categories. Crucially, LIME and SHAP Explainable AI (XAI) techniques are integrated to provide transparency by confirming that predictions are based on clinically relevant features like irregular morphology. The system achieved a high overall Accuracy of 92.50 % and a Macro-AUC of 98.82 %, successfully outperforming various prior benchmarked architectures. This work successfully validates a verifiable framework that combines high performance with the essential clinical interpretability required for safe diagnostic deployment. Future research should prioritize enhancing discrimination for critical categories, such as Melanoma NOS (F1-Score is 0.8602).", "AI": {"tldr": "A trustworthy and accurate CAD system for skin lesions uses DCGANs for data augmentation, a ResNet-50 classifier, and LIME/SHAP for interpretability, achieving high accuracy and AUC, outperforming previous models.", "motivation": "The main motivation is to provide an accurate, timely, and trustworthy Computer-Aided Diagnosis (CAD) system for multi-class skin lesions which are currently affected by subjective methods, data imbalance issues, and the lack of interpretability in Deep Learning models.", "method": "The study uses Deep Convolutional Generative Adversarial Networks (DCGANs) for data augmentation to address class imbalance and a fine-tuned ResNet-50 classifier for classification, along with LIME and SHAP to ensure clinical interpretability.", "result": "The system achieved an overall accuracy of 92.50% and a Macro-AUC of 98.82%, demonstrating an outperformance over existing architectures while ensuring interpretability.", "conclusion": "The study concludes that combining high performance with clinical interpretability through the use of DCGANs, a fine-tuned ResNet-50 classifier, and XAI techniques offers a viable solution for improving the diagnosis of skin disease categories."}}
{"id": "2512.01909", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.01909", "abs": "https://arxiv.org/abs/2512.01909", "authors": ["Lihu Chen", "Xiang Yin", "Francesca Toni"], "title": "Latent Debate: A Surrogate Framework for Interpreting LLM Thinking", "comment": "Preprint", "summary": "Understanding the internal thinking process of Large Language Models (LLMs) and the cause of hallucinations remains a key challenge. To this end, we introduce latent debate, a novel framework for interpreting model predictions through the lens of implicit internal arguments. Unlike the current work of self-consistency and multi-agent debate, which relies on explicit debates among multiple answers or multiple models, latent debate captures the hidden supporting and attacking signals that arise within a single model during a single inference. We first present a model- and task-agnostic conceptual framework, and then instantiate it symbolically to approximate the thinking process of LLMs on True/False prediction tasks. Empirical studies demonstrate that latent debate is a faithful structured surrogate model that has highly consistent predictions with the original LLM. Beyond interpretability, we demonstrate that latent debate provides a strong baseline for hallucination detection. Further analysis reveals strong correlations between hallucinations and debate patterns, such as a high degree of latent debates in the middle layers is linked to a higher risk of hallucinations. These findings position latent debate as a potential framework for understanding internal mechanisms of LLMs, especially for scenarios where internal (dis)agreements appear during the inference steps.", "AI": {"tldr": "本文介绍了一种名为隐式辩论的新框架，用于解释LLMs的预测行为，并揭示其在幻觉检测方面的潜力。", "motivation": "为了理解大型语言模型（LLMs）的内部思考过程和幻觉产生的原因，这是一个关键挑战。", "method": "提出了一种名为隐式辩论的新框架，用于通过模型预测过程中隐含的内部辩论视角来解释模型行为。该框架捕捉单个模型在单次推理过程中产生的潜在的支持和攻击信号。", "result": "实证研究表明，隐式辩论是一个忠实的结构化代理模型，其预测结果与原始LLM高度一致。此外，隐式辩论还提供了幻觉检测的强大基础。进一步分析表明，幻觉与辩论模式之间存在强相关性，如中间层的高阶隐式辩论与幻觉风险增加有关。", "conclusion": "这些发现将隐式辩论定位为一个潜在框架，可用于理解LLMs的内部机制，特别是在推理步骤中可能出现内部（不）一致性的场景中。"}}
{"id": "2512.00639", "categories": ["cs.CV", "cs.AI", "cs.CE", "cs.LG", "cs.PF"], "pdf": "https://arxiv.org/pdf/2512.00639", "abs": "https://arxiv.org/abs/2512.00639", "authors": ["Mahmoud El Hussieni"], "title": "Doppler-Enhanced Deep Learning: Improving Thyroid Nodule Segmentation with YOLOv5 Instance Segmentation", "comment": null, "summary": "The increasing prevalence of thyroid cancer globally has led to the development of various computer-aided detection methods. Accurate segmentation of thyroid nodules is a critical first step in the development of AI-assisted clinical decision support systems. This study focuses on instance segmentation of thyroid nodules using YOLOv5 algorithms on ultrasound images. We evaluated multiple YOLOv5 variants (Nano, Small, Medium, Large, and XLarge) across two dataset versions, with and without doppler images. The YOLOv5-Large algorithm achieved the highest performance with a dice score of 91\\% and mAP of 0.87 on the dataset including doppler images. Notably, our results demonstrate that doppler images, typically excluded by physicians, can significantly improve segmentation performance. The YOLOv5-Small model achieved 79\\% dice score when doppler images were excluded, while including them improved performance across all model variants. These findings suggest that instance segmentation with YOLOv5 provides an effective real-time approach for thyroid nodule detection, with potential clinical applications in automated diagnostic systems.", "AI": {"tldr": "使用YOLOv5算法对甲状腺结节实例分割的研究证明了其在包含多普勒图像的数据集上的有效性，表明该方法有潜力应用于自动诊断系统中。", "motivation": "甲状腺癌的全球发病率上升促使各类计算机辅助检测方法的发展。甲状腺结节的准确分割对于AI辅助临床决策支持系统至关重要。", "method": "本研究采用了YOLOv5算法进行甲状腺结节的实例分割，评估了多种YOLOv5变体（Nano、Small、Medium、Large、XLarge）在两组数据集上的性能，一组包含多普勒图像，另一组不包含。", "result": "研究结果显示，在包含多普勒图像的数据集中，YOLOv5-Large实现最佳性能，Dice分数为91%，mAP为0.87。不包含多普勒图像的YOLOv5-Small模型的Dice分数为79%，而包含时所有模型变体的性能均有所提升。", "conclusion": "多普勒图像虽然通常被医师排除，但可用于显著提升分割性能。本研究证明了利用YOLOv5实现甲状腺结节的实例分割，可作为实时诊断方法，并具有临床应用潜力。"}}
{"id": "2512.01925", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.01925", "abs": "https://arxiv.org/abs/2512.01925", "authors": ["Junnan Liu", "Hongwei Liu", "Songyang Zhang", "Kai Chen"], "title": "Rectifying LLM Thought from Lens of Optimization", "comment": "Work in progress", "summary": "Recent advancements in large language models (LLMs) have been driven by their emergent reasoning capabilities, particularly through long chain-of-thought (CoT) prompting, which enables thorough exploration and deliberation. Despite these advances, long-CoT LLMs often exhibit suboptimal reasoning behaviors, such as overthinking and excessively protracted reasoning chains, which can impair performance. In this paper, we analyze reasoning processes through an optimization lens, framing CoT as a gradient descent procedure where each reasoning step constitutes an update toward problem resolution. Building on this perspective, we introduce RePro (Rectifying Process-level Reward), a novel approach to refine LLM reasoning during post-training. RePro defines a surrogate objective function to assess the optimization process underlying CoT, utilizing a dual scoring mechanism to quantify its intensity and stability. These scores are aggregated into a composite process-level reward, seamlessly integrated into reinforcement learning with verifiable rewards (RLVR) pipelines to optimize LLMs. Extensive experiments across multiple reinforcement learning algorithms and diverse LLMs, evaluated on benchmarks spanning mathematics, science, and coding, demonstrate that RePro consistently enhances reasoning performance and mitigates suboptimal reasoning behaviors.", "AI": {"tldr": "本文通过将长链推理（CoT）视作梯度下降过程，提出了RePro方法，它利用双评分机制改进语言模型推理，展现出了提升推理性能的效果。", "motivation": "尽管长链推理（CoT）语言模型展示了良好的推理能力，但它们通常表现出次优的推理行为，如过度思考和冗长的推理链，这些都会影响其性能。本文的动机在于改进这些推理过程。", "method": "通过对理解过程采用优化视角，将长链推理（CoT）视为一种梯度下降程序，其中每个推理步骤均为向问题解决迈进的更新。引入RePro（过程级奖赏修正），作为一种在后期训练中修正语言模型推理过程的新方法。RePro定义了一个替代目标函数来评估CoT背后的优化过程，使用双评分机制来量化其强度和稳定性，并将其整合到具有可验证奖励的强化学习（RLVR）流程中以优化语言模型。", "result": "实验结果，涵盖了多种强化学习算法和各种语言模型，以及跨越数学、科学和编码的基准测试，表明RePro能持续提升推理性能并减少次优推理行为。", "conclusion": "RePro方法通过双评分机制评估CoT的强度和稳定性，并将其整合到强化学习流程中以优化语言模型，从而在多种基准测试中呈现出色的效果。"}}
{"id": "2512.00641", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.00641", "abs": "https://arxiv.org/abs/2512.00641", "authors": ["Razieh Ghaedi", "AmirReza BabaAhmadi", "Reyer Zwiggelaar", "Xinqi Fan", "Nashid Alam"], "title": "Graph-Attention Network with Adversarial Domain Alignment for Robust Cross-Domain Facial Expression Recognition", "comment": "17 pages, 5 figures. Accepted at the 17th Asian Conference on Machine Learning (ACML 2025), Taipei, Taiwan, December 9-12, 2025", "summary": "Cross-domain facial expression recognition (CD-FER) remains difficult due to severe domain shift between training and deployment data. We propose Graph-Attention Network with Adversarial Domain Alignment (GAT-ADA), a hybrid framework that couples a ResNet-50 as backbone with a batch-level Graph Attention Network (GAT) to model inter-sample relations under shift. Each mini-batch is cast as a sparse ring graph so that attention aggregates cross-sample cues that are informative for adaptation. To align distributions, GAT-ADA combines adversarial learning via a Gradient Reversal Layer (GRL) with statistical alignment using CORAL and MMD. GAT-ADA is evaluated under a standard unsupervised domain adaptation protocol: training on one labeled source (RAF-DB) and adapting to multiple unlabeled targets (CK+, JAFFE, SFEW 2.0, FER2013, and ExpW). GAT-ADA attains 74.39% mean cross-domain accuracy. On RAF-DB to FER2013, it reaches 98.0% accuracy, corresponding to approximately a 36-point improvement over the best baseline we re-implemented with the same backbone and preprocessing.", "AI": {"tldr": "GAT-ADA, a novel framework combining a ResNet-50 backbone with GAT and adversarial learning, significantly improves cross-domain facial expression recognition accuracy by mitigating domain shift.", "motivation": "To address the challenges of cross-domain facial expression recognition due to domain shift between training and deployment datasets.", "method": "Graph-Attention Network with Adversarial Domain Alignment (GAT-ADA), combining ResNet-50 with a batch-level GAT for inter-sample relation modeling, and uses adversarial learning with GRL plus statistical alignment using CORAL and MMD to align target and source domain distributions.", "result": "GAT-ADA achieves a mean cross-domain accuracy of 74.39%, and when adapting from RAF-DB to FER2013, it reaches 98.0% accuracy, which is a 36-point improvement over the best baseline.", "conclusion": "GAT-ADA effectively addresses the issue of cross-domain facial expression recognition by incorporating a hybrid framework that enhances domain adaptation through the use of a graph attention network and adversarial techniques."}}
{"id": "2512.01948", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.01948", "abs": "https://arxiv.org/abs/2512.01948", "authors": ["Dingling Zhang", "He Zhu", "Jincheng Ren", "Kangqi Song", "Xinran Zhou", "Boyu Feng", "Shudong Liu", "Jiabin Luo", "Weihao Xie", "Zhaohui Wang", "Tianrui Qin", "King Zhu", "Yuqing Wang", "Qianben Chen", "Yuchen Eleanor Jiang", "Wei Wang", "Jiaheng Liu", "Wangchunshu Zhou"], "title": "How Far Are We from Genuinely Useful Deep Research Agents?", "comment": "34 pages", "summary": "Deep Research Agents (DRAs) aim to automatically produce analyst-level reports through iterative information retrieval and synthesis. However, most existing DRAs were validated on question-answering benchmarks, while research on generating comprehensive reports remains overlooked. Worse, current benchmarks for report synthesis suffer from task complexity and subjective metrics -- this fails to reflect user demands and limits the practical utility of generated reports. To address these gaps, we present Fine-grained DEepResearch bench (FINDER), an enhanced benchmark consisting of 100 human-curated research tasks with 419 structured checklist items that standardize report structure, analytical depth, and factual grounding. Based on approximately 1,000 reports produced by mainstream DRAs, we further propose Deep rEsearch Failure Taxonomy (DEFT), the first failure taxonomy for deep research agents. DEFT contains 14 fine-grained failure modes across reasoning, retrieval, and generation, and is built upon grounded theory with human-LLM co-annotating and inter-annotator reliability validation. Our experimental findings reveal that current DRAs struggle not with task comprehension but with evidence integration, verification, and reasoning-resilient planning.", "AI": {"tldr": "研究提出了一种新的基准测试FINDER和失败分类法DEFT，用以评估与分类DRAs生成报告的能力，这揭示了现有DRAs在证据整合、验证和推理上存在薄弱环节。", "motivation": "现有的Deep Research Agents（DRAs）主要集中于问答基准的验证，而对生成综合报告的研究相对较少。此外，现有的报告合成基准由于任务复杂性和主观性度量，无法反映用户需求并限制了生成报告的实际效用。", "method": "该研究提出了Fine-grained DEepResearch基准（FINDER），包含100个人类策划的研究任务与419个结构化的检查列表项目，以标准化报告结构、分析深度和事实基础。同时，基于约1000份主流DRAs生成的报告，提出了Deep rEsearch Failure Taxonomy（DEFT），这个分类法包含了14个细粒度的失败模式，这些模式跨越了推理、检索和生成。", "result": "实验结果表明，现有的DRAs在任务理解上并没有太大的困难，而在于证据整合、验证和推理方面存在不足。", "conclusion": "通过FINDER和DEFT，我们获得了一种新的方法来更全面地评估DRAs，并揭示了在生成综合报告时需要突破的关键领域。"}}
{"id": "2512.00647", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.00647", "abs": "https://arxiv.org/abs/2512.00647", "authors": ["Shanhui Liu", "Rui Xu", "Yunke Wang"], "title": "MambaScope: Coarse-to-Fine Scoping for Efficient Vision Mamba", "comment": null, "summary": "Vision Mamba has emerged as a promising and efficient alternative to Vision Transformers, yet its efficiency remains fundamentally constrained by the number of input tokens. Existing token reduction approaches typically adopt token pruning or merging to reduce computation. However, they inherently lead to information loss, as they discard or compress token representations. This problem is exacerbated when applied uniformly to fine-grained token representations across all images, regardless of visual complexity. We observe that not all inputs require fine-grained processing. Simple images can be effectively handled at coarse resolution, while only complex ones may warrant refinement. Based on this insight, we propose \\textit{Coarse-to-Fine Vision Mamba (CF-ViM)}, an adaptive framework for efficient inference. CF-ViM first performs coarse-grained inference by dividing the input image into large patches, significantly reducing the token length and computation. When the model's prediction confidence is low, selected regions are re-processed at a finer resolution to recover critical visual details with minimal additional cost. This dynamic resolution assignment strategy allows CF-ViM to allocate computation adaptively according to image complexity, ensuring efficient processing without compromising essential visual information. Experiments on ImageNet demonstrate that CF-ViM outperforms both the baseline Vision Mamba and state-of-the-art token reduction techniques in terms of accuracy and efficiency.", "AI": {"tldr": "The paper proposes CF-ViM, an adaptive framework that dynamically adjusts the resolution for processing images based on their complexity, achieving both high efficiency and accuracy.", "motivation": "To address the inherent limitations of Vision Mamba, where token reduction methods lead to information loss, the authors aim to develop a mechanism that allows for more adaptive and efficient inference.", "method": "CF-ViM divides images into large patches for coarse-grained processing and reprocesses selected regions at a finer resolution when necessary, to balance computational costs with visual detail preservation.", "result": "Experiments on ImageNet show that CF-ViM performs better in terms of accuracy and efficiency compared to the baseline Vision Mamba and state-of-the-art token reduction techniques.", "conclusion": "The dynamic resolution assignment strategy used in CF-ViM effectively optimizes computational resources, leading to superior performance without sacrificing important visual information."}}
{"id": "2512.02008", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2512.02008", "abs": "https://arxiv.org/abs/2512.02008", "authors": ["Aradhye Agarwal", "Ayan Sengupta", "Tanmoy Chakraborty"], "title": "The Art of Scaling Test-Time Compute for Large Language Models", "comment": null, "summary": "Test-time scaling (TTS) -- the dynamic allocation of compute during inference -- is a promising direction for improving reasoning in large language models (LLMs). However, a systematic comparison of well-known TTS strategies under identical conditions is missing, and the influence of model type and problem difficulty on performance remains unclear. To address these gaps, we conduct the first large-scale study of TTS, spanning over thirty billion tokens generated using eight open-source LLMs (7B to 235B parameters), across four reasoning datasets. We observe three consistent trends: (1) no single TTS strategy universally dominates; (2) reasoning models exhibit distinct trace-quality patterns across problem difficulty and trace length, forming short-horizon and long-horizon categories; and (3) for a given model type, the optimal TTS performance scales monotonically with compute budget. Based on these insights, we provide a practical recipe for selecting the best TTS strategy, considering problem difficulty, model type, and compute budget, providing a practical guide to effective inference-time scaling.", "AI": {"tldr": "本研究进行了首次大规模TTS研究，覆盖八个开源LLM和四个推理数据集，揭示了TTS性能与计算预算和模型类型之间的关系，并提出根据问题难度和其他因素选择最优TTS策略的建议。", "motivation": "目前，缺乏在相同条件下对已知TTS策略的系统性比较，以及模型类型和问题难度对性能影响尚不明确。本研究旨在填补这些空白。", "method": "本研究进行了首次大规模的测试时间缩放（TTS）研究，涵盖了使用八个开源大规模语言模型（7B到235B参数）生成的超过三十亿的标记，以及四个推理数据集。", "result": "观察到三个一致的趋势：(1) 没有任何单一的TTS策略可以一统天下；(2) 推理模型在问题难度和追踪长度方面显示出不同质量的追踪模式，形成短期和长期两类；(3) 对于给定的模型类型，最佳TTS性能随着计算预算的增加而单调增加。", "conclusion": "通过这些洞察，提出了一个实用的配方，以便根据问题难度、模型类型和计算预算来选择最佳的TTS策略，提供了有效的推理时间缩放的实际指导。"}}
{"id": "2512.00676", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.00676", "abs": "https://arxiv.org/abs/2512.00676", "authors": ["Kiri L. Wagstaff"], "title": "Realistic Handwritten Multi-Digit Writer (MDW) Number Recognition Challenges", "comment": "10 pages, 6 figures", "summary": "Isolated digit classification has served as a motivating problem for decades of machine learning research. In real settings, numbers often occur as multiple digits, all written by the same person. Examples include ZIP Codes, handwritten check amounts, and appointment times. In this work, we leverage knowledge about the writers of NIST digit images to create more realistic benchmark multi-digit writer (MDW) data sets. As expected, we find that classifiers may perform well on isolated digits yet do poorly on multi-digit number recognition. If we want to solve real number recognition problems, additional advances are needed. The MDW benchmarks come with task-specific performance metrics that go beyond typical error calculations to more closely align with real-world impact. They also create opportunities to develop methods that can leverage task-specific knowledge to improve performance well beyond that of individual digit classification methods.", "AI": {"tldr": "研究了多数字书写者的数据集，并发现虽然在孤立数字分类上表现良好，但在多数字识别上效果不佳，强调了对解决实际识别问题需要进一步的研究。", "motivation": "尽管在孤立数字分类上已有大量研究，但在实际应用中，数字往往是多数字组合出现，且由同一人书写。因此，本研究旨在创建更接近真实情况的数据集，并探索针对多数字识别的新方法。", "method": "该研究通过利用NIST数字图像中书写者的信息，创建了更贴近实际应用场景的多数字书写者数据集。", "result": "发现现有的分类器在多数字识别任务上表现不如预期，说明解决真实数字识别问题需要进一步改进和创新。", "conclusion": "多数字数据集（MDW）提供了任务特定的性能度量标准，这些标准更贴近实际应用，并为利用任务特定知识来提高多数字识别性能开辟了新途径。"}}
{"id": "2512.02010", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.02010", "abs": "https://arxiv.org/abs/2512.02010", "authors": ["Jack Cook", "Junxian Guo", "Guangxuan Xiao", "Yujun Lin", "Song Han"], "title": "Four Over Six: More Accurate NVFP4 Quantization with Adaptive Block Scaling", "comment": "10 pages, 5 figures", "summary": "As large language models have grown larger, low-precision numerical formats such as NVFP4 have become increasingly popular due to the speed and memory benefits they provide. However, to accelerate computation with NVFP4, all matrix multiplication operands--weights and activations in the forward pass, and weights, activations, and gradients in the backward pass--must be quantized to NVFP4, often leading to divergence during training and performance degradation during inference. NVFP4 by evaluating multiple potential scale factors for each block of values. To address this issue, in this work we introduce Four Over Six (4/6), a modification to the NVFP4 quantization algorithm that evaluates two potential scale factors for each block of values. Unlike integer formats, floating-point formats such as FP4 have the most quantization error on near-maximal values in each block, which we find to be primarily responsible for downstream performance degradation. We find that for some blocks, scaling to smaller FP4 values makes the distribution of representable values more uniform, improving representation of near-maximal values. Importantly, 4/6 can be implemented efficiently on NVIDIA Blackwell GPUs, making it viable to use while training LLMs with NVFP4. In pre-training experiments with transformer and hybrid model architectures, we find that 4/6 prevents divergence in several cases, bringing training loss significantly closer to BF16 compared to models trained with current state-of-the-art NVFP4 training recipes. We also find that 4/6 can be easily incorporated into many different post-training quantization methods and generally improves downstream accuracy. We hope this inspires future work in training and deploying models with NVFP4.", "AI": {"tldr": "引入Four Over Six (4/6) 改进NVFP4量化算法，通过评估每个值块的两个可能缩放因子来改善量化误差，特别是在训练和推理中的性能提升。", "motivation": "解决NVFP4量化过程中引起的模型训练发散和性能下降问题。", "method": "针对FP4格式量化误差较大的问题，提出了4/6方法，通过调整缩放因子使得值的分布更加均匀，从而提高量化效果。", "result": "实验结果显示4/6方法不仅预防了模型训练过程中的发散现象，还显著降低了训练损失，靠近BF16的表现；同时在多种后训练量化方法中证明了4/6的有效性。", "conclusion": "4/6方法在NVIDIA Blackwell GPU上能有效实现，适用于基于NVFP4格式的大规模语言模型训练，提升训练稳定性和性能。"}}
{"id": "2512.00677", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.00677", "abs": "https://arxiv.org/abs/2512.00677", "authors": ["Dong In Lee", "Hyungjun Doh", "Seunggeun Chi", "Runlin Duan", "Sangpil Kim", "Karthik Ramani"], "title": "Dynamic-eDiTor: Training-Free Text-Driven 4D Scene Editing with Multimodal Diffusion Transformer", "comment": "4D Scene Editing", "summary": "Recent progress in 4D representations, such as Dynamic NeRF and 4D Gaussian Splatting (4DGS), has enabled dynamic 4D scene reconstruction. However, text-driven 4D scene editing remains under-explored due to the challenge of ensuring both multi-view and temporal consistency across space and time during editing. Existing studies rely on 2D diffusion models that edit frames independently, often causing motion distortion, geometric drift, and incomplete editing. We introduce Dynamic-eDiTor, a training-free text-driven 4D editing framework leveraging Multimodal Diffusion Transformer (MM-DiT) and 4DGS. This mechanism consists of Spatio-Temporal Sub-Grid Attention (STGA) for locally consistent cross-view and temporal fusion, and Context Token Propagation (CTP) for global propagation via token inheritance and optical-flow-guided token replacement. Together, these components allow Dynamic-eDiTor to perform seamless, globally consistent multi-view video without additional training and directly optimize pre-trained source 4DGS. Extensive experiments on multi-view video dataset DyNeRF demonstrate that our method achieves superior editing fidelity and both multi-view and temporal consistency prior approaches. Project page for results and code: https://di-lee.github.io/dynamic-eDiTor/", "AI": {"tldr": "Dynamic-eDiTor 是一种新的无训练文本驱动的4D编辑方法，能够实现跨视角和时间一致性的多视角视频编辑。", "motivation": "当前的研究主要依赖于2D扩散模型，独立编辑帧，这往往导致运动失真、几何漂移和编辑不完整。为了克服这些挑战，提出了一种能够确保编辑过程中空间和时间一致性的方法。", "method": "引入Dynamic-eDiTor，一种基于Multimodal Diffusion Transformer (MM-DiT) 和 4DGS的无训练文本驱动的4D编辑框架。该机制包括空间-时间子网格注意力（STGA）以实现局部一致的跨视角和时间融合，以及上下文令牌传播（CTP）通过令牌继承和基于光流的令牌替换实现全局传播。", "result": "实验结果表明，该方法在编辑精度以及跨多视角和时间的一致性方面优于现有方法。", "conclusion": "Dynamic-eDiTor 通过创新机制，克服了现有方法的不足，实现了无缝、全局一致的多视角视频编辑，并能够在不增加额外训练的情况下优化预先训练的4DGS。"}}
{"id": "2512.00691", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.00691", "abs": "https://arxiv.org/abs/2512.00691", "authors": ["Dingqiang Ye", "Chao Fan", "Kartik Narayan", "Bingzhe Wu", "Chengwen Luo", "Jianqiang Li", "Vishal M. Patel"], "title": "Silhouette-based Gait Foundation Model", "comment": null, "summary": "Gait patterns play a critical role in human identification and healthcare analytics, yet current progress remains constrained by small, narrowly designed models that fail to scale or generalize. Building a unified gait foundation model requires addressing two longstanding barriers: (a) Scalability. Why have gait models historically failed to follow scaling laws? (b) Generalization. Can one model serve the diverse gait tasks that have traditionally been studied in isolation? We introduce FoundationGait, the first scalable, self-supervised pretraining framework for gait understanding. Its largest version has nearly 0.13 billion parameters and is pretrained on 12 public gait datasets comprising over 2 million walking sequences. Extensive experiments demonstrate that FoundationGait, with or without fine-tuning, performs robustly across a wide spectrum of gait datasets, conditions, tasks (e.g., human identification, scoliosis screening, depression prediction, and attribute estimation), and even input modality. Notably, it achieves 48.0% zero-shot rank-1 accuracy on the challenging in-the-wild Gait3D dataset (1,000 test subjects) and 64.5% on the largest in-the-lab OU-MVLP dataset (5,000+ test subjects), setting a new milestone in robust gait recognition. Coming code and model: https://github.com/ShiqiYu/OpenGait.", "AI": {"tldr": "本文介绍了FoundationGait，这是一个用于步态理解的可扩展、自监督预训练框架。它在广泛的步态任务上表现出色，树立了新的性能里程碑。", "motivation": "搭建统一的步态基础模型需要解决两个长期存在的障碍：(a)可扩展性。历史上的步态模型为何未能遵循扩展定律？(b)泛化。一个模型是否可以服务传统上独立研究的各种步态任务？", "method": "我们引入了FoundationGait，这是第一个用于步态理解的可扩展的自监督预训练框架。其最大版本拥有近13亿个参数，并且在包括超过200万个行走序列的12个公开步态数据集上进行了预训练。", "result": "广泛的实验表明，FoundationGait在广泛的任务、条件和输入模态中表现出优越的性能，即使在没有微调的情况下也是如此。特别地，它在具有挑战性的真实世界中的Gait3D数据集上实现了零样本下48.0%的排名1准确率，在最大的实验室OU-MVLP数据集上达到64.5%的准确率，从而在稳健的步态识别中树立了新里程碑。", "conclusion": "研究表明，FoundationGait可以有效地在多种步态任务、条件、任务类型和输入模态上实现稳健的性能，包括人体识别、脊柱侧弯筛查、抑郁症预测和属性估计等，展示了其作为步态识别模型的潜力。"}}
{"id": "2512.01085", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.01085", "abs": "https://arxiv.org/abs/2512.01085", "authors": ["Wenjun Zhang", "Shekhar S. Chandra", "Aaron Nicolson"], "title": "Generalized Medical Phrase Grounding", "comment": null, "summary": "Medical phrase grounding (MPG) maps textual descriptions of radiological findings to corresponding image regions. These grounded reports are easier to interpret, especially for non-experts. Existing MPG systems mostly follow the referring expression comprehension (REC) paradigm and return exactly one bounding box per phrase. Real reports often violate this assumption. They contain multi-region findings, non-diagnostic text, and non-groundable phrases, such as negations or descriptions of normal anatomy. Motivated by this, we reformulate the task as generalised medical phrase grounding (GMPG), where each sentence is mapped to zero, one, or multiple scored regions. To realise this formulation, we introduce the first GMPG model: MedGrounder. We adopted a two-stage training regime: pre-training on report sentence--anatomy box alignment datasets and fine-tuning on report sentence--human annotated box datasets. Experiments on PadChest-GR and MS-CXR show that MedGrounder achieves strong zero-shot transfer and outperforms REC-style and grounded report generation baselines on multi-region and non-groundable phrases, while using far fewer human box annotations. Finally, we show that MedGrounder can be composed with existing report generators to produce grounded reports without retraining the generator.", "AI": {"tldr": "该研究提出了一种新的广义医疗词组接地方法（GMPG），采用了名为MedGrounder的模型，能够更好地处理复杂的医学报告，并且在实验中表现优于传统方法", "motivation": "现有医疗词组接地系统大多遵循参考表达理解框架，每个短语只能返回一个边界框，而实际上医学报告中经常有违反这一假设的情况，如多区域发现、非诊断性文本和不可接地的短语", "method": "该研究提出了一个名为MedGrounder的两阶段训练模型，首先在报告句子与解剖部位框对齐的数据集上进行预训练，然后在报告句子与人类标注框的数据集上进行微调，以实现广义医疗词组接地任务（GMPG）", "result": "实验结果显示，MedGrounder可以在使用较少的人类标注框的情况下，在多区域和不可接地短语上超越其他方法", "conclusion": "MedGrounder在两项实验中表现良好，展示了零样本迁移能力和出色的性能，同时还能与其他现存的报告生成器结合使用，无需重新训练生成器"}}
{"id": "2512.00694", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.00694", "abs": "https://arxiv.org/abs/2512.00694", "authors": ["Mengzhu Xu", "Hanzhi Liu", "Ningkang Peng", "Qianyu Chen", "Canran Xiao"], "title": "Affordance-First Decomposition for Continual Learning in Video-Language Understanding", "comment": "Under review", "summary": "Continual learning for video--language understanding is increasingly important as models face non-stationary data, domains, and query styles, yet prevailing solutions blur what should stay stable versus what should adapt, rely on static routing/capacity, or require replaying past videos. We aim to explicitly specify where stability lives and where plasticity should be focused under realistic memory and privacy constraints. We introduce Affordance-First Decomposition (AFD): videos are mapped to slowly varying affordance tokens that form a shared, time-aligned substrate, while a lightweight, query-routed, conflict-aware scheduler concentrates adaptation and grows capacity only when needed. The substrate is stabilized via weak alignment and teacher consistency, and training uses question-only replay. AFD achieves state-of-the-art across protocols: 51.6% average accuracy with -1.8% forgetting on domain-incremental VideoQA, ViLCo R@1@0.5 of 29.6% (MQ) and 20.7% (NLQ) with 18.4% stAP@0.25 (VQ), and 39.5% accuracy with -1.6% forgetting on time-incremental iVQA. Overall, AFD offers an explicit, interpretable split between a stable interaction-centered substrate and targeted adaptation.", "AI": {"tldr": "提出了一种新的先验功能分解方法（AFD），实现了视频-语言理解中稳定性与适应性的分离，在各种协议中达到了最先进结果，稳定基底和适应性的分离为理解和解释提供了新的视角。", "motivation": "传统的连续学习方法在处理视频-语言理解时，模糊了稳定性与适应性的界限，依赖静态路由/容量或需要重复过去视频。本研究旨在明确划分稳定性和可塑性的位置，同时满足现实的记忆和隐私约束。", "method": "引入了先验功能分解（AFD）：视频映射到慢变的功能代币，形成共享的、时间对齐的基底，而一个轻量级的、查询导向的、冲突感知的调度器则集中适应并仅在需要时增加容量。基底通过弱对齐和教师一致性来稳定，训练使用问题回放。", "result": "AFD在各种协议中达到最先进的结果：51.6%的平均准确率，0.5的遗忘率为-1.8%的领域增量视频问答；ViLCo R@1@0.5的准确率分别为29.6%（MQ）和20.7%（NLQ），以及18.4%的stAP@0.25（VQ）；39.5%的准确率，0.5的遗忘率为-1.6%的时间增量iVQA。", "conclusion": "AFD提供了一个明确、可解释的分割方案，即稳定的以交互为中心的基底与有针对性的适应。"}}
{"id": "2512.01242", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.01242", "abs": "https://arxiv.org/abs/2512.01242", "authors": ["Zirui Zhao", "Boye Niu", "David Hsu", "Wee Sun Lee"], "title": "Generative Adversarial Gumbel MCTS for Abstract Visual Composition Generation", "comment": null, "summary": "We study abstract visual composition, in which identity is primarily determined by the spatial configuration and relations among a small set of geometric primitives (e.g., parts, symmetry, topology). They are invariant primarily to texture and photorealistic detail. Composing such structures from fixed components under geometric constraints and vague goal specification (such as text) is non-trivial due to combinatorial placement choices, limited data, and discrete feasibility (overlap-free, allowable orientations), which create a sparse solution manifold ill-suited to purely statistical pixel-space generators. We propose a constraint-guided framework that combines explicit geometric reasoning with neural semantics. An AlphaGo-style search enforces feasibility, while a fine-tuned vision-language model scores semantic alignment as reward signals. Our algorithm uses a policy network as a heuristic in Monte-Carlo Tree Search and fine-tunes the network via search-generated plans. Inspired by the Generative Adversarial Network, we use the generated instances for adversarial reward refinement. Over time, the generation should approach the actual data more closely when the reward model cannot distinguish between generated instances and ground-truth. In the Tangram Assembly task, our approach yields higher validity and semantic fidelity than diffusion and auto-regressive baselines, especially as constraints tighten.", "AI": {"tldr": "研究基于几何约束和神经语义的框架，用于生成抽象视觉构图。该框架采用了类似AlphaGo的搜索策略，使用视觉语言模型来评分语义一致性，通过对抗性奖励优化策略网络，并在七巧板组装任务中表现出更好的有效性和语义保真度。", "motivation": "研究抽象视觉构图，在这种构图中，身份主要由几何基元（如部分、对称性、拓扑）的空间配置和关系确定，并且对于纹理和写实细节具有不变性。从固定组件在几何约束和模糊目标规格（如文本）下构建这些结构是一项挑战，因为存在着组合放置选择、数据量有限和离散可行性（无重叠、允许的方向），这些因素导致了解空间稀疏，不适用于纯粹的统计像素空间生成器。", "method": "提出了一种基于约束引导的框架，结合了显式的几何推理与神经语义。该框架采用了类似于AlphaGo的搜索策略来确保可行性，并利用经过微调的视觉语言模型来评分语义一致性作为奖励信号。算法使用了一个作为启发式的策略网络，并通过搜索生成的计划来微调网络。此外，借鉴生成对抗网络的思想，使用生成的实例来进行对抗性奖励优化。", "result": "在七巧板组装任务中，该方法在有效性（validity）和语义保真度（semantic fidelity）方面优于扩散（diffusion）和自回归（auto-regressive）基线，特别是在约束条件收紧时表现更佳。", "conclusion": "通过结合几何约束和神经语义，提出的方法能够在生成抽象视觉构图时保持更好的语义一致性，并随着时间的推移，生成的实例会更加接近实际数据。"}}
{"id": "2512.00700", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.00700", "abs": "https://arxiv.org/abs/2512.00700", "authors": ["Ka Chung Lai", "Ahmet Cetinkaya"], "title": "CAR-Net: A Cascade Refinement Network for Rotational Motion Deblurring under Angle Information Uncertainty", "comment": "Accepted to AAIML 2026", "summary": "We propose a new neural network architecture called CAR-net (CAscade Refinement Network) to deblur images that are subject to rotational motion blur. Our architecture is specifically designed for the semi-blind scenarios where only noisy information of the rotational motion blur angle is available. The core of our approach is progressive refinement process that starts with an initial deblurred estimate obtained from frequency-domain inversion; A series of refinement stages take the current deblurred image to predict and apply residual correction to the current estimate, progressively suppressing artifacts and restoring fine details. To handle parameter uncertainty, our architecture accommodates an optional angle detection module which can be trained end-to-end with refinement modules. We provide a detailed description of our architecture and illustrate its efficiency through experiments using both synthetic and real-life images. Our code and model as well as the links to the datasets are available at https://github.com/tony123105/CAR-Net", "AI": {"tldr": "本文提出了一种新型神经网络架构CAR-net，用于处理存在旋转运动模糊的图像去模糊问题，尤其适用于仅有模糊角度噪声信息的半盲场景。", "motivation": "现有去模糊技术在处理旋转运动模糊时效果不佳，尤其是在缺少精确模糊角度信息的情况下。", "method": "CAR-net采用级联细化网络架构，通过频率域反演获取初始去模糊图像，然后通过多个细化阶段逐步校正和恢复微小细节。同时，为了处理参数不确定性，网络还包含一个可选的角度检测模块，可与细化模块联合训练。", "result": "通过对合成图像和真实图像的实验展示了这种方法的有效性。", "conclusion": "CAR-net在旋转运动模糊图像去模糊问题上表现出色，尤其是在半盲场景下。"}}
{"id": "2512.01707", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.01707", "abs": "https://arxiv.org/abs/2512.01707", "authors": ["Daeun Lee", "Subhojyoti Mukherjee", "Branislav Kveton", "Ryan A. Rossi", "Viet Dac Lai", "Seunghyun Yoon", "Trung Bui", "Franck Dernoncourt", "Mohit Bansal"], "title": "StreamGaze: Gaze-Guided Temporal Reasoning and Proactive Understanding in Streaming Videos", "comment": "Project page: https://streamgaze.github.io/", "summary": "Streaming video understanding requires models not only to process temporally incoming frames, but also to anticipate user intention for realistic applications like AR glasses. While prior streaming benchmarks evaluate temporal reasoning, none measure whether MLLMs can interpret or leverage human gaze signals within a streaming setting. To fill this gap, we introduce StreamGaze, the first benchmark designed to evaluate how effectively MLLMs use gaze for temporal and proactive reasoning in streaming videos. StreamGaze introduces gaze-guided past, present, and proactive tasks that comprehensively evaluate streaming video understanding. These tasks assess whether models can use real-time gaze to follow shifting attention and infer user intentions from only past and currently observed frames. To build StreamGaze, we develop a gaze-video QA generation pipeline that aligns egocentric videos with raw gaze trajectories via fixation extraction, region-specific visual prompting, and scanpath construction. This pipeline produces spatio-temporally grounded QA pairs that closely reflect human perceptual dynamics. Across all StreamGaze tasks, we observe substantial performance gaps between state-of-the-art MLLMs and human performance, revealing fundamental limitations in gaze-based temporal reasoning, intention modeling, and proactive prediction. We further provide detailed analyses of gaze-prompting strategies, reasoning behaviors, and task-specific failure modes, offering deeper insight into why current MLLMs struggle and what capabilities future models must develop. All data and code will be publicly released to support continued research in gaze-guided streaming video understanding.", "AI": {"tldr": "提出了StreamGaze基准测试，评估MLLMs如何在流媒体视频中使用注视信号，揭示了模型在时间推理、意图预测方面的局限性。", "motivation": "现有的流媒体基准测试仅评估了时间推理，但没有评估MLLMs是否能解释或利用注视信号。为弥补这一空白，提出了StreamGaze基准测试。", "method": "引入了StreamGaze基准测试，旨在评估MLLMs如何在流媒体视频中使用注视来进行时间和前瞻性推理。StreamGaze包括注视引导的过去、现在和前瞻任务，这些任务全面评估流媒体视频的理解。开发了一个注视-视频问答生成管道来构建StreamGaze。", "result": "在所有StreamGaze任务中，观察到最先进的MLLMs与人类性能之间存在显著的性能差距，揭示了基于注视的时间推理、意图建模和前瞻性预测的基本局限性。", "conclusion": "提供了详细的注视提示策略、推理行为和任务特定失败模式的分析，深入了解为何当前MLLMs难以胜任，并指出了未来模型需要发展哪些能力。"}}
{"id": "2512.00706", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.00706", "abs": "https://arxiv.org/abs/2512.00706", "authors": ["Chengzhi Yu", "Yifan Xu", "Yifan Chen", "Wenyi Zhang"], "title": "Optimizing LVLMs with On-Policy Data for Effective Hallucination Mitigation", "comment": null, "summary": "Recently, large vision-language models (LVLMs) have risen to be a promising approach for multimodal tasks. However, principled hallucination mitigation remains a critical challenge.In this work, we first analyze the data generation process in LVLM hallucination mitigation and affirm that on-policy data significantly outperforms off-policy data, which thus calls for efficient and reliable preference annotation of on-policy data. We then point out that, existing annotation methods introduce additional hallucination in training samples, which may enhance the model's hallucination patterns, to address this problem, we propose training a hallucination classifier giving binary annotations, which guarantee clean chosen samples for the subsequent alignment. To further harness of the power of on-policy data, we design a robust iterative direct preference optimization (DPO) algorithm adopting a dynamic sample reweighting scheme. We conduct comprehensive experiments on three benchmarks with comparison to 8 state-of-the-art baselines. In particular, our approach reduces the hallucination rate of LLaVA-1.5-7B on MMHalBench by 50.8% and the average hallucination rate on Object HalBench by 79.5%; more significantly, our method fully taps into the potential of open-source models, enabling LLaVA-1.5-13B to even surpass the performance of GPT-4V.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2512.00714", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.00714", "abs": "https://arxiv.org/abs/2512.00714", "authors": ["Emmanuella Avwerosuoghene Oghenekaro"], "title": "Deep Learning-Based Computer Vision Models for Early Cancer Detection Using Multimodal Medical Imaging and Radiogenomic Integration Frameworks", "comment": null, "summary": "Early cancer detection remains one of the most critical challenges in modern healthcare, where delayed diagnosis significantly reduces survival outcomes. Recent advancements in artificial intelligence, particularly deep learning, have enabled transformative progress in medical imaging analysis. Deep learning-based computer vision models, such as convolutional neural networks (CNNs), transformers, and hybrid attention architectures, can automatically extract complex spatial, morphological, and temporal patterns from multimodal imaging data including MRI, CT, PET, mammography, histopathology, and ultrasound. These models surpass traditional radiological assessment by identifying subtle tissue abnormalities and tumor microenvironment variations invisible to the human eye. At a broader scale, the integration of multimodal imaging with radiogenomics linking quantitative imaging features with genomics, transcriptomics, and epigenetic biomarkers has introduced a new paradigm for personalized oncology. This radiogenomic fusion allows the prediction of tumor genotype, immune response, molecular subtypes, and treatment resistance without invasive biopsies.", "AI": {"tldr": "深度学习技术通过自动分析多模态医学成像数据，超越传统的放射学评估，提高早期癌症检测的可能性，并通过放射基因组学为个性化肿瘤学提供了新的范式。", "motivation": "早期癌症检测仍然是现代医疗保健面临的最紧迫挑战之一，而延迟诊断会显著降低生存率。利用深度学习技术可以改善医学成像分析，提高癌症检测的准确性和早期诊断概率。", "method": "此论文主要探讨了深度学习，特别是卷积神经网络（CNN），变压器和混合注意架构如何从多模态成像数据中自动提取复杂的空间、形态和时间模式，从而超越传统的放射学评估。此外，它还讨论了将多模态成像与放射基因组学相结合，以链接定量成像特征和基因组、转录组及表观遗传生物标志物的新方法，为个性化肿瘤学提供预测肿瘤基因型、免疫反应、分子亚型及治疗抵抗的新范式。", "result": "该研究展示了深度学习方法在提取复杂形态学和微环境变化方面的优越性，以及整合放射基因组学信息预测肿瘤特性及治疗反应的能力，提高了早期癌症检测的准确性和可能性。", "conclusion": "深度学习和多模态影像学与放射基因组学融合为早期癌症检测提供了一种非侵入性、高精度和个性化的解决方案，有助于改善癌症患者的生存率和治疗效果。"}}
{"id": "2512.00718", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.00718", "abs": "https://arxiv.org/abs/2512.00718", "authors": ["Deliang Wang", "Peng Liu"], "title": "RS-ISRefiner: Towards Better Adapting Vision Foundation Models for Interactive Segmentation of Remote Sensing Images", "comment": null, "summary": "Interactive image segmentation(IIS) plays a critical role in generating precise annotations for remote sensing imagery, where objects often exhibit scale variations, irregular boundaries and complex backgrounds. However, existing IIS methods, primarily designed for natural images, struggle to generalize to remote sensing domains due to limited annotated data and computational overhead. To address these challenges, we proposed RS-ISRefiner, a novel click-based IIS framework tailored for remote sensing images. The framework employs an adapter-based tuning strategy that preserves the general representations of Vision Foundation Models while enabling efficient learning of remote sensing-specific spatial and boundary characteristics. A hybrid attention mechanism integrating convolutional local modeling with Transformer-based global reasoning enhances robustness against scale diversity and scene complexity. Furthermore, an improved probability map modulation scheme effectively incorporates historical user interactions, yielding more stable iterative refinement and higher boundary fidelity. Comprehensive experiments on six remote sensing datasets, including iSAID, ISPRS Potsdam, SandBar, NWPU, LoveDA Urban and WHUBuilding, demonstrate that RS-ISRefiner consistently outperforms state-of-the-art IIS methods in terms of segmentation accuracy, efficiency and interaction cost. These results confirm the effectiveness and generalizability of our framework, making it highly suitable for high-quality instance segmentation in practical remote sensing scenarios.", "AI": {"tldr": "论文提出了RS-ISRefiner，这是一种新的针对遥感图像设计的交互式图像分割框架，其在多个数据集上表现出色，展现了高效性和准确性。", "motivation": "该论文旨在解决现有交互式图像分割方法在遥感图像上的泛化能力不足问题，主要原因是标注数据有限和计算成本高。", "method": "RS-ISRefiner采用基于点击的交互式图像分割框架，特别针对遥感图像设计。该框架采用适配器调优策略，保留视觉基础模型的一般表示能力，同时有效学习特定于遥感的时空和边界特征。混合注意力机制结合卷积局部建模和Transformer全局推理，增强了对尺度多样性和场景复杂性的鲁棒性。此外，改进的概率图调节方案有效整合了历史用户交互，实现了更加稳定迭代的细化和更高的边界保真度。", "result": "实验结果表明，RS-ISRefiner在六种遥感数据集上始终优于现有交互式图像分割方法，在分割准确性、效率和互动成本方面有显著提升。", "conclusion": "这些结果证明了该框架的有效性和泛化性，使其非常适合实际遥感场景中高质量实例分割任务。"}}
{"id": "2512.00723", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2512.00723", "abs": "https://arxiv.org/abs/2512.00723", "authors": ["Xingtai Gui", "Jianbo Zhao", "Wencheng Han", "Jikai Wang", "Jiahao Gong", "Feiyang Tan", "Cheng-zhong Xu", "Jianbing Shen"], "title": "TrajDiff: End-to-end Autonomous Driving without Perception Annotation", "comment": null, "summary": "End-to-end autonomous driving systems directly generate driving policies from raw sensor inputs. While these systems can extract effective environmental features for planning, relying on auxiliary perception tasks, developing perception annotation-free planning paradigms has become increasingly critical due to the high cost of manual perception annotation. In this work, we propose TrajDiff, a Trajectory-oriented BEV Conditioned Diffusion framework that establishes a fully perception annotation-free generative method for end-to-end autonomous driving. TrajDiff requires only raw sensor inputs and future trajectory, constructing Gaussian BEV heatmap targets that inherently capture driving modalities. We design a simple yet effective trajectory-oriented BEV encoder to extract the TrajBEV feature without perceptual supervision. Furthermore, we introduce Trajectory-oriented BEV Diffusion Transformer (TB-DiT), which leverages ego-state information and the predicted TrajBEV features to directly generate diverse yet plausible trajectories, eliminating the need for handcrafted motion priors. Beyond architectural innovations, TrajDiff enables exploration of data scaling benefits in the annotation-free setting. Evaluated on the NAVSIM benchmark, TrajDiff achieves 87.5 PDMS, establishing state-of-the-art performance among all annotation-free methods. With data scaling, it further improves to 88.5 PDMS, which is comparable to advanced perception-based approaches. Our code and model will be made publicly available.", "AI": {"tldr": "本文提出了TrajDiff，一种端到端自动驾驶的无标注感知方法，实现了无需手动标注的新水平，仅需原始传感器输入和未来轨迹，建立了状态-of-art性能。", "motivation": "端到端自动驾驶系统能够从原始传感器输入中生成驾驶策略，提取有效的环境特征来规划路线。然而，依赖辅助感知任务对于开发无感知标注策略变得至关重要，因为手动感知标注成本高昂。旨在不受感知标注的限制下，探索数据扩展带来的好处。", "method": "本文提出了TrajDiff，一种基于轨迹的BEV条件扩散框架，它是一种完全无感知标注的生成方法，用于端到端自动驾驶。TrajDiff仅需要原始传感器输入和未来轨迹来构建高斯BEV热图目标，这些目标内在地捕捉驾驶模式。设计了一个简单的轨迹导向的BEV编码器来提取TrajBEV特征，摒弃感知监督。此外，还引入了轨迹导向的BEV扩散Transformer (TB-DiT)，该模型利用自我状态信息和预测的TrajBEV特征直接生成多样化但合理的轨迹，消除了对手动设计的运动先验的需求。", "result": "在NAVSIM基准上测试，TrajDiff达到了87.5 PDMS，在所有无标注感知方法中处于前列。通过数据扩展，这一成绩提升至88.5 PDMS，与先进的基于感知的方法相当。", "conclusion": "TrajDiff证明了在没有感知标注的情况下，通过巧妙架构设计和充分利用数据扩展，可以达到与先进系统媲美的表现，取得了无感知标注方法中的最佳成绩，并将代码和模型公开。"}}
{"id": "2512.00743", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.00743", "abs": "https://arxiv.org/abs/2512.00743", "authors": ["Qiang Lyu", "Zicong Chen", "Chongxiao Wang", "Haolin Shi", "Shibo Gao", "Ran Piao", "Youwei Zeng", "Jianlou Si", "Fei Ding", "Jing Li", "Chun Pong Lau", "Weiqiang Wang"], "title": "Multi-GRPO: Multi-Group Advantage Estimation for Text-to-Image Generation with Tree-Based Trajectories and Multiple Rewards", "comment": "20 pages, 15 figures", "summary": "Recently, Group Relative Policy Optimization (GRPO) has shown promising potential for aligning text-to-image (T2I) models, yet existing GRPO-based methods suffer from two critical limitations. (1) \\textit{Shared credit assignment}: trajectory-level advantages derived from group-normalized sparse terminal rewards are uniformly applied across timesteps, failing to accurately estimate the potential of early denoising steps with vast exploration spaces. (2) \\textit{Reward-mixing}: predefined weights for combining multi-objective rewards (e.g., text accuracy, visual quality, text color)--which have mismatched scales and variances--lead to unstable gradients and conflicting updates. To address these issues, we propose \\textbf{Multi-GRPO}, a multi-group advantage estimation framework with two orthogonal grouping mechanisms. For better credit assignment, we introduce tree-based trajectories inspired by Monte Carlo Tree Search: branching trajectories at selected early denoising steps naturally forms \\emph{temporal groups}, enabling accurate advantage estimation for early steps via descendant leaves while amortizing computation through shared prefixes. For multi-objective optimization, we introduce \\emph{reward-based grouping} to compute advantages for each reward function \\textit{independently} before aggregation, disentangling conflicting signals. To facilitate evaluation of multiple objective alignment, we curate \\textit{OCR-Color-10}, a visual text rendering dataset with explicit color constraints. Across the single-reward \\textit{PickScore-25k} and multi-objective \\textit{OCR-Color-10} benchmarks, Multi-GRPO achieves superior stability and alignment performance, effectively balancing conflicting objectives. Code will be publicly available at \\href{https://github.com/fikry102/Multi-GRPO}{https://github.com/fikry102/Multi-GRPO}.", "AI": {"tldr": "Multi-GRPO提出了一种多组优势估计框架，解决了现有的GRPO方法中存在的共享信用分配和混合奖励问题，提高了模型的稳定性和平衡多个目标的能力。", "motivation": "针对现有的GRPO方法存在的共享信用分配不准确以及混合不同尺度和变异的多目标奖励导致不稳定梯度的问题，提出了新的解决方案。", "method": "Multi-GRPO引入了树状轨迹和基于奖励的分组机制，以提高信用分配的准确性并独立计算各个奖励函数的优势。", "result": "在单奖励和多目标基准测试中，Multi-GRPO实现了更好的稳定性和目标对齐性能。", "conclusion": "Multi-GRPO能有效解决现有GRPO方法的局限性，改进了在强化学习中的性能，特别是在处理多目标对齐任务中。"}}
{"id": "2512.00744", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.00744", "abs": "https://arxiv.org/abs/2512.00744", "authors": ["Zhengxin Chen", "Xiaohai He", "Tingrong Zhang", "Shuhua Xiong", "Chao Ren"], "title": "Joint Multi-scale Gated Transformer and Prior-guided Convolutional Network for Learned Image Compression", "comment": null, "summary": "Recently, learned image compression methods have made remarkable achievements, some of which have outperformed the traditional image codec VVC. The advantages of learned image compression methods over traditional image codecs can be largely attributed to their powerful nonlinear transform coding. Convolutional layers and shifted window transformer (Swin-T) blocks are the basic units of neural networks, and their representation capabilities play an important role in nonlinear transform coding. In this paper, to improve the ability of the vanilla convolution to extract local features, we propose a novel prior-guided convolution (PGConv), where asymmetric convolutions (AConvs) and difference convolutions (DConvs) are introduced to strengthen skeleton elements and extract high-frequency information, respectively. A re-parameterization strategy is also used to reduce the computational complexity of PGConv. Moreover, to improve the ability of the Swin-T block to extract non-local features, we propose a novel multi-scale gated transformer (MGT), where dilated window-based multi-head self-attention blocks with different dilation rates and depth-wise convolution layers with different kernel sizes are used to extract multi-scale features, and a gate mechanism is introduced to enhance non-linearity. Finally, we propose a novel joint Multi-scale Gated Transformer and Prior-guided Convolutional Network (MGTPCN) for learned image compression. Experimental results show that our MGTPCN surpasses state-of-the-art algorithms with a better trade-off between performance and complexity.", "AI": {"tldr": "本文提出了一种新的图像压缩方法MGTPCN，通过改进局部和非局部特征提取能力，实现了优于现有算法的性能和复杂度平衡。", "motivation": "传统图像编码方法在非线性转换编码方面表现欠佳，而学习图像压缩方法因其强大的非线性转换编码能力表现出色。本研究旨在通过改进局部和非局部特征的提取能力，进一步提升学习图像压缩方法的性能。", "method": "本研究提出了一种新的先验引导卷积（PGConv），增强了传统卷积对局部特征的提取能力，并引入了非对称卷积和差异卷积来分别强化骨架元素和提取高频信息。此外，为了提高Swin-T块对非局部特征的提取能力，提出了一种新的多尺度门控变换器（MGT），使用具有不同膨胀率的膨胀窗口多头自注意力块和具有不同核大小的深度卷积层来提取多尺度特征，并引入了门控机制来增强非线性。最后，提出了一种新的联合多尺度门控变换器和先验引导卷积网络（MGTPCN），用于学习图像压缩。", "result": "实验结果表明，所提出的MGTPCN比现有最先进算法在性能和复杂度之间取得了更好的权衡。", "conclusion": "本研究证明了PGConv和MGT的引入能有效提升学习图像压缩方法的性能，为图像压缩技术的发展提供了新的思路和技术支持。"}}
{"id": "2512.00748", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.00748", "abs": "https://arxiv.org/abs/2512.00748", "authors": ["Ke Liu", "Shangde Gao", "Yichao Fu", "Shangqi Gao", "Chunhua Shen"], "title": "Probabilistic Modeling of Multi-rater Medical Image Segmentation for Diversity and Personalization", "comment": null, "summary": "Medical image segmentation is inherently influenced by data uncertainty, arising from ambiguous boundaries in medical scans and inter-observer variability in diagnosis. To address this challenge, previous works formulated the multi-rater medical image segmentation task, where multiple experts provide separate annotations for each image. However, existing models are typically constrained to either generate diverse segmentation that lacks expert specificity or to produce personalized outputs that merely replicate individual annotators. We propose Probabilistic modeling of multi-rater medical image Segmentation (ProSeg) that simultaneously enables both diversification and personalization. Specifically, we introduce two latent variables to model expert annotation preferences and image boundary ambiguity. Their conditional probabilistic distributions are then obtained through variational inference, allowing segmentation outputs to be generated by sampling from these distributions. Extensive experiments on both the nasopharyngeal carcinoma dataset (NPC) and the lung nodule dataset (LIDC-IDRI) demonstrate that our ProSeg achieves a new state-of-the-art performance, providing segmentation results that are both diverse and expert-personalized. Code can be found in https://github.com/AI4MOL/ProSeg.", "AI": {"tldr": "提出ProSeg解决医学图像分割的不确定性问题，实现了多样化和个性化分割结果的新方法。", "motivation": "为了解决医学图像分割中由于医学扫描边界模糊和医生诊断的观察者间差异产生的数据不确定性问题。", "method": "提出了一种多注释医生的医学图像分割概率模型ProSeg，引入两个潜在变量来建模医生注释偏好和图像边界模糊度。通过变分推理获得这些变量的条件概率分布，从而在这些分布中采样生成分割结果。", "result": "在鼻咽癌和肺结节数据集上实现了新的最先进的性能，提供既多样又个性化专家的分割结果。", "conclusion": "ProSeg能够在新的医学图像分割任务中，同时实现多样化和个性化专家分割输出，为解决医学图像分割中的不确定性提供了一个有效的解决方案。"}}
{"id": "2512.00752", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.00752", "abs": "https://arxiv.org/abs/2512.00752", "authors": ["Shawn Li", "Ryan Rossi", "Sungchul Kim", "Sunav Choudhary", "Franck Dernoncourt", "Puneet Mathur", "Zhengzhong Tu", "Yue Zhao"], "title": "Charts Are Not Images: On the Challenges of Scientific Chart Editing", "comment": null, "summary": "Generative models, such as diffusion and autoregressive approaches, have demonstrated impressive capabilities in editing natural images. However, applying these tools to scientific charts rests on a flawed assumption: a chart is not merely an arrangement of pixels but a visual representation of structured data governed by a graphical grammar. Consequently, chart editing is not a pixel-manipulation task but a structured transformation problem. To address this fundamental mismatch, we introduce \\textit{FigEdit}, a large-scale benchmark for scientific figure editing comprising over 30,000 samples. Grounded in real-world data, our benchmark is distinguished by its diversity, covering 10 distinct chart types and a rich vocabulary of complex editing instructions. The benchmark is organized into five distinct and progressively challenging tasks: single edits, multi edits, conversational edits, visual-guidance-based edits, and style transfer. Our evaluation of a range of state-of-the-art models on this benchmark reveals their poor performance on scientific figures, as they consistently fail to handle the underlying structured transformations required for valid edits. Furthermore, our analysis indicates that traditional evaluation metrics (e.g., SSIM, PSNR) have limitations in capturing the semantic correctness of chart edits. Our benchmark demonstrates the profound limitations of pixel-level manipulation and provides a robust foundation for developing and evaluating future structure-aware models. By releasing \\textit{FigEdit} (https://github.com/adobe-research/figure-editing), we aim to enable systematic progress in structure-aware figure editing, provide a common ground for fair comparison, and encourage future research on models that understand both the visual and semantic layers of scientific charts.", "AI": {"tldr": "该论文介绍了FigEdit，这是一个针对科学图表编辑的大规模基准数据集，包含超过30,000个样本，并揭示了现有模型在结构化图表编辑任务中的性能不佳，强调了开发理解图表结构和语义层的模型的重要性。", "motivation": "为了应对现有生成模型在科学图表编辑任务中无法有效执行结构化转换的缺点，作者创建了FigEdit基准数据集。", "method": "作者构建了一个大规模的科学图表编辑基准数据集FigEdit，其中包括10种不同类型的图表和丰富的编辑指令，涵盖五种不同难度的编辑任务。", "result": "评估结果显示，现有的状态模型在对FigEdit中的科学图表进行编辑时性能较差，且传统的评估指标如SSIM, PSNR不适合评估图表编辑的语义准确性。", "conclusion": "FigEdit基准数据集揭示了像素级操作的局限性，并为开发和评估未来的结构感知模型提供了坚实的基础，推动结构化图表编辑的系统性进展。"}}
{"id": "2512.00762", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.00762", "abs": "https://arxiv.org/abs/2512.00762", "authors": ["Zhiyuan Gao", "Jiageng Mao", "Hong-Xing Yu", "Haozhe Lou", "Emily Yue-Ting Jia", "Jernej Barbic", "Jiajun Wu", "Yue Wang"], "title": "Seeing the Wind from a Falling Leaf", "comment": "Accepted at NeurIPS 2025", "summary": "A longstanding goal in computer vision is to model motions from videos, while the representations behind motions, i.e. the invisible physical interactions that cause objects to deform and move, remain largely unexplored. In this paper, we study how to recover the invisible forces from visual observations, e.g., estimating the wind field by observing a leaf falling to the ground. Our key innovation is an end-to-end differentiable inverse graphics framework, which jointly models object geometry, physical properties, and interactions directly from videos. Through backpropagation, our approach enables the recovery of force representations from object motions. We validate our method on both synthetic and real-world scenarios, and the results demonstrate its ability to infer plausible force fields from videos. Furthermore, we show the potential applications of our approach, including physics-based video generation and editing. We hope our approach sheds light on understanding and modeling the physical process behind pixels, bridging the gap between vision and physics. Please check more video results in our \\href{https://chaoren2357.github.io/seeingthewind/}{project page}.", "AI": {"tldr": "本研究提出一种从视频中恢复不可见力的方法，展示了在多个场景中的有效性，并提出了一些实际应用。", "motivation": "研究如何通过视觉观察恢复不可见力，例如通过观察叶子落地来估算风场。", "method": "提出了一种端到端可微的逆图形框架，该框架可以直接从视频中联合建模物体几何、物理特性和互动。", "result": "在合成和现实场景中验证了方法的有效性，展示了该方法能从视频中推理出合理的力场，并展示了视频生成和编辑等潜在应用。", "conclusion": "该方法有效地结合了视觉和物理过程的理解，为以像素为背景的物理过程建模提供了新的视角。"}}
{"id": "2512.00765", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.00765", "abs": "https://arxiv.org/abs/2512.00765", "authors": ["Haojie Jia", "Te Hu", "Haowen Li", "Long Jin", "Chongshi Xin", "Yuchi Yao", "Jiarui Xiao"], "title": "The Outline of Deception: Physical Adversarial Attacks on Traffic Signs Using Edge Patches", "comment": null, "summary": "Intelligent driving systems are vulnerable to physical adversarial attacks on traffic signs. These attacks can cause misclassification, leading to erroneous driving decisions that compromise road safety. Moreover, within V2X networks, such misinterpretations can propagate, inducing cascading failures that disrupt overall traffic flow and system stability. However, a key limitation of current physical attacks is their lack of stealth. Most methods apply perturbations to central regions of the sign, resulting in visually salient patterns that are easily detectable by human observers, thereby limiting their real-world practicality. This study proposes TESP-Attack, a novel stealth-aware adversarial patch method for traffic sign classification. Based on the observation that human visual attention primarily focuses on the central regions of traffic signs, we employ instance segmentation to generate edge-aligned masks that conform to the shape characteristics of the signs. A U-Net generator is utilized to craft adversarial patches, which are then optimized through color and texture constraints along with frequency domain analysis to achieve seamless integration with the background environment, resulting in highly effective visual concealment. The proposed method demonstrates outstanding attack success rates across traffic sign classification models with varied architectures, achieving over 90% under limited query budgets. It also exhibits strong cross-model transferability and maintains robust real-world performance that remains stable under varying angles and distances.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2512.00771", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.00771", "abs": "https://arxiv.org/abs/2512.00771", "authors": ["Xiaoshan Wu", "Yifei Yu", "Xiaoyang Lyu", "Yihua Huang", "Bo Wang", "Baoheng Zhang", "Zhongrui Wang", "Xiaojuan Qi"], "title": "EAG3R: Event-Augmented 3D Geometry Estimation for Dynamic and Extreme-Lighting Scenes", "comment": "Accepted at NeurIPS 2025 (spotlight)", "summary": "Robust 3D geometry estimation from videos is critical for applications such as autonomous navigation, SLAM, and 3D scene reconstruction. Recent methods like DUSt3R demonstrate that regressing dense pointmaps from image pairs enables accurate and efficient pose-free reconstruction. However, existing RGB-only approaches struggle under real-world conditions involving dynamic objects and extreme illumination, due to the inherent limitations of conventional cameras. In this paper, we propose EAG3R, a novel geometry estimation framework that augments pointmap-based reconstruction with asynchronous event streams. Built upon the MonST3R backbone, EAG3R introduces two key innovations: (1) a retinex-inspired image enhancement module and a lightweight event adapter with SNR-aware fusion mechanism that adaptively combines RGB and event features based on local reliability; and (2) a novel event-based photometric consistency loss that reinforces spatiotemporal coherence during global optimization. Our method enables robust geometry estimation in challenging dynamic low-light scenes without requiring retraining on night-time data. Extensive experiments demonstrate that EAG3R significantly outperforms state-of-the-art RGB-only baselines across monocular depth estimation, camera pose tracking, and dynamic reconstruction tasks.", "AI": {"tldr": "EAG3R is proposed to enhance the accuracy and robustness of 3D geometry estimation, particularly in challenging dynamic and low-light scenes, by leveraging asynchronous event streams with an RGB camera.", "motivation": "The motivation stems from the need for improving the robustness of 3D geometry estimation in real-world conditions involving dynamic objects and extreme illumination, where current RGB-only methods face significant challenges.", "method": "The method introduces EAG3R, which augments pointmap-based 3D geometry estimation with asynchronous event streams. It includes a retinex-inspired image enhancement module and a lightweight event adapter with a signal-to-noise-ratio (SNR)-aware fusion mechanism. Additionally, it proposes an event-based photometric consistency loss.", "result": "The EAG3R system achieves superior performance when compared to leading RGB-only methods in tasks such as camera pose tracking and 3D scene reconstruction, even in challenging dynamic and low-light environments.", "conclusion": "The paper concludes that EAG3R, through its use of event data and adaptive RGB-event feature fusion, can significantly improve the performance of 3D geometry estimation systems in real-world conditions, even without retraining for night-time scenarios."}}
{"id": "2512.00773", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.00773", "abs": "https://arxiv.org/abs/2512.00773", "authors": ["Toshiki Katsube", "Taiga Fukuhara", "Kenichiro Ando", "Yusuke Mukuta", "Kohei Uehara", "Tatsuya Harada"], "title": "DEJIMA: A Novel Large-scale Japanese Dataset for Image Captioning and Visual Question Answering", "comment": null, "summary": "This work addresses the scarcity of high-quality, large-scale resources for Japanese Vision-and-Language (V&L) modeling. We present a scalable and reproducible pipeline that integrates large-scale web collection with rigorous filtering/deduplication, object-detection-driven evidence extraction, and Large Language Model (LLM)-based refinement under grounding constraints. Using this pipeline, we build two resources: an image-caption dataset (DEJIMA-Cap) and a VQA dataset (DEJIMA-VQA), each containing 3.88M image-text pairs, far exceeding the size of existing Japanese V&L datasets. Human evaluations demonstrate that DEJIMA achieves substantially higher Japaneseness and linguistic naturalness than datasets constructed via translation or manual annotation, while maintaining factual correctness at a level comparable to human-annotated corpora. Quantitative analyses of image feature distributions further confirm that DEJIMA broadly covers diverse visual domains characteristic of Japan, complementing its linguistic and cultural representativeness. Models trained on DEJIMA exhibit consistent improvements across multiple Japanese multimodal benchmarks, confirming that culturally grounded, large-scale resources play a key role in enhancing model performance. All data sources and modules in our pipeline are licensed for commercial use, and we publicly release the resulting dataset and metadata to encourage further research and industrial applications in Japanese V&L modeling.", "AI": {"tldr": "A scalable pipeline was developed to generate large-scale Japanese V&L datasets, with two resulting datasets exceeding existing datasets in size and cultural relevance, leading to improved model performance.", "motivation": "The motivation behind this work is to address the scarcity of large-scale, high-quality Japanese V&L datasets, which are crucial for the development of robust models in this space.", "method": "This paper introduces a scalable pipeline for creating large-scale Japanese Vision-and-Language (V&L) resources. The pipeline consists of web collection, rigorous filtering, object detection, and refinement using Large Language Models under grounding constraints.", "result": "Two datasets, DEJIMA-Cap and DEJIMA-VQA, were created with over 3.88M image-caption and VQA pairs respectively. These datasets have been found to be more culturally relevant, linguistically natural, and factually accurate compared to other methods.", "conclusion": "Culturally grounded, large-scale datasets are shown to significantly enhance model performance on Japanese multimodal benchmarks. The datasets are made publicly available for commercial use, encouraging further research and industrial applications."}}
{"id": "2512.00794", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.00794", "abs": "https://arxiv.org/abs/2512.00794", "authors": ["Bo Guo", "Sijia Wen", "Yifan Zhao", "Jia Li", "Zhiming Zheng"], "title": "PolarGS: Polarimetric Cues for Ambiguity-Free Gaussian Splatting with Accurate Geometry Recovery", "comment": null, "summary": "Recent advances in surface reconstruction for 3D Gaussian Splatting (3DGS) have enabled remarkable geometric accuracy. However, their performance degrades in photometrically ambiguous regions such as reflective and textureless surfaces, where unreliable cues disrupt photometric consistency and hinder accurate geometry estimation. Reflected light is often partially polarized in a manner that reveals surface orientation, making polarization an optic complement to photometric cues in resolving such ambiguities. Therefore, we propose PolarGS, an optics-aware extension of RGB-based 3DGS that leverages polarization as an optical prior to resolve photometric ambiguities and enhance reconstruction accuracy. Specifically, we introduce two complementary modules: a polarization-guided photometric correction strategy, which ensures photometric consistency by identifying reflective regions via the Degree of Linear Polarization (DoLP) and refining reflective Gaussians with Color Refinement Maps; and a polarization-enhanced Gaussian densification mechanism for textureless area geometry recovery, which integrates both Angle and Degree of Linear Polarization (A/DoLP) into a PatchMatch-based depth completion process. This enables the back-projection and fusion of new Gaussians, leading to more complete reconstruction. PolarGS is framework-agnostic and achieves superior geometric accuracy compared to state-of-the-art methods.", "AI": {"tldr": "PolarGS通过添加光学意识扩展了RGB基础的3DGS模型，利用偏振信息解决重建中的光度学模糊问题，并显著提升重建准确性。", "motivation": "最近3DGS在表面重建方面的进展取得了几何精度上的突破。但在光度学模糊区域，如反射和无纹理表面，表现不佳。反射光是部分偏振的，会揭示表面方向信息。因此，提出PolarGS来解决这些问题。", "method": "PolarGS利用光学意识扩展RGB基础的3DGS，通过利用极化作为光学先验解决光度学模糊区域并增强重建准确性。具体来说，引入了两个互补的模块：极化引导的光度校正策略和极化增强的高斯稠密化机制。", "result": "PolarGS获得了比现有最先进方法更优的几何精度。", "conclusion": "PolarGS通过在光度学模糊区域引入偏振如何帮助解决这些问题，并证明了这一方法的有效性。"}}
{"id": "2512.00796", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.00796", "abs": "https://arxiv.org/abs/2512.00796", "authors": ["Jiajian He", "Enjie Hu", "Shiqi Chen", "Tianchen Qiu", "Huajun Feng", "Zhihai Xu", "Yueting Chen"], "title": "CircleFlow: Flow-Guided Camera Blur Estimation using a Circle Grid Target", "comment": null, "summary": "The point spread function (PSF) serves as a fundamental descriptor linking the real-world scene to the captured signal, manifesting as camera blur. Accurate PSF estimation is crucial for both optical characterization and computational vision, yet remains challenging due to the inherent ambiguity and the ill-posed nature of intensity-based deconvolution. We introduce CircleFlow, a high-fidelity PSF estimation framework that employs flow-guided edge localization for precise blur characterization. CircleFlow begins with a structured capture that encodes locally anisotropic and spatially varying PSFs by imaging a circle grid target, while leveraging the target's binary luminance prior to decouple image and kernel estimation. The latent sharp image is then reconstructed through subpixel alignment of an initialized binary structure guided by optical flow, whereas the PSF is modeled as an energy-constrained implicit neural representation. Both components are jointly optimized within a demosaicing-aware differentiable framework, ensuring physically consistent and robust PSF estimation enabled by accurate edge localization. Extensive experiments on simulated and real-world data demonstrate that CircleFlow achieves state-of-the-art accuracy and reliability, validating its effectiveness for practical PSF calibration.", "AI": {"tldr": "论文提出了CircleFlow框架用于精确PSF估计，利用流动引导的边缘定位技术实现了高质量的PSF校准。", "motivation": "鉴于基于强度的脱卷积固有的模糊性和不适定性，准确的PSF估计仍然具有挑战性，但是对于光学特性和计算视觉来说是至关重要的。", "method": "介绍了一种名为CircleFlow的高保真PSF估计框架，该框架通过流动引导的边缘定位进行精确的模糊特征化。CircleFlow以结构化捕获开始，通过成像圆网格目标来编码局部各向异性且空间变化的PSF，并利用该目标的二值亮度先验解耦图像和内核估计。通过基于光流引导的初始化二值结构的亚像素对齐来重建潜在的清晰图像，而PSF则被建模为能量约束的隐式神经表示。这两部分在一个去马赛克感知的可微框架内被联合优化，确保通过精确的边缘定位使PSF估计具有物理一致性和鲁棒性。", "result": "在模拟和现实世界数据上的广泛实验表明，CircleFlow在PSF校准的实际应用中实现了最先进的准确性和可靠性。", "conclusion": "CircleFlow通过精确的边缘定位提供了物理一致性和鲁棒性的PSF估计，实现了最先进的准确性和可靠性，验证了其在实际应用中的有效性。"}}
{"id": "2512.00805", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.00805", "abs": "https://arxiv.org/abs/2512.00805", "authors": ["Pengfei Hu", "Meng Cao", "Yingyao Wang", "Yi Wang", "Jiahua Dong", "Jun Song", "Yu Cheng", "Bo Zheng", "Xiaodan Liang"], "title": "Thinking with Drafts: Speculative Temporal Reasoning for Efficient Long Video Understanding", "comment": null, "summary": "Long video understanding is essential for human-like intelligence, enabling coherent perception and reasoning over extended temporal contexts. While the emerging thinking-with-frames paradigm, which alternates between global temporal reasoning and local frame examination, has advanced the reasoning capabilities of video multi-modal large language models (MLLMs), it suffers from a significant efficiency bottleneck due to the progressively growing and redundant multi-modal context. To address this, we propose SpecTemp, a reinforcement learning-based Speculative Temporal reasoning framework that decouples temporal perception from reasoning via a cooperative dual-model design. In SpecTemp, a lightweight draft MLLM rapidly explores and proposes salient frames from densely sampled temporal regions, while a powerful target MLLM focuses on temporal reasoning and verifies the draft's proposals, iteratively refining its attention until convergence. This design mirrors the collaborative pathways of the human brain, balancing efficiency with accuracy. To support training, we construct the SpecTemp-80K dataset, featuring synchronized dual-level annotations for coarse evidence spans and fine-grained frame-level evidence. Experiments across multiple video understanding benchmarks demonstrate that SpecTemp not only maintains competitive accuracy but also significantly accelerates inference compared with existing thinking-with-frames methods.", "AI": {"tldr": "提出了SpecTemp框架以解决视频多模态大型语言模型推理时的效率问题，同时保持或提升了准确性。", "motivation": "长视频理解对于类似人类的智能至关重要，尽管与帧结合的思考范式已经提高了视频多模态大型语言模型的推理能力，但其效率瓶颈严重影响了性能。", "method": "SpecTemp, 一种基于强化学习的推测性时间推理框架，通过合作的双模型设计将时间感知与推理解耦。在这种设计中，轻量级草稿MLLM从密集采样的时间区域内快速探索并提出显著帧，而强大的目标MLLM专注于时间推理，验证草稿的提议，并迭代优化其注意力直到收敛。", "result": "实验结果显示，在多个视频理解基准测试中，SpecTemp不仅保持了较高的准确性，而且显著加速了推理过程。", "conclusion": "SpecTemp框架有效地平衡了效率与准确性，模仿了人脑的合作通路，提高了视频多模态大型语言模型的时间推理能力。"}}
