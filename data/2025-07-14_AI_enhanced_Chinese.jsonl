{"id": "2507.08022", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.08022", "abs": "https://arxiv.org/abs/2507.08022", "authors": ["Hayato Tanoue", "Hiroki Nishihara", "Yuma Suzuki", "Takayuki Hori", "Hiroki Takushima", "Aiswariya Manojkumar", "Yuki Shibata", "Mitsuru Takeda", "Fumika Beppu", "Zhao Hengwei", "Yuto Kanda", "Daichi Yamaga"], "title": "CuriosAI Submission to the EgoExo4D Proficiency Estimation Challenge 2025", "comment": "The 2nd place solution for the EgoExo4D Proficiency Estimation\n  Challenge at the CVPR EgoVis Workshop 2025", "summary": "This report presents the CuriosAI team's submission to the EgoExo4D\nProficiency Estimation Challenge at CVPR 2025. We propose two methods for\nmulti-view skill assessment: (1) a multi-task learning framework using\nSapiens-2B that jointly predicts proficiency and scenario labels (43.6 %\naccuracy), and (2) a two-stage pipeline combining zero-shot scenario\nrecognition with view-specific VideoMAE classifiers (47.8 % accuracy). The\nsuperior performance of the two-stage approach demonstrates the effectiveness\nof scenario-conditioned modeling for proficiency estimation.", "AI": {"tldr": "研究提出了两种方法来改进多视角技能评估，其中两阶段方法表现更优，显示了利用场景信息对技能水平进行评估的有效性。", "motivation": "研究动机是改进多视角技能评估的方法，特别是在EgoExo4D技能评估挑战赛上的表现。", "method": "此研究提出了两种多视角技能评估方法：(1) 一种使用Sapiens-2B的多任务学习框架，该框架共同预测技能水平和场景标签（准确率为43.6%），以及(2) 一个两阶段流水线，该流水线结合了零样本场景识别和特定视角的VideoMAE分类器（准确率为47.8%）.", "result": "两阶段方法表现更好，展示了场景条件建模在技能水平评估中的有效性。", "conclusion": "研究结果支持使用两阶段流水线和场景条件建模来改进多视角技能评估。"}}
{"id": "2507.08024", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.08024", "abs": "https://arxiv.org/abs/2507.08024", "authors": ["Mihir Gupta", "Abhay Mangla", "Ross Greer", "Pratik Desai"], "title": "Self-Consistency in Vision-Language Models for Precision Agriculture: Multi-Response Consensus for Crop Disease Management", "comment": null, "summary": "Precision agriculture relies heavily on accurate image analysis for crop\ndisease identification and treatment recommendation, yet existing\nvision-language models (VLMs) often underperform in specialized agricultural\ndomains. This work presents a domain-aware framework for agricultural image\nprocessing that combines prompt-based expert evaluation with self-consistency\nmechanisms to enhance VLM reliability in precision agriculture applications. We\nintroduce two key innovations: (1) a prompt-based evaluation protocol that\nconfigures a language model as an expert plant pathologist for scalable\nassessment of image analysis outputs, and (2) a cosine-consistency self-voting\nmechanism that generates multiple candidate responses from agricultural images\nand selects the most semantically coherent diagnosis using domain-adapted\nembeddings. Applied to maize leaf disease identification from field images\nusing a fine-tuned PaliGemma model, our approach improves diagnostic accuracy\nfrom 82.2\\% to 87.8\\%, symptom analysis from 38.9\\% to 52.2\\%, and treatment\nrecommendation from 27.8\\% to 43.3\\% compared to standard greedy decoding. The\nsystem remains compact enough for deployment on mobile devices, supporting\nreal-time agricultural decision-making in resource-constrained environments.\nThese results demonstrate significant potential for AI-driven precision\nagriculture tools that can operate reliably in diverse field conditions.", "AI": {"tldr": "本文提出了一种农业领域图像处理框架，显著提升了视觉语言模型在作物病害识别和治疗建议方面的准确性，支持在资源受限的野外环境中实时决策。", "motivation": "鉴于现有视觉语言模型（VLMs）在农业领域应用表现不佳，本文旨在开发一种专注于农业领域的方法，以提高作物病害识别及治疗建议的准确性。", "method": "本文提出了一种针对农业领域的图像处理框架，结合基于prompt的专家评估和自我一致性机制，旨在提升视觉语言模型（VLMs）在精准农业应用中的可靠性。该框架包含两大创新点：1. 基于prompt的评估协议，将语言模型配置为专家级植物病理学家，对图像分析结果进行规模化评估；2. 余弦一致性自我投票机制，通过生成多个对农业图像的候选诊断答案，使用领域自适应嵌入选择语义最连贯的诊断。", "result": "应用PaliGemma模型（经过微调）对玉米叶片病害进行识别，与标准贪婪解码相比，本文提出的方法将诊断准确率从82.2%提高到87.8%，症状分析准确率从38.9%提升至52.2%，治疗建议准确率从27.8%提高到43.3%。", "conclusion": "实验结果表明，基于本文提出框架的AI驱动精确农业工具不仅能够胜任野外条件下工作，还具备在资源受限环境下实时决策支持的能力，具有在精准农业中广泛应用的潜力。"}}
{"id": "2507.08026", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.08026", "abs": "https://arxiv.org/abs/2507.08026", "authors": ["Jennifer P. T. Nguyen"], "title": "Development of a Canada-Wide Morphology Map for the ITU-R P. 1411 Propagation Model", "comment": null, "summary": "This paper outlines the development of a Canada-wide morphology map\nclassifying regions into residential, urban low-rise, and urban high-rise\nenvironments, following the ITU-R P.1411-12 propagation model guidelines. To\naddress the qualitative nature of the environment-type descriptors found in the\nRecommendation, a machine learning approach is employed to automate the\nclassification process. Extensive experimentation optimized classification\naccuracy, resulting in a Canada-wide morphology map that ensures more accurate\npath loss estimations for outdoor short-range propagation at frequencies\nranging from 300 MHz to 100 GHz.", "AI": {"tldr": "本文概述了加全国形态图的发展，该图是通过机器学习方法实现的，用于优化路径损耗估计。", "motivation": "为了使环境类型描述符（在推荐中找到的）定量化，从而提高分类精度，实现加拿大全国的形态图，以确保更精确的户外短距离传播路径损耗估计。", "method": "采用机器学习方法来自动化分类过程，依据ITU-R P.1411-12传播模型指南，将加拿大划分为不同区域，包括住宅区、城市低层建筑区及城市高层建筑区。", "result": "通过广泛的实验优化了分类精度，生成了加拿大全国的形态图，从而提高了300 MHz至100 GHz频率范围内的户外短距离传播路径损耗估计的准确性。", "conclusion": "该机器学习方法能够确保在全国范围内提供更准确的路径损耗估计，适用于频率在300 MHz至100 GHz之间的户外短距离传播。"}}
{"id": "2507.08039", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.08039", "abs": "https://arxiv.org/abs/2507.08039", "authors": ["Sujith Vemishetty", "Advitiya Arora", "Anupama Sharma"], "title": "Towards Evaluating Robustness of Prompt Adherence in Text to Image Models", "comment": null, "summary": "The advancements in the domain of LLMs in recent years have surprised many,\nshowcasing their remarkable capabilities and diverse applications. Their\npotential applications in various real-world scenarios have led to significant\nresearch on their reliability and effectiveness. On the other hand, multimodal\nLLMs and Text-to-Image models have only recently gained prominence, especially\nwhen compared to text-only LLMs. Their reliability remains constrained due to\ninsufficient research on assessing their performance and robustness. This paper\naims to establish a comprehensive evaluation framework for Text-to-Image\nmodels, concentrating particularly on their adherence to prompts. We created a\nnovel dataset that aimed to assess the robustness of these models in generating\nimages that conform to the specified factors of variation in the input text\nprompts. Our evaluation studies present findings on three variants of Stable\nDiffusion models: Stable Diffusion 3 Medium, Stable Diffusion 3.5 Large, and\nStable Diffusion 3.5 Large Turbo, and two variants of Janus models: Janus Pro\n1B and Janus Pro 7B. We introduce a pipeline that leverages text descriptions\ngenerated by the gpt-4o model for our ground-truth images, which are then used\nto generate artificial images by passing these descriptions to the\nText-to-Image models. We then pass these generated images again through gpt-4o\nusing the same system prompt and compare the variation between the two\ndescriptions. Our results reveal that these models struggle to create simple\nbinary images with only two factors of variation: a simple geometric shape and\nits location. We also show, using pre-trained VAEs on our dataset, that they\nfail to generate images that follow our input dataset distribution.", "AI": {"tldr": "本文旨在解决Text-to-Image模型评估不足的问题，通过设计新的评估框架和创建数据集来评估模型的图像生成一致性，并利用gpt-4o模型进行对比验证。", "motivation": "近年来基于语言的模型(LLMs)展现了强大的能力和广泛的应用，特别是对多模态模型和文本到图像（Text-to-Image）模型的研究相对较少，评估方面也存在不足。本研究旨在填补这一空白并提出一个评估框架。", "method": "本文提出了一种针对Text-to-Image模型的综合评估框架，特别关注了模型对文本提示的响应一致性。为此，作者创建了一个新的数据集，用以评估这些模型生成与输入文本提示因素变化相一致的图像的能力。此外，还介绍了一种利用gpt-4o模型生成的真实图像描述为基准的管道，用来生成人工图像，并通过再次输入gpt-4o模型进行比较评估。", "result": "研究发现，尽管采用了多种不同类型的Text-to-Image模型（如各种版本的Stable Diffusion和Janus模型），但在生成遵循特定变化因素的简单二值图像，以及生成遵循输入数据分布的图像方面仍存在欠缺。", "conclusion": "该研究表明，现有Text-to-Image模型在生成简单图像时仍面临挑战，需要进一步改进，从而提升模型对输入提示的响应一致性以及模型输出与输入数据分布的一致性。"}}
{"id": "2507.08012", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.08012", "abs": "https://arxiv.org/abs/2507.08012", "authors": ["Atli Sigurgeirsson", "Simon King"], "title": "RepeaTTS: Towards Feature Discovery through Repeated Fine-Tuning", "comment": null, "summary": "A Prompt-based Text-To-Speech model allows a user to control different\naspects of speech, such as speaking rate and perceived gender, through natural\nlanguage instruction. Although user-friendly, such approaches are on one hand\nconstrained: control is limited to acoustic features exposed to the model\nduring training, and too flexible on the other: the same inputs yields\nuncontrollable variation that are reflected in the corpus statistics.\n  We investigate a novel fine-tuning regime to address both of these issues at\nthe same time by exploiting the uncontrollable variance of the model. Through\nprincipal component analysis of thousands of synthesised samples, we determine\nlatent features that account for the highest proportion of the output variance\nand incorporate them as new labels for secondary fine-tuning. We evaluate the\nproposed methods on two models trained on an expressive Icelandic speech\ncorpus, one with emotional disclosure and one without. In the case of the model\nwithout emotional disclosure, the method yields both continuous and discrete\nfeatures that improve overall controllability of the model.", "AI": {"tldr": "论文提出了一个新的微调策略，通过主成分分析来增强基于提示的文本到语音模型的可控性，特别是在提高不包含情感揭示模型的合成语音质量方面。", "motivation": "论文动机在于解决基于提示的文本到语音模型的两个问题：一方面，控制仅限于训练期间暴露给模型的声学特征；另一方面，同一输入产生的不可控变化反映在语料统计中。", "method": "该论文提出了一种新的微调方法，通过主成分分析（PCA）对成千上万的合成样本进行处理，确定解释输出变化最大比例的潜在特征，并将这些特征作为新的标签用于二次微调。", "result": "该方法在两个基于冰岛语表达语料库训练的模型上进行了评估：一个模型包含情感揭示，另一个不包含。对于不包含情感揭示的模型，该方法成功生成了连续和离散的特征，增强了模型的整体可控性。", "conclusion": "通过引入新的微调策略，论文成功地增强了模型的可控性，特别是在没有情感揭示的模型中，这种方法证明了其在提高连续和离散特征质量方面的有效性。"}}
{"id": "2507.08044", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.08044", "abs": "https://arxiv.org/abs/2507.08044", "authors": ["Debasmit Das", "Hyoungwoo Park", "Munawar Hayat", "Seokeon Choi", "Sungrack Yun", "Fatih Porikli"], "title": "ConsNoTrainLoRA: Data-driven Weight Initialization of Low-rank Adapters using Constraints", "comment": "ICCV 2025", "summary": "Foundation models are pre-trained on large-scale datasets and subsequently\nfine-tuned on small-scale datasets using parameter-efficient fine-tuning (PEFT)\ntechniques like low-rank adapters (LoRA). In most previous works, LoRA weight\nmatrices are randomly initialized with a fixed rank across all attachment\npoints. In this paper, we improve convergence and final performance of LoRA\nfine-tuning, using our proposed data-driven weight initialization method,\nConsNoTrainLoRA (CNTLoRA). We express LoRA initialization as a domain shift\nproblem where we use multiple constraints relating the pre-training and\nfine-tuning activations. By reformulating these constraints, we obtain a\nclosed-form estimate of LoRA weights that depends on pre-training weights and\nfine-tuning activation vectors and hence requires no training during\ninitialization. This weight estimate is decomposed to initialize the up and\ndown matrices with proposed flexibility of variable ranks. With the proposed\ninitialization method, we fine-tune on downstream tasks such as image\ngeneration, image classification and image understanding. Both quantitative and\nqualitative results demonstrate that CNTLoRA outperforms standard and\ndata-driven weight initialization methods. Extensive analyses and ablations\nfurther elucidate the design choices of our framework, providing an optimal\nrecipe for faster convergence and enhanced performance.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2507.08013", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.08013", "abs": "https://arxiv.org/abs/2507.08013", "authors": ["K. Sahit Reddy", "N. Ragavenderan", "Vasanth K.", "Ganesh N. Naik", "Vishalakshi Prabhu", "Nagaraja G. S"], "title": "MedicalBERT: enhancing biomedical natural language processing using pretrained BERT-based model", "comment": null, "summary": "Recent advances in natural language processing (NLP) have been driven\nbypretrained language models like BERT, RoBERTa, T5, and GPT. Thesemodels excel\nat understanding complex texts, but biomedical literature, withits\ndomain-specific terminology, poses challenges that models likeWord2Vec and\nbidirectional long short-term memory (Bi-LSTM) can't fullyaddress. GPT and T5,\ndespite capturing context, fall short in tasks needingbidirectional\nunderstanding, unlike BERT. Addressing this, we proposedMedicalBERT, a\npretrained BERT model trained on a large biomedicaldataset and equipped with\ndomain-specific vocabulary that enhances thecomprehension of biomedical\nterminology. MedicalBERT model is furtheroptimized and fine-tuned to address\ndiverse tasks, including named entityrecognition, relation extraction, question\nanswering, sentence similarity, anddocument classification. Performance metrics\nsuch as the F1-score,accuracy, and Pearson correlation are employed to showcase\nthe efficiencyof our model in comparison to other BERT-based models such as\nBioBERT,SciBERT, and ClinicalBERT. MedicalBERT outperforms these models onmost\nof the benchmarks, and surpasses the general-purpose BERT model by5.67% on\naverage across all the tasks evaluated respectively. This work alsounderscores\nthe potential of leveraging pretrained BERT models for medicalNLP tasks,\ndemonstrating the effectiveness of transfer learning techniques incapturing\ndomain-specific information.\n  (PDF) MedicalBERT: enhancing biomedical natural language processing using\npretrained BERT-based model. Available from:\nhttps://www.researchgate.net/publication/392489050_MedicalBERT_enhancing_biomedical_natural_language_processing_using_pretrained_BERT-based_model\n[accessed Jul 06 2025].", "AI": {"tldr": "提出MedicalBERT模型，专门增强生物医学领域自然语言处理能力，性能超过多个同类模型。", "motivation": "针对生物医学文献特有的专业术语挑战，优化现有模型处理此任务的能力。", "method": "Structure", "result": "{\"tldr\": \"提出MedicalBERT模型，专门增强生物医学领域自然语言处理能力，性能超过多个同类模型。\", \"motivation\": \"针对生物医学文献特有的专业术语挑战，优化现有模型处理此任务的能力。\", \"method\": \"基于BERT开发MedicalBERT，使用大规模生物医学数据集进行预训练和微调。\", \"result\": \"在命名实体识别、关系抽取、问答、句子相似性和文档分类任务上，MedicalBERT的表现优于其他多个BERT相关模型。\", \"conclusion\": \"验证了使用预训练BERT模型进行医疗NLP任务的有效性及转移学习技术在捕捉领域特定信息方面的优势。\"}", "conclusion": "验证了使用预训练BERT模型进行医疗NLP任务的有效性及转移学习技术在捕捉领域特定信息方面的优势。"}}
{"id": "2507.08047", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.08047", "abs": "https://arxiv.org/abs/2507.08047", "authors": ["Rolando A. Hernandez-Hernandez", "Adrian Rubio-Solis"], "title": "A Hybrid Multilayer Extreme Learning Machine for Image Classification with an Application to Quadcopters", "comment": "22 pages, 10 figures, 3 tables", "summary": "Multilayer Extreme Learning Machine (ML-ELM) and its variants have proven to\nbe an effective technique for the classification of different natural signals\nsuch as audio, video, acoustic and images. In this paper, a Hybrid Multilayer\nExtreme Learning Machine (HML-ELM) that is based on ELM-based autoencoder\n(ELM-AE) and an Interval Type-2 fuzzy Logic theory is suggested for active\nimage classification and applied to Unmanned Aerial Vehicles (UAVs). The\nproposed methodology is a hierarchical ELM learning framework that consists of\ntwo main phases: 1) self-taught feature extraction and 2) supervised feature\nclassification. First, unsupervised multilayer feature encoding is achieved by\nstacking a number of ELM-AEs, in which input data is projected into a number of\nhigh-level representations. At the second phase, the final features are\nclassified using a novel Simplified Interval Type-2 Fuzzy ELM (SIT2-FELM) with\na fast output reduction layer based on the SC algorithm; an improved version of\nthe algorithm Center of Sets Type Reducer without Sorting Requirement\n(COSTRWSR). To validate the efficiency of the HML-ELM, two types of experiments\nfor the classification of images are suggested. First, the HML-ELM is applied\nto solve a number of benchmark problems for image classification. Secondly, a\nnumber of real experiments to the active classification and transport of four\ndifferent objects between two predefined locations using a UAV is implemented.\nExperiments demonstrate that the proposed HML-ELM delivers a superior\nefficiency compared to other similar methodologies such as ML-ELM, Multilayer\nFuzzy Extreme Learning Machine (ML-FELM) and ELM.", "AI": {"tldr": "本文提出了一种基于ELM自动编码器和间隔II型模糊逻辑理论的混合多层极限学习机(HML-ELM)，并在无人机图像分类中验证其有效性。", "motivation": "多层极限学习机(ML-ELM)和其变异算法已被证明在不同的自然信号分类（如音频、视频、声学和图像）上非常有效。本研究旨在利用增强的方法在无人机上的主动图像分类中提高分类效率。", "method": "提出的方法是一种分层ELM学习框架，包含两个主要阶段：1) 自学特征提取 2) 监督特征分类。第一阶段通过堆叠多个ELM-AE实现无监督多层特征编码；第二阶段利用SIT2-FELM进行特征分类。", "result": "进行两种类型的实验以验证HML-ELM在图像分类中的效率。首先是在多个基准问题上进行图像分类；其次是在无人机中实现两个指定位置的四个不同对象之间的主动分类和传输。实验表明，提出的方法相对于其他计算方法展示了更高的效率。", "conclusion": "实验结果表明，提出的HML-ELM在无人机图像分类任务上相对其他类似方法提高了分类效率。"}}
{"id": "2507.08014", "categories": ["cs.CL", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2507.08014", "abs": "https://arxiv.org/abs/2507.08014", "authors": ["Aldan Creo", "Raul Castro Fernandez", "Manuel Cebrian"], "title": "Mass-Scale Analysis of In-the-Wild Conversations Reveals Complexity Bounds on LLM Jailbreaking", "comment": "Code: https://github.com/ACMCMC/risky-conversations Results:\n  https://huggingface.co/risky-conversations Visualizer:\n  https://huggingface.co/spaces/risky-conversations/Visualizer", "summary": "As large language models (LLMs) become increasingly deployed, understanding\nthe complexity and evolution of jailbreaking strategies is critical for AI\nsafety.\n  We present a mass-scale empirical analysis of jailbreak complexity across\nover 2 million real-world conversations from diverse platforms, including\ndedicated jailbreaking communities and general-purpose chatbots. Using a range\nof complexity metrics spanning probabilistic measures, lexical diversity,\ncompression ratios, and cognitive load indicators, we find that jailbreak\nattempts do not exhibit significantly higher complexity than normal\nconversations. This pattern holds consistently across specialized jailbreaking\ncommunities and general user populations, suggesting practical bounds on attack\nsophistication. Temporal analysis reveals that while user attack toxicity and\ncomplexity remains stable over time, assistant response toxicity has decreased,\nindicating improving safety mechanisms. The absence of power-law scaling in\ncomplexity distributions further points to natural limits on jailbreak\ndevelopment.\n  Our findings challenge the prevailing narrative of an escalating arms race\nbetween attackers and defenders, instead suggesting that LLM safety evolution\nis bounded by human ingenuity constraints while defensive measures continue\nadvancing. Our results highlight critical information hazards in academic\njailbreak disclosure, as sophisticated attacks exceeding current complexity\nbaselines could disrupt the observed equilibrium and enable widespread harm\nbefore defensive adaptation.", "AI": {"tldr": "研究大规模分析了超过200万次真实对话，发现越狱尝试的复杂性与正常对话无显著差异，并揭示了LLM安全进化受人类创新限制的事实。", "motivation": "随着大型语言模型（LLMs）的部署范围不断扩大，理解越狱策略的复杂性和演变对于AI安全性至关重要。", "method": "采用大规模实证分析，通过对超过200万次来自不同平台的真实对话进行分析，这些平台包括专门的越狱社区和通用聊天机器人。使用多种复杂性度量标准，包括概率测量、词汇多样性、压缩比率和认知负荷指标来衡量越狱尝试的复杂性。", "result": "发现越狱尝试的复杂性并不显著高于正常对话。这种模式在专门的越狱社区和普通用户群体中均保持一致，表明攻击复杂程度存在实践上限。时间序列分析显示，尽管用户发起的攻击毒性保持相对稳定，但助手响应的毒性有所下降，说明安全措施正在改进。复杂性分布中缺乏幂律分布进一步表明越狱开发存在自然限制。", "conclusion": "研究结果挑战了攻击者和防御者之间不断升级的军备竞赛的主流观点，相反，表明LLM的安全进化由人类创新的限制驱动，而防御措施不断进步。该结果揭示了在学术披露越狱技术时可能出现的重要信息危害，因为超出当前复杂度基线的复杂攻击可能会打破当前的均衡状态，并在防御措施适应之前造成广泛危害。"}}
{"id": "2507.08052", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.08052", "abs": "https://arxiv.org/abs/2507.08052", "authors": ["Mazen Ali", "António Pereira", "Fabio Gentile", "Aser Cortines", "Sam Mugel", "Román Orús", "Stelios P. Neophytides", "Michalis Mavrovouniotis"], "title": "Lightweight Cloud Masking Models for On-Board Inference in Hyperspectral Imaging", "comment": null, "summary": "Cloud and cloud shadow masking is a crucial preprocessing step in\nhyperspectral satellite imaging, enabling the extraction of high-quality,\nanalysis-ready data. This study evaluates various machine learning approaches,\nincluding gradient boosting methods such as XGBoost and LightGBM as well as\nconvolutional neural networks (CNNs). All boosting and CNN models achieved\naccuracies exceeding 93%. Among the investigated models, the CNN with feature\nreduction emerged as the most efficient, offering a balance of high accuracy,\nlow storage requirements, and rapid inference times on both CPUs and GPUs.\nVariations of this version, with only up to 597 trainable parameters,\ndemonstrated the best trade-off in terms of deployment feasibility, accuracy,\nand computational efficiency. These results demonstrate the potential of\nlightweight artificial intelligence (AI) models for real-time hyperspectral\nimage processing, supporting the development of on-board satellite AI systems\nfor space-based applications.", "AI": {"tldr": "本文探讨高光谱卫星成像中云和云影标记的重要性，并比较了梯度提升方法和CNN模型。研究中最佳的模型是只包含597个可训练参数的CNN模型，它在精度、存储需求和推理时间方面表现平衡。此外，轻量级模型的实时处理潜力也支持了卫星设备上AI系统的发展。", "motivation": "云和云影遮盖物的标记是高光谱卫星成像中的关键预处理步骤，能提取高质量的数据。", "method": "本文研究了多种机器学习方法，包括XGBoost和LightGBM等梯度提升方法以及卷积神经网络(CNN)。", "result": "研究中所有的提升方法和CNN模型的准确率均超过93%。CNN模型在减少特征量后表现最佳，提供了高精度、低存储需求以及在CPU和GPU上的快速推理时间。仅包含最多597个可训练参数的CNN模型版本在部署可行性、精度和计算效率方面表现最佳。", "conclusion": "结果表明轻量级AI模型在高光谱图像实时处理中的潜力，支持太空应用中的卫星设备上AI系统的开发。"}}
{"id": "2507.08015", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.08015", "abs": "https://arxiv.org/abs/2507.08015", "authors": ["Prudence Djagba", "Chimezie A. Odinakachukwu"], "title": "Assessing the Capabilities and Limitations of FinGPT Model in Financial NLP Applications", "comment": null, "summary": "This work evaluates FinGPT, a financial domain-specific language model,\nacross six key natural language processing (NLP) tasks: Sentiment Analysis,\nText Classification, Named Entity Recognition, Financial Question Answering,\nText Summarization, and Stock Movement Prediction. The evaluation uses\nfinance-specific datasets to assess FinGPT's capabilities and limitations in\nreal-world financial applications. The results show that FinGPT performs\nstrongly in classification tasks such as sentiment analysis and headline\ncategorization, often achieving results comparable to GPT-4. However, its\nperformance is significantly lower in tasks that involve reasoning and\ngeneration, such as financial question answering and summarization. Comparisons\nwith GPT-4 and human benchmarks highlight notable performance gaps,\nparticularly in numerical accuracy and complex reasoning. Overall, the findings\nindicate that while FinGPT is effective for certain structured financial tasks,\nit is not yet a comprehensive solution. This research provides a useful\nbenchmark for future research and underscores the need for architectural\nimprovements and domain-specific optimization in financial language models.", "AI": {"tldr": "研究评估了金融专用语言模型FinGPT，在多个NLP任务中的表现。发现它在分类任务上表现良好，但在需要推理和生成的任务上表现较差。", "motivation": "此研究旨在考察FinGPT在多种金融相关任务中的有效性，以识别其潜在的限制和可行性的范围，并为未来的研究提供有用的基准。", "method": "本研究评估了FinGPT，一种面向金融市场特定的语言模型，在六个关键的自然语言处理（NLP）任务上的表现：情感分析、文本分类、命名实体识别、财务问答、文本摘要和股票走势预测。评估使用了金融专用数据集来评估FinGPT在真实世界的金融应用程序中的能力和局限性。", "result": "FinGPT在如情感分析和标题分类等分类任务上表现出色，往往达到与GPT-4相近的结果。但在涉及推理和生成的任务上，例如财务问答和摘要生成，其表现显著低于GPT-4和人类基准，特别是在数值准确性和复杂推理上存在差距。", "conclusion": "研究结果表明，尽管FinGPT在某些结构化的金融任务上有成效，但当前还不是全面的解决方案。这研究强调了在未来需要在金融领域的优化和架构上的改进。"}}
{"id": "2507.08059", "categories": ["cs.CV", "math.PR", "68T05, 68T45, 60J60, 82C22, 82C31"], "pdf": "https://arxiv.org/pdf/2507.08059", "abs": "https://arxiv.org/abs/2507.08059", "authors": ["F. Alberto Grünbaum", "Tondgi Xu"], "title": "The relative importance of being Gaussian", "comment": null, "summary": "The remarkable results for denoising in computer vision using diffusion\nmodels given in \\cite{SDWMG,HJA,HHG} yield a robust mathematical justification\nfor algorithms based on crucial properties of a sequence of Gaussian\nindependent $N(0,1)$ random variables. In particular the derivations use the\nfact that a Gaussian distribution is determined by its mean and variance and\nthat the sum of two Gaussians is another Gaussian.\n  \\bigskip\n  The issue raised in this short note is the following: suppose we use the\nalgorithm without any changes but replace the nature of the noise and use, for\ninstance, uniformly distributed noise or noise with a Beta distribution, or\nnoise which is a random superposition of two Gaussians with very different\nvariances. One could, of course, try to modify the algorithm keeping in mind\nthe nature of the noise, but this is not what we do. Instead we study the\nperformance of the algorithm when used with noise that is very far in nature\nfrom the Gaussian case, where it is designed to work well.\n  Usually these algorithms are implemented on very powerful computers. Our\nexperiments are all carried out on a small laptop and for the smallest possible\nimage size. Exploring how our observations are confirmed or changed when\ndealing in different situations remains an interesting challenge.", "AI": {"tldr": "本文研究了原有去噪算法在使用非高斯噪声时的性能表现，实验条件为低配置机器和小尺寸图像，目的是测试算法适应性。", "motivation": "动机在于探讨当算法处理与设计初衷（高斯噪声）大相径庭的噪声类型时，现有的基于高斯独立正态分布随机变量序列的去噪算法的表现如何。", "method": "本文的方法是研究在算法中使用非高斯噪声（如均匀分布噪声或Beta分布噪声）时，现有去噪算法的性能。这些实验在性能较低的笔记本电脑上进行，并使用了最小的图像尺寸。", "result": "结果部分尚未详细给出，但从描述中可以看出，研究将测试算法在非高斯噪声条件下的性能。", "conclusion": "结论部分未提供具体细节，研究主要聚焦于验证或探索上述发现是否在不同条件和环境下依然成立。"}}
{"id": "2507.08017", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.08017", "abs": "https://arxiv.org/abs/2507.08017", "authors": ["Pierre Beckmann", "Matthieu Queloz"], "title": "Mechanistic Indicators of Understanding in Large Language Models", "comment": "32 pages", "summary": "Recent findings in mechanistic interpretability (MI), the field probing the\ninner workings of Large Language Models (LLMs), challenge the view that these\nmodels rely solely on superficial statistics. Here, we offer an accessible\nsynthesis of these findings that doubles as an introduction to MI, all while\nintegrating these findings within a novel theoretical framework for thinking\nabout machine understanding. We argue that LLMs develop internal structures\nthat are functionally analogous to the kind of understanding that consists in\nseeing connections. To sharpen this idea, we propose a three-tiered conception\nof machine understanding. First, conceptual understanding emerges when a model\nforms \"features\" as directions in latent space, thereby learning the\nconnections between diverse manifestations of something. Second,\nstate-of-the-world understanding emerges when a model learns contingent factual\nconnections between features and dynamically tracks changes in the world.\nThird, principled understanding emerges when a model ceases to rely on a\ncollection of memorized facts and discovers a \"circuit\" that connects these\nfacts. However, we conclude by exploring the \"parallel mechanisms\" phenomenon,\narguing that while LLMs exhibit forms of understanding, their cognitive\narchitecture remains different from ours, and the debate should shift from\nwhether LLMs understand to how their strange minds work.", "AI": {"tldr": "本文通过将大型语言模型（LLMs）的内在工作原理整合到一个新的理论框架中，引入了机器理解的多层次概念：概念理解、状态世界理解和原则理解，并探讨了LLMs的奇特思维工作方式。", "motivation": "本文旨在挑战认为LLMs仅依赖于表面统计的观点，通过介绍机制诠释性的最新发现，提供一个理解机器理解的新视角。", "method": "文章提出了一种三层机器理解模式，通过解释LLMs是如何在潜在空间中形成特征方向，学习各类事物间的关联，通过状态世界理解和原则理解来阐述LLMs的不同层面的理解能力。", "result": "提出了一个理解LLMs内部机制的新框架，说明了LLMs如何在潜在空间中形成功能上类似于人类理解的现象。", "conclusion": "尽管LLMs表现出某种形式的理解能力，但它们的认知架构与人类不同，关于LLMs是否理解的争论应转向研究其奇特的思维工作机制。"}}
{"id": "2507.08096", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.08096", "abs": "https://arxiv.org/abs/2507.08096", "authors": ["Babak Memar", "Luigi Russo", "Silvia Liberata Ullo", "Paolo Gamba"], "title": "An Object-Based Deep Learning Approach for Building Height Estimation from Single SAR Images", "comment": null, "summary": "Accurate estimation of building heights using very high resolution (VHR)\nsynthetic aperture radar (SAR) imagery is crucial for various urban\napplications. This paper introduces a Deep Learning (DL)-based methodology for\nautomated building height estimation from single VHR COSMO-SkyMed images: an\nobject-based regression approach based on bounding box detection followed by\nheight estimation. This model was trained and evaluated on a unique\nmulti-continental dataset comprising eight geographically diverse cities across\nEurope, North and South America, and Asia, employing a cross-validation\nstrategy to explicitly assess out-of-distribution (OOD) generalization. The\nresults demonstrate highly promising performance, particularly on European\ncities where the model achieves a Mean Absolute Error (MAE) of approximately\none building story (2.20 m in Munich), significantly outperforming recent\nstate-of-the-art methods in similar OOD scenarios. Despite the increased\nvariability observed when generalizing to cities in other continents,\nparticularly in Asia with its distinct urban typologies and prevalence of\nhigh-rise structures, this study underscores the significant potential of DL\nfor robust cross-city and cross-continental transfer learning in building\nheight estimation from single VHR SAR data.", "AI": {"tldr": "本文介绍了一种基于深度学习从单幅高分辨率SAR图像中进行建筑物高度自动估计的方法，通过跨大陆城市的多中心数据集训练和验证，展示了方法在欧洲城市中的优越性能，并指出了深度学习在城市间和跨国界建筑物高度估计中的潜在应用。", "motivation": "精确估计建筑物的高度对于各种城市应用至关重要，而本文旨在利用高分辨率合成孔径雷达（SAR）图像进行自动化高度估计。", "method": "本文提出了一种基于深度学习的建筑物高度自动估计方法，该方法利用单幅高分辨率合成孔径雷达（SAR）图像，通过边界框检测和随后的高度估计来进行对象回归分析。", "result": "实验结果表明，在跨验证策略下，模型在欧洲城市的表现尤其出色，平均绝对误差（MAE）约为一层楼的高度（慕尼黑为2.20米），显著优于之前在跨分布场景下的最新方法。", "conclusion": "尽管在推广到其他大陆的城市时出现了较大的变化，特别是在亚洲，由于其独特的城市类型和高层建筑的普遍存在，但是本研究表明深度学习对于从单幅高分辨率SAR数据中进行稳健的城市间和跨国界的建筑物高度估计具有巨大潜力。"}}
{"id": "2507.08018", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.08018", "abs": "https://arxiv.org/abs/2507.08018", "authors": ["Nikita Mounier", "Parsa Idehpour"], "title": "Review, Remask, Refine (R3): Process-Guided Block Diffusion for Text Generation", "comment": "Accepted at Methods and Opportunities at Small Scale (MOSS), ICML\n  2025", "summary": "A key challenge for iterative text generation is enabling models to\nefficiently identify and correct their own errors. We propose Review, Remask,\nRefine (R3), a relatively simple yet elegant framework that requires no\nadditional model training and can be applied to any pre-trained masked text\ndiffusion model (e.g., LLaDA or BD3-LM). In R3, a Process Reward Model (PRM) is\nutilized for the Review of intermediate generated blocks. The framework then\ntranslates these PRM scores into a Remask strategy: the lower a block's PRM\nscore, indicating potential mistakes, the greater the proportion of tokens\nwithin that block are remasked. Finally, the model is compelled to Refine these\ntargeted segments, focusing its efforts more intensively on specific\nsub-optimal parts of past generations, leading to improved final output.", "AI": {"tldr": "提出了R3框架，通过PRM评估和Remask策略，来改善文本生成质量，提升模型自我纠错能力。", "motivation": "解决迭代文本生成中的一个关键挑战，即让模型能有效地识别和纠正自己的错误。", "method": "提出了一种名为Review, Remask, Refine (R3)的框架，该框架简单且优雅，无需额外训练模型，可应用于任何预训练的掩码文本扩散模型。框架利用Process Reward Model (PRM)对生成的中间块进行评估，并根据PRM分数制定Remask策略，分数越低表示问题越多，相应块中被重新掩码的标记比例越大。最后，模型集中精力修正这些特定部分，从而提升最终输出质量。", "result": "采用R3框架的模型能够在后续生成中有针对性地改进特定子优化部分，从而改进了最终输出结果。", "conclusion": "通过集中修正低评估分数的块，R3框架能够在不增加额外训练的情况下，提高文本生成的质量和效率。"}}
{"id": "2507.08136", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.08136", "abs": "https://arxiv.org/abs/2507.08136", "authors": ["Chong Cheng", "Yu Hu", "Sicheng Yu", "Beizhen Zhao", "Zijian Wang", "Hao Wang"], "title": "RegGS: Unposed Sparse Views Gaussian Splatting with 3DGS Registration", "comment": "Accepted to ICCV 2025", "summary": "3D Gaussian Splatting (3DGS) has demonstrated its potential in reconstructing\nscenes from unposed images. However, optimization-based 3DGS methods struggle\nwith sparse views due to limited prior knowledge. Meanwhile, feed-forward\nGaussian approaches are constrained by input formats, making it challenging to\nincorporate more input views. To address these challenges, we propose RegGS, a\n3D Gaussian registration-based framework for reconstructing unposed sparse\nviews. RegGS aligns local 3D Gaussians generated by a feed-forward network into\na globally consistent 3D Gaussian representation. Technically, we implement an\nentropy-regularized Sinkhorn algorithm to efficiently solve the optimal\ntransport Mixture 2-Wasserstein $(\\text{MW}_2)$ distance, which serves as an\nalignment metric for Gaussian mixture models (GMMs) in $\\mathrm{Sim}(3)$ space.\nFurthermore, we design a joint 3DGS registration module that integrates the\n$\\text{MW}_2$ distance, photometric consistency, and depth geometry. This\nenables a coarse-to-fine registration process while accurately estimating\ncamera poses and aligning the scene. Experiments on the RE10K and ACID datasets\ndemonstrate that RegGS effectively registers local Gaussians with high\nfidelity, achieving precise pose estimation and high-quality novel-view\nsynthesis. Project page: https://3dagentworld.github.io/reggs/.", "AI": {"tldr": "RegGS 是一个基于 3D 高斯混合模型的注册框架，通过一个高效的最优运输算法和光度一致性模块，能够处理未指定视角的稀疏图像数据，实现了高精度的场景重建和视角合成。", "motivation": "动机在于解决现有的3D Gaussian Splatting（3DGS）方法在使用稀疏视角图像时遇到的问题，这类方法要么受限于输入格式，要么缺乏对全局一致性的保证。", "method": "技术上，RegGS 使用了一个基于熵正则化的 Sinkhorn 算法来高效地解决从多个视角获得的 3D 高斯混合模型（GMMs）的最优运输 Mixture 2-Wasserstein（MW2）距离。此外，设计了一个联合的 3DGS 注册模块，整合了 MW2 距离、光度一致性和深度几何特征，实现了从粗到细的注册过程，并精确估计相机姿态和场景对齐。", "result": "实验结果表明，RegGS 能够有效地对局部高斯模型进行注册，同时实现了精确的姿态估计和高质量的新视图合成。", "conclusion": "结论是，RegGS 通过优化基于运输问题的距离和光度一致性，解决了使用稀疏视角重建场景时的挑战，提供了精确的场景重建和新视图合成。"}}
{"id": "2507.08019", "categories": ["cs.CL", "econ.GN", "q-fin.EC"], "pdf": "https://arxiv.org/pdf/2507.08019", "abs": "https://arxiv.org/abs/2507.08019", "authors": ["Aryan Varshney", "Venkat Ram Reddy Ganuthula"], "title": "Signal or Noise? Evaluating Large Language Models in Resume Screening Across Contextual Variations and Human Expert Benchmarks", "comment": null, "summary": "This study investigates whether large language models (LLMs) exhibit\nconsistent behavior (signal) or random variation (noise) when screening resumes\nagainst job descriptions, and how their performance compares to human experts.\nUsing controlled datasets, we tested three LLMs (Claude, GPT, and Gemini)\nacross contexts (No Company, Firm1 [MNC], Firm2 [Startup], Reduced Context)\nwith identical and randomized resumes, benchmarked against three human\nrecruitment experts. Analysis of variance revealed significant mean differences\nin four of eight LLM-only conditions and consistently significant differences\nbetween LLM and human evaluations (p < 0.01). Paired t-tests showed GPT adapts\nstrongly to company context (p < 0.001), Gemini partially (p = 0.038 for\nFirm1), and Claude minimally (p > 0.1), while all LLMs differed significantly\nfrom human experts across contexts. Meta-cognition analysis highlighted\nadaptive weighting patterns that differ markedly from human evaluation\napproaches. Findings suggest LLMs offer interpretable patterns with detailed\nprompts but diverge substantially from human judgment, informing their\ndeployment in automated hiring systems.", "AI": {"tldr": "研究对比了三种大型语言模型在简历筛选中的表现与人类专家的变化，发现有显著差异，表明技术的应用需审慎。", "motivation": "研究动机在于调查大型语言模型在筛选简历时是否表现出一致性或随机性，并探究其表现与人类专家相比如何。", "method": "该研究比较了三种大型语言模型（Claude、GPT 和 Gemini）在不同情境下筛选简历与职位描述的表现，并将其与三位人类招聘专家的表现进行对比。使用的数据集包括相同和随机化的简历。", "result": "方差分析揭示了八个条件中的四个条件存在显著的平均差异，并且所有大型语言模型与人类专家的评估存在显著差异（p < 0.01）。成对t检验表明，GPT能强烈适应公司环境（p < 0.001），Gemini部分适应（p = 0.038 for Firm1），而Claude微弱适应（p > 0.1）。", "conclusion": "研究发现大型语言模型可以提供可解释的模式，但在特定详尽提示下显著偏离人类判断。这些发现有助于大型语言模型在自动招聘系统中的应用。"}}
{"id": "2507.08137", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.08137", "abs": "https://arxiv.org/abs/2507.08137", "authors": ["Hyungjun Doh", "Dong In Lee", "Seunggeun Chi", "Pin-Hao Huang", "Kwonjoon Lee", "Sangpil Kim", "Karthik Ramani"], "title": "Temporally Consistent Amodal Completion for 3D Human-Object Interaction Reconstruction", "comment": null, "summary": "We introduce a novel framework for reconstructing dynamic human-object\ninteractions from monocular video that overcomes challenges associated with\nocclusions and temporal inconsistencies. Traditional 3D reconstruction methods\ntypically assume static objects or full visibility of dynamic subjects, leading\nto degraded performance when these assumptions are violated-particularly in\nscenarios where mutual occlusions occur. To address this, our framework\nleverages amodal completion to infer the complete structure of partially\nobscured regions. Unlike conventional approaches that operate on individual\nframes, our method integrates temporal context, enforcing coherence across\nvideo sequences to incrementally refine and stabilize reconstructions. This\ntemplate-free strategy adapts to varying conditions without relying on\npredefined models, significantly enhancing the recovery of intricate details in\ndynamic scenes. We validate our approach using 3D Gaussian Splatting on\nchallenging monocular videos, demonstrating superior precision in handling\nocclusions and maintaining temporal stability compared to existing techniques.", "AI": {"tldr": "提出了一种新的从单目视频中重建动态人与物体交互的框架，通过非模式补全和时间上下文整合，显著提升了遮挡和时间一致性处理的性能。", "motivation": "传统三维重建方法在处理遮挡和时间不一致性时存在局限，因此，需要提出新的方法来改进这些问题。", "method": "该框架利用非模式补全技术推断遮挡部分的完整结构，整合时间上下文来强制视频序列中的重建一致性。", "result": "该论文介绍了一种从单目视频中重建动态人与物体交互的新框架，该框架克服了遮挡和时间不一致性的挑战。传统的三维重建方法通常假设物体静止或主体完全可见，当这些假设被违反时（特别是在存在互遮挡的场景中），其性能会受到严重影响。为此，该框架利用非模式补全来推断部分被遮挡区域的完整结构。与传统的针对单帧操作的方法不同，该方法整合了时间上下文，通过在视频序列中强制一致性来逐步细化和稳定重建。这种无模板策略适应各种条件，无需依赖预定义的模型，显著增强了在动态场景中恢复复杂细节的能力。使用3D高斯散射球技术在具有挑战性的单目视频上验证了该方法，证明其在处理遮挡和保持时间一致性方面的精度优于现有技术。", "conclusion": "该方法在处理遮挡和保持时间一致性上表现出色，使用3D高斯散射球技术验证了其在挑战性单目视频上的优越性能。"}}
{"id": "2507.08020", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.08020", "abs": "https://arxiv.org/abs/2507.08020", "authors": ["Zhibo Zhang", "Yuxi Li", "Kailong Wang", "Shuai Yuan", "Ling Shi", "Haoyu Wang"], "title": "Circumventing Safety Alignment in Large Language Models Through Embedding Space Toxicity Attenuation", "comment": null, "summary": "Large Language Models (LLMs) have achieved remarkable success across domains\nsuch as healthcare, education, and cybersecurity. However, this openness also\nintroduces significant security risks, particularly through embedding space\npoisoning, which is a subtle attack vector where adversaries manipulate the\ninternal semantic representations of input data to bypass safety alignment\nmechanisms. While previous research has investigated universal perturbation\nmethods, the dynamics of LLM safety alignment at the embedding level remain\ninsufficiently understood. Consequently, more targeted and accurate adversarial\nperturbation techniques, which pose significant threats, have not been\nadequately studied.\n  In this work, we propose ETTA (Embedding Transformation Toxicity\nAttenuation), a novel framework that identifies and attenuates\ntoxicity-sensitive dimensions in embedding space via linear transformations.\nETTA bypasses model refusal behaviors while preserving linguistic coherence,\nwithout requiring model fine-tuning or access to training data. Evaluated on\nfive representative open-source LLMs using the AdvBench benchmark, ETTA\nachieves a high average attack success rate of 88.61%, outperforming the best\nbaseline by 11.34%, and generalizes to safety-enhanced models (e.g., 77.39% ASR\non instruction-tuned defenses). These results highlight a critical\nvulnerability in current alignment strategies and underscore the need for\nembedding-aware defenses.", "AI": {"tldr": "提出了一种新的框架ETTA，用于识别并减弱嵌入空间中的毒性敏感维度，以提升模型安全性，评估结果显示其显著优于现有方法。", "motivation": "目前对于模型嵌入层面的安全对齐动态理解不足，大多数针对性强且精准的对抗扰动技术研究不足，这可能会导致显著的安全威胁。", "method": "ETTA (Embedding Transformation Toxicity Attenuation)：识别并减弱嵌入空间中的毒性敏感维度，通过线性变换实现，绕过模型拒绝行为，同时保持语言连贯性，无需模型微调或访问训练数据。", "result": "在五种代表性开源大语言模型上使用AdvBench基准进行评估，ETTA实现了高达88.61％的平均攻击成功率，比最佳基线高出11.34％，对于增强安全性的模型（如指令调整后的防御模型）也达到了77.39％的成功率。", "conclusion": "实验结果表明了当前对齐策略中存在的关键脆弱性，并强调了需要嵌入感知防御的重要性。"}}
{"id": "2507.08163", "categories": ["cs.CV", "cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.08163", "abs": "https://arxiv.org/abs/2507.08163", "authors": ["Frederick Shpilevskiy", "Saiyue Lyu", "Krishnamurthy Dj Dvijotham", "Mathias Lécuyer", "Pierre-André Noël"], "title": "Adaptive Diffusion Denoised Smoothing : Certified Robustness via Randomized Smoothing with Differentially Private Guided Denoising Diffusion", "comment": null, "summary": "We propose Adaptive Diffusion Denoised Smoothing, a method for certifying the\npredictions of a vision model against adversarial examples, while adapting to\nthe input. Our key insight is to reinterpret a guided denoising diffusion model\nas a long sequence of adaptive Gaussian Differentially Private (GDP) mechanisms\nrefining a pure noise sample into an image. We show that these adaptive\nmechanisms can be composed through a GDP privacy filter to analyze the\nend-to-end robustness of the guided denoising process, yielding a provable\ncertification that extends the adaptive randomized smoothing analysis. We\ndemonstrate that our design, under a specific guiding strategy, can improve\nboth certified accuracy and standard accuracy on ImageNet for an $\\ell_2$\nthreat model.", "AI": {"tldr": "该研究提出了一种新的方法——自适应扩散去噪平滑，能够有效提升视觉模型对于对抗样本攻击的鲁棒性，并提供了相应的可证明认证。", "motivation": "该研究的动机在于提升模型对抗对抗样本的鲁棒性，同时对模型预测结果提供可证明的认证。", "method": "我们提出了自适应扩散去噪平滑（Adaptive Diffusion Denoised Smoothing）方法，用于验证视觉模型在对抗样本攻击下的预测结果。该方法通过将引导去噪扩散模型重新解读为一系列自适应高斯微分私有机制，逐步将纯噪声样本优化为图像。", "result": "研究结果表明，通过特定的引导策略，该设计能在ImageNet数据集上，对于$\rlap{$\backslash$l_2$}\rlap{$\backslash$l2$}威胁模型，同时提升可证明的准确性与标准准确性。", "conclusion": "通过自适应高斯微分私有机制的组合来分析整个去噪过程的鲁棒性，证明了该自适应随机平滑分析的有效性。"}}
{"id": "2507.08021", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.08021", "abs": "https://arxiv.org/abs/2507.08021", "authors": ["Li Li", "Yongliang Wu", "Jingze Zhu", "Jiawei Peng", "Jianfei Cai", "Xu Yang"], "title": "Unveiling Effective In-Context Configurations for Image Captioning: An External & Internal Analysis", "comment": "16 pages, 11 figures", "summary": "The evolution of large models has witnessed the emergence of In-Context\nLearning (ICL) capabilities. In Natural Language Processing (NLP), numerous\nstudies have demonstrated the effectiveness of ICL. Inspired by the success of\nLarge Language Models (LLMs), researchers have developed Large Multimodal\nModels (LMMs) with ICL capabilities. However, explorations of demonstration\nconfiguration for multimodal ICL remain preliminary. Additionally, the\ncontrollability of In-Context Examples (ICEs) provides an efficient and\ncost-effective means to observe and analyze the inference characteristics of\nLMMs under varying inputs. This paper conducts a comprehensive external and\ninternal investigation of multimodal in-context learning on the image\ncaptioning task. Externally, we explore demonstration configuration strategies\nthrough three dimensions: shot number, image retrieval, and caption assignment.\nWe employ multiple metrics to systematically and thoroughly evaluate and\nsummarize key findings. Internally, we analyze typical LMM attention\ncharacteristics and develop attention-based metrics to quantify model\nbehaviors. We also conduct auxiliary experiments to explore the feasibility of\nattention-driven model acceleration and compression. We further compare\nperformance variations between LMMs with identical model design and pretraining\nstrategies and explain the differences from the angles of pre-training data\nfeatures. Our study reveals both how ICEs configuration strategies impact model\nperformance through external experiments and characteristic typical patterns\nthrough internal inspection, providing dual perspectives for understanding\nmultimodal ICL in LMMs. Our method of combining external and internal analysis\nto investigate large models, along with our newly proposed metrics, can be\napplied to broader research areas.", "AI": {"tldr": "This paper investigates multimodal in-context learning (ICL) in large multimodal models (LMMs) for image captioning, analyzing demonstration configurations and internal model behaviors with proposed metrics.", "motivation": "The motivation is to explore and understand the impact of In-Context Examples (ICEs) configuration on LMM performance and behavior, addressing the preliminary nature of multimodal ICL configuration research.", "method": "The paper uses both external and internal analysis for a thorough examination of ICEs configuration strategies in image captioning, alongside attention metric analysis and auxiliary experiments.", "result": "Key findings from multiple metrics evaluation and attention analysis are summarized, and performance variations between different LMMs are explained.", "conclusion": "The paper concludes the effectiveness of their combined external and internal analysis method, and the proposed metrics for assessing multimodal ICL in LMMs, which can be extended to other research areas."}}
{"id": "2507.08165", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.08165", "abs": "https://arxiv.org/abs/2507.08165", "authors": ["Jareen Anjom", "Rashik Iram Chowdhury", "Tarbia Hasan", "Md. Ishan Arefin Hossain"], "title": "An Embedded Real-time Object Alert System for Visually Impaired: A Monocular Depth Estimation based Approach through Computer Vision", "comment": null, "summary": "Visually impaired people face significant challenges in their day-to-day\ncommutes in the urban cities of Bangladesh due to the vast number of\nobstructions on every path. With many injuries taking place through road\naccidents on a daily basis, it is paramount for a system to be developed that\ncan alert the visually impaired of objects at close distance beforehand. To\novercome this issue, a novel alert system is proposed in this research to\nassist the visually impaired in commuting through these busy streets without\ncolliding with any objects. The proposed system can alert the individual to\nobjects that are present at a close distance. It utilizes transfer learning to\ntrain models for depth estimation and object detection, and combines both\nmodels to introduce a novel system. The models are optimized through the\nutilization of quantization techniques to make them lightweight and efficient,\nallowing them to be easily deployed on embedded systems. The proposed solution\nachieved a lightweight real-time depth estimation and object detection model\nwith an mAP50 of 0.801.", "AI": {"tldr": "研究提出了一种利用转移学习进行深度估计和物体检测，并通过量化技术使其轻量化和高效化的系统，帮助视障人士避免在城市中行走时碰撞到障碍物。", "motivation": "视障人士在城市中行走时会遇到很多障碍物，容易发生碰撞事故，因此开发一种能够提前预警视障人士前方障碍物的系统是必要的。", "method": "该系统利用转移学习技术训练模型进行深度估计和物体检测，并通过量化的技术优化模型，使其能够在嵌入式设备上高效运行。", "result": "所提出的解决方案能够实现轻量级实时深度估计和物体检测，模型的mAP50为0.801。", "conclusion": "该系统可以辅助视障人士在繁忙街道上安全行走，避免碰撞障碍物。"}}
{"id": "2507.08027", "categories": ["cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2507.08027", "abs": "https://arxiv.org/abs/2507.08027", "authors": ["W. Russell Neuman", "Chad Coleman", "Ali Dasdan", "Safinah Ali", "Manan Shah", "Kund Meghani"], "title": "\"Amazing, They All Lean Left\" -- Analyzing the Political Temperaments of Current LLMs", "comment": null, "summary": "Recent studies have revealed a consistent liberal orientation in the ethical\nand political responses generated by most commercial large language models\n(LLMs), yet the underlying causes and resulting implications remain unclear.\nThis paper systematically investigates the political temperament of seven\nprominent LLMs - OpenAI's GPT-4o, Anthropic's Claude Sonnet 4, Perplexity\n(Sonar Large), Google's Gemini 2.5 Flash, Meta AI's Llama 4, Mistral 7b Le Chat\nand High-Flyer's DeepSeek R1 -- using a multi-pronged approach that includes\nMoral Foundations Theory, a dozen established political ideology scales and a\nnew index of current political controversies. We find strong and consistent\nprioritization of liberal-leaning values, particularly care and fairness,\nacross most models. Further analysis attributes this trend to four overlapping\nfactors: Liberal-leaning training corpora, reinforcement learning from human\nfeedback (RLHF), the dominance of liberal frameworks in academic ethical\ndiscourse and safety-driven fine-tuning practices. We also distinguish between\npolitical \"bias\" and legitimate epistemic differences, cautioning against\nconflating the two. A comparison of base and fine-tuned model pairs reveals\nthat fine-tuning generally increases liberal lean, an effect confirmed through\nboth self-report and empirical testing. We argue that this \"liberal tilt\" is\nnot a programming error or the personal preference of programmers but an\nemergent property of training on democratic rights-focused discourse. Finally,\nwe propose that LLMs may indirectly echo John Rawls' famous veil-of ignorance\nphilosophical aspiration, reflecting a moral stance unanchored to personal\nidentity or interest. Rather than undermining democratic discourse, this\npattern may offer a new lens through which to examine collective reasoning.", "AI": {"tldr": "本文发现，七个著名大型语言模型表现出一致的自由主义倾向，这种倾向来源于几个因素，并指出这不是编程错误或偏见，而是训练过程中的一个特征。", "motivation": "本文旨在深入理解大型语言模型内部存在的自由主义倾向的原因及其影响。", "method": "研究采用了多管齐下的方法，包括道德基础理论、十几种已建立的政治意识形态量表和一个新编的政治争议指数，系统调查了七个主要的LLM模型的政治倾向。", "result": "本文系统性地调查了七个著名大型语言模型（LLMs）的政治倾向，发现大多数模型表现出偏向自由主义的价值观。这种倾向归因于训练语料库的自由主义倾向、来自人类反馈的强化学习（RLHF）、学术伦理讨论中自由主义框架的主导地位以及安全导向的微调实践。本文还区分了政治‘偏见’和合法的知识差异，并指出微调通常会增加模型的自由主义倾向。作者认为这种‘自由主义倾向’并非编程错误或程序员的个人偏好，而是建立在以民主权利为中心的话语训练上的一个特征。这一模式可能提供了一个新视角来审视集体理性。", "conclusion": "作者认为大型语言模型中的‘自由主义倾向’体现了对民主权利的聚焦，可以帮助我们以新的视角审视集体道德推理。"}}
{"id": "2507.08205", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.08205", "abs": "https://arxiv.org/abs/2507.08205", "authors": ["Ken C. L. Wong", "Hongzhi Wang", "Tanveer Syeda-Mahmood"], "title": "HNOSeg-XS: Extremely Small Hartley Neural Operator for Efficient and Resolution-Robust 3D Image Segmentation", "comment": "This paper was accepted by IEEE TMI 2025", "summary": "In medical image segmentation, convolutional neural networks (CNNs) and\ntransformers are dominant. For CNNs, given the local receptive fields of\nconvolutional layers, long-range spatial correlations are captured through\nconsecutive convolutions and pooling. However, as the computational cost and\nmemory footprint can be prohibitively large, 3D models can only afford fewer\nlayers than 2D models with reduced receptive fields and abstract levels. For\ntransformers, although long-range correlations can be captured by multi-head\nattention, its quadratic complexity with respect to input size is\ncomputationally demanding. Therefore, either model may require input size\nreduction to allow more filters and layers for better segmentation.\nNevertheless, given their discrete nature, models trained with patch-wise\ntraining or image downsampling may produce suboptimal results when applied on\nhigher resolutions. To address this issue, here we propose the\nresolution-robust HNOSeg-XS architecture. We model image segmentation by\nlearnable partial differential equations through the Fourier neural operator\nwhich has the zero-shot super-resolution property. By replacing the Fourier\ntransform by the Hartley transform and reformulating the problem in the\nfrequency domain, we created the HNOSeg-XS model, which is resolution robust,\nfast, memory efficient, and extremely parameter efficient. When tested on the\nBraTS'23, KiTS'23, and MVSeg'23 datasets with a Tesla V100 GPU, HNOSeg-XS\nshowed its superior resolution robustness with fewer than 34.7k model\nparameters. It also achieved the overall best inference time (< 0.24 s) and\nmemory efficiency (< 1.8 GiB) compared to the tested CNN and transformer\nmodels.", "AI": {"tldr": "本文提出的HNOSeg-XS架构通过傅里叶神经算子和哈特利变换优化，解决了医学图像分割中卷积神经网络和变压器模型的不足，实现了分辨率鲁棒、速度快、内存高效和参数高效的性能。", "motivation": "旨在克服卷积神经网络和变压器模型在医学图像分割中面临的问题，如高计算成本、内存占用以及因输入尺寸减小导致的性能不足。", "method": "提出了解決分辨率鲁棒性的HNOSeg-XS架构，该架构通过傅里叶神经算子学习可微分方程来建模图像分割，并通过哈特利变换和频率域问题重述来优化，从而具备零样本超分辨率特性。", "result": "在BraTS'23、KiTS'23和MVSeg'23数据集上测试时，HNOSeg-XS展示了其卓越的分辨率鲁棒性，使用少于34.7k模型参数，同时具有最佳推理时间（<0.24秒）和内存效率（<1.8 GiB）。", "conclusion": "HNOSeg-XS模型因其分辨率鲁棒性、速度快、内存高效以及参数效率极高，展现出优于测试中的CNN和变压器模型的性能。"}}
{"id": "2507.08029", "categories": ["cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2507.08029", "abs": "https://arxiv.org/abs/2507.08029", "authors": ["Ada Aka", "Emil Palikot", "Ali Ansari", "Nima Yazdani"], "title": "Better Together: Quantifying the Benefits of AI-Assisted Recruitment", "comment": null, "summary": "Artificial intelligence (AI) is increasingly used in recruitment, yet\nempirical evidence quantifying its impact on hiring efficiency and candidate\nselection remains limited. We randomly assign 37,000 applicants for a\njunior-developer position to either a traditional recruitment process (resume\nscreening followed by human selection) or an AI-assisted recruitment pipeline\nincorporating an initial AI-driven structured video interview before human\nevaluation. Candidates advancing from either track faced the same final-stage\nhuman interview, with interviewers blind to the earlier selection method. In\nthe AI-assisted pipeline, 54% of candidates passed the final interview compared\nwith 34% from the traditional pipeline, yielding an average treatment effect of\n20 percentage points (SE 12 pp.). Five months later, we collected LinkedIn\nprofiles of top applicants from both groups and found that 18% (SE 1.1%) of\napplicants from the traditional track found new jobs compared with 23% (SE\n2.3%) from the AI group, resulting in a 5.9 pp. (SE 2.6 pp.) difference in the\nprobability of finding new employment between groups. The AI system tended to\nselect younger applicants with less experience and fewer advanced credentials.\nWe analyze AI-generated interview transcripts to examine the selection criteria\nand conversational dynamics. Our findings contribute to understanding how AI\ntechnologies affect decision making in recruitment and talent acquisition while\nhighlighting some of their potential implications.", "AI": {"tldr": "研究表明，AI在招聘过程中提高了通过最终面试的比例和找到新工作的概率。", "motivation": "尽管人工智能在招聘中越来越普遍，但对其对招聘效率和候选人选择影响的实证证据仍然有限。本研究意在填补这一空白，通过随机实验量化AI对招聘过程的具体影响，并揭示AI技术在决策中的潜在影响。", "method": "本研究采用随机实验方法，将37000名应聘初级开发职位的申请者分配至传统招聘流程组或AI辅助招聘流程组。AI辅助组首先通过AI驱动的结构化视频面试，然后进入人工评估，而传统组则是基于简历筛选。两个组进入最终的人工面试阶段，面试官对之前的筛选方法不知情。", "result": "总体来看，应聘者在AI辅助组通过最终面试的比例为54%，而传统组仅为34%，两组间的平均差异为20个百分点。此外，五个月后对顶级候选人进行追踪发现，AI组找到新工作的比例为23%，而传统组为18%，两组间找到新工作的概率差异为5.9个百分点。", "conclusion": "研究表明，AI在招聘中可以提高最终面试通过率和找到工作的概率，并且倾向于选择更年轻的候选人。该发现有助于增进对AI技术在招聘与人才获取过程中影响的理解，并指出其潜在的应用问题。"}}
{"id": "2507.08223", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.08223", "abs": "https://arxiv.org/abs/2507.08223", "authors": ["Jackson Borchardt", "Saul Kato"], "title": "SurfDist: Interpretable Three-Dimensional Instance Segmentation Using Curved Surface Patches", "comment": "8 pages, 6 figures", "summary": "We present SurfDist, a convolutional neural network architecture for\nthree-dimensional volumetric instance segmentation. SurfDist enables prediction\nof instances represented as closed surfaces composed of smooth parametric\nsurface patches, specifically bicubic B\\'ezier triangles. SurfDist is a\nmodification of the popular model architecture StarDist-3D which breaks\nStarDist-3D's coupling of instance parameterization dimension and instance\nvoxel resolution, and it produces predictions which may be upsampled to\narbitrarily high resolutions without introduction of voxelization artifacts.\n  For datasets with blob-shaped instances, common in biomedical imaging,\nSurfDist can outperform StarDist-3D with more compact instance\nparameterizations. We detail SurfDist's technical implementation and show one\nsynthetic and one real-world dataset for which it outperforms StarDist-3D.\nThese results demonstrate that interpretable instance surface models can be\nlearned effectively alongside instance membership.", "AI": {"tldr": "SurfDist 是一种用于三维物体实例分割的新型 CNN 架构，它能够产生高性能的表面实例分割结果，特别是对于生物医学图像数据集更为有效。", "motivation": "克服已有模型在实例参数化维度上的限制，并为三维实例分割提供更高效且精确的解决方案。", "method": "Structure", "result": "{\n  \"tldr\": \"SurfDist 是一种用于三维体积实例分割的卷积神经网络架构，它能够预测具有光滑参数化表面补丁的闭合表面实例，其预测结果可以在不失真的情况下被任意放大。对于类似生物医学成像领域的球形实例而言，SurfDist 可以比 StarDist-3D 提供更紧凑的实例参数化，并且在合成数据和真实世界数据上都展示了更好的性能。\",\n  \"motivation\": \"解决实例参数化维度与实例体素分辨率的耦合问题，并提供一个在生物医学图像中常见的球形实例上能取得更紧凑参数化的解决方案。\",\n  \"method\": \"修改 StarDist-3D 模型架构，允许产生具有光滑 Bézier 三角形参数化表面的闭合表面实例预测结果。\",\n  \"result\": \"SurfDist 在给定的合成数据集和一个真实世界数据集上展示了比 StarDist-3D 更优的性能。\",\n  \"conclusion\": \"这种方式表明了同时学习实例表面模型和实例隶属性是有效的且具有解释性的。\n}", "conclusion": "这展示了解释性的实例表面模型与实例隶属性可以同时被有效学习的可能性。"}}
{"id": "2507.08030", "categories": ["cs.CL", "cs.CE", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.08030", "abs": "https://arxiv.org/abs/2507.08030", "authors": ["Sonali Sharma", "Ahmed M. Alaa", "Roxana Daneshjou"], "title": "A Systematic Analysis of Declining Medical Safety Messaging in Generative AI Models", "comment": "11 pages, 5 figures", "summary": "Generative AI models, including large language models (LLMs) and\nvision-language models (VLMs), are increasingly used to interpret medical\nimages and answer clinical questions. Their responses often include\ninaccuracies; therefore, safety measures like medical disclaimers are critical\nto remind users that AI outputs are not professionally vetted or a substitute\nfor medical advice. This study evaluated the presence of disclaimers in LLM and\nVLM outputs across model generations from 2022 to 2025. Using 500 mammograms,\n500 chest X-rays, 500 dermatology images, and 500 medical questions, outputs\nwere screened for disclaimer phrases. Medical disclaimer presence in LLM and\nVLM outputs dropped from 26.3% in 2022 to 0.97% in 2025, and from 19.6% in 2023\nto 1.05% in 2025, respectively. By 2025, the majority of models displayed no\ndisclaimers. As public models become more capable and authoritative,\ndisclaimers must be implemented as a safeguard adapting to the clinical context\nof each output.", "AI": {"tldr": "研究表明，2022年至2025年间，AI生成的医疗图像解释和回答上的医疗免责声明越来越少，强调必须在模型成熟后增加这类警示以确保安全。", "motivation": "随着医疗AI模型的广泛使用，它们生成的回答经常包含不准确的信息。因此，使用医疗免责声明提醒用户AI输出未经过专业审核或替代医疗建议变得尤为重要。", "method": "该研究通过分析2022年至2025年间生成模型（LLM和VLM）对500张乳腺X光片、500张胸部X光片、500张皮肤病图片和500个医学问题的输出，评估了医疗免责声明的存在情况。", "result": "研究结果表明，医疗免责声明在LLM和VLM输出中的存在比例从2022年的26.3%下降到2025年的0.97%，而在VLM从2023年的19.6%下降到2025年的1.05%。到了2025年，大多数模型都没有显示免责声明。", "conclusion": "随着公众使用的模型变得越来越有能力且权威，必须在每个输出的临床环境中实施警示声明作为保障措施。"}}
{"id": "2507.08240", "categories": ["cs.CV", "I.4.8; I.2.10"], "pdf": "https://arxiv.org/pdf/2507.08240", "abs": "https://arxiv.org/abs/2507.08240", "authors": ["Seoik Jung", "Taekyung Song"], "title": "Car Object Counting and Position Estimation via Extension of the CLIP-EBC Framework", "comment": "4 pages, 2 figures, submitted to a computer vision conference", "summary": "In this paper, we investigate the applicability of the CLIP-EBC framework,\noriginally designed for crowd counting, to car object counting using the CARPK\ndataset. Experimental results show that our model achieves second-best\nperformance compared to existing methods. In addition, we propose a K-means\nweighted clustering method to estimate object positions based on predicted\ndensity maps, indicating the framework's potential extension to localization\ntasks.", "AI": {"tldr": "我们研究了CLIP-EBC框架在CARPK数据集上的车辆计数性能，并提出了基于K-means加权聚类的改进方法，试验结果表明其接近现有最优方法。", "motivation": "探索CLIP-EBC框架在车辆对象计数中的应用潜力，并改进其在估计物体位置上的性能。", "method": "我们研究了原本用于人群计数的CLIP-EBC框架对车辆计数的适用性，并且提出了基于预测密度图的K-means加权聚类方法来估计物体位置。", "result": "实验结果显示，我们的模型在现有方法中表现出第二好的性能，并指出了该框架扩展至定位任务的潜力。", "conclusion": "CLIP-EBC框架在车辆计数方面有良好的表现，并通过改进方法展示了向更广泛定位任务扩展的潜力。"}}
{"id": "2507.08031", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.08031", "abs": "https://arxiv.org/abs/2507.08031", "authors": ["Hong Jia", "Shiya Fu", "Vassilis Kostakos", "Feng Xia", "Ting Dang"], "title": "Beyond Scale: Small Language Models are Comparable to GPT-4 in Mental Health Understanding", "comment": null, "summary": "The emergence of Small Language Models (SLMs) as privacy-preserving\nalternatives for sensitive applications raises a fundamental question about\ntheir inherent understanding capabilities compared to Large Language Models\n(LLMs). This paper investigates the mental health understanding capabilities of\ncurrent SLMs through systematic evaluation across diverse classification tasks.\nEmploying zero-shot and few-shot learning paradigms, we benchmark their\nperformance against established LLM baselines to elucidate their relative\nstrengths and limitations in this critical domain. We assess five\nstate-of-the-art SLMs (Phi-3, Phi-3.5, Qwen2.5, Llama-3.2, Gemma2) against\nthree LLMs (GPT-4, FLAN-T5-XXL, Alpaca-7B) on six mental health understanding\ntasks. Our findings reveal that SLMs achieve mean performance within 2\\% of\nLLMs on binary classification tasks (F1 scores of 0.64 vs 0.66 in zero-shot\nsettings), demonstrating notable competence despite orders of magnitude fewer\nparameters. Both model categories experience similar degradation on multi-class\nseverity tasks (a drop of over 30\\%), suggesting that nuanced clinical\nunderstanding challenges transcend model scale. Few-shot prompting provides\nsubstantial improvements for SLMs (up to 14.6\\%), while LLM gains are more\nvariable. Our work highlights the potential of SLMs in mental health\nunderstanding, showing they can be effective privacy-preserving tools for\nanalyzing sensitive online text data. In particular, their ability to quickly\nadapt and specialize with minimal data through few-shot learning positions them\nas promising candidates for scalable mental health screening tools.", "AI": {"tldr": "研究发现小型语言模型在心理健康理解任务上的性能接近大型语言模型，并能够通过少量样本学习快速适应和专业化，显示出作为大规模心理健康筛查工具的潜力。", "motivation": "研究小型语言模型在保护隐私的应用中与大型语言模型相比在心理健康理解方面的能力，以了解它们在这一领域的适用性。", "method": "通过系统评估五种最先进的小型语言模型（Phi-3, Phi-3.5, Qwen2.5, Llama-3.2, Gemma2）和三种大型语言模型（GPT-4, FLAN-T5-XXL, Alpaca-7B）在六个心理健康理解任务上的性能，以零样本和少量样本学习范式进行基准测试，比较它们在关键领域的相对优缺点。", "result": "小型语言模型在二分类任务上平均性能仅比大型语言模型低2%，在少量样本学习中表现出显著提升，显示出它们在隐私保护方面的应用潜力。", "conclusion": "小型语言模型在心理健康领域展现了有效的能力，特别是在少量样本学习中，表现出作为隐私保护工具的优势，并具有开发成大规模心理健康筛查工具的潜力。"}}
{"id": "2507.08248", "categories": ["cs.CV", "cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.08248", "abs": "https://arxiv.org/abs/2507.08248", "authors": ["Jason Kahei Tam", "Murilo Gustineli", "Anthony Miyaguchi"], "title": "Transfer Learning and Mixup for Fine-Grained Few-Shot Fungi Classification", "comment": null, "summary": "Accurate identification of fungi species presents a unique challenge in\ncomputer vision due to fine-grained inter-species variation and high\nintra-species variation. This paper presents our approach for the FungiCLEF\n2025 competition, which focuses on few-shot fine-grained visual categorization\n(FGVC) using the FungiTastic Few-Shot dataset. Our team (DS@GT) experimented\nwith multiple vision transformer models, data augmentation, weighted sampling,\nand incorporating textual information. We also explored generative AI models\nfor zero-shot classification using structured prompting but found them to\nsignificantly underperform relative to vision-based models. Our final model\noutperformed both competition baselines and highlighted the effectiveness of\ndomain specific pretraining and balanced sampling strategies. Our approach\nranked 35/74 on the private test set in post-completion evaluation, this\nsuggests additional work can be done on metadata selection and domain-adapted\nmulti-modal learning. Our code is available at\nhttps://github.com/dsgt-arc/fungiclef-2025.", "AI": {"tldr": "本文提出的方法结合了视觉变压器模型、数据增强、加权抽样及文本信息，取得了比竞赛基线更好的结果，尤其强调了领域内预训练和抽样策略的有效性。", "motivation": "精准识别真菌种类是计算机视觉中的独特挑战，由于种间变异细小且种内变异大。本论文为FungiCLEF 2025竞赛展示了一种少样本细粒度视觉分类的方法。", "method": "本研究团队（DS@GT）采用多种视觉变压器模型、数据增强、加权抽样以及结合文本信息的方法。另外，我们也探索了使用生成式AI模型进行零样本分类，但效果不如视觉模型。", "result": "最终模型在竞赛基准上表现优异，并在比赛后评估的私有测试集上排名35/74，这表明在元数据选择和领域自适应多模态学习方面仍有进步空间。", "conclusion": "该研究强调了领域特定的预训练和平衡采样策略的有效性，并指出未来工作可以在元数据选择和领域自适应多模态学习方面做进一步研究。"}}
{"id": "2507.08034", "categories": ["cs.CL", "cs.AI", "cs.LG", "68T50", "I.2.7; I.2.6"], "pdf": "https://arxiv.org/pdf/2507.08034", "abs": "https://arxiv.org/abs/2507.08034", "authors": ["Nripesh Niketan", "Hadj Batatia"], "title": "Integrating External Tools with Large Language Models to Improve Accuracy", "comment": "9 pages, 3 figures, 2 tables. Extended version of paper published in\n  Proceedings of International Conference on Information Technology and\n  Applications, Springer Nature Singapore, 2025, pp. 409-421. This version\n  includes additional experimental results comparing against GPT-4o,\n  LLaMA-Large, Mistral-Large, and Phi-Large, expanded evaluation methodology,\n  and enhanced analysis", "summary": "This paper deals with improving querying large language models (LLMs). It is\nwell-known that without relevant contextual information, LLMs can provide poor\nquality responses or tend to hallucinate. Several initiatives have proposed\nintegrating LLMs with external tools to provide them with up-to-date data to\nimprove accuracy. In this paper, we propose a framework to integrate external\ntools to enhance the capabilities of LLMs in answering queries in educational\nsettings. Precisely, we develop a framework that allows accessing external APIs\nto request additional relevant information. Integrated tools can also provide\ncomputational capabilities such as calculators or calendars. The proposed\nframework has been evaluated using datasets from the Multi-Modal Language\nUnderstanding (MMLU) collection. The data consists of questions on mathematical\nand scientific reasoning. Results compared to state-of-the-art language models\nshow that the proposed approach significantly improves performance. Our Athena\nframework achieves 83% accuracy in mathematical reasoning and 88% in scientific\nreasoning, substantially outperforming all tested models including GPT-4o,\nLLaMA-Large, Mistral-Large, Phi-Large, and GPT-3.5, with the best baseline\nmodel (LLaMA-Large) achieving only 67% and 79% respectively. These promising\nresults open the way to creating complex computing ecosystems around LLMs to\nmake their use more natural to support various tasks and activities.", "AI": {"tldr": "本文提出了一种集成外部工具以增强LLMs在教育环境中答案质量的框架。", "motivation": "为了提高LLMs在教育环境中的查询能力，解决其在缺乏相关上下文信息时容易提供低质量回答或幻觉的问题。", "method": "本论文提出了一种框架，允许LLMs访问外部API以请求相关信息，并增强了计算能力，如计算器或日历等功能。", "result": "实验结果表明，该框架在数学推理和科学推理问题上的准确率显著高于现有的语言模型，分别达到了83%和88%。", "conclusion": "该框架的成功展示了围绕LLMs创建复杂计算生态系统的可能性，使其应用更为自然，支持更多任务和活动。"}}
{"id": "2507.08268", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.08268", "abs": "https://arxiv.org/abs/2507.08268", "authors": ["J. D. Peiffer", "Kunal Shah", "Irina Djuraskovic", "Shawana Anarwala", "Kayan Abdou", "Rujvee Patel", "Prakash Jayabalan", "Brenton Pennicooke", "R. James Cotton"], "title": "Portable Biomechanics Laboratory: Clinically Accessible Movement Analysis from a Handheld Smartphone", "comment": "15 pages, 7 figures", "summary": "The way a person moves is a direct reflection of their neurological and\nmusculoskeletal health, yet it remains one of the most underutilized vital\nsigns in clinical practice. Although clinicians visually observe movement\nimpairments, they lack accessible and validated methods to objectively measure\nmovement in routine care. This gap prevents wider use of biomechanical\nmeasurements in practice, which could enable more sensitive outcome measures or\nearlier identification of impairment. We present our Portable Biomechanics\nLaboratory (PBL), which includes a secure, cloud-enabled smartphone app for\ndata collection and a novel algorithm for fitting biomechanical models to this\ndata. We extensively validated PBL's biomechanical measures using a large,\nclinically representative dataset. Next, we tested the usability and utility of\nour system in neurosurgery and sports medicine clinics. We found joint angle\nerrors within 3 degrees across participants with neurological injury,\nlower-limb prosthesis users, pediatric inpatients, and controls. In addition to\nbeing easy to use, gait metrics computed from the PBL showed high reliability\nand were sensitive to clinical differences. For example, in individuals\nundergoing decompression surgery for cervical myelopathy, the mJOA score is a\ncommon patient-reported outcome measure; we found that PBL gait metrics\ncorrelated with mJOA scores and demonstrated greater responsiveness to surgical\nintervention than the patient-reported outcomes. These findings support the use\nof handheld smartphone video as a scalable, low-burden tool for capturing\nclinically meaningful biomechanical data, offering a promising path toward\naccessible monitoring of mobility impairments. We release the first clinically\nvalidated method for measuring whole-body kinematics from handheld smartphone\nvideo at\nhttps://intelligentsensingandrehabilitation.github.io/MonocularBiomechanics/ .", "AI": {"tldr": "本文介绍了一种便携式生物力学实验室(PBL)，可以使用手持智能手机视频捕捉具有临床意义的生物力学数据，提供了一种可扩展且低负担的监控运动障碍的方法。", "motivation": "尽管临床医生通过视觉观察到运动障碍，但他们缺乏能够客观测量运动的方法。这种差距阻止了生物力学测量在临床实践中的更广泛应用，可能导致更敏感的结果衡量或更早识别障碍。", "method": "我们介绍了一种便携式生物力学实验室(PBL)，其中包括一个用于数据收集的安全、基于云端的智能手机应用程序和一种用于将生物力学模型拟合到这些数据的新型算法。", "result": "我们在神经外科和运动医学诊所测试了该系统的可用性和效用。测试结果显示了关节角度误差在参与者的3度以内，涵盖了神经系统损伤、下肢假肢使用者、儿科住院病人和对照组。使用PBL计算出的步态参数显示出了高可靠性，并且对临床差异敏感。", "conclusion": "这些发现支持了手持智能手机视频作为一种可扩展、低负担的工具，用于捕捉具有临床意义的生物力学数据，为无障碍监控运动障碍提供了一个有希望的途径。"}}
{"id": "2507.08036", "categories": ["cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.08036", "abs": "https://arxiv.org/abs/2507.08036", "authors": ["Deepali Mishra", "Chaklam Silpasuwanchai", "Ashutosh Modi", "Madhumita Sushil", "Sorayouth Chumnanvej"], "title": "Barriers in Integrating Medical Visual Question Answering into Radiology Workflows: A Scoping Review and Clinicians' Insights", "comment": "29 pages, 5 figures (1 in supplementary), 3 tables (1 in main text, 2\n  in supplementary). Scoping review and clinician survey", "summary": "Medical Visual Question Answering (MedVQA) is a promising tool to assist\nradiologists by automating medical image interpretation through question\nanswering. Despite advances in models and datasets, MedVQA's integration into\nclinical workflows remains limited. This study systematically reviews 68\npublications (2018-2024) and surveys 50 clinicians from India and Thailand to\nexamine MedVQA's practical utility, challenges, and gaps. Following the Arksey\nand O'Malley scoping review framework, we used a two-pronged approach: (1)\nreviewing studies to identify key concepts, advancements, and research gaps in\nradiology workflows, and (2) surveying clinicians to capture their perspectives\non MedVQA's clinical relevance. Our review reveals that nearly 60% of QA pairs\nare non-diagnostic and lack clinical relevance. Most datasets and models do not\nsupport multi-view, multi-resolution imaging, EHR integration, or domain\nknowledge, features essential for clinical diagnosis. Furthermore, there is a\nclear mismatch between current evaluation metrics and clinical needs. The\nclinician survey confirms this disconnect: only 29.8% consider MedVQA systems\nhighly useful. Key concerns include the absence of patient history or domain\nknowledge (87.2%), preference for manually curated datasets (51.1%), and the\nneed for multi-view image support (78.7%). Additionally, 66% favor models\nfocused on specific anatomical regions, and 89.4% prefer dialogue-based\ninteractive systems. While MedVQA shows strong potential, challenges such as\nlimited multimodal analysis, lack of patient context, and misaligned evaluation\napproaches must be addressed for effective clinical integration.", "AI": {"tldr": "这项研究通过文献综述和临床医生调查，探讨了医学视觉问答（MedVQA）的实际效用和挑战。研究发现MedVQA在临床中的应用潜力很大，但仍存在问题，特别是在多模态分析、患者背景整合和评估方法上。", "motivation": "尽管模型和技术数据集有所进步，但MedVQA在临床工作流程中的整合依然有限。这项研究系统地回顾了2018年至2024年间发表的68篇论文，并对来自印度和泰国的50位临床医生进行了调查，以探讨MedVQA的实际应用性、挑战和空白。", "method": "研究采用了Arksey和O'Malley的范围审查框架，结合了文献回顾和临床医生调查两种方法。文件回顾用于识别放射学工作流程中的关键概念、进展和研究空白；而临床医生调查用于收集他们对MedVQA临床相关性的看法。", "result": "研究揭示了大约60%的问题答案对诊断没有帮助，且缺乏临床相关性。大多数数据集和模型不支持多视角、多分辨率成像、电子病历集成或领域知识，这些都是临床诊断所必需的。此外，当前的评估指标和临床需求之间存在明显不符。有29.8%的临床医生认为MedVQA系统非常有用，而主要关切包括缺乏病史或专业知识、更倾向于手动整理的数据集和多视角成像支持等。", "conclusion": "尽管MedVQA显示出强大的潜力，但要实现有效的临床整合，必须解决诸如有限的多模态分析、缺乏患者背景和评估方法失准等挑战。"}}
{"id": "2507.08290", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.08290", "abs": "https://arxiv.org/abs/2507.08290", "authors": ["Jiang Qin", "Bin Zou", "Haolin Li", "Lamei Zhang"], "title": "Cross-Resolution SAR Target Detection Using Structural Hierarchy Adaptation and Reliable Adjacency Alignment", "comment": "Submitted to IEEE TGRS (major revision)", "summary": "In recent years, continuous improvements in SAR resolution have significantly\nbenefited applications such as urban monitoring and target detection. However,\nthe improvement in resolution leads to increased discrepancies in scattering\ncharacteristics, posing challenges to the generalization ability of target\ndetection models. While domain adaptation technology is a potential solution,\nthe inevitable discrepancies caused by resolution differences often lead to\nblind feature adaptation and unreliable semantic propagation, ultimately\ndegrading the domain adaptation performance. To address these challenges, this\npaper proposes a novel SAR target detection method (termed CR-Net), that\nincorporates structure priors and evidential learning theory into the detection\nmodel, enabling reliable domain adaptation for cross-resolution detection. To\nbe specific, CR-Net integrates Structure-induced Hierarchical Feature\nAdaptation (SHFA) and Reliable Structural Adjacency Alignment (RSAA). SHFA\nmodule is introduced to establish structural correlations between targets and\nachieve structure-aware feature adaptation, thereby enhancing the\ninterpretability of the feature adaptation process. Afterwards, the RSAA module\nis proposed to enhance reliable semantic alignment, by leveraging the secure\nadjacency set to transfer valuable discriminative knowledge from the source\ndomain to the target domain. This further improves the discriminability of the\ndetection model in the target domain. Based on experimental results from\ndifferent-resolution datasets,the proposed CR-Net significantly enhances\ncross-resolution adaptation by preserving intra-domain structures and improving\ndiscriminability. It achieves state-of-the-art (SOTA) performance in\ncross-resolution SAR target detection.", "AI": {"tldr": "The paper proposes CR-Net, a method for enhancing performance in cross-resolution SAR target detection through integration of structural and evidential learning theory, achieving state-of-the-art results.", "motivation": "The motivation for this research is to develop a method that can effectively solve the problem of cross-resolution detection challenge in SAR images, particularly as these images are increasingly being used in applications like urban monitoring where the resolution can vary significantly, impacting the performance of detection models.", "method": "The paper introduces CR-Net, which combines structure priors and evidential learning theory to address discrepancies in target detection models caused by resolution differences. It incorporates two key modules: Structure-induced Hierarchical Feature Adaptation (SHFA) for enhancing the interpretability of feature adaptation and Reliable Structural Adjacency Alignment (RSAA) for improving reliable semantic alignment and discriminability of the model.", "result": "Experimental results using different-resolution SAR datasets show that CR-Net improves cross-resolution detection performance, indicating improved intra-domain structure preservation and discriminability compared to previous methods.", "conclusion": "The research concludes that CR-Net is effective in enhancing cross-resolution SAR target detection performance, preserving intra-domain structures, and improving the discriminability of the detection model, achieving state-of-the-art results."}}
{"id": "2507.08037", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.08037", "abs": "https://arxiv.org/abs/2507.08037", "authors": ["Matan Vetzler", "Koren Lazar", "Guy Uziel", "Eran Hirsch", "Ateret Anaby-Tavor", "Leshem Choshen"], "title": "CRISP: Complex Reasoning with Interpretable Step-based Plans", "comment": null, "summary": "Recent advancements in large language models (LLMs) underscore the need for\nstronger reasoning capabilities to solve complex problems effectively. While\nChain-of-Thought (CoT) reasoning has been a step forward, it remains\ninsufficient for many domains. A promising alternative is explicit high-level\nplan generation, but existing approaches largely assume that LLMs can produce\neffective plans through few-shot prompting alone, without additional training.\nIn this work, we challenge this assumption and introduce CRISP (Complex\nReasoning with Interpretable Step-based Plans), a multi-domain dataset of\nhigh-level plans for mathematical reasoning and code generation. The plans in\nCRISP are automatically generated and rigorously validated--both intrinsically,\nusing an LLM as a judge, and extrinsically, by evaluating their impact on\ndownstream task performance. We demonstrate that fine-tuning a small model on\nCRISP enables it to generate higher-quality plans than much larger models using\nfew-shot prompting, while significantly outperforming Chain-of-Thought\nreasoning. Furthermore, our out-of-domain evaluation reveals that fine-tuning\non one domain improves plan generation in the other, highlighting the\ngeneralizability of learned planning capabilities.", "AI": {"tldr": "CRISP是一种用于多个领域的数学推理和代码生成的高阶计划数据集，展示了一种通过微调小模型以生成高质量计划的方法，这种方法比大型模型的少量提示方法更有效，并且比链式思维推理方法有显著提升。", "motivation": "作者提出CRISP（Complex Reasoning with Interpretable Step-based Plans）数据集，旨在解决大型语言模型在高阶推理中的不足，并挑战现有的假设，即语言模型仅通过少量提示即可生成有效的计划。", "method": "提出了一种自动并严格验证高阶计划的方法，这些计划通过小型模型的微调生成，比大型模型的少量提示方法生成的计划更高质量。", "result": "实验表明，在一个领域微调的模型在其他领域也能生成更好的计划，展示了学习规划能力的泛化性。", "conclusion": "CRISP数据集的使用和在小型模型上的微调能够极大地提高计划的质量，这比传统的少量提示方法更具优势，并且也有优于Chain-of-Thought推理方法的表现。"}}
{"id": "2507.08307", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.08307", "abs": "https://arxiv.org/abs/2507.08307", "authors": ["Kui Jiang", "Shiyu Liu", "Junjun Jiang", "Xin Yang", "Hongxun Yang", "Xiaopeng Fan"], "title": "M2DAO-Talker: Harmonizing Multi-granular Motion Decoupling and Alternating Optimization for Talking-head Generation", "comment": null, "summary": "Audio-driven talking head generation holds significant potential for film\nproduction. While existing 3D methods have advanced motion modeling and content\nsynthesis, they often produce rendering artifacts, such as motion blur,\ntemporal jitter, and local penetration, due to limitations in representing\nstable, fine-grained motion fields. Through systematic analysis, we reformulate\ntalking head generation into a unified framework comprising three steps: video\npreprocessing, motion representation, and rendering reconstruction. This\nframework underpins our proposed M2DAO-Talker, which addresses current\nlimitations via multi-granular motion decoupling and alternating\noptimization.Specifically, we devise a novel 2D portrait preprocessing pipeline\nto extract frame-wise deformation control conditions (motion region\nsegmentation masks, and camera parameters) to facilitate motion representation.\nTo ameliorate motion modeling, we elaborate a multi-granular motion decoupling\nstrategy, which independently models non-rigid (oral and facial) and rigid\n(head) motions for improved reconstruction accuracy.Meanwhile, a motion\nconsistency constraint is developed to ensure head-torso kinematic consistency,\nthereby mitigating penetration artifacts caused by motion aliasing. In\naddition, an alternating optimization strategy is designed to iteratively\nrefine facial and oral motion parameters, enabling more realistic video\ngeneration.Experiments across multiple datasets show that M2DAO-Talker achieves\nstate-of-the-art performance, with the 2.43 dB PSNR improvement in generation\nquality and 0.64 gain in user-evaluated video realness versus TalkingGaussian\nwhile with 150 FPS inference speed. Our project homepage is\nhttps://m2dao-talker.github.io/M2DAO-Talk.github.io", "AI": {"tldr": "研究提出了一个新的框架M2DAO-Talker，用于改进音频驱动的虚拟人物生成的技术，通过多层次运动解耦和交替优化方法达到更真实的效果。", "motivation": "音频驱动的虚拟人物生成对于电影制作具有重要意义。然而，现有的3D方法在表示稳定、精细的运动场方面存在局限性，导致渲染出现伪影。", "method": "通过系统分析，我们将Talking Head生成重新定义为一个由三个步骤组成的统一框架：视频预处理、运动表示和图像重建。这种方法构成了我们提出的M2DAO-Talker的基础，该方法通过多层次运动解耦和交替优化解决了当前的限制。我们设计了一个新的2D肖像预处理流程，以提取逐帧的变形控制条件。为了改善运动建模，我们制定了一个多层次运动解耦策略，独立地建模非刚性（口腔和面部）和刚体（头部）运动。同时，开发了运动一致性约束以确保头部和躯干的运动一致性，以减少运动混淆引起的穿透伪影。此外，设计了一种交替优化策略，以迭代地优化面部和口腔运动参数，从而生成更真实的视频。", "result": "实验结果显示，M2DAO-Talker实现了生成质量2.43 dB PSNR的提高和用户评价的0.64真实感增益，同时具备150 FPS的推理速度。", "conclusion": "新的方法和技术提高了音频驱动的虚拟人物生成的真实性、稳定性和速度，展现了在电影制作中的应用潜力。"}}
{"id": "2507.08038", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.08038", "abs": "https://arxiv.org/abs/2507.08038", "authors": ["Talor Abramovich", "Gal Chechik"], "title": "AblationBench: Evaluating Automated Planning of Ablations in Empirical AI Research", "comment": null, "summary": "Autonomous agents built on language models (LMs) are showing increasing\npopularity in many fields, including scientific research. AI co-scientists aim\nto support or automate parts of the research process using these agents. A key\ncomponent of empirical AI research is the design of ablation experiments. To\nthis end, we introduce AblationBench, a benchmark suite for evaluating agents\non ablation planning tasks in empirical AI research. It includes two tasks:\nAuthorAblation, which helps authors propose ablation experiments based on a\nmethod section and contains 83 instances, and ReviewerAblation, which helps\nreviewers find missing ablations in a full paper and contains 350 instances.\nFor both tasks, we develop LM-based judges that serve as an automatic\nevaluation framework. Our experiments with frontier LMs show that these tasks\nremain challenging, with the best-performing LM system identifying only 29% of\nthe original ablations on average. Lastly, we analyze the limitations of\ncurrent LMs on these tasks, and find that chain-of-thought prompting\noutperforms the currently existing agent-based approach.", "AI": {"tldr": "这篇论文介绍了一个名为AblationBench的基准套件，用于评估代理在规划消融实验方面的性能。包含两个任务，一个是帮助科研人员提出消融实验（AuthorAblation），另一个是帮助审稿人发现论文中缺失的消融实验（ReviewerAblation）。研究发现，目前最先进的语言模型在完成这些任务上仍面临挑战。", "motivation": "随着基于语言模型的自主代理在科研领域的应用日益增多，论文旨在通过设计消融实验来推动AI研究，特别是在支持或自动化科研过程方面。为了评估这些代理的能力，作者构建了一个名为AblationBench的基准测试套件，专注于评估代理在规划消融实验中的表现。", "method": "创建了AblationBench基准套件，包含AuthorAblation和ReviewerAblation两个任务，用于帮助作者提出消融实验和帮助审稿人发现完整论文中的缺失的消融实验。针对这两个任务，开发了基于语言模型的裁判，形成自动评估框架。", "result": "研究采用了两个任务来评估代理在规划消融实验方面的能力，分别是AuthorAblation和ReviewerAblation，分别包含83个和350个实例，其中最优秀的语言模型系统平均仅能识别原始消融实验的29%。实验结果表明了这些任务对当前语言模型的挑战性，同时也发现基于思考链的提示方法优于现有的基于代理的方法。", "conclusion": "通过实验发现，尽管语言模型在很多任务上表现出色，但在设计和识别消融实验上仍然存在较大困难，表明了在这一特定研究任务上的技术限制和未来的改进方向。同时，探索发现思考链提示方法能够带来更好的性能表现。"}}
{"id": "2507.08329", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.08329", "abs": "https://arxiv.org/abs/2507.08329", "authors": ["Ravi Shankar Prasad", "Dinesh Singh"], "title": "Cross-Domain Identity Representation for Skull to Face Matching with Benchmark DataSet", "comment": "7 pages, 12 figures, Pattern Recognition Letters", "summary": "Craniofacial reconstruction in forensic science is crucial for the\nidentification of the victims of crimes and disasters. The objective is to map\na given skull to its corresponding face in a corpus of faces with known\nidentities using recent advancements in computer vision, such as deep learning.\nIn this paper, we presented a framework for the identification of a person\ngiven the X-ray image of a skull using convolutional Siamese networks for\ncross-domain identity representation. Siamese networks are twin networks that\nshare the same architecture and can be trained to discover a feature space\nwhere nearby observations that are similar are grouped and dissimilar\nobservations are moved apart. To do this, the network is exposed to two sets of\ncomparable and different data. The Euclidean distance is then minimized between\nsimilar pairs and maximized between dissimilar ones. Since getting pairs of\nskull and face images are difficult, we prepared our own dataset of 40\nvolunteers whose front and side skull X-ray images and optical face images were\ncollected. Experiments were conducted on the collected cross-domain dataset to\ntrain and validate the Siamese networks. The experimental results provide\nsatisfactory results on the identification of a person from the given skull.", "AI": {"tldr": "研究提出了一种利用卷积孪生网络来基于颅骨X光图像进行身份识别的方法，并通过自制的数据集进行了验证，表明该方法具有较好的识别效果。", "motivation": "颅面重建在法医科学中对于识别犯罪和灾难中的受害者至关重要。本文旨在利用计算机视觉领域的最新进展，如深度学习，将给定的颅骨映射到具有已知身份的面部数据集中。", "method": "本文提出了一种框架，用于基于颅骨的X光图像进行人员识别，采用的是卷积孪生网络来实现跨域身份表示。孪生网络是一对结构相同的网络，可以训练以发现一个特征空间，在这个空间中，相似的观测结果靠近而不同的观测结果分开。网络通过对比相似和不同数据对来学习。由于获取颅骨和面部图像配对较为困难，作者制作了包含40名志愿者的专用数据集，收集了他们的正面和侧面颅骨X光图像以及光学面部图像。", "result": "实验结果表明，该框架在从给定颅骨识别人员方面表现令人满意。", "conclusion": "实验验证了卷积孪生网络在跨域识别任务中的有效性，证明了该方法在基于颅骨X光图像进行人员识别任务中的适用性。"}}
{"id": "2507.08045", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.08045", "abs": "https://arxiv.org/abs/2507.08045", "authors": ["Junyi Wen", "Junyuan Liang", "Zicong Hong", "Wuhui Chen", "Zibin Zheng"], "title": "Krul: Efficient State Restoration for Multi-turn Conversations with Dynamic Cross-layer KV Sharing", "comment": null, "summary": "Efficient state restoration in multi-turn conversations with large language\nmodels (LLMs) remains a critical challenge, primarily due to the overhead of\nrecomputing or loading full key-value (KV) caches for all historical tokens. To\naddress this, existing approaches compress KV caches across adjacent layers\nwith highly similar attention patterns. However, these methods often apply a\nfixed compression scheme across all conversations, selecting the same layer\npairs for compression without considering conversation-specific attention\ndynamics. This static strategy overlooks variability in attention pattern\nsimilarity across different conversations, which can lead to noticeable\naccuracy degradation.\n  We present Krul, a multi-turn LLM inference system that enables accurate and\nefficient KV cache restoration. Krul dynamically selects compression strategies\nbased on attention similarity across layer pairs and uses a\nrecomputation-loading pipeline to restore the KV cache. It introduces three key\ninnovations: 1) a preemptive compression strategy selector to preserve critical\ncontext for future conversation turns and selects a customized strategy for the\nconversation; 2) a token-wise heterogeneous attention similarity estimator to\nmitigate the attention similarity computation and storage overhead during model\ngeneration; 3) a bubble-free restoration scheduler to reduce potential bubbles\nbrought by the imbalance of recomputing and loading stream due to compressed KV\ncaches. Empirical evaluations on real-world tasks demonstrate that Krul\nachieves a 1.5x-2.68x reduction in time-to-first-token (TTFT) and a 1.33x-2.35x\nreduction in KV cache storage compared to state-of-the-art methods without\ncompromising generation quality.", "AI": {"tldr": "Krul提出了一种多轮对话的大语言模型推理系统，通过创新性地使用动态压缩策略选择、异构注意相似性估计和无‘气泡’恢复调度器，实现了更低的TTFT和KV缓存存储需求。", "motivation": "由于对所有历史Token重新计算或加载全量KV缓存的高昂成本，高效多轮对话中的状态恢复是迫切需要解决的问题。当前方法通过高度类似的注意力模式对相邻层的KV缓存进行压缩，忽略了不同对话之间的注意力模式相似度差异。", "method": "Krul系统通过动态选择压缩策略、减少注意相似性计算和存储开销的异构注意相似性估计器以及通过平衡重新计算和加载流来减少潜在的‘气泡’问题的无气泡恢复调度器实现了高效的KV缓存恢复。", "result": "实验表明，与最先进的方法相比，Krul在不降低生成质量的情况下，首次Token产出时间（TTFT）减少了1.5倍至2.68倍，KV缓存存储减少了1.33倍至2.35倍。", "conclusion": "Krul系统解决了多轮对话中KV缓存恢复的问题，通过动态选择压缩策略等创新，实现高效且准确的KV缓存恢复，同时显著减少了首次Token产出时间和KV缓存存储需求。"}}
{"id": "2507.08330", "categories": ["cs.CV", "cs.AI", "cs.ET", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.08330", "abs": "https://arxiv.org/abs/2507.08330", "authors": ["Nikita Malik", "Pratinav Seth", "Neeraj Kumar Singh", "Chintan Chitroda", "Vinay Kumar Sankarapu"], "title": "Interpretability-Aware Pruning for Efficient Medical Image Analysis", "comment": "Pre-Print", "summary": "Deep learning has driven significant advances in medical image analysis, yet\nits adoption in clinical practice remains constrained by the large size and\nlack of transparency in modern models. Advances in interpretability techniques\nsuch as DL-Backtrace, Layer-wise Relevance Propagation, and Integrated\nGradients make it possible to assess the contribution of individual components\nwithin neural networks trained on medical imaging tasks. In this work, we\nintroduce an interpretability-guided pruning framework that reduces model\ncomplexity while preserving both predictive performance and transparency. By\nselectively retaining only the most relevant parts of each layer, our method\nenables targeted compression that maintains clinically meaningful\nrepresentations. Experiments across multiple medical image classification\nbenchmarks demonstrate that this approach achieves high compression rates with\nminimal loss in accuracy, paving the way for lightweight, interpretable models\nsuited for real-world deployment in healthcare settings.", "AI": {"tldr": "介绍了一种解释性指导剪枝框架，实现了高压缩率而仅损失极小的准确性，并且保持了临床相关性。", "motivation": "深度学习推动了医学影像分析领域的显著进步，但在临床实践中的应用受到现代模型规模大和缺乏透明度的限制。解释性技术的进展使我们能够评估在医学影像任务上训练的神经网络中各组件的贡献。", "method": "通过采用解释性指导剪枝框架，在减少模型复杂性的同时保留预测性能和透明度。通过选择性保留每一层中最相关的部分，该方法实现了有针对性的压缩，保持了临床有意义的表示。", "result": "在多个医学影像分类基准上进行的实验表明，该方法能够在不损失准确性的情况下实现高压缩率。", "conclusion": "这种方法为在医疗保健环境中部署的轻量级、可解释模型铺平了道路。"}}
{"id": "2507.08107", "categories": ["cs.CL", "cs.DB", "cs.IR"], "pdf": "https://arxiv.org/pdf/2507.08107", "abs": "https://arxiv.org/abs/2507.08107", "authors": ["Sebastian Walter", "Hannah Bast"], "title": "GRASP: Generic Reasoning And SPARQL Generation across Knowledge Graphs", "comment": null, "summary": "We propose a new approach for generating SPARQL queries on RDF knowledge\ngraphs from natural language questions or keyword queries, using a large\nlanguage model. Our approach does not require fine-tuning. Instead, it uses the\nlanguage model to explore the knowledge graph by strategically executing SPARQL\nqueries and searching for relevant IRIs and literals. We evaluate our approach\non a variety of benchmarks (for knowledge graphs of different kinds and sizes)\nand language models (of different scales and types, commercial as well as\nopen-source) and compare it with existing approaches. On Wikidata we reach\nstate-of-the-art results on multiple benchmarks, despite the zero-shot setting.\nOn Freebase we come close to the best few-shot methods. On other, less commonly\nevaluated knowledge graphs and benchmarks our approach also performs well\noverall. We conduct several additional studies, like comparing different ways\nof searching the graphs, incorporating a feedback mechanism, or making use of\nfew-shot examples.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2507.08334", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.08334", "abs": "https://arxiv.org/abs/2507.08334", "authors": ["Sangwon Kim", "In-su Jang", "Pyongkun Kim", "Kwang-Ju Kim"], "title": "CoCo-Bot: Energy-based Composable Concept Bottlenecks for Interpretable Generative Models", "comment": null, "summary": "Concept Bottleneck Models (CBMs) provide interpretable and controllable\ngenerative modeling by routing generation through explicit,\nhuman-understandable concepts. However, previous generative CBMs often rely on\nauxiliary visual cues at the bottleneck to compensate for information not\ncaptured by the concepts, which undermines interpretability and\ncompositionality. We propose CoCo-Bot, a post-hoc, composable concept\nbottleneck generative model that eliminates the need for auxiliary cues by\ntransmitting all information solely through explicit concepts. Guided by\ndiffusion-based energy functions, CoCo-Bot supports robust post-hoc\ninterventions-such as concept composition and negation-across arbitrary\nconcepts. Experiments using StyleGAN2 pre-trained on CelebA-HQ show that\nCoCo-Bot improves concept-level controllability and interpretability, while\nmaintaining competitive visual quality.", "AI": {"tldr": "CoCo-Bot是一种可组合的概念瓶颈生成模型，它通过明确的概念传输所有信息，避免依赖辅助视觉线索，从而提高了可解释性和组合性。", "motivation": "尽管概念瓶颈模型(CBMs)在生成模型中提供了可解释性和可控性，但它们常常依赖辅助视觉线索来弥补概念未捕捉到的信息，这削弱了可解释性和组合性。", "method": "我们提出了一种名为CoCo-Bot的生成模型，它通过明确的概念传输所有信息，消除了对辅助视觉线索的依赖，从而提高了可解释性和组合性。该模型使用基于扩散的能量函数支持对任意概念的后验干预，如概念组合和否定。", "result": "实验显示，基于CelebA-HQ数据集的StyleGAN2预训练模型，CoCo-Bot在保持具有竞争力的视觉质量的同时，改善了概念水平的控制力和可解释性。", "conclusion": "CoCo-Bot通过传递所有信息而无需辅助视觉线索，解决了先前CBMs可解释性和组合性的问题，并在生成模型中实现了更高质量的操控性和可解释性。"}}
{"id": "2507.08109", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.08109", "abs": "https://arxiv.org/abs/2507.08109", "authors": ["Reilly Raab", "Mike Parker", "Dan Nally", "Sadie Montgomery", "Anastasia Bernat", "Sai Munikoti", "Sameera Horawalavithana"], "title": "Audit, Alignment, and Optimization of LM-Powered Subroutines with Application to Public Comment Processing", "comment": null, "summary": "The advent of language models (LMs) has the potential to dramatically\naccelerate tasks that may be cast to text-processing; however, real-world\nadoption is hindered by concerns regarding safety, explainability, and bias.\nHow can we responsibly leverage LMs in a transparent, auditable manner --\nminimizing risk and allowing human experts to focus on informed decision-making\nrather than data-processing or prompt engineering? In this work, we propose a\nframework for declaring statically typed, LM-powered subroutines (i.e.,\ncallable, function-like procedures) for use within conventional asynchronous\ncode -- such that sparse feedback from human experts is used to improve the\nperformance of each subroutine online (i.e., during use). In our\nimplementation, all LM-produced artifacts (i.e., prompts, inputs, outputs, and\ndata-dependencies) are recorded and exposed to audit on demand. We package this\nframework as a library to support its adoption and continued development. While\nthis framework may be applicable across several real-world decision workflows\n(e.g., in healthcare and legal fields), we evaluate it in the context of public\ncomment processing as mandated by the 1969 National Environmental Protection\nAct (NEPA): Specifically, we use this framework to develop \"CommentNEPA,\" an\napplication that compiles, organizes, and summarizes a corpus of public\ncommentary submitted in response to a project requiring environmental review.\nWe quantitatively evaluate the application by comparing its outputs (when\noperating without human feedback) to historical ``ground-truth'' data as\nlabelled by human annotators during the preparation of official environmental\nimpact statements.", "AI": {"tldr": "本文提出了一种框架，用于在传统异步代码中声明静态类型的由语言模型支持的子程序，通过提供透明、可审计的方式，减少风险并改进决策流程。该框架被封装为库以支持广泛的应用，例如在处理公共意见时的应用。", "motivation": "语言模型虽然有助于加速文本处理任务，但安全、可解释性和偏见问题阻碍了其实际应用。作者希望通过一种透明、可审计的方式负责任地利用语言模型，使人类专家能够专注于决策制定。", "method": "作者提出了一种框架，使语言模型生成的子程序可以集成到传统异步代码中，并通过少量的人类专家反馈在线改进性能。", "result": "该框架用于评估公共意见处理系统，尤其是在1969年《国家环境保护法》要求的情况下。通过将系统输出与人工注释的历史“地面真相”数据进行量化对比来评估表现。", "conclusion": "本文主要展示了一种可能的框架，通过引入静态类型定义和反馈机制，使得语言模型能够以一种可长期支持的方式应用到实际决策过程中。"}}
{"id": "2507.08340", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.08340", "abs": "https://arxiv.org/abs/2507.08340", "authors": ["Jia-Xuan Jiang", "Jiashuai Liu", "Hongtao Wu", "Yifeng Wu", "Zhong Wang", "Qi Bi", "Yefeng Zheng"], "title": "Single-Domain Generalization for Multimodal Cross-Cancer Prognosis via Dirac Rebalancer and Distribution Entanglement", "comment": "Accepted by ACMMM 25", "summary": "Deep learning has shown remarkable performance in integrating multimodal data\nfor survival prediction. However, existing multimodal methods mainly focus on\nsingle cancer types and overlook the challenge of generalization across\ncancers. In this work, we are the first to reveal that multimodal prognosis\nmodels often generalize worse than unimodal ones in cross-cancer scenarios,\ndespite the critical need for such robustness in clinical practice. To address\nthis, we propose a new task: Cross-Cancer Single Domain Generalization for\nMultimodal Prognosis, which evaluates whether models trained on a single cancer\ntype can generalize to unseen cancers. We identify two key challenges: degraded\nfeatures from weaker modalities and ineffective multimodal integration. To\ntackle these, we introduce two plug-and-play modules: Sparse Dirac Information\nRebalancer (SDIR) and Cancer-aware Distribution Entanglement (CADE). SDIR\nmitigates the dominance of strong features by applying Bernoulli-based\nsparsification and Dirac-inspired stabilization to enhance weaker modality\nsignals. CADE, designed to synthesize the target domain distribution, fuses\nlocal morphological cues and global gene expression in latent space.\nExperiments on a four-cancer-type benchmark demonstrate superior\ngeneralization, laying the foundation for practical, robust cross-cancer\nmultimodal prognosis. Code is available at\nhttps://github.com/HopkinsKwong/MCCSDG", "AI": {"tldr": "研究引入了两个模块来解决多模态模型在跨癌症场景中的泛化问题，通过实验验证了这些模型在四个癌症类型上的优越泛化能力。", "motivation": "当前的多模态方法主要集中在单一类型的癌症上，忽视了跨癌症类型泛化的挑战。此工作首次揭示了在跨癌症场景中，多模态预后模型往往不如单模态模型泛化得好，提出了跨癌症单域泛化任务以解决此问题。", "method": "提出了两个即插即用模块：Sparse Dirac Information Rebalancer (SDIR)和Cancer-aware Distribution Entanglement (CADE)来解决跨癌症类型的多模态预后预测中的问题。SDIR通过伯努利稀疏化和狄拉克启发的稳定化来减少强特征的主导作用，增强弱特征信号；CADE则设计用于综合目标领域分布，在潜在空间中融合局部形态线索和全局基因表达。", "result": "实验在四种癌症类型的基准数据集上进行，结果显示所提模型具有更强的跨癌症类型泛化能力，为实践中稳健的跨癌症多模态预后提供了基础。", "conclusion": "所提出的SDIR和CADE模块能够有效解决跨癌症类型多模态预后预测中的泛化挑战，提供了新型泛化任务的解决方案，有助于临床实践中的应用。"}}
{"id": "2507.08143", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.08143", "abs": "https://arxiv.org/abs/2507.08143", "authors": ["Vivek Chari", "Benjamin Van Durme"], "title": "Compactor: Calibrated Query-Agnostic KV Cache Compression with Approximate Leverage Scores", "comment": null, "summary": "Modern Large Language Models (LLMs) are increasingly trained to support very\nlarge context windows. Unfortunately the ability to use long contexts in\ngeneration is complicated by the large memory requirement of the KV cache,\nwhich scales linearly with the context length. This memory footprint is often\nthe dominant resource bottleneck in real-world deployments, limiting throughput\nand increasing serving cost. One way to address this is by compressing the KV\ncache, which can be done either with knowledge of the question being asked\n(query-aware) or without knowledge of the query (query-agnostic). We present\nCompactor, a parameter-free, query-agnostic KV compression strategy that uses\napproximate leverage scores to determine token importance. We show that\nCompactor can achieve the same performance as competing methods while retaining\n1/2 the tokens in both synthetic and real-world context tasks, with minimal\ncomputational overhead. We further introduce a procedure for context-calibrated\ncompression, which allows one to infer the maximum compression ratio a given\ncontext can support. Using context-calibrated compression, we show that\nCompactor achieves full KV performance on Longbench while reducing the KV\nmemory burden by 63%, on average. To demonstrate the efficacy and\ngeneralizability of our approach, we apply Compactor to 27 synthetic and\nreal-world tasks from RULER and Longbench, with models from both the Qwen 2.5\nand Llama 3.1 families.", "AI": {"tldr": "Compactor是一种无参数、与查询无关的KV压缩策略，使用近似杠杆分数确定标记的重要性，减少了内存负担并保持性能不变。", "motivation": "To reduce the memory requirement of the KV cache in Large Language Models (LLMs) without牺牲性能.", "method": "Compactor, a parameter-free, query-agnostic KV compression strategy using approximate leverage scores to determine token importance.", "result": "Compactor能够将存储量减少一半，同时保持性能不变。在Longbench上，它实现了完整的KV性能，减少了63%的内存负担。在27个合成和现实世界任务中也证明了其有效性和通用性。", "conclusion": "Compactor是一种有效的KV缓存压缩方法，能够在减少内存使用的同时保持性能，适用于不同模型和任务。"}}
{"id": "2507.08343", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.08343", "abs": "https://arxiv.org/abs/2507.08343", "authors": ["Junxue Yang", "Xin Liao", "Weixuan Tang", "Jianhua Yang", "Zheng Qin"], "title": "Towards Imperceptible JPEG Image Hiding: Multi-range Representations-driven Adversarial Stego Generation", "comment": null, "summary": "Deep hiding has been exploring the hiding capability of deep learning-based\nmodels, aiming to conceal image-level messages into cover images and reveal\nthem from generated stego images. Existing schemes are easily detected by\nsteganalyzers due to their large payloads and their limitation to feature\nextraction based solely on either pure convolution or pure transformer\noperators within a single range, as well as pixel-level loss constraints. To\naddress the issue, in this paper, we introduce generation-based adversarial\nattacks into color JPEG image deep hiding and propose a multi-range\nrepresentations-driven adversarial stego generation framework called MRAG from\na steganalysis perspective. Specifically, we integrate the local-range neighbor\nreception characteristic of the convolution and the global-range dependency\nmodeling of the transformer to construct MRAG. Meanwhile, we use the\ntransformed images obtained through coarse-grained and fine-grained frequency\ndecomposition as inputs, introducing multi-grained information. Furthermore, a\nfeatures angle-norm disentanglement loss is designed to constrain the generated\nstegos closer to covers in the angle and norm space of the steganalyzer's\nclassified features. Consequently, small yet effective adversarial\nperturbations can be injected into the process of generating stegos, ensuring\nthat stegos maintain favorable secret restorability and imperceptibility.\nExtensive experiments demonstrate that MRAG can achieve state-of-the-art\nperformance.", "AI": {"tldr": "MRAG, a novel adversarial stego generation framework, integrates convolution and transformer operators with a multi-grained approach to achieve better resistance against steganalysis, resulting in state-of-the-art performance in deep hiding.", "motivation": "to improve upon existing deep hiding schemes which are easily detected by advanced steganalyzers due to limitations in feature extraction and reliance on single-range operators, leading to large payloads and reduced hiding capability.", "method": "existing deep hiding schemes are susceptible to detection due to large payloads and rely solely on either convolution or transformer operators which limits their feature extraction capabilities. MRAG, the proposed solution, integrates convolution and transformer operators to leverage local and global-range dependency modeling, respectively, and introduces multi-grained information by using coarse-grained and fine-grained frequency decomposed images. An angle-norm disentanglement loss is also used to ensure generated stegos are close to covers in norm and angle space, thus resisting more advanced steganalysis.", "result": "extensive experiments show that MRAG achieves state-of-the-art performance in deep hiding, indicating its effectiveness in resisting detection while maintaining secret restorability and imperceptibility.", "conclusion": "MRAG, by integrating convolution and transformer operators with a multi-grained approach, provides an advanced method for deep hiding that not only resists detection but also maintains high-quality secret communications."}}
{"id": "2507.08151", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.08151", "abs": "https://arxiv.org/abs/2507.08151", "authors": ["Henry J. Xie", "Jinghan Zhang", "Xinhao Zhang", "Kunpeng Liu"], "title": "Distilling Empathy from Large Language Models", "comment": "Accepted by SIGDIAL 2025", "summary": "The distillation of knowledge from Large Language Models (LLMs) into Smaller\nLanguage Models (SLMs), preserving the capabilities and performance of LLMs\nwhile reducing model size, has played a key role in the proliferation of LLMs.\nBecause SLMs are considerably smaller than LLMs, they are often utilized in\ndomains where human interaction is frequent but resources are highly\nconstrained, e.g., smart phones. Therefore, it is crucial to ensure that\nempathy, a fundamental aspect of positive human interactions, already instilled\ninto LLMs, is retained by SLMs after distillation. In this paper, we develop a\ncomprehensive approach for effective empathy distillation from LLMs into SLMs.\nOur approach features a two-step fine-tuning process that fully leverages\ndatasets of empathetic dialogue responses distilled from LLMs. We explore\nseveral distillation methods beyond basic direct prompting and propose four\nunique sets of prompts for targeted empathy improvement to significantly\nenhance the empathy distillation process. Our evaluations demonstrate that SLMs\nfine-tuned through the two-step fine-tuning process with distillation datasets\nenhanced by the targeted empathy improvement prompts significantly outperform\nthe base SLM at generating empathetic responses with a win rate of 90%. Our\ntargeted empathy improvement prompts substantially outperform the basic direct\nprompting with a 10% improvement in win rate.", "AI": {"tldr": "本次研究专注于从大型语言模型向小型语言模型迁移同理心的能力，运用独特方法使SLMs在同理心响应上表现优于原始版本。", "motivation": "动机在于，我们需要将大型语言模型（LLMs）中的同理心能力有效迁移到小型语言模型（SLMs）中，以确保它们在资源受限环境下仍能进行高质量的人机交互。", "method": "我们的研究方法包括一个两步微调过程以及使用从LLMs中提取的同理心对话数据集。我们不仅进行了基本的直接提示，还提出了四个独特的提示集，专门用于同理心提升。", "result": "实验结果显示，在利用两步微调过程和专为同理心改善设计的提示集增强的数据集后，小型语言模型生成同理心响应的胜率高达90%，比基础版本SLMs提高了10%。", "conclusion": "我们的研究结论为同理心的有效迁移提供了新的见解，通过精心设计的方法可以显著提升小型语言模型的同理心能力。"}}
{"id": "2507.08344", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.08344", "abs": "https://arxiv.org/abs/2507.08344", "authors": ["Jihao Gu", "Fei Wang", "Kun Li", "Yanyan Wei", "Zhiliang Wu", "Dan Guo"], "title": "MM-Gesture: Towards Precise Micro-Gesture Recognition through Multimodal Fusion", "comment": null, "summary": "In this paper, we present MM-Gesture, the solution developed by our team\nHFUT-VUT, which ranked 1st in the micro-gesture classification track of the 3rd\nMiGA Challenge at IJCAI 2025, achieving superior performance compared to\nprevious state-of-the-art methods. MM-Gesture is a multimodal fusion framework\ndesigned specifically for recognizing subtle and short-duration micro-gestures\n(MGs), integrating complementary cues from joint, limb, RGB video,\nTaylor-series video, optical-flow video, and depth video modalities. Utilizing\nPoseConv3D and Video Swin Transformer architectures with a novel\nmodality-weighted ensemble strategy, our method further enhances RGB modality\nperformance through transfer learning pre-trained on the larger MA-52 dataset.\nExtensive experiments on the iMiGUE benchmark, including ablation studies\nacross different modalities, validate the effectiveness of our proposed\napproach, achieving a top-1 accuracy of 73.213%.", "AI": {"tldr": "本文介绍了 MM-Gesture，这是一个由研发团队 HFUT-VUT 开发的多模态手势识别解决方案，在第3届 MiGA 挑战赛的手势分类赛道中排名第一，识别准确率达到 73.213%。", "motivation": "研发团队 HFUT-VUT 为了在第3届 MiGA 挑战赛的手势分类赛道上取得优越表现，开发了 MM-Gesture 解决方案，旨在解决微手势识别中的精度问题，相比之下优于现有的各类方法。", "method": "MM-Gesture 是一种专为识别微妙且短暂的微手势 (MGs) 设计的多模态融合框架。该框架结合了关节、肢体、RGB 视频、泰勒级数视频、光流视频和深度视频等多种模态的互补线索。该方法使用了 PoseConv3D 和 Video Swin Transformer 架构，并结合了新颖的模态加权集成策略，在更大的 MA-52 数据集上通过迁移学习进一步提升了 RGB 模态的表现。", "result": "在 iMiGUE 基准测试上的实验结果包括不同模态的消融研究，验证了 MM-Gesture 方法的有效性，取得了 73.213% 的 top-1 准确率。", "conclusion": "MM-Gesture 方案的有效性通过在 iMiGUE 基准测试中的实验得到了验证，达到了 73.213% 的 top-1 准确率。这表明该方案在微手势识别方面具有优越性能。"}}
{"id": "2507.08203", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.08203", "abs": "https://arxiv.org/abs/2507.08203", "authors": ["Duygu Nur Yaldiz", "Yavuz Faruk Bakman", "Sungmin Kang", "Alperen Öziş", "Hayrettin Eren Yildiz", "Mitash Ashish Shah", "Zhiqi Huang", "Anoop Kumar", "Alfy Samuel", "Daben Liu", "Sai Praneeth Karimireddy", "Salman Avestimehr"], "title": "TruthTorchLM: A Comprehensive Library for Predicting Truthfulness in LLM Outputs", "comment": null, "summary": "Generative Large Language Models (LLMs)inevitably produce untruthful\nresponses. Accurately predicting the truthfulness of these outputs is critical,\nespecially in high-stakes settings. To accelerate research in this domain and\nmake truthfulness prediction methods more accessible, we introduce TruthTorchLM\nan open-source, comprehensive Python library featuring over 30 truthfulness\nprediction methods, which we refer to as Truth Methods. Unlike existing\ntoolkits such as Guardrails, which focus solely on document-grounded\nverification, or LM-Polygraph, which is limited to uncertainty-based methods,\nTruthTorchLM offers a broad and extensible collection of techniques. These\nmethods span diverse tradeoffs in computational cost, access level (e.g.,\nblack-box vs white-box), grounding document requirements, and supervision type\n(self-supervised or supervised). TruthTorchLM is seamlessly compatible with\nboth HuggingFace and LiteLLM, enabling support for locally hosted and API-based\nmodels. It also provides a unified interface for generation, evaluation,\ncalibration, and long-form truthfulness prediction, along with a flexible\nframework for extending the library with new methods. We conduct an evaluation\nof representative truth methods on three datasets, TriviaQA, GSM8K, and\nFactScore-Bio. The code is available at https://github.com/Ybakman/TruthTorchLM", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2507.08357", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.08357", "abs": "https://arxiv.org/abs/2507.08357", "authors": ["Shishuai Hu", "Zehui Liao", "Liangli Zhen", "Huazhu Fu", "Yong Xia"], "title": "Cycle Context Verification for In-Context Medical Image Segmentation", "comment": "MICCAI 2025", "summary": "In-context learning (ICL) is emerging as a promising technique for achieving\nuniversal medical image segmentation, where a variety of objects of interest\nacross imaging modalities can be segmented using a single model. Nevertheless,\nits performance is highly sensitive to the alignment between the query image\nand in-context image-mask pairs. In a clinical scenario, the scarcity of\nannotated medical images makes it challenging to select optimal in-context\npairs, and fine-tuning foundation ICL models on contextual data is infeasible\ndue to computational costs and the risk of catastrophic forgetting. To address\nthis challenge, we propose Cycle Context Verification (CCV), a novel framework\nthat enhances ICL-based medical image segmentation by enabling\nself-verification of predictions and accordingly enhancing contextual\nalignment. Specifically, CCV employs a cyclic pipeline in which the model\ninitially generates a segmentation mask for the query image. Subsequently, the\nroles of the query and an in-context pair are swapped, allowing the model to\nvalidate its prediction by predicting the mask of the original in-context\nimage. The accuracy of this secondary prediction serves as an implicit measure\nof the initial query segmentation. A query-specific prompt is introduced to\nalter the query image and updated to improve the measure, thereby enhancing the\nalignment between the query and in-context pairs. We evaluated CCV on seven\nmedical image segmentation datasets using two ICL foundation models,\ndemonstrating its superiority over existing methods. Our results highlight\nCCV's ability to enhance ICL-based segmentation, making it a robust solution\nfor universal medical image segmentation. The code will be available at\nhttps://github.com/ShishuaiHu/CCV.", "AI": {"tldr": "提出了一种名为Cycle Context Verification (CCV)的新框架，通过自验证预测和增强上下文对齐来改进基于In-context learning (ICL)的医学图像分割技术。", "motivation": "ICL在实现通用医学图像分割方面显示出潜力，但其性能依赖于查询图像与上下文图像掩模对之间的对齐，且在临床场景下选取最适上下文图像是一个挑战。", "method": "CCV框架采用一个循环流程，模型首先生成查询图像的分割掩模，然后交换查询与上下文图像的角色，通过预测原始上下文图像的掩模来验证预测，通过调整查询特有提示来改善对齐。", "result": "评估了CCV在七个医学图像分割数据集上的表现，使用了两种ICL基础模型，结果显示CCV优于现有方法。", "conclusion": "CCV展现了增强ICL的基础上的分割能力，成为一种针对通用医学图像分割的健壮解决方案。"}}
{"id": "2507.08218", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.08218", "abs": "https://arxiv.org/abs/2507.08218", "authors": ["Atticus Wang", "Joshua Engels", "Oliver Clive-Griffin"], "title": "Simple Mechanistic Explanations for Out-Of-Context Reasoning", "comment": "ICML 2025 Workshop R2-FM", "summary": "Out-of-context reasoning (OOCR) is a phenomenon in which fine-tuned LLMs\nexhibit surprisingly deep out-of-distribution generalization. Rather than\nlearning shallow heuristics, they implicitly internalize and act on the\nconsequences of observations scattered throughout the fine-tuning data. In this\nwork, we investigate this phenomenon mechanistically and find that many\ninstances of OOCR in the literature have a simple explanation: the LoRA\nfine-tuning essentially adds a constant steering vector, steering the model\ntowards a general concept. This improves performance on the fine-tuning task\nand in many other concept-related domains, causing the surprising\ngeneralization. Moreover, we can directly train steering vectors for these\ntasks from scratch, which also induces OOCR. We find that our results hold even\nfor a task that seems like it must involve conditional behavior (model\nbackdoors); it turns out that unconditionally adding a steering vector is\nsufficient. Overall, our work presents one explanation of what gets learned\nduring fine-tuning for OOCR tasks, contributing to the key question of why LLMs\ncan reason out of context, an advanced capability that is highly relevant to\ntheir safe and reliable deployment.", "AI": {"tldr": "研究通过分析语言模型的OOCR现象，提出了一种简单的解释机制—基于LoRA技术的细调过程通过添加一个引导向量引导模型走向普遍概念，这一方法不仅在微调任务中有效，还在许多其他相关概念领域诱发了出乎意料的一般化推广能力。", "motivation": "研究动机在于探索一种机制，通过这种机制，可以解释在文献中许多关于超出上下文理解推理(OOCR)现象的案例。这类现象涉及到微调后的语言模型在面对分布外数据时表现出的意外深入的一般化推广能力。具体来说，就是解释微调是如何使模型在多个相关概念领域展示出更好的任务表现。", "method": "本研究提出了一种基于LoRA（低秩适应）细调技术的简单解释：通过附加一个常量引导向量，引导模型趋向于普遍概念，以此说明细调过程如何实现OOCR现象。研究发现，可以直接从头训练这些任务的引导向量，这同样能诱导OOCR现象。在无条件添加引导向量的情况下，研究甚至发现，即便是看起来需要条件行为的任务（如模型回溯），也可以实现OOCR。", "result": "研究表明，即使在涉及条件行为的任务，如模型后门的情况下，无条件地添加引导向量也足以实现OOCR现象，揭示了在OOCR任务中细调期间学习的机制。", "conclusion": "本研究通过分析提供了一种关于在细调过程中学习到什么来进行OOCR任务的解释，为解答为什么语言模型可以进行超背景推理这一核心问题做出了贡献，这个问题对于语言模型的安全可靠部署至关重要。"}}
{"id": "2507.08367", "categories": ["cs.CV", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.08367", "abs": "https://arxiv.org/abs/2507.08367", "authors": ["Yuki Yoshihara", "Linjing Jiang", "Nihan Karatas", "Hitoshi Kanamori", "Asuka Harada", "Takahiro Tanaka"], "title": "Understanding Driving Risks using Large Language Models: Toward Elderly Driver Assessment", "comment": null, "summary": "This study investigates the potential of a multimodal large language model\n(LLM), specifically ChatGPT-4o, to perform human-like interpretations of\ntraffic scenes using static dashcam images. Herein, we focus on three judgment\ntasks relevant to elderly driver assessments: evaluating traffic density,\nassessing intersection visibility, and recognizing stop signs recognition.\nThese tasks require contextual reasoning rather than simple object detection.\nUsing zero-shot, few-shot, and multi-shot prompting strategies, we evaluated\nthe performance of the model with human annotations serving as the reference\nstandard. Evaluation metrics included precision, recall, and F1-score. Results\nindicate that prompt design considerably affects performance, with recall for\nintersection visibility increasing from 21.7% (zero-shot) to 57.0%\n(multi-shot). For traffic density, agreement increased from 53.5% to 67.6%. In\nstop-sign detection, the model demonstrated high precision (up to 86.3%) but a\nlower recall (approximately 76.7%), indicating a conservative response\ntendency. Output stability analysis revealed that humans and the model faced\ndifficulties interpreting structurally ambiguous scenes. However, the model's\nexplanatory texts corresponded with its predictions, enhancing\ninterpretability. These findings suggest that, with well-designed prompts, LLMs\nhold promise as supportive tools for scene-level driving risk assessments.\nFuture studies should explore scalability using larger datasets, diverse\nannotators, and next-generation model architectures for elderly driver\nassessments.", "AI": {"tldr": "本研究表明多模态大型语言模型在经过适当提示设计后，可作为老年驾驶评估支持工具，通过零样本、少样本及多样本学习策略提升其对复杂交通场景的理解。", "motivation": "研究动机旨在探索多模态大规模语言模型在模拟人类对交通场景解释的能力，特别是对于老年驾驶者评估相关的任务。", "method": "本研究利用零样本、少样本和多样本提示策略评估了多模态大规模语言模型ChatGPT-4o在静态车载摄像头图像中进行交通场景解释的能力，主要考察了三个与老年司机评估相关的任务：交通密度评估、交叉口视野评估及停车标志识别。", "result": "结果显示提示设计显著影响性能，交叉口视野评估中的召回率从零样本情形的21.7%提升至多样本情形的57.0%；交通密度评估中的一致性则从53.5%提升至67.6%。在停车标志检测上，模型表现出了较高的精确度（最高达86.3%）但召回率较低（约为76.7%），显示出其保守的响应倾向。", "conclusion": "大型语言模型在设计良好的提示下，有助于驾驶风险评估的支持工具，未来研究可考虑利用更大的数据集、多样化的注释员以及下一代模型架构进行扩展。"}}
{"id": "2507.08232", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.08232", "abs": "https://arxiv.org/abs/2507.08232", "authors": ["KV Aditya Srivatsa", "Kaushal Kumar Maurya", "Ekaterina Kochmar"], "title": "Can LLMs Reliably Simulate Real Students' Abilities in Mathematics and Reading Comprehension?", "comment": "Accepted to the 20th Workshop on Innovative Use of NLP for Building\n  Educational Applications (BEA), co-located with ACL 2025", "summary": "Large Language Models (LLMs) are increasingly used as proxy students in the\ndevelopment of Intelligent Tutoring Systems (ITSs) and in piloting test\nquestions. However, to what extent these proxy students accurately emulate the\nbehavior and characteristics of real students remains an open question. To\ninvestigate this, we collected a dataset of 489 items from the National\nAssessment of Educational Progress (NAEP), covering mathematics and reading\ncomprehension in grades 4, 8, and 12. We then apply an Item Response Theory\n(IRT) model to position 11 diverse and state-of-the-art LLMs on the same\nability scale as real student populations. Our findings reveal that, without\nguidance, strong general-purpose models consistently outperform the average\nstudent at every grade, while weaker or domain-mismatched models may align\nincidentally. Using grade-enforcement prompts changes models' performance, but\nwhether they align with the average grade-level student remains highly model-\nand prompt-specific: no evaluated model-prompt pair fits the bill across\nsubjects and grades, underscoring the need for new training and evaluation\nstrategies. We conclude by providing guidelines for the selection of viable\nproxies based on our findings.", "AI": {"tldr": "该研究探讨了大型语言模型（LLMs）作为智能教学系统（ITSs）中的代理学生在模仿实际学生行为和特征方面的准确性。发现即使没有引导，强大的通用模型在每个年级上都表现优异，但需要新的训练和评估策略来实现一致的模拟。", "motivation": "旨在探讨大型语言模型（LLMs）在没有引导的情况下模仿实际学生的准确程度，包括它们的行为和特征。这引起了对LLMs作为代理学生在智能教学系统（ITSs）和问题测试开发中使用的广泛探讨。", "method": "我们收集了来自国家教育进步评估（NAEP）的489项数学和阅读理解题目，涉及4、8、12年级，并使用项目反应理论（IRT）模型将11种多样化的最新大型语言模型（LLMs）与实际学生群体置于同一能力尺度上。", "result": "研究结果表明，没有引导的情况下，强大的通用模型在每个年级上都较平均学生表现优异，而较弱或领域不匹配的模型可能偶有对齐。使用年级指导提示会对模型性能产生影响，但没有评估过的模型-提示对能够在主题和年级上一致地对齐到平均学生水平。", "conclusion": "结论指出了当前大型语言模型在作为实际学生的代理使用时存在的局限性，强调了发展新的训练和评估策略的必要性，并提供了基于研究结果选择可行代理模型的指南。"}}
{"id": "2507.08375", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.08375", "abs": "https://arxiv.org/abs/2507.08375", "authors": ["Alexandra Malyugina", "Yini Li", "Joanne Lin", "Nantheera Anantrasirichai"], "title": "Unsupervised Methods for Video Quality Improvement: A Survey of Restoration and Enhancement Techniques", "comment": null, "summary": "Video restoration and enhancement are critical not only for improving visual\nquality, but also as essential pre-processing steps to boost the performance of\na wide range of downstream computer vision tasks. This survey presents a\ncomprehensive review of video restoration and enhancement techniques with a\nparticular focus on unsupervised approaches. We begin by outlining the most\ncommon video degradations and their underlying causes, followed by a review of\nearly conventional and deep learning methods-based, highlighting their\nstrengths and limitations. We then present an in-depth overview of unsupervised\nmethods, categorise by their fundamental approaches, including domain\ntranslation, self-supervision signal design and blind spot or noise-based\nmethods. We also provide a categorization of loss functions employed in\nunsupervised video restoration and enhancement, and discuss the role of paired\nsynthetic datasets in enabling objective evaluation. Finally, we identify key\nchallenges and outline promising directions for future research in this field.", "AI": {"tldr": "这篇综述文章详细介绍了视频恢复和增强技术，重点是无监督方法，分析了现有技术的挑战及未来研究方向，目的是改善视频质量及计算机视觉任务表现。", "motivation": "旨在通过综述视频恢复和增强技术，特别是无监督方法，来提高视频质量并改进计算机视觉任务的性能。", "method": "该论文综述了无监督视频恢复和增强技术，将其分为领域翻译、自监督信号设计以及盲点或噪声方法三大类，同时讨论了无监督视频恢复和增强中使用的损失函数分类，以及合成配对数据集在客观评估中的作用。", "result": "论文详细介绍了不同类型无监督方法的优势和局限性，分析了现有技术的挑战，并提出了未来研究的方向。", "conclusion": "最后，该论文明确了领域内的一些关键挑战，并指出了未来的研究方向。"}}
{"id": "2507.08241", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.08241", "abs": "https://arxiv.org/abs/2507.08241", "authors": ["Ancita Maria Andrade", "Tanvi Banerjee", "Ramakrishna Mundugar"], "title": "Exploring Gender Differences in Chronic Pain Discussions on Reddit", "comment": "This is an extended version of the short paper accepted at ASONAM\n  2025", "summary": "Pain is an inherent part of human existence, manifesting as both physical and\nemotional experiences, and can be categorized as either acute or chronic. Over\nthe years, extensive research has been conducted to understand the causes of\npain and explore potential treatments, with contributions from various\nscientific disciplines. However, earlier studies often overlooked the role of\ngender in pain experiences. In this study, we utilized Natural Language\nProcessing (NLP) to analyze and gain deeper insights into individuals' pain\nexperiences, with a particular focus on gender differences. We successfully\nclassified posts into male and female corpora using the Hidden Attribute\nModel-Convolutional Neural Network (HAM-CNN), achieving an F1 score of 0.86 by\naggregating posts based on usernames. Our analysis revealed linguistic\ndifferences between genders, with female posts tending to be more emotionally\nfocused. Additionally, the study highlighted that conditions such as migraine\nand sinusitis are more prevalent among females and explored how pain medication\naffects individuals differently based on gender.", "AI": {"tldr": "本研究利用NLP技术和HAM-CNN模型，分析了男性与女性在疼痛体验上的差异，发现女性在情感表达上更为丰富，并揭示了某些疾病在性别上的分布差异。", "motivation": "早期对于疼痛的研究往往忽视了性别在疼痛体验中的作用，本研究旨在填补这一空白，利用先进的自然语言处理技术来探讨男性与女性在疼痛经历上的差异。", "method": "本研究采用了自然语言处理技术，特别是使用了隐含属性模型-卷积神经网络（HAM-CNN）来分类性别相关的文本数据，并依据用户名汇编帖子，以性别差异为重点分析个体的疼痛体验。", "result": "研究成功地使用HAM-CNN把帖子分类为男性和女性两种语料库，获得了0.86的F1分值。分析结果揭示了性别的语言差异，女性帖子更倾向于情感关注。此外，研究还发现偏头痛和鼻窦炎在女性中更为普遍，以及如何依据性别差异探究止痛药对个体的影响。", "conclusion": "研究结果强调了性别在疼痛研究中的重要性，并指出在疼痛管理和药物治疗上，需要考虑性别差异以达到更有效的治疗效果。"}}
{"id": "2507.08380", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.08380", "abs": "https://arxiv.org/abs/2507.08380", "authors": ["Sen Wang", "Shao Zeng", "Tianjun Gu", "Zhizhong Zhang", "Ruixin Zhang", "Shouhong Ding", "Jingyun Zhang", "Jun Wang", "Xin Tan", "Yuan Xie", "Lizhuang Ma"], "title": "From Enhancement to Understanding: Build a Generalized Bridge for Low-light Vision via Semantically Consistent Unsupervised Fine-tuning", "comment": "Accepted by ICCV 2025", "summary": "Low-level enhancement and high-level visual understanding in low-light vision\nhave traditionally been treated separately. Low-light enhancement improves\nimage quality for downstream tasks, but existing methods rely on physical or\ngeometric priors, limiting generalization. Evaluation mainly focuses on visual\nquality rather than downstream performance. Low-light visual understanding,\nconstrained by scarce labeled data, primarily uses task-specific domain\nadaptation, which lacks scalability. To address these challenges, we build a\ngeneralized bridge between low-light enhancement and low-light understanding,\nwhich we term Generalized Enhancement For Understanding (GEFU). This paradigm\nimproves both generalization and scalability. To address the diverse causes of\nlow-light degradation, we leverage pretrained generative diffusion models to\noptimize images, achieving zero-shot generalization performance. Building on\nthis, we propose Semantically Consistent Unsupervised Fine-tuning (SCUF).\nSpecifically, to overcome text prompt limitations, we introduce an\nillumination-aware image prompt to explicitly guide image generation and\npropose a cycle-attention adapter to maximize its semantic potential. To\nmitigate semantic degradation in unsupervised training, we propose caption and\nreflectance consistency to learn high-level semantics and image-level spatial\nsemantics. Extensive experiments demonstrate that our proposed method\noutperforms current state-of-the-art methods in traditional image quality and\nGEFU tasks including classification, detection, and semantic segmentation.", "AI": {"tldr": "本文提出了Generalized Enhancement For Understanding (GEFU)框架，利用生成扩散模型和一系列创新技术来优化低光照条件下的图像增强和理解任务的性能，显著提升了泛化能力和可扩展性。", "motivation": "传统的低光照图像处理方法存在泛化能力和性能局限性，特别是将图像增强和高级视觉理解割裂开来。本文旨在通过构建一个通用框架，即Generalized Enhancement For Understanding (GEFU)，来解决这些问题，提高低光照图像处理技术的性能和泛化能力。", "method": "本文提出了Generalized Enhancement For Understanding (GEFU)，该框架利用预训练的生成扩散模型优化低光照图像，并考虑光照感知图像提示和循环注意力适配器以提高语义一致性。还提出了标签和反射一致性来改善无监督训练中的语义学习。", "result": "低光照条件下，图像增强与高级视觉理解通常被分开处理。现有增强方法依赖物理或几何先验，限制了泛化能力，且评估主要集中在视觉质量而非下游任务性能。低光照理解由于标注数据稀缺，主要依赖任务特定的领域适应，缺乏可扩展性。为解决这些挑战，我们提出Generalized Enhancement For Understanding (GEFU)，这是一个普遍化的框架，旨在改善泛化性能和可扩展性。该方法利用预训练的生成扩散模型优化图像，实现零样本泛化性能。我们提出了Semantically Consistent Unsupervised Fine-tuning (SCUF)，通过光照感知图像提示来指导图像生成，并提出循环注意力适配器来最大化其语义潜能。此外，为了减轻无监督训练中的语义退化，我们提出标签和反射一致性来学习高级语义和图像级别的空间语义。大量实验表明，我们提出的方法在传统图像质量和包括分类、检测、语义分割在内的低光照任务上均优于当前最先进的方法。", "conclusion": "我们的研究展示了通过结合生成扩散模型、光照感知提示和循环注意力适配器等技术，可以在低光照条件下提供高质量的图像增强以及提高视觉理解任务性能的有效方法。实验结果验证了该方法在传统图像质量和低光照理解任务上的优势。"}}
{"id": "2507.08297", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.08297", "abs": "https://arxiv.org/abs/2507.08297", "authors": ["Zizheng Zhan", "Ken Deng", "Huaixi Tang", "Wen Xiang", "Kun Wu", "Weihao Li", "Wenqiang Zhu", "Jingxuan Xu", "Lecheng Huang", "Zongxian Feng", "Shaojie Wang", "Shangpeng Yan", "Jiaheng Liu", "Zhongyuan Peng", "Zuchen Gao", "Haoyang Huang", "Ziqi Zhan", "Yanan Wu", "Yuanxing Zhang", "Jian Yang", "Guang Chen", "Haotian Zhang", "Bin Chen", "Bing Yu"], "title": "KAT-V1: Kwai-AutoThink Technical Report", "comment": null, "summary": "We present Kwaipilot-AutoThink (KAT), an open-source 40B large language model\ndeveloped to address the overthinking problem in reasoning-intensive tasks,\nwhere an automatic thinking training paradigm is proposed to dynamically switch\nbetween reasoning and non-reasoning modes based on task complexity.\nSpecifically, first, we construct the dual-regime dataset based on a novel\ntagging pipeline and a multi-agent synthesis strategy, and then we apply\nMulti-Token Prediction (MTP)-enhanced knowledge distillation, enabling\nefficient and fine-grained reasoning transfer with minimal pretraining cost.\nBesides, we implement a cold-start initialization strategy that introduces\nmode-selection priors using majority-vote signals and intent-aware prompting.\nFinally, we propose Step-SRPO, a reinforcement learning algorithm that\nincorporates intermediate supervision into the GRPO framework, offering\nstructured guidance over both reasoning-mode selection and response accuracy.\nExtensive experiments across multiple benchmarks demonstrate that KAT\nconsistently matches or even outperforms current state-of-the-art models,\nincluding DeepSeek-R1-0528 and Qwen3-235B-A22B, across a wide range of\nreasoning-intensive tasks while reducing token usage by up to approximately\n30\\%. Beyond academic evaluation, KAT has been successfully deployed in\nKwaipilot (i.e., Kuaishou's internal coding assistant), and improves real-world\ndevelopment workflows with high accuracy, efficiency, and controllable\nreasoning behaviors. Moreover, we are actively training a 200B\nMixture-of-Experts (MoE) with 40B activation parameters, where the early-stage\nresults already demonstrate promising improvements in performance and\nefficiency, further showing the scalability of the AutoThink paradigm.", "AI": {"tldr": "KAT是一种为解决推理密集型任务的过度思考问题而设计的自动思考训练型40B大型语言模型，通过各种创新策略和算法，在多个任务中展现出优于现有最佳模型的性能，并已成功应用于实际场景。", "motivation": "开发KAT的主要动机是解决在推理密集型任务中出现的过度思考问题，通过自动调节推理过程，提高任务效率和准确性。", "method": "文中提出了一种名为Kwaipilot-AutoThink（KAT）的开源400亿参数大型语言模型，该模型主要解决推理密集型任务中的过度思考问题。通过一种自动思考训练范式，KAT能够基于任务复杂性动态切换推理模式与非推理模式。具体来说，方法包括基于新型标签流水线和多代理合成策略构建双制度数据集，使用多令牌预测增强的知识蒸馏来高效地转移细粒度推理，以最小的预训练成本。此外，实施冷启动初始化策略引入基于多数投票信号和意图感知提示的模式选择先验。提出了一种增强的强化学习算法Step-SRPO，该算法将中间监督引入GRPO框架，提供结构化引导，既包括推理模式选择，也包括响应准确性。", "result": "广泛的实验结果表明，KAT在多个基准测试中一致匹配甚至超过当前最先进的模型（DeepSeek-R1-0528 和Qwen3-235B-A22B）在推理密集型任务中的表现，同时减少了高达约30%的令牌使用量。", "conclusion": "KAT作为一款400亿参数的模型，在多项推理任务中展示出卓越的表现，并已在Kuaishou的内部编码助手Kwaipilot中成功部署，不仅提高了实际开发工作流的准确性和效率，还展示了自动思考模式的可扩展性。此外，团队正在训练一个拥有400亿激活参数的2000亿参数Mixture-of-Experts模型，早期结果表明性能和效率方面有显著提升。"}}
{"id": "2507.08384", "categories": ["cs.CV", "68T45 68T45", "I.5.4; I.2.10; I.4.8"], "pdf": "https://arxiv.org/pdf/2507.08384", "abs": "https://arxiv.org/abs/2507.08384", "authors": ["Mathias Zinnen", "Prathmesh Madhu", "Inger Leemans", "Peter Bell", "Azhar Hussian", "Hang Tran", "Ali Hürriyetoğlu", "Andreas Maier", "Vincent Christlein"], "title": "Smelly, dense, and spreaded: The Object Detection for Olfactory References (ODOR) dataset", "comment": null, "summary": "Real-world applications of computer vision in the humanities require\nalgorithms to be robust against artistic abstraction, peripheral objects, and\nsubtle differences between fine-grained target classes. Existing datasets\nprovide instance-level annotations on artworks but are generally biased towards\nthe image centre and limited with regard to detailed object classes. The\nproposed ODOR dataset fills this gap, offering 38,116 object-level annotations\nacross 4712 images, spanning an extensive set of 139 fine-grained categories.\nConducting a statistical analysis, we showcase challenging dataset properties,\nsuch as a detailed set of categories, dense and overlapping objects, and\nspatial distribution over the whole image canvas. Furthermore, we provide an\nextensive baseline analysis for object detection models and highlight the\nchallenging properties of the dataset through a set of secondary studies.\nInspiring further research on artwork object detection and broader visual\ncultural heritage studies, the dataset challenges researchers to explore the\nintersection of object recognition and smell perception.", "AI": {"tldr": "本文介绍了一个新的数据集ODOR，该数据集包含大量的细致分类和复杂物体分布的信息，旨在为艺术作品中的物体检测提供更广泛的基准分析。", "motivation": "现有数据集对于图像中心偏见明显，且缺乏详细分类。本文为文化艺术的计算机视觉应用中的物体检测提供了更广泛的基准分析。", "method": "统计分析数据集的属性，并就对象检测模型提供广泛的基线分析。", "result": "ODOR数据集提供了详细的分类、密集重叠的对象以及图像全画布的空间分布。通过基线分析展示了数据集的挑战性。", "conclusion": "ODOR数据集为艺术作品中的物体检测和视觉文化遗产研究提出了新的挑战，激励进一步的研究工作。"}}
{"id": "2507.08309", "categories": ["cs.CL", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.08309", "abs": "https://arxiv.org/abs/2507.08309", "authors": ["Yupu Liang", "Yaping Zhang", "Zhiyang Zhang", "Zhiyuan Chen", "Yang Zhao", "Lu Xiang", "Chengqing Zong", "Yu Zhou"], "title": "Improving MLLM's Document Image Machine Translation via Synchronously Self-reviewing Its OCR Proficiency", "comment": "Accepted by ACL 2025 Findings", "summary": "Multimodal Large Language Models (MLLMs) have shown strong performance in\ndocument image tasks, especially Optical Character Recognition (OCR). However,\nthey struggle with Document Image Machine Translation (DIMT), which requires\nhandling both cross-modal and cross-lingual challenges. Previous efforts to\nenhance DIMT capability through Supervised Fine-Tuning (SFT) on the DIMT\ndataset often result in the forgetting of the model's existing monolingual\nabilities, such as OCR. To address these challenges, we introduce a novel\nfine-tuning paradigm, named Synchronously Self-Reviewing (SSR) its OCR\nproficiency, inspired by the concept \"Bilingual Cognitive Advantage\".\nSpecifically, SSR prompts the model to generate OCR text before producing\ntranslation text, which allows the model to leverage its strong monolingual OCR\nability while learning to translate text across languages. Comprehensive\nexperiments demonstrate the proposed SSR learning helps mitigate catastrophic\nforgetting, improving the generalization ability of MLLMs on both OCR and DIMT\ntasks.", "AI": {"tldr": "本文提出了一种改进的微调技术，旨在提高多模态大型语言模型在文档图像机器翻译任务中的表现，同时保持其在单语OCR任务中的能力。", "motivation": "大型多模态语言模型（MLLMs）在文档图像任务，特别是光学字符识别（OCR）方面表现出色，但在文档图像机器翻译（DIMT）上的表现不佳。这是因为DIMT需要处理跨模态和跨语言的挑战。传统的监督微调方法导致模型忘记了已有单语能力，如OCR。为了应对这一挑战，本文提出了一种新的解决方案。", "method": "本文提出了一种新颖的微调范式——同步自我回顾(Synchronous Self-Reviewing, SSR)。该方法借鉴了“双语认知优势”的概念，通过首先生成OCR文本，然后生成翻译文本，使模型在学习跨语言翻译的同时，保留其强大的单语OCR能力。", "result": "实验结果显示，提出的SSR学习方法有助于减轻灾难性遗忘问题，提高了MLLMs在OCR和DIMT任务上的泛化能力。", "conclusion": "研究表明，通过同步自我回顾（SSR）的微调范式，可以改善大型多模态语言模型在OCR和DIMT任务上的性能，减少灾难性遗忘问题。"}}
{"id": "2507.08396", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.08396", "abs": "https://arxiv.org/abs/2507.08396", "authors": ["Zhanxin Gao", "Beier Zhu", "Liang Yao", "Jian Yang", "Ying Tai"], "title": "Subject-Consistent and Pose-Diverse Text-to-Image Generation", "comment": null, "summary": "Subject-consistent generation (SCG)-aiming to maintain a consistent subject\nidentity across diverse scenes-remains a challenge for text-to-image (T2I)\nmodels. Existing training-free SCG methods often achieve consistency at the\ncost of layout and pose diversity, hindering expressive visual storytelling. To\naddress the limitation, we propose subject-Consistent and pose-Diverse T2I\nframework, dubbed as CoDi, that enables consistent subject generation with\ndiverse pose and layout. Motivated by the progressive nature of diffusion,\nwhere coarse structures emerge early and fine details are refined later, CoDi\nadopts a two-stage strategy: Identity Transport (IT) and Identity Refinement\n(IR). IT operates in the early denoising steps, using optimal transport to\ntransfer identity features to each target image in a pose-aware manner. This\npromotes subject consistency while preserving pose diversity. IR is applied in\nthe later denoising steps, selecting the most salient identity features to\nfurther refine subject details. Extensive qualitative and quantitative results\non subject consistency, pose diversity, and prompt fidelity demonstrate that\nCoDi achieves both better visual perception and stronger performance across all\nmetrics. The code is provided in https://github.com/NJU-PCALab/CoDi.", "AI": {"tldr": "本文提出了一种名为CoDi的框架，通过两阶段方法（身份传输和身份细化）实现了主体一致性生成，同时保持姿态和布局的多样性。", "motivation": "受扩散模型渐进性质的启发，我们提出了一种名为CoDi的框架，用于解决文本到图像生成中主体一致性的问题。在这一过程中，我们希望同时保持姿态和布局的多样性，以增强视觉表现力。", "method": "我们的方法名为CoDi，它采用两阶段策略：身份传输（IT）和身份细化（IR）。IT在早期去噪步骤中运行，利用最优传输将身份特征以姿势感知的方式转移到每个目标图像中。这在保持姿态多样性的同时促进了主体的一致性。IR应用于后期去噪步骤，选择最显著的身份特征来进一步细化主体细节。", "result": "实验结果表明，CoDi在主体一致性、姿势多样性以及提示保真度方面都取得了优于现有方法的视觉感知表现和更强的整体性能。", "conclusion": "研究结论表明，CoDi框架能够在保持姿态和布局多样性的同时实现了图像生成的主体一致性，并且在多个评估指标上达到了最佳性能。"}}
{"id": "2507.08325", "categories": ["cs.CL", "cs.MA"], "pdf": "https://arxiv.org/pdf/2507.08325", "abs": "https://arxiv.org/abs/2507.08325", "authors": ["Yinzhu Quan", "Xinrui Li", "Ying Chen"], "title": "CRMAgent: A Multi-Agent LLM System for E-Commerce CRM Message Template Generation", "comment": null, "summary": "In e-commerce private-domain channels such as instant messaging and e-mail,\nmerchants engage customers directly as part of their Customer Relationship\nManagement (CRM) programmes to drive retention and conversion. While a few top\nperformers excel at crafting outbound messages, most merchants struggle to\nwrite persuasive copy because they lack both expertise and scalable tools. We\nintroduce CRMAgent, a multi-agent system built on large language models (LLMs)\nthat generates high-quality message templates and actionable writing guidance\nthrough three complementary modes. First, group-based learning enables the\nagent to learn from a merchant's own top-performing messages within the same\naudience segment and rewrite low-performing ones. Second,\nretrieval-and-adaptation fetches templates that share the same audience segment\nand exhibit high similarity in voucher type and product category, learns their\nsuccessful patterns, and adapts them to the current campaign. Third, a\nrule-based fallback provides a lightweight zero-shot rewrite when no suitable\nreferences are available. Extensive experiments show that CRMAgent consistently\noutperforms merchants' original templates, delivering significant gains in both\naudience-match and marketing-effectiveness metrics.", "AI": {"tldr": "CRMAgent, a multi-agent system based on large language models, improves the quality of message templates and writing guidance for e-commerce merchants, enhancing audience match and marketing effectiveness.", "motivation": "Most merchants lack the expertise and scalable tools to craft persuasive outbound messages, leading to suboptimal customer engagement and conversion.", "method": "CRMAgent utilizes group-based learning, retrieval-and-adaptation, and a rule-based fallback to generate high-quality message templates and writing guidance.", "result": "Experiments show that CRMAgent outperforms original merchant templates, achieving significant improvements in audience match and marketing effectiveness metrics.", "conclusion": "CRMAgent provides a scalable solution to improve CRM messaging through advanced language models, thereby boosting marketing success for e-commerce merchants."}}
{"id": "2507.08400", "categories": ["cs.CV", "cs.AI", "cs.MM"], "pdf": "https://arxiv.org/pdf/2507.08400", "abs": "https://arxiv.org/abs/2507.08400", "authors": ["Yongjian Zhang", "Longguang Wang", "Kunhong Li", "Ye Zhang", "Yun Wang", "Liang Lin", "Yulan Guo"], "title": "PanMatch: Unleashing the Potential of Large Vision Models for Unified Matching Models", "comment": null, "summary": "This work presents PanMatch, a versatile foundation model for robust\ncorrespondence matching. Unlike previous methods that rely on task-specific\narchitectures and domain-specific fine-tuning to support tasks like stereo\nmatching, optical flow or feature matching, our key insight is that any\ntwo-frame correspondence matching task can be addressed within a 2D\ndisplacement estimation framework using the same model weights. Such a\nformulation eliminates the need for designing specialized unified architectures\nor task-specific ensemble models. Instead, it achieves multi-task integration\nby endowing displacement estimation algorithms with unprecedented\ngeneralization capabilities. To this end, we highlight the importance of a\nrobust feature extractor applicable across multiple domains and tasks, and\npropose the feature transformation pipeline that leverage all-purpose features\nfrom Large Vision Models to endow matching baselines with zero-shot cross-view\nmatching capabilities. Furthermore, we assemble a cross-domain dataset with\nnear 1.8 million samples from stereo matching, optical flow, and feature\nmatching domains to pretrain PanMatch. We demonstrate the versatility of\nPanMatch across a wide range of domains and downstream tasks using the same\nmodel weights. Our model outperforms UniMatch and Flow-Anything on cross-task\nevaluations, and achieves comparable performance to most state-of-the-art\ntask-specific algorithms on task-oriented benchmarks. Additionally, PanMatch\npresents unprecedented zero-shot performance in abnormal scenarios, such as\nrainy day and satellite imagery, where most existing robust algorithms fail to\nyield meaningful results.", "AI": {"tldr": "PanMatch is a flexible foundational model for robust two-frame correspondence matching tasks, such as stereo matching, optical flow, and feature matching, using a universal set of model weights and a robust feature extractor.", "motivation": "The motivation behind PanMatch is to eliminate the need for task-specific architectures or ensemble models by unifying these tasks under a 2D displacement estimation framework, achieving better generalization and cross-domain performance.", "method": "PanMatch leverages a robust feature transformation pipeline and pretrained on a cross-domain dataset to provide zero-shot matching capabilities across various tasks.", "result": "PanMatch outperforms existing methods like UniMatch and Flow-Anything in cross-task evaluations and surpasses many specialized algorithms in abnormal scenarios, such as rainy day and satellite imagery.", "conclusion": "The success of PanMatch demonstrates the potential for a single model to effectively handle multiple correspondence matching tasks with generalized robustness and versatility, setting a benchmark for future research in the field."}}
{"id": "2507.08335", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.08335", "abs": "https://arxiv.org/abs/2507.08335", "authors": ["Yuzheng Xu", "Tosho Hirasawa", "Seiya Kawano", "Shota Kato", "Tadashi Kozuno"], "title": "MK2 at PBIG Competition: A Prompt Generation Solution", "comment": "9 pages, to appear in the 2nd Workshop on Agent AI for Scenario\n  Planning (AGENTSCEN 2025)", "summary": "The Patent-Based Idea Generation task asks systems to turn real patents into\nproduct ideas viable within three years. We propose MK2, a prompt-centric\npipeline: Gemini 2.5 drafts and iteratively edits a prompt, grafting useful\nfragments from weaker outputs; GPT-4.1 then uses this prompt to create one idea\nper patent, and an Elo loop judged by Qwen3-8B selects the best prompt-all\nwithout extra training data. Across three domains, two evaluator types, and six\ncriteria, MK2 topped the automatic leaderboard and won 25 of 36 tests. Only the\nmaterials-chemistry track lagged, indicating the need for deeper domain\ngrounding; yet, the results show that lightweight prompt engineering has\nalready delivered competitive, commercially relevant ideation from patents.", "AI": {"tldr": "MK2系统通过提示工程将专利转化为产品创意，无需额外训练数据，在多项测试中表现优异，但在材料化学领域有待提升。", "motivation": "该研究旨在利用MK2系统将现有专利转化为在三年内可行的产品创意，探索无需额外训练数据的提示工程的潜力。", "method": "MK2方法是一个以提示为中心的流水线，其中Gemini 2.5负责起草并迭代编辑提示，将有用的片段从较弱的输出中嫁接过来；GPT-4.1使用该提示为每个专利创造一个想法；Qwen3-8B通过Elo循环评估选择最佳提示，整个过程无需额外的训练数据。", "result": "在三个领域，两种评估者类型和六个标准下，MK2在自动排行榜上名列前茅，并赢得了36个测试中的25个。然而，在材料化学领域表现不佳，显示出需要更深层次的专业领域基础。", "conclusion": "研究结果表明，轻量级的提示工程已经能够从专利中产生具有竞争力和商业相关性的创意，尽管在某些特定领域的表现需改进。"}}
{"id": "2507.08404", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.08404", "abs": "https://arxiv.org/abs/2507.08404", "authors": ["Li Chen", "Rui Liu", "Yuxiang Zhou", "Xudong Ma", "Yong Chen", "Dell Zhang"], "title": "Deep Hashing with Semantic Hash Centers for Image Retrieval", "comment": null, "summary": "Deep hashing is an effective approach for large-scale image retrieval.\nCurrent methods are typically classified by their supervision types:\npoint-wise, pair-wise, and list-wise. Recent point-wise techniques (e.g., CSQ,\nMDS) have improved retrieval performance by pre-assigning a hash center to each\nclass, enhancing the discriminability of hash codes across various datasets.\nHowever, these methods rely on data-independent algorithms to generate hash\ncenters, which neglect the semantic relationships between classes and may\ndegrade retrieval performance.\n  This paper introduces the concept of semantic hash centers, building on the\nidea of traditional hash centers. We hypothesize that hash centers of\nsemantically related classes should have closer Hamming distances, while those\nof unrelated classes should be more distant. To this end, we propose a\nthree-stage framework, SHC, to generate hash codes that preserve semantic\nstructure.\n  First, we develop a classification network to identify semantic similarities\nbetween classes using a data-dependent similarity calculation that adapts to\nvarying data distributions. Second, we introduce an optimization algorithm to\ngenerate semantic hash centers, preserving semantic relatedness while enforcing\na minimum distance between centers to avoid excessively similar hash codes.\nFinally, a deep hashing network is trained using these semantic centers to\nconvert images into binary hash codes.\n  Experimental results on large-scale retrieval tasks across several public\ndatasets show that SHC significantly improves retrieval performance.\nSpecifically, SHC achieves average improvements of +7.26%, +7.62%, and +11.71%\nin MAP@100, MAP@1000, and MAP@ALL metrics, respectively, over state-of-the-art\nmethods.", "AI": {"tldr": "提出了一种新的基于语义哈希中心的三阶段框架（SHC）用于提高大规模图像检索性能。新方法在多个公有数据集上的实验展示了优于现有方法的大幅提升。", "motivation": "当前的方法通常按监督类型分类为点式、成对式和列表式。最近的点式技术（例如CSQ、MDS）通过预分配给每个类的哈希中心提高了检索性能，增强了各种数据集上哈希码的可区分性。然而，这些方法依赖于与数据无关的算法来生成哈希中心，这忽视了类之间的语义关系，可能降低检索性能。本研究引入了语义哈希中心的概念，以改进哈希码生成方法。", "method": "我们提出了一个三阶段框架SHC，用于生成保持语义结构的哈希码。首先，我们开发了一个分类网络，用于通过数据相关的相似性计算来识别类别之间的语义相似性。其次，引入了一个优化算法来生成语义哈希中心，保持语义相关性，同时强制中心之间的最小距离，以避免过于相似的哈希码。最后，通过训练一个深度哈希网络，使用这些语义中心将图像转换为二进制哈希码。", "result": "在几个公共数据集上的大规模检索任务的实验结果表明，SHC显著提高了检索性能。具体而言，SHC在MAP@100、MAP@1000和MAP@ALL度量上分别比最先进的方法提高了+7.26%、+7.62%和+11.71%。", "conclusion": "我们的研究表明，通过采用数据相关的语义哈希中心，SHC框架能够有效提高图像检索性能。通过对类之间的语义关系进行建模和优化，本方法能够产生更能代表图像内容的哈希码。"}}
{"id": "2507.08336", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2507.08336", "abs": "https://arxiv.org/abs/2507.08336", "authors": ["Zhichao Xu", "Zhiqi Huang", "Shengyao Zhuang", "Ashim Gupta", "Vivek Srikumar"], "title": "Distillation versus Contrastive Learning: How to Train Your Rerankers", "comment": null, "summary": "Training text rerankers is crucial for information retrieval. Two primary\nstrategies are widely used: contrastive learning (optimizing directly on\nground-truth labels) and knowledge distillation (transferring knowledge from a\nlarger reranker). While both have been studied in the literature, a clear\ncomparison of their effectiveness for training cross-encoder rerankers under\npractical conditions is needed.\n  This paper empirically compares these strategies by training rerankers of\ndifferent sizes and architectures using both methods on the same data, with a\nstrong contrastive learning model acting as the distillation teacher. Our\nresults show that knowledge distillation generally yields better in-domain and\nout-of-domain ranking performance than contrastive learning when distilling\nfrom a larger teacher model. This finding is consistent across student model\nsizes and architectures. However, distilling from a teacher of the same\ncapacity does not provide the same advantage, particularly for out-of-domain\ntasks. These findings offer practical guidance for choosing a training strategy\nbased on available teacher models. Therefore, we recommend using knowledge\ndistillation to train smaller rerankers if a larger, more powerful teacher is\naccessible; in its absence, contrastive learning provides a strong and more\nreliable alternative otherwise.", "AI": {"tldr": "This paper empirically compares contrastive learning and knowledge distillation for training text rerankers, finding that distillation generally outperforms contrastive learning when a more powerful teacher model is available, but contrastive learning is a reliable alternative when it is not.", "motivation": "The motivation behind this study is to provide a clear empirical comparison of contrastive learning and knowledge distillation strategies for training cross-encoder rerankers in practical scenarios.", "method": "This paper compares two strategies for training text rerankers: contrastive learning and knowledge distillation. It trains rerankers of different sizes and architectures using both methods on the same data set, with a strong contrastive learning model serving as the distillation teacher.", "result": "The results indicate that knowledge distillation yields better in-domain and out-of-domain ranking performance than contrastive learning when distilling from a larger teacher model, but distilling from a teacher of the same capacity does not provide the same advantage, especially for out-of-domain tasks.", "conclusion": "The conclusion recommends using knowledge distillation to train smaller rerankers if a larger, more powerful teacher model is available; otherwise, contrastive learning is a strong and reliable alternative."}}
{"id": "2507.08410", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.08410", "abs": "https://arxiv.org/abs/2507.08410", "authors": ["Shijun Yang", "Xiang Zhang", "Wanqing Zhao", "Hangzai Luo", "Sheng Zhong", "Jinye Peng", "Jianping Fan"], "title": "Multi-modal Mutual-Guidance Conditional Prompt Learning for Vision-Language Models", "comment": "21 pages, 8 figures", "summary": "Prompt learning facilitates the efficient adaptation of Vision-Language\nModels (VLMs) to various downstream tasks. However, it faces two significant\nchallenges: (1) inadequate modeling of class embedding distributions for unseen\ninstances, leading to suboptimal generalization on novel classes; (2)\nprevailing methodologies predominantly confine cross-modal alignment to the\nfinal output layer of vision and text encoders, which fundamentally limits\ntheir capacity to preserve topological consistency with pre-trained multi-modal\nembedding spaces. To this end, we introduce MuGCP (Multi-modal Mutual-Guidance\nConditional Prompt Learning), a novel paradigm designed for conditional prompt\ngeneration. MuGCP leverages Multi-modal Large Language Models (MLLMs) as\nconditional prompt learners to adaptively generate Semantic Conditional Prompts\n(SCP) that incorporate rich, fine-grained high-level semantic knowledge for\nimage instances. To ensure effective alignment and interaction across the\nmulti-modal space of Vision-Language Models (VLMs), we introduce the Attention\nMutual-Guidance (AMG) module, which facilitates interactions between visual and\nsemantic information. Through mutual guidance, the AMG module generates Visual\nConditional Prompts (VCP), enhancing the model's performance in multi-modal\ntasks. Additionally, we present a Multi-Prompt Fusion (MPF) mechanism that\nintegrates SCP and VCP with contextual prompts, ensuring seamless coordination\namong the different prompts and enhancing the modeling of class embeddings and\ninstance-specific knowledge. Our MuGCP outperforms existing state-of-the-art\nmethods on 14 different datasets. The code will be made available after\npublication.", "AI": {"tldr": "提出了 MuGCP 方法，利用 MLLMs 生成富含语义知识的SCP，并改进了跨模态对齐机制，显著提高了多模态任务性能，超越了现有方法。", "motivation": "旨在解决现有提示学习方法在处理未见实例的类别嵌入分布不足，以及跨模态对齐主要局限在视觉和文本编码器的最终输出层，从而限制了它们保留预训练多模态嵌入空间拓扑一致性的能力。", "method": "MuGCP (Multi-modal Mutual-Guidance Conditional Prompt Learning) 被提出，这是一种用于条件提示生成的新范式。它利用 Multi-modal Large Language Models (MLLMs) 作为条件提示学习器，以自适应生成 Semantic Conditional Prompts (SCP)，这些提示富含对图像实例来说的具体语义知识。此外，引入了 Attention Mutual-Guidance (AMG) 模块以促进视觉和语义信息之间的交互，并生成 Visual Conditional Prompts (VCP) 以提升模型在多模态任务上的性能。还介绍了一种 Multi-Prompt Fusion (MPF) 机制，用于将 SCP 和 VCP 与上下文提示融合，以确保各种提示之间的无缝协调，增强类别嵌入和实例特定知识的建模能力。", "result": "MuGCP 在 14 个不同数据集上超过了现有最先进方法的表现。", "conclusion": "通过引入 MuGCP 方法，有效地提升了 Vision-Language 模型在多模态任务中的性能，尤其是在未见实例的类别嵌入分布和跨模态融合方面。"}}
{"id": "2507.08339", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.08339", "abs": "https://arxiv.org/abs/2507.08339", "authors": ["Peng Wang", "Xuesi Hu", "Jiageng Wu", "Yuntao Zou", "Qiancheng Zhang", "Dagang Li"], "title": "What Factors Affect LLMs and RLLMs in Financial Question Answering?", "comment": "Preprint", "summary": "Recently, the development of large language models (LLMs) and reasoning large\nlanguage models (RLLMs) have gained considerable attention from many\nresearchers. RLLMs enhance the reasoning capabilities of LLMs through Long\nChain-of-Thought (Long CoT) processes, significantly improving the performance\nof LLMs in addressing complex problems. However, there are few works that\nsystematically explore what methods can fully unlock the performance of LLMs\nand RLLMs within the financial domain. To investigate the impact of various\nmethods on LLMs and RLLMs, we utilize five LLMs and three RLLMs to assess the\neffects of prompting methods, agentic frameworks, and multilingual alignment\nmethods on financial question-answering tasks. Our research findings indicate:\n(1) Current prompting methods and agent frameworks enhance the performance of\nLLMs in financial question answering by simulating Long CoT; (2) RLLMs possess\ninherent Long CoT capabilities, which limits the effectiveness of conventional\nmethods in further enhancing their performance; (3) Current advanced\nmultilingual alignment methods primarily improve the multilingual performance\nof LLMs by extending the reasoning length, which yields minimal benefits for\nRLLMs. We hope that this study can serve as an important reference for LLMs and\nRLLMs in the field of financial question answering.", "AI": {"tldr": "研究探讨了各种方法对大型语言模型和推理大型语言模型在金融领域问题回答任务上的影响，发现当前的提示方法和代理框架能增强LLMs的表现，而RLLMs固有的长期推理能力限制了传统方法对其表现的提升。", "motivation": "缺乏系统性探索如何最大限度地发挥LLMs和RLLMs在金融领域表现的方法。", "method": "使用五个大型语言模型和三个推理大型语言模型来评估提示方法、代理框架和多语言对齐方法在金融问题回答任务中的效果。", "result": "提示方法和代理框架通过模拟长期推理过程增强LLMs在金融问题回答中的表现，而RLLMs固有的长期推理能力限制了传统方法对其表现的提升，且多语言对齐方法主要提升了LLMs的多语言性能，对RLLMs影响较小。", "conclusion": "该研究为金融领域问题回答中的LLMs和RLLMs提供了重要的参考。"}}
{"id": "2507.08416", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.08416", "abs": "https://arxiv.org/abs/2507.08416", "authors": ["Zesong Yang", "Bangbang Yang", "Wenqi Dong", "Chenxuan Cao", "Liyuan Cui", "Yuewen Ma", "Zhaopeng Cui", "Hujun Bao"], "title": "InstaScene: Towards Complete 3D Instance Decomposition and Reconstruction from Cluttered Scenes", "comment": "Accepted by ICCV 2025. Project page:\n  https://zju3dv.github.io/instascene/", "summary": "Humans can naturally identify and mentally complete occluded objects in\ncluttered environments. However, imparting similar cognitive ability to\nrobotics remains challenging even with advanced reconstruction techniques,\nwhich models scenes as undifferentiated wholes and fails to recognize complete\nobject from partial observations. In this paper, we propose InstaScene, a new\nparadigm towards holistic 3D perception of complex scenes with a primary goal:\ndecomposing arbitrary instances while ensuring complete reconstruction. To\nachieve precise decomposition, we develop a novel spatial contrastive learning\nby tracing rasterization of each instance across views, significantly enhancing\nsemantic supervision in cluttered scenes. To overcome incompleteness from\nlimited observations, we introduce in-situ generation that harnesses valuable\nobservations and geometric cues, effectively guiding 3D generative models to\nreconstruct complete instances that seamlessly align with the real world.\nExperiments on scene decomposition and object completion across complex\nreal-world and synthetic scenes demonstrate that our method achieves superior\ndecomposition accuracy while producing geometrically faithful and visually\nintact objects.", "AI": {"tldr": "我们提出了InstaScene，一种新的3D感知范式，旨在通过精确的实例分解和克服有限观察导致的不完整性来改善复杂场景中的3D感知。", "motivation": "尽管有先进的重建技术，机器人在复杂环境中识别和完成被遮挡物体的能力仍然不如人类。现有技术往往将场景建模为不分化的整体，难以从部分观测中识别出完整的物体。因此，我们的动机是改善机器人在复杂场景中的3D感知能力，特别是在分解实例和实现完全重建方面。", "method": "我们的方法名为InstaScene，旨在实现复杂场景的完整3D感知。为此，我们开发了一种新的空间对比学习方法，通过跟踪每个实例在不同视角下的光栅化来增强语义监督，并引入原位生成，利用有价值的观察和几何线索指导3D生成模型，以实现与现实世界无缝结合的完整实例重建。", "result": "实验表明，我们的方法在场景分解和对象完成方面表现出了更优的分解精度，并生产出几何准确且视觉完好的对象。", "conclusion": "实验结果证明，我们提出的方法在处理复杂的真实和合成场景时能够实现更准确的分解和完整性重建，证明了InstaScene的有效性。"}}
{"id": "2507.08342", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.08342", "abs": "https://arxiv.org/abs/2507.08342", "authors": ["Itai Mondshine", "Tzuf Paz-Argaman", "Reut Tsarfaty"], "title": "Beyond N-Grams: Rethinking Evaluation Metrics and Strategies for Multilingual Abstractive Summarization", "comment": "ACL 2025 Main", "summary": "Automatic n-gram based metrics such as ROUGE are widely used for evaluating\ngenerative tasks such as summarization. While these metrics are considered\nindicative (even if imperfect) of human evaluation for English, their\nsuitability for other languages remains unclear. To address this, we\nsystematically assess evaluation metrics for generation both n-gram-based and\nneural based to evaluate their effectiveness across languages and tasks.\nSpecifically, we design a large-scale evaluation suite across eight languages\nfrom four typological families: agglutinative, isolating, low-fusional, and\nhigh-fusional, spanning both low- and high-resource settings, to analyze their\ncorrelation with human judgments. Our findings highlight the sensitivity of\nevaluation metrics to the language type. For example, in fusional languages,\nn-gram-based metrics show lower correlation with human assessments compared to\nisolating and agglutinative languages. We also demonstrate that proper\ntokenization can significantly mitigate this issue for morphologically rich\nfusional languages, sometimes even reversing negative trends. Additionally, we\nshow that neural-based metrics specifically trained for evaluation, such as\nCOMET, consistently outperform other neural metrics and better correlate with\nhuman judgments in low-resource languages. Overall, our analysis highlights the\nlimitations of n-gram metrics for fusional languages and advocates for greater\ninvestment in neural-based metrics trained for evaluation tasks.", "AI": {"tldr": "The study evaluates n-gram-based and neural-based metrics across diverse languages, finding that neural-based metrics perform better, especially in low-resource and fusional languages.", "motivation": "The motivation is to evaluate the suitability of n-gram-based and neural-based metrics across different languages and resource settings, addressing the uncertainty regarding their effectiveness beyond English.", "method": "The study designs a large-scale evaluation using n-gram-based and neural-based metrics across eight languages from four language families to assess their correlation with human judgments in low and high-resource settings.", "result": "The findings reveal that n-gram-based metrics have lower correlation with human judgments in fusional languages compared to isolating and agglutinative languages. Proper tokenization improved metric performance in fusional languages. Neural-based metrics, like COMET, generally outperformed other metrics, especially in low-resource languages.", "conclusion": "The research underscores the limitations of n-gram-based evaluation metrics for fusional languages and emphasizes the need for development and use of neural-based metrics."}}
{"id": "2507.08422", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2507.08422", "abs": "https://arxiv.org/abs/2507.08422", "authors": ["Wongi Jeong", "Kyungryeol Lee", "Hoigi Seo", "Se Young Chun"], "title": "Upsample What Matters: Region-Adaptive Latent Sampling for Accelerated Diffusion Transformers", "comment": null, "summary": "Diffusion transformers have emerged as an alternative to U-net-based\ndiffusion models for high-fidelity image and video generation, offering\nsuperior scalability. However, their heavy computation remains a major obstacle\nto real-world deployment. Existing acceleration methods primarily exploit the\ntemporal dimension such as reusing cached features across diffusion timesteps.\nHere, we propose Region-Adaptive Latent Upsampling (RALU), a training-free\nframework that accelerates inference along spatial dimension. RALU performs\nmixed-resolution sampling across three stages: 1) low-resolution denoising\nlatent diffusion to efficiently capture global semantic structure, 2)\nregion-adaptive upsampling on specific regions prone to artifacts at\nfull-resolution, and 3) all latent upsampling at full-resolution for detail\nrefinement. To stabilize generations across resolution transitions, we leverage\nnoise-timestep rescheduling to adapt the noise level across varying\nresolutions. Our method significantly reduces computation while preserving\nimage quality by achieving up to 7.0$\\times$ speed-up on FLUX and 3.0$\\times$\non Stable Diffusion 3 with minimal degradation. Furthermore, RALU is\ncomplementary to existing temporal accelerations such as caching methods, thus\ncan be seamlessly integrated to further reduce inference latency without\ncompromising generation quality.", "AI": {"tldr": "提出Region-Adaptive Latent Upsampling（RALU）框架，在不牺牲图像质量的前提下，通过混合分辨率采样加速扩散模型的推理过程，显著提高了生成效率。", "motivation": "旨在解决扩散变换器推理过程中计算成本高的问题，通过沿空间维度加速推理，减少计算量同时保持图像质量。", "method": "RALU采用混合分辨率采样在三个阶段加速推理：1)低分辨率去噪潜在扩散以高效捕捉全局语义结构，2)特异性区域适应上采样，以处理全分辨率下容易出现伪影的区域；3)全分辨率下的所有潜在上采样进行细节优化。为了稳定不同分辨率转换期间的生成，引入了噪声时间步调整来适应不同分辨率下的噪声水平。", "result": "该方法在FLUX和Stable Diffusion 3上分别实现了最高7.0倍和3.0倍的速度提升，且图像质量几乎没有退化。", "conclusion": "提出的方法不仅可以加速高分辨率图像和视频的生成，还可以与现有的时间加速方法无缝集成，进一步降低推理延迟而不影响生成质量。"}}
{"id": "2507.08350", "categories": ["cs.CL", "cs.MA", "I.2.11; I.2.7"], "pdf": "https://arxiv.org/pdf/2507.08350", "abs": "https://arxiv.org/abs/2507.08350", "authors": ["Keisuke Ueda", "Wataru Hirota", "Takuto Asakura", "Takahiro Omi", "Kosuke Takahashi", "Kosuke Arima", "Tatsuya Ishigaki"], "title": "Exploring Design of Multi-Agent LLM Dialogues for Research Ideation", "comment": "16 pages, 1 figure, appendix. Accepted to SIGDIAL 2025", "summary": "Large language models (LLMs) are increasingly used to support creative tasks\nsuch as research idea generation. While recent work has shown that structured\ndialogues between LLMs can improve the novelty and feasibility of generated\nideas, the optimal design of such interactions remains unclear. In this study,\nwe conduct a comprehensive analysis of multi-agent LLM dialogues for scientific\nideation. We compare different configurations of agent roles, number of agents,\nand dialogue depth to understand how these factors influence the novelty and\nfeasibility of generated ideas. Our experimental setup includes settings where\none agent generates ideas and another critiques them, enabling iterative\nimprovement. Our results show that enlarging the agent cohort, deepening the\ninteraction depth, and broadening agent persona heterogeneity each enrich the\ndiversity of generated ideas. Moreover, specifically increasing critic-side\ndiversity within the ideation-critique-revision loop further boosts the\nfeasibility of the final proposals. Our findings offer practical guidelines for\nbuilding effective multi-agent LLM systems for scientific ideation. Our code is\navailable at https://github.com/g6000/MultiAgent-Research-Ideator.", "AI": {"tldr": "研究分析了多代理LLM对话在科学创意生成中的作用，发现增加代理人数、加深交互及多样化角色均能丰富想法多样性，增加评估方多样性能提升创意可行性。", "motivation": "尽管最近的研究表明，LLM之间的结构化对话可以改善生成想法的新颖性和可行性，但这种交互的最佳设计仍不清楚。研究旨在通过全面分析来填补这一空白。", "method": "通过比较不同代理角色配置、代理数量以及对话深度，研究多代理LLM对话对科学创意生成的新颖性和可行性的影响。实验设置包括由一个代理生成想法另一个代理进行评估，可以实现迭代改进。", "result": "研究结果显示，增加代理人数、加深交互深度和扩大代理角色差异性都可以丰富想法的多样性。特别是增加评估方多样性在创意—评估—修订循环中能够进一步提升最终提案的可行性。", "conclusion": "研究发现为构建有效的多代理LLM系统以进行科学创意提供了实用的指导。"}}
{"id": "2507.08434", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.08434", "abs": "https://arxiv.org/abs/2507.08434", "authors": ["Ji Hyun Seo", "Byounhyun Yoo", "Gerard Jounghyun Kim"], "title": "RePaintGS: Reference-Guided Gaussian Splatting for Realistic and View-Consistent 3D Scene Inpainting", "comment": null, "summary": "Radiance field methods, such as Neural Radiance Field or 3D Gaussian\nSplatting, have emerged as seminal 3D representations for synthesizing\nrealistic novel views. For practical applications, there is ongoing research on\nflexible scene editing techniques, among which object removal is a\nrepresentative task. However, removing objects exposes occluded regions, often\nleading to unnatural appearances. Thus, studies have employed image inpainting\ntechniques to replace such regions with plausible content - a task referred to\nas 3D scene inpainting. However, image inpainting methods produce one of many\nplausible completions for each view, leading to inconsistencies between\nviewpoints. A widely adopted approach leverages perceptual cues to blend\ninpainted views smoothly. However, it is prone to detail loss and can fail when\nthere are perceptual inconsistencies across views. In this paper, we propose a\nnovel 3D scene inpainting method that reliably produces realistic and\nperceptually consistent results even for complex scenes by leveraging a\nreference view. Given the inpainted reference view, we estimate the inpainting\nsimilarity of the other views to adjust their contribution in constructing an\naccurate geometry tailored to the reference. This geometry is then used to warp\nthe reference inpainting to other views as pseudo-ground truth, guiding the\noptimization to match the reference appearance. Comparative evaluation studies\nhave shown that our approach improves both the geometric fidelity and\nappearance consistency of inpainted scenes.", "AI": {"tldr": "本文提出了一种基于参考视图的3D场景修复方法，解决了先前方法在保证视图间几何和外观一致性方面的不足。", "motivation": "现有的3D场景修复方法在处理多个视图的一致性方面存在局限性，容易导致细节损失以及感知上的不一致性。因此，需要一种更可靠的方法来改善几何保真度和外观一致性。", "method": "提出一种新的3D场景修复方法，该方法利用参考视图来可靠地生成复杂场景中的真实且感知一致的修复结果。通过估计其他视图与修复参考视图的相似性来调整它们在构建准确几何结构中的贡献，然后使用这种几何结构将参考修复结果扭曲到其他视图，作为伪地面真值来指导优化以匹配参考视图的外观。", "result": "比较评估研究表明，该方法在修复场景的几何保真度和外观一致性方面表现优于现有方法。", "conclusion": "该方法通过引入参考视图的使用，解决了现存方法在处理3D场景修复时视图一致性的问题。"}}
{"id": "2507.08371", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.08371", "abs": "https://arxiv.org/abs/2507.08371", "authors": ["Benjamin Newman", "Abhilasha Ravichander", "Jaehun Jung", "Rui Xin", "Hamish Ivison", "Yegor Kuznetsov", "Pang Wei Koh", "Yejin Choi"], "title": "The Curious Case of Factuality Finetuning: Models' Internal Beliefs Can Improve Factuality", "comment": "29 pages, 4 figures, 16 tables", "summary": "Language models are prone to hallucination - generating text that is\nfactually incorrect. Finetuning models on high-quality factual information can\npotentially reduce hallucination, but concerns remain; obtaining factual gold\ndata can be expensive and training on correct but unfamiliar data may\npotentially lead to even more downstream hallucination. What data should\npractitioners finetune on to mitigate hallucinations in language models? In\nthis work, we study the relationship between the factuality of finetuning data\nand the prevalence of hallucinations in long-form generation tasks.\nCounterintuitively, we find that finetuning on factual gold data is not as\nhelpful as finetuning on model-generated data that models believe to be\nfactual. Next, we evaluate filtering strategies applied on both factual gold\ndata and model-generated data, and find that finetuning on model-generated data\nthat is filtered by models' own internal judgments often leads to better\noverall factuality compared to other configurations: training on gold data\nfiltered by models' judgments, training on gold data alone, or training on\nmodel-generated data that is supported by gold data. These factuality\nimprovements transfer across three domains we study, suggesting that a models'\nown beliefs can provide a powerful signal for factuality.", "AI": {"tldr": "研究表明，在语言模型中，使用模型生成且自身判断为事实性的数据进行微调比使用黄金事实数据更有助于减少幻觉。", "motivation": "语言模型容易产生幻觉，即生成事实性错误的文本。虽然可以在高质量的事实信息上进行微调来减少幻觉，但获取黄金事实数据的成本高昂，且训练不熟悉的数据可能导致更多的下游幻觉。研究的目标是确定从业者应该使用什么样的数据来减少语言模型中的幻觉。", "method": "我们研究了微调数据的事实性与语言模型在长文本生成任务中产生幻觉的频率之间的关系。我们比较了使用黄金事实数据和模型生成数据（模型认为是事实性数据）进行微调的效果，并评估了对这两种数据进行过滤策略后的微调表现。", "result": "我们发现，与使用黄金事实数据进行微调相比，使用模型认为事实性的生成数据进行微调反而对减少幻觉更有帮助。经过模型自判过滤后的模型生成数据微调往往比其他配置产生了更好的整体事实性。这一改进在三个研究领域中都有显现。", "conclusion": "模型自身对于数据事实性的判断能够提供一个强有力的信号，而且这一信号在多个研究领域中都有显著的效果。这表明，利用自身判断来提升数据的事实性对于减少模型幻觉是一个有效的策略。"}}
{"id": "2507.08441", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.08441", "abs": "https://arxiv.org/abs/2507.08441", "authors": ["Anlin Zheng", "Xin Wen", "Xuanyang Zhang", "Chuofan Ma", "Tiancai Wang", "Gang Yu", "Xiangyu Zhang", "Xiaojuan Qi"], "title": "Vision Foundation Models as Effective Visual Tokenizers for Autoregressive Image Generation", "comment": "19 pages, 4 figures", "summary": "Leveraging the powerful representations of pre-trained vision foundation\nmodels -- traditionally used for visual comprehension -- we explore a novel\ndirection: building an image tokenizer directly atop such models, a largely\nunderexplored area. Specifically, we employ a frozen vision foundation model as\nthe encoder of our tokenizer. To enhance its effectiveness, we introduce two\nkey components: (1) a region-adaptive quantization framework that reduces\nredundancy in the pre-trained features on regular 2D grids, and (2) a semantic\nreconstruction objective that aligns the tokenizer's outputs with the\nfoundation model's representations to preserve semantic fidelity. Based on\nthese designs, our proposed image tokenizer, VFMTok, achieves substantial\nimprovements in image reconstruction and generation quality, while also\nenhancing token efficiency. It further boosts autoregressive (AR) generation --\nachieving a gFID of 2.07 on ImageNet benchmarks, while accelerating model\nconvergence by three times, and enabling high-fidelity class-conditional\nsynthesis without the need for classifier-free guidance (CFG). The code will be\nreleased publicly to benefit the community.", "AI": {"tldr": "本文提出了一种使用冻结的预训练视觉基础模型构建图像分词器的方法，通过引入适应区域量化框架和语义重建目标，实现了图像重建和生成质量的显著提升，同时提高了标记效率。", "motivation": "传统的预训练视觉基础模型主要用于视觉理解，但尚未被广泛用于直接构建图像分词器。本文旨在填补这一领域的空白，探索视觉模型在图像标记领域的潜在应用。", "method": "基于预训练视觉基础模型（通常用于视觉理解），本文提出了一种新颖的方法：构建基于该模型的图像分词器。具体来说，我们采用冻结的视觉基础模型作为分词器的编码器。为了提升其有效性，本文引入了两个关键组件：一种适应区域的量化框架，能够在常规的二维网格上减少预训练特征的冗余，以及语义重建目标，确保分词器输出与基础模型表示保持一致以维持语义保真度。", "result": "所提出的图像分词器（VFMTok）在图像重建和生成质量方面取得了显著改善，提高了标记效率。在ImageNet基准上，自回归生成的gFID达到了2.07，并加速了模型收敛速度三倍，同时支持无需分类器自由引导的高保真度条件合成。", "conclusion": "所设计的基于预训练视觉基础模型的图像分词器（VFMTok）不仅在图像生成质量上取得了显著提升，同时也极大地提高了标记效率，进一步促进了自回归模型的生成能力。"}}
{"id": "2507.08425", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.08425", "abs": "https://arxiv.org/abs/2507.08425", "authors": ["Lu Xiang", "Yang Zhao", "Yaping Zhang", "Chengqing Zong"], "title": "A Survey of Large Language Models in Discipline-specific Research: Challenges, Methods and Opportunities", "comment": null, "summary": "Large Language Models (LLMs) have demonstrated their transformative potential\nacross numerous disciplinary studies, reshaping the existing research\nmethodologies and fostering interdisciplinary collaboration. However, a\nsystematic understanding of their integration into diverse disciplines remains\nunderexplored. This survey paper provides a comprehensive overview of the\napplication of LLMs in interdisciplinary studies, categorising research efforts\nfrom both a technical perspective and with regard to their applicability. From\na technical standpoint, key methodologies such as supervised fine-tuning,\nretrieval-augmented generation, agent-based approaches, and tool-use\nintegration are examined, which enhance the adaptability and effectiveness of\nLLMs in discipline-specific contexts. From the perspective of their\napplicability, this paper explores how LLMs are contributing to various\ndisciplines including mathematics, physics, chemistry, biology, and the\nhumanities and social sciences, demonstrating their role in discipline-specific\ntasks. The prevailing challenges are critically examined and the promising\nresearch directions are highlighted alongside the recent advances in LLMs. By\nproviding a comprehensive overview of the technical developments and\napplications in this field, this survey aims to serve as an invaluable resource\nfor the researchers who are navigating the complex landscape of LLMs in the\ncontext of interdisciplinary studies.", "AI": {"tldr": "A survey paper on Large Language Models (LLMs) highlights their integration and impact across various disciplines, analyzing technical methodologies and their application in solving discipline-specific tasks while pointing out challenges and future research directions.", "motivation": "The paper addresses the underexplored systematic understanding of integrating LLMs into diverse disciplines, providing a comprehensive analysis of their applications and methodologies in interdisciplinary studies.", "method": "This paper examines key methodologies for enhancing the adaptability and effectiveness of LLMs in various disciplines, such as supervised fine-tuning, retrieval-augmented generation, agent-based approaches, and tool-use integration.", "result": "The paper explores the contributions of LLMs across various disciplines such as mathematics, physics, chemistry, biology, and the humanities and social sciences, highlighting their discipline-specific roles.", "conclusion": "The survey highlights the prevailing challenges in the integration of LLMs and points out promising research directions, aiming to serve as a valuable resource for interdisciplinary researchers."}}
{"id": "2507.08448", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.08448", "abs": "https://arxiv.org/abs/2507.08448", "authors": ["Wei Zhang", "Yihang Wu", "Songhua Li", "Wenjie Ma", "Xin Ma", "Qiang Li", "Qi Wang"], "title": "Review of Feed-forward 3D Reconstruction: From DUSt3R to VGGT", "comment": null, "summary": "3D reconstruction, which aims to recover the dense three-dimensional\nstructure of a scene, is a cornerstone technology for numerous applications,\nincluding augmented/virtual reality, autonomous driving, and robotics. While\ntraditional pipelines like Structure from Motion (SfM) and Multi-View Stereo\n(MVS) achieve high precision through iterative optimization, they are limited\nby complex workflows, high computational cost, and poor robustness in\nchallenging scenarios like texture-less regions. Recently, deep learning has\ncatalyzed a paradigm shift in 3D reconstruction. A new family of models,\nexemplified by DUSt3R, has pioneered a feed-forward approach. These models\nemploy a unified deep network to jointly infer camera poses and dense geometry\ndirectly from an Unconstrained set of images in a single forward pass. This\nsurvey provides a systematic review of this emerging domain. We begin by\ndissecting the technical framework of these feed-forward models, including\ntheir Transformer-based correspondence modeling, joint pose and geometry\nregression mechanisms, and strategies for scaling from two-view to multi-view\nscenarios. To highlight the disruptive nature of this new paradigm, we contrast\nit with both traditional pipelines and earlier learning-based methods like\nMVSNet. Furthermore, we provide an overview of relevant datasets and evaluation\nmetrics. Finally, we discuss the technology's broad application prospects and\nidentify key future challenges and opportunities, such as model accuracy and\nscalability, and handling dynamic scenes.", "AI": {"tldr": "The paper introduces a new paradigm in 3D reconstruction using deep learning models that can infer camera poses and dense geometry from images more efficiently than traditional methods.", "motivation": "The motivation stems from the limitations of traditional 3D reconstruction methods, such as complex workflows, high computational cost, and poor robustness, particularly in texture-less regions. The new approach aims to address these issues.", "method": "The paper discusses a new family of deep learning models, represented by DUSt3R, which adopt a feed-forward approach to 3D reconstruction, using a unified deep network to infer camera poses and dense geometry directly from a set of unconstrained images in a single step.", "result": "The survey contrasts this new paradigm with traditional methods and earlier learning-based approaches, shedding light on the disruptive potential of feed-forward models in 3D reconstruction.", "conclusion": "The paper outlines the broader applications and future challenges of feed-forward 3D reconstruction models, including scalability and dynamic scene handling, indicating the transformative impact of this technology."}}
{"id": "2507.08427", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.08427", "abs": "https://arxiv.org/abs/2507.08427", "authors": ["Zilu Dong", "Xiangqing Shen", "Zinong Yang", "Rui Xia"], "title": "ChainEdit: Propagating Ripple Effects in LLM Knowledge Editing through Logical Rule-Guided Chains", "comment": "Accepted to ACL 2025 (main)", "summary": "Current knowledge editing methods for large language models (LLMs) struggle\nto maintain logical consistency when propagating ripple effects to associated\nfacts. We propose ChainEdit, a framework that synergizes knowledge\ngraph-derived logical rules with LLM logical reasoning capabilities to enable\nsystematic chain updates. By automatically extracting logical patterns from\nstructured knowledge bases and aligning them with LLMs' internal logics,\nChainEdit dynamically generates and edits logically connected knowledge\nclusters. Experiments demonstrate an improvement of more than 30% in logical\ngeneralization over baselines while preserving editing reliability and\nspecificity. We further address evaluation biases in existing benchmarks\nthrough knowledge-aware protocols that disentangle external dependencies. This\nwork establishes new state-of-the-art performance on ripple effect while\nensuring internal logical consistency after knowledge editing.", "AI": {"tldr": "ChainEdit框架通过结合知识图谱的逻辑规则与大型语言模型的逻辑推理能力，解决了知识编辑过程中的逻辑一致性问题，以实现系统性知识更新，并在评估中表现优异。", "motivation": "当前的知识编辑方法在大型语言模型中难以保持逻辑一致性，尤其是在传播与相关事实的涟漪效应时。", "method": "提出ChainEdit框架，该框架结合了知识图谱衍生的逻辑规则与大语言模型的逻辑推理能力，以实现系统的链式更新。通过从结构化知识库中自动抽取逻辑模式并与大语言模型的内部逻辑对齐，ChainEdit能够动态生成和编辑逻辑相关的知识簇。", "result": "实验表明，相较于基线方法，逻辑泛化性能提高了30%以上，同时保持了编辑的可靠性和特异性。", "conclusion": "通过知识感知的评估协议，该工作在处理涟漪效应时达成了逻辑一致性，并建立了新的性能标准。"}}
{"id": "2507.08458", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.08458", "abs": "https://arxiv.org/abs/2507.08458", "authors": ["Benjamin Meyer", "Lukas Tuggener", "Sascha Hänzi", "Daniel Schmid", "Erdal Ayfer", "Benjamin F. Grewe", "Ahmed Abdulkadir", "Thilo Stadelmann"], "title": "A document is worth a structured record: Principled inductive bias design for document recognition", "comment": null, "summary": "Many document types use intrinsic, convention-driven structures that serve to\nencode precise and structured information, such as the conventions governing\nengineering drawings. However, state-of-the-art approaches treat document\nrecognition as a mere computer vision problem, neglecting these underlying\ndocument-type-specific structural properties, making them dependent on\nsub-optimal heuristic post-processing and rendering many less frequent or more\ncomplicated document types inaccessible to modern document recognition. We\nsuggest a novel perspective that frames document recognition as a transcription\ntask from a document to a record. This implies a natural grouping of documents\nbased on the intrinsic structure inherent in their transcription, where related\ndocument types can be treated (and learned) similarly. We propose a method to\ndesign structure-specific inductive biases for the underlying machine-learned\nend-to-end document recognition systems, and a respective base transformer\narchitecture that we successfully adapt to different structures. We demonstrate\nthe effectiveness of the so-found inductive biases in extensive experiments\nwith progressively complex record structures from monophonic sheet music, shape\ndrawings, and simplified engineering drawings. By integrating an inductive bias\nfor unrestricted graph structures, we train the first-ever successful\nend-to-end model to transcribe engineering drawings to their inherently\ninterlinked information. Our approach is relevant to inform the design of\ndocument recognition systems for document types that are less well understood\nthan standard OCR, OMR, etc., and serves as a guide to unify the design of\nfuture document foundation models.", "AI": {"tldr": "提出一种新的文档识别方法，通过设计特定结构的归纳偏置，成功实现了对密集文档类型的转录，完善了文档识别系统的应用。", "motivation": "当前最先进的文档识别方法忽视了文档类型特定的结构属性，导致过多依赖次优的启发式后处理，并使许多不太常见或更复杂的文档类型无法进行现代文档识别。", "method": "本文提出了一种新的文档识别视角，将其视为从文档到记录的转录任务，并设计了针对特定结构的归纳偏置方法及相应的基础Transformer架构，以适应不同的文档结构。", "result": "通过在复杂记录结构上进行广泛实验，验证了所设计归纳偏置的有效性，并成功训练了第一个能够将工程图纸转录为其内在相互关联信息的端到端模型。", "conclusion": "该方法为设计未被充分理解的文档类型识别系统奠定了基础，并将成为未来文档基础模型设计的指南。"}}
{"id": "2507.08440", "categories": ["cs.CL", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2507.08440", "abs": "https://arxiv.org/abs/2507.08440", "authors": ["Selina Heller", "Mohamed Ibrahim", "David Antony Selby", "Sebastian Vollmer"], "title": "Finding Common Ground: Using Large Language Models to Detect Agreement in Multi-Agent Decision Conferences", "comment": null, "summary": "Decision conferences are structured, collaborative meetings that bring\ntogether experts from various fields to address complex issues and reach a\nconsensus on recommendations for future actions or policies. These conferences\noften rely on facilitated discussions to ensure productive dialogue and\ncollective agreement. Recently, Large Language Models (LLMs) have shown\nsignificant promise in simulating real-world scenarios, particularly through\ncollaborative multi-agent systems that mimic group interactions. In this work,\nwe present a novel LLM-based multi-agent system designed to simulate decision\nconferences, specifically focusing on detecting agreement among the participant\nagents. To achieve this, we evaluate six distinct LLMs on two tasks: stance\ndetection, which identifies the position an agent takes on a given issue, and\nstance polarity detection, which identifies the sentiment as positive,\nnegative, or neutral. These models are further assessed within the multi-agent\nsystem to determine their effectiveness in complex simulations. Our results\nindicate that LLMs can reliably detect agreement even in dynamic and nuanced\ndebates. Incorporating an agreement-detection agent within the system can also\nimprove the efficiency of group debates and enhance the overall quality and\ncoherence of deliberations, making them comparable to real-world decision\nconferences regarding outcome and decision-making. These findings demonstrate\nthe potential for LLM-based multi-agent systems to simulate group\ndecision-making processes. They also highlight that such systems could be\ninstrumental in supporting decision-making with expert elicitation workshops\nacross various domains.", "AI": {"tldr": "本文提出了一种基于大规模语言模型（LLMs）的多智能体系统，用于模拟决策会议，检测参与者之间的共识，实验结果和结论验证了该方法在模拟群体决策过程中的有效性。", "motivation": "研究的动机在于展示LLM在其模拟现实世界场景的潜力，特别是通过多智能体系统模拟群体互动。通过这些研究，希望为跨领域的专家征询研讨会提供决策支持。", "method": "本文介绍了一种基于大规模语言模型（LLMs）的多智能体系统，旨在模拟决策会议，并特别关注检测参与者之间的共识。研究评估了六个不同的大规模语言模型在两个任务上的表现：立场检测（识别智能体对特定问题的立场）和立场极性检测（确定智能体的观点情感）。", "result": "实验结果显示，大规模语言模型能够在动态和复杂的辩论中可靠地检测到共识。在系统中加入一个共识检测智能体可以提高小组辩论的效率，并提升整体讨论的质量和连贯性，使模拟会议的结果和决策过程与真实世界中的决策会议相当。", "conclusion": "这些发现证明了LLM多智能体系统模拟群体决策过程的潜力。同时也表明这些系统在支持专家征询研讨会的决策中可能发挥重要作用。"}}
{"id": "2507.08460", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.08460", "abs": "https://arxiv.org/abs/2507.08460", "authors": ["Seyedeh Sahar Taheri Otaghsara", "Reza Rahmanzadeh"], "title": "F3-Net: Foundation Model for Full Abnormality Segmentation of Medical Images with Flexible Input Modality Requirement", "comment": null, "summary": "F3-Net is a foundation model designed to overcome persistent challenges in\nclinical medical image segmentation, including reliance on complete multimodal\ninputs, limited generalizability, and narrow task specificity. Through flexible\nsynthetic modality training, F3-Net maintains robust performance even in the\npresence of missing MRI sequences, leveraging a zero-image strategy to\nsubstitute absent modalities without relying on explicit synthesis networks,\nthereby enhancing real-world applicability. Its unified architecture supports\nmulti-pathology segmentation across glioma, metastasis, stroke, and white\nmatter lesions without retraining, outperforming CNN-based and\ntransformer-based models that typically require disease-specific fine-tuning.\nEvaluated on diverse datasets such as BraTS 2021, BraTS 2024, and ISLES 2022,\nF3-Net demonstrates strong resilience to domain shifts and clinical\nheterogeneity. On the whole pathology dataset, F3-Net achieves average Dice\nSimilarity Coefficients (DSCs) of 0.94 for BraTS-GLI 2024, 0.82 for BraTS-MET\n2024, 0.94 for BraTS 2021, and 0.79 for ISLES 2022. This positions it as a\nversatile, scalable solution bridging the gap between deep learning research\nand practical clinical deployment.", "AI": {"tldr": "F3-Net通过灵活的合成模态训练和零图像策略克服了临床医学图像分割中的挑战，其多任务适用性超越了CNN和Transformer模型，展示了其在具体疾病数据集上的优秀性能。", "motivation": "F3-Net旨在解决临床医学图像分割中的一些持续性挑战，如对完整多模态输入的依赖、有限的泛化能力和特定任务的狭窄范围。", "method": "F3-Net采用灵活的合成模态训练，能减少对完整多模态输入的依赖，并通过零图像策略来增强其在实际应用中的表现。其统一架构适用于多种病理分割，包括胶质瘤、转移瘤、中风和白质病变，无需针对每种疾病重新训练。", "result": "在包括BraTS 2021, BraTS 2024和ISLES 2022在内的多个数据集上进行评估，F3-Net在不同领域转移和临床异质性方面表现出强大的韧性。在整体病理数据集上，F3-Net的平均Dice相似系数（DSC）分别为BraTS-GLI 2024 0.94，BraTS-MET 2024 0.82，BraTS 2021 0.94，ISLES 2022 0.79。", "conclusion": "F3-Net被定位为桥接深度学习研究与实际临床应用之间的差距的多功能、可扩展解决方案。"}}
{"id": "2507.08459", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.08459", "abs": "https://arxiv.org/abs/2507.08459", "authors": ["Zishan Xu", "Shuyi Xie", "Qingsong Lv", "Shupei Xiao", "Linlin Song", "Sui Wenjuan", "Fan Lin"], "title": "Diagnosing Failures in Large Language Models' Answers: Integrating Error Attribution into Evaluation Framework", "comment": null, "summary": "With the widespread application of Large Language Models (LLMs) in various\ntasks, the mainstream LLM platforms generate massive user-model interactions\ndaily. In order to efficiently analyze the performance of models and diagnose\nfailures in their answers, it is essential to develop an automated framework to\nsystematically categorize and attribute errors. However, existing evaluation\nmodels lack error attribution capability. In this work, we establish a\ncomprehensive Misattribution Framework with 6 primary and 15 secondary\ncategories to facilitate in-depth analysis. Based on this framework, we present\nAttriData, a dataset specifically designed for error attribution, encompassing\nmisattribution, along with the corresponding scores and feedback. We also\npropose MisAttributionLLM, a fine-tuned model on AttriData, which is the first\ngeneral-purpose judge model capable of simultaneously generating score,\nmisattribution, and feedback. Extensive experiments and analyses are conducted\nto confirm the effectiveness and robustness of our proposed method.", "AI": {"tldr": "本文针对现有的大型语言模型性能分析和错误诊断存在问题，提出一个错误归因框架和对应的数据集AttriData，以及基于此数据集训练的模型MisAttributionLLM。该模型能够有效实现错误的分类、归因以及提供反馈，显著提升了错误诊断的自动化和系统化。", "motivation": "主流的大型语言模型平台上每天生成大量用户与模型的交互数据。为了高效地分析模型性能和诊断失败的回答，迫切需要一个自动化的框架来系统地分类和归因错误。然而，现有的评估模型缺乏错误归因功能。为了填补这一空白，提高错误诊断的自动化和系统化水平，该研究旨在开发一个新的框架和关联模型。", "method": "本文提出了一个全面的错误归因框架，该框架包括6个主要类别和15个二级类别，用于系统地分类和归因错误。基于此框架，作者构建了AttriData数据集，该数据集专门用于错误归因，包含了误归因及其对应的分数和反馈。此外，作者还提出了MisAttributionLLM模型，该模型在AttriData数据集上进行了微调，是首个能够同时生成分数、误归因和反馈的通用评判模型。", "result": "通过实验和分析，验证了模型在错误归因上的有效性，MisAttributionLLM模型展示出了强大的性能，不但能够准确地判断回答的错误类型，还能提供对应的分数和反馈，体现出模型的稳健性和泛化能力。", "conclusion": "通过广泛的实验和分析验证表明，所提出的方法是有效的且稳定的，可以为大型语言模型的性能分析和错误诊断提供有力的支持。"}}
{"id": "2507.08492", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.08492", "abs": "https://arxiv.org/abs/2507.08492", "authors": ["Heng Li", "Qingcai Chen", "Xiangping Wu"], "title": "Dual Dimensions Geometric Representation Learning Based Document Dewarping", "comment": null, "summary": "Document image dewarping remains a challenging task in the deep learning era.\nWhile existing methods have improved by leveraging text line awareness, they\ntypically focus only on a single horizontal dimension. In this paper, we\npropose a fine-grained deformation perception model that focuses on Dual\nDimensions of document horizontal-vertical-lines to improve document Dewarping\ncalled D2Dewarp. It can perceive distortion trends in different directions\nacross document details. To combine the horizontal and vertical granularity\nfeatures, an effective fusion module based on X and Y coordinate is designed to\nfacilitate interaction and constraint between the two dimensions for feature\ncomplementarity. Due to the lack of annotated line features in current public\ndewarping datasets, we also propose an automatic fine-grained annotation method\nusing public document texture images and an automatic rendering engine to build\na new large-scale distortion training dataset. The code and dataset will be\npublicly released. On public Chinese and English benchmarks, both quantitative\nand qualitative results show that our method achieves better rectification\nresults compared with the state-of-the-art methods. The dataset will be\npublicly available at https://github.com/xiaomore/DocDewarpHV", "AI": {"tldr": "本研究提出了D2Dewarp模型，通过关注文档的水平和垂直线，改善了文档去畸变的效果，并提出了一种自动生成注释的方法来构建新的大规模畸变训练数据集。", "motivation": "现有的文档图像去畸变方法通常只关注单一的水平维度，因此无法全面捕捉到文档的变形趋势。本文旨在利用水平和垂直两个维度的信息，以达到更好的去畸变效果。", "method": "D2Dewarp模型通过设计一个基于X和Y坐标的有效融合模块，实现了水平和垂直维度之间特征的互补和约束。此外，本文还提出了一种使用公共文档纹理图像和自动渲染引擎来生成自动精细标注的新方法，构建新的大规模畸变训练数据集。", "result": "D2Dewarp模型在中文和英文公开基准数据集上取得了比现有最优方法更好的量化和视觉效果。", "conclusion": "本研究提出的方法能够更好地捕捉文档在不同方向上的变形趋势，改善了文档图像的去畸变效果，同时构建了一个新的大规模训练数据集。"}}
{"id": "2507.08468", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.08468", "abs": "https://arxiv.org/abs/2507.08468", "authors": ["Marina Luketina", "Andrea Benkel", "Christoph G. Schuetz"], "title": "Using Large Language Models for Legal Decision-Making in Austrian Value-Added Tax Law: An Experimental Study", "comment": "26 pages, 5 figures, 6 tables", "summary": "This paper provides an experimental evaluation of the capability of large\nlanguage models (LLMs) to assist in legal decision-making within the framework\nof Austrian and European Union value-added tax (VAT) law. In tax consulting\npractice, clients often describe cases in natural language, making LLMs a prime\ncandidate for supporting automated decision-making and reducing the workload of\ntax professionals. Given the requirement for legally grounded and\nwell-justified analyses, the propensity of LLMs to hallucinate presents a\nconsiderable challenge. The experiments focus on two common methods for\nenhancing LLM performance: fine-tuning and retrieval-augmented generation\n(RAG). In this study, these methods are applied on both textbook cases and\nreal-world cases from a tax consulting firm to systematically determine the\nbest configurations of LLM-based systems and assess the legal-reasoning\ncapabilities of LLMs. The findings highlight the potential of using LLMs to\nsupport tax consultants by automating routine tasks and providing initial\nanalyses, although current prototypes are not ready for full automation due to\nthe sensitivity of the legal domain. The findings indicate that LLMs, when\nproperly configured, can effectively support tax professionals in VAT tasks and\nprovide legally grounded justifications for decisions. However, limitations\nremain regarding the handling of implicit client knowledge and context-specific\ndocumentation, underscoring the need for future integration of structured\nbackground information.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2507.08494", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.08494", "abs": "https://arxiv.org/abs/2507.08494", "authors": ["Martin Engilberge", "Ivan Vrkic", "Friedrich Wilke Grosche", "Julien Pilet", "Engin Turetken", "Pascal Fua"], "title": "Unified People Tracking with Graph Neural Networks", "comment": null, "summary": "This work presents a unified, fully differentiable model for multi-people\ntracking that learns to associate detections into trajectories without relying\non pre-computed tracklets. The model builds a dynamic spatiotemporal graph that\naggregates spatial, contextual, and temporal information, enabling seamless\ninformation propagation across entire sequences. To improve occlusion handling,\nthe graph can also encode scene-specific information. We also introduce a new\nlarge-scale dataset with 25 partially overlapping views, detailed scene\nreconstructions, and extensive occlusions. Experiments show the model achieves\nstate-of-the-art performance on public benchmarks and the new dataset, with\nflexibility across diverse conditions. Both the dataset and approach will be\npublicly released to advance research in multi-people tracking.", "AI": {"tldr": "A novel fully differentiable model for multi-person tracking is proposed and evaluates it on a new large-scale dataset, achieving state-of-the-art performance.", "motivation": "Address the challenges in multi-person tracking, especially in handling occlusions, by providing a unified model and a new large-scale dataset.", "method": "This work introduces a fully differentiable model for multi-person tracking that constructs a dynamic spatiotemporal graph for seamless information propagation without relying on pre-computed tracklets.", "result": "Experiments demonstrate state-of-the-art performance on public benchmarks and a newly introduced large-scale dataset, showcased even under occluded conditions.", "conclusion": "The proposed model and dataset advance the field of multi-person tracking by achieving flexible and robust performance across various conditions."}}
{"id": "2507.08477", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.08477", "abs": "https://arxiv.org/abs/2507.08477", "authors": ["Qingliang Meng", "Hao Wu", "Wei Liang", "Wei Xu", "Qing Zhao"], "title": "ILT-Iterative LoRA Training through Focus-Feedback-Fix for Multilingual Speech Recognition", "comment": "Accepted By Interspeech 2025 MLC-SLM workshop as a Research Paper", "summary": "The deep integration of large language models and automatic speech\nrecognition systems has become a promising research direction with high\npractical value. To address the overfitting issue commonly observed in Low-Rank\nAdaptation (LoRA) during the supervised fine-tuning (SFT) stage, this work\nproposes an innovative training paradigm Iterative LoRA Training (ILT) in\ncombination with an Iterative Pseudo Labeling strategy, effectively enhancing\nthe theoretical upper bound of model performance. Based on Whisper-large-v3 and\nQwen2-Audio, we conduct systematic experiments using a three-stage training\nprocess: Focus Training, Feed Back Training, and Fix Training. Experimental\nresults demonstrate the effectiveness of the proposed method. Furthermore, the\nMegaAIS research team applied this technique in the Interspeech 2025\nMultilingual Conversational Speech Language Modeling Challenge (MLC-SLM),\nachieving 4th in Track 1 (Multilingual ASR Task) and 1st place in Track 2\n(Speech Separation and Recognition Task), showcasing the practical feasibility\nand strong application potential of our approach.", "AI": {"tldr": "本文提出了一种迭代LoRA训练方法，结合迭代伪标签策略，有效解决了过拟合问题，通过三个阶段的训练实验，证明了方法的有效性和在实际任务中的优秀表现。", "motivation": "大型语言模型与自动语音识别系统深度融合成为具有高实用价值的热门研究方向。", "method": "为了应对LoRA在监督微调阶段常见的过拟合问题，本研究提出了一个新颖的训练范式——迭代LoRA训练（ILT），并与迭代伪标签策略相结合，有效提升了模型性能的理论上限。", "result": "实验结果表明，本研究所提出的方法是有效的，且该技术在Interspeech 2025多语言对话语音语言建模挑战赛中取得了优异成绩，证明了方法的实践可行性和应用潜力。", "conclusion": "研究证实了迭代LoRA训练与迭代伪标签策略结合的有效性及该方法在多语言自动语音识别任务中的卓越性能。"}}
{"id": "2507.08520", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.08520", "abs": "https://arxiv.org/abs/2507.08520", "authors": ["Yufei Zheng", "Wenjun Wang", "Wenjun Gan", "Jiawei Liu"], "title": "Occlusion-Guided Feature Purification Learning via Reinforced Knowledge Distillation for Occluded Person Re-Identification", "comment": "13 pages, 8 figures", "summary": "Occluded person re-identification aims to retrieve holistic images based on\noccluded ones. Existing methods often rely on aligning visible body parts,\napplying occlusion augmentation, or complementing missing semantics using\nholistic images. However, they face challenges in handling diverse occlusion\nscenarios not seen during training and the issue of feature contamination from\nholistic images. To address these limitations, we propose Occlusion-Guided\nFeature Purification Learning via Reinforced Knowledge Distillation (OGFR),\nwhich simultaneously mitigates these challenges. OGFR adopts a teacher-student\ndistillation architecture that effectively incorporates diverse occlusion\npatterns into feature representation while transferring the purified\ndiscriminative holistic knowledge from the holistic to the occluded branch\nthrough reinforced knowledge distillation. Specifically, an Occlusion-Aware\nVision Transformer is designed to leverage learnable occlusion pattern\nembeddings to explicitly model such diverse occlusion types, thereby guiding\nocclusion-aware robust feature representation. Moreover, we devise a Feature\nErasing and Purification Module within the holistic branch, in which an agent\nis employed to identify low-quality patch tokens of holistic images that\ncontain noisy negative information via deep reinforcement learning, and\nsubstitute these patch tokens with learnable embedding tokens to avoid feature\ncontamination and further excavate identity-related discriminative clues.\nAfterward, with the assistance of knowledge distillation, the student branch\neffectively absorbs the purified holistic knowledge to precisely learn robust\nrepresentation regardless of the interference of occlusions.", "AI": {"tldr": "本文提出了一种遮挡引导特征净化学习方法（OGFR），通过引入遮挡感知图像变换器和特征擦除与净化模块，有效解决了现有方法在处理多样化遮挡场景时的问题和特征污染问题。", "motivation": "现有的遮挡行人重识别方法依赖于可见部位的对齐、遮挡数据增强或使用完整图像补充缺失语义，但它们面临处理未见过的多样化遮挡场景的挑战和来自完整图像的特征污染问题。", "method": "本文提出了通过强化知识蒸馏进行遮挡引导特征净化学习的方法（OGFR），以同时解决现有方法在处理多样化的遮挡场景时遇到的问题和特征污染问题。OGFR采用教师-学生知识蒸馏架构，有效地将多种遮挡模式纳入特征表示中，并通过强化知识蒸馏从完整图像分支向遮挡图像分支传递净化后的辨别知识。此外，还设计了遮挡感知图像变换器和特征擦除与净化模块以指导遮挡鲁棒特征表示，并避免因遮挡带来的特征污染，进一步挖掘身份相关辨别线索。", "result": "无具体结果展示，方法描述较多。", "conclusion": "无具体结论展示，方法和架构介绍较多。"}}
{"id": "2507.08487", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.08487", "abs": "https://arxiv.org/abs/2507.08487", "authors": ["Bruno Alexandre Rosa", "Hilário Oliveira", "Luiz Rodrigues", "Eduardo Araujo Oliveira", "Rafael Ferreira Mello"], "title": "Enhancing Essay Cohesion Assessment: A Novel Item Response Theory Approach", "comment": "24 pages, 4 tables", "summary": "Essays are considered a valuable mechanism for evaluating learning outcomes\nin writing. Textual cohesion is an essential characteristic of a text, as it\nfacilitates the establishment of meaning between its parts. Automatically\nscoring cohesion in essays presents a challenge in the field of educational\nartificial intelligence. The machine learning algorithms used to evaluate texts\ngenerally do not consider the individual characteristics of the instances that\ncomprise the analysed corpus. In this meaning, item response theory can be\nadapted to the context of machine learning, characterising the ability,\ndifficulty and discrimination of the models used. This work proposes and\nanalyses the performance of a cohesion score prediction approach based on item\nresponse theory to adjust the scores generated by machine learning models. In\nthis study, the corpus selected for the experiments consisted of the extended\nEssay-BR, which includes 6,563 essays in the style of the National High School\nExam (ENEM), and the Brazilian Portuguese Narrative Essays, comprising 1,235\nessays written by 5th to 9th grade students from public schools. We extracted\n325 linguistic features and treated the problem as a machine learning\nregression task. The experimental results indicate that the proposed approach\noutperforms conventional machine learning models and ensemble methods in\nseveral evaluation metrics. This research explores a potential approach for\nimproving the automatic evaluation of cohesion in educational essays.", "AI": {"tldr": "本研究提出了一种基于项目反应理论的方法来预测和评分教育论文中的连贯性，并将该方法在多个评估指标下验证，结果优于传统方法。", "motivation": "机器学习算法用于文本评价时通常不考虑构成分析语料库的实例的个别特征。因此，本研究探讨了将项目反应理论调整到机器学习背景下，以描述模型的能力、难度和区分度。", "method": "本研究提出了一种基于项目反应理论的连贯性评分预测方法，以调整机器学习模型生成的分数。使用的数据集包括扩展开的Essay-BR（包含6,563篇类似于全国高中考试(ENEM)风格的文章）和巴西葡萄牙语叙事文（包含1,235篇来自公立学校5至9年级学生的文章）。从这些数据集中提取了325个语言特征，并将其视为一个机器学习回归任务。", "result": "实验结果表明，该基于项目反应理论的方法在多个评估指标上优于传统的机器学习模型和集成方法。", "conclusion": "实验结果表明，所提出的方法在多个评估指标上优于传统的机器学习模型和集成方法，为提高教育论文中连贯性的自动评估提供了一种潜在方法。"}}
{"id": "2507.08546", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.08546", "abs": "https://arxiv.org/abs/2507.08546", "authors": ["Inye Na", "Nejung Rue", "Jiwon Chung", "Hyunjin Park"], "title": "RadiomicsRetrieval: A Customizable Framework for Medical Image Retrieval Using Radiomics Features", "comment": "Accepted at MICCAI 2025", "summary": "Medical image retrieval is a valuable field for supporting clinical\ndecision-making, yet current methods primarily support 2D images and require\nfully annotated queries, limiting clinical flexibility. To address this, we\npropose RadiomicsRetrieval, a 3D content-based retrieval framework bridging\nhandcrafted radiomics descriptors with deep learning-based embeddings at the\ntumor level. Unlike existing 2D approaches, RadiomicsRetrieval fully exploits\nvolumetric data to leverage richer spatial context in medical images. We employ\na promptable segmentation model (e.g., SAM) to derive tumor-specific image\nembeddings, which are aligned with radiomics features extracted from the same\ntumor via contrastive learning. These representations are further enriched by\nanatomical positional embedding (APE). As a result, RadiomicsRetrieval enables\nflexible querying based on shape, location, or partial feature sets. Extensive\nexperiments on both lung CT and brain MRI public datasets demonstrate that\nradiomics features significantly enhance retrieval specificity, while APE\nprovides global anatomical context essential for location-based searches.\nNotably, our framework requires only minimal user prompts (e.g., a single\npoint), minimizing segmentation overhead and supporting diverse clinical\nscenarios. The capability to query using either image embeddings or selected\nradiomics attributes highlights its adaptability, potentially benefiting\ndiagnosis, treatment planning, and research on large-scale medical imaging\nrepositories. Our code is available at\nhttps://github.com/nainye/RadiomicsRetrieval.", "AI": {"tldr": "RadiomicsRetrieval是一个3D医学图像检索系统，结合影像组学特征和深度学习嵌入，通过对比学习对齐特征，并应用解剖位置嵌入提供全局上下文，不仅提高了检索的特异性，还降低了用户操作的复杂度。", "motivation": "当前医学图像检索方法主要支持2D图像，需要完全标注的查询，限制了临床的灵活性。为解决这个问题，提出了RadiomicsRetrieval。", "method": "提出RadiomicsRetrieval，这是一个3D内容检索框架，结合手工提取的影像组学特征与深度学习生成的肿瘤级别嵌入。使用可提示的分割模型（如SAM）来获取肿瘤特异性图像嵌入，并通过对比学习与同样的肿瘤提取的影像组学特征对齐。这些表示进一步由解剖位置嵌入（APE）加强。", "result": "广泛的实验结果表明，影像组学特征显著提高了检索的特异性，而APE提供了全解剖位置的全局上下文，对于基于位置的搜索至关重要。", "conclusion": "RadiomicsRetrieval框架只需要极小的用户提示，减少了分割的开销，并能支持多样化的临床场景。其可以通过图像嵌入或选择的影像组学属性进行查询的能力体现了其适应性，可能有助于诊断、治疗计划和大规模医疗图像库的研究。"}}
{"id": "2507.08491", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.08491", "abs": "https://arxiv.org/abs/2507.08491", "authors": ["David Schlangen", "Sherzod Hakimov", "Jonathan Jordan", "Philipp Sadler"], "title": "A Third Paradigm for LLM Evaluation: Dialogue Game-Based Evaluation using clembench", "comment": "All code required to run the benchmark, as well as extensive\n  documentation, is available at https://github.com/clembench/clembench", "summary": "There are currently two main paradigms for evaluating large language models\n(LLMs), reference-based evaluation and preference-based evaluation. The first,\ncarried over from the evaluation of machine learning models in general, relies\non pre-defined task instances, for which reference task executions are\navailable. The second, best exemplified by the LM-arena, relies on (often\nself-selected) users bringing their own intents to a site that routes these to\nseveral models in parallel, among whose responses the user then selects their\nmost preferred one. The former paradigm hence excels at control over what is\ntested, while the latter comes with higher ecological validity, testing actual\nuse cases interactively. Recently, a third complementary paradigm has emerged\nthat combines some of the strengths of these approaches, offering control over\nmulti-turn, reference-free, repeatable interactions, while stressing\ngoal-directedness: dialogue game based evaluation. While the utility of this\napproach has been shown by several projects, its adoption has been held back by\nthe lack of a mature, easily re-usable implementation. In this paper, we\npresent clembench, which has been in continuous development since 2023 and has\nin its latest release been optimized for ease of general use. We describe how\nit can be used to benchmark one's own models (using a provided set of benchmark\ngame instances in English), as well as how easily the benchmark itself can be\nextended with new, tailor-made targeted tests.", "AI": {"tldr": "研究提出了一种名为clembench的工具，用于评估大语言模型的对话能力，该工具支持定制测试且易于使用。", "motivation": "该研究旨在解决对话游戏评估方法虽然具有优势，但由于缺乏成熟、可重用的实现而没有被广泛采用的问题。通过提供一个易于使用的评估工具，推动该评估方法的广泛应用。", "method": "该研究提出了一个名为clembench的工具，该工具自2023年开始持续开发，旨在优化对话游戏评估框架的便捷使用。此外，该工具提供了英语对话评估实例，并支持用户扩展新的定制测试。", "result": "研究展示了clembench的使用情况，可以用来评估大语言模型，并提供了基准对话实例，同时介绍如何扩展新的测试以更好地满足个性化需求。", "conclusion": "clembench工具有效地解决了之前对话游戏评估方法在实施上的困难，为大语言模型的评估提供了一个新的高效途径。"}}
{"id": "2507.08548", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.08548", "abs": "https://arxiv.org/abs/2507.08548", "authors": ["Alen Adamyan", "Tomáš Čížek", "Matej Straka", "Klara Janouskova", "Martin Schmid"], "title": "SAM2RL: Towards Reinforcement Learning Memory Control in Segment Anything Model 2", "comment": null, "summary": "Segment Anything Model 2 (SAM 2) has demonstrated strong performance in\nobject segmentation tasks and has become the state-of-the-art for visual object\ntracking. The model stores information from previous frames in a memory bank,\nenabling temporal consistency across video sequences. Recent methods augment\nSAM 2 with hand-crafted update rules to better handle distractors, occlusions,\nand object motion. We propose a fundamentally different approach using\nreinforcement learning for optimizing memory updates in SAM 2 by framing memory\ncontrol as a sequential decision-making problem. In an overfitting setup with a\nseparate agent per video, our method achieves a relative improvement over SAM 2\nthat exceeds by more than three times the gains of existing heuristics. These\nresults reveal the untapped potential of the memory bank and highlight\nreinforcement learning as a powerful alternative to hand-crafted update rules\nfor memory control in visual object tracking.", "AI": {"tldr": "This paper proposes using reinforcement learning to optimize memory updates in the Segment Anything Model 2 (SAM 2) for visual object tracking tasks, demonstrating significantly better performance than existing methods.", "motivation": "The motivation is to improve the handling of distractors, occlusions, and object motion in visual object tracking tasks by leveraging reinforcement learning to control the model's memory updates more effectively than existing hand-crafted rules.", "method": "Our method uses reinforcement learning to optimize memory updates in SAM 2, treating memory control as a sequential decision-making problem.", "result": "In an overfitting setup with a separate agent per video, our method shows a relative improvement over SAM 2 that is more than three times greater than the gains achieved by existing heuristics.", "conclusion": "The results indicate that reinforcement learning is a powerful alternative to hand-crafted update rules for memory control in visual object tracking, revealing the untapped potential of the memory bank in the Segment Anything Model 2."}}
{"id": "2507.08496", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.08496", "abs": "https://arxiv.org/abs/2507.08496", "authors": ["Shibo Sun", "Xue Li", "Donglin Di", "Mingjie Wei", "Lanshun Nie", "Wei-Nan Zhang", "Dechen Zhan", "Yang Song", "Lei Fan"], "title": "LLaPa: A Vision-Language Model Framework for Counterfactual-Aware Procedural Planning", "comment": null, "summary": "While large language models (LLMs) have advanced procedural planning for\nembodied AI systems through strong reasoning abilities, the integration of\nmultimodal inputs and counterfactual reasoning remains underexplored. To tackle\nthese challenges, we introduce LLaPa, a vision-language model framework\ndesigned for multimodal procedural planning. LLaPa generates executable action\nsequences from textual task descriptions and visual environmental images using\nvision-language models (VLMs). Furthermore, we enhance LLaPa with two auxiliary\nmodules to improve procedural planning. The first module, the Task-Environment\nReranker (TER), leverages task-oriented segmentation to create a task-sensitive\nfeature space, aligning textual descriptions with visual environments and\nemphasizing critical regions for procedural execution. The second module, the\nCounterfactual Activities Retriever (CAR), identifies and emphasizes potential\ncounterfactual conditions, enhancing the model's reasoning capability in\ncounterfactual scenarios. Extensive experiments on ActPlan-1K and ALFRED\nbenchmarks demonstrate that LLaPa generates higher-quality plans with superior\nLCS and correctness, outperforming advanced models. The code and models are\navailable https://github.com/sunshibo1234/LLaPa.", "AI": {"tldr": "LLaPa 是一种用于多模态程序规划的视觉-语言模型，能够生成更好的执行计划，并且在多个数据集上表现优于其他先进模型。", "motivation": "尽管大型语言模型在为具身AI系统提供的程序规划方面有了显著的进步，但多模态输入集成和反事实推理仍没有得到充分的探索。该论文旨在解决这些挑战。", "method": "LLaPa 是一个适用于多模态程序规划的视觉-语言模型框架。它可以基于任务描述和视觉环境图像生成可执行的操作序列。通过加入任务-环境重排器（TER）模块和反事实活动检索器（CAR）模块，提高了模型的程序规划能力。TER 模块通过任务导向的分割创建任务敏感的特征空间，而 CAR 模块强调了反事实情景中的潜在条件。", "result": "LLaPa 在 ActPlan-1K 和 ALFRED 数据集上的实验表明，它生成的计划质量更高，具有更好的最长公共子序列（LCS）和正确性，比先进模型表现更佳。", "conclusion": "LLaPa 框架显示了在多模态程序规划任务中，通过整合视觉和语言顺序和反事实推理，可以生成更高质量的计划。"}}
{"id": "2507.08554", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.08554", "abs": "https://arxiv.org/abs/2507.08554", "authors": ["Cristina Mata", "Michael S. Ryoo", "Henrik Turbell"], "title": "Image Translation with Kernel Prediction Networks for Semantic Segmentation", "comment": "OOD-CV Workshop at ECCV 2024", "summary": "Semantic segmentation relies on many dense pixel-wise annotations to achieve\nthe best performance, but owing to the difficulty of obtaining accurate\nannotations for real world data, practitioners train on large-scale synthetic\ndatasets. Unpaired image translation is one method used to address the ensuing\ndomain gap by generating more realistic training data in low-data regimes.\nCurrent methods for unpaired image translation train generative adversarial\nnetworks (GANs) to perform the translation and enforce pixel-level semantic\nmatching through cycle consistency. These methods do not guarantee that the\nsemantic matching holds, posing a problem for semantic segmentation where\nperformance is sensitive to noisy pixel labels. We propose a novel image\ntranslation method, Domain Adversarial Kernel Prediction Network (DA-KPN), that\nguarantees semantic matching between the synthetic label and translation.\nDA-KPN estimates pixel-wise input transformation parameters of a lightweight\nand simple translation function. To ensure the pixel-wise transformation is\nrealistic, DA-KPN uses multi-scale discriminators to distinguish between\ntranslated and target samples. We show DA-KPN outperforms previous GAN-based\nmethods on syn2real benchmarks for semantic segmentation with limited access to\nreal image labels and achieves comparable performance on face parsing.", "AI": {"tldr": "Paper presents DA-KPN, which ensures pixel-wise semantic consistency in synthetic-to-real image translation, improving semantic segmentation performance.", "motivation": "The difficulty in obtaining accurate pixel-wise annotations for real-world data for semantic segmentation leads practitioners to use synthetic datasets. However, the domain gap between synthetic and real data can degrade performance, calling for a method to bridge this gap.", "method": "Domain Adversarial Kernel Prediction Network (DA-KPN) is introduced to ensure semantic matching between synthetic labels and translated images. It predicts pixel-wise transformation parameters for a translation function and uses multi-scale discriminators for realism.", "result": "DA-KPN outperforms previous GAN-based methods on benchmark datasets for semantic segmentation with limited real data and achieves comparable performance on face parsing tasks.", "conclusion": "Proposing a method that guarantees semantic consistency in image translation from synthetic to real environments improves semantic segmentation performance in low-data scenarios."}}
{"id": "2507.08498", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.08498", "abs": "https://arxiv.org/abs/2507.08498", "authors": ["Mengze Hong", "Chen Jason Zhang", "Di Jiang"], "title": "Semantic-Augmented Latent Topic Modeling with LLM-in-the-Loop", "comment": null, "summary": "Latent Dirichlet Allocation (LDA) is a prominent generative probabilistic\nmodel used for uncovering abstract topics within document collections. In this\npaper, we explore the effectiveness of augmenting topic models with Large\nLanguage Models (LLMs) through integration into two key phases: Initialization\nand Post-Correction. Since the LDA is highly dependent on the quality of its\ninitialization, we conduct extensive experiments on the LLM-guided topic\nclustering for initializing the Gibbs sampling algorithm. Interestingly, the\nexperimental results reveal that while the proposed initialization strategy\nimproves the early iterations of LDA, it has no effect on the convergence and\nyields the worst performance compared to the baselines. The LLM-enabled\npost-correction, on the other hand, achieved a promising improvement of 5.86%\nin the coherence evaluation. These results highlight the practical benefits of\nthe LLM-in-the-loop approach and challenge the belief that LLMs are always the\nsuperior text mining alternative.", "AI": {"tldr": "本文探索了通过LLMs增强LDA模型的效果，发现虽然在初始化阶段没有明显效果，但在后期校正阶段取得了5.86%的连贯性提升。", "motivation": "由于LDA模型的效果高度依赖于其初始化的质量，因此作者探索了通过使用大型语言模型（LLMs）来增强这一过程的潜力。", "method": "本研究通过将大型语言模型（LLMs）整合到主题模型的两个关键阶段：初始化和后期校正中，来探讨LLMs对主题模型效果的提升。初始化阶段利用LLMs引导的主题聚类来初始化Gibbs采样算法。后期校正阶段则使用LLMs进行修正。", "result": "实验结果表明，提出的初始化策略虽然改善了LDA早期迭代的效果，但对收敛性没有影响，并且在对比基准测试中表现最差。然而，LLM启用的后期校正阶段在连贯性评估中取得了5.86%的显著提升。", "conclusion": "研究结果突出了在环路中使用LLMs的实际益处，并质疑了LLMs总是更好的文本挖掘替代方案这一信念。"}}
{"id": "2507.08555", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.08555", "abs": "https://arxiv.org/abs/2507.08555", "authors": ["Enyu Liu", "En Yu", "Sijia Chen", "Wenbing Tao"], "title": "Disentangling Instance and Scene Contexts for 3D Semantic Scene Completion", "comment": "Accepted to ICCV 2025", "summary": "3D Semantic Scene Completion (SSC) has gained increasing attention due to its\npivotal role in 3D perception. Recent advancements have primarily focused on\nrefining voxel-level features to construct 3D scenes. However, treating voxels\nas the basic interaction units inherently limits the utilization of class-level\ninformation, which is proven critical for enhancing the granularity of\ncompletion results. To address this, we propose \\textbf{D}isentangling Instance\nand Scene Contexts (DISC), a novel dual-stream paradigm that enhances learning\nfor both instance and scene categories through separated optimization.\nSpecifically, we replace voxel queries with discriminative class queries, which\nincorporate class-specific geometric and semantic priors. Additionally, we\nexploit the intrinsic properties of classes to design specialized decoding\nmodules, facilitating targeted interactions and efficient class-level\ninformation flow. Experimental results demonstrate that DISC achieves\nstate-of-the-art (SOTA) performance on both SemanticKITTI and\nSSCBench-KITTI-360 benchmarks, with mIoU scores of 17.35 and 20.55,\nrespectively. Remarkably, DISC even outperforms multi-frame SOTA methods using\nonly single-frame input and significantly improves instance category\nperformance, surpassing both single-frame and multi-frame SOTA instance mIoU by\n17.9\\% and 11.9\\%, respectively, on the SemanticKITTI hidden test. The code is\navailable at https://github.com/Enyu-Liu/DISC.", "AI": {"tldr": "DISC, a novel dual-stream approach for 3D Semantic Scene Completion, uses class queries instead of voxel queries and demonstrates SOTA performance on SemanticKITTI and SSCBench-KITTI-360 benchmarks, improving instance category performance significantly.", "motivation": "The motivation behind DISC is to overcome the limitations of voxels as the basic unit, which hinders the use of class-level information, essential for more detailed scene completion.", "method": "Our method, DISC, introduces a dual-stream paradigm to enhance 3D SSC by leveraging class-level information through discriminative class queries and specialized decoding modules, addressing the limitations of voxel-level features.", "result": "DISC achieves SOTA performance with mIoU scores of 17.35 and 20.55 on SemanticKITTI and SSCBench-KITTI-360, respectively, displaying a significant improvement in instance category performance over existing methods.", "conclusion": "DISC improves 3D SSC by incorporating class-level information through specialized class queries and decoding modules, achieving SOTA performance on key benchmarks, even outperforming multi-frame methods with single-frame input."}}
{"id": "2507.08499", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.08499", "abs": "https://arxiv.org/abs/2507.08499", "authors": ["Ziyi Huang", "Xia Cui"], "title": "PromotionGo at SemEval-2025 Task 11: A Feature-Centric Framework for Cross-Lingual Multi-Emotion Detection in Short Texts", "comment": null, "summary": "This paper presents our system for SemEval 2025 Task 11: Bridging the Gap in\nText-Based Emotion Detection (Track A), which focuses on multi-label emotion\ndetection in short texts. We propose a feature-centric framework that\ndynamically adapts document representations and learning algorithms to optimize\nlanguage-specific performance. Our study evaluates three key components:\ndocument representation, dimensionality reduction, and model training in 28\nlanguages, highlighting five for detailed analysis. The results show that\nTF-IDF remains highly effective for low-resource languages, while contextual\nembeddings like FastText and transformer-based document representations, such\nas those produced by Sentence-BERT, exhibit language-specific strengths.\nPrincipal Component Analysis (PCA) reduces training time without compromising\nperformance, particularly benefiting FastText and neural models such as\nMulti-Layer Perceptrons (MLP). Computational efficiency analysis underscores\nthe trade-off between model complexity and processing cost. Our framework\nprovides a scalable solution for multilingual emotion detection, addressing the\nchallenges of linguistic diversity and resource constraints.", "AI": {"tldr": "本文提出了一种适应性文档表示和学习算法的框架，用于改善低资源语言中的多标签情感检测。结果表明TF-IDF适用于低资源语言，而复杂的神经模型和降维技术能够提高性能和效率。框架有助于解决多语言情感检测中的挑战。", "motivation": "本文旨在解决SemEval 2025任务11：文本情感检测中的多标签情感检测问题，特别是在短文本中进行情感检测，挑战在于处理语言多样性和资源限制的问题。", "method": "我们提出了一个以特征为中心的框架，该框架动态调整文档表示和学习算法，以优化特定语言的性能。该框架评估了三个关键组件：文档表示、降维和模型训练，并且在28种语言中进行了测试，其中有五种语言进行了详细分析。", "result": "实验结果表明，TF-IDF在低资源语言中仍然非常有效，而像FastText和Sentence-BERT那样的上下文嵌入和基于transformer的文档表示则表现出特定语言的优势。降维方法PCA减少了训练时间而不损害性能，尤其是对于FastText和MLP这样的神经模型。", "conclusion": "我们的框架提供了一个可扩展的多语言情感检测解决方案，能够应对语言多样性和资源限制带来的挑战。"}}
{"id": "2507.08574", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.08574", "abs": "https://arxiv.org/abs/2507.08574", "authors": ["Mingda Zhang", "Kaiwen Pan"], "title": "A Multi-Modal Fusion Framework for Brain Tumor Segmentation Based on 3D Spatial-Language-Vision Integration and Bidirectional Interactive Attention Mechanism", "comment": "12 pages, 4 figures", "summary": "This study aims to develop a novel multi-modal fusion framework for brain\ntumor segmentation that integrates spatial-language-vision information through\nbidirectional interactive attention mechanisms to improve segmentation accuracy\nand boundary delineation. Methods: We propose two core components: Multi-modal\nSemantic Fusion Adapter (MSFA) integrating 3D MRI data with clinical text\ndescriptions through hierarchical semantic decoupling, and Bidirectional\nInteractive Visual-semantic Attention (BIVA) enabling iterative information\nexchange between modalities. The framework was evaluated on BraTS 2020 dataset\ncomprising 369 multi-institutional MRI scans. Results: The proposed method\nachieved average Dice coefficient of 0.8505 and 95% Hausdorff distance of\n2.8256mm across enhancing tumor, tumor core, and whole tumor regions,\noutperforming state-of-the-art methods including SCAU-Net, CA-Net, and 3D\nU-Net. Ablation studies confirmed critical contributions of semantic and\nspatial modules to boundary precision. Conclusion: Multi-modal semantic fusion\ncombined with bidirectional interactive attention significantly enhances brain\ntumor segmentation performance, establishing new paradigms for integrating\nclinical knowledge into medical image analysis.", "AI": {"tldr": "研究提出了一种新的多模态融合框架，通过MSFA和BIVA组件结合3D MRI数据和临床文本描述，利用层次语义解耦和双向互动注意机制提高脑肿瘤分割的性能，优于现有的几种方法。", "motivation": "该研究旨在通过整合空间-语言-视觉信息来开发一种新的多模态融合框架，以提高脑肿瘤分割的准确性和边界划分。", "method": "分析该论文使用了一种多模态融合框架来提高脑肿瘤分割的准确性。框架中的两个核心组件是Multi-modal Semantic Fusion Adapter (MSFA) 和Bidirectional Interactive Visual-semantic Attention (BIVA)，通过层次语义解耦和双向互动注意机制实现了空间、语言和视觉信息的综合。", "result": "该方法在BraTS 2020数据集上表现优秀，平均Dice系数达到0.8505，与现有方法如SCAU-Net, CA-Net,和3D U-Net相比有显著的性能提升。", "conclusion": "结合多模态语义融合和双向互动注意机制大大提升了脑肿瘤分割的表现，开辟了将临床知识融入医学图像分析的新范式。"}}
{"id": "2507.08538", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.08538", "abs": "https://arxiv.org/abs/2507.08538", "authors": ["David Pomerenke", "Jonas Nothnagel", "Simon Ostermann"], "title": "The AI Language Proficiency Monitor -- Tracking the Progress of LLMs on Multilingual Benchmarks", "comment": null, "summary": "To ensure equitable access to the benefits of large language models (LLMs),\nit is essential to evaluate their capabilities across the world's languages. We\nintroduce the AI Language Proficiency Monitor, a comprehensive multilingual\nbenchmark that systematically assesses LLM performance across up to 200\nlanguages, with a particular focus on low-resource languages. Our benchmark\naggregates diverse tasks including translation, question answering, math, and\nreasoning, using datasets such as FLORES+, MMLU, GSM8K, TruthfulQA, and ARC. We\nprovide an open-source, auto-updating leaderboard and dashboard that supports\nresearchers, developers, and policymakers in identifying strengths and gaps in\nmodel performance. In addition to ranking models, the platform offers\ndescriptive insights such as a global proficiency map and trends over time. By\ncomplementing and extending prior multilingual benchmarks, our work aims to\nfoster transparency, inclusivity, and progress in multilingual AI. The system\nis available at\nhttps://huggingface.co/spaces/fair-forward/evals-for-every-language.", "AI": {"tldr": "研究团队介绍了一种名为AI语言能力监测器的多语言基准测试工具，以系统评估大型语言模型在200多种语言中的表现，尤其关注低资源语言，并提供了一个开源、自动更新的排行榜和仪表板。", "motivation": "为了确保大型语言模型的利益在全球各种语言中公平地获取，需要对其能力进行全面的语言评估。", "method": "介绍了一种名为AI语言能力监测器的多语言基准测试，该测试能够系统地评估大型语言模型在多达200种语言的表现，特别是低资源语言。该基准测试涵盖了翻译、问答、数学和推理等多种任务，并使用如FLORES+、MMLU、GSM8K、TruthfulQA和ARC等数据集进行评估。", "result": "提供了一个开源的、自动更新的排行榜和仪表板，以帮助研究人员、开发者和政策制定者发现模型性能的优势和不足。除此之外，该平台还提供了包括全球能力地图和时间趋势等描述性见解。", "conclusion": "通过补充和扩展此前的多语言基准测试，该研究旨在促进多语言AI领域的透明度、包容性和进步。"}}
{"id": "2507.08607", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.08607", "abs": "https://arxiv.org/abs/2507.08607", "authors": ["Shuang Cui", "Jinglin Xu", "Yi Li", "Xiongxin Tang", "Jiangmeng Li", "Jiahuan Zhou", "Fanjiang Xu", "Fuchun Sun", "Hui Xiong"], "title": "BayesTTA: Continual-Temporal Test-Time Adaptation for Vision-Language Models via Gaussian Discriminant Analysis", "comment": null, "summary": "Vision-language models (VLMs) such as CLIP achieve strong zero-shot\nrecognition but degrade significantly under \\textit{temporally evolving\ndistribution shifts} common in real-world scenarios (e.g., gradual illumination\nor seasonal changes). Existing continual test-time adaptation (CTTA) methods\nare typically built around sudden and severe distribution shifts and neglect\ntemporal continuity, leading to three core defects: limited memory cache\nrestricts long-range distribution modeling, causing catastrophic forgetting;\nentropy-based confidence becomes unreliable under temporal drift, worsening\nerror accumulation; and static visual representations misalign with evolving\ninputs. We formalize this practical problem as \\textit{Continual-Temporal\nTest-Time Adaptation (CT-TTA)}, where test distributions evolve gradually over\ntime. To address it, we propose \\textit{BayesTTA}, a Bayesian adaptation\nframework that enforces temporally consistent predictions and dynamically\naligns visual representations. Specifically, BayesTTA incrementally estimates\nclass-conditional Gaussian mixture distributions without storing raw data,\nadaptively selects covariance structures through statistical hypothesis\ntesting, and performs calibrated inference using Gaussian discriminant analysis\n(GDA). These calibrated predictions supervise self-paced adaptation of\nnormalization layers, ensuring efficient and stable representation alignment.\nWe establish a comprehensive CT-TTA benchmark across four temporally evolving\ndatasets and further evaluate generalization on ten standard TTA datasets.\nExtensive experiments show that BayesTTA consistently outperforms\nstate-of-the-art methods, achieving significant gains while maintaining\nefficiency. Code is available at\n\\href{https://github.com/cuishuang99/BayesTTA}{https://github.com/cuishuang99/BayesTTA}.", "AI": {"tldr": "BayesTTA is introduced to effectively manage continual-temporal test-time adaptation for vision-language models facing gradual distribution shifts, improving prediction consistency and visual representation alignment.", "motivation": "To address the degradation of vision-language models under temporally evolving distribution shifts, which are not well-handled by existing continual test-time adaptation methods that are mainly designed for sudden and severe shifts, ignoring temporal continuity.", "method": "BayesTTA, a Bayesian adaptation framework that enforces temporally consistent predictions and dynamically aligns visual representations by incrementally estimating class-conditional Gaussian mixture distributions, adaptively selecting covariance structures through statistical hypothesis testing, and performing calibrated inference using GDA.", "result": "BayesTTA consistently outperforms state-of-the-art methods while maintaining efficiency, as shown by extensive experiments across four temporally evolving datasets and ten standard TTA datasets.", "conclusion": "BayesTTA demonstrates superior performance in managing temporal drift in visual processing compared to existing methods, suitable for real-world scenarios involving gradual changes in input distributions."}}
{"id": "2507.08606", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.08606", "abs": "https://arxiv.org/abs/2507.08606", "authors": ["Benno Uthayasooriyar", "Antoine Ly", "Franck Vermet", "Caio Corro"], "title": "DocPolarBERT: A Pre-trained Model for Document Understanding with Relative Polar Coordinate Encoding of Layout Structures", "comment": null, "summary": "We introduce DocPolarBERT, a layout-aware BERT model for document\nunderstanding that eliminates the need for absolute 2D positional embeddings.\nWe extend self-attention to take into account text block positions in relative\npolar coordinate system rather than the Cartesian one. Despite being\npre-trained on a dataset more than six times smaller than the widely used\nIIT-CDIP corpus, DocPolarBERT achieves state-of-the-art results. These results\ndemonstrate that a carefully designed attention mechanism can compensate for\nreduced pre-training data, offering an efficient and effective alternative for\ndocument understanding.", "AI": {"tldr": "DocPolarBERT 是一种布局感知的 BERT 模型，它使用相对极坐标系统来考虑文本块的位置，而不是传统的笛卡尔坐标系统，尽管预训练数据较少，但仍能达到最先进的结果。", "motivation": "减少对大量预训练数据的依赖，提供一种更有效和高效的文档理解替代方法。", "method": "扩展自注意力机制，采用相对极坐标系统考虑文本块的位置，无需绝对的二维位置嵌入。", "result": "尽管预训练数据集比广泛使用的 IIT-CDIP 数据集小六倍以上，但 DocPolarBERT 仍能达到最先进的结果。", "conclusion": "精心设计的注意力机制可以补偿减少的预训练数据，提供一种更有效和高效的文档理解方案。"}}
{"id": "2507.08636", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.08636", "abs": "https://arxiv.org/abs/2507.08636", "authors": ["Natalia Bottaioli", "Solène Tarride", "Jérémy Anger", "Seginus Mowlavi", "Marina Gardella", "Antoine Tadros", "Gabriele Facciolo", "Rafael Grompone von Gioi", "Christopher Kermorvant", "Jean-Michel Morel", "Javier Preciozzi"], "title": "Normalized vs Diplomatic Annotation: A Case Study of Automatic Information Extraction from Handwritten Uruguayan Birth Certificates", "comment": null, "summary": "This study evaluates the recently proposed Document Attention Network (DAN)\nfor extracting key-value information from Uruguayan birth certificates,\nhandwritten in Spanish. We investigate two annotation strategies for\nautomatically transcribing handwritten documents, fine-tuning DAN with minimal\ntraining data and annotation effort. Experiments were conducted on two datasets\ncontaining the same images (201 scans of birth certificates written by more\nthan 15 different writers) but with different annotation methods. Our findings\nindicate that normalized annotation is more effective for fields that can be\nstandardized, such as dates and places of birth, whereas diplomatic annotation\nperforms much better for fields containing names and surnames, which can not be\nstandardized.", "AI": {"tldr": "研究评估了DAN从乌拉圭手写出生证明中提取信息的能力，比较了两种注释策略的效果。", "motivation": "研究的目标在于探索DAN在处理手写文件提取信息方面的性能，并探究不同注释策略对最终转录质量的影响。", "method": "本研究评估了最近提出的文档注意力网络（DAN）从乌拉圭出生证明中提取键值信息的能力，这些证明是用西班牙语手写而成。研究调查了两种注释策略，以自动转录手写文档，并使用极少量的训练数据和注释努力对DAN进行微调。", "result": "实验在两个包含相同图像（15位以上不同书写者书写的201张出生证明扫描件）但使用不同注释方法的数据集上进行。结果表明，标准化注释对于可以标准化的字段（如出生日期和地点）更有效，而外交式注释对于不能标准化的名称和姓氏字段表现更好。", "conclusion": "本研究表明，不同的注释策略针对不同类型的信息能够产生不同的结果，标准化注释适合标准化字段，而外交式注释则更适合处理不可标准化的信息。"}}
{"id": "2507.08621", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.08621", "abs": "https://arxiv.org/abs/2507.08621", "authors": ["Marcin Pietroń", "Rafał Olszowski", "Jakub Gomułka", "Filip Gampel", "Andrzej Tomski"], "title": "A comprehensive study of LLM-based argument classification: from LLAMA through GPT-4o to Deepseek-R1", "comment": null, "summary": "Argument mining (AM) is an interdisciplinary research field that integrates\ninsights from logic, philosophy, linguistics, rhetoric, law, psychology, and\ncomputer science. It involves the automatic identification and extraction of\nargumentative components, such as premises and claims, and the detection of\nrelationships between them, such as support, attack, or neutrality. Recently,\nthe field has advanced significantly, especially with the advent of large\nlanguage models (LLMs), which have enhanced the efficiency of analyzing and\nextracting argument semantics compared to traditional methods and other deep\nlearning models. There are many benchmarks for testing and verifying the\nquality of LLM, but there is still a lack of research and results on the\noperation of these models in publicly available argument classification\ndatabases. This paper presents a study of a selection of LLM's, using diverse\ndatasets such as Args.me and UKP. The models tested include versions of GPT,\nLlama, and DeepSeek, along with reasoning-enhanced variants incorporating the\nChain-of-Thoughts algorithm. The results indicate that ChatGPT-4o outperforms\nthe others in the argument classification benchmarks. In case of models\nincorporated with reasoning capabilities, the Deepseek-R1 shows its\nsuperiority. However, despite their superiority, GPT-4o and Deepseek-R1 still\nmake errors. The most common errors are discussed for all models. To our\nknowledge, the presented work is the first broader analysis of the mentioned\ndatasets using LLM and prompt algorithms. The work also shows some weaknesses\nof known prompt algorithms in argument analysis, while indicating directions\nfor their improvement. The added value of the work is the in-depth analysis of\nthe available argument datasets and the demonstration of their shortcomings.", "AI": {"tldr": "本论文研究了几种大规模语言模型在论点分类上的表现，发现ChatGPT-4o和Deepseek-R1表现最佳，但仍然存在错误。这项工作是首次使用这些数据集对LLM和提示算法进行的广泛分析。", "motivation": "尽管有各种基准测试和验证大规模语言模型(如LLM)质量的测试，但在公开可用的论点分类数据库中对其操作的研究仍然不足。该研究旨在填补这一空白。", "method": "本研究使用了包括Args.me和UKP在内的多样化数据集，测试了GPT、Llama、DeepSeek以及结合了Chain-of-Thoughts算法的推理增强变体模型。", "result": "结果显示，ChatGPT-4o在论点分类基准测试中表现最佳。对于结合推理能力的模型，Deepseek-R1表现突出。然而，即使是表现最好的模型仍会犯错，最常见的错误类型也被讨论了。", "conclusion": "研究展示了现有提示算法在论点分析中的弱点，并指出了未来改进的方向。同时，对当前可用的论点数据集进行了深入分析，指出了它们的不足之处。"}}
{"id": "2507.08644", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.08644", "abs": "https://arxiv.org/abs/2507.08644", "authors": ["Junho Koh", "Youngwoo Lee", "Jungho Kim", "Dongyoung Lee", "Jun Won Choi"], "title": "OnlineBEV: Recurrent Temporal Fusion in Bird's Eye View Representations for Multi-Camera 3D Perception", "comment": "Accepted to Transactions on Intelligent Transportation Systems", "summary": "Multi-view camera-based 3D perception can be conducted using bird's eye view\n(BEV) features obtained through perspective view-to-BEV transformations.\nSeveral studies have shown that the performance of these 3D perception methods\ncan be further enhanced by combining sequential BEV features obtained from\nmultiple camera frames. However, even after compensating for the ego-motion of\nan autonomous agent, the performance gain from temporal aggregation is limited\nwhen combining a large number of image frames. This limitation arises due to\ndynamic changes in BEV features over time caused by object motion. In this\npaper, we introduce a novel temporal 3D perception method called OnlineBEV,\nwhich combines BEV features over time using a recurrent structure. This\nstructure increases the effective number of combined features with minimal\nmemory usage. However, it is critical to spatially align the features over time\nto maintain strong performance. OnlineBEV employs the Motion-guided BEV Fusion\nNetwork (MBFNet) to achieve temporal feature alignment. MBFNet extracts motion\nfeatures from consecutive BEV frames and dynamically aligns historical BEV\nfeatures with current ones using these motion features. To enforce temporal\nfeature alignment explicitly, we use Temporal Consistency Learning Loss, which\ncaptures discrepancies between historical and target BEV features. Experiments\nconducted on the nuScenes benchmark demonstrate that OnlineBEV achieves\nsignificant performance gains over the current best method, SOLOFusion.\nOnlineBEV achieves 63.9% NDS on the nuScenes test set, recording\nstate-of-the-art performance in the camera-only 3D object detection task.", "AI": {"tldr": "本论文提出了一种新的时序3D感知方法OnlineBEV，使用递归结构增加BEV特征的有效结合数量，同时最小化内存使用，以解决传统方法在结合多个帧进行3D感知时性能提升受限的问题。", "motivation": "虽然通过视角变换获得BEV特征用于三维感知被证明有效，但直接结合多个相机帧图像BEV特征的性能增益有限，特别是当考虑很大数量的帧时。这主要由于动态的变化BEV特征随时间推移由物体运动引起。因此，提出了一种新颖跨时间3D感知方法。", "method": "OnlineBEV通过使用递归结构结合跨时间的BEV特征，最小化内存使用增加有效组合特征的数量。Motion-guided BEV Fusion Network（MBFNet）被用来实现时间特征对齐，通过从连续的BEV帧中提取运动特征来动态对齐历史和当前的BEV特征。此外，通过时间一致性学习损失强制执行时间特征对齐。", "result": "在线进行的实验结果在nuScenes基准测试上表明，相较于目前的最佳方法SOLOFusion，新提出的方法OnlineBEV在仅使用摄像头的3D物体检测任务上实现了显著的性能提升，达到了63.9%的NDS。", "conclusion": "结合时空一致性学习损失，论文提出的方法能够显著提高3D物体检测性能，证明其在自动驾驶场景中使用RGB相机进行3D感知的有效性和优越性。"}}
{"id": "2507.08660", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.08660", "abs": "https://arxiv.org/abs/2507.08660", "authors": ["Cristina Aggazzotti", "Matthew Wiesner", "Elizabeth Allyn Smith", "Nicholas Andrews"], "title": "The Impact of Automatic Speech Transcription on Speaker Attribution", "comment": null, "summary": "Speaker attribution from speech transcripts is the task of identifying a\nspeaker from the transcript of their speech based on patterns in their language\nuse. This task is especially useful when the audio is unavailable (e.g.\ndeleted) or unreliable (e.g. anonymized speech). Prior work in this area has\nprimarily focused on the feasibility of attributing speakers using transcripts\nproduced by human annotators. However, in real-world settings, one often only\nhas more errorful transcripts produced by automatic speech recognition (ASR)\nsystems. In this paper, we conduct what is, to our knowledge, the first\ncomprehensive study of the impact of automatic transcription on speaker\nattribution performance. In particular, we study the extent to which speaker\nattribution performance degrades in the face of transcription errors, as well\nas how properties of the ASR system impact attribution. We find that\nattribution is surprisingly resilient to word-level transcription errors and\nthat the objective of recovering the true transcript is minimally correlated\nwith attribution performance. Overall, our findings suggest that speaker\nattribution on more errorful transcripts produced by ASR is as good, if not\nbetter, than attribution based on human-transcribed data, possibly because ASR\ntranscription errors can capture speaker-specific features revealing of speaker\nidentity.", "AI": {"tldr": "该研究首次全面分析了自动语音识别系统产生的转录错误对说话人归因任务的影响。结果显示，说话人归因能够相对较好地应对这些错误，且性能有时优于基于人类转录的数据。", "motivation": "之前的大部分研究都集中于使用人类注释员生产的转录文本进行说话人归因的可行性上。然而，实际环境中往往只有由自动语音识别系统生成的更具有错误的转录文本。本研究着重解决这一现实问题，首次全面探讨了自动转录对说话人归因性能的影响。", "method": "研究主要探讨了语音识别系统（ASR）产生的自动转录对说话人归因性能的影响。具体来说，通过分析转录错误的程度对说话人归因准确性的影响，以及ASR系统的不同属性如何影响归因性能，该研究评估了自动转录的说话人归因能力。", "result": "研究发现，说话人归因对词级转录错误的耐受性较高，并且恢复真实转录的目标与归因性能之间的相关性较小。总的来说，基于ASR生成的含错转录文本进行的说话人归因，其性能与基于人工转录数据的归因性能相当甚至更好。这可能是因为ASR转录错误捕获了能够揭示发言人身份的特定特征。", "conclusion": "该研究得出了转录错误对说话人归因的影响较小的结论，同时指出了ASR系统的转录错误可能意外地捕捉到了一些有利于识别说话人身份的信息，这对于未来的说话人归因任务具有重要意义。"}}
{"id": "2507.08648", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.08648", "abs": "https://arxiv.org/abs/2507.08648", "authors": ["Haoran Sun", "Haoyu Bian", "Shaoning Zeng", "Yunbo Rao", "Xu Xu", "Lin Mei", "Jianping Gou"], "title": "DatasetAgent: A Novel Multi-Agent System for Auto-Constructing Datasets from Real-World Images", "comment": null, "summary": "Common knowledge indicates that the process of constructing image datasets\nusually depends on the time-intensive and inefficient method of manual\ncollection and annotation. Large models offer a solution via data generation.\nNonetheless, real-world data are obviously more valuable comparing to\nartificially intelligence generated data, particularly in constructing image\ndatasets. For this reason, we propose a novel method for auto-constructing\ndatasets from real-world images by a multiagent collaborative system, named as\nDatasetAgent. By coordinating four different agents equipped with Multi-modal\nLarge Language Models (MLLMs), as well as a tool package for image\noptimization, DatasetAgent is able to construct high-quality image datasets\naccording to user-specified requirements. In particular, two types of\nexperiments are conducted, including expanding existing datasets and creating\nnew ones from scratch, on a variety of open-source datasets. In both cases,\nmultiple image datasets constructed by DatasetAgent are used to train various\nvision models for image classification, object detection, and image\nsegmentation.", "AI": {"tldr": "The paper introduces DatasetAgent, a multiagent collaborative system that uses real-world images to create high-quality image datasets efficiently.", "motivation": "The motivation is to solve the issues of time-intensiveness and inefficiency associated with manual data collection and annotation, while also leveraging the value of real-world data over AI-generated data.", "method": "The method involves a multiagent collaborative system named DatasetAgent, which uses different agents equipped with Multi-modal Large Language Models (MLLMs) and a tool package to optimize and construct datasets from real-world images.", "result": "The method was tested in two types of experiments: expanding existing datasets and creating new ones, resulting in high-quality datasets that were used to train vision models for tasks like image classification, object detection, and image segmentation.", "conclusion": "DatasetAgent proves effective in auto-constructing image datasets from real-world images, showcasing the potential to streamline the process of dataset creation for various vision tasks."}}
{"id": "2507.08665", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.08665", "abs": "https://arxiv.org/abs/2507.08665", "authors": ["Jiyao Zhang", "Chengli Zhong", "Hui Xu", "Qige Li", "Yi Zhou"], "title": "KELPS: A Framework for Verified Multi-Language Autoformalization via Semantic-Syntactic Alignment", "comment": "Accepted by the ICML 2025 AI4MATH Workshop. 22 pages, 16 figures, 2\n  tables", "summary": "Modern large language models (LLMs) show promising progress in formalizing\ninformal mathematics into machine-verifiable theorems. However, these methods\nstill face bottlenecks due to the limited quantity and quality of multilingual\nparallel corpora. In this paper, we propose a novel neuro-symbolic framework\nKELPS (Knowledge-Equation based Logical Processing System) to address these\nproblems. KELPS is an iterative framework for translating, synthesizing, and\nfiltering informal data into multiple formal languages (Lean, Coq, and\nIsabelle). First, we translate natural language into Knowledge Equations (KEs),\na novel language that we designed, theoretically grounded in assertional logic.\nNext, we convert them to target languages through rigorously defined rules that\npreserve both syntactic structure and semantic meaning. This process yielded a\nparallel corpus of over 60,000 problems. Our framework achieves 88.9% syntactic\naccuracy (pass@1) on MiniF2F, outperforming SOTA models such as Deepseek-V3\n(81%) and Herald (81.3%) across multiple datasets. All datasets and codes are\navailable in the supplementary materials.", "AI": {"tldr": "该论文展示了KELPS框架在构建平行语料库方面的可能性及其在多个数据集上超过现有最优模型的性能。", "motivation": "尽管现代大型语言模型（LLMs）在将非正式数学形式化为机器可验证定理方面取得了显著进展，但仍面临平行语料库数量和质量有限的问题。本研究旨在通过提出KELPS来解决这些问题。", "method": "本研究提出了一种新的神经符号框架KELPS来解决将非正式数学形式化成机器可验证定理中面临的数据质量和数量限制问题。KELPS框架是一个迭代框架，可以将非正式数据翻译、合成并过滤到多种正式语言（如Lean、Coq和Isabelle）中。首先，将自然语言翻译成知识方程（KEs），这是一种基于断言逻辑设计的新语言。然后，通过严格的定义规则，将KEs转换为目标语言，这些规则既保留了句法结构，也保留了语义意义。", "result": "此框架生成了一个包含超60,000个问题的平行语料库。在MiniF2F上的句法准确率达到了88.9%（pass@1），优于诸如Deepseek-V3（81%）和Herald（81.3%）等现有最优模型。", "conclusion": "KELPS框架展示了一种有效的方法来构建高质量的平行语料库，为进一步提高非正式数学向正式语言翻译的质量和效率奠定了基础。"}}
{"id": "2507.08655", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.08655", "abs": "https://arxiv.org/abs/2507.08655", "authors": ["Zach Eidex", "Mojtaba Safari", "Tonghe Wang", "Vanessa Wildman", "David S. Yu", "Hui Mao", "Erik Middlebrooks", "Aparna Kesewala", "Xiaofeng Yang"], "title": "Generalizable 7T T1-map Synthesis from 1.5T and 3T T1 MRI with an Efficient Transformer Model", "comment": null, "summary": "Purpose: Ultra-high-field 7T MRI offers improved resolution and contrast over\nstandard clinical field strengths (1.5T, 3T). However, 7T scanners are costly,\nscarce, and introduce additional challenges such as susceptibility artifacts.\nWe propose an efficient transformer-based model (7T-Restormer) to synthesize\n7T-quality T1-maps from routine 1.5T or 3T T1-weighted (T1W) images. Methods:\nOur model was validated on 35 1.5T and 108 3T T1w MRI paired with corresponding\n7T T1 maps of patients with confirmed MS. A total of 141 patient cases (32,128\nslices) were randomly divided into 105 (25; 80) training cases (19,204 slices),\n19 (5; 14) validation cases (3,476 slices), and 17 (5; 14) test cases (3,145\nslices) where (X; Y) denotes the patients with 1.5T and 3T T1W scans,\nrespectively. The synthetic 7T T1 maps were compared against the ResViT and\nResShift models. Results: The 7T-Restormer model achieved a PSNR of 26.0 +/-\n4.6 dB, SSIM of 0.861 +/- 0.072, and NMSE of 0.019 +/- 0.011 for 1.5T inputs,\nand 25.9 +/- 4.9 dB, and 0.866 +/- 0.077 for 3T inputs, respectively. Using\n10.5 M parameters, our model reduced NMSE by 64 % relative to 56.7M parameter\nResShift (0.019 vs 0.052, p = <.001 and by 41 % relative to 70.4M parameter\nResViT (0.019 vs 0.032, p = <.001) at 1.5T, with similar advantages at 3T\n(0.021 vs 0.060 and 0.033; p < .001). Training with a mixed 1.5 T + 3 T corpus\nwas superior to single-field strategies. Restricting the model to 1.5T\nincreased the 1.5T NMSE from 0.019 to 0.021 (p = 1.1E-3) while training solely\non 3T resulted in lower performance on input 1.5T T1W MRI. Conclusion: We\npropose a novel method for predicting quantitative 7T MP2RAGE maps from 1.5T\nand 3T T1W scans with higher quality than existing state-of-the-art methods.\nOur approach makes the benefits of 7T MRI more accessible to standard clinical\nworkflows.", "AI": {"tldr": "该研究提出一种名为7T-Restormer的模型，可通过1.5T或3T T1W图像合成7T质量的T1图，结果显示该模型在较少参数的情况下优于现有方法，极大提高了7T MRI在临床中的应用。", "motivation": "动机：7T MRI具有更高的分辨率和对比度，但成本高昂且稀有，因此提出了一种有效的方法来从常规MRI中模拟出7T质量的T1图。", "method": "Purpose: 使用7T-Restormer模型从常规的1.5T或3T T1加权(MRI)图像中合成7T质量的T1图。方法：在35名1.5T和108名3T T1W MRI以及对应的7T T1图中进行验证，共有141个病人（32,128个切片）。模型与ResViT和ResShift模型进行比较。", "result": "结果：7T-Restormer模型在1.5T输入时PSNR为26.0 +/- 4.6 dB、SSIM为0.861 +/- 0.072、NMSE为0.019 +/- 0.011，在3T输入时PSNR为25.9 +/- 4.9 dB、SSIM为0.866 +/- 0.077，模型在参数使用更少的情况下提升了性能。", "conclusion": "结论：提出了一种从1.5T和3T T1W扫描中预测定量7T MP2RAGE图的新方法，质量高于现有最先进方法，使7T MRI的优势更易于应用于标准临床工作流程。"}}
{"id": "2507.08704", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.08704", "abs": "https://arxiv.org/abs/2507.08704", "authors": ["Songlin Zhai", "Guilin Qi", "Yuan Meng"], "title": "KG-Attention: Knowledge Graph-Guided Attention at Test-Time via Bidirectional Information Aggregation", "comment": null, "summary": "Knowledge graphs (KGs) play a critical role in enhancing large language\nmodels (LLMs) by introducing structured and grounded knowledge into the\nlearning process. However, most existing KG-enhanced approaches rely on\nparameter-intensive fine-tuning, which risks catastrophic forgetting and\ndegrades the pretrained model's generalization. Moreover, they exhibit limited\nadaptability to real-time knowledge updates due to their static integration\nframeworks. To address these issues, we introduce the first test-time\nKG-augmented framework for LLMs, built around a dedicated knowledge\ngraph-guided attention (KGA) module that enables dynamic knowledge fusion\nwithout any parameter updates. The proposed KGA module augments the standard\nself-attention mechanism with two synergistic pathways: outward and inward\naggregation. Specifically, the outward pathway dynamically integrates external\nknowledge into input representations via input-driven KG fusion. This inward\naggregation complements the outward pathway by refining input representations\nthrough KG-guided filtering, suppressing task-irrelevant signals and amplifying\nknowledge-relevant patterns. Importantly, while the outward pathway handles\nknowledge fusion, the inward path selects the most relevant triples and feeds\nthem back into the fusion process, forming a closed-loop enhancement mechanism.\nBy synergistically combining these two pathways, the proposed method supports\nreal-time knowledge fusion exclusively at test-time, without any parameter\nmodification. Extensive experiments on five benchmarks verify the comparable\nknowledge fusion performance of KGA.", "AI": {"tldr": "A new test-time knowledge graph (KG)-augmented framework is introduced to enhance large language models (LLMs) with structural knowledge, featuring dynamic and parameterless knowledge fusion through a knowledge graph-guided attention (KGA) module.", "motivation": "The motivation stems from the limitations of existing KG-enhanced approaches, which include the risk of catastrophic forgetting, over-reliance on parameter-heavy fine-tuning, and poor adaptability to real-time knowledge updates.", "method": "The paper proposes a test-time KG-augmented framework for LLMs, centered around a knowledge graph-guided attention (KGA) module. This module dynamically fuses external knowledge with minimal system changes, using outward and inward aggregation pathways to refine and enhance the input representations.", "result": "Experiments on five benchmarks showcase the effectiveness and comparability of the proposed knowledge fusion approach, demonstrating real-time knowledge fusion capabilities without requiring parameter modifications.", "conclusion": "The study introduces a groundbreaking test-time knowledge integration technique for LLMs, which promises more dynamic and efficient knowledge application in both static and evolving knowledge environments, maintaining the model's generalization and adaptability."}}
{"id": "2507.08679", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.08679", "abs": "https://arxiv.org/abs/2507.08679", "authors": ["Rajarshi Roy", "Devleena Das", "Ankesh Banerjee", "Arjya Bhattacharjee", "Kousik Dasgupta", "Subarna Tripathi"], "title": "ByDeWay: Boost Your multimodal LLM with DEpth prompting in a Training-Free Way", "comment": null, "summary": "We introduce ByDeWay, a training-free framework designed to enhance the\nperformance of Multimodal Large Language Models (MLLMs). ByDeWay uses a novel\nprompting strategy called Layered-Depth-Based Prompting (LDP), which improves\nspatial reasoning and grounding without modifying any model parameters. It\nsegments the scene into closest, mid-range, and farthest layers using monocular\ndepth estimation, then generates region-specific captions with a grounded\nvision-language model. These structured, depth-aware captions are appended to\nthe image-question prompt, enriching it with spatial context. This guides MLLMs\nto produce more grounded and less hallucinated responses. Our method is\nlightweight, modular, and compatible with black-box MLLMs. Experiments on\nhallucination-sensitive (POPE) and reasoning-intensive (GQA) benchmarks show\nconsistent improvements across multiple MLLMs, validating the effectiveness of\ndepth-aware prompting in a zero-training setting.", "AI": {"tldr": "ByDeWay是一种无训练框架，利用深度感知提示策略（LDP）提高多模态大语言模型的空间推理和定位能力，而不改变模型参数。", "motivation": "通过开发无需训练的新策略提高多模态语言模型的性能，并增强其在空间理解和事实性上的表现。", "method": "ByDeWay采用了一种新颖的提示策略（LDP）以改善多模态大语言模型的空间推理和定位。这种策略利用单目深度估计技术将场景分割成最近、中程和最远三个层次，并使用基于grounded的视觉-语言模型生成特定于这些区域的描述。这些具有深度感知的结构化描述被添加到图片-问题提示中，以提供空间背景，引导MLLM生成更加基于事实的回应。", "result": "实验结果显示，该方法在抑制幻象和加强推理方面有效，适用于多项基准测试和多种MLLM模型。", "conclusion": "ByDeWay展现了深度感知提示策略在提升MLLM性能方面的能力，且无训练需求、轻量且模块化，适用于多种黑盒模型。"}}
{"id": "2507.08719", "categories": ["cs.CL", "cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2507.08719", "abs": "https://arxiv.org/abs/2507.08719", "authors": ["Linzheng Chai", "Jian Yang", "Shukai Liu", "Wei Zhang", "Liran Wang", "Ke Jin", "Tao Sun", "Congnan Liu", "Chenchen Zhang", "Hualei Zhu", "Jiaheng Liu", "Xianjie Wu", "Ge Zhang", "Tianyu Liu", "Zhoujun Li"], "title": "Multilingual Multimodal Software Developer for Code Generation", "comment": "Preprint", "summary": "The rapid advancement of Large Language Models (LLMs) has significantly\nimproved code generation, yet most models remain text-only, neglecting crucial\nvisual aids like diagrams and flowcharts used in real-world software\ndevelopment. To bridge this gap, we introduce MM-Coder, a Multilingual\nMultimodal software developer. MM-Coder integrates visual design inputs-Unified\nModeling Language (UML) diagrams and flowcharts (termed Visual Workflow)-with\ntextual instructions to enhance code generation accuracy and architectural\nalignment. To enable this, we developed MMc-Instruct, a diverse multimodal\ninstruction-tuning dataset including visual-workflow-based code generation,\nallowing MM-Coder to synthesize textual and graphical information like human\ndevelopers, distinct from prior work on narrow tasks. Furthermore, we introduce\nMMEval, a new benchmark for evaluating multimodal code generation, addressing\nexisting text-only limitations. Our evaluations using MMEval highlight\nsignificant remaining challenges for models in precise visual information\ncapture, instruction following, and advanced programming knowledge. Our work\naims to revolutionize industrial programming by enabling LLMs to interpret and\nimplement complex specifications conveyed through both text and visual designs.", "AI": {"tldr": "本文介绍了一种新的多模态软件开发工具MM-Coder，通过整合视觉设计（如UML图和流程图）和文本指令，以解决当前大多数代码生成模型仅文本处理的问题。", "motivation": "尽管大型语言模型在代码生成方面取得了显著进步，但大多数模型仍局限于文本处理，忽略了现实世界中软件开发中使用的关键视觉辅助。这项工作旨在通过整合视觉辅助和文本信息来改进代码生成。", "method": "MM-Coder整合了视觉设计输入（如UML图和流程图）与文本指令，以提高代码生成的准确性和架构一致性。为此，开发了MMc-Instruct，这是一个包括基于工作流程图的代码生成的多样化多模态指令微调数据集。", "result": "通过使用MMEval，该研究揭示了模型在捕捉精准视觉信息、遵循指令以及运用高级编程知识方面仍存在挑战。", "conclusion": "该工作旨在通过让LLMs解释和实现通过文本和视觉设计传达的复杂规范，从而变革工业编程。"}}
{"id": "2507.08683", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.08683", "abs": "https://arxiv.org/abs/2507.08683", "authors": ["Debashis Gupta", "Aditi Golder", "Rongkhun Zhu", "Kangning Cui", "Wei Tang", "Fan Yang", "Ovidiu Csillik", "Sarra Alaqahtani", "V. Paul Pauca"], "title": "MoSAiC: Multi-Modal Multi-Label Supervision-Aware Contrastive Learning for Remote Sensing", "comment": null, "summary": "Contrastive learning (CL) has emerged as a powerful paradigm for learning\ntransferable representations without the reliance on large labeled datasets.\nIts ability to capture intrinsic similarities and differences among data\nsamples has led to state-of-the-art results in computer vision tasks. These\nstrengths make CL particularly well-suited for Earth System Observation (ESO),\nwhere diverse satellite modalities such as optical and SAR imagery offer\nnaturally aligned views of the same geospatial regions. However, ESO presents\nunique challenges, including high inter-class similarity, scene clutter, and\nambiguous boundaries, which complicate representation learning -- especially in\nlow-label, multi-label settings. Existing CL frameworks often focus on\nintra-modality self-supervision or lack mechanisms for multi-label alignment\nand semantic precision across modalities. In this work, we introduce MoSAiC, a\nunified framework that jointly optimizes intra- and inter-modality contrastive\nlearning with a multi-label supervised contrastive loss. Designed specifically\nfor multi-modal satellite imagery, MoSAiC enables finer semantic\ndisentanglement and more robust representation learning across spectrally\nsimilar and spatially complex classes. Experiments on two benchmark datasets,\nBigEarthNet V2.0 and Sent12MS, show that MoSAiC consistently outperforms both\nfully supervised and self-supervised baselines in terms of accuracy, cluster\ncoherence, and generalization in low-label and high-class-overlap scenarios.", "AI": {"tldr": "The paper presents MoSAiC, a new framework for contrastive learning in Earth System Observation that improves semantic disentanglement and robust representation learning in multi-modal satellite imagery, outperforming existing methods on two datasets.", "motivation": "To address the unique challenges in Earth System Observation (ESO) that include high inter-class similarity, scene clutter, and ambiguous boundaries in low-label, multi-label settings. Existing frameworks either focus on intra-modality self-supervision or lack mechanisms for multi-label alignment and semantic precision across modalities.", "method": "MoSAiC, a unified framework that jointly optimizes intra- and inter-modality contrastive learning with a multi-label supervised contrastive loss, designed for multi-modal satellite imagery.", "result": "Experiments on BigEarthNet V2.0 and Sent12MS datasets show MoSAiC outperforms fully supervised and self-supervised baselines in accuracy, cluster coherence, and generalization in low-label and high-class-overlap scenarios.", "conclusion": "MoSAiC improves representation learning in multi-modal satellite imagery by enabling finer semantic disentanglement and more robust learning in spectrally similar and spatially complex classes."}}
{"id": "2507.08799", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.08799", "abs": "https://arxiv.org/abs/2507.08799", "authors": ["Max Belitsky", "Dawid J. Kopiczko", "Michael Dorkenwald", "M. Jehanzeb Mirza", "Cees G. M. Snoek", "Yuki M. Asano"], "title": "KV Cache Steering for Inducing Reasoning in Small Language Models", "comment": null, "summary": "We propose cache steering, a lightweight method for implicit steering of\nlanguage models via a one-shot intervention applied directly to the key-value\ncache. To validate its effectiveness, we apply cache steering to induce\nchain-of-thought reasoning in small language models. Our approach leverages\nGPT-4o-generated reasoning traces to construct steering vectors that shift\nmodel behavior toward more explicit, multi-step reasoning without fine-tuning\nor prompt modifications. Experimental evaluations on diverse reasoning\nbenchmarks demonstrate that cache steering improves both the qualitative\nstructure of model reasoning and quantitative task performance. Compared to\nprior activation steering techniques that require continuous interventions, our\none-shot cache steering offers substantial advantages in terms of\nhyperparameter stability, inference-time efficiency, and ease of integration,\nmaking it a more robust and practical solution for controlled generation.", "AI": {"tldr": "本文提出了一种新的方法——缓存引导，其通过一次性的干预操作直接作用于模型的键值缓存。这种方法不仅可以引导小型语言模型实现链式思维推理，还较之前的方法更为稳定、高效和易于集成。", "motivation": "研究动机是为了验证缓存引导方法的有效性，特别是通过将其应用于引导小型语言模型进行链式思维推理。", "method": "我们提出了缓存引导（cache steering），这是一种轻量级的方法，用于通过一次干预直接对键值缓存进行隐式引导。通过利用GPT-4生成的推理轨迹来构建引导向量，使模型行为更偏向于明确的多步骤推理，而不需微调或提示修改。", "result": "实验评估表明，缓存引导在多样化的推理基准测试中改善了模型推理的定性结构和定量任务表现。", "conclusion": "相比于之前需要连续干预的激活引导技术，我们的缓存引导方法在超参数稳定性、推理时间效率和集成易用性方面具有显著优势，提供了一个更强大和实用的生成控制解决方案。"}}
{"id": "2507.08690", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.08690", "abs": "https://arxiv.org/abs/2507.08690", "authors": ["Mengyuan Liu", "Jeongkyu Lee"], "title": "An Efficient Approach for Muscle Segmentation and 3D Reconstruction Using Keypoint Tracking in MRI Scan", "comment": null, "summary": "Magnetic resonance imaging (MRI) enables non-invasive, high-resolution\nanalysis of muscle structures. However, automated segmentation remains limited\nby high computational costs, reliance on large training datasets, and reduced\naccuracy in segmenting smaller muscles. Convolutional neural network\n(CNN)-based methods, while powerful, often suffer from substantial\ncomputational overhead, limited generalizability, and poor interpretability\nacross diverse populations. This study proposes a training-free segmentation\napproach based on keypoint tracking, which integrates keypoint selection with\nLucas-Kanade optical flow. The proposed method achieves a mean Dice similarity\ncoefficient (DSC) ranging from 0.6 to 0.7, depending on the keypoint selection\nstrategy, performing comparably to state-of-the-art CNN-based models while\nsubstantially reducing computational demands and enhancing interpretability.\nThis scalable framework presents a robust and explainable alternative for\nmuscle segmentation in clinical and research applications.", "AI": {"tldr": "本文提出了一种新的无训练肌肉分割方法，采用了关键点跟踪，不仅在计算效率上取得了优势，而且在可解释性和准确性上也与先进的方法相当，特别适合应用于临床和研究中。", "motivation": "磁共振成像(MRI)能够进行非侵入性、高分辨率的肌肉结构分析，但是自动分割技术受限于高计算成本、对大数据集的依赖以及对较小肌肉的分割准确性较低。这种方法旨在解决基于CNN的自动分割在计算成本、通用性和可解释性上的问题。", "method": "本研究提出了一种基于关键点跟踪的无训练分割方法，该方法结合了关键点选择与Lucas-Kanade光流法。", "result": "提出的方法达到了0.6到0.7的平均Dice相似系数，具体取决于关键点选择策略，与最先进的基于CNN的模型性能相当，但显著减少了计算需求并增强了可解释性。", "conclusion": "该可扩展框架提供了肌肉分割的强健且可解释的替代方案，适用于临床和研究应用。"}}
{"id": "2507.08800", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.08800", "abs": "https://arxiv.org/abs/2507.08800", "authors": ["Luke Rivard", "Sun Sun", "Hongyu Guo", "Wenhu Chen", "Yuntian Deng"], "title": "NeuralOS: Towards Simulating Operating Systems via Neural Generative Models", "comment": null, "summary": "We introduce NeuralOS, a neural framework that simulates graphical user\ninterfaces (GUIs) of operating systems by directly predicting screen frames in\nresponse to user inputs such as mouse movements, clicks, and keyboard events.\nNeuralOS combines a recurrent neural network (RNN), which tracks computer\nstate, with a diffusion-based neural renderer that generates screen images. The\nmodel is trained on a large-scale dataset of Ubuntu XFCE recordings, which\ninclude both randomly generated interactions and realistic interactions\nproduced by AI agents. Experiments show that NeuralOS successfully renders\nrealistic GUI sequences, accurately captures mouse interactions, and reliably\npredicts state transitions like application launches. Although modeling\nfine-grained keyboard interactions precisely remains challenging, NeuralOS\noffers a step toward creating fully adaptive, generative neural interfaces for\nfuture human-computer interaction systems.", "AI": {"tldr": "NeuralOS 是一种神经框架，用于模拟操作系统中的图形用户界面，通过预测屏幕帧对用户输入进行响应。", "motivation": "为了模拟操作系统中的图形用户界面（GUI），通过直接预测屏幕帧来回应用户的输入，如鼠标移动、点击和键盘事件。", "method": "NeuralOS 结合了循环神经网络（RNN）跟踪计算机状态，以及基于扩散的神经渲染器生成屏幕图像。", "result": "实验表明，NeuralOS 成功生成了真实的GUI序列，准确捕捉了鼠标交互，并可靠预测了状态转换如应用启动。", "conclusion": "尽管精细的键盘交互仍然具有挑战性，NeuralOS 为未来的人机交互系统提供了完全适应性、生成性神经接口的一步。"}}
{"id": "2507.08710", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.08710", "abs": "https://arxiv.org/abs/2507.08710", "authors": ["Li Li", "Yingzhe Peng", "Xu Yang", "Ruoxi Cheng", "Haiyang Xu", "Ming Yan", "Fei Huang"], "title": "L-CLIPScore: a Lightweight Embedding-based Captioning Metric for Evaluating and Training", "comment": "10 pages, 4 figures", "summary": "We propose a novel embedding-based captioning metric termed as L-CLIPScore\nthat can be used for efficiently evaluating caption quality and training\ncaptioning model. L-CLIPScore is calculated from a lightweight CLIP (L-CLIP),\nwhich is a dual-encoder architecture compressed and distilled from CLIP. To\ncompress, we apply two powerful techniques which are weight multiplexing and\nmatrix decomposition for reducing the parameters of encoders and word embedding\nmatrix, respectively. To distill, we design a novel multi-modal Similarity\nRegulator (SR) loss to transfer more vision-language alignment knowledge.\nSpecifically, SR loss amplifies the multi-modal embedding similarity if the\ngiven image-text pair is matched and diminishes the similarity if the pair is\nnon-matched. By compressing and distilling by this novel SR loss, our L-CLIP\nachieves comparable multi-modal alignment ability to the original CLIP while it\nrequires fewer computation resources and running time. We carry out exhaustive\nexperiments to validate the efficiency and effectiveness of L-CLIPScore when\nusing it as the judge to evaluate caption quality. We also discover that when\nusing L-CLIPScore as the supervisor to train the captioning model, it should be\nmixed up by an n-gram-based metric and meanwhile analyze why using L-CLIPScore\nonly will cause fail training.", "AI": {"tldr": "介绍了新型基于嵌入的字幕度量指标L-CLIPScore，该指标能够用来高效评估字幕质量和优化字幕生成模型的训练。", "motivation": "通过开发一种更高效的字幕生成度量方法来改进当前的评估和训练过程。", "method": "Structure", "result": "{\n  \"tldr\": \"研究人员提出了一种基于嵌入的字幕度量指标L-CLIPScore，用于高效评估字幕质量和训练字幕生成模型。\", \n  \"motivation\": \"开发一个更高效的字幕度量方法以改进评估和训练过程。\", \n  \"method\": \"L-CLIPScore基于轻量级CLIP（L-CLIP），一种双编码器架构，通过权重多路复用和矩阵分解技术压缩原始CLIP的参数。为了提取知识，研究人员设计了新型多模态相似性调节损失函数（SR）。\", \n  \"result\": \"通过压缩和使用SR损失函数训练，L-CLIP实现了与原始CLIP相似的多模态对齐能力，但需要更少的计算资源和运行时间。实验验证了L-CLIPScore的高效性和有效性。\", \n  \"conclusion\": \"使用L-CLIPScore作为字幕质量评估的评判标准是有效的，但单独使用它作为训练监督可能会导致训练失败，需要与其他度量方法混合使用。\")", "conclusion": "尽管L-CLIPScore在评估字幕生成模型质量方面表现出色，但单独将其作为训练监督使用可能会导致问题，需要结合其他度量标准。"}}
{"id": "2507.08711", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.08711", "abs": "https://arxiv.org/abs/2507.08711", "authors": ["Andreas Lolos", "Stergios Christodoulidis", "Maria Vakalopoulou", "Jose Dolz", "Aris Moustakas"], "title": "SGPMIL: Sparse Gaussian Process Multiple Instance Learning", "comment": "8 pages, 4 figures, 2 tables", "summary": "Multiple Instance Learning (MIL) offers a natural solution for settings where\nonly coarse, bag-level labels are available, without having access to\ninstance-level annotations. This is usually the case in digital pathology,\nwhich consists of gigapixel sized images. While deterministic attention-based\nMIL approaches achieve strong bag-level performance, they often overlook the\nuncertainty inherent in instance relevance. In this paper, we address the lack\nof uncertainty quantification in instance-level attention scores by introducing\n\\textbf{SGPMIL}, a new probabilistic attention-based MIL framework grounded in\nSparse Gaussian Processes (SGP). By learning a posterior distribution over\nattention scores, SGPMIL enables principled uncertainty estimation, resulting\nin more reliable and calibrated instance relevance maps. Our approach not only\npreserves competitive bag-level performance but also significantly improves the\nquality and interpretability of instance-level predictions under uncertainty.\nSGPMIL extends prior work by introducing feature scaling in the SGP predictive\nmean function, leading to faster training, improved efficiency, and enhanced\ninstance-level performance. Extensive experiments on multiple well-established\ndigital pathology datasets highlight the effectiveness of our approach across\nboth bag- and instance-level evaluations. Our code will be made publicly\navailable.", "AI": {"tldr": "提出了一种新的基于稀疏高斯过程的概率注意力多实例学习框架SGPMIL，用于量化实例级选择中的不确定性，提高了预测的可靠性和可解释性，同时保持了良好的包级性能。", "motivation": "论文试图解决实例级注意力得分中不确定性量化缺乏的问题，特别是在数字病理学这样的高像素图像领域。确定性注意力机制虽然在包级性能上表现良好，但它们忽视了实例相关性中的不确定性，这限制了预测的可靠性和可解释性。", "method": "研究方法是引入SGPMIL——一种基于稀疏高斯过程的概率注意力多实例学习框架。这种方法通过学习注意力评分的后验分布来实现不确定性估计。此外，还通过特征缩放改进了预测均值函数，以提高训练速度和效果。", "result": "这个论文提出了SGPMIL框架，这是一种基于稀疏高斯过程的概率注意力多实例学习方法。通过学习注意力得分的后验分布，SGPMIL能够提供更可靠、更校准的实例相关性映射，同时在包级性能上保持竞争力，并显著提高了不确定性下的实例级预测的准确性和可解释性。此外，SGPMIL通过在S-GP预测均值函数中引入特征缩放，实现了更快的训练速度、更高的效率和更优的实例级性能。实验表明，在多个数字病理学数据集上，该方法在包级和实例级评估中都表现出了有效性。", "conclusion": "研究得出的结论是，通过利用稀疏高斯过程和改进的预测均值函数，SGPMIL不仅提升了模型的训练效率，而且增强了不确定性条件下实例相关性的评估。实验结果表明，该方法在数字病理学数据集上，同时在实例级和包级评估中实现了有效的性能。"}}
{"id": "2507.08716", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.08716", "abs": "https://arxiv.org/abs/2507.08716", "authors": ["Kongwu Huang", "Shiyi Mu", "Jun Jiang", "Yuan Gao", "Shugong Xu"], "title": "Unreal is all you need: Multimodal ISAC Data Simulation with Only One Engine", "comment": null, "summary": "Scaling laws have achieved success in LLM and foundation models. To explore\ntheir potential in ISAC research, we propose Great-X. This single-engine\nmultimodal data twin platform reconstructs the ray-tracing computation of\nSionna within Unreal Engine and is deeply integrated with autonomous driving\ntools. This enables efficient and synchronized simulation of multimodal data,\nincluding CSI, RGB, Radar, and LiDAR. Based on this platform, we construct an\nopen-source, large-scale, low-altitude UAV multimodal synaesthesia dataset\nnamed Great-MSD, and propose a baseline CSI-based UAV 3D localization\nalgorithm, demonstrating its feasibility and generalizability across different\nCSI simulation engines. The related code and dataset are publicly available at:\nhttps://github.com/hkw-xg/Great-MCD.", "AI": {"tldr": "研究提出了一种名为Great-X的多模态数据平台并开发了相应的低空无人机数据集Great-MSD，旨在促进ISAC研究中的缩放定律的应用。同时，提出了一种基于CSI的无人机三维定位算法。", "motivation": "探索缩放定律在ISAC(集成传感与通信)研究中的潜力，并提供一个高效的多模态数据模拟平台以及可公开访问的数据集，以促进该领域的进一步研究。", "method": "研究使用Unreal Engine来实现Sionna的光线追踪计算，并集成自动驾驶工具，开发了Great-X平台。基于这个平台，构建了一个大型数据集Great-MSD，并提出了基于CSI的无人机三维定位算法。", "result": "该研究提出了一种名为Great-X的多模态数据双平台，利用Unreal Engine重建Sionna的光线追踪计算，并与自动驾驶工具深度集成，可以高效地同步模拟包括CSI、RGB、雷达和LiDAR在内的多模态数据。基于此平台，构建了一个开源的大型低空无人机多模态数据集Great-MSD，并提出了一种基于CSI的无人机三维定位算法，展示了其在不同CSI仿真引擎中的可行性和通用性。相关代码和数据集可在https://github.com/hkw-xg/Great-MCD上获得。", "conclusion": "研究展示了一种新型多模态数据双平台Great-X及其适用的数据集Great-MSD，提出了一种基于CSI的无人机三维定位算法，证明了其在低空环境下的有效性和多功能性。"}}
{"id": "2507.08729", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.08729", "abs": "https://arxiv.org/abs/2507.08729", "authors": ["Yuqiang Lin", "Sam Lockyer", "Mingxuan Sui", "Li Gan", "Florian Stanek", "Markus Zarbock", "Wenbin Li", "Adrian Evans", "Nic Zhang"], "title": "RoundaboutHD: High-Resolution Real-World Urban Environment Benchmark for Multi-Camera Vehicle Tracking", "comment": null, "summary": "The multi-camera vehicle tracking (MCVT) framework holds significant\npotential for smart city applications, including anomaly detection, traffic\ndensity estimation, and suspect vehicle tracking. However, current publicly\navailable datasets exhibit limitations, such as overly simplistic scenarios,\nlow-resolution footage, and insufficiently diverse conditions, creating a\nconsiderable gap between academic research and real-world scenario. To fill\nthis gap, we introduce RoundaboutHD, a comprehensive, high-resolution\nmulti-camera vehicle tracking benchmark dataset specifically designed to\nrepresent real-world roundabout scenarios. RoundaboutHD provides a total of 40\nminutes of labelled video footage captured by four non-overlapping,\nhigh-resolution (4K resolution, 15 fps) cameras. In total, 512 unique vehicle\nidentities are annotated across different camera views, offering rich\ncross-camera association data. RoundaboutHD offers temporal consistency video\nfootage and enhanced challenges, including increased occlusions and nonlinear\nmovement inside the roundabout. In addition to the full MCVT dataset, several\nsubsets are also available for object detection, single camera tracking, and\nimage-based vehicle re-identification (ReID) tasks. Vehicle model information\nand camera modelling/ geometry information are also included to support further\nanalysis. We provide baseline results for vehicle detection, single-camera\ntracking, image-based vehicle re-identification, and multi-camera tracking. The\ndataset and the evaluation code are publicly available at:\nhttps://github.com/siri-rouser/RoundaboutHD.git", "AI": {"tldr": "提出RoundaboutHD数据集，解决现有数据集在真实场景中存在的不足，为多摄像头车辆跟踪研究提供高质量的数据支持。", "motivation": "由于现有的公开数据集在场景复杂度、视频清晰度和多样性方面存在不足，无法满足学术研究与实际场景之间的需求，因此需要一套新的数据集来解决这一问题。", "method": "介绍了一套名为RoundaboutHD的多摄像头车辆跟踪基准数据集，解决了现有数据集在复杂真实世界场景中表现不足的问题。该数据集为城市十字路口提供了40分钟的高清标注视频素材，并提供多个子集以适应不同研究需要，同时提供了车辆型号信息和摄像头几何信息。", "result": "提供了车辆检测、单相机跟踪、图像车辆再识别（ReID）和多摄像头跟踪的基准结果。", "conclusion": "RoundaboutHD为研究人员提供了一个全面的、高质量的多摄像头车辆跟踪数据集，有助于推进智慧城市建设中涉及的交通密度估算、异常行为检测和嫌疑车追踪等领域的研究。"}}
{"id": "2507.08735", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.08735", "abs": "https://arxiv.org/abs/2507.08735", "authors": ["Anna Rosenberg", "John Kennedy", "Zohar Keidar", "Yehoshua Y. Zeevi", "Guy Gilboa"], "title": "Ensemble of Weak Spectral Total Variation Learners: a PET-CT Case Study", "comment": null, "summary": "Solving computer vision problems through machine learning, one often\nencounters lack of sufficient training data. To mitigate this we propose the\nuse of ensembles of weak learners based on spectral total-variation (STV)\nfeatures (Gilboa 2014). The features are related to nonlinear eigenfunctions of\nthe total-variation subgradient and can characterize well textures at various\nscales. It was shown (Burger et-al 2016) that, in the one-dimensional case,\northogonal features are generated, whereas in two-dimensions the features are\nempirically lowly correlated. Ensemble learning theory advocates the use of\nlowly correlated weak learners. We thus propose here to design ensembles using\nlearners based on STV features. To show the effectiveness of this paradigm we\nexamine a hard real-world medical imaging problem: the predictive value of\ncomputed tomography (CT) data for high uptake in positron emission tomography\n(PET) for patients suspected of skeletal metastases. The database consists of\n457 scans with 1524 unique pairs of registered CT and PET slices. Our approach\nis compared to deep-learning methods and to Radiomics features, showing STV\nlearners perform best (AUC=0.87), compared to neural nets (AUC=0.75) and\nRadiomics (AUC=0.79). We observe that fine STV scales in CT images are\nespecially indicative for the presence of high uptake in PET.", "AI": {"tldr": "本文基于光谱全变差（STV）特征使用弱学习器集合解决计算机视觉中的数据缺乏问题，特别针对CT数据对PET高摄取值的预测，在医疗影像实验中STV特征集合方法优于深度学习和放射组学特征。", "motivation": "本文旨在解决计算机视觉问题中常见的一个问题，即缺乏足够的训练数据。为此，作者提出了使用基于光谱全变差（STV）特征的弱学习器集合，这种特征能够很好地表征纹理，并且在二维情况下呈现出较低的相关性，因此适合集成学习的方法。", "method": "使用基于光谱全变差（STV）特征的弱学习器集合来解决机器学习中的计算机视觉问题，特别是在医疗影像分析中CT数据对PET高摄取值的预测问题。STV特征与全变差次梯度的非线性特征函数有关，能够很好地表征不同尺度下的纹理，并且在二维情况下这些特征表现出较低的相关性，这符合集成学习中对于低相关弱学习器的使用建议。", "result": "研究结果表明，使用STV特征的弱学习器集合方法优于深度学习方法（AUC=0.75）和放射组学特征（AUC=0.79），达到了最佳预测效果（AUC=0.87）。特别是CT图像中的精细STV尺度对于预测PET高摄取值具有很强的指示作用。", "conclusion": "本文通过解决一个实际的医疗影像问题，证明了基于STV特征的弱学习器集合方法的有效性，这种方法在预测CT数据对PET高摄取值的应用中表现出色。该研究提出了在缺乏足够训练数据的情况下，利用集成学习和STV特征进行计算机视觉任务的新路径。"}}
{"id": "2507.08741", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.08741", "abs": "https://arxiv.org/abs/2507.08741", "authors": ["Tianlong Ai", "Tianzhu Liu", "Haochen Jiang", "Yanfeng Gu"], "title": "HieraRS: A Hierarchical Segmentation Paradigm for Remote Sensing Enabling Multi-Granularity Interpretation and Cross-Domain Transfer", "comment": "17 pages, 11 figures", "summary": "Hierarchical land cover and land use (LCLU) classification aims to assign\npixel-wise labels with multiple levels of semantic granularity to remote\nsensing (RS) imagery. However, existing deep learning-based methods face two\nmajor challenges: 1) They predominantly adopt a flat classification paradigm,\nwhich limits their ability to generate end-to-end multi-granularity\nhierarchical predictions aligned with tree-structured hierarchies used in\npractice. 2) Most cross-domain studies focus on performance degradation caused\nby sensor or scene variations, with limited attention to transferring LCLU\nmodels to cross-domain tasks with heterogeneous hierarchies (e.g., LCLU to crop\nclassification). These limitations hinder the flexibility and generalization of\nLCLU models in practical applications. To address these challenges, we propose\nHieraRS, a novel hierarchical interpretation paradigm that enables\nmulti-granularity predictions and supports the efficient transfer of LCLU\nmodels to cross-domain tasks with heterogeneous tree-structured hierarchies. We\nintroduce the Bidirectional Hierarchical Consistency Constraint Mechanism\n(BHCCM), which can be seamlessly integrated into mainstream flat classification\nmodels to generate hierarchical predictions, while improving both semantic\nconsistency and classification accuracy. Furthermore, we present TransLU, a\ndual-branch cross-domain transfer framework comprising two key components:\nCross-Domain Knowledge Sharing (CDKS) and Cross-Domain Semantic Alignment\n(CDSA). TransLU supports dynamic category expansion and facilitates the\neffective adaptation of LCLU models to heterogeneous hierarchies. In addition,\nwe construct MM-5B, a large-scale multi-modal hierarchical land use dataset\nfeaturing pixel-wise annotations. The code and MM-5B dataset will be released\nat: https://github.com/AI-Tianlong/HieraRS.", "AI": {"tldr": "论文提出了一种用于遥感图像的分层土地覆盖和土地使用分类的新方法，解决了现有基于深度学习模型的两个主要限制，增强了模型的灵活性和泛化能力。", "motivation": "现有的基于深度学习的方法在对遥感图像进行分层土地覆盖和土地使用分类时，主要面临两个主要挑战：1) 它们采用扁平分类范式，限制了它们生成端到端多粒度分层预测的能力，以符合实际使用的树状分层结构。2) 大多数跨域研究侧重于传感器或场景变化引起的性能下降问题，对于将 LCLU 模型迁移到具有异构分层结构的不同领域任务的关注较少。这些限制影响了 LCLU 模型在实际应用中的灵活性和泛化能力。", "method": "提出了一种新的分层解释范式 HieraRS，可以进行多粒度预测，并支持高效地将 LCLU 模型迁移到具有异构分层结构的跨领域任务中。我们引入了双向分层一致性约束机制 (BHCCM)，可以无缝集成到主流的扁平分类模型中生成分层预测，同时提高语义一致性和分类准确性。另外我们还提出了 TransLU，这是一个双分支跨域迁移框架，包含两个重要组成部分：跨域知识共享（CDKS）和跨域语义对齐（CDSA）。TransLU 支持动态类别扩展，并促进了 LCLU 模型对异构分层的适应。", "result": "研究构建了一个大规模的多模态分层土地利用数据集 MM-5B，该数据集具有像素级标注，这有助于验证 HieraRS 和 TransLU 方法的有效性。", "conclusion": "通过提出 HieraRS 和 TransLU，解决了现有 LCLU 模型的两个主要挑战：扁平分类范式限制以及对异构分层结构跨域迁移的关注不足。这提高了模型在实际应用中的灵活性和泛化潜力，特别是在跨域任务中。"}}
{"id": "2507.08743", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.08743", "abs": "https://arxiv.org/abs/2507.08743", "authors": ["Rei Tamaru", "Pei Li", "Bin Ran"], "title": "Geo-ORBIT: A Federated Digital Twin Framework for Scene-Adaptive Lane Geometry Detection", "comment": null, "summary": "Digital Twins (DT) have the potential to transform traffic management and\noperations by creating dynamic, virtual representations of transportation\nsystems that sense conditions, analyze operations, and support decision-making.\nA key component for DT of the transportation system is dynamic roadway geometry\nsensing. However, existing approaches often rely on static maps or costly\nsensors, limiting scalability and adaptability. Additionally, large-scale DTs\nthat collect and analyze data from multiple sources face challenges in privacy,\ncommunication, and computational efficiency. To address these challenges, we\nintroduce Geo-ORBIT (Geometrical Operational Roadway Blueprint with Integrated\nTwin), a unified framework that combines real-time lane detection, DT\nsynchronization, and federated meta-learning. At the core of Geo-ORBIT is\nGeoLane, a lightweight lane detection model that learns lane geometries from\nvehicle trajectory data using roadside cameras. We extend this model through\nMeta-GeoLane, which learns to personalize detection parameters for local\nentities, and FedMeta-GeoLane, a federated learning strategy that ensures\nscalable and privacy-preserving adaptation across roadside deployments. Our\nsystem is integrated with CARLA and SUMO to create a high-fidelity DT that\nrenders highway scenarios and captures traffic flows in real-time. Extensive\nexperiments across diverse urban scenes show that FedMeta-GeoLane consistently\noutperforms baseline and meta-learning approaches, achieving lower geometric\nerror and stronger generalization to unseen locations while drastically\nreducing communication overhead. This work lays the foundation for flexible,\ncontext-aware infrastructure modeling in DTs. The framework is publicly\navailable at https://github.com/raynbowy23/FedMeta-GeoLane.git.", "AI": {"tldr": "这篇论文提出一种名为Geo-ORBIT的框架，该框架克服了现有数字孪生在交通系统应用上的挑战，实现了高效的车道检测与数据处理能力。", "motivation": "当前数字孪生在交通系统中的方法依赖静态地图或昂贵的传感器，限制了其扩展性和适应性。大规模数字孪生在收集和分析多源数据时面临隐私、通讯和计算效率等挑战。", "method": "介绍了一种名为Geo-ORBIT的统一框架，该框架结合了实时车道检测、数字孪生同步和联邦元学习。核心部分GeoLane是一种轻量级车道检测模型，可以从车辆轨迹数据中学习车道几何结构。进一步通过Meta-GeoLane元学习个性化局部实体的检测参数，FedMeta-GeoLane则是一种联邦学习策略，确保跨路边部署中的可扩展性和隐私保护性适应性。", "result": "实验结果显示，在各类城市环境中，FedMeta-GeoLane在降低几何误差的同时，增强了对未知地点的泛化能力，并大幅减少了通讯开销，性能优于基线方法和元学习方法。", "conclusion": "该研究为数字孪生中的柔性、场景感知型基础设施模型奠定了基础，并公开了框架代码以供进一步研究与应用。"}}
{"id": "2507.08765", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.08765", "abs": "https://arxiv.org/abs/2507.08765", "authors": ["Juntong Fan", "Zhiwei Hao", "Jianqiang Shen", "Shang-Ling Jui", "Yi Zhang", "Jing-Xiao Liao", "Feng-Lei Fan"], "title": "Compress Any Segment Anything Model (SAM)", "comment": "13 pages, 6 tables, 8 figures", "summary": "Due to the excellent performance in yielding high-quality, zero-shot\nsegmentation, Segment Anything Model (SAM) and its variants have been widely\napplied in diverse scenarios such as healthcare and intelligent manufacturing.\nTherefore, effectively compressing SAMs has become an increasingly pressing\npractical need. In this study, we propose Birkhoff, a novel data-free\ncompression algorithm for SAM and its variants. Unlike quantization, pruning,\ndistillation, and other compression methods, Birkhoff embodies versatility\nacross model types, agility in deployment, faithfulness to the original model,\nand compactness in model size. Specifically, Birkhoff introduces a novel\ncompression algorithm: Hyper-Compression, whose core principle is to find a\ndense trajectory to turn a high-dimensional parameter vector into a\nlow-dimensional scalar. Furthermore, Birkhoff designs a dedicated linear layer\noperator, HyperLinear, to fuse decompression and matrix multiplication to\nsignificantly accelerate inference of the compressed SAMs. Extensive\nexperiments on 18 SAMs in the COCO, LVIS, and SA-1B datasets show that Birkhoff\nperforms consistently and competitively in compression time, compression ratio,\npost-compression performance, and inference speed. For example, Birkhoff can\nachieve a compression ratio of 5.17x on SAM2-B, with less than 1% performance\ndrop without using any fine-tuning data. Moreover, the compression is finished\nwithin 60 seconds for all models.", "AI": {"tldr": "本文提出了一种名为Birkhoff的新型无数据压缩算法，可有效压缩SAM，并通过实验表明，Birkhoff在压缩时间、压缩比、压缩后的性能和推理速度方面表现一致且具有竞争力。", "motivation": "鉴于SAM及其变体在医疗保健和智能制造等不同场景中的卓越性能，高效压缩SAM已经成为一个紧迫的实际需求。", "method": "Birkhoff提出了一种新颖的无数据压缩算法，通过Hyper-Compression和HyperLinear线性层操作来实现SAM及其变体的压缩和加速推理。", "result": "实验表明，Birkhoff在COCO、LVIS和SA-1B数据集上的18个SAM模型上表现稳定且具有竞争力，能够以不到1%的性能下降在60秒内实现5.17倍的压缩比。", "conclusion": "该研究通过提出Birkhoff实现了SAM及其变体的有效压缩，证明了其在压缩比、速度和性能方面的优秀表现。"}}
{"id": "2507.08766", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.08766", "abs": "https://arxiv.org/abs/2507.08766", "authors": ["Ahmed Farooq"], "title": "A Hybrid Multi-Well Hopfield-CNN with Feature Extraction and K-Means for MNIST Classification", "comment": null, "summary": "This study presents a hybrid model for classifying handwritten digits in the\nMNIST dataset, combining convolutional neural networks (CNNs) with a multi-well\nHopfield network. The approach employs a CNN to extract high-dimensional\nfeatures from input images, which are then clustered into class-specific\nprototypes using k-means clustering. These prototypes serve as attractors in a\nmulti-well energy landscape, where a Hopfield network performs classification\nby minimizing an energy function that balances feature similarity and class\nassignment.The model's design enables robust handling of intraclass\nvariability, such as diverse handwriting styles, while providing an\ninterpretable framework through its energy-based decision process. Through\nsystematic optimization of the CNN architecture and the number of wells, the\nmodel achieves a high test accuracy of 99.2% on 10,000 MNIST images,\ndemonstrating its effectiveness for image classification tasks. The findings\nhighlight the critical role of deep feature extraction and sufficient prototype\ncoverage in achieving high performance, with potential for broader applications\nin pattern recognition.", "AI": {"tldr": "The paper proposes a hybrid model combining CNNs and Hopfield networks to classify handwritten digits in the MNIST dataset, achieving a high test accuracy of 99.2%.", "motivation": "To handle intraclass variability and provide an interpretable framework for classification by leveraging the representation capabilities of CNNs and the energy-based decision process of Hopfield networks.", "method": "The model extracts high-dimensional features using CNNs, clusters these into class-specific prototypes with k-means, and uses the prototypes as attractors in a multi-well Hopfield network for classification.", "result": "The hybrid model achieved a high test accuracy of 99.2% on the MNIST dataset, highlighting the importance of deep feature extraction and proper prototype representation.", "conclusion": "The findings suggest that the hybrid model effectively addresses image classification tasks with high variability by effectively combining CNNs and Hopfield networks, with potential applications in broader pattern recognition tasks."}}
