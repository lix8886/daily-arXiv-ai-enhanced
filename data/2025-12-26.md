<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 4]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [VL4Gaze: Unleashing Vision-Language Models for Gaze Following](https://arxiv.org/abs/2512.20735)
*Shijing Wang,Chaoqun Cui,Yaping Huang,Hyung Jin Chang,Yihua Cheng*

Main category: cs.CV

> 本文介绍了VL4Gaze，这是一个首个针对视觉语言模型在人类注视理解方面潜力的大型基准数据库，通过对注视理解的四个任务进行评估和训练，展示了在特定任务监督下的显著改善。

<details>
  <summary>Details</summary>

**Motivation:** 现有的视觉语言模型（VLMs）在场景级推理上已经取得了显著进展，但尚未有一个系统化的基准对这些模型进行注视理解的评估或训练。因此，提出VL4Gaze以填补这部分空白，探索VLMs在理解人类注视方面的潜力。

**Method:** VL4Gaze被引入作为首个大规模基准，旨在调查、评估并解锁视觉语言模型（VLMs）在理解人类注视方面的潜力。VL4Gaze包含489,000个自动生成的问题-答案对和124,000张图像，将其分为四个互补任务：注视对象描述、注视方向描述、注视点定位以及歧义问题识别。

**Result:** 实验结果表明，即使大规模的VLMs在没有特定任务监督的情况下也难以可靠地推断注视语义和空间定位。然而，在VL4Gaze上的训练能够在所有任务上带来显著和一致的改进。

**Conclusion:** 研究表明，针对多任务的监督对于开发VLMs在理解注视方面的能力是至关重要的。VL4Gaze的发布将支持未来的进一步研究和发展。

**Abstract:** Human gaze provides essential cues for interpreting attention, intention, and social interaction in visual scenes, yet gaze understanding remains largely unexplored in current vision-language models (VLMs). While recent VLMs achieve strong scene-level reasoning across a range of visual tasks, there exists no benchmark that systematically evaluates or trains them for gaze interpretation, leaving open the question of whether gaze understanding can emerge from general-purpose vision-language pre-training. To address this gap, we introduce VL4Gaze, the first large-scale benchmark designed to investigate, evaluate, and unlock the potential of VLMs for gaze understanding. VL4Gaze contains 489K automatically generated question-answer pairs across 124K images and formulates gaze understanding as a unified VQA problem through four complementary tasks: (1) gaze object description, (2) gaze direction description, (3) gaze point location, and (4) ambiguous question recognition. We comprehensively evaluate both commercial and open-source VLMs under in-context learning and fine-tuning settings. The results show that even large-scale VLMs struggle to reliably infer gaze semantics and spatial localization without task-specific supervision. In contrast, training on VL4Gaze brings substantial and consistent improvements across all tasks, highlighting the importance of targeted multi-task supervision for developing gaze understanding capabilities in VLMs. We will release the dataset and code to support further research and development in this direction.

</details>


### [2] [TrashDet: Iterative Neural Architecture Search for Efficient Waste Detection](https://arxiv.org/abs/2512.20746)
*Tony Tran,Bin Hu*

Main category: cs.CV

> The paper presents a framework for developing scalable, efficient trash detectors for TinyML, resulting in TrashDets, which outperform previous models in both accuracy and efficiency on resource-limited devices.

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to develop efficient trash detection systems for edge and IoT devices under TinyML constraints, improving upon previous models in terms of both computational efficiency and accuracy.

**Method:** The proposed method uses an iterative hardware-aware neural architecture search to create a Once-for-All supernet and optimize it for different deployment scenarios, yielding a family of TrashDets.

**Result:** The strongest detector, TrashDet-l, achieves 19.5 mAP50 with 30.5M parameters, demonstrating superior accuracy and efficiency. The TrashDet family spans diverse budgets with parameter counts from 1.2M to 30.5M. When compared to the ai87-fpndetector baseline on the MAX78002 microcontroller, TrashDets achieve significant improvements in energy efficiency, latency, and average power.

**Conclusion:** TrashDets are a scalable solution for TinyML applications, providing high detection accuracy with significant improvements in energy consumption, latency, and power efficiency compared to existing detectors, especially for trash detection on microcontrollers.

**Abstract:** This paper addresses trash detection on the TACO dataset under strict TinyML constraints using an iterative hardware-aware neural architecture search framework targeting edge and IoT devices. The proposed method constructs a Once-for-All-style ResDets supernet and performs iterative evolutionary search that alternates between backbone and neck/head optimization, supported by a population passthrough mechanism and an accuracy predictor to reduce search cost and improve stability. This framework yields a family of deployment-ready detectors, termed TrashDets. On a five-class TACO subset (paper, plastic, bottle, can, cigarette), the strongest variant, TrashDet-l, achieves 19.5 mAP50 with 30.5M parameters, improving accuracy by up to 3.6 mAP50 over prior detectors while using substantially fewer parameters. The TrashDet family spans 1.2M to 30.5M parameters with mAP50 values between 11.4 and 19.5, providing scalable detector options for diverse TinyML deployment budgets on resource-constrained hardware. On the MAX78002 microcontroller with the TrashNet dataset, two specialized variants, TrashDet-ResNet and TrashDet-MBNet, jointly dominate the ai87-fpndetector baseline, with TrashDet-ResNet achieving 7525~$μ$J energy per inference at 26.7 ms latency and 37.45 FPS, and TrashDet-MBNet improving mAP50 by 10.2%; together they reduce energy consumption by up to 88%, latency by up to 78%, and average power by up to 53% compared to existing TinyML detectors.

</details>


### [3] [OccuFly: A 3D Vision Benchmark for Semantic Scene Completion from the Aerial Perspective](https://arxiv.org/abs/2512.20770)
*Markus Gross,Sai B. Matha,Aya Fahmy,Rui Song,Daniel Cremers,Henri Meess*

Main category: cs.CV

> 此研究为首个基于相机的空中3D场景完成基准OccuFly，可自动转移标签，减少手动3D标注工作量，旨在推进基于UAV的3D感知研究。

<details>
  <summary>Details</summary>

**Motivation:** 鉴于之前对于无人飞行器（UAV）地块无语义场景理解的研究不足，且LiDAR传感器在UAV上应用受限，研究动机在于解决这一问题，推动三维感知在航空领域的进展。

**Method:** 提出OccuFly数据集，并提供一种采用相机作为传感器的自动化标签转移框架，以解决LiDAR在UAV上应用的限制问题。

**Result:** 结构分析完成。该研究聚焦于无人飞行器的三维感知，特别是使用相机代替LiDAR传感器进行场景结构和语义的三维重建。研究的主要贡献是OccuFly基准数据集和一种基于相机的自动标记转移框架，旨在减少三维标注工作量，并提供了针对空中视角的具体挑战。

**Conclusion:** 研究展示了OccuFly基准数据集在解决LiDAR限制方面的有效性，并强调了从空中视角进行3D语义场景理解的挑战，为后续研究奠定基础。

**Abstract:** Semantic Scene Completion (SSC) is crucial for 3D perception in mobile robotics, as it enables holistic scene understanding by jointly estimating dense volumetric occupancy and per-voxel semantics. Although SSC has been widely studied in terrestrial domains such as autonomous driving, aerial scenarios like autonomous flying remain largely unexplored, thereby limiting progress on downstream applications. Furthermore, LiDAR sensors represent the primary modality for SSC data generation, which poses challenges for most uncrewed aerial vehicles (UAVs) due to flight regulations, mass and energy constraints, and the sparsity of LiDAR-based point clouds from elevated viewpoints. To address these limitations, we introduce OccuFly, the first real-world, camera-based aerial SSC benchmark, captured at altitudes of 50m, 40m, and 30m during spring, summer, fall, and winter. OccuFly covers urban, industrial, and rural scenarios, provides 22 semantic classes, and the data format adheres to established conventions to facilitate seamless integration with existing research. Crucially, we propose a LiDAR-free data generation framework based on camera modality, which is ubiquitous on modern UAVs. By utilizing traditional 3D reconstruction, our framework automates label transfer by lifting a subset of annotated 2D masks into the reconstructed point cloud, thereby substantially minimizing manual 3D annotation effort. Finally, we benchmark the state-of-the-art on OccuFly and highlight challenges specific to elevated viewpoints, yielding a comprehensive vision benchmark for holistic aerial 3D scene understanding.

</details>


### [4] [NULLBUS: Multimodal Mixed-Supervision for Breast Ultrasound Segmentation via Nullable Global-Local Prompts](https://arxiv.org/abs/2512.20783)
*Raja Mallina,Bryar Shareef*

Main category: cs.CV

> 提出NullBUS框架，通过可空提示技术解决乳腺超声图像分割缺少元数据的问题，实现优越的分割性能。

<details>
  <summary>Details</summary>

**Motivation:** 现有的乳腺超声图像分割方法受限于公共数据集中可靠的元数据或报告的缺乏，影响了训练效果和鲁棒性。

**Method:** 提出了一种名为NullBUS的多模态混合监督框架，能够在单一模型中同时学习带有和不带提示的图像。为了处理缺失的文本，引入了可空提示，通过学习的空值嵌入和存在掩码实现，这使得该模型可以在元数据缺失时依赖图像证据，在有文本提示时使用文本。

**Result:** 在三个公共乳腺超声数据集的统一评估池上，NullBUS达到了0.8568的平均IoU和0.9103的平均Dice，显示了在混合提示可用性下的顶级性能。

**Conclusion:** NullBUS框架通过引入可空提示，可以在没有文本或有文本提示的条件下都能取得优異的效果，解决了公共数据集中缺乏可靠元数据的问题。

**Abstract:** Breast ultrasound (BUS) segmentation provides lesion boundaries essential for computer-aided diagnosis and treatment planning. While promptable methods can improve segmentation performance and tumor delineation when text or spatial prompts are available, many public BUS datasets lack reliable metadata or reports, constraining training to small multimodal subsets and reducing robustness. We propose NullBUS, a multimodal mixed-supervision framework that learns from images with and without prompts in a single model. To handle missing text, we introduce nullable prompts, implemented as learnable null embeddings with presence masks, enabling fallback to image-only evidence when metadata are absent and the use of text when present. Evaluated on a unified pool of three public BUS datasets, NullBUS achieves a mean IoU of 0.8568 and a mean Dice of 0.9103, demonstrating state-of-the-art performance under mixed prompt availability.

</details>
