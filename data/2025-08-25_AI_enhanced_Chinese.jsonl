{"id": "2508.15790", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.15790", "abs": "https://arxiv.org/abs/2508.15790", "authors": ["Nan Wang", "Yongqi Fan", "yansha zhu", "ZongYu Wang", "Xuezhi Cao", "Xinyan He", "Haiyun Jiang", "Tong Ruan", "Jingping Liu"], "title": "KG-o1: Enhancing Multi-hop Question Answering in Large Language Models via Knowledge Graph Integration", "comment": null, "summary": "Large Language Models (LLMs) face challenges in knowledge-intensive reasoning\ntasks like classic multi-hop question and answering, which involves reasoning\nacross multiple facts. This difficulty arises because the chain of thoughts\n(CoTs) generated by LLMs in such tasks often deviate from real or a priori\nreasoning paths. In contrast, knowledge graphs (KGs) explicitly represent the\nlogical connections between facts through entities and relationships. This\nreflects a significant gap. Meanwhile, large reasoning models (LRMs), such as\no1, have demonstrated that long-step reasoning significantly enhances the\nperformance of LLMs. Building on these insights, we propose KG-o1, a four-stage\napproach that integrates KGs to enhance the multi-hop reasoning abilities of\nLLMs. We first filter out initial entities and generate complex subgraphs.\nSecondly, we construct logical paths for subgraphs and then use knowledge\ngraphs to build a dataset with a complex and extended brainstorming process,\nwhich trains LLMs to imitate long-term reasoning. Finally, we employ rejection\nsampling to generate a self-improving corpus for direct preference optimization\n(DPO), further refining the LLMs reasoning abilities. We conducted experiments\non two simple and two complex datasets. The results show that KG-o1 models\nexhibit superior performance across all tasks compared to existing LRMs.", "AI": {"tldr": "提出了一种名为KG-o1的方法，结合知识图谱来提升大型语言模型在多跳推理任务中的表现。", "motivation": "大型语言模型在知识密集型推理任务（如经典的多跳问题解答）中面临挑战，因为生成的思维链通常偏离现实或先验的推理路径。这项研究旨在通过融合知识图谱，以增强大型语言模型在多跳推理任务上的表现。", "method": "通过四阶段方法集成知识图谱来增强大型语言模型的多跳推理能力。首先，筛选初始实体并生成复杂子图。接着，为子图构建逻辑路径并使用知识图谱构建一个复杂且扩展的脑暴数据集，该数据集训练大型语言模型模仿长期推理。最后，使用拒绝采样生成自我改进语料库，进一步优化直接偏好优化(DPO)，从而提升大型语言模型的推理能力。", "result": "实验结果表明，KG-o1模型在所有任务上的表现优于现有的大型推理模型。", "conclusion": "KG-o1方法有效提升了大型语言模型在多跳推理任务中的性能，证明了知识图谱集成的潜力和价值。"}}
