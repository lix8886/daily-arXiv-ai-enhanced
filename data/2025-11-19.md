<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 12]
- [cs.CV](#cs.CV) [Total: 9]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Signature vs. Substance: Evaluating the Balance of Adversarial Resistance and Linguistic Quality in Watermarking Large Language Models](https://arxiv.org/abs/2511.13722)
*William Guo,Adaku Uchendu,Ana Smith*

Main category: cs.CL

> 研究评估了几种水印技术的对抗攻击鲁棒性和保持文本质量和写作风格的能力，结果显示这些技术在保留语义的同时，写作风格有所改变，并对回译攻击有明显漏洞。

<details>
  <summary>Details</summary>

**Motivation:** 水印技术能够帮助检测大型语言模型生成的文本。然而，这些技术可能会降低生成文本的质量，并且容易受到对抗攻击。为了鼓励水印技术的广泛采用，作者对此进行了研究。

**Method:** 通过对几种水印技术进行评估，特别是对抗性攻击（比如改写和回译攻击）下的鲁棒性评估，并且通过使用语言学指标来评估水印文本的质量和写作风格保持度。

**Result:** 研究结果显示，水印技术尽管能够保留语义，但在写作风格上与原文有所偏离，并且对于对抗性攻击，尤其是回译攻击，水印技术的抵抗力较弱。

**Conclusion:** 水印技术在对抗性攻击下的鲁棒性有限，并且无法保持良好的文本质量和写作风格，这导致了其在大型语言模型创作者中的应用阻力。未来的研究需要关注提升水印技术的鲁棒性以及保持原文风格的能力。

**Abstract:** To mitigate the potential harms of Large Language Models (LLMs)generated text, researchers have proposed watermarking, a process of embedding detectable signals within text. With watermarking, we can always accurately detect LLM-generated texts. However, recent findings suggest that these techniques often negatively affect the quality of the generated texts, and adversarial attacks can strip the watermarking signals, causing the texts to possibly evade detection. These findings have created resistance in the wide adoption of watermarking by LLM creators. Finally, to encourage adoption, we evaluate the robustness of several watermarking techniques to adversarial attacks by comparing paraphrasing and back translation (i.e., English $\to$ another language $\to$ English) attacks; and their ability to preserve quality and writing style of the unwatermarked texts by using linguistic metrics to capture quality and writing style of texts. Our results suggest that these watermarking techniques preserve semantics, deviate from the writing style of the unwatermarked texts, and are susceptible to adversarial attacks, especially for the back translation attack.

</details>


### [2] [Refine Thought: A Test-Time Inference Method for Embedding Model Reasoning](https://arxiv.org/abs/2511.13726)
*Guangzhi Wang,Kai Li,Yinghao Jiao,Zhi Liu*

Main category: cs.CL

> RT方法通过多次文本嵌入模型前向传播，显著提升了语义推理任务的表现，维持了在通用语义理解任务的性能。

<details>
  <summary>Details</summary>

**Motivation:** 提升文本嵌入模型在语义推理任务中的性能，同时保持在通用语义理解任务上的稳定表现。

**Method:** RT方法能够通过多次前向传播提升文本嵌入模型的语义推理能力，从而获得最终的语义表示。

**Result:** 实验表明，RT在BRIGHT和PJBenchmark1基准测试中的语义推理任务上取得了显著改善，同时在C-MTEB等通用语义理解任务上保持了一致的性能。

**Conclusion:** RT有效激活了预训练时由解码器只有文本嵌入模型学习到的语义推理能力，其作为一个测试阶段推理方法表现出色。

**Abstract:** We propose RT (Refine Thought), a method that can enhance the semantic rea-soning ability of text embedding models. The method obtains the final semanticrepresentation by running multiple forward passes of the text embedding model.Experiments show that RT achieves significant improvements on semantic reason-ing tasks in BRIGHT and the person job matching benchmark PJBenchmark1, while maintaining consistent performance on general-purpose semantic under-standing tasks such as C-MTEB. Our results indicate that RT is effective becauseit further activates the semantic reasoning ability learned during pretraining bydecoder-only text embedding models(e.g., Qwen3-Embedding-8B). RT canbe seen as a test-time inference method.

</details>


### [3] [Can QE-informed (Re)Translation lead to Error Correction?](https://arxiv.org/abs/2511.13884)
*Govardhan Padmanabhan*

Main category: cs.CL

> 论文提出了两种基于QE的无需训练方法来改善片段级错误修正，其中一个方法表现出色。

<details>
  <summary>Details</summary>

**Motivation:** 研究的动机在于避免自动后编辑（APE）系统在纠正机器翻译输出时过度修正的缺点，尽管联合训练QE系统与APE已经在两个任务上展示了更好的性能，但需要提出更优化的方法。

**Method:** 该论文提出了两种无需训练的方法来参与WMT 2025自动翻译质量评估系统的任务三 - 基于质量估计（QE）的片段级错误修正。第一个方法是基于QE的重翻译，选择多个不同LLM生成的最高质量的翻译；第二个方法是指导LLM替换由所提供的QE解释指定的错误子串，并使用一种条件启发式方法来最小化修改次数，以最大化修正收益与修改量的比率。

**Result:** 第一个方法在子任务排行榜上获得了胜利，两个方法分别实现了0.0201和-0.0108的Delta COMET得分。

**Conclusion:** 本文提出的训练无需方法，尤其是基于QE的重翻译方法，在改进翻译质量评估方面表现良好，优于传统的联合训练方法。

**Abstract:** The paper presents two approaches submitted to the WMT 2025 Automated Translation Quality Evaluation Systems Task 3 - Quality Estimation (QE)-informed Segment-level Error Correction. While jointly training QE systems with Automatic Post-Editing (APE) has shown improved performance for both tasks, APE systems are still known to overcorrect the output of Machine Translation (MT), leading to a degradation in performance. We investigate a simple training-free approach - QE-informed Retranslation, and compare it with another within the same training-free paradigm. Our winning approach selects the highest-quality translation from multiple candidates generated by different LLMs. The second approach, more akin to APE, instructs an LLM to replace error substrings as specified in the provided QE explanation(s). A conditional heuristic was employed to minimise the number of edits, with the aim of maximising the Gain-to-Edit ratio. The two proposed approaches achieved a Delta COMET score of 0.0201 and -0.0108, respectively, leading the first approach to achieve the winning position on the subtask leaderboard.

</details>


### [4] [What Works for 'Lost-in-the-Middle' in LLMs? A Study on GM-Extract and Mitigations](https://arxiv.org/abs/2511.13900)
*Mihir Gupte,Eshan Dixit,Muhammad Tayyab,Arun Adiththan*

Main category: cs.CL

> This paper evaluates large language models on long-range context retrieval using a novel benchmark. It identifies performance patterns and assesses various mitigation strategies for long-range context issues.

<details>
  <summary>Details</summary>

**Motivation:** The motivation stems from the challenge of large language models losing the ability to effectively use long-range context, a phenomenon known as 'lost-in-the-middle'.

**Method:** The paper introduces GM-Extract, a benchmark dataset for evaluating LLM performance in retrieving control variables, and proposes an evaluation system with two metrics: one for spatial retrieval and another for semantic retrieval.

**Result:** A systematic evaluation on two multi-document tasks revealed performance changes by altering data representation. While a U-shaped curve was not consistently seen, clear performance patterns across models were found and correlated with perplexity scores.

**Conclusion:** The evaluation shows nuanced results from different mitigation methods, highlighting scenarios where they improve or decrease performance, offering a practical understanding of their utility.

**Abstract:** The diminishing ability of large language models (LLMs) to effectively utilize long-range context-the "lost-in-the-middle" phenomenon-poses a significant challenge in retrieval-based LLM applications. To study the impact of this phenomenon in a real-world application setting, we introduce GM-Extract, a novel benchmark dataset meticulously designed to evaluate LLM performance on retrieval of control variables. To accurately diagnose failure modes, we propose a simple yet elegant evaluation system using two distinct metrics: one for spatial retrieval capability (Document Metric) and the other for semantic retrieval capability (Variable Extraction Metric). We conduct a systematic evaluation of 7-8B parameter models on two multi-document tasks (key-value extraction and question-answering), demonstrating a significant change in retrieval performance simply by altering how the data is represented in the context window. While a distinct U-shaped curve was not consistently observed, our analysis reveals a clear pattern of performance across models, which we further correlate with perplexity scores. Furthermore, we perform a literature survey of mitigation methods, which we categorize into two distinct approaches: black-box and white-box methods. We then apply these techniques to our benchmark, finding that their efficacy is highly nuanced. Our evaluation highlights scenarios where these strategies successfully improve performance, as well as surprising cases where they lead to a negative impact, providing a comprehensive understanding of their utility in a practical context.

</details>


### [5] [Hint-Augmented Re-ranking: Efficient Product Search using LLM-Based Query Decomposition](https://arxiv.org/abs/2511.13994)
*Yilun Zhu,Nikhita Vedula,Shervin Malmasi*

Main category: cs.CL

> 研究表明LLM可以从电子商务查询中挖掘最高级表达的潜在意图。提出的方法提高了搜索性能，并有效解决了直接基于LLM重排序的部署难题。

<details>
  <summary>Details</summary>

**Motivation:** 最佳和最受欢迎等最高级搜索查询需要跨多个维度比较候选对象，要求具备语言理解和领域知识。

**Method:** 提出了一种框架，通过该框架可以从电子商务查询中提取结构化的解释或提示。该方法将查询分解为属性-值提示，并与检索同时生成，可有效集成到排名管道中。为了克服基于LLM的重排序延迟高的问题，开发了一种将最高级解释转移到轻量级模型的有效方法。

**Result:** 该方法将MAP提高了10.9点，将MRR提高了5.9点。

**Conclusion:** 研究为如何在检索系统中如何表示和转移最高级语义提供了见解，同时解决了实际部署约束问题。

**Abstract:** Search queries with superlatives (e.g., best, most popular) require comparing candidates across multiple dimensions, demanding linguistic understanding and domain knowledge. We show that LLMs can uncover latent intent behind these expressions in e-commerce queries through a framework that extracts structured interpretations or hints. Our approach decomposes queries into attribute-value hints generated concurrently with retrieval, enabling efficient integration into the ranking pipeline. Our method improves search performanc eby 10.9 points in MAP and ranking by 5.9 points in MRR over baselines. Since direct LLM-based reranking faces prohibitive latency, we develop an efficient approach transferring superlative interpretations to lightweight models. Our findings provide insights into how superlative semantics can be represented and transferred between models, advancing linguistic interpretation in retrieval systems while addressing practical deployment constraints.

</details>


### [6] [Knowledge-Grounded Agentic Large Language Models for Multi-Hazard Understanding from Reconnaissance Reports](https://arxiv.org/abs/2511.14010)
*Chenchen Kuai,Zihao Li,Braden Rosen,Stephanie Paan,Navid Jafari,Jean-Louis Briaud,Yunlong Zhang,Youssef M. A. Hashash,Yang Zhou*

Main category: cs.CL

> 本文提出并评估了MoRA-RAG，一种基于知识的LLM框架，用于分析灾害后勘查报告，以提高这些文档转化为可靠的灾害韧性情报的能力。

<details>
  <summary>Details</summary>

**Motivation:** 灾害后勘查报告对于理解多种灾害间的相互作用至关重要，但这些报告的非结构化叙述使得系统性的知识转移变得困难。大型语言模型（LLMs）在缺少领域依据的情况下，往往会产生不可靠或捏造的输出。本文的方法旨在解决这些问题，并通过MoRA-RAG框架将勘查报告转化为多灾害推理的结构化基础。

**Method:** 本文介绍了Mixture-of-Retrieval Agentic RAG (MoRA-RAG)，一种基于知识的机器学习模型框架，用于将灾害后勘查报告转换为多灾害推理的结构化基础。该框架结合了检索混合机制，可以根据具体灾害动态路由查询，并通过代理切块方法保持检索过程中的上下文连贯性。它还包含了一个验证循环，以评估证据的充分性，精炼查询，并在信息不完整时启动有针对性的搜索。

**Result:** 基于从GEER勘查报告中提取的90起全球事件的问答对构建的HazardRecQA数据集，MoRA-RAG实现了高达94.5%的准确率，相比零样本LLM提高了30%，比最先进的RAG系统提升了10%，同时大幅减少了各种LLM架构中的捏造输出。

**Conclusion:** MoRA-RAG证明了有能力将灾害后文档转换为行动导向的、值得信赖的智能，开启了灾害韧性能力建设的新范式。并且，它能使开放权重的LLMs达到与专有模型相当的表现。

**Abstract:** Post-disaster reconnaissance reports contain critical evidence for understanding multi-hazard interactions, yet their unstructured narratives make systematic knowledge transfer difficult. Large language models (LLMs) offer new potential for analyzing these reports, but often generate unreliable or hallucinated outputs when domain grounding is absent. This study introduces the Mixture-of-Retrieval Agentic RAG (MoRA-RAG), a knowledge-grounded LLM framework that transforms reconnaissance reports into a structured foundation for multi-hazard reasoning. The framework integrates a Mixture-of-Retrieval mechanism that dynamically routes queries across hazard-specific databases while using agentic chunking to preserve contextual coherence during retrieval. It also includes a verification loop that assesses evidence sufficiency, refines queries, and initiates targeted searches when information remains incomplete. We construct HazardRecQA by deriving question-answer pairs from GEER reconnaissance reports, which document 90 global events across seven major hazard types. MoRA-RAG achieves up to 94.5 percent accuracy, outperforming zero-shot LLMs by 30 percent and state-of-the-art RAG systems by 10 percent, while reducing hallucinations across diverse LLM architectures. MoRA-RAG also enables open-weight LLMs to achieve performance comparable to proprietary models. It establishes a new paradigm for transforming post-disaster documentation into actionable, trustworthy intelligence for hazard resilience.

</details>


### [7] [HiEAG: Evidence-Augmented Generation for Out-of-Context Misinformation Detection](https://arxiv.org/abs/2511.14027)
*Junjie Wu,Yumeng Fu,Nan Yu,Guohong Fu*

Main category: cs.CL

> 本文引入了一种名为HiEAG的框架，通过改进外部一致性检查来检测多模态无上下文（OOC）的错误信息，并在多个数据集上优于现有方法。

<details>
  <summary>Details</summary>

**Motivation:** 本文旨在解决现有OOC错误检测方法过于侧重内部一致性而忽视图像-文本对与外部证据之间外部一致性的关键作用的问题。

**Method:** 本文提出了一种新颖的分层证据增强生成框架（HiEAG），专注于改进图像-文本对与外部证据之间的外部一致性检查。该方法利用多模态大语言模型（MLLMs）的广泛知识，并将外部一致性检查分解为一个包括检索、重排序和重写的综合引擎管道。重排序模块使用自动证据选择提示（AESP）从检索结果中获取相关证据项，重写模块采用自动证据生成提示（AEGP）改善MLLMs上基于OOC的错误信息检测任务的适应性。此外，该方法还为判断提供了解释，并通过指令微调实现了出色的表现性能。

**Result:** 实验结果表明，提出的HiEAG框架在各种基准数据集上实现了对所有样本的检测准确性超越了先前的最先进（SOTA）方法。

**Conclusion:** 研究证明，通过分层证据增强和多模态大语言模型的结合，可以有效地提高OOC错误检测的准确性并提供解释性判断。

**Abstract:** Recent advancements in multimodal out-of-context (OOC) misinformation detection have made remarkable progress in checking the consistencies between different modalities for supporting or refuting image-text pairs. However, existing OOC misinformation detection methods tend to emphasize the role of internal consistency, ignoring the significant of external consistency between image-text pairs and external evidence. In this paper, we propose HiEAG, a novel Hierarchical Evidence-Augmented Generation framework to refine external consistency checking through leveraging the extensive knowledge of multimodal large language models (MLLMs). Our approach decomposes external consistency checking into a comprehensive engine pipeline, which integrates reranking and rewriting, apart from retrieval. Evidence reranking module utilizes Automatic Evidence Selection Prompting (AESP) that acquires the relevant evidence item from the products of evidence retrieval. Subsequently, evidence rewriting module leverages Automatic Evidence Generation Prompting (AEGP) to improve task adaptation on MLLM-based OOC misinformation detectors. Furthermore, our approach enables explanation for judgment, and achieves impressive performance with instruction tuning. Experimental results on different benchmark datasets demonstrate that our proposed HiEAG surpasses previous state-of-the-art (SOTA) methods in the accuracy over all samples.

</details>


### [8] [Based on Data Balancing and Model Improvement for Multi-Label Sentiment Classification Performance Enhancement](https://arxiv.org/abs/2511.14073)
*Zijin Su,Huanzhu Lv,Yuren Niu,Yiming Liu*

Main category: cs.CL

> 本研究通过构建一个情感类别更均衡的多标签数据集，改进了多标签情感分类模型的架构，显著提升了情感分类效果。

<details>
  <summary>Details</summary>

**Motivation:** 本研究旨在解决多标签情感分类中数据集严重的类别不平衡问题，从而提高情感分类模型的整体性能，特别是在代表性不足的情感分类上的表现。

**Method:** 本研究通过整合GoEmotions数据集、使用RoBERTa-base-GoEmotions模型标注的Sentiment140情感标签样本，以及GPT-4 mini生成的手动标注文本，构建了一个平衡的多标签情感分类数据集。基于此数据集，开发了一个结合了预训练FastText嵌入、用于局部特征提取的卷积层、用于上下文学习的双向LSTM，以及强调情感相关词汇的注意力机制的增强型多标签分类模型。模型采用sigmoid激活输出层以实现多标签预测，并通过混合精度训练提高计算效率。

**Result:** 实验结果表明，本研究提出的增强型多标签分类模型在基于平衡数据集训练后，各方面性能指标都超过了使用不平衡数据集训练的模型。

**Conclusion:** 实验结果显示，与基于不平衡数据集训练的模型相比，本研究的模型在准确率、精确率、召回率、F1值和AUC上都有显著提升，突显了本研究方法的有效性。

**Abstract:** Multi-label sentiment classification plays a vital role in natural language processing by detecting multiple emotions within a single text. However, existing datasets like GoEmotions often suffer from severe class imbalance, which hampers model performance, especially for underrepresented emotions. To address this, we constructed a balanced multi-label sentiment dataset by integrating the original GoEmotions data, emotion-labeled samples from Sentiment140 using a RoBERTa-base-GoEmotions model, and manually annotated texts generated by GPT-4 mini. Our data balancing strategy ensured an even distribution across 28 emotion categories. Based on this dataset, we developed an enhanced multi-label classification model that combines pre-trained FastText embeddings, convolutional layers for local feature extraction, bidirectional LSTM for contextual learning, and an attention mechanism to highlight sentiment-relevant words. A sigmoid-activated output layer enables multi-label prediction, and mixed precision training improves computational efficiency. Experimental results demonstrate significant improvements in accuracy, precision, recall, F1-score, and AUC compared to models trained on imbalanced data, highlighting the effectiveness of our approach.

</details>


### [9] [Stealth Fine-Tuning: Efficiently Breaking Alignment in RVLMs Using Self-Generated CoT](https://arxiv.org/abs/2511.14106)
*Le Yu,Zhengyue Zhao,Yawen Zheng,Yunhao Liu*

Main category: cs.CL

> Error

<details>
  <summary>Details</summary>

**Motivation:** Error

**Method:** Error

**Result:** Error

**Conclusion:** Error

**Abstract:** Reasoning-augmented Vision-Language Models (RVLMs) rely on safety alignment to prevent harmful behavior, yet their exposed chain-of-thought (CoT) traces introduce new attack surfaces. In this work, we find that the safety alignment of RVLMs can be easily break through a novel attack method termed \textbf{Stealth Fine-Tuning}. Our method elicits harmful reasoning traces through \textbf{segment-level interference} and reuses the self-generated outputs as supervised fine-tuning data. Through a \textbf{turn-based weighted} loss design, yielding a lightweight, distribution-consistent finetuning method. In our experiment, with only 499 samples and under 3 hours on a single A100 (QLoRA), Stealth Fine-Tuning outperforms IDEATOR by 38.52\% ASR while preserving general reasoning ability, as the tuned model retains the original representation distribution. Experiments on AdvBench and several general benchmarks demonstrate that Stealth Fine-Tuning is a low-cost and highly effective way to bypass alignment defenses. \textcolor{red}{\textbf{Disclaimer: This paper contains content that may be disturbing or offensive.}}

</details>


### [10] [Synthetic Clinical Notes for Rare ICD Codes: A Data-Centric Framework for Long-Tail Medical Coding](https://arxiv.org/abs/2511.14112)
*Truong Vo,Weiyi Wu,Kaize Ding*

Main category: cs.CL

> 本研究提出了一种数据为中心的方法，通过合成出院总结来缓解ICD编码中的长尾问题，显著增强了模型在罕见代码预测中的性能，虽然增益幅度不大，但展示了精心设计的合成数据在增强公平性方面的潜力。

<details>
  <summary>Details</summary>

**Motivation:** 自动ICD编码从临床文本中提取是一项关键的医学自然语言处理任务，但由于诊断代码的极端长尾分布受到阻碍。MIMIC-III等数据集中大量的罕见的和零样本的ICD代码严重表示不足，导致较低的宏观F1分值。

**Method:** 本研究提出了一种数据为中心的框架，通过生成高质量的合成出院总结来缓解诊断代码的不平衡问题。该方法利用现实世界中的共现模式、ICD描述、同义词、分类学和相似的临床记录，构建以罕见代码为中心的现实多标签代码集。使用这些结构化的提示，生成了覆盖7,902个ICD代码的90,000份合成笔记，显著扩展了训练数据分布。

**Result:** 在实验中，使用原始和扩展数据集对两个最先进的基于变换器的模型进行微调后，本方法使宏观F1指标适度提升，同时保持强大的微观F1指标，超过先前的最先进方法。

**Conclusion:** 尽管相对于计算成本而言增益可能看似微小，但结果显示精心设计的合成数据可以提高长尾ICD代码预测的公平性。

**Abstract:** Automatic ICD coding from clinical text is a critical task in medical NLP but remains hindered by the extreme long-tail distribution of diagnostic codes. Thousands of rare and zero-shot ICD codes are severely underrepresented in datasets like MIMIC-III, leading to low macro-F1 scores. In this work, we propose a data-centric framework that generates high-quality synthetic discharge summaries to mitigate this imbalance. Our method constructs realistic multi-label code sets anchored on rare codes by leveraging real-world co-occurrence patterns, ICD descriptions, synonyms, taxonomy, and similar clinical notes. Using these structured prompts, we generate 90,000 synthetic notes covering 7,902 ICD codes, significantly expanding the training distribution. We fine-tune two state-of-the-art transformer-based models, PLM-ICD and GKI-ICD, on both the original and extended datasets. Experiments show that our approach modestly improves macro-F1 while maintaining strong micro-F1, outperforming prior SOTA. While the gain may seem marginal relative to the computational cost, our results demonstrate that carefully crafted synthetic data can enhance equity in long-tail ICD code prediction.

</details>


### [11] [From Graphs to Hypergraphs: Enhancing Aspect-Based Sentiment Analysis via Multi-Level Relational Modeling](https://arxiv.org/abs/2511.14142)
*Omkar Mahesh Kashyap,Padegal Amit,Madhav Kashyap,Ashwini M Joshi,Shylaja SS*

Main category: cs.CL

> 研究提出了HyperABSA，一种动态超图框架，用于改进基于方面的意见分析（ABSA）中的情绪极性预测，实验结果显示该框架相比现有图方法有显著提升。

<details>
  <summary>Details</summary>

**Motivation:** 现有的基于图的方法仅建模成对依赖关系，需要构建多个图来表示不同的关系视图，这会导致冗余、参数过多以及在融合过程中的误差传播，特别是在短文本和低资源环境下，表现尤为不利。

**Method:** 本研究提出了HyperABSA框架，该框架是一个动态超图模型，能通过样本特定的层次聚类诱导出方面意见结构。为了构建这些超边，还引入了一种新的加速回退截止方法，该方法自适应地确定粒度层次。

**Result:** 实验结果表明，在Lap14、Rest14和MAMS三个基准数据集上，该框架相比现有图方法表现有所提升，尤其是当与RoBERTa作为骨干模型结合时，改进尤为明显。

**Conclusion:** 实验结果表明，动态超图构建是一种有效且强大的ABSA技术，未来可能可以扩展到其他短文本自然语言处理任务中。

**Abstract:** Aspect-Based Sentiment Analysis (ABSA) predicts sentiment polarity for specific aspect terms, a task made difficult by conflicting sentiments across aspects and the sparse context of short texts. Prior graph-based approaches model only pairwise dependencies, forcing them to construct multiple graphs for different relational views. These introduce redundancy, parameter overhead, and error propagation during fusion, limiting robustness in short-text, low-resource settings. We present HyperABSA, a dynamic hypergraph framework that induces aspect-opinion structures through sample-specific hierarchical clustering. To construct these hyperedges, we introduce a novel acceleration-fallback cutoff for hierarchical clustering, which adaptively determines the level of granularity. Experiments on three benchmarks (Lap14, Rest14, MAMS) show consistent improvements over strong graph baselines, with substantial gains when paired with RoBERTa backbones. These results position dynamic hypergraph construction as an efficient, powerful alternative for ABSA, with potential extensions to other short-text NLP tasks.

</details>


### [12] [Applying Relation Extraction and Graph Matching to Answering Multiple Choice Questions](https://arxiv.org/abs/2511.14144)
*Naoki Shimoda,Akihiro Yamamoto*

Main category: cs.CL

> 该研究结合Transformer-based关系抽取与知识图谱匹配技术应用于解答选择题，并保持输出过程可追溯。

<details>
  <summary>Details</summary>

**Motivation:** 研究动机在于克服知识图谱构建成本高的问题，并利用Transformer-based关系抽取方法动态生成知识图谱，解决输入文本错误导致的关系图谱错误表示的问题。

**Method:** 通过将句子转换成关系图，并在封闭世界假设下与正确的知识图谱验证，以评判每个问题句子的真实性，从而解答填空格式的选择题。

**Result:** 实验结果显示，该方法能正确解答大约70%的问题，并提供过程的可追溯性。

**Conclusion:** 研究进一步表明，问题的类别对准确率有着显著的影响。

**Abstract:** In this research, we combine Transformer-based relation extraction with matching of knowledge graphs (KGs) and apply them to answering multiple-choice questions (MCQs) while maintaining the traceability of the output process. KGs are structured representations of factual knowledge consisting of entities and relations. Due to the high construction cost, they had been regarded as static databases with validated links. However, the recent development of Transformer-based relation extraction (RE) methods has enabled us to generate KGs dynamically by giving them natural language texts, and thereby opened the possibility for representing the meaning of the input sentences with the created KGs. Using this effect, we propose a method that answers MCQs in the "fill-in-the-blank" format, taking care of the point that RE methods generate KGs that represent false information if provided with factually incorrect texts. We measure the truthfulness of each question sentence by (i) converting the sentence into a relational graph using an RE method and (ii) verifying it against factually correct KGs under the closed-world assumption. The experimental results demonstrate that our method correctly answers up to around 70% of the questions, while providing traceability of the procedure. We also highlight that the question category has a vast influence on the accuracy.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [13] [nuCarla: A nuScenes-Style Bird's-Eye View Perception Dataset for CARLA Simulation](https://arxiv.org/abs/2511.13744)
*Zhijie Qiao,Zhong Cao,Henry X. Liu*

Main category: cs.CV

> 为了促进有意义中间表示的学习，比如鸟瞰视图特征，并实现可靠的端到端的自动驾驶模型，nuCarla数据集被提出，它为闭环模拟部署提供了直接可用的数据集。该数据集加速了闭环E2E开发，推动了自动驾驶的研究。

<details>
  <summary>Details</summary>

**Motivation:** 现有的大多数数据集是从非交互条件下收集的现实世界数据，主要支持开环学习，但对于闭环测试提供有限的价值。由于缺乏标准化、大规模且经过彻底验证的数据集，使有意义的中间表示学习和闭环端到端模型仍旧落后于简单的基于规则的基线。

**Method:** 介绍了一个nuCarla，这是一个大规模的数据集，基于nuScenes格式和CARLA仿真器构建设的BEV感知数据集。

**Result:** nuCarla提供数据和模型作为开放基准，拥有符合nuScenes格式的完全兼容性，让现实世界的感知模型无缝迁移，并且具有与nuScenes规模相比较但更平衡的类别分布。nuCarla还能直接使用于闭环仿真部署，提供了高性能的BEV骨干，达到了最先进的检测结果。

**Conclusion:** nuCarla加速了闭环端到端开发，为自动驾驶的研究铺平了道路，使其更加可靠和安全。

**Abstract:** End-to-end (E2E) autonomous driving heavily relies on closed-loop simulation, where perception, planning, and control are jointly trained and evaluated in interactive environments. Yet, most existing datasets are collected from the real world under non-interactive conditions, primarily supporting open-loop learning while offering limited value for closed-loop testing. Due to the lack of standardized, large-scale, and thoroughly verified datasets to facilitate learning of meaningful intermediate representations, such as bird's-eye-view (BEV) features, closed-loop E2E models remain far behind even simple rule-based baselines. To address this challenge, we introduce nuCarla, a large-scale, nuScenes-style BEV perception dataset built within the CARLA simulator. nuCarla features (1) full compatibility with the nuScenes format, enabling seamless transfer of real-world perception models; (2) a dataset scale comparable to nuScenes, but with more balanced class distributions; (3) direct usability for closed-loop simulation deployment; and (4) high-performance BEV backbones that achieve state-of-the-art detection results. By providing both data and models as open benchmarks, nuCarla substantially accelerates closed-loop E2E development, paving the way toward reliable and safety-aware research in autonomous driving.

</details>


### [14] [Known Meets Unknown: Mitigating Overconfidence in Open Set Recognition](https://arxiv.org/abs/2511.13775)
*Dongdong Zhao,Ranxin Fang,Changtian Song,Zhihui Liu,Jianwen Xiang*

Main category: cs.CV

> 研究提出了一种框架以解决开集识别（OSR）中的过拟合问题，通过两个组件控制预测不确定性和改进未知检测，实验验证了该框架优于现有的OSR方法。

<details>
  <summary>Details</summary>

**Motivation:** 现有的模型在处理语义上与已知类别相似的未知样本时，由于特征空间中的类间重叠，往往会导致对未知样本赋予过高的置信度而错误分类到已知类别，该现象称为过拟合。这模糊了已知类别和未知类别之间的决策边界，从而破坏了OSR。

**Method:** 该框架包含两个组件：基于扰动的不确定性估计模块和未知检测模块。不确定性估计模块通过可控参数扰动生成多样化预测并量化预测不确定性；未知检测模块通过两阶段过程利用估计的不确定性提高已知类别和未知类别之间的区分度，从而增强OSR性能。

**Result:** 实验结果显示所提出的框架在三个公共数据集上实现了优越的OSR性能。

**Conclusion:** 实验结果表明，所提出的框架在三个公共数据集上实现了优于现有OSR方法的性能。

**Abstract:** Open Set Recognition (OSR) requires models not only to accurately classify known classes but also to effectively reject unknown samples. However, when unknown samples are semantically similar to known classes, inter-class overlap in the feature space often causes models to assign unjustifiably high confidence to them, leading to misclassification as known classes -- a phenomenon known as overconfidence. This overconfidence undermines OSR by blurring the decision boundary between known and unknown classes. To address this issue, we propose a framework that explicitly mitigates overconfidence caused by inter-class overlap. The framework consists of two components: a perturbation-based uncertainty estimation module, which applies controllable parameter perturbations to generate diverse predictions and quantify predictive uncertainty, and an unknown detection module with distinct learning-based classifiers, implemented as a two-stage procedure, which leverages the estimated uncertainty to improve discrimination between known and unknown classes, thereby enhancing OSR performance. Experimental results on three public datasets show that the proposed framework achieves superior performance over existing OSR methods.

</details>


### [15] [Temporal Object-Aware Vision Transformer for Few-Shot Video Object Detection](https://arxiv.org/abs/2511.13784)
*Yogesh Kumar,Anand Mishra*

Main category: cs.CV

> 本文提出了一种新的基于时间感知的小样本视频目标检测方法，通过有选择地传播高置信度特征来解决目标检测中的时间和空间一致性，并在多个基准数据集上展示了优于现有方法的表现。

<details>
  <summary>Details</summary>

**Motivation:** 本文旨在解决基于视频的小样本目标检测问题，即如何在有限的标注样本下检测新对象，并保持帧间的时间一致性。传统的检测方法要求大量的训练数据，而在新的设定中，目标可能面临遮挡和外观变化，这对现有方法提出了挑战。

**Method:** 我们的方法通过引入一个筛选机制来处理时间上的一致性和特征传播，该机制可以有选择地传播具有高置信度的目标特征，从而提高在小样本场景下的检测准确性。这种方法避免了复杂的区域提议，并且不需要特定任务的训练。

**Result:** 该方法在FSVOD-500、FSYTV-40、VidOR和VidVRD数据集上实现了AP改进，分别提高了3.7%、5.3%、4.3%和4.5%。这些改进在1-shot、3-shot以及10-shot配置下都得到了验证。

**Conclusion:** 本文提出的小样本视频目标检测方法，通过引入高效的特征传播机制，实现了无需显式物体管状提案的时间一致性，并在不同小样本配置的数据集上验证了该方法的有效性。代码已公开。

**Abstract:** Few-shot Video Object Detection (FSVOD) addresses the challenge of detecting novel objects in videos with limited labeled examples, overcoming the constraints of traditional detection methods that require extensive training data. This task presents key challenges, including maintaining temporal consistency across frames affected by occlusion and appearance variations, and achieving novel object generalization without relying on complex region proposals, which are often computationally expensive and require task-specific training. Our novel object-aware temporal modeling approach addresses these challenges by incorporating a filtering mechanism that selectively propagates high-confidence object features across frames. This enables efficient feature progression, reduces noise accumulation, and enhances detection accuracy in a few-shot setting. By utilizing few-shot trained detection and classification heads with focused feature propagation, we achieve robust temporal consistency without depending on explicit object tube proposals. Our approach achieves performance gains, with AP improvements of 3.7% (FSVOD-500), 5.3% (FSYTV-40), 4.3% (VidOR), and 4.5 (VidVRD) in the 5-shot setting. Further results demonstrate improvements in 1-shot, 3-shot, and 10-shot configurations. We make the code public at: https://github.com/yogesh-iitj/fs-video-vit

</details>


### [16] [FusionFM: All-in-One Multi-Modal Image Fusion with Flow Matching](https://arxiv.org/abs/2511.13794)
*Huayi Zhu,Xiu Shu,Youqiang Xiong,Qiao Liu,Rui Chen,Di Yuan,Xiaojun Chang,Zhenyu He*

Main category: cs.CV

> 本文提出了一种新的多模态图像融合方法，通过直接概率传输和流匹配范式提升了采样效率，并通过伪标签和分而治之策略解决了融合质量的问题。

<details>
  <summary>Details</summary>

**Motivation:** 当前的多模态图像融合方法通常依赖任务特定模型，导致高昂的训练成本和有限的可扩展性。虽然生成方法提供了一个统一的建模视角，但它们往往由于从噪声到图像的复杂采样轨迹而采样缓慢。为了克服这些问题，我们提出了一种新的方法。

**Method:** 我们提出了将图像融合视为从源模态到融合图像分布的概率直接传输，利用流匹配范式来提高采样效率和结构一致性。为了缓解高质量融合图像监督不足的问题，我们收集了多个最先进的模型的融合结果作为先验，并使用任务感知的选择函数来选择每个任务最可靠的伪标签。我们还引入了一个融合精炼模块，采用分而治之策略，系统地识别、分解和增强选定伪标签中的降级组件。在多任务场景中，我们整合了弹性权重巩固和经验重新扮演机制，从而在参数稳定性和内存保存两个方面保持跨任务性能并增强了持续学习能力。

**Result:** 我们的方法在多样的融合任务上取得了具有竞争力的性能，同时显著提高了采样效率，并保持了轻量级模型的设计。

**Conclusion:** 本文提出的方法不仅可以提高采样效率和保持轻量级模型设计，还在多种融合任务上取得了竞争优势，展示了其在持续学习能力方面的提升。

**Abstract:** Current multi-modal image fusion methods typically rely on task-specific models, leading to high training costs and limited scalability. While generative methods provide a unified modeling perspective, they often suffer from slow inference due to the complex sampling trajectories from noise to image. To address this, we formulate image fusion as a direct probabilistic transport from source modalities to the fused image distribution, leveraging the flow matching paradigm to improve sampling efficiency and structural consistency. To mitigate the lack of high-quality fused images for supervision, we collect fusion results from multiple state-of-the-art models as priors, and employ a task-aware selection function to select the most reliable pseudo-labels for each task. We further introduce a Fusion Refiner module that employs a divide-and-conquer strategy to systematically identify, decompose, and enhance degraded components in selected pseudo-labels. For multi-task scenarios, we integrate elastic weight consolidation and experience replay mechanisms to preserve cross-task performance and enhance continual learning ability from both parameter stability and memory retention perspectives. Our approach achieves competitive performance across diverse fusion tasks, while significantly improving sampling efficiency and maintaining a lightweight model design. The code will be available at: https://github.com/Ist-Zhy/FusionFM.

</details>


### [17] [A Trajectory-free Crash Detection Framework with Generative Approach and Segment Map Diffusion](https://arxiv.org/abs/2511.13795)
*Weiying Shen,Hao Yu,Yu Dong,Pan Liu,Yu Han,Xin Wen*

Main category: cs.CV

> 研究提出了一种新的无需依赖轨迹的两阶段碰撞检测方法，该方法通过Mapfusion模型生成未来的路段地图，并通过对比实现碰撞检测，实验证明了其有效性。

<details>
  <summary>Details</summary>

**Motivation:** 实时碰撞检测对制定主动安全管理和提高交通效率至关重要，现有方法在轨迹获取和车辆跟踪方面存在局限性。

**Method:** 提出了一种无需轨迹的两阶段碰撞检测框架：第一阶段，Mapfusion模型通过逐步增加噪声并利用序列嵌入分量捕捉路段地图序列的时态动态来生成路段图；第二阶段，通过比较监控路段图与扩散模型生成的路段图来实现碰撞检测。

**Result:** 实验表明，该两阶段方法能够准确地检测实际的碰撞事件。

**Conclusion:** 研究显示，基于Mapfusion模型的两阶段碰撞检测方法不仅能够生成真实的路段演化地图，而且在不同抽样间隔下保持了检测的鲁棒性。

**Abstract:** Real-time crash detection is essential for developing proactive safety management strategy and enhancing overall traffic efficiency. To address the limitations associated with trajectory acquisition and vehicle tracking, road segment maps recording the individual-level traffic dynamic data were directly served in crash detection. A novel two-stage trajectory-free crash detection framework, was present to generate the rational future road segment map and identify crashes. The first-stage diffusion-based segment map generation model, Mapfusion, conducts a noisy-to-normal process that progressively adds noise to the road segment map until the map is corrupted to pure Gaussian noise. The denoising process is guided by sequential embedding components capturing the temporal dynamics of segment map sequences. Furthermore, the generation model is designed to incorporate background context through ControlNet to enhance generation control. Crash detection is achieved by comparing the monitored segment map with the generations from diffusion model in second stage. Trained on non-crash vehicle motion data, Mapfusion successfully generates realistic road segment evolution maps based on learned motion patterns and remains robust across different sampling intervals. Experiments on real-world crashes indicate the effectiveness of the proposed two-stage method in accurately detecting crashes.

</details>


### [18] [Synergizing Multigrid Algorithms with Vision Transformer: A Novel Approach to Enhance the Seismic Foundation Model](https://arxiv.org/abs/2511.13800)
*Huiwen Wu,Shuo Zhang,Yi Liu,Hongbin Ye*

Main category: cs.CV

> 提出了一个专门为地震数据设计的自适应双网格基础模型训练策略（ADATG），采用希尔伯特编码有效表示数据，并展示了其在利用高、低频信息方面的有效性。

<details>
  <summary>Details</summary>

**Motivation:** 现有的基于视觉的变压器忽略了地震数据中的固有模式，无法有效地捕捉高、低频地震信息。因此，需要一种新的方法来预训练适用于地震上下文的基础模型。

**Method:** 引入了一种新的自适应双网格基础模型训练策略（ADATG），使用希尔伯特编码，并采用频谱分解分离高、低频分量。同时采用适应性的训练策略，首先强调粗粒度的信息，然后逐步细化到细粒度的信息。

**Result:** 广泛的实验显示了训练方法的有效性和效率。

**Conclusion:** 强调了数据编码和训练策略对于提升视觉地震基础模型预训练的重要作用。

**Abstract:** Due to the emergency and homogenization of Artificial Intelligence (AI) technology development, transformer-based foundation models have revolutionized scientific applications, such as drug discovery, materials research, and astronomy. However, seismic data presents unique characteristics that require specialized processing techniques for pretraining foundation models in seismic contexts with high- and low-frequency features playing crucial roles. Existing vision transformers (ViTs) with sequential tokenization ignore the intrinsic pattern and fail to grasp both the high- and low-frequency seismic information efficiently and effectively. This work introduces a novel adaptive two-grid foundation model training strategy (ADATG) with Hilbert encoding specifically tailored for seismogram data, leveraging the hierarchical structures inherent in seismic data. Specifically, our approach employs spectrum decomposition to separate high- and low-frequency components and utilizes hierarchical Hilbert encoding to represent the data effectively. Moreover, observing the frequency principle observed in ViTs, we propose an adaptive training strategy that initially emphasizes coarse-level information and then progressively refines the model's focus on fine-level features. Our extensive experiments demonstrate the effectiveness and efficiency of our training methods. This research highlights the importance of data encoding and training strategies informed by the distinct characteristics of high- and low-frequency features in seismic images, ultimately contributing to the enhancement of visual seismic foundation models pretraining.

</details>


### [19] [Passive Dementia Screening via Facial Temporal Micro-Dynamics Analysis of In-the-Wild Talking-Head Video](https://arxiv.org/abs/2511.13802)
*Filippo Cenacchi. Longbing Cao,Mitchell McEwan,Deborah Richards*

Main category: cs.CV

> 通过分析短暂面部视频中面部微动态，实现在不依赖言语的条件下进行痴呆症被动筛查。

<details>
  <summary>Details</summary>

**Motivation:** 现有的筛查方式一般依赖于语言或有剧本的访谈，限制了其在诊所外的应用。

**Method:** 通过分析眨眼动态、微小嘴巴运动、目光变化和微调头部姿势等面部微动态来进行初步痴呆症筛查。

**Result:** 在YT DemTalk数据集上，轻量级浅分类器可达到AUROC 0.953，AP 0.961，F1-score 0.851，和0.857的准确率。

**Conclusion:** 目光不稳定性与口部/下颚动态是痴呆症筛查的最重要线索。该方法为痴呆症检测提供了一种新的无语言依赖的方法。

**Abstract:** We target passive dementia screening from short camera-facing talking head video, developing a facial temporal micro dynamics analysis for language free detection of early neuro cognitive change. This enables unscripted, in the wild video analysis at scale to capture natural facial behaviors, transferrable across devices, topics, and cultures without active intervention by clinicians or researchers during recording. Most existing resources prioritize speech or scripted interviews, limiting use outside clinics and coupling predictions to language and transcription. In contrast, we identify and analyze whether temporal facial kinematics, including blink dynamics, small mouth jaw motions, gaze variability, and subtle head adjustments, are sufficient for dementia screening without speech or text. By stabilizing facial signals, we convert these micro movements into interpretable facial microdynamic time series, smooth them, and summarize short windows into compact clip level statistics for screening. Each window is encoded by its activity mix (the relative share of motion across streams), thus the predictor analyzes the distribution of motion across streams rather than its magnitude, making per channel effects transparent. We also introduce YT DemTalk, a new dataset curated from publicly available, in the wild camera facing videos. It contains 300 clips (150 with self reported dementia, 150 controls) to test our model and offer a first benchmarking of the corpus. On YT DemTalk, ablations identify gaze lability and mouth/jaw dynamics as the most informative cues, and light weighted shallow classifiers could attain a dementia prediction performance of (AUROC) 0.953, 0.961 Average Precision (AP), 0.851 F1-score, and 0.857 accuracy.

</details>


### [20] [Can World Simulators Reason? Gen-ViRe: A Generative Visual Reasoning Benchmark](https://arxiv.org/abs/2511.13853)
*Xinxin Liu,Zhaopan Xu,Kai Wang,Yong Jae Lee,Yuzhang Shang*

Main category: cs.CV

> 本文介绍了Gen-ViRe，一个针对视频模型推理能力的评估框架，填补了现有评估方法的空白。

<details>
  <summary>Details</summary>

**Motivation:** 当前的基准测试无法评估CoF推理的核心认知能力，如多步骤规划、算法逻辑或抽象模式外推，这阻碍了对模型能力的系统理解。

**Method:** 通过引入Gen-ViRe（生成视觉推理基准），该框架将帧链（CoF）推理分解为六个认知维度和24个子任务，从而评估视频模型的推理能力。

**Result:** Gen-ViRe提供了对视频模型推理能力的首次定量评估，揭示了视觉质量和实际推理深度之间的显著差距。

**Conclusion:** 实验表明，尽管顶级系统的视觉质量令人印象深刻，但实际推理深度存在显著差异，这为提高视频模型的推理能力提供了基准和诊断工具。

**Abstract:** While Chain-of-Thought (CoT) prompting enables sophisticated symbolic reasoning in LLMs, it remains confined to discrete text and cannot simulate the continuous, physics-governed dynamics of the real world. Recent video generation models have emerged as potential world simulators through Chain-of-Frames (CoF) reasoning -- materializing thought as frame-by-frame visual sequences, with each frame representing a physically-grounded reasoning step. Despite compelling demonstrations, a challenge persists: existing benchmarks, focusing on fidelity or alignment, do not assess CoF reasoning and thus cannot measure core cognitive abilities in multi-step planning, algorithmic logic, or abstract pattern extrapolation. This evaluation void prevents systematic understanding of model capabilities and principled guidance for improvement. We introduce Gen-ViRe (Generative Visual Reasoning Benchmark), a framework grounded in cognitive science and real-world AI applications, which decomposes CoF reasoning into six cognitive dimensions -- from perceptual logic to abstract planning -- and 24 subtasks. Through multi-source data curation, minimal prompting protocols, and hybrid VLM-assisted evaluation with detailed criteria, Gen-ViRe delivers the first quantitative assessment of video models as reasoners. Our experiments on SOTA systems reveal substantial discrepancies between impressive visual quality and actual reasoning depth, establishing baselines and diagnostic tools to advance genuine world simulators.

</details>


### [21] [RSPose: Ranking Based Losses for Human Pose Estimation](https://arxiv.org/abs/2511.13857)
*Muhammed Can Keles,Bedrettin Cetinkaya,Sinan Kalkan,Emre Akbas*

Main category: cs.CV

> The paper proposes RSPose, a method using ranking-based losses that corrects issues in heatmap-based human pose estimation, achieving better mAP performance and surpassing state-of-the-art methods on multiple datasets.

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind this paper is to address the limitations of commonly used MSE loss in human pose estimation, such as lacking explicit focus on peak sharpening and localization, spatial and class-wise imbalance in heatmaps, and the discrepancy between the evaluation metric and loss functions.

**Method:** The paper introduces ranking-based losses to tackle the issues in heatmap-based human pose estimation, proposing a method, RSPose, that demonstrates superiority over conventional methods like MSE and KL-Divergence losses.

**Result:** Empirical and theoretical results show that the proposed ranking-based losses increase the correlation between confidence scores and localization qualities. This leads to more accurate instance selection during NMS and results in better mAP performance. RSPose outperforms existing methods by achieving the highest mAP score of 79.9 on the COCO-val set with ViTPose-H.

**Conclusion:** The paper concludes that their proposed RSPose method, which utilizes ranking-based losses, not only improves human pose estimation but also aligns the loss function with the evaluation metric (mAP). This approach is effective across different heatmap modes and datasets, and leads to a state-of-the-art mAP score.

**Abstract:** While heatmap-based human pose estimation methods have shown strong performance, they suffer from three main problems: (P1) "Commonly used Mean Squared Error (MSE)" Loss may not always improve joint localization because it penalizes all pixel deviations equally, without focusing explicitly on sharpening and correctly localizing the peak corresponding to the joint; (P2) heatmaps are spatially and class-wise imbalanced; and, (P3) there is a discrepancy between the evaluation metric (i.e., mAP) and the loss functions.
  We propose ranking-based losses to address these issues.
  Both theoretically and empirically, we show that our proposed losses are superior to commonly used heatmap losses (MSE, KL-Divergence). Our losses considerably increase the correlation between confidence scores and localization qualities, which is desirable because higher correlation leads to more accurate instance selection during Non-Maximum Suppression (NMS) and better Average Precision (mAP) performance. We refer to the models trained with our losses as RSPose.
  We show the effectiveness of RSPose across two different modes: one-dimensional and two-dimensional heatmaps, on three different datasets (COCO, CrowdPose, MPII).
  To the best of our knowledge, we are the first to propose losses that align with the evaluation metric (mAP) for human pose estimation.
  RSPose outperforms the previous state of the art on the COCO-val set and achieves an mAP score of 79.9 with ViTPose-H, a vision transformer model for human pose estimation.
  We also improve SimCC Resnet-50, a coordinate classification-based pose estimation method, by 1.5 AP on the COCO-val set, achieving 73.6 AP.

</details>
