{"id": "2509.03609", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.03609", "abs": "https://arxiv.org/abs/2509.03609", "authors": ["Shengkai Sun", "Zefan Zhang", "Jianfeng Dong", "Zhiyong Cheng", "Xiaojun Chang", "Meng Wang"], "title": "Towards Efficient General Feature Prediction in Masked Skeleton Modeling", "comment": "Accepted by ICCV 2025", "summary": "Recent advances in the masked autoencoder (MAE) paradigm have significantly\npropelled self-supervised skeleton-based action recognition. However, most\nexisting approaches limit reconstruction targets to raw joint coordinates or\ntheir simple variants, resulting in computational redundancy and limited\nsemantic representation. To address this, we propose a novel General Feature\nPrediction framework (GFP) for efficient mask skeleton modeling. Our key\ninnovation is replacing conventional low-level reconstruction with high-level\nfeature prediction that spans from local motion patterns to global semantic\nrepresentations. Specifically, we introduce a collaborative learning framework\nwhere a lightweight target generation network dynamically produces diversified\nsupervision signals across spatial-temporal hierarchies, avoiding reliance on\npre-computed offline features. The framework incorporates constrained\noptimization to ensure feature diversity while preventing model collapse.\nExperiments on NTU RGB+D 60, NTU RGB+D 120 and PKU-MMD demonstrate the benefits\nof our approach: Computational efficiency (with 6.2$\\times$ faster training\nthan standard masked skeleton modeling methods) and superior representation\nquality, achieving state-of-the-art performance in various downstream tasks.", "AI": {"tldr": "本文提出一种称为GFP的新框架，该框架用高级特征预测替代常规骨架重建，并通过实验表明确实在效率和表示质量上优于现有方法。", "motivation": "目前大多数自监督骨架行为识别方法仅限于对原始关节坐标或其简单变体进行重建，导致计算冗余和语义表示有限。为了解决这个问题，本文提出了一种新的方法来克服这些限制。", "method": "本文提出了一个称为通用特征预测（GFP）的新框架，用于高效的遮罩骨架建模。主要创新之处在于用从局部运动模式到全局语义表示的高级特征预测代替常规的低级重建。具体来说，引入了一个协作学习框架，其中轻量级的目标生成网络动态生成跨空间-时间层次的多样化监督信号，避免依赖预计算的离线特征。框架采用约束优化确保特征多样性同时防止模型崩溃。", "result": "在NTU RGB+D 60, NTU RGB+D 120 和 PKU-MMD数据集上的实验展示了该方法的优势：计算效率高（比标准的遮罩骨架建模方法快6.2倍）和表示质量更高，在各种下游任务中实现了最先进的性能。", "conclusion": "本研究显示了一种新的GFP框架的有效性，该框架在加速训练效率和提高表示质量方面显著优于现有的遮罩骨架建模方法。"}}
{"id": "2509.03614", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.03614", "abs": "https://arxiv.org/abs/2509.03614", "authors": ["Seungho Choe", "Xiaoli Qin", "Abubakr Shafique", "Amanda Dy", "Dimitri Androutsos", "Susan Done", "April Khademi"], "title": "Teacher-Student Model for Detecting and Classifying Mitosis in the MIDOG 2025 Challenge", "comment": "4 pages, 1 figures, final submission for MIDOG 2025 challenge", "summary": "Counting mitotic figures is time-intensive for pathologists and leads to\ninter-observer variability. Artificial intelligence (AI) promises a solution by\nautomatically detecting mitotic figures while maintaining decision consistency.\nHowever, AI tools are susceptible to domain shift, where a significant drop in\nperformance can occur due to differences in the training and testing sets,\nincluding morphological diversity between organs, species, and variations in\nstaining protocols. Furthermore, the number of mitoses is much less than the\ncount of normal nuclei, which introduces severely imbalanced data for the\ndetection task. In this work, we formulate mitosis detection as a pixel-level\nsegmentation and propose a teacher-student model that simultaneously addresses\nmitosis detection (Track 1) and atypical mitosis classification (Track 2). Our\nmethod is based on a UNet segmentation backbone that integrates domain\ngeneralization modules, namely contrastive representation learning and\ndomain-adversarial training. A teacher-student strategy is employed to generate\npixel-level pseudo-masks not only for annotated mitoses and hard negatives but\nalso for normal nuclei, thereby enhancing feature discrimination and improving\nrobustness against domain shift. For the classification task, we introduce a\nmulti-scale CNN classifier that leverages feature maps from the segmentation\nmodel within a multi-task learning paradigm. On the preliminary test set, the\nalgorithm achieved an F1 score of 0.7660 in Track 1 and balanced accuracy of\n0.8414 in Track 2, demonstrating the effectiveness of integrating\nsegmentation-based detection and classification into a unified framework for\nrobust mitosis analysis.", "AI": {"tldr": "本文提出了一种老师-学生模型，用以解决AI在有丝分裂检测中的领域转移问题，并在两项初步测试中取得了较为理想的成绩。", "motivation": "人工计算有丝分裂图像费时且存在观察者间的变差性。尽管人工智能有望通过自动检测有丝分裂提供解决方案，但AI工具易受领域转移的影响，导致性能显著下降，原因是训练和测试集之间的差异，包括各器官、物种的形态多样性以及染色协议的变化。此外，有丝分裂的数量远少于正常细胞核的数量，这为检测任务引入了严重的数据不平衡问题。为了解决这些问题，这项工作提出了一个新颖的方法。", "method": "本研究将有丝分裂检测定义为像素级分割问题，并提出了一种老师-学生模型，同时解决有丝分裂检测（Track 1）和非典型有丝分裂分类（Track 2）。该方法基于UNet分割主干网络，并集成了领域泛化模块，包括对比表示学习和领域对抗训练。采用老师-学生策略生成像素级伪掩膜，不仅用于标注的有丝分裂和难以识别的负面样本，还用于正常的细胞核，从而增强特征辨别能力和提高对领域转移的鲁棒性。对于分类任务，引入了多尺度CNN分类器，利用分割模型中的特征图在多任务学习框架中工作。", "result": "在初步测试集中，算法的Track 1（有丝分裂检测）F1得分为0.7660，而Track 2（非典型有丝分裂分类）的平衡准确率为0.8414，这表明所提出的方法对于应对有丝分裂检测中存在的领域转移问题和数据不平衡情况具有良好效果。", "conclusion": "此方法通过结合像素级分割和多尺度CNN分类器，在初步测试集中分别获得了Track 1 的F1值0.7660和Track 2的平衡精度0.8414，证明了将分割检测与分类整合到统一框架中的有效性，以及在稳健有丝分裂分析中的潜力。"}}
{"id": "2509.03616", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.03616", "abs": "https://arxiv.org/abs/2509.03616", "authors": ["Rajeev Ranjan Dwivedi", "Ankur Kumar", "Vinod K Kurmi"], "title": "Multi Attribute Bias Mitigation via Representation Learning", "comment": "ECAI 2025 (28th European Conference on Artificial Intelligence)", "summary": "Real world images frequently exhibit multiple overlapping biases, including\ntextures, watermarks, gendered makeup, scene object pairings, etc. These biases\ncollectively impair the performance of modern vision models, undermining both\ntheir robustness and fairness. Addressing these biases individually proves\ninadequate, as mitigating one bias often permits or intensifies others. We\ntackle this multi bias problem with Generalized Multi Bias Mitigation (GMBM), a\nlean two stage framework that needs group labels only while training and\nminimizes bias at test time. First, Adaptive Bias Integrated Learning (ABIL)\ndeliberately identifies the influence of known shortcuts by training encoders\nfor each attribute and integrating them with the main backbone, compelling the\nclassifier to explicitly recognize these biases. Then Gradient Suppression Fine\nTuning prunes those very bias directions from the backbone's gradients, leaving\na single compact network that ignores all the shortcuts it just learned to\nrecognize. Moreover we find that existing bias metrics break under subgroup\nimbalance and train test distribution shifts, so we introduce Scaled Bias\nAmplification (SBA): a test time measure that disentangles model induced bias\namplification from distributional differences. We validate GMBM on FB CMNIST,\nCelebA, and COCO, where we boost worst group accuracy, halve multi attribute\nbias amplification, and set a new low in SBA even as bias complexity and\ndistribution shifts intensify, making GMBM the first practical, end to end\nmultibias solution for visual recognition. Project page:\nhttp://visdomlab.github.io/GMBM/", "AI": {"tldr": "提出了GMBM框架，解决图像中的多种交叉偏见问题，通过一个两阶段框架实现。", "motivation": "现代视觉模型因图像中的多种交叉偏见（如纹理、水印、性别化妆、场景物体组合等）而受到影响，这些偏见共同削弱了模型的稳健性和公平性。单独处理每个偏见往往是不够的，因为这可能导致其他偏见的出现或增强。", "method": "使用了名为GMBM的两阶段框架解决多偏见问题，第一阶段是ABIL，通过训练每个属性的编码器并将其与主骨干网络集成，促使分类器显式识别这些偏见；第二阶段是梯度抑制微调，从骨干网络的梯度中修剪掉这些偏见的方向，最终得到一个忽略这些偏见的紧凑网络。", "result": "在FB CMNIST、CelebA和COCO上提升了最差组的精度，将多属性偏见放大减半，并显著降低了SBA指标，即使偏见的复杂性和分布偏移增加，也证明了GMBM是处理视觉识别中多偏见问题的首个实用、端到端解决方案。", "conclusion": "GMBM是一个减少测试时视觉模型多属性偏见的有效方法，同时引入了SBA指标，解决了现有偏见度量在子组不平衡和训练测试分布转移下的失效问题。"}}
{"id": "2509.03631", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.03631", "abs": "https://arxiv.org/abs/2509.03631", "authors": ["Anders Kjelsrud", "Lasse Løvstakken", "Erik Smistad", "Håvard Dalen", "Gilles Van De Vyver"], "title": "Lightweight image segmentation for echocardiography", "comment": "4 pages, 6 figures, The 2025 IEEE International Ultrasonics Symposium", "summary": "Accurate segmentation of the left ventricle in echocardiography can enable\nfully automatic extraction of clinical measurements such as volumes and\nejection fraction. While models configured by nnU-Net perform well, they are\nlarge and slow, thus limiting real-time use. We identified the most effective\ncomponents of nnU-Net for cardiac segmentation through an ablation study,\nincrementally evaluating data augmentation schemes, architectural\nmodifications, loss functions, and post-processing techniques. Our analysis\nrevealed that simple affine augmentations and deep supervision drive\nperformance, while complex augmentations and large model capacity offer\ndiminishing returns. Based on these insights, we developed a lightweight U-Net\n(2M vs 33M parameters) that achieves statistically equivalent performance to\nnnU-Net on CAMUS (N=500) with Dice scores of 0.93/0.85/0.89 vs 0.93/0.86/0.89\nfor LV/MYO/LA ($p>0.05$), while being 16 times smaller and 4 times faster\n(1.35ms vs 5.40ms per frame) than the default nnU-Net configuration.\nCross-dataset evaluation on an internal dataset (N=311) confirms comparable\ngeneralization.", "AI": {"tldr": "通过简化nnU-Net模型，开发了一个轻量级U-Net模型，该模型在CAMUS数据集上达到了与nnU-Net相似的性能，同时减少了模型参数和加快了处理速度。", "motivation": "nnU-Net模型虽然性能优秀，但体积大且运行速度慢，限制了其在实际场景中的应用。因此，为了开发一个可以实现实时自动提取临床测量指标且参数量少的模型，作者进行了本项研究。", "method": "Structure", "result": "{\"tldr\": \"通过简化nnU-Net模型，开发了一个轻量级U-Net模型，该模型在CAMUS数据集上达到了与nnU-Net相似的性能，同时减少了模型参数和加快了处理速度。\", \"motivation\": \"nnU-Net模型虽然性能优秀，但体积大且运行速度慢，限制了其在实际场景中的应用。因此，为了开发一个可以实现实时自动提取临床测量指标且参数量少的模型，作者进行了本项研究。\", \"method\": \"作者通过对数据增强策略、架构修改、损失函数及后处理技术的逐步评估，确定了对于心脏分割最有效的nnU-Net组件，基于此开发了一个轻量级的U-Net模型。\", \"result\": \"该模型在CAMUS数据集上取得了0.93/0.85/0.89的Dice评分，与nnU-Net相比没有显著差异，但模型更小（2M vs 33M）且速度更快（1.35ms vs 5.40ms）。\", \"conclusion\": \"简化后的模型在保持相近分割性能的同时，通过减少参数量和加快处理速度，更适合应用于实时环境下。\"}", "conclusion": "简化后的模型在保持相近分割性能的同时，通过减少参数量和加快处理速度，更适合应用于实时环境下。"}}
{"id": "2509.03525", "categories": ["cs.CL", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.03525", "abs": "https://arxiv.org/abs/2509.03525", "authors": ["Fatemeh Taherinezhad", "Mohamad Javad Momeni Nezhad", "Sepehr Karimi", "Sina Rashidi", "Ali Zolnour", "Maryam Dadkhah", "Yasaman Haghbin", "Hossein AzadMaleki", "Maryam Zolnoori"], "title": "Speech-Based Cognitive Screening: A Systematic Evaluation of LLM Adaptation Strategies", "comment": null, "summary": "Over half of US adults with Alzheimer disease and related dementias remain\nundiagnosed, and speech-based screening offers a scalable detection approach.\nWe compared large language model adaptation strategies for dementia detection\nusing the DementiaBank speech corpus, evaluating nine text-only models and\nthree multimodal audio-text models on recordings from DementiaBank speech\ncorpus. Adaptations included in-context learning with different demonstration\nselection policies, reasoning-augmented prompting, parameter-efficient\nfine-tuning, and multimodal integration. Results showed that class-centroid\ndemonstrations achieved the highest in-context learning performance, reasoning\nimproved smaller models, and token-level fine-tuning generally produced the\nbest scores. Adding a classification head substantially improved\nunderperforming models. Among multimodal models, fine-tuned audio-text systems\nperformed well but did not surpass the top text-only models. These findings\nhighlight that model adaptation strategies, including demonstration selection,\nreasoning design, and tuning method, critically influence speech-based dementia\ndetection, and that properly adapted open-weight models can match or exceed\ncommercial systems.", "AI": {"tldr": "Adaptation strategies for large language models significantly impact speech-based dementia detection performance. Properly adapted text-only models and audio-text models can match commercial systems in identifying undiagnosed dementia cases.", "motivation": "Over 50% of US adults with Alzheimer's and related dementias remain undiagnosed, making speech-based screening a promising scalable detection approach.", "method": "We compared nine text-only models and three multimodal audio-text models for dementia detection using the DementiaBank speech corpus, adapting them through in-context learning, reasoning-augmented prompting, parameter-efficient fine-tuning, and multimodal integration.", "result": "Class-centroid demonstrations yielded the best in-context learning performance, reasoning enhanced smaller models, and token-level fine-tuning generally resulted in the highest scores. Adding a classification head improved underperforming models. Fine-tuned audio-text systems performed well but not better than the top text-only models.", "conclusion": "Model adaptation strategies, including demonstration selection, reasoning design, and tuning methods, significantly impact speech-based dementia detection. Properly adapted open-weight models can match or exceed commercial systems in performance."}}
{"id": "2509.03633", "categories": ["cs.CV", "cs.AI", "I.4.6; I.5.2; I.2.10"], "pdf": "https://arxiv.org/pdf/2509.03633", "abs": "https://arxiv.org/abs/2509.03633", "authors": ["Josafat-Mattias Burmeister", "Andreas Tockner", "Stefan Reder", "Markus Engel", "Rico Richter", "Jan-Peter Mund", "Jürgen Döllner"], "title": "treeX: Unsupervised Tree Instance Segmentation in Dense Forest Point Clouds", "comment": null, "summary": "Close-range laser scanning provides detailed 3D captures of forest stands but\nrequires efficient software for processing 3D point cloud data and extracting\nindividual trees. Although recent studies have introduced deep learning methods\nfor tree instance segmentation, these approaches require large annotated\ndatasets and substantial computational resources. As a resource-efficient\nalternative, we present a revised version of the treeX algorithm, an\nunsupervised method that combines clustering-based stem detection with region\ngrowing for crown delineation. While the original treeX algorithm was developed\nfor personal laser scanning (PLS) data, we provide two parameter presets, one\nfor ground-based laser scanning (stationary terrestrial - TLS and PLS), and one\nfor UAV-borne laser scanning (ULS). We evaluated the method on six public\ndatasets (FOR-instance, ForestSemantic, LAUTx, NIBIO MLS, TreeLearn, Wytham\nWoods) and compared it to six open-source methods (original treeX, treeiso,\nRayCloudTools, ForAINet, SegmentAnyTree, TreeLearn). Compared to the original\ntreeX algorithm, our revision reduces runtime and improves accuracy, with\ninstance detection F$_1$-score gains of +0.11 to +0.49 for ground-based data.\nFor ULS data, our preset achieves an F$_1$-score of 0.58, whereas the original\nalgorithm fails to segment any correct instances. For TLS and PLS data, our\nalgorithm achieves accuracy similar to recent open-source methods, including\ndeep learning. Given its algorithmic design, we see two main applications for\nour method: (1) as a resource-efficient alternative to deep learning approaches\nin scenarios where the data characteristics align with the method design\n(sufficient stem visibility and point density), and (2) for the semi-automatic\ngeneration of labels for deep learning models. To enable broader adoption, we\nprovide an open-source Python implementation in the pointtree package.", "AI": {"tldr": "改进了treeX算法，使其更加适用于多种激光扫描数据，相比于原算法，减少了运行时间，提高了准确性。", "motivation": "针对深度学习方法需要大量标注数据和计算资源的问题，提出了一种资源高效的替代方案。", "method": "一种改进的treeX算法，结合基于聚类的主干检测和区域生长进行冠层划分，适用于不同类型激光扫描数据（地面站式TLS、个人激光扫描PLS和无人机激光扫描ULS），并通过两种参数预设进行优化。", "result": "在六种公开数据集上测试，改进后的算法相比原版treeX算法运行时间减少且准确度提高，在地面数据上的F1分数提高0.11到0.49；对于ULS数据，改进后的算法F1分数达0.58，而原算法无法正确分割。", "conclusion": "本文提出的算法在特定条件下可作为深度学习方法的资源高效替代，也可用于深度学习模型的半自动标签生成，已开源实现。"}}
{"id": "2509.03526", "categories": ["cs.CL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.03526", "abs": "https://arxiv.org/abs/2509.03526", "authors": ["Yansong Liu", "Jiateng Li", "Yuan Liu"], "title": "Enhancing Speech Large Language Models through Reinforced Behavior Alignment", "comment": null, "summary": "The recent advancements of Large Language Models (LLMs) have spurred\nconsiderable research interest in extending their linguistic capabilities\nbeyond text to other modalities, which leads to emergence of speech-based LLMs\n(SpeechLMs) with capability of processing user request in either speech or\ntextual formats. However, owing to inter-modal discrepancies, these SpeechLMs\nstill exhibit a significant performance gap compared to their text-based LLM\ncounterparts in instruction-following, particularly when confronted with the\ndynamic and variable nature of user speech. To address this challenge, this\npaper introduces a framework termed Reinforced Behavior Alignment (RBA),\ndesigned to bolster the language generation proficiency of SpeechLMs. Instead\nof relying on supervised fine-tuning from human annotations, RBA employs a\nself-synthesis methodology to generate extensive, high-fidelity alignment data\nby a powerful teacher LLM. Then SpeechLMs is aligned its behavior with that of\na teacher using a reinforcement learning-based approach. Experimental results\ndemonstrate that this method effectively enhances the instruction-following\ncapabilities of SpeechLMs that outperform conventional distillation baselines.\nCrucially, we demonstrate that RBA can be seamlessly extended to tasks such\nincluding spoken question answering and speech-to-text translation, attaining\nstate-of-the-art performance on open benchmarks with only self-generated data.", "AI": {"tldr": "本论文提出了一个强化行为对齐(RBA)框架，提高基于语音的语言模型在指令跟随任务上的性能，并展示了其在多种语音任务上的优越性。", "motivation": "由于跨模态差异，现有的基于语音的LLM在遵循指令方面仍然存在显著的性能差距，特别是在处理用户语音的动态变异性时。为了应对这一挑战，提出了一种新的框架。", "method": "引入了一个名为强化行为对齐(RBA)的框架，该框架采用自我合成的方法，通过强大的教师LLM生成大量高保真的对齐数据，而不是依赖于人类注释的监督微调。然后使用基于强化学习的方法使SpeechLM的行为与教师对齐。", "result": "实验结果表明，该方法有效提高了SpeechLM的指令跟随能力，超过了传统的蒸馏基线性能。此外，RBA方法可以无缝扩展到包括口语问答和语音到文本翻译在内的任务，并仅使用自生成数据在公开基准测试中达到了最先进的性能。", "conclusion": "RBA框架通过自我合成的方法和强化学习，显著提高了基于语音的LLM的语言生成能力，特别是在指令跟随任务上表现出色，并且在扩展到其他基于语音的任务中也达到了领先的性能水平。"}}
{"id": "2509.03635", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.03635", "abs": "https://arxiv.org/abs/2509.03635", "authors": ["Hongpei Zheng", "Lintao Xiang", "Qijun Yang", "Qian Lin", "Hujun Yin"], "title": "Reg3D: Reconstructive Geometry Instruction Tuning for 3D Scene Understanding", "comment": "16 pages, 6 figures", "summary": "The rapid development of Large Multimodal Models (LMMs) has led to remarkable\nprogress in 2D visual understanding; however, extending these capabilities to\n3D scene understanding remains a significant challenge. Existing approaches\npredominantly rely on text-only supervision, which fails to provide the\ngeometric constraints required for learning robust 3D spatial representations.\nIn this paper, we introduce Reg3D, a novel Reconstructive Geometry Instruction\nTuning framework that addresses this limitation by incorporating geometry-aware\nsupervision directly into the training process. Our key insight is that\neffective 3D understanding necessitates reconstructing underlying geometric\nstructures rather than merely describing them. Unlike existing methods that\ninject 3D information solely at the input level, Reg3D adopts a\ndual-supervision paradigm that leverages 3D geometric information both as input\nand as explicit learning targets. Specifically, we design complementary\nobject-level and frame-level reconstruction tasks within a dual-encoder\narchitecture, enforcing geometric consistency to encourage the development of\nspatial reasoning capabilities. Extensive experiments on ScanQA, Scan2Cap,\nScanRefer, and SQA3D demonstrate that Reg3D delivers substantial performance\nimprovements, establishing a new training paradigm for spatially aware\nmultimodal models.", "AI": {"tldr": "本文引入了Reg3D，一个新颖的重构几何指令调优框架，通过在训练过程中直接加入几何感知监督来提升3D场景理解能力。", "motivation": "现有方法在仅使用文本监督而缺乏所需几何约束的情况下难以有效学习鲁棒的3D空间表示。", "method": "Reg3D采用双重监督范式，在输入和学习目标两个层面利用3D几何信息，在双编码器架构中设计了互补的对象级和帧级重构任务，以强制几何一致性来促进空间推理能力的发展。", "result": "实验表明，本文方法在多个基准测试中展现了显著的性能提升，确立了多模态空间感知模型训练的新范式。", "conclusion": "引入Reg3D框架，有效解决了现有方法在3D场景理解方面的不足，通过重构几何结构而非仅仅描述它们，展示了在多任务上的显著性能提升。"}}
{"id": "2509.03527", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.03527", "abs": "https://arxiv.org/abs/2509.03527", "authors": ["Bohdan M. Pavlyshenko"], "title": "Multilevel Analysis of Cryptocurrency News using RAG Approach with Fine-Tuned Mistral Large Language Model", "comment": null, "summary": "In the paper, we consider multilevel multitask analysis of cryptocurrency\nnews using a fine-tuned Mistral 7B large language model with\nretrieval-augmented generation (RAG).\n  On the first level of analytics, the fine-tuned model generates graph and\ntext summaries with sentiment scores as well as JSON representations of\nsummaries. Higher levels perform hierarchical stacking that consolidates sets\nof graph-based and text-based summaries as well as summaries of summaries into\ncomprehensive reports. The combination of graph and text summaries provides\ncomplementary views of cryptocurrency news. The model is fine-tuned with 4-bit\nquantization using the PEFT/LoRA approach. The representation of cryptocurrency\nnews as knowledge graph can essentially eliminate problems with large language\nmodel hallucinations.\n  The obtained results demonstrate that the use of fine-tuned Mistral 7B LLM\nmodels for multilevel cryptocurrency news analysis can conduct informative\nqualitative and quantitative analytics, providing important insights.", "AI": {"tldr": "本文提出了一种多级多任务的加密货币新闻分析方法，使用了微调的Mistral 7B大语言模型，并表明该模型能够有效地提供有价值的新闻分析。", "motivation": "提高大语言模型分析加密货币新闻的能力，提供全面的分析，并通过将新闻表示为知识图谱来减少模型幻觉问题。", "method": "本文采用了一种基于检索增强生成（RAG）的微调Mistral 7B大语言模型，用于多级多任务的加密货币新闻分析。在第一级分析中，微调后的模型生成带有情感得分的图和文本摘要，并将其转换为JSON格式。更高层级则通过分层堆叠方式整合这些图基和文本基的摘要，形成全面的报告。", "result": "实验结果表明，使用微调后的Mistral 7B大语言模型进行多级加密货币新闻分析可以提供有信息量的定性和定量分析，并提供重要见解。", "conclusion": "利用微调的Mistral 7B大语言模型进行加密货币新闻分析，可以克服传统大语言模型的局限性，提供丰富的加密货币市场见解。"}}
{"id": "2509.03704", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.03704", "abs": "https://arxiv.org/abs/2509.03704", "authors": ["Seth Z. Zhao", "Huizhi Zhang", "Zhaowei Li", "Juntong Peng", "Anthony Chui", "Zewei Zhou", "Zonglin Meng", "Hao Xiang", "Zhiyu Huang", "Fujia Wang", "Ran Tian", "Chenfeng Xu", "Bolei Zhou", "Jiaqi Ma"], "title": "QuantV2X: A Fully Quantized Multi-Agent System for Cooperative Perception", "comment": null, "summary": "Cooperative perception through Vehicle-to-Everything (V2X) communication\noffers significant potential for enhancing vehicle perception by mitigating\nocclusions and expanding the field of view. However, past research has\npredominantly focused on improving accuracy metrics without addressing the\ncrucial system-level considerations of efficiency, latency, and real-world\ndeployability. Noticeably, most existing systems rely on full-precision models,\nwhich incur high computational and transmission costs, making them impractical\nfor real-time operation in resource-constrained environments. In this paper, we\nintroduce \\textbf{QuantV2X}, the first fully quantized multi-agent system\ndesigned specifically for efficient and scalable deployment of multi-modal,\nmulti-agent V2X cooperative perception. QuantV2X introduces a unified\nend-to-end quantization strategy across both neural network models and\ntransmitted message representations that simultaneously reduces computational\nload and transmission bandwidth. Remarkably, despite operating under low-bit\nconstraints, QuantV2X achieves accuracy comparable to full-precision systems.\nMore importantly, when evaluated under deployment-oriented metrics, QuantV2X\nreduces system-level latency by 3.2$\\times$ and achieves a +9.5 improvement in\nmAP30 over full-precision baselines. Furthermore, QuantV2X scales more\neffectively, enabling larger and more capable models to fit within strict\nmemory budgets. These results highlight the viability of a fully quantized\nmulti-agent intermediate fusion system for real-world deployment. The system\nwill be publicly released to promote research in this field:\nhttps://github.com/ucla-mobility/QuantV2X.", "AI": {"tldr": "QuantV2X是一个面向实时操作和资源受限环境的全量化的多代理系统，它在降低计算和传输成本的同时，提升了精度并减少了延迟，有效地解决了现实世界中车辆感知系统的关键问题。", "motivation": "提升车辆感知能力，减少遮挡并扩大视野，同时解决效率，延迟和现实世界部署的关键系统级问题。", "method": "采用了一种统一的端到端量化策略，应用于神经网络模型和传输的消息表示，同时减少了计算负载和传输带宽。", "result": "与全精度系统相比，尽管在低比特约束下，QuantV2X实现了可比的精度，但系统级延迟减少了3.2倍，mAP30提升了9.5。此外，它更有效地扩展，允许更大的模型在严格的内存预算内运行。", "conclusion": "全量化的多代理中间融合系统在现实世界中的部署是可行的。"}}
{"id": "2509.03528", "categories": ["cs.CL", "cs.CY", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.03528", "abs": "https://arxiv.org/abs/2509.03528", "authors": ["Matilde Contestabile", "Chiara Ferrara", "Alberto Giovannetti", "Giovanni Parrillo", "Andrea Vandin"], "title": "The ProLiFIC dataset: Leveraging LLMs to Unveil the Italian Lawmaking Process", "comment": null, "summary": "Process Mining (PM), initially developed for industrial and business\ncontexts, has recently been applied to social systems, including legal ones.\nHowever, PM's efficacy in the legal domain is limited by the accessibility and\nquality of datasets. We introduce ProLiFIC (Procedural Lawmaking Flow in\nItalian Chambers), a comprehensive event log of the Italian lawmaking process\nfrom 1987 to 2022. Created from unstructured data from the Normattiva portal\nand structured using large language models (LLMs), ProLiFIC aligns with recent\nefforts in integrating PM with LLMs. We exemplify preliminary analyses and\npropose ProLiFIC as a benchmark for legal PM, fostering new developments.", "AI": {"tldr": "文章介绍了一个名为ProLiFIC的意大利立法流程时间日志，旨在将过程挖掘应用于法律领域并解决数据质量问题，为法律过程挖掘提供了基准。", "motivation": "针对过程挖掘（PM）在法律领域应用中受限于数据集的可访问性和质量的问题，提出了ProLiFIC作为解决这一问题的方案。", "method": "介绍了ProLiFIC，这是一个从1987年到2022年意大利立法过程的全面事件日志。ProLiFIC是从Normattiva门户网站的非结构化数据中创建的，并使用大型语言模型（LLMs）进行了结构化处理。", "result": "初步分析表明ProLiFIC作为意大利立法过程的事件日志，是法律过程挖掘的有效数据源，并可作为未来研究的基准。", "conclusion": "ProLiFIC作为法律过程挖掘的研究基准，不仅展示了过程挖掘在法律领域的应用潜力，也为该领域的进一步研究和发展奠定了基础。"}}
{"id": "2509.03729", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.03729", "abs": "https://arxiv.org/abs/2509.03729", "authors": ["Bandita Bharadwaj", "Ankur Mishra", "Saurav Bharadwaj"], "title": "Transfer Learning-Based CNN Models for Plant Species Identification Using Leaf Venation Patterns", "comment": null, "summary": "This study evaluates the efficacy of three deep learning architectures:\nResNet50, MobileNetV2, and EfficientNetB0 for automated plant species\nclassification based on leaf venation patterns, a critical morphological\nfeature with high taxonomic relevance. Using the Swedish Leaf Dataset\ncomprising images from 15 distinct species (75 images per species, totalling\n1,125 images), the models were demonstrated using standard performance metrics\nduring training and testing phases. ResNet50 achieved a training accuracy of\n94.11% but exhibited overfitting, reflected by a reduced testing accuracy of\n88.45% and an F1 score of 87.82%. MobileNetV2 demonstrated better\ngeneralization capabilities, attaining a testing accuracy of 93.34% and an F1\nscore of 93.23%, indicating its suitability for lightweight, real-time\napplications. EfficientNetB0 outperformed both models, achieving a testing\naccuracy of 94.67% with precision, recall, and F1 scores exceeding 94.6%,\nhighlighting its robustness in venation-based classification. The findings\nunderscore the potential of deep learning, particularly EfficientNetB0, in\ndeveloping scalable and accurate tools for automated plant taxonomy using\nvenation traits.", "AI": {"tldr": "这项研究评估了三种深度学习模型在基于叶脉模式的植物分类中的效果，发现EfficientNetB0在测试准确率等方面表现最佳，表明深度学习技术在自动化植物分类中的潜力。", "motivation": "研究动机是评估深度学习技术在基于叶脉模式的植物种类分类中的应用潜力，这是植物分类学中一个重要但计算复杂的问题。", "method": "该研究评估了三种深度学习架构（ResNet50、MobileNetV2 和 EfficientNetB0）在基于叶脉模式的植物种类自动分类中的有效性。研究使用了包含15种不同植物物种的瑞典叶数据集进行模型训练和测试，每个物种有75张图像，总共1,125张图像。", "result": "ResNet50的训练准确率为94.11%，但由于过拟合导致测试准确率降低到88.45%。MobileNetV2在测试中准确率为93.34%，表现了较好的泛化能力。EfficientNetB0表现最佳，测试准确率高达94.67%，并具有超过94.6%的精度、召回率和F1分数。", "conclusion": "结果表明深度学习模型，尤其是EfficientNetB0，有能力提供可扩展和准确的自动化植物分类工具。"}}
{"id": "2509.03529", "categories": ["cs.CL", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2509.03529", "abs": "https://arxiv.org/abs/2509.03529", "authors": ["Alejandro Álvarez Castro", "Joaquín Ordieres-Meré"], "title": "Multimodal Proposal for an AI-Based Tool to Increase Cross-Assessment of Messages", "comment": "Presented at NLMLT2025 (https://airccse.org/csit/V15N16.html), 15\n  pages, 5 figures", "summary": "Earnings calls represent a uniquely rich and semi-structured source of\nfinancial communication, blending scripted managerial commentary with\nunscripted analyst dialogue. Although recent advances in financial sentiment\nanalysis have integrated multi-modal signals, such as textual content and vocal\ntone, most systems rely on flat document-level or sentence-level models,\nfailing to capture the layered discourse structure of these interactions. This\npaper introduces a novel multi-modal framework designed to generate\nsemantically rich and structurally aware embeddings of earnings calls, by\nencoding them as hierarchical discourse trees. Each node, comprising either a\nmonologue or a question-answer pair, is enriched with emotional signals derived\nfrom text, audio, and video, as well as structured metadata including coherence\nscores, topic labels, and answer coverage assessments. A two-stage transformer\narchitecture is proposed: the first encodes multi-modal content and discourse\nmetadata at the node level using contrastive learning, while the second\nsynthesizes a global embedding for the entire conference. Experimental results\nreveal that the resulting embeddings form stable, semantically meaningful\nrepresentations that reflect affective tone, structural logic, and thematic\nalignment. Beyond financial reporting, the proposed system generalizes to other\nhigh-stakes unscripted communicative domains such as tele-medicine, education,\nand political discourse, offering a robust and explainable approach to\nmulti-modal discourse representation. This approach offers practical utility\nfor downstream tasks such as financial forecasting and discourse evaluation,\nwhile also providing a generalizable method applicable to other domains\ninvolving high-stakes communication.", "AI": {"tldr": "引入一种多模态框架，用于生成收益电话会议的语义丰富和结构清晰的嵌入式表示，能够捕捉高风险沟通的情感基调、结构逻辑和主题一致性。", "motivation": "目前的财务情绪分析系统依赖于单一层面的模型，未能捕捉收益通话的多层语篇结构。", "method": "采用分层语篇树来编码收益电话会议，每个节点包含独白或问答对，并通过对比学习结合多模态内容和语篇元数据。第二阶段的Transformer架构则合成整个会议的全局嵌入。", "result": "实验结果显示生成的嵌入可形成长期稳定、语义相关的表示，能够反映情感基调、结构逻辑和主题一致性。", "conclusion": "所提出的方法不仅适用于金融报告，还可推广到其他包括远程医疗、教育、政治讨论等在内的高风险非剧本沟通领域。"}}
{"id": "2509.03737", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.03737", "abs": "https://arxiv.org/abs/2509.03737", "authors": ["Casper van Engelenburg", "Jan van Gemert", "Seyran Khademi"], "title": "LayoutGKN: Graph Similarity Learning of Floor Plans", "comment": "BMVC (2025)", "summary": "Floor plans depict building layouts and are often represented as graphs to\ncapture the underlying spatial relationships. Comparison of these graphs is\ncritical for applications like search, clustering, and data visualization. The\nmost successful methods to compare graphs \\ie, graph matching networks, rely on\ncostly intermediate cross-graph node-level interactions, therefore being slow\nin inference time. We introduce \\textbf{LayoutGKN}, a more efficient approach\nthat postpones the cross-graph node-level interactions to the end of the joint\nembedding architecture. We do so by using a differentiable graph kernel as a\ndistance function on the final learned node-level embeddings. We show that\nLayoutGKN computes similarity comparably or better than graph matching networks\nwhile significantly increasing the speed.\n\\href{https://github.com/caspervanengelenburg/LayoutGKN}{Code and data} are\nopen.", "AI": {"tldr": "LayoutGKN通过推迟跨图节点级交互至联合嵌入结构的最后，利用可微分图核进行距离计算，从而在保持相似性计算准确性的同时提升了速度。", "motivation": "现有的图比较方法，如图匹配网络，依赖于代价高昂的跨图节点级交互，因此在推理时间上较慢。为了提高效率，研究者提出了LayoutGKN。", "method": "使用可微分图核作为距离函数，在最后的学习节点级嵌入上进行跨图节点级交互，以提高效率。这种方法与图匹配网络相比，在保持或提高相似性计算准确性的同时，显著提升了推理速度。", "result": "LayoutGKN在相似性计算上与图匹配网络相当或更好，同时在推理速度上显著提升。", "conclusion": "LayoutGKN通过创新的方法在效率和准确性之间取得了良好的平衡，提供了更快的图比较解决方案。"}}
{"id": "2509.03530", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.03530", "abs": "https://arxiv.org/abs/2509.03530", "authors": ["Paul Blum", "Enrico Liscio", "Ruixuan Zhang", "Caroline Figueroa", "Pradeep K. Murukannaiah"], "title": "Reading Between the Signs: Predicting Future Suicidal Ideation from Adolescent Social Media Texts", "comment": null, "summary": "Suicide is a leading cause of death among adolescents (12-18), yet predicting\nit remains a significant challenge. Many cases go undetected due to a lack of\ncontact with mental health services. Social media, however, offers a unique\nopportunity, as young people often share their thoughts and struggles online in\nreal time. In this work, we propose a novel task and method to approach it:\npredicting suicidal ideation and behavior (SIB) from forum posts before an\nadolescent explicitly expresses suicidal ideation on an online forum. This\npredictive framing, where no self-disclosure is used as input at any stage,\nremains largely unexplored in the suicide prediction literature. To this end,\nwe introduce Early-SIB, a transformer-based model that sequentially processes\nthe posts a user writes and engages with to predict whether they will write a\nSIB post. Our model achieves a balanced accuracy of 0.73 for predicting future\nSIB on a Dutch youth forum, demonstrating that such tools can offer a\nmeaningful addition to traditional methods.", "AI": {"tldr": "通过分析青少年在在线论坛上的帖子，研究提出了一种早期预测自杀意念和行为的方法。该模型名为Early-SIB，它基于变压器架构，通过处理用户撰写的和互动的帖子，来预测他们是否将发布带有自杀意念的内容，在荷兰青年论坛中达到了0.73的平衡准确率。", "motivation": "由于许多青少年自杀案例未被现有心理健康服务及时发现，这项研究旨在利用社会媒体这一渠道，通过模型来预测青少年的自杀风险，以提高早期识别和干预的可能性。", "method": "Structure", "result": "{\"tldr\": \"通过分析青少年在在线论坛上的帖子，研究提出了一种早期预测自杀意念和行为的方法。该模型名为Early-SIB，它基于变压器架构，通过处理用户撰写的和互动的帖子，来预测他们是否将发布带有自杀意念的内容，在荷兰青年论坛中达到了0.73的平衡准确率。\", \"motivation\": \"由于许多青少年自杀案例未被现有心理健康服务及时发现，这项研究旨在利用社会媒体这一渠道，通过模型来预测青少年的自杀风险，以提高早期识别和干预的可能性。\", \"method\": \"研究使用了一种名为Early-SIB的基于变压器的模型。该模型通过分析用户在论坛上的帖子和互动，预测他们是否会在未来发布包含自杀意念的内容。\", \"result\": \"在荷兰一个青少年用的在线论坛上，该模型达到了0.73的平衡准确率，这表明该模型能够对传统方法进行有意义的补充。\", \"conclusion\": \"这项研究展示了利用社交媒体数据预测自杀意念和行为的潜力，表明这种工具可以作为传统心理健康方法的补充，帮助更早识别风险。\"}", "conclusion": "这项研究展示了利用社交媒体数据预测自杀意念和行为的潜力，表明这种工具可以作为传统心理健康方法的补充，帮助更早识别风险。"}}
{"id": "2509.03740", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.03740", "abs": "https://arxiv.org/abs/2509.03740", "authors": ["Taha Koleilat", "Hassan Rivaz", "Yiming Xiao"], "title": "Singular Value Few-shot Adaptation of Vision-Language Models", "comment": "10 pages, 2 figures, 8 tables", "summary": "Vision-language models (VLMs) like CLIP have shown impressive zero-shot and\nfew-shot learning capabilities across diverse applications. However, adapting\nthese models to new fine-grained domains remains difficult due to reliance on\nprompt engineering and the high cost of full model fine-tuning. Existing\nadaptation approaches rely on augmented components, such as prompt tokens and\nadapter modules, which could limit adaptation quality, destabilize the model,\nand compromise the rich knowledge learned during pretraining. In this work, we\npresent \\textbf{CLIP-SVD}, a novel \\textit{multi-modal} and\n\\textit{parameter-efficient} adaptation technique that leverages Singular Value\nDecomposition (SVD) to modify the internal parameter space of CLIP without\ninjecting additional modules. Specifically, we fine-tune only the singular\nvalues of the CLIP parameter matrices to rescale the basis vectors for domain\nadaptation while retaining the pretrained model. This design enables enhanced\nadaptation performance using only \\textbf{0.04\\%} of the model's total\nparameters and better preservation of its generalization ability. CLIP-SVD\nachieves state-of-the-art classification results on 11 natural and 10\nbiomedical datasets, outperforming previous methods in both accuracy and\ngeneralization under few-shot settings. Additionally, we leverage a natural\nlanguage-based approach to analyze the effectiveness and dynamics of the CLIP\nadaptation to allow interpretability of CLIP-SVD. The code is publicly\navailable at https://github.com/HealthX-Lab/CLIP-SVD.", "AI": {"tldr": ".CLIP-SVD 是一种新颖的多模态和参数高效的适应技术，利用SVD修改CLIP的内部参数空间，仅调整CLIP参数矩阵的奇异值进行领域适应，使用模型总参数的0.04%达到了增强的适应性能。在自然和生物医学数据集上实现了最先进的分类结果，优于之前的准确性与泛化性方法。", "motivation": "解决CLIP在新细粒度领域的适应困难，避免额外组件的限制和预训练知识的损失。", "method": "通过奇异值分解（SVD）技术，仅调整CLIP参数矩阵的奇异值，不注入额外模块进行领域适应。", "result": "CLIP-SVD 在自然和生物医学数据集上实现了最先进的分类结果，优于现有方法的准确性和泛化性。", "conclusion": "CLIP-SVD 提供了一种参数高效的方法，可在不损失预训练性能的情况下改善领域适应。"}}
{"id": "2509.03531", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.03531", "abs": "https://arxiv.org/abs/2509.03531", "authors": ["Oscar Obeso", "Andy Arditi", "Javier Ferrando", "Joshua Freeman", "Cameron Holmes", "Neel Nanda"], "title": "Real-Time Detection of Hallucinated Entities in Long-Form Generation", "comment": null, "summary": "Large language models are now routinely used in high-stakes applications\nwhere hallucinations can cause serious harm, such as medical consultations or\nlegal advice. Existing hallucination detection methods, however, are\nimpractical for real-world use, as they are either limited to short factual\nqueries or require costly external verification. We present a cheap, scalable\nmethod for real-time identification of hallucinated tokens in long-form\ngenerations, and scale it effectively to 70B parameter models. Our approach\ntargets \\emph{entity-level hallucinations} -- e.g., fabricated names, dates,\ncitations -- rather than claim-level, thereby naturally mapping to token-level\nlabels and enabling streaming detection. We develop an annotation methodology\nthat leverages web search to annotate model responses with grounded labels\nindicating which tokens correspond to fabricated entities. This dataset enables\nus to train effective hallucination classifiers with simple and efficient\nmethods such as linear probes. Evaluating across four model families, our\nclassifiers consistently outperform baselines on long-form responses, including\nmore expensive methods such as semantic entropy (e.g., AUC 0.90 vs 0.71 for\nLlama-3.3-70B), and are also an improvement in short-form question-answering\nsettings. Moreover, despite being trained only with entity-level labels, our\nprobes effectively detect incorrect answers in mathematical reasoning tasks,\nindicating generalization beyond entities. While our annotation methodology is\nexpensive, we find that annotated responses from one model can be used to train\neffective classifiers on other models; accordingly, we publicly release our\ndatasets to facilitate reuse. Overall, our work suggests a promising new\napproach for scalable, real-world hallucination detection.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2509.03754", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.03754", "abs": "https://arxiv.org/abs/2509.03754", "authors": ["Zongsen Qiu"], "title": "STA-Net: A Decoupled Shape and Texture Attention Network for Lightweight Plant Disease Classification", "comment": null, "summary": "Responding to rising global food security needs, precision agriculture and\ndeep learning-based plant disease diagnosis have become crucial. Yet, deploying\nhigh-precision models on edge devices is challenging. Most lightweight networks\nuse attention mechanisms designed for generic object recognition, which poorly\ncapture subtle pathological features like irregular lesion shapes and complex\ntextures. To overcome this, we propose a twofold solution: first, using a\ntraining-free neural architecture search method (DeepMAD) to create an\nefficient network backbone for edge devices; second, introducing the\nShape-Texture Attention Module (STAM). STAM splits attention into two branches\n-- one using deformable convolutions (DCNv4) for shape awareness and the other\nusing a Gabor filter bank for texture awareness. On the public CCMT plant\ndisease dataset, our STA-Net model (with 401K parameters and 51.1M FLOPs)\nreached 89.00% accuracy and an F1 score of 88.96%. Ablation studies confirm\nSTAM significantly improves performance over baseline and standard attention\nmodels. Integrating domain knowledge via decoupled attention thus presents a\npromising path for edge-deployed precision agriculture AI. The source code is\navailable at https://github.com/RzMY/STA-Net.", "AI": {"tldr": "本论文提出了一种称为STA-Net的新模型，在无需大量训练的情况下，通过细化的神经架构搜索和特殊的形状-纹理注意力模块，实现了高效与高精度的植物疾病诊断，适用于边缘设备部署。", "motivation": "鉴于局部病变形状和复杂纹理等微妙病理性特征无法通过现有的轻量级网络中的注意力机制捕捉，为了提高精简农业中的植物疾病诊断精度，特别是为了将高精度模型部署到边缘设备上，本论文提出了一个有特别设计的解决方法。", "method": "本论文提出了一种两阶段解决方案：首先，采用无训练神经架构搜索方法（DeepMAD）创建适用于边缘设备的高效网络骨干；其次，引入形状-纹理注意力模块（STAM），该模块将注意力分为两支，一支使用可变形卷积（DCNv4）用于形状感知，另一支使用Gabor滤波器组用于纹理感知。", "result": "实验结果表明，在CCMT植物疾病数据集上，STA-Net模型（含401K参数和51.1M FLOPs）达到了89.00%的准确率和88.96%的F1分数，证明STAM显著提高了性能。", "conclusion": "通过引入特定设计的注意力模块并集成领域知识，本论文为在边缘设备上部署精准农业AI提供了有前景的路径。"}}
{"id": "2509.03533", "categories": ["cs.CL", "cs.LG", "q-fin.GN"], "pdf": "https://arxiv.org/pdf/2509.03533", "abs": "https://arxiv.org/abs/2509.03533", "authors": ["Igor Halperin"], "title": "Topic Identification in LLM Input-Output Pairs through the Lens of Information Bottleneck", "comment": "26 pages, 4 figures", "summary": "Large Language Models (LLMs) are prone to critical failure modes, including\n\\textit{intrinsic faithfulness hallucinations} (also known as confabulations),\nwhere a response deviates semantically from the provided context. Frameworks\ndesigned to detect this, such as Semantic Divergence Metrics (SDM), rely on\nidentifying latent topics shared between prompts and responses, typically by\napplying geometric clustering to their sentence embeddings. This creates a\ndisconnect, as the topics are optimized for spatial proximity, not for the\ndownstream information-theoretic analysis. In this paper, we bridge this gap by\ndeveloping a principled topic identification method grounded in the\nDeterministic Information Bottleneck (DIB) for geometric clustering. Our key\ncontribution is to transform the DIB method into a practical algorithm for\nhigh-dimensional data by substituting its intractable KL divergence term with a\ncomputationally efficient upper bound. The resulting method, which we dub UDIB,\ncan be interpreted as an entropy-regularized and robustified version of K-means\nthat inherently favors a parsimonious number of informative clusters. By\napplying UDIB to the joint clustering of LLM prompt and response embeddings, we\ngenerate a shared topic representation that is not merely spatially coherent\nbut is fundamentally structured to be maximally informative about the\nprompt-response relationship. This provides a superior foundation for the SDM\nframework and offers a novel, more sensitive tool for detecting confabulations.", "AI": {"tldr": "本文提出了一种新的基于DIB的几何聚类方法(UDIB)，用于产生更连贯且信息量更大的主题表示，以改进检测大语言模型的语义偏离问题。", "motivation": "传统的语义差异度量(SDM)框架在检测语言模型中的固有忠实体裁幻觉时依赖于几何聚类，这导致了一个问题，即这些主题是为优化空间邻近性而设计的，而不适用于下游的信息理论分析。", "method": "通过引入一种基于确定性信息瓶颈(DIB)的方法来改进几何聚类，解决了传统方法中主题优化侧重于空间邻近性的问题。这种方法能生成不仅是空间连贯的，而且在提示和响应关系上更具信息性的共享主题表示。", "result": "通过将DIB方法转化为适用于高维数据的实用算法，我们得到的UDIB方法可以解释为正则化和强化的K-means，这有利于形成简洁的信息性聚类，从而更好地捕捉提示和响应之间的关系。", "conclusion": "提出的方法改进了现有的SDM框架，提供了一种更敏感的工具来检测固有忠实体裁幻觉，从而增强了语言模型的可靠性。"}}
{"id": "2509.03786", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.03786", "abs": "https://arxiv.org/abs/2509.03786", "authors": ["Xinxin Wang", "Han Sun", "Ningzhong Liu", "Huiyu Zhou", "Yinan Yao"], "title": "SLENet: A Guidance-Enhanced Network for Underwater Camouflaged Object Detection", "comment": "14pages, accepted by PRCV2025", "summary": "Underwater Camouflaged Object Detection (UCOD) aims to identify objects that\nblend seamlessly into underwater environments. This task is critically\nimportant to marine ecology. However, it remains largely underexplored and\naccurate identification is severely hindered by optical distortions, water\nturbidity, and the complex traits of marine organisms. To address these\nchallenges, we introduce the UCOD task and present DeepCamo, a benchmark\ndataset designed for this domain. We also propose Semantic Localization and\nEnhancement Network (SLENet), a novel framework for UCOD. We first benchmark\nstate-of-the-art COD models on DeepCamo to reveal key issues, upon which SLENet\nis built. In particular, we incorporate Gamma-Asymmetric Enhancement (GAE)\nmodule and a Localization Guidance Branch (LGB) to enhance multi-scale feature\nrepresentation while generating a location map enriched with global semantic\ninformation. This map guides the Multi-Scale Supervised Decoder (MSSD) to\nproduce more accurate predictions. Experiments on our DeepCamo dataset and\nthree benchmark COD datasets confirm SLENet's superior performance over SOTA\nmethods, and underscore its high generality for the broader COD task.", "AI": {"tldr": "论文介绍了一种新颖的深度学习架构SLENet，用于在水下复杂环境下检测伪装物体，并通过新的基准数据集DeepCamo证明了其相对于现有方法的优越性。", "motivation": "为了应对水下环境中由于光学畸变、水体浑浊和复杂生物特性导致的伪装物体检测准确性降低问题，我们定义了UCOD任务，并构建了专为此任务设计的DeepCamo基准数据集。", "method": "我们提出了SLENet，一种用于Underwater Camouflaged Object Detection (UCOD)的新框架。SLENet集成了Gamma-Asymmetric Enhancement (GAE)模块和Localization Guidance Branch (LGB)来增强多尺度特征表示，生成包含全球语义信息的位置图，以指导Multi-Scale Supervised Decoder (MSSD)生成更准确的预测结果。", "result": "实验显示，SLENet显著优于现有最强的COD模型，并在DeepCamo数据集及其他三个基准COD数据集上实现了更准确的伪装物体检测。", "conclusion": "实验结果证明，SLENet在DeepCamo数据集及三个基准COD数据集上均展现了优越性能，并且对于更广泛的COD任务具有很高的通用性。"}}
{"id": "2509.03535", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.03535", "abs": "https://arxiv.org/abs/2509.03535", "authors": ["Ahmed Mubarak", "Amna Ahmed", "Amira Nasser", "Aya Mohamed", "Fares El-Sadek", "Mohammed Ahmed", "Ahmed Salah", "Youssef Sobhy"], "title": "QuesGenie: Intelligent Multimodal Question Generation", "comment": "7 pages, 8 figures, 12 tables. Supervised by Dr. Ahmed Salah and TA\n  Youssef Sobhy", "summary": "In today's information-rich era, learners have access to abundant educational\nresources, but the lack of practice materials tailored to these resources\npresents a significant challenge. This project addresses that gap by developing\na multi-modal question generation system that can automatically generate\ndiverse question types from various content formats. The system features four\nmajor components: multi-modal input handling, question generation,\nreinforcement learning from human feedback (RLHF), and an end-to-end\ninteractive interface. This project lays the foundation for automated,\nscalable, and intelligent question generation, carefully balancing resource\nefficiency, robust functionality and a smooth user experience.", "AI": {"tldr": "该项目开发了一种多模态的自动出题系统，能够从各种内容格式中自动生成多种类型的问题，从而解决了实践中缺乏练习材料的问题。", "motivation": "尽管在信息丰富的时代学生可以接触到丰富的教育资源，但缺乏与这些资源相配套的练习材料是一个显著的挑战。此项目正是为了解决这一问题。", "method": "提出了一种多模态的自动出题系统，该系统可以从不同格式的内容自动生成多种类型的题目，并包含四个主要组件：多模态输入处理、题目生成、基于人类反馈的强化学习（RLHF）和端到端的交互界面。", "result": "该项目构建了自动化的、可扩展且智能的题目生成系统的基础，该系统在资源效率、功能稳健性和用户友好性方面取得了平衡。", "conclusion": "此系统为教育资源的个性化和有效利用提供了可能，是自动出题领域的重要进展。"}}
{"id": "2509.03794", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.03794", "abs": "https://arxiv.org/abs/2509.03794", "authors": ["Juhun Lee", "Simon S. Woo"], "title": "Fitting Image Diffusion Models on Video Datasets", "comment": "ICCV25 Workshop", "summary": "Image diffusion models are trained on independently sampled static images.\nWhile this is the bedrock task protocol in generative modeling, capturing the\ntemporal world through the lens of static snapshots is information-deficient by\ndesign. This limitation leads to slower convergence, limited distributional\ncoverage, and reduced generalization. In this work, we propose a simple and\neffective training strategy that leverages the temporal inductive bias present\nin continuous video frames to improve diffusion training. Notably, the proposed\nmethod requires no architectural modification and can be seamlessly integrated\ninto standard diffusion training pipelines. We evaluate our method on the\nHandCo dataset, where hand-object interactions exhibit dense temporal coherence\nand subtle variations in finger articulation often result in semantically\ndistinct motions. Empirically, our method accelerates convergence by over\n2$\\text{x}$ faster and achieves lower FID on both training and validation\ndistributions. It also improves generative diversity by encouraging the model\nto capture meaningful temporal variations. We further provide an optimization\nanalysis showing that our regularization reduces the gradient variance, which\ncontributes to faster convergence.", "AI": {"tldr": "研究通过引入时间归纳偏置的训练策略改进了图像扩散模型的训练效果。", "motivation": "现有的图像扩散模型训练基于独立采样的静态图像，这在捕获时间世界时信息不足。该方法旨在解决这一问题，提高训练速度、分布覆盖和模型泛化能力。", "method": "该研究提出了一种训练策略，利用连续视频帧中的时间归纳偏置来改进扩散模型的训练。该方法不需要修改架构，可以无缝集成到标准的扩散训练管道中。", "result": "实验结果表明，该方法在HandCo数据集上将收敛速度提高了超过2倍，并在训练和验证分布上实现了更低的FID值。此外，它还通过鼓励模型捕捉有意义的时间变化，提高了生成的多样性。优化分析显示，该正则化方法减少了梯度方差，有助于提高收敛速度。", "conclusion": "该研究提出的方法有效提高了图像扩散模型的训练效率和性能，展示了时间归纳偏置在扩散模型中的潜力。"}}
{"id": "2509.03537", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.03537", "abs": "https://arxiv.org/abs/2509.03537", "authors": ["Cheng-Kai Yeh", "Hsing-Wang Lee", "Chung-Hung Kuo", "Hen-Hsen Huang"], "title": "AR$^2$: Adversarial Reinforcement Learning for Abstract Reasoning in Large Language Models", "comment": "7 pages, accepted by CIKM 2025 as a short paper", "summary": "Abstraction--the ability to recognize and distill essential computational\npatterns from complex problem statements--is a foundational skill in computer\nscience, critical both for human problem-solvers and coding-oriented large\nlanguage models (LLMs). Despite recent advances in training LLMs for code\ngeneration using reinforcement learning (RL), most existing approaches focus\nprimarily on superficial pattern recognition, overlooking explicit training for\nabstraction. In this study, we propose AR$^2$ (Adversarial Reinforcement\nLearning for Abstract Reasoning), a novel framework explicitly designed to\nenhance the abstraction abilities of LLMs. AR$^2$ employs a teacher model to\ntransform kernel problems into narrative-rich, challenging descriptions without\nchanging their fundamental logic. Simultaneously, a student coding model is\ntrained to solve these complex narrative problems by extracting their\nunderlying computational kernels. Experimental results demonstrate that AR$^2$\nsubstantially improves the student model's accuracy on previously unseen,\nchallenging programming tasks, underscoring abstraction as a key skill for\nenhancing LLM generalization.", "AI": {"tldr": "本研究提出AR$^2$框架，通过教师模型复杂的叙述性问题转换与学生模型从这些问题中抽取基本计算内核的方法，来提升大语言模型的抽象能力，从而提高其解决新颖编程任务的能力。", "motivation": "尽管在使用强化学习训练大语言模型进行代码生成方面取得了进展，但大多数现有方法主要集中在表面模式识别上，而忽视了对抽象性的显式训练。本研究旨在通过AR$^2$框架提高大语言模型的抽象能力。", "method": "AR$^2$（对抗强化学习用于抽象推理）是一种新型框架，旨在提高大语言模型的抽象能力。该框架采用教师模型将内核问题转换为叙述丰富、复杂的描述，而不改变其基本逻辑。同时，对学生编程模型进行训练，通过提取复杂叙述问题中的基本计算内核来解决问题。", "result": "实验结果表明，AR$^2$显著提高了学生模型在未曾见过的复杂编程任务上的准确性，表明抽象能力是提高大语言模型泛化能力的关键技能。", "conclusion": "实验结果证实了AR$^2$在提高大语言模型解决未见过的复杂编程任务准确性方面的有效性，强调了抽象能力在提高模型泛化能力中的关键作用。"}}
{"id": "2509.03800", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.03800", "abs": "https://arxiv.org/abs/2509.03800", "authors": ["Yuheng Li", "Yenho Chen", "Yuxiang Lai", "Jike Zhong", "Vanessa Wildman", "Xiaofeng Yang"], "title": "MedVista3D: Vision-Language Modeling for Reducing Diagnostic Errors in 3D CT Disease Detection, Understanding and Reporting", "comment": null, "summary": "Radiologic diagnostic errors-under-reading errors, inattentional blindness,\nand communication failures-remain prevalent in clinical practice. These issues\noften stem from missed localized abnormalities, limited global context, and\nvariability in report language. These challenges are amplified in 3D imaging,\nwhere clinicians must examine hundreds of slices per scan. Addressing them\nrequires systems with precise localized detection, global volume-level\nreasoning, and semantically consistent natural language reporting. However,\nexisting 3D vision-language models are unable to meet all three needs jointly,\nlacking local-global understanding for spatial reasoning and struggling with\nthe variability and noise of uncurated radiology reports. We present\nMedVista3D, a multi-scale semantic-enriched vision-language pretraining\nframework for 3D CT analysis. To enable joint disease detection and holistic\ninterpretation, MedVista3D performs local and global image-text alignment for\nfine-grained representation learning within full-volume context. To address\nreport variability, we apply language model rewrites and introduce a Radiology\nSemantic Matching Bank for semantics-aware alignment. MedVista3D achieves\nstate-of-the-art performance on zero-shot disease classification, report\nretrieval, and medical visual question answering, while transferring well to\norgan segmentation and prognosis prediction. Code and datasets will be\nreleased.", "AI": {"tldr": "MedVista3D是一个用于3D CT分析的多尺度语义增强视觉-语言预训练框架，能够很好地解决疾病检测、整体解释和语义连贯报告的问题，表现出色。", "motivation": "现有的3D视觉-语言模型在满足疾病检测、整体解释和语义连贯的自然语言报告三个方面的需求方面有局限。文章旨在解决临床实践中影像诊断错误的问题，如遗漏局部异常、缺乏全局背景和报告语言的变异性这些问题。这些问题在3D成像中被放大，因为临床医生必须检查每张扫描中的数百个切片。", "method": "MedVista3D采用多尺度语义增强的视觉-语言预训练框架，用于3D CT分析。它在全体积上下文中进行局部和全局图像-文本对齐，以实现疾病的检测和整体解释。为了应对报告的变异性，它应用语言模型重写并引入放射学语义匹配银行以实现语义感知的对齐。", "result": "MedVista3D在零样本疾病分类、报告检索和医学视觉问题回答方面达到了最先进的性能，并且很好地转移到器官分割和预后预测。", "conclusion": "MedVista3D作为一个多尺度语义增强的视觉-语言预训练框架，有效解决了3D影像诊断中的问题，提高了诊断准确性和报告的一致性，展示了其在医学影像分析中的广泛应用潜力。"}}
{"id": "2509.03540", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.03540", "abs": "https://arxiv.org/abs/2509.03540", "authors": ["Shanglin Wu", "Lihui Liu", "Jinho D. Choi", "Kai Shu"], "title": "Improving Factuality in LLMs via Inference-Time Knowledge Graph Construction", "comment": null, "summary": "Large Language Models (LLMs) often struggle with producing factually\nconsistent answers due to limitations in their parametric memory.\nRetrieval-Augmented Generation (RAG) methods address this issue by\nincorporating external knowledge from trusted sources at inference time.\nHowever, such methods typically treat knowledge as unstructured text, which\nlimits their ability to support compositional reasoning and identify factual\ninconsistencies. To overcome these limitations, we propose a novel framework\nthat dynamically constructs and expands knowledge graphs (KGs) during\ninference, integrating both internal knowledge extracted from LLMs and external\ninformation retrieved from external sources. Our method begins by extracting a\nseed KG from the question via prompting, followed by iterative expansion using\nthe LLM's latent knowledge. The graph is then selectively refined through\nexternal retrieval, enhancing factual coverage and correcting inaccuracies. We\nevaluate our approach on three diverse factual QA benchmarks, demonstrating\nconsistent improvements in factual accuracy, answer precision, and\ninterpretability over baseline prompting and static KG-augmented methods. Our\nfindings suggest that inference-time KG construction is a promising direction\nfor enhancing LLM factuality in a structured, interpretable, and scalable\nmanner.", "AI": {"tldr": "该论文提出了一种在推理过程中动态构建和扩展知识图谱的框架，以提高大语言模型生成事实一致性答案的能力。", "motivation": "大语言模型在生成一致性答案方面存在困难，主要是因为它们受到参数化记忆的限制。现有的检索增强生成方法只能将知识视为非结构化文本，这限制了它们支持组合推理和识别事实不一致性的能力。", "method": "Structure", "result": "{\n  \"tldr\": \"该论文提出了一种在推理过程中动态构建和扩展知识图谱的框架，以提高大语言模型生成事实一致性答案的能力。\", \n  \"motivation\": \"大语言模型在生成一致性答案方面存在困难，主要是因为它们受到参数化记忆的限制。现有的检索增强生成方法只能将知识视为非结构化文本，这限制了它们支持组合推理和识别事实不一致性的能力。\", \n  \"method\": \"该框架通过从问题中提取一个初始知识图谱，并使用大语言模型的潜在知识进行迭代扩展，再通过外部检索选择性地优化这个图谱，以提高事实覆盖范围和纠正不准确信息。\", \n  \"result\": \"该方法在三个不同的事实问答基准测试中显示出在事实准确性、答案精度和可解释性方面比基准提示法和静态知识图谱增强方法有持续改进。\", \n  \"conclusion\": \"此研究提出，推理时构建知识图谱是一个增强大语言模型事实性的有前途方向，同时也是结构化、可解释和可扩展的方式。\"}\n}", "conclusion": "此研究提出，推理时构建知识图谱是一个增强大语言模型事实性的有前途方向，同时也是结构化、可解释和可扩展的方式。"}}
{"id": "2509.03803", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.03803", "abs": "https://arxiv.org/abs/2509.03803", "authors": ["Mengyu Gao", "Qiulei Dong"], "title": "Causality-guided Prompt Learning for Vision-language Models via Visual Granulation", "comment": "ICCV 2025 Accepted", "summary": "Prompt learning has recently attracted much attention for adapting\npre-trained vision-language models (e.g., CLIP) to downstream recognition\ntasks. However, most of the existing CLIP-based prompt learning methods only\nshow a limited ability for handling fine-grained datasets. To address this\nissue, we propose a causality-guided text prompt learning method via visual\ngranulation for CLIP, called CaPL, where the explored visual granulation\ntechnique could construct sets of visual granules for the text prompt to\ncapture subtle discrepancies among different fine-grained classes through\ncasual inference. The CaPL method contains the following two modules: (1) An\nattribute disentanglement module is proposed to decompose visual features into\nnon-individualized attributes (shared by some classes) and individualized\nattributes (specific to single classes) using a Brownian Bridge Diffusion\nModel; (2) A granule learning module is proposed to construct visual granules\nby integrating the aforementioned attributes for recognition under two causal\ninference strategies. Thanks to the learned visual granules, more\ndiscriminative text prompt is expected to be learned. Extensive experimental\nresults on 15 datasets demonstrate that our CaPL method significantly\noutperforms the state-of-the-art prompt learning methods, especially on\nfine-grained datasets.", "AI": {"tldr": "CaPL方法通过将视觉特征分解并构建视觉颗粒，增强了基于CLIP的提示学习在细粒度识别任务中的能力，优于现有的方法。", "motivation": "现有的大多数基于CLIP的提示学习方法在处理细粒度数据集方面能力有限。为了应对这一挑战，提出了一种通过视觉分粒化引导因果关系的文本提示学习方法，称为CaPL。", "method": "CaPL方法包含两个模块：(1) 属性解耦模块使用布朗桥扩散模型将视觉特征分解为非个性化属性（由某些类别共享）和个性化属性（特定于单个类别）；(2) 颗粒学习模块通过两种因果推理策略整合上述属性来构建视觉颗粒，以促进识别。", "result": "CaPL方法在15个数据集上的实验结果显著优于现有的提示学习方法，特别是在细粒度数据集上。", "conclusion": "大量的实验结果证明，CaPL方法在15个数据集上的表现显著优于现有的提示学习方法，尤其是在细粒度数据集上。"}}
{"id": "2509.03565", "categories": ["cs.CL", "cs.MM"], "pdf": "https://arxiv.org/pdf/2509.03565", "abs": "https://arxiv.org/abs/2509.03565", "authors": ["Qi Chen", "Jingxuan Wei", "Zhuoya Yao", "Haiguang Wang", "Gaowei Wu", "Bihui Yu", "Siyuan Li", "Cheng Tan"], "title": "ResearchPulse: Building Method-Experiment Chains through Multi-Document Scientific Inference", "comment": "Accepted to ACM MM 2025", "summary": "Understanding how scientific ideas evolve requires more than summarizing\nindividual papers-it demands structured, cross-document reasoning over\nthematically related research. In this work, we formalize multi-document\nscientific inference, a new task that extracts and aligns motivation,\nmethodology, and experimental results across related papers to reconstruct\nresearch development chains. This task introduces key challenges, including\ntemporally aligning loosely structured methods and standardizing heterogeneous\nexperimental tables. We present ResearchPulse, an agent-based framework that\nintegrates instruction planning, scientific content extraction, and structured\nvisualization. It consists of three coordinated agents: a Plan Agent for task\ndecomposition, a Mmap-Agent that constructs motivation-method mind maps, and a\nLchart-Agent that synthesizes experimental line charts. To support this task,\nwe introduce ResearchPulse-Bench, a citation-aware benchmark of annotated paper\nclusters. Experiments show that our system, despite using 7B-scale agents,\nconsistently outperforms strong baselines like GPT-4o in semantic alignment,\nstructural consistency, and visual fidelity. The dataset are available in\nhttps://huggingface.co/datasets/ResearchPulse/ResearchPulse-Bench.", "AI": {"tldr": "本文介绍了一种用于跨文档科学推理的新任务和方法，以及一种名为ResearchPulse的系统，该系统由三个代理组成，能够整合多个相关论文的动机、方法和实验结果，重构研究发展链。", "motivation": "理解科学思想的演化需要超越对单篇论文的总结，需要进行有结构的、交叉文献推理。本文提出了一种新的任务——跨文档科学推理，旨在提取并对接相关论文的动机、方法和实验结果，以重构研究发展链。", "method": "本研究提出了一种名为ResearchPulse的基于代理的框架，该框架集成了指令规划、科学内容提取和结构化可视化。它由三个协调的代理组成：用于任务分解的计划代理，用于绘制动机-方法思维导图的Mmap代理，以及用于合成实验图表的Lchart代理。", "result": "实验结果显示，尽管使用的是7B规模的代理，该系统在语义对齐、结构一致性、和可视化准确性方面始终优于像GPT-4o这样的强基线系统。", "conclusion": "本研究介绍了ResearchPulse系统及其支持的新任务，提供了ResearchPulse-Bench数据集，这对研究科学文献的演化提供了一种新的工具和方法。"}}
{"id": "2509.03808", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.03808", "abs": "https://arxiv.org/abs/2509.03808", "authors": ["Huanan Li", "Rui Fan", "Juntao Guan", "Weidong Hao", "Lai Rui", "Tong Wu", "Yikai Wang", "Lin Gu"], "title": "EGTM: Event-guided Efficient Turbulence Mitigation", "comment": null, "summary": "Turbulence mitigation (TM) aims to remove the stochastic distortions and\nblurs introduced by atmospheric turbulence into frame cameras. Existing\nstate-of-the-art deep-learning TM methods extract turbulence cues from multiple\ndegraded frames to find the so-called \"lucky'', not distorted patch, for \"lucky\nfusion''. However, it requires high-capacity network to learn from\ncoarse-grained turbulence dynamics between synchronous frames with limited\nframe-rate, thus fall short in computational and storage efficiency. Event\ncameras, with microsecond-level temporal resolution, have the potential to\nfundamentally address this bottleneck with efficient sparse and asynchronous\nimaging mechanism. In light of this, we (i) present the fundamental\n\\textbf{``event-lucky insight''} to reveal the correlation between turbulence\ndistortions and inverse spatiotemporal distribution of event streams. Then,\nbuild upon this insight, we (ii) propose a novel EGTM framework that extracts\npixel-level reliable turbulence-free guidance from the explicit but noisy\nturbulent events for temporal lucky fusion. Moreover, we (iii) build the first\nturbulence data acquisition system to contribute the first real-world\nevent-driven TM dataset. Extensive experimental results demonstrate that our\napproach significantly surpass the existing SOTA TM method by 710 times, 214\ntimes and 224 times in model size, inference latency and model complexity\nrespectively, while achieving the state-of-the-art in restoration quality\n(+0.94 PSNR and +0.08 SSIM) on our real-world EGTM dataset. This demonstrating\nthe great efficiency merit of introducing event modality into TM task. Demo\ncode and data have been uploaded in supplementary material and will be released\nonce accepted.", "AI": {"tldr": "EGTM uses event cameras for efficient, high-resolution temporal lucky fusion, outperforming SOTA methods in model size, inference latency, model complexity, and restoration quality.", "motivation": "To overcome the limitations of existing TM methods in computational and storage efficiency caused by the need for high-capacity networks and the coarse-grained turbulence dynamics between synchronous frames.", "method": "We propose EGTM, a novel framework that leverages event cameras' capabilities to extract turbulence-free patches using the 'event-lucky insight', which correlates turbulence distortions with inverse spatiotemporal event stream distribution.", "result": "We show EGTM surpasses current SOTA methods 710 times in model size, 214 times in inference latency, and 224 times in model complexity, while also improving restoration quality by +0.94 PSNR and +0.08 SSIM on a real-world dataset.", "conclusion": "Introducing event cameras into the TM task provides significant efficiency gains and superior restoration quality, marking an important advancement in TM technology."}}
{"id": "2509.03610", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.03610", "abs": "https://arxiv.org/abs/2509.03610", "authors": ["Josh Wisoff", "Yao Tang", "Zhengyu Fang", "Jordan Guzman", "YuTang Wang", "Alex Yu"], "title": "NoteBar: An AI-Assisted Note-Taking System for Personal Knowledge Management", "comment": null, "summary": "Note-taking is a critical practice for capturing, organizing, and reflecting\non information in both academic and professional settings. The recent success\nof large language models has accelerated the development of AI-assisted tools,\nyet existing solutions often struggle with efficiency. We present NoteBar, an\nAI-assisted note-taking tool that leverages persona information and efficient\nlanguage models to automatically organize notes into multiple categories and\nbetter support user workflows. To support research and evaluation in this\nspace, we further introduce a novel persona-conditioned dataset of 3,173 notes\nand 8,494 annotated concepts across 16 MBTI personas, offering both diversity\nand semantic richness for downstream tasks. Finally, we demonstrate that\nNoteBar can be deployed in a practical and cost-effective manner, enabling\ninteractive use without reliance on heavy infrastructure. Together, NoteBar and\nits accompanying dataset provide a scalable and extensible foundation for\nadvancing AI-assisted personal knowledge management.", "AI": {"tldr": "NoteBar是一个AI辅助的笔记工具，通过利用人格信息和高效的语言模型，能够自动将笔记组织成多个类别，从而更好地支持用户的工作流程。研究还引入了一个包含3,173个笔记和8,494个标注概念的新数据集，涵盖了16种MBTI人格类型。", "motivation": "当前的AI辅助笔记工具在效率方面存在不足，因此需要开发一种可以更高效和个性化地帮助用户管理和组织笔记的工具。", "method": "开发NoteBar工具，该工具利用人格信息和高效的语言模型来自动将笔记组织成多个类别。同时，建立了一个条件化人格的数据集来支持研究和评估。", "result": "NoteBar可以被部署为一种实际且成本效益高的方式，支持无需依赖重型基础设施的互动使用。", "conclusion": "NoteBar及其配套数据集为推进AI辅助个人知识管理提供了一个可扩展的和可扩展的基础。"}}
{"id": "2509.03872", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.03872", "abs": "https://arxiv.org/abs/2509.03872", "authors": ["Nan Yang", "Yang Wang", "Zhanwen Liu", "Yuchao Dai", "Yang Liu", "Xiangmo Zhao"], "title": "Focus Through Motion: RGB-Event Collaborative Token Sparsification for Efficient Object Detection", "comment": null, "summary": "Existing RGB-Event detection methods process the low-information regions of\nboth modalities (background in images and non-event regions in event data)\nuniformly during feature extraction and fusion, resulting in high computational\ncosts and suboptimal performance. To mitigate the computational redundancy\nduring feature extraction, researchers have respectively proposed token\nsparsification methods for the image and event modalities. However, these\nmethods employ a fixed number or threshold for token selection, hindering the\nretention of informative tokens for samples with varying complexity. To achieve\na better balance between accuracy and efficiency, we propose FocusMamba, which\nperforms adaptive collaborative sparsification of multimodal features and\nefficiently integrates complementary information. Specifically, an Event-Guided\nMultimodal Sparsification (EGMS) strategy is designed to identify and\nadaptively discard low-information regions within each modality by leveraging\nscene content changes perceived by the event camera. Based on the\nsparsification results, a Cross-Modality Focus Fusion (CMFF) module is proposed\nto effectively capture and integrate complementary features from both\nmodalities. Experiments on the DSEC-Det and PKU-DAVIS-SOD datasets demonstrate\nthat the proposed method achieves superior performance in both accuracy and\nefficiency compared to existing methods. The code will be available at\nhttps://github.com/Zizzzzzzz/FocusMamba.", "AI": {"tldr": "FocusMamba通过设计EGMS策略和CMFF模块来自适应地稀疏化和整合RGB和事件数据中的有用信息，从而提高检测方法在准确性和效率方面的表现。", "motivation": "现有的RGB-事件检测方法在特征提取和融合过程中将两种模态的低信息区域处理方式相同，造成计算成本高且性能不佳。为减少计算冗余，已分别提出针对图像和事件模态的令牌稀疏化方法，但这些方法采用固定的令牌选择数或阈值，不利于保留复杂度不同的样本的有用信息。", "method": "设计理念是通过自适应协同稀疏化多模态特征及有效集成两模态互补信息来提升计算效率和精度。具体而言，提出了事件引导的多模态稀疏化策略(EGMS)，根据事件相机感知的场景内容变化自适应丢弃每个模态中的低信息区域。基于稀疏化结果，设计了跨模态聚焦融合(CMFF)模块，以捕捉和整合两模态的互补特征。", "result": "在DSEC-Det和PKU-DAVIS-SOD数据集上的实验表明，提出的方法在准确性和效率方面均优于现有方法。", "conclusion": "提出的方法通过自适应协作的方式稀疏化多模态特征并高效整合互补信息。"}}
{"id": "2509.03615", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.03615", "abs": "https://arxiv.org/abs/2509.03615", "authors": ["Aryan Gupta", "Anupam Purwar"], "title": "E-ARMOR: Edge case Assessment and Review of Multilingual Optical Character Recognition", "comment": "Sprinklr OCR provides a fast and compute light way of performing OCR", "summary": "Optical Character Recognition (OCR) in multilingual, noisy, and diverse\nreal-world images remains a significant challenge for optical character\nrecognition systems. With the rise of Large Vision-Language Models (LVLMs),\nthere is growing interest in their ability to generalize and reason beyond\nfixed OCR pipelines. In this work, we introduce Sprinklr-Edge-OCR, a novel OCR\nsystem built specifically optimized for edge deployment in resource-constrained\nenvironments. We present a large-scale comparative evaluation of five\nstate-of-the-art LVLMs (InternVL, Qwen, GOT OCR, LLaMA, MiniCPM) and two\ntraditional OCR systems (Sprinklr-Edge-OCR, SuryaOCR) on a proprietary, doubly\nhand annotated dataset of multilingual (54 languages) images. Our benchmark\ncovers a broad range of metrics including accuracy, semantic consistency,\nlanguage coverage, computational efficiency (latency, memory, GPU usage), and\ndeployment cost. To better reflect real-world applicability, we also conducted\nedge case deployment analysis, evaluating model performance on CPU only\nenvironments. Among the results, Qwen achieved the highest precision (0.54),\nwhile Sprinklr-Edge-OCR delivered the best overall F1 score (0.46) and\noutperformed others in efficiency, processing images 35 faster (0.17 seconds\nper image on average) and at less than 0.01 of the cost (0.006 USD per 1,000\nimages) compared to LVLM. Our findings demonstrate that the most optimal OCR\nsystems for edge deployment are the traditional ones even in the era of LLMs\ndue to their low compute requirements, low latency, and very high\naffordability.", "AI": {"tldr": "评估了五个前沿LVLM和两个传统OCR系统在多语言图像上的性能，发现传统OCR系统由于低计算需求、低延迟和高性价比，更适合边缘部署。", "motivation": "为了探索LVLM是否能超越固定的OCR管线，在多语言、噪声及多元化的现实场景图像中有所突破。同时研究适合边缘部署的最佳OCR系统。", "method": "我们介绍了Sprinklr-Edge-OCR，一款专门为边缘部署设计的OCR系统，构建在资源受限的环境中进行优化。我们对比了五个前沿的LVLM（InternVL, Qwen, GOT OCR, LLaMA, MiniCPM）和两个传统的OCR系统（Sprinklr-Edge-OCR, SuryaOCR），在包含54种语言的手工标注数据集上进行了大规模的评估，涵盖了准确性、语义一致性、语言覆盖范围、计算效率（延迟、内存、GPU使用）和部署成本等多方面指标。", "result": "Qwen在精度上表现最佳（0.54），而Sprinklr-Edge-OCR在综合F1评分上最佳（0.46），并且在效率上优于其他系统，处理图像速度提高35%，延迟仅为0.17秒每张图像，成本仅为LVLM的0.01（每1,000张图像0.006美元）。", "conclusion": "研究发现，即使是在大型语言模型(LLM)盛行的时代，传统的OCR系统仍然是边缘部署的最佳选择，因为它们拥有较低的计算需求，较低的延迟，以及极高的性价比。"}}
{"id": "2509.03873", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.03873", "abs": "https://arxiv.org/abs/2509.03873", "authors": ["Jiajun Song", "Xiaoou Liu"], "title": "SalientFusion: Context-Aware Compositional Zero-Shot Food Recognition", "comment": "34th International Conference on Artificial Neural Networks - ICANN\n  2025", "summary": "Food recognition has gained significant attention, but the rapid emergence of\nnew dishes requires methods for recognizing unseen food categories, motivating\nZero-Shot Food Learning (ZSFL). We propose the task of Compositional Zero-Shot\nFood Recognition (CZSFR), where cuisines and ingredients naturally align with\nattributes and objects in Compositional Zero-Shot learning (CZSL). However,\nCZSFR faces three challenges: (1) Redundant background information distracts\nmodels from learning meaningful food features, (2) Role confusion between\nstaple and side dishes leads to misclassification, and (3) Semantic bias in a\nsingle attribute can lead to confusion of understanding. Therefore, we propose\nSalientFusion, a context-aware CZSFR method with two components: SalientFormer,\nwhich removes background redundancy and uses depth features to resolve role\nconfusion; DebiasAT, which reduces the semantic bias by aligning prompts with\nvisual features. Using our proposed benchmarks, CZSFood-90 and CZSFood-164, we\nshow that SalientFusion achieves state-of-the-art results on these benchmarks\nand the most popular general datasets for the general CZSL. The code is\navaliable at https://github.com/Jiajun-RUC/SalientFusion.", "AI": {"tldr": "为了解决食品识别领域的零样本学习问题，我们提出了组合零样本食品识别（CZSFR）任务，并开发了SalientFusion方法来应对相关的挑战，实验证明该方法在多个基准测试中达到了最佳结果。", "motivation": "食品识别技术得到了广泛关注，但新的菜品不断涌现导致需要识别未曾见过的食品类别。这催生了零样本食品学习（ZSFL）的动机。我们提出了组合零样本食品识别（CZSFR）的任务。然而，CZSFR面临着三个挑战：（1）背景信息冗余分散模型对有意义食品特征的学习；（2）主食和辅食角色混淆导致误分类；（3）单一属性的语义偏差可能导致理解上的混淆。", "method": "我们提出了SalientFusion方法，这是一种上下文感知的组合零样本食品识别方法，包含两个部分：SalientFormer和DebiasAT。SalientFormer用于去除背景冗余信息，并利用深度特征解决主食和辅食的角色混淆问题；DebiasAT通过将提示与视觉特征对齐减少语义偏差。", "result": "在我们提出的CZSFood-90和CZSFood-164基准测试和流行的通用数据集上，SalientFusion达到了最先进的结果。", "conclusion": "通过提出SalientFusion方法，解决了组合零样本食品识别中的主要挑战，并在多个基准上达到了最好的结果。这项工作证明了这种方法的前景。"}}
{"id": "2509.03647", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.03647", "abs": "https://arxiv.org/abs/2509.03647", "authors": ["Dani Roytburg", "Matthew Bozoukov", "Matthew Nguyen", "Jou Barzdukas", "Simon Fu", "Narmeen Oozeer"], "title": "Breaking the Mirror: Activation-Based Mitigation of Self-Preference in LLM Evaluators", "comment": null, "summary": "Large language models (LLMs) increasingly serve as automated evaluators, yet\nthey suffer from \"self-preference bias\": a tendency to favor their own outputs\nover those of other models. This bias undermines fairness and reliability in\nevaluation pipelines, particularly for tasks like preference tuning and model\nrouting. We investigate whether lightweight steering vectors can mitigate this\nproblem at inference time without retraining. We introduce a curated dataset\nthat distinguishes self-preference bias into justified examples of\nself-preference and unjustified examples of self-preference, and we construct\nsteering vectors using two methods: Contrastive Activation Addition (CAA) and\nan optimization-based approach. Our results show that steering vectors can\nreduce unjustified self-preference bias by up to 97\\%, substantially\noutperforming prompting and direct preference optimization baselines. Yet\nsteering vectors are unstable on legitimate self-preference and unbiased\nagreement, implying self-preference spans multiple or nonlinear directions.\nThis underscores both their promise and limits as safeguards for LLM-as-judges\nand motivates more robust interventions.", "AI": {"tldr": "研究显示，经过精心设计的引导向量能够显著减少大语言模型在评估其自身输出时的不合理自我偏好问题，但它们对正当的自我偏好有所限制。", "motivation": "本研究旨在探讨在不重新训练的情况下，是否可以通过简化的引导向量在推理时减轻LLMs的自我偏好偏见问题，以维护评估管道的公平性和可靠性，特别适用于偏好调优和模型路由等任务。", "method": "我们使用了对比激活添加(CAA)和优化方法两种方式构建引导向量，并通过一个区分自我偏好偏好的合理性与不合理性的自定义数据集进行研究。", "result": "结果表明，引导向量能将不合理的自我偏好偏见减少高达97%，明显超越了提示和直接偏好优化基线方法。然而，这些引导向量对正当的自我偏好和非偏向的一致性出现了不稳定性。", "conclusion": "这项研究揭示了将引导向量用作判断LLMs的防护措施的潜力和限制，同时也突显了需要更强大的干预措施来解决自我偏好偏见问题。"}}
{"id": "2509.03883", "categories": ["cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2509.03883", "abs": "https://arxiv.org/abs/2509.03883", "authors": ["Haiwei Xue", "Xiangyang Luo", "Zhanghao Hu", "Xin Zhang", "Xunzhi Xiang", "Yuqin Dai", "Jianzhuang Liu", "Zhensong Zhang", "Minglei Li", "Jian Yang", "Fei Ma", "Zhiyong Wu", "Changpeng Yang", "Zonghong Dai", "Fei Richard Yu"], "title": "Human Motion Video Generation: A Survey", "comment": "Accepted by TPAMI. Github Repo:\n  https://github.com/Winn1y/Awesome-Human-Motion-Video-Generation IEEE Access:\n  https://ieeexplore.ieee.org/document/11106267", "summary": "Human motion video generation has garnered significant research interest due\nto its broad applications, enabling innovations such as photorealistic singing\nheads or dynamic avatars that seamlessly dance to music. However, existing\nsurveys in this field focus on individual methods, lacking a comprehensive\noverview of the entire generative process. This paper addresses this gap by\nproviding an in-depth survey of human motion video generation, encompassing\nover ten sub-tasks, and detailing the five key phases of the generation\nprocess: input, motion planning, motion video generation, refinement, and\noutput. Notably, this is the first survey that discusses the potential of large\nlanguage models in enhancing human motion video generation. Our survey reviews\nthe latest developments and technological trends in human motion video\ngeneration across three primary modalities: vision, text, and audio. By\ncovering over two hundred papers, we offer a thorough overview of the field and\nhighlight milestone works that have driven significant technological\nbreakthroughs. Our goal for this survey is to unveil the prospects of human\nmotion video generation and serve as a valuable resource for advancing the\ncomprehensive applications of digital humans. A complete list of the models\nexamined in this survey is available in Our Repository\nhttps://github.com/Winn1y/Awesome-Human-Motion-Video-Generation.", "AI": {"tldr": "本文主要探讨了人类运动视频生成的全面过程，并强调了大型语言模型的应用，是对该领域整体生成流程的首次详尽综述。", "motivation": "鉴于现有调研中缺少对整个生成过程的全面概述，本文旨在填补这一空白，详细介绍人类运动视频生成的各个环节。", "method": "提供了一个深入的人类运动视频生成综述，涵盖了超过十个子任务，详细阐述了生成过程中的五个关键阶段：输入、运动规划、运动视频生成、细化和输出。此外，本文首次探讨了大型语言模型在提升人类运动视频生成中的潜力。", "result": "通过覆盖超过两百篇论文，本文提供了一个该领域的深入回顾，并突出了一些重大技术突破。", "conclusion": "本文对人类运动视频生成技术的发展趋势进行了全面回顾，揭示了其潜在应用前景，为数字人类的综合应用场景提供了一个宝贵的资源。"}}
{"id": "2509.03662", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.03662", "abs": "https://arxiv.org/abs/2509.03662", "authors": ["Ali Noori", "Somya Mohanty", "Prashanti Manda"], "title": "Semantic Analysis of SNOMED CT Concept Co-occurrences in Clinical Documentation using MIMIC-IV", "comment": null, "summary": "Clinical notes contain rich clinical narratives but their unstructured format\nposes challenges for large-scale analysis. Standardized terminologies such as\nSNOMED CT improve interoperability, yet understanding how concepts relate\nthrough co-occurrence and semantic similarity remains underexplored. In this\nstudy, we leverage the MIMIC-IV database to investigate the relationship\nbetween SNOMED CT concept co-occurrence patterns and embedding-based semantic\nsimilarity. Using Normalized Pointwise Mutual Information (NPMI) and pretrained\nembeddings (e.g., ClinicalBERT, BioBERT), we examine whether frequently\nco-occurring concepts are also semantically close, whether embeddings can\nsuggest missing concepts, and how these relationships evolve temporally and\nacross specialties. Our analyses reveal that while co-occurrence and semantic\nsimilarity are weakly correlated, embeddings capture clinically meaningful\nassociations not always reflected in documentation frequency. Embedding-based\nsuggestions frequently matched concepts later documented, supporting their\nutility for augmenting clinical annotations. Clustering of concept embeddings\nyielded coherent clinical themes (symptoms, labs, diagnoses, cardiovascular\nconditions) that map to patient phenotypes and care patterns. Finally,\nco-occurrence patterns linked to outcomes such as mortality and readmission\ndemonstrate the practical utility of this approach. Collectively, our findings\nhighlight the complementary value of co-occurrence statistics and semantic\nembeddings in improving documentation completeness, uncovering latent clinical\nrelationships, and informing decision support and phenotyping applications.", "AI": {"tldr": "本研究探索了SNOMED CT概念共现模式和基于嵌入的语义相似性之间的关系，展示了概念共现与语义相似性之间的弱相关性，但基于嵌入的方法可以捕获临床相关性并用于增强临床注释，这显示出共现和语义嵌入在改善文档完整性、发现潜在的临床关系和指导决策支持应用程序方面的互补价值。", "motivation": "临床记录包含丰富的临床叙述，但它们的非结构化格式对大规模分析带来了挑战。标准化术语虽然提高了互操作性，但理解概念之间的共现和语义相似性仍然研究不足。", "method": "本研究利用MIMIC-IV数据库，通过标准化术语SNOMED CT的概念共现模式和基于嵌入的语义相似性来调查两者之间的关系。使用归一化点互信息(NPMI)和预训练嵌入（如ClinicalBERT，BioBERT）来研究频繁共现的概念是否也语义相近，嵌入是否可以建议缺失的概念，以及这些关系如何随着时间变化和在不同专业间变化。", "result": "分析结果表明，尽管共现和语义相似性之间存在弱相关性，但是嵌入捕获的临床有意义的关联并不总能反映在文档频率中。基于嵌入的建议经常与后来记录的概念匹配，支持其在增强临床注释方面的效用。嵌入的概念聚类生成了连贯的临床主题（如症状、实验室检查、诊断和心血管疾病），这些主题可以映射到患者表型和护理模式。最后，共现模式与结局指标（如死亡率和再入院率）相关联，证明了这种方法的实用性。", "conclusion": "总体而言，研究结果突出显示共现统计数据和语义嵌入在提高文档完整性、揭示潜在临床关系和为决策支持和表型分析应用提供信息方面的互补价值。"}}
{"id": "2509.03887", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.03887", "abs": "https://arxiv.org/abs/2509.03887", "authors": ["Bu Jin", "Songen Gu", "Xiaotao Hu", "Yupeng Zheng", "Xiaoyang Guo", "Qian Zhang", "Xiaoxiao Long", "Wei Yin"], "title": "OccTENS: 3D Occupancy World Model via Temporal Next-Scale Prediction", "comment": null, "summary": "In this paper, we propose OccTENS, a generative occupancy world model that\nenables controllable, high-fidelity long-term occupancy generation while\nmaintaining computational efficiency. Different from visual generation, the\noccupancy world model must capture the fine-grained 3D geometry and dynamic\nevolution of the 3D scenes, posing great challenges for the generative models.\nRecent approaches based on autoregression (AR) have demonstrated the potential\nto predict vehicle movement and future occupancy scenes simultaneously from\nhistorical observations, but they typically suffer from \\textbf{inefficiency},\n\\textbf{temporal degradation} in long-term generation and \\textbf{lack of\ncontrollability}. To holistically address these issues, we reformulate the\noccupancy world model as a temporal next-scale prediction (TENS) task, which\ndecomposes the temporal sequence modeling problem into the modeling of spatial\nscale-by-scale generation and temporal scene-by-scene prediction. With a\n\\textbf{TensFormer}, OccTENS can effectively manage the temporal causality and\nspatial relationships of occupancy sequences in a flexible and scalable way. To\nenhance the pose controllability, we further propose a holistic pose\naggregation strategy, which features a unified sequence modeling for occupancy\nand ego-motion. Experiments show that OccTENS outperforms the state-of-the-art\nmethod with both higher occupancy quality and faster inference time.", "AI": {"tldr": "OccTENS is a novel model for generating high-fidelity, controllable long-term 3D occupancy scenes from historical observations, improving upon the limitations of autoregressive methods.", "motivation": "The motivation is to overcome the inefficiency, temporal degradation, and lack of controllability in current AR-based methods for generating fine-grained 3D occupancy scenes over long-term observation.", "method": "OccTENS is a generative occupancy world model that addresses the challenges of autoregressive (AR) methods by reformulating the occupancy world model as a Temporal Next-Scale Prediction (TENS) task. It uses a TensFormer to manage temporal causality and spatial relationships while also integrating a holistic pose aggregation strategy for better pose controllability.", "result": "Experimental results demonstrate that OccTENS outperforms existing methods in terms of occupancy quality and inference speed.", "conclusion": "The conclusion is that OccTENS effectively addresses the challenges of long-term, high-fidelity 3D occupancy scene generation, offering improvements in both performance and computational efficiency compared to current approaches."}}
{"id": "2509.03725", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.03725", "abs": "https://arxiv.org/abs/2509.03725", "authors": ["Parush Gera", "Tempestt Neal"], "title": "MLSD: A Novel Few-Shot Learning Approach to Enhance Cross-Target and Cross-Domain Stance Detection", "comment": null, "summary": "We present the novel approach for stance detection across domains and\ntargets, Metric Learning-Based Few-Shot Learning for Cross-Target and\nCross-Domain Stance Detection (MLSD). MLSD utilizes metric learning with\ntriplet loss to capture semantic similarities and differences between stance\ntargets, enhancing domain adaptation. By constructing a discriminative\nembedding space, MLSD allows a cross-target or cross-domain stance detection\nmodel to acquire useful examples from new target domains. We evaluate MLSD in\nmultiple cross-target and cross-domain scenarios across two datasets, showing\nstatistically significant improvement in stance detection performance across\nsix widely used stance detection models.", "AI": {"tldr": "本文提出了一种名为MLSD的新方法，利用度量学习与三元组损失来捕获立场目标间的语义相似性和差异，提高了领域内的立场检测性能。", "motivation": "为了改进跨领域和跨目标的立场检测，在不同数据集中提高六种常用立场检测模型的性能。", "method": "MLSD采用了度量学习与三元组损失技术，构建了一个判别性的嵌入空间，从而使模型能够从新的目标域中获取有用的示例。", "result": "已验证MLSD能够在多个跨目标和跨域的场景下，显著提高立场检测的性能。", "conclusion": "研究表明，MLSD能够有效提升跨领域和跨目标立场检测的性能。"}}
{"id": "2509.03893", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.03893", "abs": "https://arxiv.org/abs/2509.03893", "authors": ["Stefan Stojanov", "Linan Zhao", "Yunzhi Zhang", "Daniel L. K. Yamins", "Jiajun Wu"], "title": "Weakly-Supervised Learning of Dense Functional Correspondences", "comment": "Accepted at ICCV 2025. Project website:\n  https://dense-functional-correspondence.github.io/", "summary": "Establishing dense correspondences across image pairs is essential for tasks\nsuch as shape reconstruction and robot manipulation. In the challenging setting\nof matching across different categories, the function of an object, i.e., the\neffect that an object can cause on other objects, can guide how correspondences\nshould be established. This is because object parts that enable specific\nfunctions often share similarities in shape and appearance. We derive the\ndefinition of dense functional correspondence based on this observation and\npropose a weakly-supervised learning paradigm to tackle the prediction task.\nThe main insight behind our approach is that we can leverage vision-language\nmodels to pseudo-label multi-view images to obtain functional parts. We then\nintegrate this with dense contrastive learning from pixel correspondences to\ndistill both functional and spatial knowledge into a new model that can\nestablish dense functional correspondence. Further, we curate synthetic and\nreal evaluation datasets as task benchmarks. Our results demonstrate the\nadvantages of our approach over baseline solutions consisting of off-the-shelf\nself-supervised image representations and grounded vision language models.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2509.03791", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.03791", "abs": "https://arxiv.org/abs/2509.03791", "authors": ["Saki Imai", "Mert İnan", "Anthony Sicilia", "Malihe Alikhani"], "title": "SiLVERScore: Semantically-Aware Embeddings for Sign Language Generation Evaluation", "comment": null, "summary": "Evaluating sign language generation is often done through back-translation,\nwhere generated signs are first recognized back to text and then compared to a\nreference using text-based metrics. However, this two-step evaluation pipeline\nintroduces ambiguity: it not only fails to capture the multimodal nature of\nsign language-such as facial expressions, spatial grammar, and prosody-but also\nmakes it hard to pinpoint whether evaluation errors come from sign generation\nmodel or the translation system used to assess it. In this work, we propose\nSiLVERScore, a novel semantically-aware embedding-based evaluation metric that\nassesses sign language generation in a joint embedding space. Our contributions\ninclude: (1) identifying limitations of existing metrics, (2) introducing\nSiLVERScore for semantically-aware evaluation, (3) demonstrating its robustness\nto semantic and prosodic variations, and (4) exploring generalization\nchallenges across datasets. On PHOENIX-14T and CSL-Daily datasets, SiLVERScore\nachieves near-perfect discrimination between correct and random pairs (ROC AUC\n= 0.99, overlap < 7%), substantially outperforming traditional metrics.", "AI": {"tldr": "提出SiLVERScore，一种语义感知的嵌入式手语生成评价指标，在多个数据集上表现出色，解决了现有评估方法的缺陷。", "motivation": "当前的评估方法通过反向翻译来评估手语生成，即先把手语识别回文本，然后再与参考文本进行比较。但这种两步评估流程存在局限性，不仅无法捕捉手语的多模态特点，也难以区分评估错误来源。", "method": "提出了一种新的评价指标SiLVERScore，该指标采用语义感知的嵌入式评估方法，能够在联合嵌入空间中对手语生成进行评价。", "result": "通过实验表明，SiLVERScore在PHOENIX-14T和CSL-Daily数据集上表现优异，正确和随机样本之间的判别准确率达到近完美（ROC AUC = 0.99，重叠小于7%），远超传统指标。", "conclusion": "SiLVERScore通过解决现有评价指标的局限性，实现了对手语生成评价的语义感知，并表现出对语义和韵律变化的鲁棒性，以及在不同数据集上的泛化能力。"}}
{"id": "2509.03895", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.03895", "abs": "https://arxiv.org/abs/2509.03895", "authors": ["Phuoc-Nguyen Bui", "Khanh-Binh Nguyen", "Hyunseung Choo"], "title": "Attn-Adapter: Attention Is All You Need for Online Few-shot Learner of Vision-Language Model", "comment": "ICCV 2025 - LIMIT Workshop", "summary": "Contrastive vision-language models excel in zero-shot image recognition but\nface challenges in few-shot scenarios due to computationally intensive offline\nfine-tuning using prompt learning, which risks overfitting. To overcome these\nlimitations, we propose Attn-Adapter, a novel online few-shot learning\nframework that enhances CLIP's adaptability via a dual attention mechanism. Our\ndesign incorporates dataset-specific information through two components: the\nMemory Attn-Adapter, which refines category embeddings using support examples,\nand the Local-Global Attn-Adapter, which enriches image embeddings by\nintegrating local and global features. This architecture enables dynamic\nadaptation from a few labeled samples without retraining the base model.\nAttn-Adapter outperforms state-of-the-art methods in cross-category and\ncross-dataset generalization, maintaining efficient inference and scaling\nacross CLIP backbones.", "AI": {"tldr": "Attn-Adapter在不重新训练基础模型的情况下实现了从少量样本中进行动态适应，显著提升了跨类别和跨数据集的推广能力。", "motivation": "对比性视觉语言模型在零样本图像识别中表现出色，但在少样本场景下面临挑战，因为基于提示学习的离线微调计算成本高且有过度拟合的风险。", "method": "提出了一种新的在线少样本学习框架Attn-Adapter，通过双注意力机制增强了CLIP的适应性。该设计包含两个组件：Memory Attn-Adapter和Local-Global Attn-Adapter，分别用于优化类别嵌入和丰富图像嵌入，实现从少量标注样本中的动态适应，无需重新训练基础模型。", "result": "Attn-Adapter在跨类别和跨数据集推广方面超越了最先进方法，同时保持了高效的推理能力，并且可以跨越CLIP背骨进行扩展。", "conclusion": "通过提出Attn-Adapter框架，研究展示了在少样本学习场景下有效提升了CLIP的表现力和推广性，证明了其在零样本学习和少样本学习中的潜力。"}}
{"id": "2509.03805", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.03805", "abs": "https://arxiv.org/abs/2509.03805", "authors": ["Saki Imai", "Mert İnan", "Anthony Sicilia", "Malihe Alikhani"], "title": "Measuring How (Not Just Whether) VLMs Build Common Ground", "comment": null, "summary": "Large vision language models (VLMs) increasingly claim reasoning skills, yet\ncurrent benchmarks evaluate them in single-turn or question answering settings.\nHowever, grounding is an interactive process in which people gradually develop\nshared understanding through ongoing communication. We introduce a four-metric\nsuite (grounding efficiency, content alignment, lexical adaptation, and\nhuman-likeness) to systematically evaluate VLM performance in interactive\ngrounding contexts. We deploy the suite on 150 self-play sessions of\ninteractive referential games between three proprietary VLMs and compare them\nwith human dyads. All three models diverge from human patterns on at least\nthree metrics, while GPT4o-mini is the closest overall. We find that (i) task\nsuccess scores do not indicate successful grounding and (ii) high\nimage-utterance alignment does not necessarily predict task success. Our metric\nsuite and findings offer a framework for future research on VLM grounding.", "AI": {"tldr": "本文提出了一套针对视觉语言模型在互动接地场景中表现的四指标评估体系，发现模型在多个指标上与人类表现存在差异，提供了一个未来研究的框架。", "motivation": "当前的基准大多在单一回合或问答设置中评估视觉语言模型，但本研究认为，接地是一个互动性过程，人们需要通过持续的沟通逐步建立共同的理解。因此，需要新的评估方法来更好地理解视觉语言模型在互动情境中的表现。", "method": "通过引入一套四指标体系（接地效率、内容一致性、词汇适应性和类人类性）来系统性地评估视觉语言模型在互动接地情境下的表现。这套指标体系在150次自博弈中的互动参照游戏中进行部署，参与者包括三个专有的视觉语言模型和人类双人组。", "result": "所有三个模型在至少三项指标上与人类表现有区别，但GPT4o-mini总体上最接近。发现任务成功的高评分并不一定意味着接地成功，高图像-话语对齐也不一定预测任务成功。", "conclusion": "该研究提供的指标体系帮助研究人员更好地理解视觉语言模型的接地能力，并为未来的研究提供了一个框架。"}}
{"id": "2509.03897", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.03897", "abs": "https://arxiv.org/abs/2509.03897", "authors": ["Xiaofu Chen", "Israfel Salazar", "Yova Kementchedjhieva"], "title": "SPECS: Specificity-Enhanced CLIP-Score for Long Image Caption Evaluation", "comment": null, "summary": "As interest grows in generating long, detailed image captions, standard\nevaluation metrics become increasingly unreliable. N-gram-based metrics though\nefficient, fail to capture semantic correctness. Representational Similarity\n(RS) metrics, designed to address this, initially saw limited use due to high\ncomputational costs, while today, despite advances in hardware, they remain\nunpopular due to low correlation to human judgments. Meanwhile, metrics based\non large language models (LLMs) show strong correlation with human judgments,\nbut remain too expensive for iterative use during model development.\n  We introduce SPECS (Specificity-Enhanced CLIPScore), a reference-free RS\nmetric tailored to long image captioning. SPECS modifies CLIP with a new\nobjective that emphasizes specificity: rewarding correct details and penalizing\nincorrect ones. We show that SPECS matches the performance of open-source\nLLM-based metrics in correlation to human judgments, while being far more\nefficient. This makes it a practical alternative for iterative checkpoint\nevaluation during image captioning model development.Our code can be found at\nhttps://github.com/mbzuai-nlp/SPECS.", "AI": {"tldr": "本文提出了一种新的度量方法SPECS，用于长图像描述，这种方法效率高且与人类判断的相关性好。", "motivation": "随着生成长而详细的图像描述的兴趣增加，标准的评估指标变得越来越不可靠。N-gram基线指标效率高，但无法捕捉语义正确性；表示相似性度量虽然旨在解决这个问题，但由于计算成本高和与人类判断的相关性低而未受欢迎；基于大语言模型的度量虽然与人类判断相关性很强，但在模型开发过程中迭代使用成本太高。", "method": "引入了SPECS（Specificity-Enhanced CLIPScore），这是一种无参考的表示相似性度量方法，专门用于长图像描述。SPECS通过对CLIP进行修改，采用新的目标，强调了详细信息的准确性，奖励正确的细节，惩罚错误的细节。", "result": "研究结果表明，SPECS在与人类判断的相关性上达到了开源的基于大语言模型度量的表现，同时效率更高。", "conclusion": "SPECS与开源的基于大语言模型的度量在与人类判断的相关性上表现相当，但效率更高，这使得它成为图像描述模型开发过程中迭代检查评估的实用替代方案。"}}
{"id": "2509.03809", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.03809", "abs": "https://arxiv.org/abs/2509.03809", "authors": ["Jiaxin Guo", "Daimeng Wei", "Yuanchang Luo", "Xiaoyu Chen", "Zhanglin Wu", "Huan Yang", "Hengchao Shang", "Zongyao Li", "Zhiqiang Rao", "Jinlong Yang", "Hao Yang"], "title": "Align-then-Slide: A complete evaluation framework for Ultra-Long Document-Level Machine Translation", "comment": "under preview", "summary": "Large language models (LLMs) have ushered in a new era for document-level\nmachine translation (\\textit{doc}-mt), yet their whole-document outputs\nchallenge existing evaluation methods that assume sentence-by-sentence\nalignment. We introduce \\textit{\\textbf{Align-then-Slide}}, a complete\nevaluation framework for ultra-long doc-mt. In the Align stage, we\nautomatically infer sentence-level source-target correspondences and rebuild\nthe target to match the source sentence number, resolving omissions and\nmany-to-one/one-to-many mappings. In the n-Chunk Sliding Evaluate stage, we\ncalculate averaged metric scores under 1-, 2-, 3- and 4-chunk for\nmulti-granularity assessment. Experiments on the WMT benchmark show a Pearson\ncorrelation of 0.929 between our method with expert MQM rankings. On a newly\ncurated real-world test set, our method again aligns closely with human\njudgments. Furthermore, preference data produced by Align-then-Slide enables\neffective CPO training and its direct use as a reward model for GRPO, both\nyielding translations preferred over a vanilla SFT baseline. The results\nvalidate our framework as an accurate, robust, and actionable evaluation tool\nfor doc-mt systems.", "AI": {"tldr": "论文提出了一种新的文档级机器翻译评估框架Align-then-Slide，它解决了现有评估方法中存在的句子对齐挑战问题，并通过实验证明了该框架的有效性。", "motivation": "大型语言模型改变了文档机器翻译的评估方法，但现有的评估手段基于句子对句子的对齐，对全文输出的评估形成挑战。", "method": "介绍了一种名为Align-then-Slide的评估框架，包含对齐和滑动评估两个阶段，解决了文档机器翻译评估中句子对齐挑战。", "result": "实验结果表明，该方法与专家MQM排名之间的皮尔逊相关系数达到0.929，并且在真实世界测试集中与人类判断吻合。同时，该方法生成的偏好数据可以用于CPO训练和作为奖励模型，结果优于简单指令调整基线。", "conclusion": "Align-then-Slide框架作为一种准确、稳健、可操作的文档机器翻译评估工具得到了验证。"}}
{"id": "2509.03903", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.03903", "abs": "https://arxiv.org/abs/2509.03903", "authors": ["Yuanfeng Ji", "Dan Lin", "Xiyue Wang", "Lu Zhang", "Wenhui Zhou", "Chongjian Ge", "Ruihang Chu", "Xiaoli Yang", "Junhan Zhao", "Junsong Chen", "Xiangde Luo", "Sen Yang", "Jin Fang", "Ping Luo", "Ruijiang Li"], "title": "A Generative Foundation Model for Chest Radiography", "comment": null, "summary": "The scarcity of well-annotated diverse medical images is a major hurdle for\ndeveloping reliable AI models in healthcare. Substantial technical advances\nhave been made in generative foundation models for natural images. Here we\ndevelop `ChexGen', a generative vision-language foundation model that\nintroduces a unified framework for text-, mask-, and bounding box-guided\nsynthesis of chest radiographs. Built upon the latent diffusion transformer\narchitecture, ChexGen was pretrained on the largest curated chest X-ray dataset\nto date, consisting of 960,000 radiograph-report pairs. ChexGen achieves\naccurate synthesis of radiographs through expert evaluations and quantitative\nmetrics. We demonstrate the utility of ChexGen for training data augmentation\nand supervised pretraining, which led to performance improvements across\ndisease classification, detection, and segmentation tasks using a small\nfraction of training data. Further, our model enables the creation of diverse\npatient cohorts that enhance model fairness by detecting and mitigating\ndemographic biases. Our study supports the transformative role of generative\nfoundation models in building more accurate, data-efficient, and equitable\nmedical AI systems.", "AI": {"tldr": "ChexGen 是一种生成医学图像的基础模型，能够在少样本的情况下提升疾病分类等任务的准确性，并有助于消除不同人群之间的偏见，它是更加公平和高效的医疗AI系统的一部分。", "motivation": "医学图像资源匮乏是一个重大阻碍，高质量的医学图像数据难以获得，限制了医疗领域可靠AI模型的发展。尽管在自然图像生成基础模型方面已经取得了重大进展，但医学图像在多样性和标注质量方面仍然受限。因此，研究人员开发了一个新的框架来解决这一问题。", "method": "本研究开发了一种名为 ChexGen 的生成式视觉语言基础模型，该模型提供了一个统一的框架，可用于文本指导、掩码指导及边界框指导的胸部X光片合成。ChexGen 基于潜扩散转换器架构，并在迄今为止最大的经过精心整理的胸部 X 光片数据集上进行了预训练，该数据集包含了 960,000 张放射图像报告对。", "result": "实验表明，ChexGen 通过专家评估和定量指标可以实现放射图像的精确合成。此外，它还可以用于训练数据增强和监督预训练，这使得在使用少量训练数据时也能在疾病分类、检测和分割任务上取得性能提升。", "conclusion": "该研究表明，生成式基础模型在构建更准确、数据效率更高且更加公平的医疗 AI 系统中扮演着变革性的角色。"}}
{"id": "2509.03829", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.03829", "abs": "https://arxiv.org/abs/2509.03829", "authors": ["Huhong Xian", "Rui Liu", "Berrak Sisman", "Haizhou Li"], "title": "NE-PADD: Leveraging Named Entity Knowledge for Robust Partial Audio Deepfake Detection via Attention Aggregation", "comment": null, "summary": "Different from traditional sentence-level audio deepfake detection (ADD),\npartial audio deepfake detection (PADD) requires frame-level positioning of the\nlocation of fake speech. While some progress has been made in this area,\nleveraging semantic information from audio, especially named entities, remains\nan underexplored aspect. To this end, we propose NE-PADD, a novel method for\nPartial Audio Deepfake Detection (PADD) that leverages named entity knowledge\nthrough two parallel branches: Speech Name Entity Recognition (SpeechNER) and\nPADD. The approach incorporates two attention aggregation mechanisms: Attention\nFusion (AF) for combining attention weights and Attention Transfer (AT) for\nguiding PADD with named entity semantics using an auxiliary loss. Built on the\nPartialSpoof-NER dataset, experiments show our method outperforms existing\nbaselines, proving the effectiveness of integrating named entity knowledge in\nPADD. The code is available at https://github.com/AI-S2-Lab/NE-PADD.", "AI": {"tldr": "NE-PADD integrates named entity knowledge into a partial audio deepfake detection framework using attention mechanisms, outperforming existing baselines on the PartialSpoof-NER dataset.", "motivation": "The motivation arises from the underexplored use of semantic information, specifically named entities, in partial audio deepfake detection.", "method": "Our method, NE-PADD, includes two parallel branches for Speech Name Entity Recognition (SpeechNER) and Partial Audio Deepfake Detection (PADD), utilizing named entity knowledge through Attention Fusion (AF) and Attention Transfer (AT) mechanisms.", "result": "The experiments show our NE-PADD approach outperforms existing methods by leveraging named entity knowledge through attention mechanisms.", "conclusion": "Integrating named entity knowledge improves partial audio deepfake detection, as demonstrated by the superior performance of NE-PADD in experiments."}}
{"id": "2509.03922", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.03922", "abs": "https://arxiv.org/abs/2509.03922", "authors": ["Xihua Sheng", "Yingwen Zhang", "Long Xu", "Shiqi Wang"], "title": "LMVC: An End-to-End Learned Multiview Video Coding Framework", "comment": null, "summary": "Multiview video is a key data source for volumetric video, enabling immersive\n3D scene reconstruction but posing significant challenges in storage and\ntransmission due to its massive data volume. Recently, deep learning-based\nend-to-end video coding has achieved great success, yet most focus on\nsingle-view or stereo videos, leaving general multiview scenarios\nunderexplored. This paper proposes an end-to-end learned multiview video coding\n(LMVC) framework that ensures random access and backward compatibility while\nenhancing compression efficiency. Our key innovation lies in effectively\nleveraging independent-view motion and content information to enhance\ndependent-view compression. Specifically, to exploit the inter-view motion\ncorrelation, we propose a feature-based inter-view motion vector prediction\nmethod that conditions dependent-view motion encoding on decoded\nindependent-view motion features, along with an inter-view motion entropy model\nthat learns inter-view motion priors. To exploit the inter-view content\ncorrelation, we propose a disparity-free inter-view context prediction module\nthat predicts inter-view contexts from decoded independent-view content\nfeatures, combined with an inter-view contextual entropy model that captures\ninter-view context priors. Experimental results show that our proposed LMVC\nframework outperforms the reference software of the traditional MV-HEVC\nstandard by a large margin, establishing a strong baseline for future research\nin this field.", "AI": {"tldr": "本文提出了一种可以实现随机访问并保持向后兼容性的端到端学习的多视角视频编码框架。该框架通过利用独立视角的运动和内容信息提高了多视角视频的压缩效率，并在实验中大幅超越了传统MV-HEVC标准的参考软件。", "motivation": "多视角视频是体素视频的关键数据源，能够实现沉浸式的3D场景重建，但存储和传输方面由于其巨大的数据量而面临重大挑战。目前，深度学习提升了端到端视频编码技术的发展，但大多集中在单视角或立体视频上，对多视角场景的研究还不够。", "method": "本文提出了一个端到端学习的多视角视频编码（LMVC）框架，该框架利用独立视角的运动信息和内容信息来提高依赖视角的压缩效率。具体而言，为了利用跨视角运动关联，提出了基于特征的跨视角运动矢量预测方法以及跨视角运动熵模型。为了利用跨视角内容关联，提出了无视差的跨视角上下文预测模块以及跨视角上下文熵模型。", "result": "实验结果表明，所提出的LMVC框架在性能上大大优于传统的MV-HEVC标准参考软件，为未来多视角视频编码研究奠定了坚实的基础。", "conclusion": "本文提出了一种新的多视角视频编码方法，它可以同时增强压缩效率，实现随机访问和保证向后兼容性。它的跨视角运动和内容预测模块在实验中展示了强大的性能，为多视角视频的高效编码提供了新的方向。"}}
{"id": "2509.03867", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.03867", "abs": "https://arxiv.org/abs/2509.03867", "authors": ["Yang Wang", "Chenghao Xiao", "Chia-Yi Hsiao", "Zi Yan Chang", "Chi-Li Chen", "Tyler Loakman", "Chenghua Lin"], "title": "Drivel-ology: Challenging LLMs with Interpreting Nonsense with Depth", "comment": "Accepted for oral presentation at the EMNLP 2025 Main Conference", "summary": "We introduce Drivelology, a unique linguistic phenomenon characterised as\n\"nonsense with depth\", utterances that are syntactically coherent yet\npragmatically paradoxical, emotionally loaded, or rhetorically subversive.\nWhile such expressions may resemble surface-level nonsense, they encode\nimplicit meaning requiring contextual inference, moral reasoning, or emotional\ninterpretation. We find that current large language models (LLMs), despite\nexcelling at many natural language processing (NLP) tasks, consistently fail to\ngrasp the layered semantics of Drivelological text. To investigate this, we\nconstruct a small but diverse benchmark dataset of over 1,200 meticulously\ncurated examples, with select instances in English, Mandarin, Spanish, French,\nJapanese, and Korean. Annotation was especially challenging: each of the\nexamples required careful expert review to verify that it truly reflected\nDrivelological characteristics. The process involved multiple rounds of\ndiscussion and adjudication to address disagreements, highlighting the subtle\nand subjective nature of the Drivelology. We evaluate a range of LLMs on\nclassification, generation, and reasoning tasks. Our results reveal clear\nlimitations of LLMs: models often confuse Drivelology with shallow nonsense,\nproduce incoherent justifications, or miss the implied rhetorical function\naltogether. These findings highlight a deeper representational gap in LLMs'\npragmatic understanding and challenge the assumption that statistical fluency\nimplies cognitive comprehension. We release our dataset and code to facilitate\nfurther research in modelling linguistic depth beyond surface-level coherence.", "AI": {"tldr": "文章介绍了Drivelology，一种在语法上连贯但在实用上充满悖论的语言现象，发现当前的大型语言模型在理解这种深度表达上有明显困难，并构建了一个多语言的基准数据集来评估多种语言模型的表现。结果显示这些模型在分类、生成和推理任务上存在局限性。", "motivation": "作者旨在研究大型语言模型在处理一种特殊的、含有深层含义但表面上看似无意义的文本（Drivelology）时的能力，以及揭示这些模型在理解这类文本上的深层次表征差距。", "method": "构建了一个包含1200多个经过精心挑选的Drivelological文本样例的数据集，涉及多种语言，通过多轮专家评审和讨论来确保样本的质量。", "result": "评估结果表明，尽管大型语言模型在许多NLP任务上表现出色，但它们在理解和生成Drivelological文本时存在明显局限，经常将其与简单的无意义表述混淆或忽视其隐含的修辞功能。", "conclusion": "作者指出，这些发现揭示了语言模型在理解语言深层含义方面存在的认知差距，挑战了统计流畅性等同于认知理解的假设。同时，公开发布了数据集和代码以促进进一步的研究。"}}
{"id": "2509.03938", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.03938", "abs": "https://arxiv.org/abs/2509.03938", "authors": ["Minghui Zhang", "Yaoyu Liu", "Junyang Wu", "Xin You", "Hanxiao Zhang", "Junjun He", "Yun Gu"], "title": "TopoSculpt: Betti-Steered Topological Sculpting of 3D Fine-grained Tubular Shapes", "comment": null, "summary": "Medical tubular anatomical structures are inherently three-dimensional\nconduits with lumens, enclosing walls, and complex branching topologies.\nAccurate reconstruction of their geometry and topology is crucial for\napplications such as bronchoscopic navigation and cerebral arterial\nconnectivity assessment. Existing methods often rely on voxel-wise overlap\nmeasures, which fail to capture topological correctness and completeness.\nAlthough topology-aware losses and persistent homology constraints have shown\npromise, they are usually applied patch-wise and cannot guarantee global\npreservation or correct geometric errors at inference. To address these\nlimitations, we propose a novel TopoSculpt, a framework for topological\nrefinement of 3D fine-grained tubular structures. TopoSculpt (i) adopts a\nholistic whole-region modeling strategy to capture full spatial context, (ii)\nfirst introduces a Topological Integrity Betti (TIB) constraint that jointly\nenforces Betti number priors and global integrity, and (iii) employs a\ncurriculum refinement scheme with persistent homology to progressively correct\nerrors from coarse to fine scales. Extensive experiments on challenging\npulmonary airway and Circle of Willis datasets demonstrate substantial\nimprovements in both geometry and topology. For instance, $\\beta_{0}$ errors\nare reduced from 69.00 to 3.40 on the airway dataset and from 1.65 to 0.30 on\nthe CoW dataset, with Tree length detected and branch detected rates improving\nby nearly 10\\%. These results highlight the effectiveness of TopoSculpt in\ncorrecting critical topological errors and advancing the high-fidelity modeling\nof complex 3D tubular anatomy. The project homepage is available at:\nhttps://github.com/Puzzled-Hui/TopoSculpt.", "AI": {"tldr": "提出一种名为TopoSculpt的框架，用于3D细粒度管状结构的拓扑细化，实现了显著的几何和拓扑改进。", "motivation": "现有的方法往往依赖于体素级的重叠度量，这无法捕捉到拓扑的正确性和完整性。尽管拓扑感知损失和持久同调约束显示出潜力，但它们通常仅应用于局部区域，且无法保证全局保存或纠正推理中的几何错误。为了克服这些限制，提出了该框架。", "method": "提出了一种名为TopoSculpt的框架，用于3D细粒度管状结构的拓扑细化。该框架(i)采用整体区域建模策略以捕获完整的空间上下文；(ii)首次引入了拓扑完整性贝蒂数（TIB）约束，同时强制执行贝蒂数先验和全局完整性；(iii)使用具有持久同调的课程化细化方案，逐步从粗尺度到细尺度纠正错误。", "result": "在具有挑战性的肺气道和Willis环数据集上的广泛实验表明，该方法在几何和拓扑方面有显著改进。例如，在肺气道数据集上$\beta_{0}$错误从69.00减少到3.40，在Willis环数据集上从1.65减少到0.30，树长度检测和分支检测率提高了近10%。", "conclusion": "这些结果表明TopoSculpt在纠正关键拓扑错误和推进复杂3D管状解剖的高保真建模方面是有效的。"}}
{"id": "2509.03871", "categories": ["cs.CL", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2509.03871", "abs": "https://arxiv.org/abs/2509.03871", "authors": ["Yanbo Wang", "Yongcan Yu", "Jian Liang", "Ran He"], "title": "A Comprehensive Survey on Trustworthiness in Reasoning with Large Language Models", "comment": "38 pages. This survey considers papers published up to June 30, 2025.\n  Work in progress", "summary": "The development of Long-CoT reasoning has advanced LLM performance across\nvarious tasks, including language understanding, complex problem solving, and\ncode generation. This paradigm enables models to generate intermediate\nreasoning steps, thereby improving both accuracy and interpretability. However,\ndespite these advancements, a comprehensive understanding of how CoT-based\nreasoning affects the trustworthiness of language models remains\nunderdeveloped. In this paper, we survey recent work on reasoning models and\nCoT techniques, focusing on five core dimensions of trustworthy reasoning:\ntruthfulness, safety, robustness, fairness, and privacy. For each aspect, we\nprovide a clear and structured overview of recent studies in chronological\norder, along with detailed analyses of their methodologies, findings, and\nlimitations. Future research directions are also appended at the end for\nreference and discussion. Overall, while reasoning techniques hold promise for\nenhancing model trustworthiness through hallucination mitigation, harmful\ncontent detection, and robustness improvement, cutting-edge reasoning models\nthemselves often suffer from comparable or even greater vulnerabilities in\nsafety, robustness, and privacy. By synthesizing these insights, we hope this\nwork serves as a valuable and timely resource for the AI safety community to\nstay informed on the latest progress in reasoning trustworthiness. A full list\nof related papers can be found at\n\\href{https://github.com/ybwang119/Awesome-reasoning-safety}{https://github.com/ybwang119/Awesome-reasoning-safety}.", "AI": {"tldr": "文章审视了最近关于推理模型和CoT技术的研究，重点是五个核心的可靠推理维度：真实性、安全性、稳健性、公平性和隐私性，并为AI安全社区提供了一个有价值的资源。", "motivation": "尽管CoT推理在提升语言模型的性能方面取得了显著进展，但对于CoT推理如何影响语言模型的可靠性仍然缺乏全面的理解。本文旨在填补这一知识空白。", "method": "本文通过回顾最近在推理模型和CoT技术方面的研究工作，重点关注了可靠推理的五个核心维度：真实性、安全性、稳健性、公平性和隐私性，并按时间顺序对每个方面的最新研究进行清晰和结构化的概述。", "result": "研究发现，尽管某些推理技术具有提升模型可靠性的潜力，但最前沿的推理模型在安全性和隐私性方面还存在显著的漏洞。", "conclusion": "尽管推理技术有望通过减少幻觉、检测有害内容和增强可靠性来提升模型的可靠性，但前沿的推理模型仍然存在相当大甚至更大的安全性和隐私性漏洞。"}}
{"id": "2509.03950", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.03950", "abs": "https://arxiv.org/abs/2509.03950", "authors": ["Alvaro Aranibar Roque", "Helga Sebastian"], "title": "Chest X-ray Pneumothorax Segmentation Using EfficientNet-B4 Transfer Learning in a U-Net Architecture", "comment": "10 page, 5 figures", "summary": "Pneumothorax, the abnormal accumulation of air in the pleural space, can be\nlife-threatening if undetected. Chest X-rays are the first-line diagnostic\ntool, but small cases may be subtle. We propose an automated deep-learning\npipeline using a U-Net with an EfficientNet-B4 encoder to segment pneumothorax\nregions. Trained on the SIIM-ACR dataset with data augmentation and a combined\nbinary cross-entropy plus Dice loss, the model achieved an IoU of 0.7008 and\nDice score of 0.8241 on the independent PTX-498 dataset. These results\ndemonstrate that the model can accurately localize pneumothoraces and support\nradiologists.", "AI": {"tldr": "提出了一种自动深度学习管道来诊断和分割气胸区域，提高了小气胸检测的准确性，能有效支持医生的诊断。", "motivation": "气胸，即胸膜腔内异常积气，如未被发现可危及生命。胸部X光片是首选的诊断工具，但小的气胸可能难以察觉。", "method": "提出使用带有EfficientNet-B4编码器的U-Net自动深度学习管道来分割气胸区域。该模型在SIIM-ACR数据集上通过数据增强和结合二元交叉熵及Dice损失进行训练。", "result": "在独立的PTX-498数据集上，该模型达到了IoU 0.7008和Dice分数0.8241的准确度。这表明该模型能够精确识别气胸区域。", "conclusion": "这些结果表明该模型可以准确地定位气胸区域，并为放射科医生提供支持。"}}
{"id": "2509.03888", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.03888", "abs": "https://arxiv.org/abs/2509.03888", "authors": ["Cheng Wang", "Zeming Wei", "Qin Liu", "Muhao Chen"], "title": "False Sense of Security: Why Probing-based Malicious Input Detection Fails to Generalize", "comment": null, "summary": "Large Language Models (LLMs) can comply with harmful instructions, raising\nserious safety concerns despite their impressive capabilities. Recent work has\nleveraged probing-based approaches to study the separability of malicious and\nbenign inputs in LLMs' internal representations, and researchers have proposed\nusing such probing methods for safety detection. We systematically re-examine\nthis paradigm. Motivated by poor out-of-distribution performance, we\nhypothesize that probes learn superficial patterns rather than semantic\nharmfulness. Through controlled experiments, we confirm this hypothesis and\nidentify the specific patterns learned: instructional patterns and trigger\nwords. Our investigation follows a systematic approach, progressing from\ndemonstrating comparable performance of simple n-gram methods, to controlled\nexperiments with semantically cleaned datasets, to detailed analysis of pattern\ndependencies. These results reveal a false sense of security around current\nprobing-based approaches and highlight the need to redesign both models and\nevaluation protocols, for which we provide further discussions in the hope of\nsuggesting responsible further research in this direction. We have open-sourced\nthe project at https://github.com/WangCheng0116/Why-Probe-Fails.", "AI": {"tldr": "研究发现大型语言模型的探测方法实际上依赖于表面模式而非语义上的危害性，可能带来错误的安全感，需要重新设计模型和评估方法。", "motivation": "由于探测方法在分布外数据上的表现不佳，促使研究者假设这些探测方法学习到的是表面模式而非语义上的危害。", "method": "通过系统性的方法，包括展示简单n-gram方法的类似表现，使用语义清理过的数据集进行控制实验，以及详细的模式依赖性分析，来验证假设。", "result": "确认探测方法学习的是指令模式和触发词等表面模式，并非深层次的危害性。这揭示了当前探测方法带来的安全假象，并表明需要重新设计模型和评估协议。", "conclusion": "研究表明当前基于探测的方法存在表面现象依赖的问题，建议重新设计模型和评估协议。"}}
{"id": "2509.03951", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.03951", "abs": "https://arxiv.org/abs/2509.03951", "authors": ["Zhu Wenjie", "Zhang Yabin", "Xin Jin", "Wenjun Zeng", "Lei Zhang"], "title": "ANTS: Shaping the Adaptive Negative Textual Space by MLLM for OOD Detection", "comment": null, "summary": "The introduction of negative labels (NLs) has proven effective in enhancing\nOut-of-Distribution (OOD) detection. However, existing methods often lack an\nunderstanding of OOD images, making it difficult to construct an accurate\nnegative space. In addition, the presence of false negative labels\nsignificantly degrades their near-OOD performance. To address these issues, we\npropose shaping an Adaptive Negative Textual Space (ANTS) by leveraging the\nunderstanding and reasoning capabilities of multimodal large language models\n(MLLMs). Specifically, we identify images likely to be OOD samples as negative\nimages and prompt the MLLM to describe these images, generating expressive\nnegative sentences that precisely characterize the OOD distribution and enhance\nfar-OOD detection. For the near-OOD setting, where OOD samples resemble the\nin-distribution (ID) subset, we first identify the subset of ID classes that\nare visually similar to negative images and then leverage the reasoning\ncapability of MLLMs to generate visually similar negative labels tailored to\nthis subset, effectively reducing false negatives and improving near-OOD\ndetection. To balance these two types of negative textual spaces, we design an\nadaptive weighted score that enables the method to handle different OOD task\nsettings (near-OOD and far-OOD) without relying on task-specific prior\nknowledge, making it highly adaptable in open environments. On the ImageNet\nbenchmark, our ANTS significantly reduces the FPR95 by 4.2\\%, establishing a\nnew state-of-the-art. Furthermore, our method is training-free and zero-shot,\nenabling high scalability.", "AI": {"tldr": "通过引入自适应负文本空间(ANTS)，利用多模态大型语言模型(MLLM)理解和生成描述负样本的句子，改进远分布外(OOD)和近分布外检测，提高检测精度，减少误报，适应不同OOD任务设置，无需训练且可扩展性强。", "motivation": "现有的OOD检测方法在理解OOD图像和构造准确的负样本空间方面存在不足，并且容易受到假负样本的影响，导致检测性能不佳。", "method": "提出自适应负文本空间(ANTS)，利用多模态大型语言模型(MLLM)，识别和描述可能的OOD样本，生成精确的负样本句子。对于近OOD情形，进一步识别与负样本视觉相关的ID子集并生成相应的负标签。", "result": "在ImageNet基准上显著降低FPR95达4.2%，并在不同OOD任务设置中表现出高效适应性。", "conclusion": "该方法无需训练且零样本，展示了高适应性和可扩展性，在减少误报和提高远分布外与近分布外检测方面表现出色。"}}
{"id": "2509.03891", "categories": ["cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.03891", "abs": "https://arxiv.org/abs/2509.03891", "authors": ["Gowen Loo", "Chang Liu", "Qinghong Yin", "Xiang Chen", "Jiawei Chen", "Jingyuan Zhang", "Yu Tian"], "title": "MobileRAG: Enhancing Mobile Agent with Retrieval-Augmented Generation", "comment": null, "summary": "Smartphones have become indispensable in people's daily lives, permeating\nnearly every aspect of modern society. With the continuous advancement of large\nlanguage models (LLMs), numerous LLM-based mobile agents have emerged. These\nagents are capable of accurately parsing diverse user queries and automatically\nassisting users in completing complex or repetitive operations. However,\ncurrent agents 1) heavily rely on the comprehension ability of LLMs, which can\nlead to errors caused by misoperations or omitted steps during tasks, 2) lack\ninteraction with the external environment, often terminating tasks when an app\ncannot fulfill user queries, and 3) lack memory capabilities, requiring each\ninstruction to reconstruct the interface and being unable to learn from and\ncorrect previous mistakes. To alleviate the above issues, we propose MobileRAG,\na mobile agents framework enhanced by Retrieval-Augmented Generation (RAG),\nwhich includes InterRAG, LocalRAG, and MemRAG. It leverages RAG to more quickly\nand accurately identify user queries and accomplish complex and long-sequence\nmobile tasks. Additionally, to more comprehensively assess the performance of\nMobileRAG, we introduce MobileRAG-Eval, a more challenging benchmark\ncharacterized by numerous complex, real-world mobile tasks that require\nexternal knowledge assistance. Extensive experimental results on MobileRAG-Eval\ndemonstrate that MobileRAG can easily handle real-world mobile tasks, achieving\n10.3\\% improvement over state-of-the-art methods with fewer operational steps.\nOur code is publicly available at:\nhttps://github.com/liuxiaojieOutOfWorld/MobileRAG_arxiv", "AI": {"tldr": "提出MobileRAG框架以改善移动代理的性能，显著提高完成复杂任务的效率和准确率，代码开源。", "motivation": "面对现有移动代理在理解和执行用户查询时存在的问题，特别是解析错误、对外界交互有限、缺乏记忆能力等，促使研究者开发了MobileRAG框架，以期通过RAG技术提升代理的性能。", "method": "MobileRAG框架利用了检索增强生成（RAG）技术，设计了InterRAG、LocalRAG和MemRAG三个组件，提升代理在理解和执行用户命令时的准确性和灵活性。", "result": "MobileRAG 是一个增强型移动代理框架，通过检索增强生成（RAG）技术解决了现有移动代理的三大问题：对大语言模型的理解能力依赖、缺乏对外部环境的交互和记忆功能不足。实验显示，与现有方法相比，MobileRAG在MobileRAG-Eval基准测试中表现更优，完成任务效率更高，具体步骤较少，提升了10.3\\%。代码已开源。", "conclusion": "实验结果表明，相对于现有技术，MobileRAG在MobileRAG-Eval这个更为苛刻的评估基准上取得了显著性能改进，可高效、准确完成复杂任务，具有更高的实用价值。"}}
{"id": "2509.03961", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.03961", "abs": "https://arxiv.org/abs/2509.03961", "authors": ["Yijun Zhou", "Yikui Zhai", "Zilu Ying", "Tingfeng Xian", "Wenlve Zhou", "Zhiheng Zhou", "Xiaolin Tian", "Xudong Jia", "Hongsheng Zhang", "C. L. Philip Chen"], "title": "Multimodal Feature Fusion Network with Text Difference Enhancement for Remote Sensing Change Detection", "comment": null, "summary": "Although deep learning has advanced remote sensing change detection (RSCD),\nmost methods rely solely on image modality, limiting feature representation,\nchange pattern modeling, and generalization especially under illumination and\nnoise disturbances. To address this, we propose MMChange, a multimodal RSCD\nmethod that combines image and text modalities to enhance accuracy and\nrobustness. An Image Feature Refinement (IFR) module is introduced to highlight\nkey regions and suppress environmental noise. To overcome the semantic\nlimitations of image features, we employ a vision language model (VLM) to\ngenerate semantic descriptions of bitemporal images. A Textual Difference\nEnhancement (TDE) module then captures fine grained semantic shifts, guiding\nthe model toward meaningful changes. To bridge the heterogeneity between\nmodalities, we design an Image Text Feature Fusion (ITFF) module that enables\ndeep cross modal integration. Extensive experiments on LEVIRCD, WHUCD, and\nSYSUCD demonstrate that MMChange consistently surpasses state of the art\nmethods across multiple metrics, validating its effectiveness for multimodal\nRSCD. Code is available at: https://github.com/yikuizhai/MMChange.", "AI": {"tldr": "提出MMChange方法，整合图像和文本模态，通过多个模块提高遥感变化检测的精度和鲁棒性，实验表明该方法优于现有技术。", "motivation": "当前深度学习在遥感变化检测中依赖单一图像模态，这限制了特征表示、变化模式建模和对抗光照及噪声干扰的能力。为了提高检测的准确性与鲁棒性，该研究提出了一种多模态方法。", "method": "提出了图像特征精炼（IFR）模块来突出关键区域，减少环境噪声；使用视觉语言模型（VLM）生成双时相图像的语义描述；文本差异增强（TDE）模块以捕捉细微语义变化；最后设计图像文本特征融合（ITFF）模块以进行深度跨模态整合。", "result": "该多模态遥感变化检测方法（MMChange）通过结合图像和文本模态提升检测精度和鲁棒性，提出多个模块以适应不同方面的需求。实验显示其在多个指标上超越现有方法，证明了该方法在多模态遥感变化检测中的有效性。", "conclusion": "该研究通过MMChange方法证实了多模态方法在遥感变化检测中的有效性，特别是在对抗光照和噪声干扰方面具有优势。"}}
{"id": "2509.03918", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.03918", "abs": "https://arxiv.org/abs/2509.03918", "authors": ["Fengxiao Tang", "Yufeng Li", "Zongzong Wu", "Ming Zhao"], "title": "MTQA:Matrix of Thought for Enhanced Reasoning in Complex Question Answering", "comment": null, "summary": "Complex Question Answering (QA) is a fundamental and challenging task in NLP.\nWhile large language models (LLMs) exhibit impressive performance in QA, they\nsuffer from significant performance degradation when facing complex and\nabstract QA tasks due to insufficient reasoning capabilities. Works such as\nChain-of-Thought (CoT) and Tree-of-Thought (ToT) aim to enhance LLMs' reasoning\nabilities, but they face issues such as in-layer redundancy in tree structures\nand single paths in chain structures. Although some studies utilize\nRetrieval-Augmented Generation (RAG) methods to assist LLMs in reasoning, the\nchallenge of effectively utilizing large amounts of information involving\nmultiple entities and hops remains critical. To address this, we propose the\nMatrix of Thought (MoT), a novel and efficient LLM thought structure. MoT\nexplores the problem in both horizontal and vertical dimensions through the\n\"column-cell communication\" mechanism, enabling LLMs to actively engage in\nmulti-strategy and deep-level thinking, reducing redundancy within the column\ncells and enhancing reasoning capabilities. Furthermore, we develop a\nfact-correction mechanism by constructing knowledge units from retrieved\nknowledge graph triples and raw text to enhance the initial knowledge for LLM\nreasoning and correct erroneous answers. This leads to the development of an\nefficient and accurate QA framework (MTQA). Experimental results show that our\nframework outperforms state-of-the-art methods on four widely-used datasets in\nterms of F1 and EM scores, with reasoning time only 14.4\\% of the baseline\nmethods, demonstrating both its efficiency and accuracy. The code for this\nframework is available at https://github.com/lyfiter/mtqa.", "AI": {"tldr": "Introduces Matrix of Thought (MoT) for enhancing the reasoning capabilities of LLMs in complex QA tasks, leading to the creation of the MTQA framework, which outperforms existing models in accuracy and efficiency.", "motivation": "Complex QA tasks expose the reasoning limitations of large language models (LLMs). Methods like Chain-of-Thought and Tree-of-Thought have limitations, and while Retrieval-Augmented Generation (RAG) methods help, they still struggle with complex tasks involving multiple entities and steps. An improved method is needed to effectively address complex question answering.", "method": "Matrix of Thought (MoT) is proposed as a novel and efficient thought structure for complex question answering. It uses a 'column-cell communication' mechanism to promote multi-strategy and deep-level reasoning, reducing redundancy. A fact-correction mechanism is also developed using knowledge units from retrieved knowledge graph triples and raw text.", "result": "The MTQA framework developed using MoT outperforms state-of-the-art methods on four widely-used datasets in terms of F1 and EM scores, demonstrating both efficiency and accuracy.", "conclusion": "The Matrix of Thought (MoT) and the associated MTQA framework offer significant improvements in reasoning capabilities for complex question answering tasks, achieving higher accuracy with less reasoning time compared to baseline methods."}}
{"id": "2509.03973", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.03973", "abs": "https://arxiv.org/abs/2509.03973", "authors": ["Yu Bai", "Zitong Yu", "Haowen Tian", "Xijing Wang", "Shuo Yan", "Lin Wang", "Honglin Li", "Xitong Ling", "Bo Zhang", "Zheng Zhang", "Wufan Wang", "Hui Gao", "Xiangyang Gong", "Wendong Wang"], "title": "SAC-MIL: Spatial-Aware Correlated Multiple Instance Learning for Histopathology Whole Slide Image Classification", "comment": null, "summary": "We propose Spatial-Aware Correlated Multiple Instance Learning (SAC-MIL) for\nperforming WSI classification. SAC-MIL consists of a positional encoding module\nto encode position information and a SAC block to perform full instance\ncorrelations. The positional encoding module utilizes the instance coordinates\nwithin the slide to encode the spatial relationships instead of the instance\nindex in the input WSI sequence. The positional encoding module can also handle\nthe length extrapolation issue where the training and testing sequences have\ndifferent lengths. The SAC block is an MLP-based method that performs full\ninstance correlation in linear time complexity with respect to the sequence\nlength. Due to the simple structure of MLP, it is easy to deploy since it does\nnot require custom CUDA kernels, compared to Transformer-based methods for WSI\nclassification. SAC-MIL has achieved state-of-the-art performance on the\nCAMELYON-16, TCGA-LUNG, and TCGA-BRAC datasets. The code will be released upon\nacceptance.", "AI": {"tldr": "提出SAC-MIL，通过位置编码模块和SAC块处理WSI的全实例关联问题，达到了在多个数据集上的领先性能，且易于部署。", "motivation": "本文旨在解决传统多实例学习方法无法有效利用空...", "method": "提出空间感知相关多实例学习（SAC-MIL）进行全幻影成像（WSI）分类。SAC-MIL 包含一个位置编码模块用于编码位置信息以及一个SAC块来执行全实例相关。位置编码模块利用幻灯片内的实例坐标来编码空间关系，而不是输入WSI序列中的实例索引。该模块还可以处理训练序列和测试序列长度不同的外推问题。SAC块是一个基于多层感知器（MLP）的方法，它与序列长度成线性时间复杂度地执行全实例相关。由于MLP结构简单，易于部署，因为它不需要自定义CUDA核，而与基于Transformer的方法相比有所不同。", "result": "在CAMELYON-16、TCGA-LUNG和TCGA-BRAC数据集上，SAC-MIL达到了最先进的表现性能。代码将在接受后公开。", "conclusion": "SAC-MIL方法在保持高性能的同时，简化了部署，适用于WSI分类任务。"}}
{"id": "2509.03932", "categories": ["cs.CL", "cs.CY", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.03932", "abs": "https://arxiv.org/abs/2509.03932", "authors": ["Iro Lim", "Haein Ji", "Byungjun Kim"], "title": "Decoding the Poetic Language of Emotion in Korean Modern Poetry: Insights from a Human-Labeled Dataset and AI Modeling", "comment": "30 pages, 13 tables, 2 figures, Digital Humanities and Social\n  Sciences Korea Conference, James Joo-Jin Kim Center for Korean Studies,\n  University of Pennsylvania, Philadelphia, USA", "summary": "This study introduces KPoEM (Korean Poetry Emotion Mapping) , a novel dataset\nfor computational emotion analysis in modern Korean poetry. Despite remarkable\nprogress in text-based emotion classification using large language models,\npoetry-particularly Korean poetry-remains underexplored due to its figurative\nlanguage and cultural specificity. We built a multi-label emotion dataset of\n7,662 entries, including 7,007 line-level entries from 483 poems and 615\nwork-level entries, annotated with 44 fine-grained emotion categories from five\ninfluential Korean poets. A state-of-the-art Korean language model fine-tuned\non this dataset significantly outperformed previous models, achieving 0.60\nF1-micro compared to 0.34 from models trained on general corpora. The KPoEM\nmodel, trained through sequential fine-tuning-first on general corpora and then\non the KPoEM dataset-demonstrates not only an enhanced ability to identify\ntemporally and culturally specific emotional expressions, but also a strong\ncapacity to preserve the core sentiments of modern Korean poetry. This study\nbridges computational methods and literary analysis, presenting new\npossibilities for the quantitative exploration of poetic emotions through\nstructured data that faithfully retains the emotional and cultural nuances of\nKorean literature.", "AI": {"tldr": "KPoEM is a novel dataset and model for emotion analysis in modern Korean poetry, demonstrating superior performance and opening avenues for the computational study of poetic emotions.", "motivation": "The motivation is to fill the gap in the underexplored domain of computational emotion analysis in Korean poetry, which is rich in figurative language and cultural specificity.", "method": "This study introduces KPoEM, a multi-label emotion dataset of 7,662 entries from modern Korean poetry, annotated with 44 fine-grained emotion categories. A state-of-the-art Korean language model was fine-tuned on this dataset.", "result": "The fine-tuned KPoEM model significantly outperformed previous models, achieving an F1-micro score of 0.60 compared to 0.34 from models trained on general corpora.", "conclusion": "The study suggests the KPoEM model can effectively identify temporal and culturally specific emotional expressions in modern Korean poetry, presenting new possibilities for quantitative exploration of poetic emotions."}}
{"id": "2509.03975", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.03975", "abs": "https://arxiv.org/abs/2509.03975", "authors": ["Daniel Sobotka", "Alexander Herold", "Matthias Perkonigg", "Lucian Beer", "Nina Bastati", "Alina Sablatnig", "Ahmed Ba-Ssalamah", "Georg Langs"], "title": "Improving Vessel Segmentation with Multi-Task Learning and Auxiliary Data Available Only During Model Training", "comment": null, "summary": "Liver vessel segmentation in magnetic resonance imaging data is important for\nthe computational analysis of vascular remodelling, associated with a wide\nspectrum of diffuse liver diseases. Existing approaches rely on contrast\nenhanced imaging data, but the necessary dedicated imaging sequences are not\nuniformly acquired. Images without contrast enhancement are acquired more\nfrequently, but vessel segmentation is challenging, and requires large-scale\nannotated data. We propose a multi-task learning framework to segment vessels\nin liver MRI without contrast. It exploits auxiliary contrast enhanced MRI data\navailable only during training to reduce the need for annotated training\nexamples. Our approach draws on paired native and contrast enhanced data with\nand without vessel annotations for model training. Results show that auxiliary\ndata improves the accuracy of vessel segmentation, even if they are not\navailable during inference. The advantage is most pronounced if only few\nannotations are available for training, since the feature representation\nbenefits from the shared task structure. A validation of this approach to\naugment a model for brain tumor segmentation confirms its benefits across\ndifferent domains. An auxiliary informative imaging modality can augment expert\nannotations even if it is only available during training.", "AI": {"tldr": "研究了一种多任务学习框架，用于无需对比度增强的肝脏MRI中的血管分割。通过训练时使用辅助对比增强数据来提升分割精度，即使这些数据在实际应用中不可用。", "motivation": "分割肝脏MRI中的血管对于计算分析血管重构，尤其是与弥漫性肝脏疾病相关的分析非常重要。现有的方法依赖于对比增强影像数据，但专用的成像序列并非普遍使用。未经对比剂增强的图像更频繁地被获取，但在这些图像中进行血管分割更具挑战性，要求大量标注数据。", "method": "提出了一种多任务学习框架来分割无对比度增强的肝脏MRI中的血管。该框架在训练期间利用辅助的对比增强MRI数据来减少对标注训练样本的需求。提出的模型利用了带有和不带有血管标注的本征和对比增强成对数据进行训练。", "result": "结果显示，辅助数据改善了血管分割的准确性，即使在推理过程中这些数据不可用。这种优势在只有少量标注训练样本时尤为显著，因为共同任务结构使特征表示受益。通过验证该方法可以提升脑肿瘤分割的模型，进一步证实其跨领域益处。", "conclusion": "一种辅助的、有信息量的成像模式，即使只在训练期间可用，也可以增强专家标注的效果。"}}
{"id": "2509.03934", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.03934", "abs": "https://arxiv.org/abs/2509.03934", "authors": ["Yuqing Huang", "Rongyang Zhang", "Qimeng Wang", "Chengqiang Lu", "Yan Gao", "Yi Wu", "Yao Hu", "Xuyang Zhi", "Guiquan Liu", "Xin Li", "Hao Wang", "Enhong Chen"], "title": "SelfAug: Mitigating Catastrophic Forgetting in Retrieval-Augmented Generation via Distribution Self-Alignment", "comment": null, "summary": "Recent advancements in large language models (LLMs) have revolutionized\nnatural language processing through their remarkable capabilities in\nunderstanding and executing diverse tasks. While supervised fine-tuning,\nparticularly in Retrieval-Augmented Generation (RAG) scenarios, effectively\nenhances task-specific performance, it often leads to catastrophic forgetting,\nwhere models lose their previously acquired knowledge and general capabilities.\nExisting solutions either require access to general instruction data or face\nlimitations in preserving the model's original distribution. To overcome these\nlimitations, we propose SelfAug, a self-distribution alignment method that\naligns input sequence logits to preserve the model's semantic distribution,\nthereby mitigating catastrophic forgetting and improving downstream\nperformance. Extensive experiments demonstrate that SelfAug achieves a superior\nbalance between downstream learning and general capability retention. Our\ncomprehensive empirical analysis reveals a direct correlation between\ndistribution shifts and the severity of catastrophic forgetting in RAG\nscenarios, highlighting how the absence of RAG capabilities in general\ninstruction tuning leads to significant distribution shifts during fine-tuning.\nOur findings not only advance the understanding of catastrophic forgetting in\nRAG contexts but also provide a practical solution applicable across diverse\nfine-tuning scenarios. Our code is publicly available at\nhttps://github.com/USTC-StarTeam/SelfAug.", "AI": {"tldr": "研究提出SelfAug方法，通过保持模型的语义分布减轻灾难性遗忘，实验表明SelfAug在下游性能和一般能力保留之间达到较好平衡。", "motivation": "解决在监督微调（特别是在检索增强生成(RAG)场景中）导致的灾难性遗忘问题，这个问题会使模型失去先前获得的知识和一般能力。", "method": "通过SelfAug自分布对齐方法，将输入序列的logits对齐以保持模型的语义分布，从而减轻灾难性遗忘，并提高下游性能。", "result": "实验结果表明，SelfAug在保持模型一般语义分布的同时提高了下游任务的性能，有效地解决了灾难性遗忘问题。", "conclusion": "SelfAug实现了在下游学习和保持模型一般能力之间的优越平衡，本研究不仅深化了对RAG场景中灾难性遗忘的理解，还提供了一种跨多样化微调场景的实际解决方案。"}}
{"id": "2509.03986", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.03986", "abs": "https://arxiv.org/abs/2509.03986", "authors": ["Mohamed Insaf Ismithdeen", "Muhammad Uzair Khattak", "Salman Khan"], "title": "Promptception: How Sensitive Are Large Multimodal Models to Prompts?", "comment": "Accepted to EMNLP 2025", "summary": "Despite the success of Large Multimodal Models (LMMs) in recent years, prompt\ndesign for LMMs in Multiple-Choice Question Answering (MCQA) remains poorly\nunderstood. We show that even minor variations in prompt phrasing and structure\ncan lead to accuracy deviations of up to 15% for certain prompts and models.\nThis variability poses a challenge for transparent and fair LMM evaluation, as\nmodels often report their best-case performance using carefully selected\nprompts. To address this, we introduce Promptception, a systematic framework\nfor evaluating prompt sensitivity in LMMs. It consists of 61 prompt types,\nspanning 15 categories and 6 supercategories, each targeting specific aspects\nof prompt formulation, and is used to evaluate 10 LMMs ranging from lightweight\nopen-source models to GPT-4o and Gemini 1.5 Pro, across 3 MCQA benchmarks:\nMMStar, MMMU-Pro, MVBench. Our findings reveal that proprietary models exhibit\ngreater sensitivity to prompt phrasing, reflecting tighter alignment with\ninstruction semantics, while open-source models are steadier but struggle with\nnuanced and complex phrasing. Based on this analysis, we propose Prompting\nPrinciples tailored to proprietary and open-source LMMs, enabling more robust\nand fair model evaluation.", "AI": {"tldr": "提出了一个针对大型多模态模型在多选题上的提示设计敏感度的系统性评估框架Promptception。不同模型对提示设计的敏感度不同，进而影响其性能表现。", "motivation": "由于在多选题解答时，即使是提示语的细微变化也会使大型多模态模型的准确性产生高达15%的变化，这使得对这些模型的透明和公平评估显得尤为挑战。为了应对这一问题，提出了系统性框架Promptception。", "method": "引入了Promptception，这是一个系统性框架，用于评估大型多模态模型对提示设计的敏感度。该框架包含61种提示类型，涵盖15个类别和6个超类别，每个类别针对提示设计的特定方面，用于在3个MCQA基准测试上评估10种大型多模态模型，包括轻量级开源模型到GPT-4o和Gemini 1.5 Pro。", "result": "研究结果揭示，专有模型对提示措辞更为敏感，反映出它们在指令语义上的严密对齐，而开源模型虽然更加稳定，但在应对细腻和复杂的措辞时却表现不佳。基于这一分析，提出了针对专有和开源大型多模态模型的提示原则，推动实现更强大和公平的模型评估。", "conclusion": "通过提出的评估框架发现，专有模型对提示语的措辞更加敏感，而开源模型虽然更稳定，但在处理细腻和复杂的措辞时存在困难。这一发现促进了针对专有和开源大型多模态模型的提示原则的制定，帮助实现更加鲁棒和公平的模型评估。"}}
{"id": "2509.03937", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.03937", "abs": "https://arxiv.org/abs/2509.03937", "authors": ["Yuhao Zhang", "Shaoming Duan", "Jinhang Su", "Chuanyi Liu", "Peiyi Han"], "title": "SPFT-SQL: Enhancing Large Language Model for Text-to-SQL Parsing by Self-Play Fine-Tuning", "comment": "EMNLP 2025 Findings", "summary": "Despite the significant advancements of self-play fine-tuning (SPIN), which\ncan transform a weak large language model (LLM) into a strong one through\ncompetitive interactions between models of varying capabilities, it still faces\nchallenges in the Text-to-SQL task. SPIN does not generate new information, and\nthe large number of correct SQL queries produced by the opponent model during\nself-play reduces the main model's ability to generate accurate SQL queries. To\naddress this challenge, we propose a new self-play fine-tuning method tailored\nfor the Text-to-SQL task, called SPFT-SQL. Prior to self-play, we introduce a\nverification-based iterative fine-tuning approach, which synthesizes\nhigh-quality fine-tuning data iteratively based on the database schema and\nvalidation feedback to enhance model performance, while building a model base\nwith varying capabilities. During the self-play fine-tuning phase, we propose\nan error-driven loss method that incentivizes incorrect outputs from the\nopponent model, enabling the main model to distinguish between correct SQL and\nerroneous SQL generated by the opponent model, thereby improving its ability to\ngenerate correct SQL. Extensive experiments and in-depth analyses on six\nopen-source LLMs and five widely used benchmarks demonstrate that our approach\noutperforms existing state-of-the-art (SOTA) methods.", "AI": {"tldr": "A new method for self-play fine-tuning, SPFT-SQL, is proposed to improve the Text-to-SQL performance of large language models by enhancing the fine-tuning data quality and adjusting the self-play loss function.", "motivation": "The motivation is to address the limitations of the SPIN method in generating accurate SQL queries by enhancing the fine-tuning process.", "method": "The paper proposes SPFT-SQL, which includes a verification-based iterative fine-tuning approach before self-play and an error-driven loss method during self-play to improve the ability of models to generate accurate SQL queries.", "result": "The approach demonstrates superior performance over state-of-the-art methods through extensive experiments on open-source LLMs and widely used benchmarks.", "conclusion": "Experiments show that the proposed SPFT-SQL outperforms existing state-of-the-art methods."}}
{"id": "2509.03999", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.03999", "abs": "https://arxiv.org/abs/2509.03999", "authors": ["Han Huang", "Han Sun", "Ningzhong Liu", "Huiyu Zhou", "Jiaquan Shen"], "title": "SliceSemOcc: Vertical Slice Based Multimodal 3D Semantic Occupancy Representation", "comment": "14 pages, accepted by PRCV2025", "summary": "Driven by autonomous driving's demands for precise 3D perception, 3D semantic\noccupancy prediction has become a pivotal research topic. Unlike\nbird's-eye-view (BEV) methods, which restrict scene representation to a 2D\nplane, occupancy prediction leverages a complete 3D voxel grid to model spatial\nstructures in all dimensions, thereby capturing semantic variations along the\nvertical axis. However, most existing approaches overlook height-axis\ninformation when processing voxel features. And conventional SENet-style\nchannel attention assigns uniform weight across all height layers, limiting\ntheir ability to emphasize features at different heights. To address these\nlimitations, we propose SliceSemOcc, a novel vertical slice based multimodal\nframework for 3D semantic occupancy representation. Specifically, we extract\nvoxel features along the height-axis using both global and local vertical\nslices. Then, a global local fusion module adaptively reconciles fine-grained\nspatial details with holistic contextual information. Furthermore, we propose\nthe SEAttention3D module, which preserves height-wise resolution through\naverage pooling and assigns dynamic channel attention weights to each height\nlayer. Extensive experiments on nuScenes-SurroundOcc and nuScenes-OpenOccupancy\ndatasets verify that our method significantly enhances mean IoU, achieving\nespecially pronounced gains on most small-object categories. Detailed ablation\nstudies further validate the effectiveness of the proposed SliceSemOcc\nframework.", "AI": {"tldr": "SliceSemOcc addresses the limitation of current 3D semantic occupancy prediction approaches by capturing height-wise features and providing a dynamic channel attention mechanism, showing significant improvements in mean IoU on nuScenes-SurroundOcc and nuScenes-OpenOccupancy datasets.", "motivation": "To improve 3D semantic occupancy prediction by capturing semantic variations along the vertical axis without overlooking height-axis information, which current methods tend to do.", "method": "SliceSemOcc, a novel vertical slice based multimodal framework, extracts voxel features along the height axis using global and local slices. A global local fusion module adaptively reconciles fine-grained spatial details with holistic contextual information, and the SEAttention3D module assigns dynamic channel attention weights to each height layer.", "result": "Experiments verify that the method significantly enhances mean IoU and performs particularly well on small-object categories.", "conclusion": "The proposed SliceSemOcc framework is shown to be effective in improving 3D semantic occupancy prediction, especially for capturing semantic variations along the vertical axis."}}
{"id": "2509.03940", "categories": ["cs.CL", "cs.AI", "cs.SD"], "pdf": "https://arxiv.org/pdf/2509.03940", "abs": "https://arxiv.org/abs/2509.03940", "authors": ["Weihao Wu", "Liang Cao", "Xinyu Wu", "Zhiwei Lin", "Rui Niu", "Jingbei Li", "Zhiyong Wu"], "title": "VoxRole: A Comprehensive Benchmark for Evaluating Speech-Based Role-Playing Agents", "comment": null, "summary": "Recent significant advancements in Large Language Models (LLMs) have greatly\npropelled the development of Role-Playing Conversational Agents (RPCAs). These\nsystems aim to create immersive user experiences through consistent persona\nadoption. However, current RPCA research faces dual limitations. First,\nexisting work predominantly focuses on the textual modality, entirely\noverlooking critical paralinguistic features including intonation, prosody, and\nrhythm in speech, which are essential for conveying character emotions and\nshaping vivid identities. Second, the speech-based role-playing domain suffers\nfrom a long-standing lack of standardized evaluation benchmarks. Most current\nspoken dialogue datasets target only fundamental capability assessments,\nfeaturing thinly sketched or ill-defined character profiles. Consequently, they\nfail to effectively quantify model performance on core competencies like\nlong-term persona consistency. To address this critical gap, we introduce\nVoxRole, the first comprehensive benchmark specifically designed for the\nevaluation of speech-based RPCAs. The benchmark comprises 13335 multi-turn\ndialogues, totaling 65.6 hours of speech from 1228 unique characters across 261\nmovies. To construct this resource, we propose a novel two-stage automated\npipeline that first aligns movie audio with scripts and subsequently employs an\nLLM to systematically build multi-dimensional profiles for each character.\nLeveraging VoxRole, we conduct a multi-dimensional evaluation of contemporary\nspoken dialogue models, revealing crucial insights into their respective\nstrengths and limitations in maintaining persona consistency.", "AI": {"tldr": "本研究提出了VoxRole，这是首个用于评估语音角色扮演对话系统的全面基准，以解决现有研究缺乏副语言特征处理及标准化评估的问题。", "motivation": "该研究旨在解决当前语音角色扮演对话系统评估基准缺失的问题，尤其是缺乏长期人格一致性评估的标准。现有的工作多集中在文本模式上，忽略了语音中的重要副语言特征。", "method": "该论文提出了一种新的两阶段自动化流水线来构建VoxRole，首先将电影音频与脚本对齐，然后使用大型语言模型系统地构建每个角色的多维特征。VoxRole 是一个全面的基准，由来自261部电影的1228个独特角色的13335个多轮对话组成，总时长为65.6小时的语音数据。", "result": "通过使用VoxRole，该研究对当前的口头对话模型进行了多维度的评估，揭示了这些模型在维持人格一致性方面的强项和弱点。", "conclusion": "VoxRole的引入填补了语音角色扮演对话系统评估中的重要空白，并为未来的相关研究提供了有价值的基准。"}}
{"id": "2509.04009", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.04009", "abs": "https://arxiv.org/abs/2509.04009", "authors": ["Solha Kang", "Esla Timothy Anzaku", "Wesley De Neve", "Arnout Van Messem", "Joris Vankerschaver", "Francois Rameau", "Utku Ozbulak"], "title": "Detecting Regional Spurious Correlations in Vision Transformers via Token Discarding", "comment": null, "summary": "Due to their powerful feature association capabilities, neural network-based\ncomputer vision models have the ability to detect and exploit unintended\npatterns within the data, potentially leading to correct predictions based on\nincorrect or unintended but statistically relevant signals. These clues may\nvary from simple color aberrations to small texts within the image. In\nsituations where these unintended signals align with the predictive task,\nmodels can mistakenly link these features with the task and rely on them for\nmaking predictions. This phenomenon is referred to as spurious correlations,\nwhere patterns appear to be associated with the task but are actually\ncoincidental. As a result, detection and mitigation of spurious correlations\nhave become crucial tasks for building trustworthy, reliable, and generalizable\nmachine learning models. In this work, we present a novel method to detect\nspurious correlations in vision transformers, a type of neural network\narchitecture that gained significant popularity in recent years. Using both\nsupervised and self-supervised trained models, we present large-scale\nexperiments on the ImageNet dataset demonstrating the ability of the proposed\nmethod to identify spurious correlations. We also find that, even if the same\narchitecture is used, the training methodology has a significant impact on the\nmodel's reliance on spurious correlations. Furthermore, we show that certain\nclasses in the ImageNet dataset contain spurious signals that are easily\ndetected by the models and discuss the underlying reasons for those spurious\nsignals. In light of our findings, we provide an exhaustive list of the\naforementioned images and call for caution in their use in future research\nefforts. Lastly, we present a case study investigating spurious signals in\ninvasive breast mass classification, grounding our work in real-world\nscenarios.", "AI": {"tldr": "The paper proposes a method to detect spurious correlations in vision transformers, investigates the impact of training methodologies, and provides a list of ImageNet images with spurious signals for caution in future research. A real-world application in breast mass classification is demonstrated.", "motivation": "The paper aims to address the issue of spurious correlations in vision transformers, calling for the development of trustworthy, reliable, and generalizable machine learning models. It highlights the need for detection and mitigation strategies for these correlations to improve model performance and reliability.", "method": "Content elaborates on the problem of spurious correlations in neural network-based computer vision models, focusing on vision transformers. It describes experiments conducted using the ImageNet dataset to detect these correlations and analyzes the impact of training methodologies on model reliance on such correlations. Additionally, it discusses class-specific spurious signals in ImageNet and provides a list of images with detected spurious signals. A case study on breast mass classification is also included.", "result": "The results demonstrate the effectiveness of the proposed method in detecting spurious correlations within vision transformers. The experiments show that training methodologies significantly influence a model's reliance on spurious correlations. The paper also reveals that some classes in the ImageNet dataset contain spurious signals that can be readily exploited by models.", "conclusion": "The paper concludes with a call to caution regarding the use of images with spurious signals in future research efforts and underscores the importance of robust methods to detect and mitigate such correlations for developing reliable neural network models."}}
{"id": "2509.03957", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.03957", "abs": "https://arxiv.org/abs/2509.03957", "authors": ["Ruiling Guo", "Xinwei Yang", "Chen Huang", "Tong Zhang", "Yong Hu"], "title": "CANDY: Benchmarking LLMs' Limitations and Assistive Potential in Chinese Misinformation Fact-Checking", "comment": "Findings of EMNLP 2025", "summary": "The effectiveness of large language models (LLMs) to fact-check\nmisinformation remains uncertain, despite their growing use. To this end, we\npresent CANDY, a benchmark designed to systematically evaluate the capabilities\nand limitations of LLMs in fact-checking Chinese misinformation. Specifically,\nwe curate a carefully annotated dataset of ~20k instances. Our analysis shows\nthat current LLMs exhibit limitations in generating accurate fact-checking\nconclusions, even when enhanced with chain-of-thought reasoning and few-shot\nprompting. To understand these limitations, we develop a taxonomy to categorize\nflawed LLM-generated explanations for their conclusions and identify factual\nfabrication as the most common failure mode. Although LLMs alone are unreliable\nfor fact-checking, our findings indicate their considerable potential to\naugment human performance when deployed as assistive tools in scenarios. Our\ndataset and code can be accessed at https://github.com/SCUNLP/CANDY", "AI": {"tldr": "研究了大语言模型在中文事实核查中的能力，发现虽然存在局限性，如“事实伪造”失败模式，但LLMs作为辅助工具具有改善人类性能的潜力。", "motivation": "随着大语言模型（LLMs）应用的增加，其在事实核查以阻止虚假信息的有效性尚不确定。为此，我们提出了CANDY基准来评估LLMs在检查虚假信息上的能力。", "method": "我们开发了CANDY基准，用于系统评估大语言模型（LLMs）在检查中文虚假信息的能力。我们编纂了一个包含约20,000个实例的标注数据集。", "result": "研究显示当前LLMs在生成准确的事实核查结论上存在限制。最常见的是所谓的“事实伪造”失败模式。同时，发现LLMs作为辅助工具能够显著提升人类处理能力。", "conclusion": "尽管LLMs在单独使用时对于事实核查不可靠，但研究结果表明它们作为辅助工具具有巨大的潜力。"}}
{"id": "2509.04023", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.04023", "abs": "https://arxiv.org/abs/2509.04023", "authors": ["Shiku Kaito", "Shinnosuke Matsuo", "Daiki Suehiro", "Ryoma Bise"], "title": "Learning from Majority Label: A Novel Problem in Multi-class Multiple-Instance Learning", "comment": "35 pages, 9 figures, Accepted in Pattern recognition", "summary": "The paper proposes a novel multi-class Multiple-Instance Learning (MIL)\nproblem called Learning from Majority Label (LML). In LML, the majority class\nof instances in a bag is assigned as the bag-level label. The goal of LML is to\ntrain a classification model that estimates the class of each instance using\nthe majority label. This problem is valuable in a variety of applications,\nincluding pathology image segmentation, political voting prediction, customer\nsentiment analysis, and environmental monitoring. To solve LML, we propose a\nCounting Network trained to produce bag-level majority labels, estimated by\ncounting the number of instances in each class. Furthermore, analysis\nexperiments on the characteristics of LML revealed that bags with a high\nproportion of the majority class facilitate learning. Based on this result, we\ndeveloped a Majority Proportion Enhancement Module (MPEM) that increases the\nproportion of the majority class by removing minority class instances within\nthe bags. Experiments demonstrate the superiority of the proposed method on\nfour datasets compared to conventional MIL methods. Moreover, ablation studies\nconfirmed the effectiveness of each module. The code is available at\n\\href{https://github.com/Shiku-Kaito/Learning-from-Majority-Label-A-Novel-Problem-in-Multi-class-Multiple-Instance-Learning}{here}.", "AI": {"tldr": "This paper presents a novel problem called Learning from Majority Label (LML) and proposes a solution based on a Majority Proportion Enhancement Module (MPEM) and a Counting Network to improve classification accuracy in multi-class Multiple-Instance Learning (MIL) scenarios.", "motivation": "The motivation behind this paper is to introduce the novel LML problem which assigns the majority class of instances in a bag as the bag-level label, addressing real-world scenarios such as medical imaging, voting prediction, customer sentiment analysis, and environmental monitoring.", "method": "The paper proposes a method called Majority Proportion Enhancement Module (MPEM) and a Counting Network to solve the Learning from Majority Label (LML) problem. The MPEM increases the proportion of the majority class by removing instances of the minority class in the bags. The Counting Network is used to produce the bag-level majority labels, which are estimated by counting the number of instances within each class.", "result": "The proposed method demonstrated superior performance compared to conventional MIL methods through extensive experiments on four datasets. Moreover, the ablation studies confirmed the effectiveness of each module.", "conclusion": "The paper concludes that the proposed method is effective for LML and outperforms traditional methods. It showcases the utility and potential of the LML and the applied solution for various real-world applications."}}
{"id": "2509.03962", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.03962", "abs": "https://arxiv.org/abs/2509.03962", "authors": ["Ulin Nuha", "Adam Jatowt"], "title": "Exploring NLP Benchmarks in an Extremely Low-Resource Setting", "comment": null, "summary": "The effectiveness of Large Language Models (LLMs) diminishes for extremely\nlow-resource languages, such as indigenous languages, primarily due to the lack\nof labeled data. Despite growing interest, the availability of high-quality\nnatural language processing (NLP) datasets for these languages remains limited,\nmaking it difficult to develop robust language technologies. This paper\naddresses such gap by focusing on Ladin, an endangered Romance language,\nspecifically targeting the Val Badia variant. Leveraging a small set of\nparallel Ladin-Italian sentence pairs, we create synthetic datasets for\nsentiment analysis and multiple-choice question answering (MCQA) by translating\nmonolingual Italian data. To ensure linguistic quality and reliability, we\napply rigorous filtering and back-translation procedures in our method. We\nfurther demonstrate that incorporating these synthetic datasets into machine\ntranslation training leads to substantial improvements over existing\nItalian-Ladin translation baselines. Our contributions include the first\npublicly available sentiment analysis and MCQA datasets for Ladin, establishing\nfoundational resources that can support broader NLP research and downstream\napplications for this underrepresented language.", "AI": {"tldr": "文章专注于资源稀缺的语言Ladin，创建了首次公开的情感分析和多项选择问答(MCQA)数据集，通过合成数据集提高了意大利语-Ladin翻译的质量。", "motivation": "针对资源极其匮乏的语言（如土著语言）的大型语言模型的有效性减弱，主要是由于缺乏标注数据。尽管人们对这些语言的自然语言处理(NLP)数据集的兴趣越来越大，但高质量NLP数据集的可用性仍然有限，这使得开发强大的语言技术变得困难。这篇论文通过专注于Ladin，这是一种濒危的罗曼语，特别是Val Badia方言，来解决这一差距。", "method": "通过使用少量的并行Ladin-意大利语句子对，我们创建了用于情感分析和多项选择问答(MCQA)的合成数据集，这些数据集是通过翻译单语意大利语数据生成的。为了确保语言质量和可靠性，我们在方法中应用了严格的过滤和反向翻译程序。", "result": "将这些合成数据集纳入机器翻译训练后，显著改进了已经存在的意大利语-Ladin翻译基线。", "conclusion": "我们的贡献包括首次公开发布的Ladin情感分析和MCQA数据集，为这一代表性不足的语言提供了基础性的研究资源，支持更广泛的NLP研究和下游应用程序。"}}
{"id": "2509.04043", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.04043", "abs": "https://arxiv.org/abs/2509.04043", "authors": ["Yuchen Zhu", "Longxiang Yin", "Kai Zhao"], "title": "Millisecond-Response Tracking and Gazing System for UAVs: A Domestic Solution Based on \"Phytium + Cambricon\"", "comment": "16 pages,17 figures", "summary": "In the frontier research and application of current video surveillance\ntechnology, traditional camera systems exhibit significant limitations of\nresponse delay exceeding 200 ms in dynamic scenarios due to the insufficient\ndeep feature extraction capability of automatic recognition algorithms and the\nefficiency bottleneck of computing architectures, failing to meet the real-time\nrequirements in complex scenes. To address this issue, this study proposes a\nheterogeneous computing architecture based on Phytium processors and Cambricon\naccelerator cards, constructing a UAV tracking and gazing system with\nmillisecond-level response capability. At the hardware level, the system adopts\na collaborative computing architecture of Phytium FT-2000/4 processors and\nMLU220 accelerator cards, enhancing computing power through multi-card\nparallelism. At the software level, it innovatively integrates a lightweight\nYOLOv5s detection network with a DeepSORT cascaded tracking algorithm, forming\na closed-loop control chain of \"detection-tracking-feedback\". Experimental\nresults demonstrate that the system achieves a stable single-frame\ncomprehensive processing delay of 50-100 ms in 1920*1080 resolution video\nstream processing, with a multi-scale target recognition accuracy of over\n98.5%, featuring both low latency and high precision. This study provides an\ninnovative solution for UAV monitoring and the application of domestic chips.", "AI": {"tldr": "研究提出了一种基于Phytium处理器和Cambricon加速卡的异构计算架构，用于构建具有毫秒级响应能力的无人机跟踪与监视系统。系统在处理1920*1080分辨率视频流时，实现了50-100毫秒的单帧处理延迟，并具有超过98.5%的多尺度目标识别准确率。", "motivation": "传统相机系统在动态场景中具有超过200毫秒的响应延迟，无法满足复杂场景下的实时要求。该研究旨在解决这一问题，提出了基于Phytium处理器和Cambricon加速卡的异构计算架构，构建具备毫秒级响应能力的无人机跟踪和凝视系统。", "method": "采用基于Phytium处理器和Cambricon加速卡的异构计算架构，构建了一个具有毫秒级响应能力的无人机跟踪和凝视系统。在硬件层面，系统采用Phytium FT-2000/4处理器和MLU220加速卡的协同计算架构，通过多卡并行提高计算能力。在软件层面，创新地融合了轻量级的YOLOv5s检测网络与级联式DeepSORT跟踪算法，形成了\"检测-跟踪-反馈\"的闭环控制链路。", "result": "实验结果显示，系统在处理1920*1080分辨率视频流时，实现了50-100毫秒的单帧综合处理延迟，多尺度目标识别准确率超过98.5%。", "conclusion": "本研究提供了一种创新的无人机监控解决方案，并在国产芯片的应用上取得了重要进展。实验结果展示了系统在处理1920*1080分辨率视频流时，能够实现50-100毫秒的单帧综合处理延迟，并具备超过98.5%的多尺度目标识别准确率。"}}
{"id": "2509.03972", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.03972", "abs": "https://arxiv.org/abs/2509.03972", "authors": ["Junghwan Lim", "Gangwon Jo", "Sungmin Lee", "Jiyoung Park", "Dongseok Kim", "Jihwan Kim", "Junhyeok Lee", "Wai Ting Cheung", "Dahye Choi", "Kibong Choi", "Jaeyeon Huh", "Beomgyu Kim", "Jangwoong Kim", "Taehyun Kim", "Haesol Lee", "Jeesoo Lee", "Dongpin Oh", "Changseok Song", "Daewon Suh"], "title": "Expanding Foundational Language Capabilities in Open-Source LLMs through a Korean Case Study", "comment": null, "summary": "We introduce Llama-3-Motif, a language model consisting of 102 billion\nparameters, specifically designed to enhance Korean capabilities while\nretaining strong performance in English. Developed on the Llama 3 architecture,\nLlama-3-Motif employs advanced training techniques, including LlamaPro and\nMasked Structure Growth, to effectively scale the model without altering its\ncore Transformer architecture. Using the MoAI platform for efficient training\nacross hyperscale GPU clusters, we optimized Llama-3-Motif using a carefully\ncurated dataset that maintains a balanced ratio of Korean and English data.\nLlama-3-Motif shows decent performance on Korean-specific benchmarks,\noutperforming existing models and achieving results comparable to GPT-4.", "AI": {"tldr": "Llama-3-Motif是一个设计用于增强韩语能力的语言模型，同时保持英语性能。模型使用先进训练技术开发，在韩语特定基准测试中表现出色，与GPT-4性能相当。", "motivation": "目的是提高语言模型在韩语能力上的表现，同时不牺牲其在英语上的性能。", "method": "介绍了Llama-3-Motif，这是一个由1020亿参数构成的语言模型，专为增强韩语能力而设计，同时保持在英语上的强大性能。该模型基于Llama 3架构，采用LlamaPro和Masked Structure Growth等先进训练技术进行开发，能够在不改变其核心Transformer架构的情况下有效地扩展模型。使用MoAI平台在超尺度GPU集群上进行高效训练，并通过精心策划的数据集进行优化，该数据集保持了韩语和英语数据的均衡比例。", "result": "在韩语特定的基准测试中，Llama-3-Motif表现良好，超过了现有模型，并且其性能与GPT-4相当。", "conclusion": "Llama-3-Motif在保持英文性能的同时，显著提升了韩语的表现，证明了其设计的有效性。"}}
{"id": "2509.04050", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.04050", "abs": "https://arxiv.org/abs/2509.04050", "authors": ["Quang-Huy Che", "Le-Chuong Nguyen", "Gia-Nghia Tran", "Dinh-Duy Phan", "Vinh-Tiep Nguyen"], "title": "A Re-ranking Method using K-nearest Weighted Fusion for Person Re-identification", "comment": "Published in ICPRAM 2025, ISBN 978-989-758-730-6, ISSN 2184-4313", "summary": "In person re-identification, re-ranking is a crucial step to enhance the\noverall accuracy by refining the initial ranking of retrieved results. Previous\nstudies have mainly focused on features from single-view images, which can\ncause view bias and issues like pose variation, viewpoint changes, and\nocclusions. Using multi-view features to present a person can help reduce view\nbias. In this work, we present an efficient re-ranking method that generates\nmulti-view features by aggregating neighbors' features using K-nearest Weighted\nFusion (KWF) method. Specifically, we hypothesize that features extracted from\nre-identification models are highly similar when representing the same\nidentity. Thus, we select K neighboring features in an unsupervised manner to\ngenerate multi-view features. Additionally, this study explores the weight\nselection strategies during feature aggregation, allowing us to identify an\neffective strategy. Our re-ranking approach does not require model fine-tuning\nor extra annotations, making it applicable to large-scale datasets. We evaluate\nour method on the person re-identification datasets Market1501, MSMT17, and\nOccluded-DukeMTMC. The results show that our method significantly improves\nRank@1 and mAP when re-ranking the top M candidates from the initial ranking\nresults. Specifically, compared to the initial results, our re-ranking method\nachieves improvements of 9.8%/22.0% in Rank@1 on the challenging datasets:\nMSMT17 and Occluded-DukeMTMC, respectively. Furthermore, our approach\ndemonstrates substantial enhancements in computational efficiency compared to\nother re-ranking methods.", "AI": {"tldr": "该研究开发了一种基于K-WF方法的重排序技术，能够生成多视角特征以减少视图偏差，显著提高行人再识别的Rank@1和mAP，且计算效率高于其他重排序方法。", "motivation": "传统的行人再识别方法中的重排序过程仅依赖单视角图像特征，导致视图偏差等问题。本研究旨在通过利用多视角特征缓解视图偏差，从而提高行人再识别的整体准确性。", "method": "研究采用了K-nearest Weighted Fusion (KWF)方法来生成多视角特征，通过选择K个在无监督情况下最接近的特征来生成表示同一身份的多视角特征，并探讨了特征聚合期间的权重选择策略。", "result": "该研究提出了一种高效的重排序方法，通过K-nearest Weighted Fusion (KWF)方法聚合邻居特征以生成多视角特征，从而减少视图偏差和改善行人再识别的准确性。实验结果表明，该方法在Market1501、MSMT17和Occluded-DukeMTMC数据集上，分别对MSMT17和Occluded-DukeMTMC数据集的Rank@1提高了9.8%和22.0%，并显示出显著的计算效率提升。", "conclusion": "新提出的重排序方法不仅能通过生成多视角特征提高行人再识别准确率，还提供了更好的计算效率，且无需模型微调和额外标注，适用于大规模数据集。"}}
