{"id": "2509.22674", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.22674", "abs": "https://arxiv.org/abs/2509.22674", "authors": ["Yash Thube"], "title": "Pathological Truth Bias in Vision-Language Models", "comment": "10 pages, 12 figures. Code for MATS released at\n  https://github.com/thubZ09/mats-spatial-reasoning", "summary": "Vision Language Models (VLMs) are improving quickly, but standard benchmarks\ncan hide systematic failures that reduce real world trust. We introduce MATS\n(Multimodal Audit for Truthful Spatialization), a compact behavioral audit that\nmeasures whether models reject visually contradicted statements, and two\nmetrics Spatial Consistency Score (SCS) and Incorrect Agreement Rate (IAR).\nInstruction tuned generative VLMs (LLaVA 1.5, QwenVLchat) exhibit very low SCS\nand high IAR, while contrastive encoders (CLIP, SigLIP) are far more robust.\nActivation patching causally localizes failure loci (mid to late cross\nattention for generative models, pooled projection components for contrastive\nmodels) and suggests concrete repair paths.", "AI": {"tldr": "研究提出MATS来评估视觉语言模型在面对视觉矛盾陈述时的表现，发现生成式模型容易失效，而对比编码器则更可靠。", "motivation": "视觉语言模型（VLMs）虽然在迅速进步，但标准基准测试可能会隐藏系统性失败，从而降低现实世界的信任度。", "method": "引入了MATS（多模态真实空间化审计），用于测量模型是否拒绝视觉矛盾的陈述，并提出了两个指标：空间一致性得分（SCS）和错误同意率（IAR）。", "result": "指令调优的生成式VLMs（如LLaVA 1.5，QwenVLchat）显示出很低的SCS和很高的IAR，而对比编码器（如CLIP，SigLIP）则更为稳健。", "conclusion": "激活补丁法定位了模型失效的具体原因，并提出了具体的修复路径，该方法可以用于改进模型。"}}
{"id": "2509.22686", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.22686", "abs": "https://arxiv.org/abs/2509.22686", "authors": ["Shinji Yamashita", "Yuma Kinoshita", "Hitoshi Kiya"], "title": "Scale and Rotation Estimation of Similarity-Transformed Images via Cross-Correlation Maximization Based on Auxiliary Function Method", "comment": "accepted to APSIPA ASC 2025 (to appear). 5 pages, 4 figures", "summary": "This paper introduces a highly efficient algorithm capable of jointly\nestimating scale and rotation between two images with sub-pixel precision.\nImage alignment serves as a critical process for spatially registering images\ncaptured from different viewpoints, and finds extensive use in domains such as\nmedical imaging and computer vision. Traditional phase-correlation techniques\nare effective in determining translational shifts; however, they are inadequate\nwhen addressing scale and rotation changes, which often arise due to camera\nzooming or rotational movements. In this paper, we propose a novel algorithm\nthat integrates scale and rotation estimation based on the Fourier transform in\nlog-polar coordinates with a cross-correlation maximization strategy,\nleveraging the auxiliary function method. By incorporating sub-pixel-level\ncross-correlation our method enables precise estimation of both scale and\nrotation. Experimental results demonstrate that the proposed method achieves\nlower mean estimation errors for scale and rotation than conventional Fourier\ntransform-based techniques that rely on discrete cross-correlation.", "AI": {"tldr": "The paper introduces an efficient algorithm for joint scale and rotation estimation between two images with sub-pixel precision, using Fourier transform in log-polar coordinates and a cross-correlation maximization strategy.", "motivation": "Traditional phase-correlation methods are limited in handling scale and rotation changes, common in camera zooming or rotational movements, and are incapable of providing accurate alignment in these cases.", "method": "The proposed method integrates scale and rotation estimation through Fourier transform in log-polar coordinates and maximizes cross-correlation using an auxiliary function method to achieve sub-pixel precision.", "result": "Experiments show lower mean estimation errors for scale and rotation compared to conventional Fourier transform-based techniques that employ discrete cross-correlation.", "conclusion": "The new algorithm offers a more precise alternative to existing methods for image alignment when scale and rotation changes are involved."}}
{"id": "2509.22688", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.22688", "abs": "https://arxiv.org/abs/2509.22688", "authors": ["Xu Jia"], "title": "Robust Object Detection for Autonomous Driving via Curriculum-Guided Group Relative Policy Optimization", "comment": null, "summary": "Multimodal Large Language Models (MLLMs) excel in vision-language reasoning\nbut often struggle with structured perception tasks requiring precise\nlocalization and robustness. We propose a reinforcement learning framework that\naugments Group Relative Policy Optimization (GRPO) with curriculum-based data\nscheduling and difficulty-aware filtering. This approach stabilizes\noptimization under sparse, noisy rewards and enables progressive adaptation to\ncomplex samples. Evaluations on autonomous driving benchmarks demonstrate\nsubstantial improvements in detection accuracy and robustness. Ablation studies\nconfirm the importance of reward design, KL regularization, and curriculum\npacing for convergence stability and generalization. Our findings highlight\nreinforcement-driven optimization with structured data curricula as a scalable\npath toward robust and interpretable multimodal detection.", "AI": {"tldr": "为了改进多模式大型语言模型在结构化感知任务中的表现，提出了一种强化学习框架，该框架通过策略优化和课程学习来提升模型的鲁棒性和精确性，实验结果和消融研究均支持该方法的有效性。", "motivation": "多模式大型语言模型（MLLMs）在视觉-语言推理方面表现出色，但在需要精确定位和鲁棒性的结构化感知任务中表现不佳。", "method": "提出了一种基于强化学习的框架，该框架通过结合基于课程的数据调度和困难感知过滤来增强组相对策略优化（GRPO）。", "result": "在自动驾驶基准测试中展示了检测精度和鲁棒性的显著提升。消融研究证实了奖励设计、KL正则化和课程节奏对于收敛稳定性和泛化能力的重要性。", "conclusion": "发现以强化学习驱动的优化结合结构化数据课程作为提升鲁棒性和可解释性多模式检测的可扩展路径。"}}
{"id": "2509.22689", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.22689", "abs": "https://arxiv.org/abs/2509.22689", "authors": ["Ha-Hieu Pham", "Minh Le", "Han Huynh", "Nguyen Quoc Khanh Le", "Huy-Hieu Pham"], "title": "Graph-Theoretic Consistency for Robust and Topology-Aware Semi-Supervised Histopathology Segmentation", "comment": null, "summary": "Semi-supervised semantic segmentation (SSSS) is vital in computational\npathology, where dense annotations are costly and limited. Existing methods\noften rely on pixel-level consistency, which propagates noisy pseudo-labels and\nproduces fragmented or topologically invalid masks. We propose Topology Graph\nConsistency (TGC), a framework that integrates graph-theoretic constraints by\naligning Laplacian spectra, component counts, and adjacency statistics between\nprediction graphs and references. This enforces global topology and improves\nsegmentation accuracy. Experiments on GlaS and CRAG demonstrate that TGC\nachieves state-of-the-art performance under 5-10% supervision and significantly\nnarrows the gap to full supervision. Code is available at\nhttps://github.com/hieuphamha19/TGC.", "AI": {"tldr": "提出了一种新的框架Topology Graph Consistency (TGC)，用于半监督语义分割，在计算病理学中显著改善了噪声伪标签的问题，并在GlaS和CRAG数据集上展示了优越的性能。", "motivation": "半监督语义分割（Semi-supervised semantic segmentation, SSSS）对于计算病理学至关重要，在计算病理学中，密集的注释成本高昂且有限。现有方法通常依赖于像素级一致性，这会传播噪声伪标签并生成破碎或拓扑无效的掩码。", "method": "我们提出了一种名为拓扑图一致性（Topology Graph Consistency, TGC）的框架，该框架通过在预测图和参考图之间对齐拉普拉斯谱、连通分量数量和邻接统计量，整合了图论约束，从而强制全局拓扑结构，提高了分割准确性。", "result": "实验结果表明，TGC在5-10%的监督下达到了最先进的性能，并大大缩短了与完全监督之间的差距。", "conclusion": "TGC是一个能够显著提高半监督语义分割效果的方法，在计算病理学领域显示出巨大潜力。"}}
{"id": "2509.22699", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.22699", "abs": "https://arxiv.org/abs/2509.22699", "authors": ["Alessandra Urbinati", "Mirko Lai", "Simona Frenda", "Marco Antonio Stranisci"], "title": "Are you sure? Measuring models bias in content moderation through uncertainty", "comment": "accepted at Findings of ACL: EMNLP 2025", "summary": "Automatic content moderation is crucial to ensuring safety in social media.\nLanguage Model-based classifiers are being increasingly adopted for this task,\nbut it has been shown that they perpetuate racial and social biases. Even if\nseveral resources and benchmark corpora have been developed to challenge this\nissue, measuring the fairness of models in content moderation remains an open\nissue. In this work, we present an unsupervised approach that benchmarks models\non the basis of their uncertainty in classifying messages annotated by people\nbelonging to vulnerable groups. We use uncertainty, computed by means of the\nconformal prediction technique, as a proxy to analyze the bias of 11 models\nagainst women and non-white annotators and observe to what extent it diverges\nfrom metrics based on performance, such as the $F_1$ score. The results show\nthat some pre-trained models predict with high accuracy the labels coming from\nminority groups, even if the confidence in their prediction is low. Therefore,\nby measuring the confidence of models, we are able to see which groups of\nannotators are better represented in pre-trained models and lead the debiasing\nprocess of these models before their effective use.", "AI": {"tldr": "本研究提出了一种无监督方法，利用模型分类的不确定性来检验其对女性和非白人标注者是否存在偏见。", "motivation": "自动化内容审查对于确保社交媒体的安全至关重要，但基于语言模型的分类器已被证实会延续种族和社会偏见。尽管开发了多种资源和基准语料库来应对这一问题，但在内容审查中测量模型的公平性仍然是一个开放问题。", "method": "采用了无监督的方法来评估模型在分类属于弱势群体的人所标注的信息时的不确定性。使用了符合预测技术计算的不确定性作为分析模型对女性和非白人标注者偏见的代理指标。", "result": "结果表明，一些预训练模型能够高精度预测来自少数群体的标签，即使它们对预测的置信度较低。通过测量模型的置信度，可以识别出在预训练模型中更好代表的注释者群体。", "conclusion": "通过衡量模型的置信度，可以识别出预训练模型中更受代表的注释者群体，并在模型实际应用前指导其去偏过程。"}}
{"id": "2509.22690", "categories": ["cs.CV", "68T45, 65D19"], "pdf": "https://arxiv.org/pdf/2509.22690", "abs": "https://arxiv.org/abs/2509.22690", "authors": ["Andrea Asperti", "Salvatore Fiorilla", "Simone Nardi", "Lorenzo Orsini"], "title": "A review of Recent Techniques for Person Re-Identification", "comment": null, "summary": "Person re-identification (ReId), a crucial task in surveillance, involves\nmatching individuals across different camera views. The advent of Deep\nLearning, especially supervised techniques like Convolutional Neural Networks\nand Attention Mechanisms, has significantly enhanced person Re-ID. However, the\nsuccess of supervised approaches hinges on vast amounts of annotated data,\nposing scalability challenges in data labeling and computational costs. To\naddress these limitations, recent research has shifted towards unsupervised\nperson re-identification. Leveraging abundant unlabeled data, unsupervised\nmethods aim to overcome the need for pairwise labelled data. Although\ntraditionally trailing behind supervised approaches, unsupervised techniques\nhave shown promising developments in recent years, signalling a narrowing\nperformance gap. Motivated by this evolving landscape, our survey pursues two\nprimary objectives. First, we review and categorize significant publications in\nsupervised person re-identification, providing an in-depth overview of the\ncurrent state-of-the-art and emphasizing little room for further improvement in\nthis domain. Second, we explore the latest advancements in unsupervised person\nre-identification over the past three years, offering insights into emerging\ntrends and shedding light on the potential convergence of performance between\nsupervised and unsupervised paradigms. This dual-focus survey aims to\ncontribute to the evolving narrative of person re-identification, capturing\nboth the mature landscape of supervised techniques and the promising outcomes\nin the realm of unsupervised learning.", "AI": {"tldr": "The paper surveys both supervised and unsupervised person re-identification techniques, highlighting advancements and the narrowing performance gap between them.", "motivation": "To address the scalability issues of supervised person re-identification due to the need for large amounts of labeled data, the paper reviews both supervised and unsupervised techniques, focusing on recent advancements.", "method": "The paper categorizes and reviews significant publications in supervised person re-identification and explores recent advancements in unsupervised person re-identification.", "result": "The survey uncovers limited scope for improvement in supervised methods and highlights promising developments in unsupervised methods.", "conclusion": "The paper concludes by suggesting that performance gaps between supervised and unsupervised methods are narrowing, with potential convergence on the horizon."}}
{"id": "2509.22703", "categories": ["cs.CL", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2509.22703", "abs": "https://arxiv.org/abs/2509.22703", "authors": ["Srikant Panda", "Amit Agarwal", "Hitesh Laxmichand Patel"], "title": "AccessEval: Benchmarking Disability Bias in Large Language Models", "comment": null, "summary": "Large Language Models (LLMs) are increasingly deployed across diverse domains\nbut often exhibit disparities in how they handle real-life queries. To\nsystematically investigate these effects within various disability contexts, we\nintroduce \\textbf{AccessEval (Accessibility Evaluation)}, a benchmark\nevaluating 21 closed- and open-source LLMs across 6 real-world domains and 9\ndisability types using paired Neutral and Disability-Aware Queries. We\nevaluated model outputs with metrics for sentiment, social perception, and\nfactual accuracy.\n  Our analysis reveals that responses to disability-aware queries tend to have\na more negative tone, increased stereotyping, and higher factual error compared\nto neutral queries. These effects show notable variation by domain and\ndisability type, with disabilities affecting hearing, speech, and mobility\ndisproportionately impacted. These disparities reflect persistent forms of\nableism embedded in model behavior.\n  By examining model performance in real-world decision-making contexts, we\nbetter illuminate how such biases can translate into tangible harms for\ndisabled users. This framing helps bridges the gap between technical evaluation\nand user impact, reinforcing importance of bias mitigation in day-to-day\napplications. Our dataset is publicly available at:\nhttps://huggingface.co/datasets/Srikant86/AccessEval", "AI": {"tldr": "AccessEval测试了大语言模型在处理不同类型的残疾意识查询时的准确性、社会感知和情感倾向，发现这些模型在处理残疾相关的查询时表现出更多的负面情感、刻板印象和事实错误。", "motivation": "LLMs在处理现实生活查询时表现不一，研究其在不同残疾上下文中的表现，揭示大规模语言模型中的偏见为何可能会对残障用户造成实质性伤害。", "method": "通过引入AccessEval（无障碍评估）基准测试，系统化地研究在不同残疾语境中的影响，测试了21个闭源和开源的大语言模型，在6个现实世界领域和9种残疾类型中，使用配对的中性和残疾导向查询。", "result": "研究结果显示，对于残疾意识查询，模型的回答往往具有更多的负面情绪、增加的刻板印象和更高的事实错误。这些影响在领域和残疾类型之间存在显著差异，其中听力、言语和移动障碍的影响尤为严重。", "conclusion": "通过在现实世界决策上下文中检验模型性能，更好地揭示了这种偏见如何转化为针对残障用户的固定伤害。这有助于在技术和用户影响之间的评估上建立桥梁，强化日常应用中减少偏见的重要性。"}}
{"id": "2509.22691", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.22691", "abs": "https://arxiv.org/abs/2509.22691", "authors": ["Yan Wen", "Peng Ye", "Lin Zhang", "Baopu Li", "Jiakang Yuan", "Yaoxin Yang", "Tao Chen"], "title": "Sequential Token Merging: Revisiting Hidden States", "comment": null, "summary": "Vision Mambas (ViMs) achieve remarkable success with sub-quadratic\ncomplexity, but their efficiency remains constrained by quadratic token scaling\nwith image resolution. While existing methods address token redundancy, they\noverlook ViMs' intrinsic Limited Directional Sequential Dependence (LDSD) - a\ncritical information flow mechanism revealed in our analysis. We further\nidentify Mamba's selective scan enables gradual information aggregation in\nhidden states. Based on these insights, we propose Sequential Token Merging\n(STM), featuring: 1) Bidirectional nearest neighbor merging to preserve\nsequential dependencies through symmetric spatial aggregation, and 2) Hidden\nstates protection to stabilize the hidden states around the class token. STM\nstrategically leverages Mamba's layer-wise loss convergence to convert temporal\nforgetfulness into stability. Experiments demonstrate STM's superiority: 1.0%\naccuracy drop for ViM-Ti at 20% token reduction, and only 1.4% degradation for\nViM-S at 40% reduction. Our method achieves state-of-the-art efficiency with\nminimal complexity, while providing new insights into state-space model\ndynamics. Codes will be released soon.", "AI": {"tldr": "The paper introduces Sequential Token Merging (STM) to improve the efficiency of Vision Mambas (ViMs) by addressing their intrinsic Limited Directional Sequential Dependence (LDSD), resulting in better token efficiency with minimal accuracy losses and paving the way for more efficient state-space models. Codes will be released soon for further research and development in this field.", "motivation": "The existing methods do not fully address the efficiency issues of Vision Mambas (ViMs) due to overlooked intrinsic Limited Directional Sequential Dependence (LDSD). The goal is to improve token efficiency by reducing redundancy while preserving critical information flow.", "method": "Sequential Token Merging (STM) is proposed, which includes bidirectional nearest neighbor merging to maintain sequential dependencies and hidden states protection to stabilize states around the class token. This method uses the layer-wise loss convergence of Mamba to enhance stability over time.", "result": "Experiments show that STM can achieve a 20% token reduction with only a 1.0% accuracy drop for ViM-Ti and a 40% token reduction with only a 1.4% degradation for ViM-S.", "conclusion": "STM successfully improves the efficiency of ViMs with minimal accuracy degradation, establishing a new state-of-the-art in efficiency. The method also provides new insights into the state-space model dynamics."}}
{"id": "2509.22713", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.22713", "abs": "https://arxiv.org/abs/2509.22713", "authors": ["Kaishuai Xu", "Wenjun Hou", "Yi Cheng", "Wenjie Li"], "title": "RAR$^2$: Retrieval-Augmented Medical Reasoning via Thought-Driven Retrieval", "comment": "Accepted by EMNLP 2025 Findings", "summary": "Large Language Models (LLMs) have shown promising performance on diverse\nmedical benchmarks, highlighting their potential in supporting real-world\nclinical tasks. Retrieval-Augmented Generation (RAG) has emerged as a key\napproach for mitigating knowledge gaps and hallucinations by incorporating\nexternal medical information. However, RAG still struggles with complex medical\nquestions that require intensive reasoning, as surface-level input often fails\nto reflect the true knowledge needs of the task. Existing methods typically\nfocus on refining queries without explicitly modeling the reasoning process,\nlimiting their ability to retrieve and integrate clinically relevant knowledge.\nIn this work, we propose RAR$^2$, a joint learning framework that improves both\nReasoning-Augmented Retrieval and Retrieval-Augmented Reasoning. RAR$^2$\nconstructs a thought process to uncover implicit knowledge requirements and\nuses it to guide retrieval and answer generation. We build a training dataset\nof mixed preference pairs and apply Direct Preference Optimization (DPO) to\ntrain the model. Moreover, we design two test-time scaling strategies to\nexplore the boundaries of our framework. Experiments demonstrate the\neffectiveness of RAR$^2$ across several biomedical question answering datasets,\noutperforming RAG baselines with or without fine-tuning.", "AI": {"tldr": "RAR$^2$框架提升了医学问答的性能，通过结合推理增强检索和检索增强推理来优化模型。实验表明该框架优于传统的RAG方法。", "motivation": "现有的RAG方法在处理需要复杂推理的医学问题上表现不佳。这些问题通常需要超出表面层的输入才能反映出任务的真实知识需求。", "method": "RAR$^2$, 一个联合学习框架，同时优化推理增强检索和检索增强推理。RAR$^2$ 构建了一个思维过程来揭示隐含的知识需求，并用它来指导检索和回答生成。", "result": "实验结果证明了RAR$^2$在多个生物医学问答数据集上的有效性，超过了RAG基线模型，无论是否进行了微调。", "conclusion": "RAR$^2$通过优化推理和检索的联合过程，能够更有效地识别和整合临床相关知识，在医学问答任务中有良好的表现。"}}
{"id": "2509.22692", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.22692", "abs": "https://arxiv.org/abs/2509.22692", "authors": ["Le Zhang", "Ao Li", "Qibin Hou", "Ce Zhu", "Yonina C. Eldar"], "title": "Deep Learning Empowered Super-Resolution: A Comprehensive Survey and Future Prospects", "comment": "Accepted by Proceedings of the IEEE", "summary": "Super-resolution (SR) has garnered significant attention within the computer\nvision community, driven by advances in deep learning (DL) techniques and the\ngrowing demand for high-quality visual applications. With the expansion of this\nfield, numerous surveys have emerged. Most existing surveys focus on specific\ndomains, lacking a comprehensive overview of this field. Here, we present an\nin-depth review of diverse SR methods, encompassing single image\nsuper-resolution (SISR), video super-resolution (VSR), stereo super-resolution\n(SSR), and light field super-resolution (LFSR). We extensively cover over 150\nSISR methods, nearly 70 VSR approaches, and approximately 30 techniques for SSR\nand LFSR. We analyze methodologies, datasets, evaluation protocols, empirical\nresults, and complexity. In addition, we conducted a taxonomy based on each\nbackbone structure according to the diverse purposes. We also explore valuable\nyet under-studied open issues in the field. We believe that this work will\nserve as a valuable resource and offer guidance to researchers in this domain.\nTo facilitate access to related work, we created a dedicated repository\navailable at https://github.com/AVC2-UESTC/Holistic-Super-Resolution-Review.", "AI": {"tldr": "该论文提供了关于各类超分辨率方法的全面综述，包括150多种单图像超分辨率（SISR）、近70种视频超分辨率（VSR）和大约30种立体超分辨率（SSR）以及光场超分辨率（LFSR）技术。论文还探讨了领域的开放性问题。", "motivation": "由于深度学习技术的发展和高质量视觉应用需求的增长，超分辨率成为计算机视觉领域的一个热门话题。目前大多数综述只专注于某些特定领域，缺乏一个全面的覆盖。因此，该论文旨在填补这一空白。", "method": "Structure", "result": "{\n  \"tldr\": \"该论文提供了关于各类超分辨率方法的全面综述，包括150多种单图像超分辨率（SISR）、近70种视频超分辨率（VSR）和大约30种立体超分辨率（SSR）以及光场超分辨率（LFSR）技术。论文还探讨了领域的开放性问题。\", \n  \"motivation\": \"由于深度学习技术的发展和高质量视觉应用需求的增长，超分辨率成为计算机视觉领域的一个热门话题。目前大多数综述只专注于某些特定领域，缺乏一个全面的覆盖。因此，该论文旨在填补这一空白。\", \n  \"method\": \"论文综述了多种超分辨率方法，并对这些方法进行了分类和分析，还涵盖了相关数据集、评估协议、实证结果和复杂性。\", \n  \"result\": \"该论文分析了多种方法并基于不同的骨干结构对进行类别划分，探讨了有价值但较少被研究的问题。\", \n  \"conclusion\": \"此研究提供了一个宝贵的资源，用于指导超分辨率领域的研究人员。此外，创建了一个名为https://github.com/AVC2-UESTC/Holistic-Super-Resolution-Review的公开库来便于访问相关信息。\" \n}", "conclusion": "此研究提供了一个宝贵的资源，用于指导超分辨率领域的研究人员。此外，创建了一个名为https://github.com/AVC2-UESTC/Holistic-Super-Resolution-Review的公开库来便于访问相关信息。"}}
{"id": "2509.22715", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.22715", "abs": "https://arxiv.org/abs/2509.22715", "authors": ["Jiho Park", "Jongyoon Song", "Minjin Choi", "Kyuho Heo", "Taehun Huh", "Ji Won Kim"], "title": "TRUEBench: Can LLM Response Meet Real-world Constraints as Productivity Assistant?", "comment": "Accepted to EMNLP 2025 Findings", "summary": "Large language models (LLMs) are increasingly integral as productivity\nassistants, but existing benchmarks fall short in rigorously evaluating their\nreal-world instruction-following capabilities. Current benchmarks often (i)\nlack sufficient multilinguality, (ii) fail to capture the implicit constraints\ninherent in user requests, and (iii) overlook the complexities of multi-turn\ndialogue. To address these critical gaps and provide a more realistic\nassessment, we introduce TRUEBench (Trustworthy Real-world Usage Evaluation\nBenchmark)1, a novel benchmark specifically designed for LLM-based productivity\nassistants. TRUEBench distinguishes itself by featuring input prompts across 12\nlanguages, incorporating intra-instance multilingual instructions, employing\nrigorous evaluation criteria to capture both explicit and implicit constraints,\nand including complex multi-turn dialogue scenarios with both accumulating\nconstraints and context switches. Furthermore, to ensure reliability in\nevaluation, we refined constraints using an LLM validator. Extensive\nexperiments demonstrate that TRUEBench presents significantly greater\nchallenges than existing benchmarks; for instance, a strong model like OpenAI\no1 achieved only a 69.07% overall pass rate. TRUEBench offers a demanding and\nrealistic assessment of LLMs in practical productivity settings, highlighting\ntheir capabilities and limitations.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2509.22697", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.22697", "abs": "https://arxiv.org/abs/2509.22697", "authors": ["Abhiroop Chatterjee", "Susmita Ghosh"], "title": "Learning Hyperspectral Images with Curated Text Prompts for Efficient Multimodal Alignment", "comment": "Accepted at the IEEE/CVF International Conference on Computer Vision\n  (ICCV 2025), Workshop on Curated Data for Efficient Learning", "summary": "As data requirements continue to grow, efficient learning increasingly\ndepends on the curation and distillation of high-value data rather than\nbrute-force scaling of model sizes. In the case of a hyperspectral image (HSI),\nthe challenge is amplified by the high-dimensional 3D voxel structure, where\neach spatial location is associated with hundreds of contiguous spectral\nchannels. While vision and language models have been optimized effectively for\nnatural image or text tasks, their cross-modal alignment in the hyperspectral\ndomain remains an open and underexplored problem. In this article, we make an\nattempt to optimize a Vision-Language Model (VLM) for hyperspectral scene\nunderstanding by exploiting a CLIP-style contrastive training framework. Our\nframework maps voxel-level embeddings from a vision backbone onto the latent\nspace of a frozen large embedding model (LEM), where a trainable probe aligns\nvision features with the model's textual token representations. The two\nmodalities are aligned via a contrastive loss restricted to a curated set of\nhard (closest wrong classes) and semi-hard (random distractors) negatives,\nalong with positive pairs. To further enhance alignment, descriptive prompts\nthat encode class semantics are introduced and act as structured anchors for\nthe HSI embeddings. It is seen that the proposed method updates only 0.07\npercent of the total parameters, yet yields state-of-the-art performance. For\nexample, on Indian Pines (IP) the model produces better results over unimodal\nand multimodal baselines by +0.92 Overall Accuracy (OA) and +1.60 Kappa\n($\\kappa$), while on Pavia University (PU) data it provides gains of +0.69 OA\nand +0.90 $\\kappa$. Moreover, this is achieved with the set of parameters,\nnearly 50$\\times$ smaller than DCTN and 90$\\times$ smaller than SS-TMNet.", "AI": {"tldr": "文章提出了一种基于CLIP框架的对比学习方法，用于优化视觉语言模型处理高光谱图像的任务。通过引入描述性质的提示，提高视觉和文本特征的对齐效果，仅需更新总参数的0.07%就获得了显著的性能提升。", "motivation": "因模型缩放成本高，文章希望优化高光谱图像处理方法，特别是视觉语言模型在高光谱领域的跨模态对齐问题，以更高效地利用数据。", "method": "该方法利用CLIP样式的对比框架，通过一个可训练的探测器将视觉特征与大嵌入模型的文本词表示对齐，同时引入描述性质的提示以增强这种对齐。", "result": "实验结果显示，该方法在Indian Pines (IP) 和Pavia University (PU) 数据集上显著优于一模态和多模态基线模型，且参数量远低于传统的DCTN和SS-TMNet模型。", "conclusion": "实验结果证明，该方法通过少量参数更新，便可大幅提升高光谱图像分析性能，为视觉语言模型在高光谱领域的应用提供了一种高效新途径。"}}
{"id": "2509.22729", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.22729", "abs": "https://arxiv.org/abs/2509.22729", "authors": ["Sadia Abdulhalim", "Muaz Albaghdadi", "Moshiur Farazi"], "title": "Multi-Modal Sentiment Analysis with Dynamic Attention Fusion", "comment": "Paper accepted for presentation at the ACS/IEEE 22nd International\n  Conference on Computer Systems and Applications (AICCSA 2025)", "summary": "Traditional sentiment analysis has long been a unimodal task, relying solely\non text. This approach overlooks non-verbal cues such as vocal tone and prosody\nthat are essential for capturing true emotional intent. We introduce Dynamic\nAttention Fusion (DAF), a lightweight framework that combines frozen text\nembeddings from a pretrained language model with acoustic features from a\nspeech encoder, using an adaptive attention mechanism to weight each modality\nper utterance. Without any finetuning of the underlying encoders, our proposed\nDAF model consistently outperforms both static fusion and unimodal baselines on\na large multimodal benchmark. We report notable gains in F1-score and\nreductions in prediction error and perform a variety of ablation studies that\nsupport our hypothesis that the dynamic weighting strategy is crucial for\nmodeling emotionally complex inputs. By effectively integrating verbal and\nnon-verbal information, our approach offers a more robust foundation for\nsentiment prediction and carries broader impact for affective computing\napplications -- from emotion recognition and mental health assessment to more\nnatural human computer interaction.", "AI": {"tldr": "DAF模型结合文本和语音特征进行情感分析，提高了预测准确性。", "motivation": "传统的基于文本的单模态情感分析忽视了如语音语调等非语言线索，这些线索对捕获真实情感意图至关重要。", "method": "提出Dynamic Attention Fusion (DAF)，一个轻量级框架，该框架利用自适应注意力机制将来自预训练语言模型的冻结文本嵌入与来自语音编码器的声学特征相结合。", "result": "在大规模多模态基准测试中，无需调整底层编码器的情况下，提出的DAF模型在F1分数上取得了显著增益，并减少了预测误差。", "conclusion": "通过有效整合语言和非语言信息，该方法为情感预测提供了更强大的基础，并对情感计算应用（如情感识别、心理健康评估及更自然的人机交互）具有更广泛的影响。"}}
{"id": "2509.22700", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.22700", "abs": "https://arxiv.org/abs/2509.22700", "authors": ["Zhuang Qi", "Pan Yu", "Lei Meng", "Sijin Zhou", "Han Yu", "Xiaoxiao Li", "Xiangxu Meng"], "title": "Global Prompt Refinement with Non-Interfering Attention Masking for One-Shot Federated Learning", "comment": "NeurIPS'25 accepted", "summary": "Federated Prompt Learning (FPL) enables communication-efficient adaptation by\ntuning lightweight prompts on top of frozen pre-trained models. Existing FPL\nmethods typically rely on global information, which is only available after the\nsecond training round, to facilitate collaboration among client models.\nTherefore, they are inherently dependent on multi-round communication to fully\nexhibit their strengths. Moreover, existing one-shot federated learning methods\ntypically focus on fitting seen tasks, but lack cross-task generalization. To\nbridge this gap, we propose the Global Prompt Refinement with Non-Interfering\nAttention Masking (GPR-NIAM) method for one-shot FPL. The core idea is to\ndesign a masking mechanism that restricts excessive interaction between the\noriginal text embeddings and the learnable prompt embeddings. GPR-NIAM achieves\nthis through the collaboration of two key modules. Firstly, the attention\nisolation module suppresses attention from the learnable prompt tokens to the\noriginal text tokens, and reweights the reverse attention which preserves\ngeneralization across tasks. Secondly, the cross-silo collaborative refinement\nmodule integrates decentralized visual knowledge into a unified base and\ncalibrates the global prompt through multi-source cross-modal knowledge\nalignment, further mitigating the inconsistency caused by data heterogeneity.\nExtensive experiments conducted on ten benchmark datasets under two tasks show\nthat GPR-NIAM outperforms eight state-of-the-art methods in both class-level\nand domain-level generalization.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2509.22738", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.22738", "abs": "https://arxiv.org/abs/2509.22738", "authors": ["Parikshit Bansal", "Sujay Sanghavi"], "title": "Enabling Approximate Joint Sampling in Diffusion LMs", "comment": null, "summary": "In autoregressive language models, each token is sampled by conditioning on\nall the past tokens; the overall string has thus been sampled from the correct\nunderlying joint distribution represented by the model. In contrast, masked\ndiffusion language models generate text by unmasking tokens out of order and\npotentially in parallel. Generating an overall string sampled from the correct\nunderlying joint distribution would (again) require exactly one token unmasking\nin every full-model forward pass. The more tokens unmasked in parallel, the\nfurther away the string is from the true joint; this can be seen in the\nresulting drop in accuracy (but, increase in speed). In this paper we devise a\nway to {\\em approximately} sample multiple tokens from the joint distribution\nin a single full-model forward pass; we do so by developing a new lightweight\nsingle-layer ``sampler\" on top of an existing large diffusion LM. One forward\npass of the full model can now be followed by multiple forward passes of only\nthis sampler layer, to yield multiple unmasked tokens. Our sampler is trained\nto mimic exact joint sampling from the (frozen) full model. We show the\neffectiveness of our approximate joint sampling for both pretrained-only\n(Dream-7B-Base) and instruction-tuned (Dream-7B-Instruct) models on language\nmodeling and math \\& coding tasks. When four tokens are unmasked for each\nfull-model denoising step, our sampling algorithm achieves a MAUVE score of\n0.87 (vs marginal baseline of 0.31) with respect to the true joint\ndistribution.", "AI": {"tldr": "本文提出了一种在单次完整模型前向传播中近似地从联合分布中采样多个令牌的方法，通过开发一个新的轻量级单层采样器来实现。该方法提高了采样效率，同时在语言建模和数学编程任务上保持了高性能。", "motivation": "传统的自动回归语言模型通过条件生成每个令牌来采样字符串，而掩码扩散语言模型则通过非顺序及并行的方式解除屏蔽来生成文本。解除屏蔽令牌的并行数量越多，生成的字符串就离真正的联合分布越远。我们旨在提出一种方法，以近似的方式在单次完整的模型前向传播过程中从联合分布中采样多个令牌。", "method": "我们提出了一种新的轻量级单层“采样器”，它位于现有的大型扩散语言模型之上。通过这种方法，在一次完整的模型前向传播后，可以进行多次仅通过采样器层的前向传播，从而生成多个未屏蔽的令牌。采样器被训练来模仿从固定完整模型的联合分布中进行精确采样。", "result": "我们的逼近联合采样方法在仅预训练（Dream-7B-Base）和指令调优（Dream-7B-Instruct）模型的语言建模和数学及编程任务上表明了有效性。当为每个完整的模型去噪步骤解除屏蔽四个令牌时，我们的采样算法相对于真实联合分布的MAUVE得分为0.87，相较于边缘基准的0.31有了显著提升。", "conclusion": "研究表明，我们提出的方法可以在不牺牲性能的情况下，显著提高文本生成的速度。该方法成功地通过有限的完整模型前向传播来实现接近真实联合分布的采样，对于实际应用具有重要意义。"}}
{"id": "2509.22708", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.22708", "abs": "https://arxiv.org/abs/2509.22708", "authors": ["Ahed Alboody"], "title": "GZSL-MoE: Apprentissage G{é}n{é}ralis{é} Z{é}ro-Shot bas{é} sur le M{é}lange d'Experts pour la Segmentation S{é}mantique de Nuages de Points 3DAppliqu{é} {à} un Jeu de Donn{é}es d'Environnement de Collaboration Humain-Robot", "comment": "in French language. 28e Conf{\\'e}rence Nationale en Intelligence\n  Artificielle. Plate-Forme Intelligence Artificielle 2025, Association Fran{\\c\n  c}aise pour l'Intelligence Artificielle, https://pfia2025.u-bourgogne.fr/,\n  Jun 2025, Dijon, France", "summary": "Generative Zero-Shot Learning approach (GZSL) has demonstrated significant\npotential in 3D point cloud semantic segmentation tasks. GZSL leverages\ngenerative models like GANs or VAEs to synthesize realistic features (real\nfeatures) of unseen classes. This allows the model to label unseen classes\nduring testing, despite being trained only on seen classes. In this context, we\nintroduce the Generalized Zero-Shot Learning based-upon Mixture-of-Experts\n(GZSL-MoE) model. This model incorporates Mixture-of-Experts layers (MoE) to\ngenerate fake features that closely resemble real features extracted using a\npre-trained KPConv (Kernel Point Convolution) model on seen classes. The main\ncontribution of this paper is the integration of Mixture-of-Experts into the\nGenerator and Discriminator components of the Generative Zero-Shot Learning\nmodel for 3D point cloud semantic segmentation, applied to the COVERED dataset\n(CollabOratiVE Robot Environment Dataset) for Human-Robot Collaboration (HRC)\nenvironments. By combining the Generative Zero-Shot Learning model with\nMixture-of- Experts, GZSL-MoE for 3D point cloud semantic segmentation provides\na promising solution for understanding complex 3D environments, especially when\ncomprehensive training data for all object classes is unavailable. The\nperformance evaluation of the GZSL-MoE model highlights its ability to enhance\nperformance on both seen and unseen classes. Keywords Generalized Zero-Shot\nLearning (GZSL), 3D Point Cloud, 3D Semantic Segmentation, Human-Robot\nCollaboration, COVERED (CollabOratiVE Robot Environment Dataset), KPConv,\nMixture-of Experts", "AI": {"tldr": "本文介绍了一种结合Mixture-of-Experts的广义零样本学习模型（GZSL-MoE），应用于3D点云的语义分割任务，特别是在人机协作环境中，该模型通过合成未见类别的特征来改进传统的零样本学习。", "motivation": "广义零样本学习已在3D点云的语义分割中显示出重大潜力。然而，当全面的训练数据不可用时，要识别未见类别仍是一个挑战。本文旨在通过结合Mixture-of-Experts技术解决这一问题，从而提高了对复杂3D环境的理解能力。", "method": "本文提出了基于专家混合的广义零样本学习模型（GZSL-MoE），该模型结合了生成对抗网络（GAN）或变分自编码器（VAE）以及专家混合层（MoE），用于改进3D点云语义分割任务中的零样本学习能力。MoE层被融入到生成器和判别器中，目的是产生与使用预训练KPConv模型提取的真实特征相近的合成特征，以增强对未见类别的识别。", "result": "GZSL-MoE模型在语义分割任务中的性能评估显示，它不仅提升了已见类别的性能，对未见类别的识别能力也有所提升，这证明了模型在处理包含未见过的对象类别的3D环境中的有效性。", "conclusion": "本文提出的GZSL-MoE模型，通过结合Mixture-of-Experts技术改进了零样本学习模型的生成器和判别器，特别适用于处理在Human-Robot Collaboration环境中训练数据不足的问题，这对于理解复杂3D环境具有重要意义。"}}
{"id": "2509.22739", "categories": ["cs.CL", "cs.AI", "cs.LG", "stat.ML", "I.2.6; I.2.7"], "pdf": "https://arxiv.org/pdf/2509.22739", "abs": "https://arxiv.org/abs/2509.22739", "authors": ["Sasha Cui", "Zhongren Chen"], "title": "Painless Activation Steering: An Automated, Lightweight Approach for Post-Training Large Language Models", "comment": null, "summary": "Language models (LMs) are typically post-trained for desired capabilities and\nbehaviors via weight-based or prompt-based steering, but the former is\ntime-consuming and expensive, and the latter is not precisely controllable and\noften requires manual trial-and-error. While activation steering (AS) promises\na cheap, fast, and controllable alternative to the two existing post-training\nmethods, current AS techniques require hand-crafted prompt pairs or\nlabor-intensive feature annotation, making them more inconvenient than the\nplug-and-play methods such as Reinforcement Learning (RL) and Supervised\nFine-Tuning (SFT). We introduce Painless Activation Steering (PAS), a family of\nfully automated methods that make AS readily usable with any given labeled\ndataset, with no need for prompt construction, feature labeling, or human\nintervention. We evaluate PAS on three open-weight models\n(Llama3.1-8B-Instruct, DeepSeek-R1-Distill-8B, and Nous-Hermes-2) and 18 tasks;\nwe find that PAS reliably improves performance for behavior tasks, but not for\nintelligence-oriented tasks. The introspective variant (iPAS) delivers the\nstrongest causal steering effects (10.1% on Bias, 5.2% on Morality, and 34.8%\non Alignment). We also show PAS delivers additional gains on top of In-Context\nLearning (ICL) and SFT. PAS constructs a fast, lightweight activation vector\nthat can be cheaply trained, easily stored, and activated at will. Our results\nprovide a characterization of where AS helps, where it fails, and how to deploy\nit as a practical, automated LM post-training option.", "AI": {"tldr": "PAS, an automated method for steering activation (AS), improves performance for behavior tasks in LMs without the need for manual prompt creation or labeling.", "motivation": "The motivation is to provide a cheaper, faster, and more controllable alternative to existing post-training methods like weight-based training and prompt-based steering, which can be time-consuming and less precise.", "method": "Painless Activation Steering (PAS) is introduced as a fully automated method for activation steering (AS) that can be applied with any labeled dataset without the need for prompts or human labeling.", "result": "PAS was evaluated on multiple open-weight models and tasks, showing reliable performance improvements for behavior tasks but not for intelligence-oriented tasks. iPAS, the introspective variant, showed strong causal steering effects across specific metrics.", "conclusion": "The conclusion is that PAS, a practical and automated method for LM post-training, effectively improves behavior tasks and can be used in conjunction with other methods like In-Context Learning (ICL) and Supervised Fine-Tuning (SFT)."}}
{"id": "2509.22719", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.22719", "abs": "https://arxiv.org/abs/2509.22719", "authors": ["Adithya Giri"], "title": "IBiT: Utilizing Inductive Biases to Create a More Data Efficient Attention Mechanism", "comment": null, "summary": "In recent years, Transformer-based architectures have become the dominant\nmethod for Computer Vision applications. While Transformers are explainable and\nscale well with dataset size, they lack the inductive biases of Convolutional\nNeural Networks. While these biases may be learned on large datasets, we show\nthat introducing these inductive biases through learned masks allow Vision\nTransformers to learn on much smaller datasets without Knowledge Distillation.\nThese Transformers, which we call Inductively Biased Image Transformers (IBiT),\nare significantly more accurate on small datasets, while retaining the\nexplainability Transformers.", "AI": {"tldr": "文章提出了在Vision Transformers中引入归纳偏置的方法，该方法有助于模型在小数据集上实现更好的性能，同时保持模型的可解释性。", "motivation": "鉴于Transformers模型虽然具有很好的可解释性和随着数据集大小增加而扩展的能力，但缺乏卷积神经网络的归纳偏置特性。而这些偏置特性可以在大数据集上学习到，但在小数据集上则难以实现。", "method": "文章提出了在Vision Transformers中引入通过学习到的掩码来实现归纳偏置的方法。", "result": "实验结果显示，引入这种归纳偏置的Vision Transformers（称为IBiT）在小数据集上的准确性更高，并且保持了Transformers的可解释性。", "conclusion": "通过引入归纳偏置，Vision Transformers不仅在小数据集上表现更好，而且保留了其固有的可解释性优势。"}}
{"id": "2509.22750", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.22750", "abs": "https://arxiv.org/abs/2509.22750", "authors": ["Jeonghyun Park", "Ingeol Baek", "Seunghyun Yoon", "Haeun Jang", "Aparna Garimella", "Akriti Jain", "Nedim Lipka", "Hwanhee Lee"], "title": "MIRAGE: Multi-hop Reasoning with Ambiguity Evaluation for Illusory Questions", "comment": "18 figures, 11 tables", "summary": "Real-world Multi-hop Question Answering (QA) often involves ambiguity that is\ninseparable from the reasoning process itself. This ambiguity creates a\ndistinct challenge, where multiple reasoning paths emerge from a single\nquestion, each requiring independent resolution. Since each sub-question is\nambiguous, the model must resolve ambiguity at every step. Thus, answering a\nsingle question requires handling multiple layers of ambiguity throughout the\nreasoning chain. We find that current Large Language Models (LLMs) struggle in\nthis setting, typically exploring wrong reasoning paths and producing\nincomplete answers. To facilitate research on multi-hop ambiguity, we introduce\nMultI-hop Reasoning with AmbiGuity Evaluation for Illusory Questions (MIRAGE),\na benchmark designed to analyze and evaluate this challenging intersection of\nambiguity interpretation and multi-hop reasoning. MIRAGE contains 1,142\nhigh-quality examples of ambiguous multi-hop questions, categorized under a\ntaxonomy of syntactic, general, and semantic ambiguity, and curated through a\nrigorous multi-LLM verification pipeline. Our experiments reveal that even\nstate-of-the-art models struggle on MIRAGE, confirming that resolving ambiguity\ncombined with multi-step inference is a distinct and significant challenge. To\nestablish a robust baseline, we propose CLarifying Ambiguity with a Reasoning\nand InstructiON (CLARION), a multi-agent framework that significantly\noutperforms existing approaches on MIRAGE, paving the way for more adaptive and\nrobust reasoning systems.", "AI": {"tldr": "研究探讨了多跳问答中的歧义问题，并为研究提供了基准MIRAGE和适应歧义推理的CLARION框架，后者在评估中表现突出。", "motivation": "当前的大型语言模型在处理多跳推理中的歧义问题时遇到挑战，容易走错推理路径并产生不完整答案。为促进该难题的研究，作者创建了一个评估基准MIRAGE。", "method": "提出了一种称为CLARION的多代理框架来解决多跳推理中的歧义问题，显著优于现有方法。CLARION框架用于处理复杂的推理链和歧义子问题，以获得更完整和精确的答案。", "result": "实验结果表明，即使是最先进的模型在MIRAGE上也表现不佳，CLARION框架在MIRAGE上显著优于现有方法。这证实了处理歧义与多步推断相结合的挑战性。", "conclusion": "通过引入MIRAGE评估基准和CLARION框架，研究揭示了解决多跳推理中歧义问题的重要性，为构建更加适应和健壮的推理系统铺平了道路。"}}
{"id": "2509.22720", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.22720", "abs": "https://arxiv.org/abs/2509.22720", "authors": ["Zezhong Fan", "Xiaohan Li", "Luyi Ma", "Kai Zhao", "Liang Peng", "Topojoy Biswas", "Evren Korpeoglu", "Kaushiki Nag", "Kannan Achan"], "title": "LayoutAgent: A Vision-Language Agent Guided Compositional Diffusion for Spatial Layout Planning", "comment": "NeurIPS 2025 Workshop on SPACE in Vision, Language, and Embodied AI", "summary": "Designing realistic multi-object scenes requires not only generating images,\nbut also planning spatial layouts that respect semantic relations and physical\nplausibility. On one hand, while recent advances in diffusion models have\nenabled high-quality image generation, they lack explicit spatial reasoning,\nleading to unrealistic object layouts. On the other hand, traditional spatial\nplanning methods in robotics emphasize geometric and relational consistency,\nbut they struggle to capture semantic richness in visual scenes. To bridge this\ngap, in this paper, we propose LayoutAgent, an agentic framework that unifies\nvision-language reasoning with compositional diffusion for layout generation.\nGiven multiple input images with target objects in them, our method first\nemploys visual-language model to preprocess the inputs through segmentation,\nobject size estimation, scene graph construction, and prompt rewriting. Then we\nleverage compositional diffusion-a method traditionally used in robotics-to\nsynthesize bounding boxes that respect object relations encoded in the scene\ngraph for spatial layouts. In the end, a foreground-conditioned image generator\ncomposes the complete scene by rendering the objects into the planned layout\nguided by designed prompts. Experiments demonstrate that LayoutAgent\noutperforms other state-of-the-art layout generation models in layout\ncoherence, spatial realism and aesthetic alignment.", "AI": {"tldr": "LayoutAgent 提出了一种统一视觉-语言推理与组合扩散的方法来生成逼真的多对象场景布局。", "motivation": "促使设计 LayoutAgent 的动机是当前图像生成方法在生成逼真的多对象场景方面存在局限性，特别是在空间布局方面，缺乏视觉上的丰富性和语义一致性。", "method": "LayoutAgent 方法首先使用视觉-语言模型对输入图像进行预处理，然后采用组合扩散合成尊重场景图对象关系的包围盒，最后使用前景条件图像生成器组成完整的场景。", "result": "{\n  \"tldr\": \"LayoutAgent is proposed to unify vision-language reasoning with compositional diffusion for realistic multi-object scene layout generation, exhibiting superior performance in layout coherence, spatial realism, and aesthetic alignment.\", \n  \"motivation\": \"The motivation is to address the limitations of current approaches in generating realistic multi-object scenes where spatial layouts are either semantically or visually lacking, aiming to develop a method that can bridge the gap between image generation and spatial planning capabilities.\", \n  \"method\": \"LayoutAgent uses a visual-language model for preprocessing multiple input images, followed by compositional diffusion for synthesize bounding boxes respecting object relations, and an image generator that composes the complete scene using designed prompts.\", \n  \"result\": \"Experiments show that LayoutAgent outperforms other state-of-the-art layout generation models in terms of layout coherence, spatial realism, and aesthetic alignment in generated scenes.\", \n  \"conclusion\": \"The conclusion is that by integrating vision-language processing with compositional diffusion, LayoutAgent effectively addresses the challenges of generating realistic multi-object scene layouts, offering a superior method compared to existing models in layout generation.\", \n  \"name\": \"Structure\", \n  \"arguments\": {\n    \"result\": \"Experiments demonstrate that LayoutAgent outperforms other state-of-the-art layout generation models in layout coherence, spatial realism and aesthetic alignment.\", \n    \"motivation\": \"While recent advances in diffusion models enable high-quality image generation, they lack explicit spatial reasoning. Traditional spatial planning methods struggle to capture semantic richness in visual scenes, leading to a need to bridge this gap.\", \n    \"method\": \"LayoutAgent employs visual-language model to preprocess the inputs and uses compositional diffusion to synthesize bounding boxes respecting object relations, guided by designed prompts for image generation.\", \n    \"tldr\": \"LayoutAgent proposes a method unifying vision-language reasoning with compositional diffusion for generating realistic multi-object scene layouts.\", \n    \"conclusion\": \"LayoutAgent effectively bridges the gap between high-quality image generation and spatial-planning by integrating vision-language processing with compositional diffusion, showing superior performance in layout coherence, spatial realism, and aesthetic alignment.\", \n    \"result\": \"Experiments demonstrate that LayoutAgent outperforms other state-of-the-art layout generation models in layout coherence, spatial realism and aesthetic alignment. \"\n  }\n}", "conclusion": "通过将视觉-语言处理与组合扩散相结合，LayoutAgent 有效地解决了生成逼真的多对象场景布局的挑战，并在场景布局一致性、空间真实性和审美一致性方面表现优于现有模型。"}}
{"id": "2509.22768", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.22768", "abs": "https://arxiv.org/abs/2509.22768", "authors": ["Ekaterina Trofimova", "Zosia Shamina", "Maria Selifanova", "Artem Zaitsev", "Remi Savchuk", "Maxim Minets", "Daria Ozerova", "Emil Sataev", "Denis Zuenko", "Andrey E. Ustyuzhanin"], "title": "ML2B: Multi-Lingual ML Benchmark For AutoML", "comment": null, "summary": "Large language models (LLMs) have recently demonstrated strong capabilities\nin generating machine learning (ML) code, enabling end-to-end pipeline\nconstruction from natural language instructions. However, existing benchmarks\nfor ML code generation are mainly restricted to English, overlooking the global\nand multilingual nature of ML research and practice. To address this gap, we\npresent ML2B, the first benchmark for evaluating multilingual ML code\ngeneration. ML2B consists of 30 Kaggle competitions translated into 13 natural\nlanguages, covering tabular, text, and image data types, with structured\nmetadata and validated human-reviewed translations. For evaluation, we employ\nAIDE, an automated framework for end-to-end assessment of data science\npipelines, and provide insights into cross-lingual model performance. Our\nresults reveal substantial 15-45% performance degradation on non-English tasks,\nhighlighting critical challenges in multilingual representation learning for\ncode generation. The benchmark, evaluation framework, and comprehensive results\nare made available through our GitHub repository to facilitate future research\nin multilingual ML code generation: https://github.com/enaix/ml2b.", "AI": {"tldr": "本文介绍了一个新的多语言机器学习代码生成基准测试ML2B，它包含了13种语言的Kaggle竞赛，评估显示非英语任务的性能下降，指出了多语言代码生成中的挑战。", "motivation": "现有的机器学习代码生成基准测试主要局限于英语，忽视了全球和多语言的机器学习研究和实践特性。为了填补这一空白，提出了ML2B基准测试。", "method": "ML2B基准测试是第一个用于评估多语言机器学习代码生成的基准测试，包含了13种自然语言翻译的30个Kaggle竞赛，涵盖了表格、文本和图像数据类型，提供了结构化的元数据和经过验证的人工翻译。用于评估的是一个名为AIDE的自动化框架，用于评估端到端的数据科学管道。", "result": "实验结果显示在非英语任务上表现下降了15-45%，凸显了多语言表示学习代码生成中的关键挑战。", "conclusion": "这项研究表明，现有的模型在处理多语言机器学习代码生成任务时存在显著的性能下降问题。这为未来的研究指出了方向，特别是在多语言表示学习方面。"}}
{"id": "2509.22737", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.22737", "abs": "https://arxiv.org/abs/2509.22737", "authors": ["Jie Cai", "Kangning Yang", "Lan Fu", "Jiaming Ding", "Jinlong Li", "Huiming Sun", "Daitao Xing", "Jinglin Shen", "Zibo Meng"], "title": "CompareBench: A Benchmark for Visual Comparison Reasoning in Vision-Language Models", "comment": null, "summary": "We introduce CompareBench, a benchmark for evaluating visual comparison\nreasoning in vision-language models (VLMs), a fundamental yet understudied\nskill. CompareBench consists of 1000 QA pairs across four tasks: quantity\n(600), temporal (100), geometric (200), and spatial (100). It is derived from\ntwo auxiliary datasets that we constructed: TallyBench (2000 counting images\nwith QA) and HistCaps (515 historical images with bilingual captions). We\nevaluate both closed-source APIs (OpenAI, Gemini, Claude) and open-source\nmodels (Qwen2.5-VL and Qwen3-VL series). Results show clear scaling trends but\nalso reveal critical limitations: even the strongest models consistently fail\nat temporal ordering and spatial relations, and they often make mistakes in\nbasic counting and geometric comparisons that are trivial for humans. These\nfindings demonstrate that visual comparison remains a systematic blind spot for\ncurrent VLMs. By providing controlled, diverse, and diagnostic evaluation,\nCompareBench establishes a foundation for advancing more reliable multimodal\nreasoning.", "AI": {"tldr": "研究团队开发了CompareBench基准测试，用于评估视觉语言模型的视觉比较推理能力，发现现有模型在基本任务上也存在局限性，为未来的改进提供了方向。", "motivation": "目的是评估视觉语言模型的一个基本但研究较少的技能——视觉比较推理，并借此发现现有模型的局限性。", "method": "该研究提出了一个名为CompareBench的基准测试，用于评估视觉语言模型的视觉比较推理能力。该基准测试包含1000个问题答案对，涵盖四个任务：数量(600)，时间(100)，几何(200)和空间(100)。", "result": "结果显示了一些明显的规模趋势，但也揭示了一些关键的局限性：即使是表现最强的模型，在时间顺序和空间关系上的表现也不理想，甚至在基本的计数和几何比较上也会出现人类看来很容易的错误。", "conclusion": "研究表明视觉比较仍然是当前视觉语言模型的一个系统性盲点。通过提供控制良好、多样化和诊断性的评估，CompareBench为增进更加可靠的多模态推理提供了基础。"}}
{"id": "2509.22808", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.22808", "abs": "https://arxiv.org/abs/2509.22808", "authors": ["Mohamed Maged", "Alhassan Ehab", "Ali Mekky", "Besher Hassan", "Shady Shehata"], "title": "ArFake: A Multi-Dialect Benchmark and Baselines for Arabic Spoof-Speech Detection", "comment": null, "summary": "With the rise of generative text-to-speech models, distinguishing between\nreal and synthetic speech has become challenging, especially for Arabic that\nhave received limited research attention. Most spoof detection efforts have\nfocused on English, leaving a significant gap for Arabic and its many dialects.\nIn this work, we introduce the first multi-dialect Arabic spoofed speech\ndataset. To evaluate the difficulty of the synthesized audio from each model\nand determine which produces the most challenging samples, we aimed to guide\nthe construction of our final dataset either by merging audios from multiple\nmodels or by selecting the best-performing model, we conducted an evaluation\npipeline that included training classifiers using two approaches: modern\nembedding-based methods combined with classifier heads; classical machine\nlearning algorithms applied to MFCC features; and the RawNet2 architecture. The\npipeline further incorporated the calculation of Mean Opinion Score based on\nhuman ratings, as well as processing both original and synthesized datasets\nthrough an Automatic Speech Recognition model to measure the Word Error Rate.\nOur results demonstrate that FishSpeech outperforms other TTS models in Arabic\nvoice cloning on the Casablanca corpus, producing more realistic and\nchallenging synthetic speech samples. However, relying on a single TTS for\ndataset creation may limit generalizability.", "AI": {"tldr": "本文介绍了首个多方言阿拉伯语语音欺骗数据集，并通过训练分类器和使用RawNet2等方法评估了不同模型生成的合成语音难度。研究发现，在卡萨布兰卡语料库中，FishSpeech比其他TTS模型在阿拉伯语语音克隆上表现更佳，合成的语音更具真实性和挑战性。但仅依赖单一TTS模型创建数据集可能限制其普遍适用性。", "motivation": "鉴于生成式文本到语音模型的发展，甄别真实和合成语音变得富有挑战性，尤其是对于研究较少的阿拉伯语音。以往语音欺骗检测的努力主要集中在英语上，而对于阿拉伯语及其众多方言则关注不足，因此，本文旨在填补这一研究空白。", "method": "文中介绍了使用两种方法来评估不同模型生成的阿拉伯语语音合成样本的难度：一种是基于现代嵌入方法结合分类器的方法，另一种是基于MFCC特征的传统机器学习算法，此外还使用了RawNet2架构。评估过程中还加入了基于人工评分计算的平均意见得分，以及通过自动语音识别模型处理原始和合成数据集来测量词错误率。", "result": "研究结果表明，在卡萨布兰卡语料库中，FishSpeech相较于其他TTS模型在阿拉伯语语音克隆方面表现更优，生成的合成语音更为逼真且更具挑战性。", "conclusion": "研究表明FishSpeech在生成逼真且具挑战性的阿拉伯语语音样本方面表现最佳，然而，过于依赖某一TTS模型可能会影响数据集的普适性。"}}
{"id": "2509.22761", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.22761", "abs": "https://arxiv.org/abs/2509.22761", "authors": ["Yapeng Mi", "Hengli Li", "Yanpeng Zhao", "Chenxi Li", "Huimin Wu", "Xiaojian Ma", "Song-Chun Zhu", "Ying Nian Wu", "Qing Li"], "title": "MILR: Improving Multimodal Image Generation via Test-Time Latent Reasoning", "comment": "21 pages,13 figures,7 tables", "summary": "Reasoning-augmented machine learning systems have shown improved performance\nin various domains, including image generation. However, existing\nreasoning-based methods for image generation either restrict reasoning to a\nsingle modality (image or text) or rely on high-quality reasoning data for\nfine-tuning. To tackle these limitations, we propose MILR, a test-time method\nthat jointly reasons over image and text in a unified latent vector space.\nReasoning in MILR is performed by searching through vector representations of\ndiscrete image and text tokens. Practically, this is implemented via the policy\ngradient method, guided by an image quality critic. We instantiate MILR within\nthe unified multimodal understanding and generation (MUG) framework that\nnatively supports language reasoning before image synthesis and thus\nfacilitates cross-modal reasoning. The intermediate model outputs, which are to\nbe optimized, serve as the unified latent space, enabling MILR to operate\nentirely at test time. We evaluate MILR on GenEval, T2I-CompBench, and WISE,\nachieving state-of-the-art results on all benchmarks. Notably, on\nknowledge-intensive WISE, MILR attains an overall score of 0.63, improving over\nthe baseline by 80%. Our further analysis indicates that joint reasoning in the\nunified latent space is the key to its strong performance. Moreover, our\nqualitative studies reveal MILR's non-trivial ability in temporal and cultural\nreasoning, highlighting the efficacy of our reasoning method.", "AI": {"tldr": "MILR, a reasoning-augmented method enabling joint reasoning over image and text in a latent vector space, outperforms baselines by 80% on the knowledge-intensive WISE benchmark, demonstrating its efficacy in cross-modal reasoning.", "motivation": "To overcome the limitations of existing reasoning-based methods in image generation, which either limit reasoning to a single modality or require high-quality reasoning data for fine-tuning.", "method": "MILR is a test-time method that performs reasoning over both image and text within a unified latent vector space, utilizing vector representations of discrete image and text tokens guided by an image quality critic implemented through the policy gradient method.", "result": "MILR achieves state-of-the-art results on GenEval, T2I-CompBench, and WISE, notably improving over the baseline by 80% on WISE.", "conclusion": "Joint reasoning in the unified latent space is a critical factor in MILR's strong performance, and it also shows capability in temporal and cultural reasoning."}}
{"id": "2509.22812", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.22812", "abs": "https://arxiv.org/abs/2509.22812", "authors": ["Kai Zhang", "Christopher Malon", "Lichao Sun", "Martin Renqiang Min"], "title": "EditGRPO: Reinforcement Learning with Post -Rollout Edits for Clinically Accurate Chest X-Ray Report Generation", "comment": null, "summary": "Radiology report generation requires advanced medical image analysis,\neffective temporal reasoning, and accurate text generation. Although recent\ninnovations, particularly multimodal large language models (MLLMs), have shown\nimproved performance, their supervised fine-tuning (SFT) objective is not\nexplicitly aligned with clinical efficacy. In this work, we introduce EditGRPO,\na mixed-policy reinforcement learning (RL) algorithm designed specifically to\noptimize the generation through clinically motivated rewards. EditGRPO\nintegrates on-policy exploration with off-policy guidance by injecting\nsentence-level detailed corrections during training rollouts. This mixed-policy\napproach addresses the exploration dilemma and sampling efficiency issues\ntypically encountered in RL. Applied to a Qwen2.5-VL-3B MLLM initialized with\nsupervised fine-tuning (SFT), EditGRPO outperforms both SFT and vanilla GRPO\nbaselines, achieving an average improvement of 3.4% in CheXbert, GREEN,\nRadgraph, and RATEScore metrics across four major chest X-ray report generation\ndatasets. Notably, EditGRPO also demonstrates superior out-of-domain\ngeneralization, with an average performance gain of 5.9% on unseen datasets.", "AI": {"tldr": "研究介绍了一种新的混合策略强化学习算法——EditGRPO，其通过详细纠正信息在训练过程中结合了on-policy探索和off-policy指导，从而提高了胸部X光报告生成的质量和泛化性。", "motivation": "尽管近期的创新，特别是多模态大型语言模型（MLLMs），已经展示了改进的性能，但它们的监督微调（SFT）目标没有明确与临床效能对齐。因此，有必要引入一种新的方法来优化医学影像分析、时间推理和文本生成这些领域中的临床报告生成任务。", "method": "本文介绍了一种名为EditGRPO的混合策略强化学习算法，该算法专门针对通过临床动机奖励来优化生成过程。EditGRPO通过在训练周期中注入句子级的详细纠正信息，集成了on-policy探索与off-policy指导。这种混合策略方法解决了强化学习中通常遇到的探索困境和采样效率问题。", "result": "与SFT和vanilla GRPO基线方法相比，应用于经过监督微调（SFT）初始化的Qwen2.5-VL-3B MLLM的EditGRPO，在CheXbert、GREEN、Radgraph和RATEScore等四个主要胸部X光报告生成数据集上实现了平均3.4%的改进。此外，对于未见过的数据集，EditGRPO还表现出更强的泛化能力，平均性能提升了5.9%。", "conclusion": "本文所提出的EditGRPO方法显著改善了放射学报告生成任务的训练效果和泛化性能，这对提高临床实践中的报告质量和可靠性具有重要意义。"}}
{"id": "2509.22763", "categories": ["cs.CV", "cs.AI", "cs.LG", "68T07, 68U10", "I.4.9; I.4.6"], "pdf": "https://arxiv.org/pdf/2509.22763", "abs": "https://arxiv.org/abs/2509.22763", "authors": ["Tangqi Shi", "Pietro Lio"], "title": "UESA-Net: U-Shaped Embedded Multidirectional Shrinkage Attention Network for Ultrasound Nodule Segmentation", "comment": "22 pages,2 figures,4 tables", "summary": "Background: Breast and thyroid cancers pose an increasing public-health\nburden. Ultrasound imaging is a cost-effective, real-time modality for lesion\ndetection and segmentation, yet suffers from speckle noise, overlapping\nstructures, and weak global-local feature interactions. Existing networks\nstruggle to reconcile high-level semantics with low-level spatial details. We\naim to develop a segmentation framework that bridges the semantic gap between\nglobal context and local detail in noisy ultrasound images.\n  Methods: We propose UESA-Net, a U-shaped network with multidirectional\nshrinkage attention. The encoder-decoder architecture captures long-range\ndependencies and fine-grained structures of lesions. Within each encoding\nblock, attention modules operate along horizontal, vertical, and depth\ndirections to exploit spatial details, while a shrinkage (threshold) strategy\nintegrates prior knowledge and local features. The decoder mirrors the encoder\nbut applies a pairwise shrinkage mechanism, combining prior low-level physical\ncues with corresponding encoder features to enhance context modeling.\n  Results: On two public datasets - TN3K (3493 images) and BUSI (780 images) -\nUESA-Net achieved state-of-the-art performance with intersection-over-union\n(IoU) scores of 0.8487 and 0.6495, respectively.\n  Conclusions: UESA-Net effectively aggregates multidirectional spatial\ninformation and prior knowledge to improve robustness and accuracy in breast\nand thyroid ultrasound segmentation, demonstrating superior performance to\nexisting methods on multiple benchmarks.", "AI": {"tldr": "UESA-Net, a U-shaped network with multidirectional shrinkage attention, is proposed for better ultrasound image segmentation of breast and thyroid cancers, achieving state-of-the-art performance.", "motivation": "To develop a segmentation framework that improves the detection and segmentation of lesions in noisy ultrasound images of breast and thyroid cancers, addressing the semantic gap between global context and local detail.", "method": "UESA-Net utilizes an encoder-decoder structure with attention modules that operate in horizontal, vertical, and depth directions. It incorporates a shrinkage strategy to enhance the integration of prior knowledge and local features for better context modeling.", "result": "UESA-Net demonstrated superior performance on TN3K and BUSI datasets with IoU scores of 0.8487 and 0.6495, respectively.", "conclusion": "UESA-Net effectively enhances robustness and accuracy in ultrasound segmentation by integrating multidirectional spatial information and prior knowledge, outperforming existing methods."}}
{"id": "2509.22824", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.22824", "abs": "https://arxiv.org/abs/2509.22824", "authors": ["Chi Ruan", "Dongfu Jiang", "Yubo Wang", "Wenhu Chen"], "title": "Critique-Coder: Enhancing Coder Models by Critique Reinforcement Learning", "comment": null, "summary": "Reinforcement Learning (RL) has emerged as a popular training paradigm,\nparticularly when paired with reasoning models. While effective, it primarily\nfocuses on generating responses and lacks mechanisms to explicitly foster\ncritique or reflection. Several recent studies, like Critique-Fine-Tuning (CFT)\nand Critique-Guided-Distillation (CGD) have shown the benefits of explicitly\nteaching LLMs how to critique. Motivated by them, we propose Critique\nReinforcement Learning (CRL), where the model is tasked with generating a\ncritique for a given (question, solution) pair. The reward is determined solely\nby whether the final judgment label $c \\in \\{\\texttt{True}, \\texttt{False}\\}$\nof the generated critique aligns with the ground-truth judgment $c^*$. Building\non this point, we introduce \\textsc{Critique-Coder}, which is trained on a\nhybrid of RL and CRL by substituting 20\\% of the standard RL data with CRL\ndata. We fine-tune multiple models (\\textsc{Critique-Coder}) and evaluate them\non different benchmarks to show their advantages over RL-only models. We show\nthat \\textsc{Critique-Coder} consistently outperforms RL-only baselines on all\nthe evaluated benchmarks. Notably, our \\textsc{Critique-Coder-8B} can reach\nover 60\\% on LiveCodeBench (v5), outperforming other reasoning models like\nDeepCoder-14B and GPT-o1. Beyond code generation, \\textsc{Critique-Coder} also\ndemonstrates enhanced general reasoning abilities, as evidenced by its better\nperformance on logic reasoning tasks from the BBEH dataset. This indicates that\nthe application of CRL on coding datasets enhances general reasoning and\ncritique abilities, which are transferable across a broad range of tasks.\nHence, we believe that CRL works as a great complement to standard RL for LLM\nreasoning.", "AI": {"tldr": "本文提出了一种新的强化学习方法——CRL，通过训练模型对给定的问题和解决方案对进行批判，来增强模型的批判和反思能力。通过训练模型，并在多个基准测试中评估它们，实验结果表明CRL训练的模型在各种任务上都优于标准RL训练的模型。", "motivation": "强化学习虽然在训练模型生成响应方面表现良好，但它缺乏明确鼓励批判或反思的机制。基于此动机，作者提出了CRL，以增强模型的批判能力。", "method": "本文提出了CRL，用于任务需求生成批判，CRL的奖励完全由生成批判的最终判断标签与真值的匹配度决定。同时，本文引入了一个新的模型Critique-Coder，通过混合使用RL和CRL训练数据来训练Critique-Coder。", "result": "多个基准测试显示，Critique-Coder模型的性能优于标准RL训练的模型，特别是在LiveCodeBench (v5)上表现最好，超过了其他诸如DeepCoder-14B和GPT-o1等推理模型。", "conclusion": "CRL通过改进标准的RL方法，能有效提升模型在批判和推理方面的能力，这种改进不仅限于代码生成任务，还能应用于广泛的其他任务上，因而适合作为标准RL的一种补充。"}}
{"id": "2509.22769", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.22769", "abs": "https://arxiv.org/abs/2509.22769", "authors": ["Fernando Julio Cendra", "Kai Han"], "title": "PartCo: Part-Level Correspondence Priors Enhance Category Discovery", "comment": "Project page: https://visual-ai.github.io/partco", "summary": "Generalized Category Discovery (GCD) aims to identify both known and novel\ncategories within unlabeled data by leveraging a set of labeled examples from\nknown categories. Existing GCD methods primarily depend on semantic labels and\nglobal image representations, often overlooking the detailed part-level cues\nthat are crucial for distinguishing closely related categories. In this paper,\nwe introduce PartCo, short for Part-Level Correspondence Prior, a novel\nframework that enhances category discovery by incorporating part-level visual\nfeature correspondences. By leveraging part-level relationships, PartCo\ncaptures finer-grained semantic structures, enabling a more nuanced\nunderstanding of category relationships. Importantly, PartCo seamlessly\nintegrates with existing GCD methods without requiring significant\nmodifications. Our extensive experiments on multiple benchmark datasets\ndemonstrate that PartCo significantly improves the performance of current GCD\napproaches, achieving state-of-the-art results by bridging the gap between\nsemantic labels and part-level visual compositions, thereby setting new\nbenchmarks for GCD. Project page: https://visual-ai.github.io/partco", "AI": {"tldr": "PartCo 是一种新的框架，通过引入零件级视觉特征对应关系来改进类别发现，从而弥补语义标签和零件级视觉组成之间的差距，并在多个基准数据集上达到了最先进的结果。", "motivation": "现有的一般化类别发现（GCD）方法主要依赖语义标签和全局图像表示，忽视了对于区分紧密相关的类别至关重要的局部细节线索。", "method": "PartCo 引入了零件级视觉特征对应关系，捕捉更精细的语义结构，增强了现有GCD方法下的类别发现能力，并且能够无缝地集成到现有的GCD方法中。", "result": "在多个基准数据集上的大量实验表明，PartCo 显著提升了当前GCD方法的性能，达到了最先进的水平。", "conclusion": "PartCo 通过引入零件级视觉特征对应关系，弥补了现有GCD方法中语义标签和部分级视觉组成的差距，成功提升了类别发现的性能，并为GCD设立了新标杆。"}}
{"id": "2509.22830", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.22830", "abs": "https://arxiv.org/abs/2509.22830", "authors": ["Hwan Chang", "Yonghyun Jun", "Hwanhee Lee"], "title": "ChatInject: Abusing Chat Templates for Prompt Injection in LLM Agents", "comment": null, "summary": "The growing deployment of large language model (LLM) based agents that\ninteract with external environments has created new attack surfaces for\nadversarial manipulation. One major threat is indirect prompt injection, where\nattackers embed malicious instructions in external environment output, causing\nagents to interpret and execute them as if they were legitimate prompts. While\nprevious research has focused primarily on plain-text injection attacks, we\nfind a significant yet underexplored vulnerability: LLMs' dependence on\nstructured chat templates and their susceptibility to contextual manipulation\nthrough persuasive multi-turn dialogues. To this end, we introduce ChatInject,\nan attack that formats malicious payloads to mimic native chat templates,\nthereby exploiting the model's inherent instruction-following tendencies.\nBuilding on this foundation, we develop a persuasion-driven Multi-turn variant\nthat primes the agent across conversational turns to accept and execute\notherwise suspicious actions. Through comprehensive experiments across frontier\nLLMs, we demonstrate three critical findings: (1) ChatInject achieves\nsignificantly higher average attack success rates than traditional prompt\ninjection methods, improving from 5.18% to 32.05% on AgentDojo and from 15.13%\nto 45.90% on InjecAgent, with multi-turn dialogues showing particularly strong\nperformance at average 52.33% success rate on InjecAgent, (2)\nchat-template-based payloads demonstrate strong transferability across models\nand remain effective even against closed-source LLMs, despite their unknown\ntemplate structures, and (3) existing prompt-based defenses are largely\nineffective against this attack approach, especially against Multi-turn\nvariants. These findings highlight vulnerabilities in current agent systems.", "AI": {"tldr": "提出了一种名为ChatInject的攻击方法，显示了大语言模型在某些类型的攻击中的脆弱性，强调了对于现有防御策略的不足之处。", "motivation": "旨在探讨大语言模型对于依赖结构化聊天模板和易受说服性多轮对话情境操纵的依赖性这一尚未充分研究的漏洞。", "method": "引入了一种名为ChatInject的攻击方法，该方法将恶意载荷格式化为模仿原生聊天模板，以此利用模型的指令跟随倾向。发展了一种基于说服的多轮变体，通过对话轮次使代理接受并执行通常有疑点的动作。", "result": "实验表明，ChatInject攻击成功率显著高于传统指令注入方法。多轮对话尤其表现出色，平均成功率高达52.33%。基于聊天模板的载荷显示了跨模型的强烈可转移性，即使是对封闭源LLM也有效。现有基于提示的防御措施对此攻击方法基本上无效，尤其是对抗多轮变体。", "conclusion": "研究结果突显了当前代理系统中的漏洞。"}}
{"id": "2509.22793", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.22793", "abs": "https://arxiv.org/abs/2509.22793", "authors": ["Komal Kumar", "Rao Muhammad Anwer", "Fahad Shahbaz Khan", "Salman Khan", "Ivan Laptev", "Hisham Cholakkal"], "title": "DEFT: Decompositional Efficient Fine-Tuning for Text-to-Image Models", "comment": "13 Figures, 21 pages, accepted in NeurIPS 2025", "summary": "Efficient fine-tuning of pre-trained Text-to-Image (T2I) models involves\nadjusting the model to suit a particular task or dataset while minimizing\ncomputational resources and limiting the number of trainable parameters.\nHowever, it often faces challenges in striking a trade-off between aligning\nwith the target distribution: learning a novel concept from a limited image for\npersonalization and retaining the instruction ability needed for unifying\nmultiple tasks, all while maintaining editability (aligning with a variety of\nprompts or in-context generation). In this work, we introduce DEFT,\nDecompositional Efficient Fine-Tuning, an efficient fine-tuning framework that\nadapts a pre-trained weight matrix by decomposing its update into two\ncomponents with two trainable matrices: (1) a projection onto the complement of\na low-rank subspace spanned by a low-rank matrix, and (2) a low-rank update.\nThe single trainable low-rank matrix defines the subspace, while the other\ntrainable low-rank matrix enables flexible parameter adaptation within that\nsubspace. We conducted extensive experiments on the Dreambooth and Dreambench\nPlus datasets for personalization, the InsDet dataset for object and scene\nadaptation, and the VisualCloze dataset for a universal image generation\nframework through visual in-context learning with both Stable Diffusion and a\nunified model. Our results demonstrated state-of-the-art performance,\nhighlighting the emergent properties of efficient fine-tuning. Our code is\navailable on \\href{https://github.com/MAXNORM8650/DEFT}{DEFTBase}.", "AI": {"tldr": "DEFT框架通过在权重更新中应用低秩分解技术，实现高效微调，同时保持多种任务中的可编辑性，在多个数据集与模型上取得了领先性能。", "motivation": "解决文本到图像模型微调中的问题，包括在适应特定任务或数据集时，利用最少的计算资源和限制可训练参数数量，同时保持不同上下文生成的理解能力。", "method": "DEFT, 分解式高效微调框架通过将预训练权重矩阵的更新分解为两个部分，其中一个部分是向低秩矩阵张成的低秩子空间的补空间的投影，另一个部分是低秩更新。单个可训练的低秩矩阵定义了子空间，而另一个可训练的低秩矩阵允许在该子空间内灵活地调整参数。", "result": "在个性化方面使用了Dreambooth和Dreambench Plus，物体和场景适应方面使用了InsDet数据集，以及通过视觉上下文学习实现的通用图像生成框架VisualCloze数据集，并结合了Stable Diffusion和统一模型，展现了最先进的性能，凸显了有效微调的潜力。", "conclusion": "实验结果表明DEFT能够高效地进行微调，并突出展现了高效微调框架的潜力和性能优势。"}}
{"id": "2509.22845", "categories": ["cs.CL", "cs.IR", "cs.LG", "H.3.3; I.2.7; I.2.6"], "pdf": "https://arxiv.org/pdf/2509.22845", "abs": "https://arxiv.org/abs/2509.22845", "authors": ["Kai Hua", "Zhiyuan Feng", "Chongyang Tao", "Rui Yan", "Lu Zhang"], "title": "Learning to Detect Relevant Contexts and Knowledge for Response Selection in Retrieval-based Dialogue Systems", "comment": "10 pages, 4 figures, accepted by CIKM 2020", "summary": "Recently, knowledge-grounded conversations in the open domain gain great\nattention from researchers. Existing works on retrieval-based dialogue systems\nhave paid tremendous efforts to utilize neural networks to build a matching\nmodel, where all of the context and knowledge contents are used to match the\nresponse candidate with various representation methods. Actually, different\nparts of the context and knowledge are differentially important for recognizing\nthe proper response candidate, as many utterances are useless due to the topic\nshift. Those excessive useless information in the context and knowledge can\ninfluence the matching process and leads to inferior performance. To address\nthis problem, we propose a multi-turn \\textbf{R}esponse \\textbf{S}election\n\\textbf{M}odel that can \\textbf{D}etect the relevant parts of the\n\\textbf{C}ontext and \\textbf{K}nowledge collection (\\textbf{RSM-DCK}). Our\nmodel first uses the recent context as a query to pre-select relevant parts of\nthe context and knowledge collection at the word-level and utterance-level\nsemantics. Further, the response candidate interacts with the selected context\nand knowledge collection respectively. In the end, The fused representation of\nthe context and response candidate is utilized to post-select the relevant\nparts of the knowledge collection more confidently for matching. We test our\nproposed model on two benchmark datasets. Evaluation results indicate that our\nmodel achieves better performance than the existing methods, and can\neffectively detect the relevant context and knowledge for response selection.", "AI": {"tldr": "本文提出了一个名为RSM-DCK的对话响应选择模型，通过层次结构的选择机制，可以有效提高对话模型在检索响应时的准确性和鲁棒性。", "motivation": "当前的检索式对话系统在匹配模型上投入了大量努力以利用神经网络，但过多无用信息会影响匹配过程并导致性能下降。为了处理这一问题，提出了能够提取上下文中重要部分的模型。", "method": "本文提出了一种名为RSM-DCK的多轮对话响应选择模型，该模型可以检测上下文和知识集合中相关部分。模型首先使用最近的上下文作为查询，以词级别和话语级别语义预选相关部分。接着，响应候选者与所选上下文和知识集合分别进行交互。最后，上下文和响应候选者的融合表示用于更自信地后选相关知识集合以进行匹配。", "result": "该模型在两个基准数据集上进行了测试，评估结果表明方法优于现有方法，并能够有效地识别响应选择中相关的上下文和知识。", "conclusion": "本模型能够增强对话系统在选取上下文和知识集合中更为相关部分的能力，最终提升了匹配效果与对话质量。"}}
{"id": "2509.22799", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.22799", "abs": "https://arxiv.org/abs/2509.22799", "authors": ["Xuan He", "Dongfu Jiang", "Ping Nie", "Minghao Liu", "Zhengxuan Jiang", "Mingyi Su", "Wentao Ma", "Junru Lin", "Chun Ye", "Yi Lu", "Keming Wu", "Benjamin Schneider", "Quy Duc Do", "Zhuofeng Li", "Yiming Jia", "Yuxuan Zhang", "Guo Cheng", "Haozhe Wang", "Wangchunshu Zhou", "Qunshu Lin", "Yuanxing Zhang", "Ge Zhang", "Wenhao Huang", "Wenhu Chen"], "title": "VideoScore2: Think before You Score in Generative Video Evaluation", "comment": null, "summary": "Recent advances in text-to-video generation have produced increasingly\nrealistic and diverse content, yet evaluating such videos remains a fundamental\nchallenge due to their multi-faceted nature encompassing visual quality,\nsemantic alignment, and physical consistency. Existing evaluators and reward\nmodels are limited to single opaque scores, lack interpretability, or provide\nonly coarse analysis, making them insufficient for capturing the comprehensive\nnature of video quality assessment. We present VideoScore2, a\nmulti-dimensional, interpretable, and human-aligned framework that explicitly\nevaluates visual quality, text-to-video alignment, and physical/common-sense\nconsistency while producing detailed chain-of-thought rationales. Our model is\ntrained on a large-scale dataset VideoFeedback2 containing 27,168\nhuman-annotated videos with both scores and reasoning traces across three\ndimensions, using a two-stage pipeline of supervised fine-tuning followed by\nreinforcement learning with Group Relative Policy Optimization (GRPO) to\nenhance analytical robustness. Extensive experiments demonstrate that\nVideoScore2 achieves superior performance with 44.35 (+5.94) accuracy on our\nin-domain benchmark VideoScore-Bench-v2 and 50.37 (+4.32) average performance\nacross four out-of-domain benchmarks (VideoGenReward-Bench, VideoPhy2, etc),\nwhile providing interpretable assessments that bridge the gap between\nevaluation and controllable generation through effective reward modeling for\nBest-of-N sampling. Project Page: https://tiger-ai-lab.github.io/VideoScore2/", "AI": {"tldr": "提出VideoScore2，一个多维、可解释且与人类评价一致的框架，用于评估视频的质量、文本与视频的对齐程度以及物理和常识一致性，并提供详细的推理过程。", "motivation": "现有的评估器和奖励模型要么只能提供单一的、不透明的评分，要么解释性较差，或者只能提供粗略的分析，无法全面捕捉视频质量评估的特性。", "method": "该框架通过监督微调和强化学习（使用Group Relative Policy Optimization (GRPO)）的两阶段训练流程，训练在大规模数据集VideoFeedback2上的模型。", "result": "VideoScore2在内部基准VideoScore-Bench-v2上取得了44.35（+5.94）的精度，并在四个外部基准上平均性能为50.37（+4.32），且提供了可解释的评估。", "conclusion": "实验结果表明，VideoScore2不仅性能优越，而且提供了解释性评估，有效连接了评估和可控生成通过有效的奖励建模进行Best-of-N采样。"}}
{"id": "2509.22854", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.22854", "abs": "https://arxiv.org/abs/2509.22854", "authors": ["Jiaqian Li", "Yanshu Li", "Ligong Han", "Ruixiang Tang", "Wenya Wang"], "title": "Towards Generalizable Implicit In-Context Learning with Attention Routing", "comment": null, "summary": "Implicit in-context learning (ICL) has newly emerged as a promising paradigm\nthat simulates ICL behaviors in the representation space of Large Language\nModels (LLMs), aiming to attain few-shot performance at zero-shot cost.\nHowever, existing approaches largely rely on injecting shift vectors into\nresidual flows, which are typically constructed from labeled demonstrations or\ntask-specific alignment. Such designs fall short of utilizing the structural\nmechanisms underlying ICL and suffer from limited generalizability. To address\nthis, we propose In-Context Routing (ICR), a novel implicit ICL method that\ninternalizes generalizable ICL patterns at the attention logits level. It\nextracts reusable structural directions that emerge during ICL and employs a\nlearnable input-conditioned router to modulate attention logits accordingly,\nenabling a train-once-and-reuse framework. We evaluate ICR on 12 real-world\ndatasets spanning diverse domains and multiple LLMs. The results show that ICR\nconsistently outperforms prior implicit ICL methods that require task-specific\nretrieval or training, while demonstrating robust generalization to\nout-of-domain tasks where existing methods struggle. These findings position\nICR to push the boundary of ICL's practical value.", "AI": {"tldr": "In-Context Routing (ICR) is proposed as a more generalizable implicit in-context learning method that outperforms existing approaches in evaluations and across out-of-domain tasks.", "motivation": "To improve generalizability over existing implicit in-context learning methods which largely rely on injecting shift vectors into residual flows, constructed from labeled demonstrations or task-specific alignment.", "method": "In-Context Routing (ICR), a novel implicit ICL method that internalizes generalizable ICL patterns at the attention logits level, extracting reusable structural directions and employing a learnable input-conditioned router to modulate attention logits.", "result": "ICR consistently outperforms prior implicit ICL methods that require task-specific retrieval or training on 12 real-world datasets across various domains and multiple LLMs, while showing robust generalization to out-of-domain tasks.", "conclusion": "ICR pushes the boundary of ICL's practical value by enabling a train-once-and-reuse framework and demonstrating better generalization and performance than existing methods."}}
{"id": "2509.22813", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2509.22813", "abs": "https://arxiv.org/abs/2509.22813", "authors": ["Sahar Dastani", "Ali Bahri", "Gustavo Adolfo Vargas Hakim", "Moslem Yazdanpanah", "Mehrdad Noori", "David Osowiechi", "Samuel Barbeau", "Ismail Ben Ayed", "Herve Lombaert", "Christian Desrosiers"], "title": "TRUST: Test-Time Refinement using Uncertainty-Guided SSM Traverses", "comment": null, "summary": "State Space Models (SSMs) have emerged as efficient alternatives to Vision\nTransformers (ViTs), with VMamba standing out as a pioneering architecture\ndesigned for vision tasks. However, their generalization performance degrades\nsignificantly under distribution shifts. To address this limitation, we propose\nTRUST (Test-Time Refinement using Uncertainty-Guided SSM Traverses), a novel\ntest-time adaptation (TTA) method that leverages diverse traversal permutations\nto generate multiple causal perspectives of the input image. Model predictions\nserve as pseudo-labels to guide updates of the Mamba-specific parameters, and\nthe adapted weights are averaged to integrate the learned information across\ntraversal scans. Altogether, TRUST is the first approach that explicitly\nleverages the unique architectural properties of SSMs for adaptation.\nExperiments on seven benchmarks show that TRUST consistently improves\nrobustness and outperforms existing TTA methods.", "AI": {"tldr": "TRUST is a new method for test-time adaptation in state space models, improving their robustness and performance under distribution shifts.", "motivation": "The motivation is to improve the generalization performance of SSMs like VMamba under distribution shifts, as their performance often degrades significantly in these scenarios.", "method": "State Space Models (SSMs) are used, with TRUST introduced as a test-time adaptation (TTA) method that uses diverse traversal permutations to generate multiple causal perspectives, and model predictions serve as pseudo-labels to guide updates of the model parameters.", "result": "TRUST is shown to improve robustness and outperform existing TTA methods, as evidenced by experiments on seven benchmarks.", "conclusion": "TRUST is the first approach that explicitly leverages the unique architectural properties of SSMs for adaptation, leading to improved performance in terms of robustness."}}
{"id": "2509.22856", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.22856", "abs": "https://arxiv.org/abs/2509.22856", "authors": ["R. Alexander Knipper", "Charles S. Knipper", "Kaiqi Zhang", "Valerie Sims", "Clint Bowers", "Santu Karmaker"], "title": "The Bias is in the Details: An Assessment of Cognitive Bias in LLMs", "comment": null, "summary": "As Large Language Models (LLMs) are increasingly embedded in real-world\ndecision-making processes, it becomes crucial to examine the extent to which\nthey exhibit cognitive biases. Extensively studied in the field of psychology,\ncognitive biases appear as systematic distortions commonly observed in human\njudgments. This paper presents a large-scale evaluation of eight\nwell-established cognitive biases across 45 LLMs, analyzing over 2.8 million\nLLM responses generated through controlled prompt variations. To achieve this,\nwe introduce a novel evaluation framework based on multiple-choice tasks,\nhand-curate a dataset of 220 decision scenarios targeting fundamental cognitive\nbiases in collaboration with psychologists, and propose a scalable approach for\ngenerating diverse prompts from human-authored scenario templates. Our analysis\nshows that LLMs exhibit bias-consistent behavior in 17.8-57.3% of instances\nacross a range of judgment and decision-making contexts targeting anchoring,\navailability, confirmation, framing, interpretation, overattribution, prospect\ntheory, and representativeness biases. We find that both model size and prompt\nspecificity play a significant role on bias susceptibility as follows: larger\nsize (>32B parameters) can reduce bias in 39.5% of cases, while higher prompt\ndetail reduces most biases by up to 14.9%, except in one case\n(Overattribution), which is exacerbated by up to 8.8%.", "AI": {"tldr": "研究大规模评估了45个LLMs中的8个基本认知偏见，发现LLMs在不同判断和决策情境中表现出偏见的一致性行为，模型大小和提示的特定性会影响偏见的易感性。", "motivation": "随着大型语言模型（LLMs）越来越多地嵌入到现实世界的决策过程中，检查它们表现出的认知偏差的程度变得至关重要。", "method": "本研究提出了一种基于多项选择任务的新评估框架，与心理学家合作定制了220个决策场景的数据集，针对基本的认知偏见，并提出了一种可扩展的方法，从人类编写的情景模板生成多样化的提示。", "result": "分析结果显示，各模型在针对锚定、可用性、确认、框架、解释、过度归因、前景理论、代表性偏见等方面判断和决策情境中，表现出偏见一致性行为的比例为17.8%-57.3%。研究还发现，模型大小超过32B参数时可降低偏见39.5%，而更高细粒度的提示可以将大多数偏见减少高达14.9%，除过度归因（被煽动高达8.8%之外）。", "conclusion": "本研究证明了LLMs在判断和决策情境中展现认知偏见一致性行为，并进一步探索了模型大小和提示特定性对偏见影响的复杂性。"}}
{"id": "2509.22820", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.22820", "abs": "https://arxiv.org/abs/2509.22820", "authors": ["Jaeik Kim", "Woojin Kim", "Woohyeon Park", "Jaeyoung Do"], "title": "MMPB: It's Time for Multi-Modal Personalization", "comment": null, "summary": "Visual personalization is essential in user-facing AI systems such as smart\nhomes and healthcare, where aligning model behavior with user-centric concepts\nis critical. However, recent large Vision-Language Models (VLMs), despite their\nbroad applicability, remain underexplored in their ability to adapt to\nindividual users. In this paper, we introduce MMPB, the first extensive\nbenchmark for evaluating VLMs on personalization. MMPB comprises 10k\nimage-query pairs and includes 111 personalizable concepts across four\ncategories: humans, animals, objects, and characters, with the human category\nenriched with preference-grounded queries. We structure personalization into\nthree main task types, each highlighting a different key property of VLMs.\nUsing 23 widely used VLMs including both open- and closed-source models, we\nevaluate personalization performance via a three-stage protocol: concept\ninjection, multi-turn dialogue, and personalized querying. Our findings\nindicate that most VLMs (including some closed-source models) struggle with\npersonalization, particularly in maintaining consistency over dialogue,\nhandling user preferences, and adapting to visual cues. Our analysis reveals\nthat the challenges in VLM personalization (such as refusal behaviors and\nlong-context forgetting) highlight substantial room for improvement. By\nidentifying these limitations and offering a scalable benchmark, MMPB offers\nvaluable insights and a solid foundation for future research toward truly\npersonalized multi-modal AI. Project Page: aidaslab.github.io/MMPB", "AI": {"tldr": "本研究通过MMPB全面评估VLM的个性化能力，发现这些模型在多个维度上存在局限性，并提出未来的研究方向。", "motivation": "视觉个性化在面向用户的AI系统中至关重要，如智能家居和医疗保健，但现有VLM在个性化方面的潜力未充分探索。本研究旨在评估VLM的个性化能力。", "method": "引入了MMPB，这是首个全面评估视觉-语言模型（VLM）个性化能力的基准测试。MMPB包含10k图像-查询对，并涵盖111个可个性化概念，分为四类：人类、动物、物体和角色。评估个性化性能的过程分为三阶段：概念注入、多轮对话和个人化查询。", "result": "结果表明，大多数VLM在维持对话一致性、处理用户偏好和适应视觉线索方面遇到困难。结论揭示了VLM在个性化领域的挑战和发展空间。", "conclusion": "MMPB通过识别VLM的局限性并提供可扩展基准，为未来的多模态AI研究提供了有价值的洞见和坚实基础。"}}
{"id": "2509.22870", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.22870", "abs": "https://arxiv.org/abs/2509.22870", "authors": ["Passant Elchafei", "Mayar Osama", "Mohamed Rageh", "Mervat Abuelkheir"], "title": "Lexicon-Enriched Graph Modeling for Arabic Document Readability Prediction", "comment": null, "summary": "We present a graph-based approach enriched with lexicons to predict\ndocument-level readability in Arabic, developed as part of the Constrained\nTrack of the BAREC Shared Task 2025. Our system models each document as a\nsentence-level graph, where nodes represent sentences and lemmas, and edges\ncapture linguistic relationships such as lexical co-occurrence and class\nmembership. Sentence nodes are enriched with features from the SAMER lexicon as\nwell as contextual embeddings from the Arabic transformer model. The graph\nneural network (GNN) and transformer sentence encoder are trained as two\nindependent branches, and their predictions are combined via late fusion at\ninference. For document-level prediction, sentence-level outputs are aggregated\nusing max pooling to reflect the most difficult sentence. Experimental results\nshow that this hybrid method outperforms standalone GNN or transformer branches\nacross multiple readability metrics. Overall, the findings highlight that\nfusion offers advantages at the document level, but the GNN-only approach\nremains stronger for precise prediction of sentence-level readability.", "AI": {"tldr": "研究阐述了一种结合词汇表信息和上下文嵌入的图基方法来预测阿拉伯语文档的可读性，并发现融合多种模型的预测结果能提高文档层面的预测准确度。", "motivation": "研究的动机是提出一种有效的方法来预测阿拉伯语文档级别的可读性。", "method": "该方法使用基于图的方法，结合词汇表信息和上下文嵌入来预测文档可读性。通过图神经网络和转换器句编码器的晚期融合来结合两种模型的预测结果。", "result": "我们提出了一种基于图的方法，结合词汇表来预测阿拉伯语文档级别的可读性，这是作为BAREC共享任务2025约束性赛道的一部分开发的。该系统将每个文档建模为一个句子级图，节点表示句子和词根，边捕捉诸如词汇共现和类别成员资格等语言关系。句子节点通过Samer词汇表中的特征和阿拉伯语转换器模型的上下文嵌入进行丰富。图神经网络（GNN）和转换器句编码器作为两个独立的分支进行训练，其预测结果在推理过程中通过晚期融合相结合。对于文档级别的预测，句子级别的输出使用最大池化聚集，以反映最困难的句子。实验结果表明，这种方法在多个可读性指标上优于单独的GNN或转换器分支。总体而言，研究结果表明融合在文档级别有优势，但GNN单独的方法在句子级别的可读性预测方面仍然更强大。", "conclusion": "研究得出结论，融合模型在文档级可读性预测中表现出色，但单独使用GNN模型在句子级别的可读性预测中更为准确。"}}
{"id": "2509.22836", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.22836", "abs": "https://arxiv.org/abs/2509.22836", "authors": ["Roie Kazoom", "Alon Goldberg", "Hodaya Cohen", "Ofer Hadar"], "title": "Seeing Isn't Believing: Context-Aware Adversarial Patch Synthesis via Conditional GAN", "comment": null, "summary": "Adversarial patch attacks pose a severe threat to deep neural networks, yet\nmost existing approaches rely on unrealistic white-box assumptions, untargeted\nobjectives, or produce visually conspicuous patches that limit real-world\napplicability. In this work, we introduce a novel framework for fully\ncontrollable adversarial patch generation, where the attacker can freely choose\nboth the input image x and the target class y target, thereby dictating the\nexact misclassification outcome. Our method combines a generative U-Net design\nwith Grad-CAM-guided patch placement, enabling semantic-aware localization that\nmaximizes attack effectiveness while preserving visual realism. Extensive\nexperiments across convolutional networks (DenseNet-121, ResNet-50) and vision\ntransformers (ViT-B/16, Swin-B/16, among others) demonstrate that our approach\nachieves state-of-the-art performance across all settings, with attack success\nrates (ASR) and target-class success (TCS) consistently exceeding 99%.\n  Importantly, we show that our method not only outperforms prior white-box\nattacks and untargeted baselines, but also surpasses existing non-realistic\napproaches that produce detectable artifacts. By simultaneously ensuring\nrealism, targeted control, and black-box applicability-the three most\nchallenging dimensions of patch-based attacks-our framework establishes a new\nbenchmark for adversarial robustness research, bridging the gap between\ntheoretical attack strength and practical stealthiness.", "AI": {"tldr": "This paper introduces a novel framework for adversarial patch generation that can generate visually realistic and highly effective patches with fully controllable input and target class, achieving state-of-the-art performance across multiple network architectures.", "motivation": "The motivation behind this paper is to address the limitations of existing adversarial patch attacks which are either based on unrealistic white-box assumptions, untargeted objectives, or produce visibly obvious patches.", "method": "Our method combines a generative U-Net design with Grad-CAM-guided patch placement, enabling semantic-aware localization that maximizes attack effectiveness while preserving visual realism.", "result": "Extensive experiments demonstrate that the approach achieves state-of-the-art performance, with attack success rates (ASR) and target-class success (TCS) consistently exceeding 99%, surpassing both past white-box attacks and untargeted baselines.", "conclusion": "By ensuring realism, targeted control, and black-box applicability, the framework proposed sets a new benchmark for adversarial robustness, bridging the gap between theoretical attack strength and practical stealthiness."}}
{"id": "2509.22876", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.22876", "abs": "https://arxiv.org/abs/2509.22876", "authors": ["Gabriela Pinto", "Palash Goyal", "Yiwen Song", "Souradip Chakraborty", "Zifeng Wang", "Tomas Pfister", "Hamid Palangi"], "title": "HEART: Emotionally-driven test-time scaling of Language Models", "comment": null, "summary": "Test-time scaling has shown considerable success in improving the performance\nof language models on complex reasoning tasks without requiring fine-tuning.\nHowever, current strategies such as self-reflection primarily focus on logical\nor structural refinement. They do not leverage the guiding potential of\naffective feedback. Inspired by psychological research showing that emotions\ncan modulate cognitive performance, we introduce HEART--a novel framework that\nuses emotionally-driven prompts for iterative self-correction. HEART provides\nfeedback on a model's incorrect response using a curated set of concise,\nemotionally charged phrases based on the six universal emotions categorized by\nDr. Paul Ekman. By systematically varying the emotional tone of the feedback\nacross iterations, our method guides the model to escape flawed reasoning paths\nand explore more promising alternatives. We evaluate our framework on\nchallenging reasoning benchmarks including OlympiadBench, Humanity's Last Exam,\nand SimpleQA. Our results reveal a significant new phenomenon: when guided by\nan oracle verifier, this affective iteration protocol unlocks significantly\ndeeper reasoning, leading to consistent and substantial increases in accuracy\nover state-of-the-art baselines with the same verifier. However, we also\nidentify a critical bottleneck for practical deployment. In a verifier-free\nsetting, it struggles to harness these gains consistently, highlighting as a\nkey challenge for future work. Our findings suggest that the next frontier in\nmachine reasoning may lie not just in refining logic, but also in understanding\nand leveraging the `HEART' of the models.", "AI": {"tldr": "HEART框架通过情感反馈进行迭代自我修正，显著提升了语言模型在复杂推理任务上的性能，但在没有外部验证的情况下仍存在问题。", "motivation": "现有的test-time scaling策略，如自我反省，主要集中在逻辑或结构性改进上，忽视了情感反馈的指导潜力。受心理研究启发，提出HEART框架，探索情感在引导认知表现上的潜力。", "method": "HEART框架使用基于六种基本情绪（由Paul Ekman博士分类）的情感驱动提示进行迭代自我修正，旨在通过情感反馈指导模型克服逻辑或结构上的短板，探索更合理的推理路径。", "result": "在具有挑战性的推理基准测试如OlympiadBench, Humanity's Last Exam和SimpleQA上，HEART方法在有oracle验证器的指导下展现了明显的更深层次的推理能力，并且在准确性上取得了显著且一致的进步。", "conclusion": "研究结果表明，机器推理的下一步可能不仅在于逻辑的精炼，还在于理解和利用模型背后的情感核心。然而，实际部署中仍面临关键瓶颈，在没有验证器的情况下，性能一致性较差，这将是未来研究的关键挑战。"}}
{"id": "2509.22839", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.22839", "abs": "https://arxiv.org/abs/2509.22839", "authors": ["Ibrahim Delibasoglu", "Fredrik Heintz"], "title": "Learning Temporal Saliency for Time Series Forecasting with Cross-Scale Attention", "comment": null, "summary": "Explainability in time series forecasting is essential for improving model\ntransparency and supporting informed decision-making. In this work, we present\nCrossScaleNet, an innovative architecture that combines a patch-based\ncross-attention mechanism with multi-scale processing to achieve both high\nperformance and enhanced temporal explainability. By embedding attention\nmechanisms into the training process, our model provides intrinsic\nexplainability for temporal saliency, making its decision-making process more\ntransparent. Traditional post-hoc methods for temporal saliency detection are\ncomputationally expensive, particularly when compared to feature importance\ndetection. While ablation techniques may suffice for datasets with fewer\nfeatures, identifying temporal saliency poses greater challenges due to its\ncomplexity. We validate CrossScaleNet on synthetic datasets with known saliency\nground truth and on established public benchmarks, demonstrating the robustness\nof our method in identifying temporal saliency. Experiments on real-world\ndatasets for forecasting task show that our approach consistently outperforms\nmost transformer-based models, offering better explainability without\nsacrificing predictive accuracy. Our evaluations demonstrate superior\nperformance in both temporal saliency detection and forecasting accuracy.\nMoreover, we highlight that existing models claiming explainability often fail\nto maintain strong performance on standard benchmarks. CrossScaleNet addresses\nthis gap, offering a balanced approach that captures temporal saliency\neffectively while delivering state-of-the-art forecasting performance across\ndatasets of varying complexity.", "AI": {"tldr": "本文提出了CrossScaleNet，一种结合注意力机制和多尺度处理的新型架构，实现了高性能和高透明度的时间序列预测。实验证明其在时间显著性检测和预测准确性方面均优于现有的基于变压器的模型。", "motivation": "提高时间序列预测模型的可解释性，以提高模型透明度并支持决策制定。", "method": "CrossScaleNet结合基于patch的跨注意力机制与多尺度处理，以实现高性能和增强的时间解释性。通过在训练过程中嵌入注意力机制，该模型提供了内在的时间显著性解释性，使其决策过程更加透明。", "result": "实验表明，CrossScaleNet在合成数据集和公共基准数据集上表现出了识别时间显著性的鲁棒性。在实际预测任务上，该方法在大多数基于变压器的模型之外提供了更好的解释性和预测准确性。", "conclusion": "CrossScaleNet提供了一种平衡的方法，既能有效捕捉时间显著性，又能在不同复杂度的数据集上实现一流的预测表现。"}}
{"id": "2509.22887", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2509.22887", "abs": "https://arxiv.org/abs/2509.22887", "authors": ["EunJeong Hwang", "Yuwei Yin", "Giuseppe Carenini", "Peter West", "Vered Shwartz"], "title": "Infusing Theory of Mind into Socially Intelligent LLM Agents", "comment": null, "summary": "Theory of Mind (ToM)-an understanding of the mental states of others-is a key\naspect of human social intelligence, yet, chatbots and LLM-based social agents\ndo not typically integrate it. In this work, we demonstrate that LLMs that\nexplicitly use ToM get better at dialogue, achieving goals more effectively.\nAfter showing that simply prompting models to generate mental states between\ndialogue turns already provides significant benefit, we further introduce\nToMAgent (ToMA), a ToM-focused dialogue agent. ToMA is trained by pairing ToM\nwith dialogue lookahead to produce mental states that are maximally useful for\nachieving dialogue goals. Experiments on the Sotopia interactive social\nevaluation benchmark demonstrate the effectiveness of our method over a range\nof baselines. Comprehensive analysis shows that ToMA exhibits more strategic,\ngoal-oriented reasoning behaviors, which enable long-horizon adaptation, while\nmaintaining better relationships with their partners. Our results suggest a\nstep forward in integrating ToM for building socially intelligent LLM agents.", "AI": {"tldr": "引入ToMAgent，通过整合ToM和对话前瞻，训练LLM生成有助于实现对话目标的心理状态，提高了对话代理的社会智能。", "motivation": "动机是解决当前聊天机器人和基于LLM的社交代理通常不集成ToM的问题，旨在展示显式使用ToM的LLM在对话中表现得更好，能够更有效地实现目标。", "method": "在本研究中，通过简单提示模型在对话轮次之间生成心理状态，已经能显著提高对话质量。进一步提出了ToMAgent（ToMA），该智能对话代理通过将ToM与对话前瞻相结合，训练模型生成最有助于实现对话目标的心理状态。", "result": "实验结果表明，在Sotopia交互式社交评估基准上的测试，本方法在一系列基线中表现优异。", "conclusion": "研究结果表明，ToMA显示出更具战略性和目标导向的行为，能够在保持良好人际关系的同时，实现长期适应。这是将ToM整合到构建社交智能LLM代理的一个飞跃。"}}
