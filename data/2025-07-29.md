<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 21]
- [cs.CV](#cs.CV) [Total: 17]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Advancing Mental Disorder Detection: A Comparative Evaluation of Transformer and LSTM Architectures on Social Media](https://arxiv.org/abs/2507.19511)
*Khalid Hasan,Jamil Saquer,Mukulika Ghosh*

Main category: cs.CL

> 研究比较了变压器模型和LSTM模型在Reddit心理健康障碍文本分类上的表现，数据显示变压器模型更为出色，特别是RoBERTa模型。LSTM模型搭配BERT嵌入也表现良好但计算需求更低，这对临床应用和数字心理健康干预有重要影响。

<details>
  <summary>Details</summary>

**Motivation:** 鉴于心理健康障碍的普遍性和自动工具在早期检测和监控中的必要性，这项研究利用自然语言处理的最新进展来评估不同模型在心理健康障碍分类上的表现。

**Method:** 该研究对比了基于变压器架构的先进模型（如BERT、RoBERTa、DistilBERT、ALBERT、ELECTRA）和基于LSTM的方法，并使用了不同的文本嵌入技术来分类Reddit上的心理健康障碍。研究还构建了一个大型标注数据集，并通过统计判决分析和主题建模验证了数据集的可靠性。

**Result:** 实验结果表明，变压器模型的表现优于传统的深度学习方法。RoBERTa达到了最高的分类表现，其在保留测试集上F1分数为99.54%，在外部测试集上F1分数为96.05%。增强BERT嵌入的LSTM模型同样表现优秀，虽然计算资源需求较低，但在外部数据集上的F1分数超过94%。

**Conclusion:** 研究结果指出了基于变压器的模型在实时、可扩展的心理健康监控中的有效性。讨论了这些发现对临床应用及数字化心理健康干预的意义，提供对于先进自然语言处理方法在心理障碍检测中的能力与局限性的见解。

**Abstract:** The rising prevalence of mental health disorders necessitates the development
of robust, automated tools for early detection and monitoring. Recent advances
in Natural Language Processing (NLP), particularly transformer-based
architectures, have demonstrated significant potential in text analysis. This
study provides a comprehensive evaluation of state-of-the-art transformer
models (BERT, RoBERTa, DistilBERT, ALBERT, and ELECTRA) against Long Short-Term
Memory (LSTM) based approaches using different text embedding techniques for
mental health disorder classification on Reddit. We construct a large annotated
dataset, validating its reliability through statistical judgmental analysis and
topic modeling. Experimental results demonstrate the superior performance of
transformer models over traditional deep-learning approaches. RoBERTa achieved
the highest classification performance, with a 99.54% F1 score on the hold-out
test set and a 96.05% F1 score on the external test set. Notably, LSTM models
augmented with BERT embeddings proved highly competitive, achieving F1 scores
exceeding 94% on the external dataset while requiring significantly fewer
computational resources. These findings highlight the effectiveness of
transformer-based models for real-time, scalable mental health monitoring. We
discuss the implications for clinical applications and digital mental health
interventions, offering insights into the capabilities and limitations of
state-of-the-art NLP methodologies in mental disorder detection.

</details>


### [2] [Setting The Table with Intent: Intent-aware Schema Generation and Editing for Literature Review Tables](https://arxiv.org/abs/2507.19521)
*Vishakh Padmakumar,Joseph Chee Chang,Kyle Lo,Doug Downey,Aakanksha Naik*

Main category: cs.CL

> 本文解决了模式生成中的两个主要问题:参考评估的模糊性和缺乏编辑方法,通过合成意图增强表格语料库和提出基于大语言模型的编辑技术,提高模式生成的质量。

<details>
  <summary>Details</summary>

**Motivation:** 由于学术文献数量的增加,研究人员需要组织、比较和对比文献集。虽然大型语言模型可以帮助生成比较论文的共同方面的模式,模式生成的进步缓慢,主要由于参考评估的模糊性和缺乏编辑/细化方法。

**Method:** 提出了一种方法来增强未标注的表格语料库与合成意图,并使用该数据集展示了如何通过结合表格意图显著提高基线性能,重新构造参考模式。此外,还提出了几种基于大语言模型的模式编辑技术,并展示了通过编辑技术可以进一步改善这些方法生成的模式。

**Result:** 展示了结合表格意图显著提高了基线性能,重新构造参考模式。此外,展示了基于大语言模型的编辑技术可以进一步改善生成的模式。

**Conclusion:** 本文首次解决了模式生成中的两个关键问题:参考评估的模糊性和缺乏编辑方法。通过提出新的数据集和编辑技术,证明了可以提高模式生成的质量。

**Abstract:** The increasing volume of academic literature makes it essential for
researchers to organize, compare, and contrast collections of documents. Large
language models (LLMs) can support this process by generating schemas defining
shared aspects along which to compare papers. However, progress on schema
generation has been slow due to: (i) ambiguity in reference-based evaluations,
and (ii) lack of editing/refinement methods. Our work is the first to address
both issues. First, we present an approach for augmenting unannotated table
corpora with synthesized intents and apply it to create a dataset for studying
schema generation conditioned on a given information need, thus reducing
ambiguity. With this dataset, we show how incorporating table intents
significantly improves baseline performance in reconstructing reference
schemas. Next, we propose several LLM-based schema editing techniques. We start
by comprehensively benchmarking several single-shot schema generation methods,
including prompted LLM workflows and fine-tuned models, showing that smaller,
open-weight models can be fine-tuned to be competitive with state-of-the-art
prompted LLMs. Then we demonstrate that our editing techniques can further
improve schemas generated by these methods.

</details>


### [3] [Mind the Language Gap in Digital Humanities: LLM-Aided Translation of SKOS Thesauri](https://arxiv.org/abs/2507.19537)
*Felix Kraus,Nicolas Blumenröhr,Danah Tonne,Achim Streit*

Main category: cs.CL

> 介绍了WOKIE，该工具是一个开源、模块化、可直接使用的管线，用于SKOS词典的自动化翻译，解决了数字人文中语言多样性的问题，促进了知识资源的访问和互操作性。

<details>
  <summary>Details</summary>

**Motivation:** 解决数字人文领域的语言多样性问题，该问题限制了知识资源的访问、重用及语义互操作性。

**Method:** 采用了一种结合外部翻译服务与大型语言模型（LLMs）进行针对性优化的方法，以平衡翻译的质量、可扩展性与成本。WOKIE设计用于日常硬件上运行且易于扩展，无需专门的机器翻译或LLMs知识。

**Result:** 通过多个数字人文词典的实验，在15种不同语言中，使用不同参数、翻译服务及大型语言模型进行系统分析，结果表明WOKIE适合进行无障碍自动化翻译，并提升了本体匹配的性能。

**Conclusion:** WOKIE能够增强词典的可访问性、重用率及跨语言互操作性，从而支持更加包容的多语言研究基础设施。

**Abstract:** We introduce WOKIE, an open-source, modular, and ready-to-use pipeline for
the automated translation of SKOS thesauri. This work addresses a critical need
in the Digital Humanities (DH), where language diversity can limit access,
reuse, and semantic interoperability of knowledge resources. WOKIE combines
external translation services with targeted refinement using Large Language
Models (LLMs), balancing translation quality, scalability, and cost. Designed
to run on everyday hardware and be easily extended, the application requires no
prior expertise in machine translation or LLMs. We evaluate WOKIE across
several DH thesauri in 15 languages with different parameters, translation
services and LLMs, systematically analysing translation quality, performance,
and ontology matching improvements. Our results show that WOKIE is suitable to
enhance the accessibility, reuse, and cross-lingual interoperability of
thesauri by hurdle-free automated translation and improved ontology matching
performance, supporting more inclusive and multilingual research
infrastructures.

</details>


### [4] [Mitigating Geospatial Knowledge Hallucination in Large Language Models: Benchmarking and Dynamic Factuality Aligning](https://arxiv.org/abs/2507.19586)
*Shengyuan Wang,Jie Feng,Tianhui Liu,Dan Pei,Yong Li*

Main category: cs.CL

> The study presents an evaluation framework and mitigation strategy for geospatial hallucinations in LLMs, significantly enhancing their performance in geospatial knowledge tasks.

<details>
  <summary>Details</summary>

**Motivation:** The aim is to address the issue of geospatial hallucinations (inaccuracies in geospatial information) in large language models, as the phenomenon has been less explored compared to general knowledge hallucination.

**Method:** The paper proposes a comprehensive evaluation framework for geospatial hallucinations in LLMs using structured geospatial knowledge graphs and introduces a dynamic factuality aligning method based on Kahneman-Tversky Optimization (KTO) to mitigate geospatial hallucinations.

**Result:** The proposed method demonstrates a performance improvement of over 29.6% on the benchmark, enhancing the trustworthiness of LLMs in geospatial knowledge and reasoning tasks.

**Conclusion:** The research shows the effectiveness of the evaluation framework and the factuality aligning method in mitigating geospatial hallucinations, thereby improving the reliability of LLMs for geospatial tasks.

**Abstract:** Large language models (LLMs) possess extensive world knowledge, including
geospatial knowledge, which has been successfully applied to various geospatial
tasks such as mobility prediction and social indicator prediction. However,
LLMs often generate inaccurate geospatial knowledge, leading to geospatial
hallucinations (incorrect or inconsistent representations of geospatial
information) that compromise their reliability. While the phenomenon of general
knowledge hallucination in LLMs has been widely studied, the systematic
evaluation and mitigation of geospatial hallucinations remain largely
unexplored. To address this gap, we propose a comprehensive evaluation
framework for geospatial hallucinations, leveraging structured geospatial
knowledge graphs for controlled assessment. Through extensive evaluation across
20 advanced LLMs, we uncover the hallucinations in their geospatial knowledge.
Building on these insights, we introduce a dynamic factuality aligning method
based on Kahneman-Tversky Optimization (KTO) to mitigate geospatial
hallucinations in LLMs, leading to a performance improvement of over 29.6% on
the proposed benchmark. Extensive experimental results demonstrate the
effectiveness of our benchmark and learning algorithm in enhancing the
trustworthiness of LLMs in geospatial knowledge and reasoning tasks.

</details>


### [5] [Efficient Attention Mechanisms for Large Language Models: A Survey](https://arxiv.org/abs/2507.19595)
*Yutao Sun,Zhenyu Li,Yike Zhang,Tengyu Pan,Bowen Dong,Yuyi Guo,Jianyong Wang*

Main category: cs.CL

> 本文综述了用于解决Transformer架构中自注意力机制效率问题的线性注意力和稀疏注意力机制，并讨论了它们在大规模预训练语言模型中的应用。

<details>
  <summary>Details</summary>

**Motivation:** 解决Transformer中自注意力机制的二次复杂度问题，使其适用于长上下文的高效建模。

**Method:** Transformer架构以其强大的语言建模能力而占据了主导地位，但自注意力机制的时间和内存复杂度为二次方，成为高效长上下文建模的主要障碍。本文综述了两种主要的高效注意力机制类别：线性注意力机制和稀疏注意力机制，并探讨了它们在大规模预训练语言模型中的应用。

**Result:** 通过系统全面地综述线性注意力和稀疏注意力机制的开发，本文为设计高效可扩展的语言模型提供了理论和实践上的指导。

**Conclusion:** 综述表明，通过采用线性注意力和稀疏注意力机制，可以在保持上下文覆盖率的同时提高Transformer模型的效率，为设计高效的长上下文语言模型指明了方向。

**Abstract:** Transformer-based architectures have become the prevailing backbone of large
language models. However, the quadratic time and memory complexity of
self-attention remains a fundamental obstacle to efficient long-context
modeling. To address this limitation, recent research has introduced two
principal categories of efficient attention mechanisms. Linear attention
methods achieve linear complexity through kernel approximations, recurrent
formulations, or fastweight dynamics, thereby enabling scalable inference with
reduced computational overhead. Sparse attention techniques, in contrast, limit
attention computation to selected subsets of tokens based on fixed patterns,
block-wise routing, or clustering strategies, enhancing efficiency while
preserving contextual coverage. This survey provides a systematic and
comprehensive overview of these developments, integrating both algorithmic
innovations and hardware-level considerations. In addition, we analyze the
incorporation of efficient attention into largescale pre-trained language
models, including both architectures built entirely on efficient attention and
hybrid designs that combine local and global components. By aligning
theoretical foundations with practical deployment strategies, this work aims to
serve as a foundational reference for advancing the design of scalable and
efficient language models.

</details>


### [6] [MOCHA: Are Code Language Models Robust Against Multi-Turn Malicious Coding Prompts?](https://arxiv.org/abs/2507.19598)
*Muntasir Wahed,Xiaona Zhou,Kiet A. Nguyen,Tianjiao Yu,Nirav Diwan,Gang Wang,Dilek Hakkani-Tür,Ismini Lourentzou*

Main category: cs.CL

> 本文介绍了code decomposition attacks，这是一种通过将恶意任务分解为多个看似无害的子任务来绕过安全检查的方法，并提出了一个大规模基准测试enchmarkname{}，用于评估代码生成LLMs的稳健性。实验结果表明了这些模型在多轮次恶意提示下的薄弱点。

<details>
  <summary>Details</summary>

**Motivation:** 尽管大型语言模型在代码生成能力上取得了显著进展，但它们对于多轮恶意编程提示的稳健性问题仍然尚未充分探讨，这就是本文的研究动机。

**Method:** 本文提出了code decomposition attacks的概念，即将恶意编程任务分解为一系列看似无害的子任务，以逃避安全过滤器的检测。为系统性评估大模型的鲁棒性，作者还提出了一个大规模基准测试enchmarkname{}，用于评估代码LLMs对单轮和多轮恶意提示的鲁棒性。

**Result:** 实验结果表明，开放和封闭源代码模型在多轮攻击场景中存在持续的脆弱性。针对MOCHA进行微调可以提高拒绝率，同时保持代码生成能力，并在没有额外监督的情况下提高对外部对抗性数据集的鲁棒性，拒绝率增加高达32.4%。

**Conclusion:** 研究表明，提出的方法能够有效地检测和缓解code decomposition attacks，并通过微调增强了模型的对外敌对攻击的鲁棒性。

**Abstract:** Recent advancements in Large Language Models (LLMs) have significantly
enhanced their code generation capabilities. However, their robustness against
adversarial misuse, particularly through multi-turn malicious coding prompts,
remains underexplored. In this work, we introduce code decomposition attacks,
where a malicious coding task is broken down into a series of seemingly benign
subtasks across multiple conversational turns to evade safety filters. To
facilitate systematic evaluation, we introduce \benchmarkname{}, a large-scale
benchmark designed to evaluate the robustness of code LLMs against both
single-turn and multi-turn malicious prompts. Empirical results across open-
and closed-source models reveal persistent vulnerabilities, especially under
multi-turn scenarios. Fine-tuning on MOCHA improves rejection rates while
preserving coding ability, and importantly, enhances robustness on external
adversarial datasets with up to 32.4% increase in rejection rates without any
additional supervision.

</details>


### [7] [HITSZ's End-To-End Speech Translation Systems Combining Sequence-to-Sequence Auto Speech Recognition Model and Indic Large Language Model for IWSLT 2025 in Indic Track](https://arxiv.org/abs/2507.19616)
*Xuchen Wei,Yangxin Wu,Yaoyin Zhang,Henglyu Liu,Kehai Chen,Xuefeng Bai,Min Zhang*

Main category: cs.CL

> HITSZ 提交了 IWSLT 2025 Indic 轨道的成果，该成果应对英语和印度语之间的语音转文字翻译。通过结合预训练的 Whisper ASR 和 Indic 专用Krutm LLM，平均 BLEU 分数分别达到 28.88（英语转印度语）和 27.86（印度语转英语）。同时，Chain-of-Thought 方法显示出潜力但存在输出格式一致性的挑战。

<details>
  <summary>Details</summary>

**Motivation:** 为了改善低资源情况下的翻译质量，HITSZ 提出了一个将预训练的 ASR 模型与 Indic 专用大语言模型结合的端到端系统。

**Method:** 端到端的系统将预训练的 Whisper ASR 模型与 Krutrim 语言模型相结合，应用于英语和印度语的语音转文字翻译。同时，还研究了 Chain-of-Thought 方法的可能性。

**Result:** 实验结果显示，此系统的平均 BLEU 分数分别为：从英语到印度语为 28.88，从印度语到英语为 27.86。Chain-of-Thought 方法显示出了通过成功解析的输出增加翻译质量的潜力，如在泰米尔语到英语翻译中，有 13.84 的 BLEU 提升。然而，也发现难以确保模型始终遵守所需的 Chain-of-Thought 输出格式。

**Conclusion:** 虽然 Chain-of-Thought 方法证明了提升翻译质量的可能，但在格式一致性上面临挑战。结合集成的预训练 Whispher ASR 和 Krutrim，模型在低资源环境中的翻译效果有了显著成效。同时，研究了进一步提高翻译质量的可能性。

**Abstract:** This paper presents HITSZ's submission for the IWSLT 2025 Indic track,
focusing on speech-to-text translation (ST) for English-to-Indic and
Indic-to-English language pairs. To enhance translation quality in this
low-resource scenario, we propose an end-to-end system integrating the
pre-trained Whisper automated speech recognition (ASR) model with Krutrim, an
Indic-specialized large language model (LLM). Experimental results demonstrate
that our end-to-end system achieved average BLEU scores of $28.88$ for
English-to-Indic directions and $27.86$ for Indic-to-English directions.
Furthermore, we investigated the Chain-of-Thought (CoT) method. While this
method showed potential for significant translation quality improvements on
successfully parsed outputs (e.g. a $13.84$ BLEU increase for
Tamil-to-English), we observed challenges in ensuring the model consistently
adheres to the required CoT output format.

</details>


### [8] [MCIF: Multimodal Crosslingual Instruction-Following Benchmark from Scientific Talks](https://arxiv.org/abs/2507.19634)
*Sara Papi,Maike Züfle,Marco Gaido,Beatrice Savoldi,Danni Liu,Ioannis Douros,Luisa Bentivogli,Jan Niehues*

Main category: cs.CL

> 介绍MCIF，该基准测试旨在评估多模态大规模语言模型（MLLMs）的多语言和多模态能力，解决了现有基准测试在这些方面评估不足的问题。

<details>
  <summary>Details</summary>

**Motivation:** 现有的基准测试在多语言和多模态能力方面的评估不足：它们往往是单一语言的，主要集中在某一模态，依赖于短上下文，或者缺乏人工注释，这阻碍了模型在跨语言、多模态和任务复杂性方面表现的全面评估。

**Method:** 提出MCIF (Multimodal Crosslingual Instruction Following)，这是一个多语言的人工注释基准，基于科学演讲，旨在评估跨语言、多模态环境中的指令跟随能力，涵盖短文本和长文本输入。MCIF跨越了三个核心模态（语音、视觉和文本）和四种不同的语言（英语、德语、意大利语和中文）。

**Result:** 未提供具体实验结果。

**Conclusion:** MCIF基准测试发布在CC-BY 4.0许可下，鼓励开放研究和MLLMs的发展。

**Abstract:** Recent advances in large language models have catalyzed the development of
multimodal LLMs (MLLMs) that integrate text, speech, and vision within unified
frameworks. As MLLMs evolve from narrow, monolingual, task-specific systems to
general-purpose instruction-following models, a key frontier lies in evaluating
their multilingual and multimodal capabilities over both long and short
contexts. However, existing benchmarks fall short in evaluating these
dimensions jointly: they are often limited to English, mostly focus on one
single modality at a time, rely on short-form contexts, or lack human
annotations -- hindering comprehensive assessment of model performance across
languages, modalities, and task complexity. To address these gaps, we introduce
MCIF (Multimodal Crosslingual Instruction Following), the first multilingual
human-annotated benchmark based on scientific talks that is designed to
evaluate instruction-following in crosslingual, multimodal settings over both
short- and long-form inputs. MCIF spans three core modalities -- speech,
vision, and text -- and four diverse languages (English, German, Italian, and
Chinese), enabling a comprehensive evaluation of MLLMs' abilities to interpret
instructions across languages and combine them with multimodal contextual
information. MCIF is released under a CC-BY 4.0 license to encourage open
research and progress in MLLMs development.

</details>


### [9] [RoD-TAL: A Benchmark for Answering Questions in Romanian Driving License Exams](https://arxiv.org/abs/2507.19666)
*Andrei Vlad Man,Răzvan-Alexandru Smădu,Cristian-George Craciun,Dumitru-Clementin Cercel,Florin Pop,Mihaela-Claudia Cercel*

Main category: cs.CL

> 研究了LLMs和VLMs对罗马尼亚交通法的理解与推理能力，通过RoD-TAL数据集测试了多个模型。发现领域特定微调和特定提示可以优化性能，但在视觉推理上挑战仍存。

<details>
  <summary>Details</summary>

**Motivation:** 随着AI与法律系统的交汇，尤其在像罗马尼亚语这样的资源不足的语言中，需要支持法律教育的工具。因此，评估LLMs和VLMs在理解与推理罗马尼亚交通法方面的能力变得十分重要。

**Method:** 我们通过引入RoD-TAL这一多模态数据集，包含了罗马尼亚驾驶测试中的文本及图像问题、标注的法律参考和人类解释，来评估LLMs和VLMs对罗马尼亚交通法的理解与推理能力。我们实现了和评估了包含信息检索、问答、视觉信息检索和视觉问答等任务的检索增强生成（RAG）流水线、密集检索器和推理优化模型。

**Result:** 实验表明，针对特定领域的微调可以显著提高检索性能，而链式思路的提示和专门的推理模型能提高问答准确率，超过了通过驾驶考试所需的最低分数。然而，视觉推理仍然是一个挑战。

**Conclusion:** LLMs和VLMs在特定领域的法律教育中有显著的应用潜力，特别是在问答任务中，但它们在处理视觉推理问题上仍具有局限性，显示了这些模型在应用中的潜在和限制。

**Abstract:** The intersection of AI and legal systems presents a growing need for tools
that support legal education, particularly in under-resourced languages such as
Romanian. In this work, we aim to evaluate the capabilities of Large Language
Models (LLMs) and Vision-Language Models (VLMs) in understanding and reasoning
about Romanian driving law through textual and visual question-answering tasks.
To facilitate this, we introduce RoD-TAL, a novel multimodal dataset comprising
Romanian driving test questions, text-based and image-based, alongside
annotated legal references and human explanations. We implement and assess
retrieval-augmented generation (RAG) pipelines, dense retrievers, and
reasoning-optimized models across tasks including Information Retrieval (IR),
Question Answering (QA), Visual IR, and Visual QA. Our experiments demonstrate
that domain-specific fine-tuning significantly enhances retrieval performance.
At the same time, chain-of-thought prompting and specialized reasoning models
improve QA accuracy, surpassing the minimum grades required to pass driving
exams. However, visual reasoning remains challenging, highlighting the
potential and the limitations of applying LLMs and VLMs to legal education.

</details>


### [10] [Towards Inclusive NLP: Assessing Compressed Multilingual Transformers across Diverse Language Benchmarks](https://arxiv.org/abs/2507.19699)
*Maitha Alshehhi,Ahmed Sharshar,Mohsen Guizani*

Main category: cs.CL

> 研究比较了多种语言大语言模型在资源丰富的语言和资源匮乏的语言中的表现，发现多语言模型性能更优，量化有效但剪枝可能减少性能。

<details>
  <summary>Details</summary>

**Motivation:** 尽管大型语言模型在资源丰富的语言上取得了显著成功，但它们在资源匮乏的语言环境中的表现尚不明确。本研究旨在探索这一领域。

**Method:** 该研究通过评估多语言和单语言大语言模型（LLMs）在阿拉伯语、英语和印度语环境中的表现，重点考察模型压缩策略（如剪枝和量化）的影响。

**Result:** 研究发现，多语言模型在所有语言环境中均优于其特定语言的版本，表明跨语言迁移具有显著优势。量化技术（4位和8位）能有效保持模型准确率并提高效率，但激进的剪枝会严重影响大型模型的性能。

**Conclusion:** 研究指出了构建可扩展和公平的多语言NLP解决方案的关键策略，并强调了在资源匮乏环境下解决幻觉和泛化错误问题的必要性。

**Abstract:** Although LLMs have attained significant success in high-resource languages,
their capacity in low-resource linguistic environments like Kannada and Arabic
is not yet fully understood. This work benchmarking the performance of
multilingual and monolingual Large Language Models (LLMs) across Arabic,
English, and Indic languages, with particular emphasis on the effects of model
compression strategies such as pruning and quantization. Findings shows
significant performance differences driven by linguistic diversity and resource
availability on SOTA LLMS as BLOOMZ, AceGPT, Jais, LLaMA-2, XGLM, and AraGPT2.
We find that multilingual versions of the model outperform their
language-specific counterparts across the board, indicating substantial
cross-lingual transfer benefits. Quantization (4-bit and 8-bit) is effective in
maintaining model accuracy while promoting efficiency, but aggressive pruning
significantly compromises performance, especially in bigger models. Our
findings pinpoint key strategies to construct scalable and fair multilingual
NLP solutions and underscore the need for interventions to address
hallucination and generalization errors in the low-resource setting.

</details>


### [11] [Ta-G-T: Subjectivity Capture in Table to Text Generation via RDF Graphs](https://arxiv.org/abs/2507.19710)
*Ronak Upasham,Tathagata Dey,Pushpak Bhattacharyya*

Main category: cs.CL

> 本文提出了一种新的Table-to-Text生成管道，通过RDF三元组中间表示来生成既包含客观又包含主观解释的文本，并通过小规模微调的T5模型实现了与大规模语言模型相当的性能，甚至在某些指标上优于它们。

<details>
  <summary>Details</summary>

**Motivation:** 现有的Table-to-Text生成方法主要集中于提供表格数据的客观描述，对于需要主观解释的文本生成研究不足。因此，该研究旨在填补这一空白，提出一种结合中间表示的方法来增强生成文本的主观性和客观性。

**Method:** Structure

**Result:** {
  "tldr": "本文提出了一种新的Table-to-Text生成管道，通过RDF三元组中间表示来生成既包含客观又包含主观解释的文本，并通过小规模微调的T5模型实现了与大规模语言模型相当的性能，甚至在某些指标上优于它们。",
  "motivation": "现有的Table-to-Text生成方法主要集中于提供表格数据的客观描述，对于需要主观解释的文本生成研究不足。因此，该研究旨在填补这一空白，提出一种结合中间表示的方法来增强生成文本的主观性和客观性。",
  "method": "文中的三阶段管道方法包括：1) 提取资源描述框架（RDF）三元组；2) 将文本聚合为连贯的故事；3) 向生成的文本中加入主观性解释，以丰富其内容。文中使用的模型是小规模微调的T5模型。",
  "result": "通过定量和定性分析，验证了方法在保持事实准确性的同时，能够在主观解释上进行有效平衡，并且证明了此方法在性能上可以与大规模的语言模型相匹敌，甚至在某些指标上超越它们。",
  "conclusion": "这项工作是首次提出的将中间表示集成到Table-to-Text生成中的结构化管道，其创新在于通过小模型实现高性能的同时，还能生成包含主观性解释的文本，是对现有T2T生成流程的一个突破。")

**Conclusion:** 这项工作是首次提出的将中间表示集成到Table-to-Text生成中的结构化管道，其创新在于通过小模型实现高性能的同时，还能生成包含主观性解释的文本，是对现有T2T生成流程的一个突破。

**Abstract:** In Table-to-Text (T2T) generation, existing approaches predominantly focus on
providing objective descriptions of tabular data. However, generating text that
incorporates subjectivity, where subjectivity refers to interpretations beyond
raw numerical data, remains underexplored. To address this, we introduce a
novel pipeline that leverages intermediate representations to generate both
objective and subjective text from tables. Our three-stage pipeline consists
of: 1) extraction of Resource Description Framework (RDF) triples, 2)
aggregation of text into coherent narratives, and 3) infusion of subjectivity
to enrich the generated text. By incorporating RDFs, our approach enhances
factual accuracy while maintaining interpretability. Unlike large language
models (LLMs) such as GPT-3.5, Mistral-7B, and Llama-2, our pipeline employs
smaller, fine-tuned T5 models while achieving comparable performance to GPT-3.5
and outperforming Mistral-7B and Llama-2 in several metrics. We evaluate our
approach through quantitative and qualitative analyses, demonstrating its
effectiveness in balancing factual accuracy with subjective interpretation. To
the best of our knowledge, this is the first work to propose a structured
pipeline for T2T generation that integrates intermediate representations to
enhance both factual correctness and subjectivity.

</details>


### [12] [Basic Reading Distillation](https://arxiv.org/abs/2507.19741)
*Zhi Zhou,Sirui Miao,Xiangyu Duan,Hao Yang,Min Zhang*

Main category: cs.CL

> 本文提出了基础阅读蒸馏（BRD）的方法，用于训练小模型模仿大型模型的基础阅读行为，以增强其在各种任务上的表现。

<details>
  <summary>Details</summary>

**Motivation:** 大型语言模型（LLMs）在自然语言处理领域表现出色，但计算资源需求高，限制了实际部署。通过知识蒸馏或任务蒸馏技术将大型模型的知识转移到小模型上，但这些方法忽略了对小模型进行通用文本基础阅读教育的需求。

**Method:** 提出基础阅读蒸馏（BRD）方法，通过模仿大模型在无关下游任务的一般文本上的基础阅读行为，如命名实体识别、问题提出与回答，对小模型进行训练。

**Result:** 在语言推理基准测试和BIG-bench任务上，接受BRD训练的小模型性能可以优于或接近20倍大的LLMs。

**Conclusion:** BRD能够有效地影响小模型的概率分布，并且与知识蒸馏或任务蒸馏具有正交性，即BRD可以与这两种方法结合使用以进一步提高小模型的性能。

**Abstract:** Large language models (LLMs) have demonstrated remarkable abilities in
various natural language processing areas, but they demand high computation
resources which limits their deployment in real-world. Distillation is one
technique to solve this problem through either knowledge distillation or task
distillation. Both distillation approaches train small models to imitate
specific features of LLMs, but they all neglect basic reading education for
small models on generic texts that are \emph{unrelated} to downstream tasks. In
this paper, we propose basic reading distillation (BRD) which educates a small
model to imitate LLMs basic reading behaviors, such as named entity
recognition, question raising and answering, on each sentence. After such basic
education, we apply the small model on various tasks including language
inference benchmarks and BIG-bench tasks. It shows that the small model can
outperform or perform comparable to over 20x bigger LLMs. Analysis reveals that
BRD effectively influences the probability distribution of the small model, and
has orthogonality to either knowledge distillation or task distillation.

</details>


### [13] [JT-Math: A Multi-Stage Framework for Advanced Mathematical Reasoning in Large Language Models](https://arxiv.org/abs/2507.19748)
*Yifan Hao,Fangning Chao,Yaqian Hao,Zhaojun Cui,Huan Bai,Haiyu Zhang,Yankai Liu,Chao Deng,Junlan Feng*

Main category: cs.CL

> 介绍了JT-Math-8B系列开放源代码模型，用于提高解决复杂数学问题的能力，在同类模型中表现优异。

<details>
  <summary>Details</summary>

**Motivation:** 解决当前顶尖模型在处理需要深入概念理解和复杂的多步骤推理的复杂问题时表现不佳的挑战。

**Method:** 通过一个系统化的多阶段优化框架，介绍了一个开放源代码模型系列JT-Math-8B，包括基础模型、指令模型和思考模型。预训练语料库是一个高质量的210B-token数据集，通过专门的数据管道，利用基于模型的验证以确保数据的质量和多样性。指令模型通过监督微调和基于GRPO的强化学习方法进行优化，以获得直接、简洁的答案。思考模型通过结合长链条思维方法和新型多阶段强化学习课程进行训练，以解决复杂问题，逐步增加任务难度和上下文长度高达32K token。

**Result:** JT-Math-8B在同类开放源代码模型中取得了最先进的成果，超越了如OpenAI的O1-mini和GPT-4o等著名模型，并在竞赛级别的数学上表现出色。

**Conclusion:** JT-Math-8B证明了其在解决问题能力方面的优越性，特别是在处理复杂数学逻辑方面，为改进大型语言模型的数学推理能力提供了新的思路。

**Abstract:** Mathematical reasoning is a cornerstone of artificial general intelligence
and a primary benchmark for evaluating the capabilities of Large Language
Models (LLMs). While state-of-the-art models show promise, they often falter
when faced with complex problems that demand deep conceptual understanding and
intricate, multi-step deliberation. To address this challenge, we introduce
JT-Math-8B, a series of open-source models comprising base, instruct, and
thinking versions, built upon a systematic, multi-stage optimization framework.
Our pre-training corpus is a high-quality, 210B-token dataset curated through a
dedicated data pipeline that uses model-based validation to ensure quality and
diversity. The Instruct Model is optimized for direct, concise answers through
Supervised Fine-Tuning (SFT) and a GRPO-based reinforcement learning (RL)
method. The Thinking Model is trained for complex problem-solving using a Long
Chain-of-Thought (Long CoT) approach, combining SFT with a novel, multi-stage
RL curriculum that progressively increases task difficulty and context length
up to 32K tokens. JT-Math-8B achieves state-of-the-art results among
open-source models of similar size, surpassing prominent models like OpenAI's
O1-mini and GPT-4o , and demonstrating superior performance on
competition-level mathematics.

</details>


### [14] [Are You There God? Lightweight Narrative Annotation of Christian Fiction with LMs](https://arxiv.org/abs/2507.19756)
*Rebecca M. M. Hicke,Brian Haggard,Mia Ferrante,Rayhan Khanna,David Mimno*

Main category: cs.CL

> 研究通过计算工具分析了美国福音派文化下的基督教小说，聚焦神迹主题，揭示了《创世记》系列与广泛基督教小说以及男性和女性作者作品间的差异。

<details>
  <summary>Details</summary>

**Motivation:** 尽管美国福音派政治活动备受瞩目，但其文化与文学方面，尤其是基督教小说，却较少受到学术界的关注。本研究旨在通过计算方法对这一文学类别进行深入探讨。

**Method:** 研究团队利用人类注释者开发定义和编码手册，并通过调整使其适用于轻量级语言模型，用以分析《创世记》系列和其他基督教小说中的神迹描述。

**Result:** 该研究主要关注美国福音派文化中的基督教小说，并运用计算工具对其进行了广泛的主题概述及神迹描绘的具体分析。研究团队首先开发了用于人类注释者的定义和编码手册，并将其转换为适合由轻量级语言模型使用的指令。结果显示，与广受欢迎的《创世记》系列相比，基督教小说中存在显著且有意义的差异。此外，还发现男性和女性作者的作品之间存在差异。

**Conclusion:** 研究结果表明，使用计算工具可以很好地分析基督教小说的不同方面，揭示不同类型作品之间以及不同性别作者作品之间的显著差异。

**Abstract:** In addition to its more widely studied political activities, the American
Evangelical movement has a well-developed but less externally visible cultural
and literary side. Christian Fiction, however, has been little studied, and
what scholarly attention there is has focused on the explosively popular Left
Behind series. In this work, we use computational tools to provide both a broad
topical overview of Christian Fiction as a genre and a more directed
exploration of how its authors depict divine acts. Working with human
annotators we first developed definitions and a codebook for "acts of God." We
then adapted those instructions designed for human annotators for use by a
recent, lightweight LM with the assistance of a much larger model. The
laptop-scale LM is capable of matching human annotations, even when the task is
subtle and challenging. Using these annotations, we show that significant and
meaningful differences exist between the Left Behind books and Christian
Fiction more broadly and between books by male and female authors.

</details>


### [15] [UloRL:An Ultra-Long Output Reinforcement Learning Approach for Advancing Large Language Models' Reasoning Abilities](https://arxiv.org/abs/2507.19766)
*Dong Du,Shulin Liu,Tao Yang,Shaohua Chen,Yang Li*

Main category: cs.CL

> 提出了一种超长输出强化学习方法UloRL，通过分割输出和动态屏蔽技术提高大语言模型的训练效率和推理能力。

<details>
  <summary>Details</summary>

**Motivation:** 传统的强化学习框架在处理超长输出时面临效率问题，由于长尾序列分布和训练过程中的熵崩溃。为了应对这些挑战，提高大语言模型的推理能力。

**Method:** 将超长输出解码分割成短片段，通过减少长尾样本导致的延迟来实现高效训练。同时，引入动态屏蔽掌握良好的正令牌（MPTs）以防止熵崩溃。

**Result:** 实验结果表明，分段回放的RL实现了2.06倍的训练速度提升，而使用128k-token输出的RL培训将Qwen3-30B-A3B模型在AIME2025上的表现从70.9%提高到85.1%，在BeyondAIME上的表现从50.7%提高到61.9%。

**Conclusion:** 研究结果强调了该方法在提高超长序列生成的大型语言模型推理能力方面的潜力。研究团队会公开代码和模型供社区进一步使用。

**Abstract:** Recent advances in large language models (LLMs) have highlighted the
potential of reinforcement learning with verifiable rewards (RLVR) to enhance
reasoning capabilities through extended output sequences. However, traditional
RL frameworks face inefficiencies when handling ultra-long outputs due to
long-tail sequence distributions and entropy collapse during training. To
address these challenges, we propose an Ultra-Long Output Reinforcement
Learning (UloRL) approach for advancing large language models' reasoning
abilities. Specifically, we divide ultra long output decoding into short
segments, enabling efficient training by mitigating delays caused by long-tail
samples. Additionally, we introduce dynamic masking of well-Mastered Positive
Tokens (MPTs) to prevent entropy collapse. Experimental results demonstrate the
effectiveness of our approach. On the Qwen3-30B-A3B model, RL with segment
rollout achieved 2.06x increase in training speed, while RL training with
128k-token outputs improves the model's performance on AIME2025 from 70.9\% to
85.1\% and on BeyondAIME from 50.7\% to 61.9\%, even surpassing Qwen3-235B-A22B
with remarkable gains. These findings underscore the potential of our methods
to advance the reasoning capabilities of LLMs with ultra-long sequence
generation. We will release our code and model for further use by the
community.

</details>


### [16] [Flora: Effortless Context Construction to Arbitrary Length and Scale](https://arxiv.org/abs/2507.19786)
*Tianxiang Chen,Zhentao Tan,Xiaofan Bo,Yue Wu,Tao Gong,Qi Chu,Jieping Ye,Nenghai Yu*

Main category: cs.CL

> 本文介绍的Flora通过一种无需大型语言模型或人类介入的策略，构建了多样且任意长度的长上下文，有效提升了大型语言模型在长上下文下的性能，同时保持良好的短期上下文性能。

<details>
  <summary>Details</summary>

**Motivation:** 由于长文本的稀有性，高计算需求以及遗忘短期能力的问题，处理长上下文对于大型语言模型来说是一个挑战。现有的方法通常需要大型语言模型或人为干预，成本高昂且在长度和多样性上有限，同时现有长上下文大型语言模型在短期上下文性能上的下降仍然相当显著。

**Method:** Flora采用了一种简单（无需人类或大型语言模型介入）的长上下文构建策略。Flora通过任意组合基于类别的短期指令，指示大型语言模型根据长上下文元指令生成响应，从而显著提升大型语言模型的长上下文性能。这使Flora能够生成任意长度和丰富多样性上下文，同时仅轻微影响短期上下文性能。

**Result:** 实验结果证明，经Flora增强后的Llama3-8B-Instruct和QwQ-32B在三个长上下文基准测试中表现出色，同时在短期上下文任务上保持强大性能。

**Conclusion:** Flora展示了其在构建长上下文方面的优势，提供了丰富多样且任意长度的上下文，并且在不显著影响短期性能的情况下提升了大型语言模型的长上下文能力。

**Abstract:** Effectively handling long contexts is challenging for Large Language Models
(LLMs) due to the rarity of long texts, high computational demands, and
substantial forgetting of short-context abilities. Recent approaches have
attempted to construct long contexts for instruction tuning, but these methods
often require LLMs or human interventions, which are both costly and limited in
length and diversity. Also, the drop in short-context performances of present
long-context LLMs remains significant. In this paper, we introduce Flora, an
effortless (human/LLM-free) long-context construction strategy. Flora can
markedly enhance the long-context performance of LLMs by arbitrarily assembling
short instructions based on categories and instructing LLMs to generate
responses based on long-context meta-instructions. This enables Flora to
produce contexts of arbitrary length and scale with rich diversity, while only
slightly compromising short-context performance. Experiments on
Llama3-8B-Instruct and QwQ-32B show that LLMs enhanced by Flora excel in three
long-context benchmarks while maintaining strong performances in short-context
tasks. Our data-construction code is available at
\href{https://github.com/txchen-USTC/Flora}{https://github.com/txchen-USTC/Flora}.

</details>


### [17] [HCAttention: Extreme KV Cache Compression via Heterogeneous Attention Computing for LLMs](https://arxiv.org/abs/2507.19823)
*Dongquan Yang,Yifan Yang,Xiaotian Yu,Xianbiao Qi,Rong Xiao*

Main category: cs.CL

> 本文提出HCAttention方法，该方法结合键量化、值卸载和动态KV缓存淘汰技术，能够在极端内存限制下进行高效的推理，显著减少了KV缓存的内存占用，并且在长输入处理上达到了最先进的水平。

<details>
  <summary>Details</summary>

**Motivation:** 现有的KV缓存压缩方法在内存减少超过85%时性能会显著下降，并且在近似注意力计算中利用GPU-CPU协作的策略还处于探索阶段。

**Method:** HCAttention, 一种异构注意力计算框架，结合了键量化、值卸载和动态KV缓存淘汰，以在极端内存约束下实现高效的推理。该方法与现有的transformer架构兼容，且不需要模型微调。

**Result:** 实验结果表明，该方法在LongBench基准上能够保留全注意力模型的准确性，同时将KV缓存内存占用减少到原尺寸的25%。当缓存占用仅为12.5%时该方法仍然具有竞争力，并且在KV缓存压缩中达到了新的最先进水平。

**Conclusion:** HCAttention首次将Llama-3-8B模型扩展到单个80GB内存的A100 GPU上处理400万个token。

**Abstract:** Processing long-context inputs with large language models presents a
significant challenge due to the enormous memory requirements of the Key-Value
(KV) cache during inference. Existing KV cache compression methods exhibit
noticeable performance degradation when memory is reduced by more than 85%.
Additionally, strategies that leverage GPU-CPU collaboration for approximate
attention remain underexplored in this setting. We propose HCAttention, a
heterogeneous attention computation framework that integrates key quantization,
value offloading, and dynamic KV eviction to enable efficient inference under
extreme memory constraints. The method is compatible with existing transformer
architectures and does not require model fine-tuning. Experimental results on
the LongBench benchmark demonstrate that our approach preserves the accuracy of
full-attention model while shrinking the KV cache memory footprint to 25% of
its original size. Remarkably, it stays competitive with only 12.5% of the
cache, setting a new state-of-the-art in LLM KV cache compression. To the best
of our knowledge, HCAttention is the first to extend the Llama-3-8B model to
process 4 million tokens on a single A100 GPU with 80GB memory.

</details>


### [18] [DRIVE: Disfluency-Rich Synthetic Dialog Data Generation Framework for Intelligent Vehicle Environments](https://arxiv.org/abs/2507.19867)
*Anshul Chavda,M Jagadeesh,Chintalapalli Raja Kullayappa,B Jayaprakash,Medchalimi Sruthi,Pushpak Bhattacharyya*

Main category: cs.CL

> DiscoDrive 是一个包含 3500 个多轮对话的合成语料库，专注于汽车领域的对话，包含真实对话中的自发性不流利特征，提高了训练和数据增强的效果，提升了自然性和连贯性。

<details>
  <summary>Details</summary>

**Motivation:** 为解决现有数据集未能捕捉到驾驶员与AI对话中的真实不流利特征，引入了DiscoDrive，以改善基于对话的AI在车载场景中的表现。

**Method:** 采用两阶段、提示驱动的管道，在合成过程中动态集成不流利特征，生成了DiscoDrive语料库。

**Result:** DiscoDrive作为训练资源及数据增强资源在低资源场景下的表现优越，提升了模型的多个评价指标，并且在人类评估中，表现出更高的自然性和连贯性。

**Conclusion:** DiscoDrive填补了现有资源中的空白，为训练和增强车载对话AI提供了一个多功能的语料库，能够处理更真实的车内对话情境。

**Abstract:** In-car conversational AI is becoming increasingly critical as autonomous
vehicles and smart assistants gain widespread adoption. Yet, existing datasets
fail to capture the spontaneous disfluencies such as hesitations, false starts,
repetitions, and self-corrections that characterize real driver-AI dialogs. To
address this, we introduce DiscoDrive, a synthetic corpus of 3500 multi-turn
dialogs across seven automotive domains, generated using a two-stage,
prompt-driven pipeline that dynamically integrates disfluencies during
synthesis. We show that DiscoDrive is effective both as a training resource,
enabling DialoGPT-Medium and T5-Base to match or exceed KVRET-trained models on
the MultiWOZ 2.2 and Schema-Guided Dialogue (SGD) relevant test sets (BLEU-4
improvements of 0.26 to 0.61; METEOR +2.10; ROUGE-L +3.48; BERTScore F1
improvements of 1.35 to 3.48), and as a data augmentation resource in
low-resource scenarios, delivering additional gains of up to BLEU-4 +0.38,
METEOR +1.95, ROUGE-L +2.87, and BERTScore F1 +4.00 when combined with 10
percent of KVRET. Human evaluations further confirm that dialogs sampled from
DiscoDrive are rated higher than KVRET's human-collected dialogs in naturalness
(3.8 vs 3.6) and coherence (4.1 vs 4.0), and are perceived as more
context-appropriate than leading post-hoc methods (such as LARD), without
compromising clarity. DiscoDrive fills a critical gap in existing resources and
serves as a versatile corpus for both training and augmenting conversational
AI, enabling robust handling of real-world, disfluent in-car interactions.

</details>


### [19] [The Polish Vocabulary Size Test: A Novel Adaptive Test for Receptive Vocabulary Assessment](https://arxiv.org/abs/2507.19869)
*Danil Fokin,Monika Płużyczka,Grigory Golovin*

Main category: cs.CL

> 开发并验证了一种基于计算机自适应测试技术的波兰语词汇量评估工具-PVST。

<details>
  <summary>Details</summary>

**Motivation:** 当前缺乏一种能够准确、高效地评估波兰语词汇量的工具，特别是在母语者和非母语者之间进行比较时。因此，研究开发了波兰词汇量测试（PVST）以填补这一空白。

**Method:** 研究采用了项目反应理论和计算机自适应测试技术开发测试工具，并进行了一项包含1475名参与者的初步研究来验证测试的有效性。

**Result:** 研究开发了波兰词汇量测试（PVST），一项用于评估波兰语母语者和非母语者的接受词汇量的新工具。该测试基于项目反应理论和计算机自适应测试技术，可以根据每个测试者的水平动态调整难度，既保证了测试的准确性，又缩短了测试时间。通过一项包含1475名参与者的初步研究验证了该测试的有效性。研究表明，母语者相比非母语者拥有更大的词汇量，并且对于母语者而言，词汇量随年龄增长而增加。PVST在线测试地址为myvocab.info/pl。

**Conclusion:** 该研究成功开发了一个能够高效且准确评估波兰语词汇量的工具，对于语言教育和研究有重要应用价值。

**Abstract:** We present the Polish Vocabulary Size Test (PVST), a novel tool for assessing
the receptive vocabulary size of both native and non-native Polish speakers.
Based on Item Response Theory and Computerized Adaptive Testing, PVST
dynamically adjusts to each test-taker's proficiency level, ensuring high
accuracy while keeping the test duration short. To validate the test, a pilot
study was conducted with 1.475 participants. Native Polish speakers
demonstrated significantly larger vocabularies compared to non-native speakers.
For native speakers, vocabulary size showed a strong positive correlation with
age. The PVST is available online at myvocab.info/pl.

</details>


### [20] [Zero-shot Performance of Generative AI in Brazilian Portuguese Medical Exam](https://arxiv.org/abs/2507.19885)
*Cesar Augusto Madid Truyts,Amanda Gomes Rabelo,Gabriel Mesquita de Souza,Daniel Scaldaferri Lages,Adriano Jose Pereira,Uri Adrian Prync Flato,Eduardo Pontes dos Reis,Joaquim Edson Vieira,Paulo Sergio Panse Silveira,Edson Amaro Junior*

Main category: cs.CL

> Error

<details>
  <summary>Details</summary>

**Motivation:** Error

**Method:** Error

**Result:** Error

**Conclusion:** Error

**Abstract:** Artificial intelligence (AI) has shown the potential to revolutionize
healthcare by improving diagnostic accuracy, optimizing workflows, and
personalizing treatment plans. Large Language Models (LLMs) and Multimodal
Large Language Models (MLLMs) have achieved notable advancements in natural
language processing and medical applications. However, the evaluation of these
models has focused predominantly on the English language, leading to potential
biases in their performance across different languages.
  This study investigates the capability of six LLMs (GPT-4.0 Turbo,
LLaMA-3-8B, LLaMA-3-70B, Mixtral 8x7B Instruct, Titan Text G1-Express, and
Command R+) and four MLLMs (Claude-3.5-Sonnet, Claude-3-Opus, Claude-3-Sonnet,
and Claude-3-Haiku) to answer questions written in Brazilian spoken portuguese
from the medical residency entrance exam of the Hospital das Cl\'inicas da
Faculdade de Medicina da Universidade de S\~ao Paulo (HCFMUSP) - the largest
health complex in South America. The performance of the models was benchmarked
against human candidates, analyzing accuracy, processing time, and coherence of
the generated explanations.
  The results show that while some models, particularly Claude-3.5-Sonnet and
Claude-3-Opus, achieved accuracy levels comparable to human candidates,
performance gaps persist, particularly in multimodal questions requiring image
interpretation. Furthermore, the study highlights language disparities,
emphasizing the need for further fine-tuning and data set augmentation for
non-English medical AI applications.
  Our findings reinforce the importance of evaluating generative AI in various
linguistic and clinical settings to ensure a fair and reliable deployment in
healthcare. Future research should explore improved training methodologies,
improved multimodal reasoning, and real-world clinical integration of AI-driven
medical assistance.

</details>


### [21] [A Gold Standard Dataset and Evaluation Framework for Depression Detection and Explanation in Social Media using LLMs](https://arxiv.org/abs/2507.19899)
*Prajval Bolegave,Pushpak Bhattacharya*

Main category: cs.CL

> 研究开发了一个高质量的数据集用于评估语言模型在抑郁症症状识别中的表现，并采用定制化的提示策略对比了几种先进模型的表现。结果表明，先进模型在心理健康任务中表现出显著差异，强调了人类专业知识指导模型行为的重要性。

<details>
  <summary>Details</summary>

**Motivation:** 本研究旨在通过分析社交媒体帖子实现抑郁症的早期检测，以便及时提供心理健康干预。

**Method:** 构建了一个高质量、专家标注的数据集，包含1,017条社交媒体帖子，这些帖子被标记了抑郁片段并映射到12个抑郁症状类别。该研究评估了先进语言模型（LLM）生成的自然语言解释的可靠性和质量，采用了零样本和少样本提示策略。

**Result:** 通过综合实证分析，不同模型在心理健康的解释任务中的表现存在显著差异。

**Conclusion:** 研究结果强调了人类专业知识在指导LLM行为中的价值，并为构建更安全、透明的人工智能系统以促进心理健康提供了一个步骤。

**Abstract:** Early detection of depression from online social media posts holds promise
for providing timely mental health interventions. In this work, we present a
high-quality, expert-annotated dataset of 1,017 social media posts labeled with
depressive spans and mapped to 12 depression symptom categories. Unlike prior
datasets that primarily offer coarse post-level labels
\cite{cohan-etal-2018-smhd}, our dataset enables fine-grained evaluation of
both model predictions and generated explanations.
  We develop an evaluation framework that leverages this clinically grounded
dataset to assess the faithfulness and quality of natural language explanations
generated by large language models (LLMs). Through carefully designed prompting
strategies, including zero-shot and few-shot approaches with domain-adapted
examples, we evaluate state-of-the-art proprietary LLMs including GPT-4.1,
Gemini 2.5 Pro, and Claude 3.7 Sonnet.
  Our comprehensive empirical analysis reveals significant differences in how
these models perform on clinical explanation tasks, with zero-shot and few-shot
prompting. Our findings underscore the value of human expertise in guiding LLM
behavior and offer a step toward safer, more transparent AI systems for
psychological well-being.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [22] [Tuning adaptive gamma correction (TAGC) for enhancing images in low ligh](https://arxiv.org/abs/2507.19574)
*Ghufran Abualhail Alhamzawi,Ali Saeed Alfoudi,Ali Hakem Alsaeedi,Suha Mohammed Hadi,Amjed Abbas Ahmed,Md. Riad Hassan,Nurhizam Safie Mohd Satar,Waeel Yahya Yasseen*

Main category: cs.CV

> The paper presents a model named TAGC for enhancing images in low-light conditions, which effectively improves low-light images while keeping details, contrast, and color natural.

<details>
  <summary>Details</summary>

**Motivation:** The motivation arises from addressing the challenges of low-light images such as low contrast, noise, and blurred details which are caused by insufficient illumination. The need for an effective method to enhance the quality of these images is highlighted.

**Method:** The method involves analyzing the color luminance of the low-light image and calculating the average color to determine the adaptive gamma coefficient. The gamma value is automatically and adaptively calculated according to different levels of illumination suitable for the image, requiring no human intervention or manual adjustment.

**Result:** The model has been found to be effective in enhancing low-light images, maintaining natural visual quality, and is suitable for various applications.

**Conclusion:** The tuning adaptive gamma correction (TAGC) model is a more efficient solution for processing low-light images in applications such as night surveillance, improving medical images, and photography in low-light settings.

**Abstract:** Enhancing images in low-light conditions is an important challenge in
computer vision. Insufficient illumination negatively affects the quality of
images, resulting in low contrast, intensive noise, and blurred details. This
paper presents a model for enhancing low-light images called tuning adaptive
gamma correction (TAGC). The model is based on analyzing the color luminance of
the low-light image and calculating the average color to determine the adaptive
gamma coefficient. The gamma value is calculated automatically and adaptively
at different illumination levels suitable for the image without human
intervention or manual adjustment. Based on qualitative and quantitative
evaluation, tuning adaptive gamma correction model has effectively improved
low-light images while maintaining details, natural contrast, and correct color
distribution. It also provides natural visual quality. It can be considered a
more efficient solution for processing low-light images in multiple
applications such as night surveillance, improving the quality of medical
images, and photography in low-light environments.

</details>


### [23] [Is Exchangeability better than I.I.D to handle Data Distribution Shifts while Pooling Data for Data-scarce Medical image segmentation?](https://arxiv.org/abs/2507.19575)
*Ayush Roy,Samin Enam,Jun Xia,Vishnu Suresh Lokhande,Won Hwa Kim*

Main category: cs.CV

> This paper introduces a method to improve feature representations in deep networks for medical image segmentation, achieving state-of-the-art performance and overcoming distributional shifts associated with data pooling or addition.

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to address the 'Data Addition Dilemma' in medical imaging, where pooling or adding more data can lead to distributional shifts that decrease model performance. This is particularly relevant as data scarcity poses a significant challenge for deep learning models in this domain.

**Method:** The paper proposes a method inspired by causal frameworks to control foreground-background feature discrepancies across all layers of deep networks, which improves feature representation crucial for data addition scenarios.

**Result:** The method achieves state-of-the-art performance in medical image segmentation tasks on histopathology and ultrasound images across five datasets, showcasing more refined and accurate segmentation maps than prominent baseline models.

**Conclusion:** The study concludes that their proposed method effectively mitigates the negative impact of distribution shifts caused by data pooling or addition by improving the foreground-background feature representations in deep networks, thereby enhancing segmentation performance in medical imaging tasks.

**Abstract:** Data scarcity is a major challenge in medical imaging, particularly for deep
learning models. While data pooling (combining datasets from multiple sources)
and data addition (adding more data from a new dataset) have been shown to
enhance model performance, they are not without complications. Specifically,
increasing the size of the training dataset through pooling or addition can
induce distributional shifts, negatively affecting downstream model
performance, a phenomenon known as the "Data Addition Dilemma". While the
traditional i.i.d. assumption may not hold in multi-source contexts, assuming
exchangeability across datasets provides a more practical framework for data
pooling. In this work, we investigate medical image segmentation under these
conditions, drawing insights from causal frameworks to propose a method for
controlling foreground-background feature discrepancies across all layers of
deep networks. This approach improves feature representations, which are
crucial in data-addition scenarios. Our method achieves state-of-the-art
segmentation performance on histopathology and ultrasound images across five
datasets, including a novel ultrasound dataset that we have curated and
contributed. Qualitative results demonstrate more refined and accurate
segmentation maps compared to prominent baselines across three model
architectures. The code will be available on Github.

</details>


### [24] [T-MPEDNet: Unveiling the Synergy of Transformer-aware Multiscale Progressive Encoder-Decoder Network with Feature Recalibration for Tumor and Liver Segmentation](https://arxiv.org/abs/2507.19590)
*Chandravardhan Singh Raghaw,Jasmer Singh Sanjotra,Mohammad Zia Ur Rehman,Shubhi Bansal,Shahid Shafi Dar,Nagendra Kumar*

Main category: cs.CV

> 研究提出了一种新的网络T-MPEDNet，用于提高CT扫描中肝和肿瘤的自动分割精度，并在两个基准数据集上验证了其有效性。

<details>
  <summary>Details</summary>

**Motivation:** 肝和肿瘤的自动分割在CT扫描中面临巨大挑战，如肿瘤的异质性和肝脏的视觉特征多样性。此研究旨在解决这些挑战，提供高效的解决方案。

**Method:** 提出了一种名为T-MPEDNet的新网络，用于CT扫描中肝和肿瘤的自动分割。该网络采用了具有跳过连接的渐进式编解码器结构，融合了Transformer启发的动态注意机制和多尺度特征利用。此外，还应用了形态边界细化以提高精确度。

**Result:** 实验结果表明，T-MPEDNet在LiTS和3DIRCADb两个公共基准数据集上，分别达到了97.6%和89.1%（肝和肿瘤分割）的Dice相似系数，在3DIRCADb数据集上分别达到了98.3%和83.3%。

**Conclusion:** T-MPEDNet在CT扫描中对于肝和肿瘤的自动分割展示出了优越性和可靠性。

**Abstract:** Precise and automated segmentation of the liver and its tumor within CT scans
plays a pivotal role in swift diagnosis and the development of optimal
treatment plans for individuals with liver diseases and malignancies. However,
automated liver and tumor segmentation faces significant hurdles arising from
the inherent heterogeneity of tumors and the diverse visual characteristics of
livers across a broad spectrum of patients. Aiming to address these challenges,
we present a novel Transformer-aware Multiscale Progressive Encoder-Decoder
Network (T-MPEDNet) for automated segmentation of tumor and liver. T-MPEDNet
leverages a deep adaptive features backbone through a progressive
encoder-decoder structure, enhanced by skip connections for recalibrating
channel-wise features while preserving spatial integrity. A
Transformer-inspired dynamic attention mechanism captures long-range contextual
relationships within the spatial domain, further enhanced by multi-scale
feature utilization for refined local details, leading to accurate prediction.
Morphological boundary refinement is then employed to address indistinct
boundaries with neighboring organs, capturing finer details and yielding
precise boundary labels. The efficacy of T-MPEDNet is comprehensively assessed
on two widely utilized public benchmark datasets, LiTS and 3DIRCADb. Extensive
quantitative and qualitative analyses demonstrate the superiority of T-MPEDNet
compared to twelve state-of-the-art methods. On LiTS, T-MPEDNet achieves
outstanding Dice Similarity Coefficients (DSC) of 97.6% and 89.1% for liver and
tumor segmentation, respectively. Similar performance is observed on 3DIRCADb,
with DSCs of 98.3% and 83.3% for liver and tumor segmentation, respectively.
Our findings prove that T-MPEDNet is an efficacious and reliable framework for
automated segmentation of the liver and its tumor in CT scans.

</details>


### [25] [SurgPIS: Surgical-instrument-level Instances and Part-level Semantics for Weakly-supervised Part-aware Instance Segmentation](https://arxiv.org/abs/2507.19592)
*Meng Wei,Charlie Budd,Oluwatosin Alabi,Miaojing Shi,Tom Vercauteren*

Main category: cs.CV

> 本文提出了一种新的手术工具分割方法SurgPIS，它通过将任务视为统一的部件感知实例分割（PIS）问题，并采用弱监督学习来解决大型数据集缺乏的问题。实验结果显示，SurgPIS在PIS、IIS、PSS及仪器级语义分割方面均实现了最先进的性能。

<details>
  <summary>Details</summary>

**Motivation:** 目前的方法仅处理仪器级实例分割（IIS）或部件级语义分割（PSS）任务，但缺乏两者之间的互动。本研究旨在通过将手术工具分割作为一个统一的部件感知实例分割（PIS）问题，解决这一不足。

**Method:** 我们的方法采用基于变压器的掩码分类方法，并引入从仪器级对象查询派生的部分特定查询，明确地将零件与其父级仪器实例链接起来。为了应对缺乏同时具有实例级和零件级标签的大规模数据集的问题，我们提出了一种弱监督学习策略，使SurgPIS能够从分别标记为IIS或PSS目的的数据集中学习。在训练过程中，我们将PIS预测组合成IIS或PSS掩码，允许我们使用部分标记的数据集计算损失。我们开发了一种学生-教师策略来保持在部分标记数据中丢失PIS信息（如IIS标记数据的部分）的预测一致性。

**Result:** 实验结果显示，SurgPIS在PIS、IIS、PSS及仪器级语义分割方面实现了最先进的性能。

**Conclusion:** 实验表明，所提出的方法能够在多种数据集上有效工作，展示出在PIS（以及IIS、PSS和仪器级语义分割）方面实现了最先进性能的SurgPIS的有效性。

**Abstract:** Consistent surgical instrument segmentation is critical for automation in
robot-assisted surgery. Yet, existing methods only treat instrument-level
instance segmentation (IIS) or part-level semantic segmentation (PSS)
separately, without interaction between these tasks. In this work, we formulate
a surgical tool segmentation as a unified part-aware instance segmentation
(PIS) problem and introduce SurgPIS, the first PIS model for surgical
instruments. Our method adopts a transformer-based mask classification approach
and introduces part-specific queries derived from instrument-level object
queries, explicitly linking parts to their parent instrument instances. In
order to address the lack of large-scale datasets with both instance- and
part-level labels, we propose a weakly-supervised learning strategy for SurgPIS
to learn from disjoint datasets labelled for either IIS or PSS purposes. During
training, we aggregate our PIS predictions into IIS or PSS masks, thereby
allowing us to compute a loss against partially labelled datasets. A
student-teacher approach is developed to maintain prediction consistency for
missing PIS information in the partially labelled data, e.g., parts of the IIS
labelled data. Extensive experiments across multiple datasets validate the
effectiveness of SurgPIS, achieving state-of-the-art performance in PIS as well
as IIS, PSS, and instrument-level semantic segmentation.

</details>


### [26] [Object-centric Video Question Answering with Visual Grounding and Referring](https://arxiv.org/abs/2507.19599)
*Haochen Wang,Qirui Chen,Cilin Yan,Jiayin Cai,Xiaolong Jiang,Yao Hu,Weidi Xie,Stratis Gavves*

Main category: cs.CV

> 本文研究了一种新的VideoLLM模型，提升现有视频大型语言模型的灵活性，支持使用文本和视觉提示与视频进行交互。

<details>
  <summary>Details</summary>

**Motivation:** 现有的视频理解模型主要关注高层次的理解，并且局限于文本响应，这限制了面向对象的多轮交互的灵活性。我们的研究旨在通过引入新的模型方法来解决这些限制，增强用户与视频交互的灵活性和多样性。

**Method:** 我们提出了一个名为VideoLLM的模型，它可以进行输入对象引用和输出定位的视频推理任务，允许用户通过文本和视觉提示与视频进行交互。此外，我们设计了一个新的方法STOM（时空叠加模块），该方法能够将任意单一时段的视觉提示传播到整个视频的其余帧。还构建了一个名为VideoInfer的手工标注的以对象为中心的视频指令数据集，其中包括需要推理的问题和答案对。

**Result:** 在VideoInfer和其它现有的基准测试中进行的全面试验发现，所提出的模型在视频问答和分割中均超过了基线模型，显示了其在多模态对象中心视频和图像理解中的鲁棒性。

**Conclusion:** 实验结果验证了所提模型在视频问答和分割任务上的优越性，表明模型在多模态、以对象为中心的视频和图像理解中发挥了重要的作用。

**Abstract:** Video Large Language Models (VideoLLMs) have recently demonstrated remarkable
progress in general video understanding. However, existing models primarily
focus on high-level comprehension and are limited to text-only responses,
restricting the flexibility for object-centric, multiround interactions. In
this paper, we make three contributions: (i) we address these limitations by
introducing a VideoLLM model, capable of performing both object referring for
input and grounding for output in video reasoning tasks, i.e., allowing users
to interact with videos using both textual and visual prompts; (ii) we propose
STOM (Spatial-Temporal Overlay Module), a novel approach that propagates
arbitrary visual prompts input at any single timestamp to the remaining frames
within a video; (iii) we present VideoInfer, a manually curated object-centric
video instruction dataset featuring questionanswering pairs that require
reasoning. We conduct comprehensive experiments on VideoInfer and other
existing benchmarks across video question answering and referring object
segmentation. The results on 12 benchmarks of 6 tasks show that our proposed
model consistently outperforms baselines in both video question answering and
segmentation, underscoring its robustness in multimodal, object-centric video
and image understanding. Project page:
https://qirui-chen.github.io/RGA3-release/.

</details>


### [27] [Exemplar Med-DETR: Toward Generalized and Robust Lesion Detection in Mammogram Images and beyond](https://arxiv.org/abs/2507.19621)
*Sheethal Bhat,Bogdan Georgescu,Adarsh Bhandary Panambur,Mathias Zinnen,Tri-Thien Nguyen,Awais Mansoor,Karim Khalifa Elbarbary,Siming Bayer,Florin-Cristian Ghesu,Sasa Grbic,Andreas Maier*

Main category: cs.CV

> 本文介绍了一种新的多模态对比检测器Exemplar Med-DETR，解决了已有方法在医学图像异常检测中的不足，达到了前沿性能。

<details>
  <summary>Details</summary>

**Motivation:** 由于医学图像中的异常检测面临特征表现差异及解剖结构和异常间复杂关系的挑战，尤其是在乳腺X线摄影中，密集乳腺组织可能会掩盖病灶，使放射学解释变得复杂。已有检测方法利用解剖和语义上下文学习有效的类别特定特征的能力有限，限制了其在不同任务和成像模式中的应用。因此提出了本研究以解决这一问题。

**Method:** 引入了Exemplar Med-DETR，这是一种新的多模态对比检测器，能够实现基于特征的检测。它采用通过迭代策略训练的交叉注意力机制，包含内在生成的直观的类别特定示例特征。

**Result:** 在三种不同的成像模态来自四个公共数据集上，本方法达成了前沿性能。在越南密集乳腺X光影像中，本方法获得了积斑检测0.7的mAP和钙化物0.55的mAP，与现有方法相比绝对提高了16个百分点。在来自不同分布的中国人群中的100个乳腺X光影中的放射科医师支持评估显示病灶检测性能提升了两倍。此外在胸部X光影像和动脉造影中，分别达到了积斑检测0.25和狭窄检测0.37的mAP，分别提高了4和7个百分点。

**Conclusion:** 此方法有望促进医学影像中健壮且可泛化的检测系统的发展。

**Abstract:** Detecting abnormalities in medical images poses unique challenges due to
differences in feature representations and the intricate relationship between
anatomical structures and abnormalities. This is especially evident in
mammography, where dense breast tissue can obscure lesions, complicating
radiological interpretation. Despite leveraging anatomical and semantic
context, existing detection methods struggle to learn effective class-specific
features, limiting their applicability across different tasks and imaging
modalities. In this work, we introduce Exemplar Med-DETR, a novel multi-modal
contrastive detector that enables feature-based detection. It employs
cross-attention with inherently derived, intuitive class-specific exemplar
features and is trained with an iterative strategy. We achieve state-of-the-art
performance across three distinct imaging modalities from four public datasets.
On Vietnamese dense breast mammograms, we attain an mAP of 0.7 for mass
detection and 0.55 for calcifications, yielding an absolute improvement of 16
percentage points. Additionally, a radiologist-supported evaluation of 100
mammograms from an out-of-distribution Chinese cohort demonstrates a twofold
gain in lesion detection performance. For chest X-rays and angiography, we
achieve an mAP of 0.25 for mass and 0.37 for stenosis detection, improving
results by 4 and 7 percentage points, respectively. These results highlight the
potential of our approach to advance robust and generalizable detection systems
for medical imaging.

</details>


### [28] [Pre- and Post-Treatment Glioma Segmentation with the Medical Imaging Segmentation Toolkit](https://arxiv.org/abs/2507.19626)
*Adrian Celaya,Tucker Netherton,Dawid Schellingerhout,Caroline Chung,Beatrice Riviere,David Fuentes*

Main category: cs.CV

> This paper details the enhanced postprocessing capabilities of the Medical Imaging Segmentation Toolkit (MIST), aimed at improving medical image segmentation, especially for the BraTS 2025 challenge, through customizable strategies.

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind this work is the need for standardized and customizable tooling to rigorously compare medical image segmentation methods, addressing the current lack thereof.

**Method:** The paper introduces advancements in the Medical Imaging Segmentation Toolkit (MIST) with a focus on its enhanced postprocessing module that supports various morphological operations and custom strategies for medical image segmentation.

**Result:** The evaluation of three different postprocessing strategies using the BraTS ranking protocol showed that MIST facilitates the production of high-quality segmentations by enabling rapid experimentation and targeted refinement.

**Conclusion:** MIST, being an open source and extensible toolkit, supports reproducible and scalable research in medical image segmentation, particularly for the BraTS 2025 glioma segmentation challenge.

**Abstract:** Medical image segmentation continues to advance rapidly, yet rigorous
comparison between methods remains challenging due to a lack of standardized
and customizable tooling. In this work, we present the current state of the
Medical Imaging Segmentation Toolkit (MIST), with a particular focus on its
flexible and modular postprocessing framework designed for the BraTS 2025 pre-
and post-treatment glioma segmentation challenge. Since its debut in the 2024
BraTS adult glioma post-treatment segmentation challenge, MIST's postprocessing
module has been significantly extended to support a wide range of transforms,
including removal or replacement of small objects, extraction of the largest
connected components, and morphological operations such as hole filling and
closing. These transforms can be composed into user-defined strategies,
enabling fine-grained control over the final segmentation output. We evaluate
three such strategies - ranging from simple small-object removal to more
complex, class-specific pipelines - and rank their performance using the BraTS
ranking protocol. Our results highlight how MIST facilitates rapid
experimentation and targeted refinement, ultimately producing high-quality
segmentations for the BraTS 2025 challenge. MIST remains open source and
extensible, supporting reproducible and scalable research in medical image
segmentation.

</details>


### [29] [SynPAIN: A Synthetic Dataset of Pain and Non-Pain Facial Expressions](https://arxiv.org/abs/2507.19673)
*Babak Taati,Muhammad Muzammil,Yasamin Zarghami,Abhishek Moturu,Airhossein Kazerouni,Hailey Reimer,Alex Mihailidis,Thomas Hadjistavropoulos*

Main category: cs.CV

> 该研究创建了一个大规模合成面部表情数据集SynPAIN，包含10,710张面部表情图像，旨在用于解决现有疼痛检测数据集中存在的多样性不足、隐私限制和老年人代表性不足的问题。

<details>
  <summary>Details</summary>

**Motivation:** 为了针对疼痛检测数据集中存在的问题，如种族/民族多样性不足、隐私限制和目标群体老年人的代表性不足，开发了SynPAIN数据集。

**Method:** 使用商业生成AI工具创建了种族/民族、年龄和性别平衡的合成身份，并加入了具有临床意义的疼痛表情。

**Result:** 验证发现合成疼痛表情存在预期的疼痛模式，并且年龄匹配的合成数据扩充在真实临床数据上的疼痛检测性能提高了7.0%的平均精度。

**Conclusion:** SynPAIN数据集为首次公开的、具有多样化人口统计学特征的老年人疼痛检测合成数据集，同时建立了衡量和减轻算法偏见的框架。

**Abstract:** Accurate pain assessment in patients with limited ability to communicate,
such as older adults with dementia, represents a critical healthcare challenge.
Robust automated systems of pain detection may facilitate such assessments.
Existing pain detection datasets, however, suffer from limited ethnic/racial
diversity, privacy constraints, and underrepresentation of older adults who are
the primary target population for clinical deployment. We present SynPAIN, a
large-scale synthetic dataset containing 10,710 facial expression images (5,355
neutral/expressive pairs) across five ethnicities/races, two age groups (young:
20-35, old: 75+), and two genders. Using commercial generative AI tools, we
created demographically balanced synthetic identities with clinically
meaningful pain expressions. Our validation demonstrates that synthetic pain
expressions exhibit expected pain patterns, scoring significantly higher than
neutral and non-pain expressions using clinically validated pain assessment
tools based on facial action unit analysis. We experimentally demonstrate
SynPAIN's utility in identifying algorithmic bias in existing pain detection
models. Through comprehensive bias evaluation, we reveal substantial
performance disparities across demographic characteristics. These performance
disparities were previously undetectable with smaller, less diverse datasets.
Furthermore, we demonstrate that age-matched synthetic data augmentation
improves pain detection performance on real clinical data, achieving a 7.0%
improvement in average precision. SynPAIN addresses critical gaps in pain
assessment research by providing the first publicly available, demographically
diverse synthetic dataset specifically designed for older adult pain detection,
while establishing a framework for measuring and mitigating algorithmic bias.
The dataset is available at https://doi.org/10.5683/SP3/WCXMAP

</details>


### [30] [Efficient Learning for Product Attributes with Compact Multimodal Models](https://arxiv.org/abs/2507.19679)
*Mandar Kulkarni*

Main category: cs.CV

> 本文提出了一种用于视觉语言模型的标签高效半监督微调策略，该策略通过直接偏好优化和少量标注数据，有效利用大量未标注数据，显著提高了模型在电商平台产品属性预测任务上的性能。

<details>
  <summary>Details</summary>

**Motivation:** 本文的动机在于解决视觉语言模型的监督微调面临的数据标注成本高且规模挑战大的问题。

**Method:** 本文研究了用于紧凑型视觉语言模型（20亿到30亿参数）的标签高效半监督微调策略，这些策略通过直接偏好优化（DPO）利用未标注的产品列表。该方法首先使用基于API的小标注数据集进行PEFT训练，生成低秩适配器模块，然后通过生成每个未标注样本的多个推理和回答链，根据自洽性将其分为偏好和非偏好，最后使用DPO损失进行模型微调，并使用更新后的模型进行下一轮迭代。

**Result:** 在涵盖十二个电子商务垂直领域的数据集上，基于DPO的微调方法仅使用未标注数据显著优于监督模型，并且实验表明，随着未标注数据量的增加，DPO训练的准确率会提升，表明可以有效地利用大量未标注样本提高性能。

**Conclusion:** 研究结果表明，通过PEFT与DPO组合使用，可以实现高效收敛并最小化计算开销，仅使用未标注数据进行DPO训练的方法在多个电子商务类别上显著优于监督学习，并且随着未标注数据量的增加，DPO训练的性能表现也逐步提升。

**Abstract:** Image-based product attribute prediction in e-commerce is a crucial task with
numerous applications. The supervised fine-tuning of Vision Language Models
(VLMs) faces significant scale challenges due to the cost of manual or API
based annotation. In this paper, we investigate label-efficient semi-supervised
fine-tuning strategies for compact VLMs (2B-3B parameters) that leverage
unlabeled product listings through Direct Preference Optimization (DPO).
Beginning with a small, API-based, annotated, and labeled set, we first employ
PEFT to train low-rank adapter modules. To update the adapter weights with
unlabeled data, we generate multiple reasoning-and-answer chains per unlabeled
sample and segregate these chains into preferred and dispreferred based on
self-consistency. We then fine-tune the model with DPO loss and use the updated
model for the next iteration. By using PEFT fine-tuning with DPO, our method
achieves efficient convergence with minimal compute overhead. On a dataset
spanning twelve e-commerce verticals, DPO-based fine-tuning, which utilizes
only unlabeled data, demonstrates a significant improvement over the supervised
model. Moreover, experiments demonstrate that accuracy with DPO training
improves with more unlabeled data, indicating that a large pool of unlabeled
samples can be effectively leveraged to improve performance.

</details>


### [31] [DeepJIVE: Learning Joint and Individual Variation Explained from Multimodal Data Using Deep Learning](https://arxiv.org/abs/2507.19682)
*Matthew Drexler,Benjamin Risk,James J Lah,Suprateek Kundu,Deqiang Qiu*

Main category: cs.CV

> 我们介绍了一种新的深度学习方法DeepJIVE，用于进行联合与个体方差解释，能够成功揭示多模态数据中的重要关系，并有助于识别疾病相关的生物标志物。

<details>
  <summary>Details</summary>

**Motivation:** 传统的多模态数据集成方法虽能提供对每个数据类型的共享或独特结构的全面评估，但存在着无法处理高维数据和识别非线性结构的问题，因此我们提出了DeepJIVE方法来解决这些问题。

**Method:** 我们提出了一种名为DeepJIVE的深度学习方法，用于执行联合与个体方差解释(JIVE)，并探讨了三种实现DeepJIVE身份和正交性约束的策略及其对应的损失函数。

**Result:** 实验结果表明，DeepJIVE能够成功地揭示多模态数据集中的联合与个体变化。DeepJIVE应用于阿尔茨海默病神经影像学计划(ADNI)时，也识别出了淀粉样正电子发射断层扫描(PET)和磁共振(MR)成像之间的生物学上合理的共变模式。

**Conclusion:** 结论部分指出，所提出的DeepJIVE可以成为多模态数据分析的有用工具。

**Abstract:** Conventional multimodal data integration methods provide a comprehensive
assessment of the shared or unique structure within each individual data type
but suffer from several limitations such as the inability to handle
high-dimensional data and identify nonlinear structures. In this paper, we
introduce DeepJIVE, a deep-learning approach to performing Joint and Individual
Variance Explained (JIVE). We perform mathematical derivation and experimental
validations using both synthetic and real-world 1D, 2D, and 3D datasets.
Different strategies of achieving the identity and orthogonality constraints
for DeepJIVE were explored, resulting in three viable loss functions. We found
that DeepJIVE can successfully uncover joint and individual variations of
multimodal datasets. Our application of DeepJIVE to the Alzheimer's Disease
Neuroimaging Initiative (ADNI) also identified biologically plausible
covariation patterns between the amyloid positron emission tomography (PET) and
magnetic resonance (MR) images. In conclusion, the proposed DeepJIVE can be a
useful tool for multimodal data analysis.

</details>


### [32] [Co-Win: Joint Object Detection and Instance Segmentation in LiDAR Point Clouds via Collaborative Window Processing](https://arxiv.org/abs/2507.19691)
*Haichuan Li,Tomi Westerlund*

Main category: cs.CV

> 本文提出了一种新颖的BEV感知框架Co-Win，用于增强自动驾驶系统中的场景理解和下游决策。

<details>
  <summary>Details</summary>

**Motivation:** 本文旨在解决复杂城市环境中准确感知和场景理解的难题，这对于确保自动驾驶的安全和效率至关重要。

**Method:** 本文提出了一种名为Co-Win的新颖的鸟瞰视图（BEV）感知框架，该框架结合了点云编码与高效的并行窗口特征提取，以应对环境理解中的多模态性挑战。该方法采用分层架构，包括专门的编码器、窗口主干和基于查询的解码头，以有效捕捉多样化的空间特征和对象关系。

**Result:** Co-Win架构通过逐步特征提取步骤处理点云数据，既可以确保预测的掩膜是数据一致的，又能保持上下文的相关性。

**Conclusion:** Co-Win 架构能够生成高质量的实例预测，这对于增强自动驾驶系统中的下游决策和规划至关重要。

**Abstract:** Accurate perception and scene understanding in complex urban environments is
a critical challenge for ensuring safe and efficient autonomous navigation. In
this paper, we present Co-Win, a novel bird's eye view (BEV) perception
framework that integrates point cloud encoding with efficient parallel
window-based feature extraction to address the multi-modality inherent in
environmental understanding. Our method employs a hierarchical architecture
comprising a specialized encoder, a window-based backbone, and a query-based
decoder head to effectively capture diverse spatial features and object
relationships. Unlike prior approaches that treat perception as a simple
regression task, our framework incorporates a variational approach with
mask-based instance segmentation, enabling fine-grained scene decomposition and
understanding. The Co-Win architecture processes point cloud data through
progressive feature extraction stages, ensuring that predicted masks are both
data-consistent and contextually relevant. Furthermore, our method produces
interpretable and diverse instance predictions, enabling enhanced downstream
decision-making and planning in autonomous driving systems.

</details>


### [33] [Bias Analysis for Synthetic Face Detection: A Case Study of the Impact of Facial Attribute](https://arxiv.org/abs/2507.19705)
*Asmae Lamsaf,Lucia Cascone,Hugo Proença,João Neves*

Main category: cs.CV

> 研究分析了合成人脸检测器的偏见，并通过一个评估框架来评估多个面部属性的检测偏差，揭示了检测器偏见的根源。

<details>
  <summary>Details</summary>

**Motivation:** 尽管已经开发了许多检测模型并发布了几个数据集来可靠地识别合成内容，但这些模型和训练数据可能存在偏见，导致某些人口群体的检测失败，并引发重大的社会、法律和伦理问题。

**Method:** 本研究提出了一套评估框架，用于分析合成人脸检测器在多种面部属性上的偏见。该框架利用了属性标签均匀分布的合成数据生成方法，以减少数据偏斜对偏见分析的影响。

**Result:** 研究表明，总体而言，合成脸检测器具有对特定面部特征存在与否的偏见。通过分析检测器训练集中面部属性的平衡性以及对图像配对控制属性修改的检测器激活图，研究揭示了观察到的偏见的起源。

**Conclusion:** 研究为理解和分析合成人脸检测器的偏见提供了重要见解，并强调了在开发检测器和生成训练数据时需要关注的公平性和伦理问题。

**Abstract:** Bias analysis for synthetic face detection is bound to become a critical
topic in the coming years. Although many detection models have been developed
and several datasets have been released to reliably identify synthetic content,
one crucial aspect has been largely overlooked: these models and training
datasets can be biased, leading to failures in detection for certain
demographic groups and raising significant social, legal, and ethical issues.
In this work, we introduce an evaluation framework to contribute to the
analysis of bias of synthetic face detectors with respect to several facial
attributes. This framework exploits synthetic data generation, with evenly
distributed attribute labels, for mitigating any skew in the data that could
otherwise influence the outcomes of bias analysis. We build on the proposed
framework to provide an extensive case study of the bias level of five
state-of-the-art detectors in synthetic datasets with 25 controlled facial
attributes. While the results confirm that, in general, synthetic face
detectors are biased towards the presence/absence of specific facial
attributes, our study also sheds light on the origins of the observed bias
through the analysis of the correlations with the balancing of facial
attributes in the training sets of the detectors, and the analysis of detectors
activation maps in image pairs with controlled attribute modifications.

</details>


### [34] [Quaternion-Based Robust PCA for Efficient Moving Target Detection and Background Recovery in Color Videos](https://arxiv.org/abs/2507.19730)
*Liyang Wang,Shiqian Wu,Shun Fang,Qile Zhu,Jiaxin Wu,Sos Again*

Main category: cs.CV

> This paper introduces the uQRPCA+ framework for moving target detection in color videos, offering state-of-the-art performance in both target segmentation and background recovery by reducing computational complexity and proposing novel methods for color channel processing.

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind the paper is to address the limitations of Quaternion-based RPCA (QRPCA) by reducing its computational complexity and improving its ability to process color channels effectively. The ultimate goal is to enhance the creation of synthetic data that can enrich datasets and improve the generalization ability of deep learning models for moving target detection in color videos.

**Method:** The paper employs a quaternion Riemannian manifold to minimize the computational complexity of Quaternion Singular Value Decomposition (QSVD) to o(1). Additionally, the universal QRPCA (uQRPCA) framework is proposed, followed by the uQRPCA+ method that includes the Color Rank-1 Batch (CR1B) technique to improve background recovery.

**Result:** The experimental results indicate that the uQRPCA+ method outperforms existing open-source approaches in moving target detection and background recovery tasks.

**Conclusion:** The uQRPCA+ framework successfully enhances moving target detection by optimizing the processing of color videos and offering state-of-the-art performance. The work signifies progress towards automated generation of labeled datasets and improving deep model performance in diverse video scenarios.

**Abstract:** Moving target detection is a challenging computer vision task aimed at
generating accurate segmentation maps in diverse in-the-wild color videos
captured by static cameras. If backgrounds and targets can be simultaneously
extracted and recombined, such synthetic data can significantly enrich
annotated in-the-wild datasets and enhance the generalization ability of deep
models. Quaternion-based RPCA (QRPCA) is a promising unsupervised paradigm for
color image processing. However, in color video processing, Quaternion Singular
Value Decomposition (QSVD) incurs high computational costs, and rank-1
quaternion matrix fails to yield rank-1 color channels. In this paper, we
reduce the computational complexity of QSVD to o(1) by utilizing a quaternion
Riemannian manifold. Furthermor, we propose the universal QRPCA (uQRPCA)
framework, which achieves a balance in simultaneously segmenting targets and
recovering backgrounds from color videos. Moreover, we expand to uQRPCA+ by
introducing the Color Rank-1 Batch (CR1B) method to further process and obtain
the ideal low-rank background across color channels. Experiments demonstrate
our uQRPCA+ achieves State Of The Art (SOTA) performance on moving target
detection and background recovery tasks compared to existing open-source
methods. Our implementation is publicly available on GitHub at
https://github.com/Ruchtech/uQRPCA

</details>


### [35] [Leveraging Sparse LiDAR for RAFT-Stereo: A Depth Pre-Fill Perspective](https://arxiv.org/abs/2507.19738)
*Jinsu Yoo,Sooyoung Jeon,Zanming Huang,Tai-Yu Pan,Wei-Lun Chao*

Main category: cs.CV

> 通过分析LiDAR指导在RAFT-Stereo框架中的应用，本文提出了一种新的解决方案，即通过预填充稀疏的视差图来改进LiDAR指导的效果，并且通过结合不同的预填充方法，使提出的GRAFT-Stereo在不同数据集下优于现有的LiDAR指导方法。

<details>
  <summary>Details</summary>

**Motivation:** 本文的目标是探索如何利用稀疏的LiDAR点云来提高立体匹配算法的性能，并发现了当LiDAR点稀疏时，立体匹配效果会显著下降，从而寻求解决方案。

**Method:** 我们探讨了在RAFT-Stereo框架中使用LiDAR进行指导，以通过向初始视差图注入精确的LiDAR深度来提高立体匹配的准确性。我们发现，当LiDAR点稀疏时（例如，每帧只有几百个点），LiDAR指导的效果会急剧下降，并从信号处理的角度给出了一种新颖的解释。这一见解导致了一个出人意料的简单解决方案，使得LiDAR指导的RAFT-Stereo能够成功：预先通过插值填充稀疏的初始视差图。有趣的是，我们发现，在通过早期融合将LiDAR深度注入图像特征时，预先填充同样有效，但这背后原因不同，需要采用不同的预填充方法。

**Result:** 通过结合这两种方案，提出的Guided RAFT-Stereo (GRAFT-Stereo)在各种数据集下的稀疏LiDAR条件下显著优于现有的LiDAR指导方法。

**Conclusion:** 我们期望这项研究能够激励更多高效利用LiDAR指导的立体匹配方法。

**Abstract:** We investigate LiDAR guidance within the RAFT-Stereo framework, aiming to
improve stereo matching accuracy by injecting precise LiDAR depth into the
initial disparity map. We find that the effectiveness of LiDAR guidance
drastically degrades when the LiDAR points become sparse (e.g., a few hundred
points per frame), and we offer a novel explanation from a signal processing
perspective. This insight leads to a surprisingly simple solution that enables
LiDAR-guided RAFT-Stereo to thrive: pre-filling the sparse initial disparity
map with interpolation. Interestingly, we find that pre-filling is also
effective when injecting LiDAR depth into image features via early fusion, but
for a fundamentally different reason, necessitating a distinct pre-filling
approach. By combining both solutions, the proposed Guided RAFT-Stereo
(GRAFT-Stereo) significantly outperforms existing LiDAR-guided methods under
sparse LiDAR conditions across various datasets. We hope this study inspires
more effective LiDAR-guided stereo methods.

</details>


### [36] [Latest Object Memory Management for Temporally Consistent Video Instance Segmentation](https://arxiv.org/abs/2507.19754)
*Seunghun Lee,Jiwan Seo,Minwoo Choi,Kiljoon Han,Jaehoon Jeong,Zane Durante,Ehsan Adeli,Sang Hyun Park,Sunghoon Im*

Main category: cs.CV

> 本文提出了一种名为Latest Object Memory Management (LOMM)的方法，显著改善了视频实例分割中的长时间一致性问题，通过Latest Object Memory (LOM)和Decoupled Object Association (DOA)策略，提高了匹配精度和身份一致性，在YouTube-VIS 2022数据集上达到了54.0的AP得分。

<details>
  <summary>Details</summary>

**Motivation:** 本文旨在解决视频实例分割中的长时间一致性问题，通过改进长时间实例跟踪来提高性能和可靠性。

**Method:** Latest Object Memory Management (LOMM)采用Latest Object Memory (LOM)来处理长时间实例跟踪，通过明确建模每个帧中的对象存在情况，不断更新对象的最新状态以实现一致的跟踪和准确的身份管理。同时引入了Decoupled Object Association (DOA)策略，分别处理新出现和已存在的对象，以提高匹配精度和身份的一致性。

**Result:** 广泛的实验和消融研究证明，所提出的方法优于传统方法，达到了新的基准。在YouTube-VIS 2022数据集上，其AP得分为54.0。

**Conclusion:** 该研究证明了Latest Object Memory Management (LOMM)能显著改进长时间实例跟踪，并在视频实例分割领域取得了最优结果。

**Abstract:** In this paper, we present Latest Object Memory Management (LOMM) for
temporally consistent video instance segmentation that significantly improves
long-term instance tracking. At the core of our method is Latest Object Memory
(LOM), which robustly tracks and continuously updates the latest states of
objects by explicitly modeling their presence in each frame. This enables
consistent tracking and accurate identity management across frames, enhancing
both performance and reliability through the VIS process. Moreover, we
introduce Decoupled Object Association (DOA), a strategy that separately
handles newly appearing and already existing objects. By leveraging our memory
system, DOA accurately assigns object indices, improving matching accuracy and
ensuring stable identity consistency, even in dynamic scenes where objects
frequently appear and disappear. Extensive experiments and ablation studies
demonstrate the superiority of our method over traditional approaches, setting
a new benchmark in VIS. Notably, our LOMM achieves state-of-the-art AP score of
54.0 on YouTube-VIS 2022, a dataset known for its challenging long videos.
Project page: https://seung-hun-lee.github.io/projects/LOMM/

</details>


### [37] [MoFRR: Mixture of Diffusion Models for Face Retouching Restoration](https://arxiv.org/abs/2507.19770)
*Jiaxin Liu,Qichao Ying,Zhenxing Qian,Sheng Li,Runqi Zhang,Jian Liu,Xinpeng Zhang*

Main category: cs.CV

> The paper introduces FRR and MoFRR to address the challenge of restoring original faces from retouched ones on social media images, using specialized experts and a dual-branch structure.

<details>
  <summary>Details</summary>

**Motivation:** Existing methods focus on detecting face retouching, but there is a lack of methods to accurately recover the original faces from retouched ones, which motivates this research.

**Method:** Face Retouching Restoration (FRR) is introduced, focusing on restoring original faces from retouched ones. The proposed solution, MoFRR, uses a mixture of diffusion models, with specialized experts for distinct retouching types and a shared expert for universal retouching, using a dual-branch structure.

**Result:** Experiments on RetouchingFFHQ++ show the effectiveness of MoFRR in FRR.

**Conclusion:** The paper contributes to the field by proposing a novel task and solution for restoring original faces from retouched images, demonstrating its effectiveness on a new dataset.

**Abstract:** The widespread use of face retouching on social media platforms raises
concerns about the authenticity of face images. While existing methods focus on
detecting face retouching, how to accurately recover the original faces from
the retouched ones has yet to be answered. This paper introduces Face
Retouching Restoration (FRR), a novel computer vision task aimed at restoring
original faces from their retouched counterparts. FRR differs from traditional
image restoration tasks by addressing the complex retouching operations with
various types and degrees, which focuses more on the restoration of the
low-frequency information of the faces. To tackle this challenge, we propose
MoFRR, Mixture of Diffusion Models for FRR. Inspired by DeepSeek's expert
isolation strategy, the MoFRR uses sparse activation of specialized experts
handling distinct retouching types and the engagement of a shared expert
dealing with universal retouching traces. Each specialized expert follows a
dual-branch structure with a DDIM-based low-frequency branch guided by an
Iterative Distortion Evaluation Module (IDEM) and a Cross-Attention-based
High-Frequency branch (HFCAM) for detail refinement. Extensive experiments on a
newly constructed face retouching dataset, RetouchingFFHQ++, demonstrate the
effectiveness of MoFRR for FRR.

</details>


### [38] [Self-Guided Masked Autoencoder](https://arxiv.org/abs/2507.19773)
*Jeongwoo Shin,Inseo Lee,Junho Lee,Joonseok Lee*

Main category: cs.CV

> 研究揭示掩码自编码器学习模式，并提出了一种自引导掩码自编码器方法，提高学习效率的同时保持了自监督特性。

<details>
  <summary>Details</summary>

**Motivation:** 尽管掩码自编码器（Masked Autoencoder，简称MAE）在计算机视觉的多个下游任务上取得了成功，但其内部学习机制尚未被完全理解。本文旨在通过深入分析揭示MAE的学习机制，并在其基础上提出改进方法。

**Method:** 文章提出了一种名为自引导掩码自编码器（self-guided masked autoencoder）的方法。该方法在预训练的早期阶段内部生成基于其在块聚类中的进展的智能掩码，用以替代传统的随机掩码策略。

**Result:** 实验结果表明，该方法在多项下游任务上表现出了显著的性能提升，且无需依赖任何外部模型或额外信息，保持了自监督学习的本质。

**Conclusion:** 研究表明，MAE在预训练的早期阶段就学会了基于模式的块级别聚类。通过引入自引导机制，这种方法巩固了MAE的学习效果，且无需引入外部模型或额外的数据信息。

**Abstract:** Masked Autoencoder (MAE) is a self-supervised approach for representation
learning, widely applicable to a variety of downstream tasks in computer
vision. In spite of its success, it is still not fully uncovered what and how
MAE exactly learns. In this paper, with an in-depth analysis, we discover that
MAE intrinsically learns pattern-based patch-level clustering from surprisingly
early stages of pretraining. Upon this understanding, we propose self-guided
masked autoencoder, which internally generates informed mask by utilizing its
progress in patch clustering, substituting the naive random masking of the
vanilla MAE. Our approach significantly boosts its learning process without
relying on any external models or supplementary information, keeping the
benefit of self-supervised nature of MAE intact. Comprehensive experiments on
various downstream tasks verify the effectiveness of the proposed method.

</details>
