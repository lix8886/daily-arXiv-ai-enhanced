{"id": "2510.19858", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.19858", "abs": "https://arxiv.org/abs/2510.19858", "authors": ["Jindi Wang", "Yidi Zhang", "Zhaoxing Li"], "title": "DeBERTa-KC: A Transformer-Based Classifier for Knowledge Construction in Online Learning Discourse", "comment": null, "summary": "This study presents DeBERTa-KC, a transformer-based model for automatic\nclassification of knowledge construction (KC) levels in online science learning\ndiscourse. Using comments collected from four popular YouTube science channels\n(2022--2024), a balanced corpus of 20,000 manually annotated samples was\ncreated across four KC categories: \\textit{nonKC}, \\textit{Share},\n\\textit{Explore}, and \\textit{Negotiate}. The proposed model extends DeBERTa-v3\nwith Focal Loss, Label Smoothing, and R-Drop regularization to address class\nimbalance and enhance generalization. A reproducible end-to-end pipeline was\nimplemented, encompassing data extraction, annotation, preprocessing, training,\nand evaluation. Across 10-fold stratified cross-validation, DeBERTa-KC achieved\na macro-F1 of $0.836 \\pm 0.008$, significantly out-performing both classical\nand transformer baselines ($p<0.01$). Per-category results indicate strong\nsensitivity to higher-order epistemic engagement, particularly in\n\\textit{Explore} and \\textit{Negotiate} discourse. These findings demonstrate\nthat large language models can effectively capture nuanced indicators of\nknowledge construction in informal digital learning environments, offering\nscalable, theory-informed approaches to discourse analysis and the development\nof automated tools for assessing epistemic engagement.", "AI": {"tldr": "The research develops DeBERTa-KC, a transformer model for classifying knowledge construction levels in online science discourse, with strong performance and significant improvement over existing models.", "motivation": "The study aims to leverage transformer models for analyzing science learning discourse online, with the goal of building a scalable and informed tool for assessing epistemic engagement and advancing automated tools for discourse analysis.", "method": "The paper introduces DeBERTa-KC, a transformer model optimized for the classification of knowledge construction levels in online science learning. It uses 20,000 manually annotated comments from YouTube, focusing on four categories of knowledge construction, and improves upon the DeBERTa-v3 model with techniques for handling class imbalance and enhancing generalization.", "result": "DeBERTa-KC demonstrates a macro-F1 score of 0.836 ± 0.008 on a 10-fold cross-validation, achieving better results than classical and transformer baselines, particularly in higher-order epistemic categories.", "conclusion": "The findings highlight that large language models can capture complex educational signals in informal learning environments, contributing to the advancement of automated tools for epistemic engagement assessment."}}
{"id": "2510.19866", "categories": ["cs.CL", "cs.AI", "G.1.10; G.4; I.2.6; I.2.7"], "pdf": "https://arxiv.org/pdf/2510.19866", "abs": "https://arxiv.org/abs/2510.19866", "authors": ["Xincheng Liu"], "title": "An Evaluation of the Pedagogical Soundness and Usability of AI-Generated Lesson Plans Across Different Models and Prompt Frameworks in High-School Physics", "comment": "20 pages, 6 tables", "summary": "This study evaluates the pedagogical soundness and usability of AI-generated\nlesson plans across five leading large language models: ChatGPT (GPT-5), Claude\nSonnet 4.5, Gemini 2.5 Flash, DeepSeek V3.2, and Grok 4. Beyond model choice,\nthree structured prompt frameworks were tested: TAG (Task, Audience, Goal),\nRACE (Role, Audience, Context, Execution), and COSTAR (Context, Objective,\nStyle, Tone, Audience, Response Format).\n  Fifteen lesson plans were generated for a single high-school physics topic,\nThe Electromagnetic Spectrum. The lesson plans were analyzed through four\nautomated computational metrics: (1) readability and linguistic complexity, (2)\nfactual accuracy and hallucination detection, (3) standards and curriculum\nalignment, and (4) cognitive demand of learning objectives.\n  Results indicate that model selection exerted the strongest influence on\nlinguistic accessibility, with DeepSeek producing the most readable teaching\nplan (FKGL = 8.64) and Claude generating the densest language (FKGL = 19.89).\n  The prompt framework structure most strongly affected the factual accuracy\nand pedagogical completeness, with the RACE framework yielding the lowest\nhallucination index and the highest incidental alignment with NGSS curriculum\nstandards. Across all models, the learning objectives in the fifteen lesson\nplans clustered at the Remember and Understand tiers of Bloom's taxonomy. There\nwere limited higher-order verbs in the learning objectives extracted.\n  Overall, the findings suggest that readability is significantly governed by\nmodel design, while instructional reliability and curricular alignment depend\nmore on the prompt framework. The most effective configuration for lesson plans\nidentified in the results was to combine a readability-optimized model with the\nRACE framework and an explicit checklist of physics concepts, curriculum\nstandards, and higher-order objectives.", "AI": {"tldr": "研究表明，在五个大型语言模型中，模型选择显著影响语言可读性，而提示框架结构则对事实准确性和教学完整性有较大影响。最有效的配置是结合可读性优化的模型与RACE框架，以及明确的物理概念、课程标准和高层次目标清单。", "motivation": "研究动机在于评估五个前沿大型语言模型生成的AI教学计划的教育合理性和可用性，并探讨模型选择和提示框架对教学计划质量的影响。", "method": "研究通过四个自动计算指标评估了十五份针对高中物理主题《电磁波谱》生成的教学计划：可读性和语言复杂性、事实准确性和幻觉检测、课程标准吻合度以及学习目标的认知需求。", "result": "结果显示，模型选择对语言可读性影响最大，其中DeepSeek生成的最易读（FKGL = 8.64），Claude生成的最复杂（FKGL = 19.89）。RACE框架产生最低的幻觉指数和最高的NGSS课程标准吻合度。所有模型的学习目标集中在布卢姆分类学的识记和理解层级，高层次动词较少。", "conclusion": "研究表明，可读性很大程度上受模型设计影响，而教学可靠性和课程吻合度更多依赖于提示框架。结合可读性优化的模型、RACE框架和明确的物理概念、课程标准及高层次目标清单是生成有效教学计划最有效的配置。"}}
{"id": "2510.19871", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.19871", "abs": "https://arxiv.org/abs/2510.19871", "authors": ["Yatai Ji", "Teng Wang", "Yuying Ge", "Zhiheng Liu", "Sidi Yang", "Ying Shan", "Ping Luo"], "title": "From Denoising to Refining: A Corrective Framework for Vision-Language Diffusion Model", "comment": null, "summary": "Discrete diffusion models have emerged as a promising direction for\nvision-language tasks, offering bidirectional context modeling and theoretical\nparallelization. However, their practical application is severely hindered by a\ntrain-inference discrepancy, which leads to catastrophic error cascades:\ninitial token errors during parallel decoding pollute the generation context,\ntriggering a chain reaction of compounding errors and leading to syntactic\nerrors and semantic hallucinations. To address this fundamental challenge, we\nreframe the generation process from passive denoising to active refining. We\nintroduce ReDiff, a refining-enhanced diffusion framework that teaches the\nmodel to identify and correct its own errors. Our approach features a two-stage\ntraining process: first, we instill a foundational revision capability by\ntraining the model to revise synthetic errors; second, we implement a novel\nonline self-correction loop where the model is explicitly trained to revise its\nown flawed drafts by learning from an expert's corrections. This mistake-driven\nlearning endows the model with the crucial ability to revisit and refine its\nalready generated output, effectively breaking the error cascade. Extensive\nexperiments demonstrate that ReDiff significantly improves the coherence and\nfactual accuracy of generated content, enabling stable and efficient parallel\ngeneration far superior to traditional denoising methods. Our codes and models\nare available at https://rediff-hku.github.io/.", "AI": {"tldr": "ReDiff通过主动优化方法解决了离散扩散模型中的训练-推断差异问题，提高了生成内容的质量。", "motivation": "旨在解决离散扩散模型在视觉-语言任务中的训练-推断差异问题，该问题会导致错误级联，进而影响生成内容的语法和语义。", "method": "通过将生成过程从被动的去噪转换为主动的优化，引入ReDiff框架，该框架包括两个阶段的训练过程：首先，训练模型修正合成的错误；其次，实现一个在线自我校正回路，让模型能够从专家的校正中学习，修正自己产生的错误草稿。", "result": "实验证明，ReDiff显著提高了生成内容的连贯性和事实准确性，并实现了比传统去噪方法更稳定和高效的平行生成。", "conclusion": "ReDiff通过主动优化的两阶段训练过程，极大地改善了离散扩散模型的性能，能够在视觉-语言任务中生成更高质量的内容。"}}
{"id": "2510.19875", "categories": ["cs.CL", "cs.AI", "68T40", "I.2.11"], "pdf": "https://arxiv.org/pdf/2510.19875", "abs": "https://arxiv.org/abs/2510.19875", "authors": ["J Rosser", "José Luis Redondo García", "Gustavo Penha", "Konstantina Palla", "Hugues Bouchard"], "title": "Stream: Scaling up Mechanistic Interpretability to Long Context in LLMs via Sparse Attention", "comment": null, "summary": "As Large Language Models (LLMs) scale to million-token contexts, traditional\nMechanistic Interpretability techniques for analyzing attention scale\nquadratically with context length, demanding terabytes of memory beyond 100,000\ntokens. We introduce Sparse Tracing, a novel technique that leverages dynamic\nsparse attention to efficiently analyze long context attention patterns. We\npresent Stream, a compilable hierarchical pruning algorithm that estimates\nper-head sparse attention masks in near-linear time $O(T \\log T)$ and linear\nspace $O(T)$, enabling one-pass interpretability at scale. Stream performs a\nbinary-search-style refinement to retain only the top-$k$ key blocks per query\nwhile preserving the model's next-token behavior. We apply Stream to long\nchain-of-thought reasoning traces and identify thought anchors while pruning\n97-99\\% of token interactions. On the RULER benchmark, Stream preserves\ncritical retrieval paths while discarding 90-96\\% of interactions and exposes\nlayer-wise routes from the needle to output. Our method offers a practical\ndrop-in tool for analyzing attention patterns and tracing information flow\nwithout terabytes of caches. By making long context interpretability feasible\non consumer GPUs, Sparse Tracing helps democratize chain-of-thought monitoring.\nCode is available at https://anonymous.4open.science/r/stream-03B8/.", "AI": {"tldr": "Introduced Sparse Tracing with the Stream algorithm to efficiently interpret large LLMs with long context, reducing computational costs significantly.", "motivation": "To address the issue of traditional Mechanistic Interpretability techniques scaling quadratically with context length, making it infeasible for large LLMs.", "method": "Sparse Tracing, a technique using dynamic sparse attention to analyze long context attention patterns of Large Language Models (LLMs) using the Stream algorithm for hierarchical pruning to estimate sparse attention masks.", "result": "Sparse Tracing enables one-pass interpretability with near-linear time and linear space complexity, while identifying thought anchors and preserving critical retrieval paths in long chain-of-thought reasoning. Stream algorithm achieves up to 97-99% pruning of token interactions with minimal impact on model performance.", "conclusion": "Sparse Tracing offers a feasible solution for long context interpretability on consumer GPUs, democratizing the analysis of LLMs' attention patterns and information flow."}}
{"id": "2510.19840", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.19840", "abs": "https://arxiv.org/abs/2510.19840", "authors": ["Sai Teja Erukude", "Viswa Chaitanya Marella", "Suhasnadh Reddy Veluru"], "title": "Fourier-Based GAN Fingerprint Detection using ResNet50", "comment": "6 pages. Published in IEEE", "summary": "The rapid rise of photorealistic images produced from Generative Adversarial\nNetworks (GANs) poses a serious challenge for image forensics and industrial\nsystems requiring reliable content authenticity. This paper uses\nfrequency-domain analysis combined with deep learning to solve the problem of\ndistinguishing StyleGAN-generated images from real ones. Specifically, a\ntwo-dimensional Discrete Fourier Transform (2D DFT) was applied to transform\nimages into the Fourier domain, where subtle periodic artifacts become\ndetectable. A ResNet50 neural network is trained on these transformed images to\ndifferentiate between real and synthetic ones. The experiments demonstrate that\nthe frequency-domain model achieves a 92.8 percent and an AUC of 0.95,\nsignificantly outperforming the equivalent model trained on raw spatial-domain\nimages. These results indicate that the GAN-generated images have unique\nfrequency-domain signatures or \"fingerprints\". The method proposed highlights\nthe industrial potential of combining signal processing techniques and deep\nlearning to enhance digital forensics and strengthen the trustworthiness of\nindustrial AI systems.", "AI": {"tldr": "A technique combining frequency-domain analysis and deep learning (ResNet50) is used to detect photorealistic images generated by StyleGAN, achieving better results than models trained on raw images.", "motivation": "The rapid rise of photorealistic images from GANs complicates image forensics and industrial systems that require reliable content authentication. This work aims to address this challenge using frequency-domain analysis and deep learning.", "method": "The paper applies a 2D Discrete Fourier Transform to images to transform them into the Fourier domain, where subtle periodic artifacts are detectable. A ResNet50 neural network is then trained to differentiate between StyleGAN-generated and real images using this transformed data.", "result": "The frequency-domain model achieved a 92.8 percent accuracy and an AUC of 0.95, outperforming the equivalent model trained on raw spatial-domain images.", "conclusion": "The proposed method shows that GAN-generated images have unique frequency-domain signatures. It highlights the potential of combining signal processing and deep learning to improve digital forensics and the reliability of industrial AI systems."}}
{"id": "2510.19879", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.19879", "abs": "https://arxiv.org/abs/2510.19879", "authors": ["Lang Zhou", "Amrish Jhingoer", "Yinghao Luo", "Klaske Vliegenthart--Jongbloed", "Carlijn Jordans", "Ben Werkhoven", "Tom Seinen", "Erik van Mulligen", "Casper Rokx", "Yunlei Li"], "title": "Automated HIV Screening on Dutch EHR with Large Language Models", "comment": "28 pages, 6 figures", "summary": "Efficient screening and early diagnosis of HIV are critical for reducing\nonward transmission. Although large scale laboratory testing is not feasible,\nthe widespread adoption of Electronic Health Records (EHRs) offers new\nopportunities to address this challenge. Existing research primarily focuses on\napplying machine learning methods to structured data, such as patient\ndemographics, for improving HIV diagnosis. However, these approaches often\noverlook unstructured text data such as clinical notes, which potentially\ncontain valuable information relevant to HIV risk. In this study, we propose a\nnovel pipeline that leverages a Large Language Model (LLM) to analyze\nunstructured EHR text and determine a patient's eligibility for further HIV\ntesting. Experimental results on clinical data from Erasmus University Medical\nCenter Rotterdam demonstrate that our pipeline achieved high accuracy while\nmaintaining a low false negative rate.", "AI": {"tldr": "研究提出了一种新方法，使用大型语言模型分析电子健康记录中的非结构化文本，以提高HIV筛查和早期诊断的效率，并在实际数据中验证了其高准确率和低假阴性率。", "motivation": "虽然大规模实验室检测不太现实，但电子健康记录（EHRs）的广泛采用为提高HIV筛查和早期诊断效率提供了新的机会。当前的研究主要集中在利用机器学习方法处理结构化数据，例如患者人口统计信息，但这些方法经常忽略了包含重要HIV风险信息的非结构化文本数据。", "method": "本研究提出了一种利用大型语言模型（LLM）分析电子健康记录中的非结构化文本数据，以确定患者是否需要进一步进行HIV检测的新方法。", "result": "实验结果表明，在荷兰鹿特丹伊拉斯谟大学医学中心的临床数据上，我们的方法达到了高准确率，同时保持了较低的假阴性率。", "conclusion": "本研究证明，通过使用大型语言模型分析非结构化电子健康记录数据，可以提高对HIV检测的筛查效率，且误诊率低。"}}
{"id": "2510.19955", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.19955", "abs": "https://arxiv.org/abs/2510.19955", "authors": ["Márcus Vinícius Lobo Costa", "Sherlon Almeida da Silva", "Bárbara Caroline Benato", "Leo Sampaio Ferraz Ribeiro", "Moacir Antonelli Ponti"], "title": "Transformed Multi-view 3D Shape Features with Contrastive Learning", "comment": null, "summary": "This paper addresses the challenges in representation learning of 3D shape\nfeatures by investigating state-of-the-art backbones paired with both\ncontrastive supervised and self-supervised learning objectives. Computer vision\nmethods struggle with recognizing 3D objects from 2D images, often requiring\nextensive labeled data and relying on Convolutional Neural Networks (CNNs) that\nmay overlook crucial shape relationships. Our work demonstrates that Vision\nTransformers (ViTs) based architectures, when paired with modern contrastive\nobjectives, achieve promising results in multi-view 3D analysis on our\ndownstream tasks, unifying contrastive and 3D shape understanding pipelines.\nFor example, supervised contrastive losses reached about 90.6% accuracy on\nModelNet10. The use of ViTs and contrastive learning, leveraging ViTs' ability\nto understand overall shapes and contrastive learning's effectiveness,\novercomes the need for extensive labeled data and the limitations of CNNs in\ncapturing crucial shape relationships. The success stems from capturing global\nshape semantics via ViTs and refining local discriminative features through\ncontrastive optimization. Importantly, our approach is empirical, as it is\ngrounded on extensive experimental evaluation to validate the effectiveness of\ncombining ViTs with contrastive objectives for 3D representation learning.", "AI": {"tldr": "This paper shows that Vision Transformers (ViTs) with modern contrastive objectives can significantly enhance 3D shape understanding and recognition from 2D images, achieving high accuracy and reducing the reliance on large labeled datasets.", "motivation": "The paper aims to overcome the challenges in 3D object recognition from 2D images using extensive labeled data and to address CNNs' limitations in capturing crucial shape relationships.", "method": "Our work uses Vision Transformers (ViTs) paired with contrastive and self-supervised learning objectives to improve 3D shape feature representation learning.", "result": "Supervised contrastive losses reached about 90.6% accuracy on the ModelNet10 dataset, demonstrating the effectiveness of ViTs and contrastive learning in multi-view 3D analysis.", "conclusion": "The study concludes that the combination of ViTs and contrastive learning can unify contrastive and 3D shape understanding pipelines, improving 3D object recognition by capturing global shape semantics and refining local discriminative features."}}
{"id": "2510.19886", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.19886", "abs": "https://arxiv.org/abs/2510.19886", "authors": ["Artur Donaldson", "Bharathan Balaji", "Cajetan Oriekezie", "Manish Kumar", "Laure Patouillard"], "title": "An Expert-grounded benchmark of General Purpose LLMs in LCA", "comment": null, "summary": "Purpose: Artificial intelligence (AI), and in particular large language\nmodels (LLMs), are increasingly being explored as tools to support life cycle\nassessment (LCA). While demonstrations exist across environmental and social\ndomains, systematic evidence on their reliability, robustness, and usability\nremains limited. This study provides the first expert-grounded benchmark of\nLLMs in LCA, addressing the absence of standardized evaluation frameworks in a\nfield where no clear ground truth or consensus protocols exist.\n  Methods: We evaluated eleven general-purpose LLMs, spanning both commercial\nand open-source families, across 22 LCA-related tasks. Seventeen experienced\npractitioners reviewed model outputs against criteria directly relevant to LCA\npractice, including scientific accuracy, explanation quality, robustness,\nverifiability, and adherence to instructions. We collected 168 expert reviews.\n  Results: Experts judged 37% of responses to contain inaccurate or misleading\ninformation. Ratings of accuracy and quality of explanation were generally\nrated average or good on many models even smaller models, and format adherence\nwas generally rated favourably. Hallucination rates varied significantly, with\nsome models producing hallucinated citations at rates of up to 40%. There was\nno clear-cut distinction between ratings on open-weight versus closed-weight\nLLMs, with open-weight models outperforming or competing on par with\nclosed-weight models on criteria such as accuracy and quality of explanation.\n  Conclusion: These findings highlight the risks of applying LLMs na\\\"ively in\nLCA, such as when LLMs are treated as free-form oracles, while also showing\nbenefits especially around quality of explanation and alleviating labour\nintensiveness of simple tasks. The use of general-purpose LLMs without\ngrounding mechanisms presents ...", "AI": {"tldr": "本研究对LCA领域中的大型语言模型进行了专家基准测试，结果揭示了这些模型存在的不准确性、解释质量的差异和幻觉引用生成问题，同时指出了解释质量改善及简单任务的劳动密集性减轻的益处。在没有适当锚定机制的情况下使用通用大型语言模型存在风险。建议谨慎使用。", "motivation": "由于缺乏标准化的评估框架，尤其是在没有明确基准或共识协议的情况下，关于这些模型在生命周期评估（LCA）中的可靠性、稳健性和可使用性的系统证据一直较为有限。", "method": "我们评估了十一款通用大型语言模型（包括商业和开源模型）在22个生命周期评估（LCA）相关任务上的表现。十七位经验丰富的从业者根据与LCA实践直接相关的标准（如科学准确性、解释质量、稳健性、可验证性和遵守指令情况）审查了模型输出。我们收集了168专家评审。", "result": "专家认为37%的响应包含不准确或误导性的信息。许多模型（即使是较小的模型）在准确性、解释质量、格式遵守性方面的评分大多被评为平均或良好。幻觉率波动显著，某些模型会以高达40%的概率生成幻觉引用。在开放式与封闭式大型语言模型评级之间没有明显的区别，开放源代码模型在准确性、解释质量等方面的表现可以媲美甚至优于封闭源代码模型。", "conclusion": "这些发现强调了在LCA中盲目使用大型语言模型的风险，同时也表明在解释质量改善和减轻简单任务的劳动密集性方面存在益处。通用大型语言模型在没有适当锚定机制的情况下使用... "}}
{"id": "2510.19981", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.19981", "abs": "https://arxiv.org/abs/2510.19981", "authors": ["Martha Teiko Teye", "Ori Maoz", "Matthias Rottmann"], "title": "FutrTrack: A Camera-LiDAR Fusion Transformer for 3D Multiple Object Tracking", "comment": null, "summary": "We propose FutrTrack, a modular camera-LiDAR multi-object tracking framework\nthat builds on existing 3D detectors by introducing a transformer-based\nsmoother and a fusion-driven tracker. Inspired by query-based tracking\nframeworks, FutrTrack employs a multimodal two-stage transformer refinement and\ntracking pipeline. Our fusion tracker integrates bounding boxes with multimodal\nbird's-eye-view (BEV) fusion features from multiple cameras and LiDAR without\nthe need for an explicit motion model. The tracker assigns and propagates\nidentities across frames, leveraging both geometric and semantic cues for\nrobust re-identification under occlusion and viewpoint changes. Prior to\ntracking, we refine sequences of bounding boxes with a temporal smoother over a\nmoving window to refine trajectories, reduce jitter, and improve spatial\nconsistency. Evaluated on nuScenes and KITTI, FutrTrack demonstrates that\nquery-based transformer tracking methods benefit significantly from multimodal\nsensor features compared with previous single-sensor approaches. With an aMOTA\nof 74.7 on the nuScenes test set, FutrTrack achieves strong performance on 3D\nMOT benchmarks, reducing identity switches while maintaining competitive\naccuracy. Our approach provides an efficient framework for improving\ntransformer-based trackers to compete with other neural-network-based methods\neven with limited data and without pretraining.", "AI": {"tldr": "FutrTrack是一种基于transformer的多模态相机-LiDAR多目标跟踪框架，通过优化多模态数据处理和使用时间平滑器，在nuScenes和KITTI数据集上展示了卓越的3D跟踪性能。", "motivation": "FutrTrack旨在通过利用多模态传感器数据提高多目标跟踪的性能，特别是在遮挡和视角变化下的鲁棒重新识别能力。该方法旨在解决传统方法中存在的极限，并通过优化多模式数据的处理提高跟踪过程的效果。", "method": "提出了一种称为FutrTrack的多模态相机-LiDAR多目标跟踪框架，该框架基于现有的3D检测器，并引入了基于transformer的平滑器和融合驱动的跟踪器。FutrTrack采用了一种多模态两阶段transformer细化和跟踪管道。其融合跟踪器无需显式的运动模型，即可集成来自多摄像头和LiDAR的多模态鸟瞰图(BEV)融合特征与边界框。在跟踪前，通过时间平滑器在移动窗口内细化边界框序列，以优化轨迹、降低抖动并提高空间一致性。", "result": "在nuScenes和KITTI数据集上的评估显示，FutrTrack相对于之前的单传感器方法，在多模态传感器特征的支持下，其基于查询的transformer跟踪方法有显著提升。在nuScenes测试集上达到74.7的aMOTA，显示出在3D多目标跟踪基准上的强大性能，同时减少了身份切换并保持了相对较高的精度。", "conclusion": "FutrTrack展示了通过引入多模态数据融合和backbone优化，可以显著提升基于transformer的跟踪器性能，使其能够与其它神经网络方法竞争，即使在数据有限且没有预训练的情况下。"}}
{"id": "2510.19892", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.19892", "abs": "https://arxiv.org/abs/2510.19892", "authors": ["Nishant Balepur", "Dang Nguyen", "Dayeon Ki"], "title": "Can They Dixit? Yes they Can! Dixit as a Playground for Multimodal Language Model Capabilities", "comment": "Accepted as a Spotlight paper at the EMNLP 2025 Wordplay Workshop", "summary": "Multi-modal large language models (MLMs) are often assessed on static,\nindividual benchmarks -- which cannot jointly assess MLM capabilities in a\nsingle task -- or rely on human or model pairwise comparisons -- which is\nhighly subjective, expensive, and allows models to exploit superficial\nshortcuts (e.g., verbosity) to inflate their win-rates. To overcome these\nissues, we propose game-based evaluations to holistically assess MLM\ncapabilities. Games require multiple abilities for players to win, are\ninherently competitive, and are governed by fix, objective rules, and makes\nevaluation more engaging, providing a robust framework to address the\naforementioned challenges. We manifest this evaluation specifically through\nDixit, a fantasy card game where players must generate captions for a card that\ntrick some, but not all players, into selecting the played card. Our\nquantitative experiments with five MLMs show Dixit win-rate rankings are\nperfectly correlated with those on popular MLM benchmarks, while games between\nhuman and MLM players in Dixit reveal several differences between agent\nstrategies and areas of improvement for MLM reasoning.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2510.20011", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.20011", "abs": "https://arxiv.org/abs/2510.20011", "authors": ["Kushan Choudhury", "Shubhrodeep Roy", "Ankur Chanda", "Shubhajit Biswas", "Somenath Kuiry"], "title": "Improving Predictive Confidence in Medical Imaging via Online Label Smoothing", "comment": "Accepted and presented in International Conference on Advancing\n  Science and Technologies in Health Science", "summary": "Deep learning models, especially convolutional neural networks, have achieved\nimpressive results in medical image classification. However, these models often\nproduce overconfident predictions, which can undermine their reliability in\ncritical healthcare settings. While traditional label smoothing offers a simple\nway to reduce such overconfidence, it fails to consider relationships between\nclasses by treating all non-target classes equally. In this study, we explore\nthe use of Online Label Smoothing (OLS), a dynamic approach that adjusts soft\nlabels throughout training based on the model's own prediction patterns. We\nevaluate OLS on the large-scale RadImageNet dataset using three widely used\narchitectures: ResNet-50, MobileNetV2, and VGG-19. Our results show that OLS\nconsistently improves both Top-1 and Top-5 classification accuracy compared to\nstandard training methods, including hard labels, conventional label smoothing,\nand teacher-free knowledge distillation. In addition to accuracy gains, OLS\nleads to more compact and well-separated feature embeddings, indicating\nimproved representation learning. These findings suggest that OLS not only\nstrengthens predictive performance but also enhances calibration, making it a\npractical and effective solution for developing trustworthy AI systems in the\nmedical imaging domain.", "AI": {"tldr": "在本研究中，探讨了在线标签平滑(OLS)技术的使用，这是一种在训练过程中基于模型预测模式动态调整软标签的方法。结果表明，在RadImageNet数据集上使用ResNet-50, MobileNetV2和VGG-19三种架构评估下，OLS方法提高了分类准确度，并显示出更紧凑且分离度高的特征嵌入，增强了表示学习和预测的校准度，从而成为在医学影像领域开发可信AI系统的有效方法。", "motivation": "深度学习模型，特别是卷积神经网络，在医学影像分类中取得了显著效果。然而，这些模型经常产生过度自信的预测，可能会降低其在关键医疗保健环境中可靠性。虽然传统的标签平滑技术提供了一种减少过度自信的方法，但它将所有非目标类同等对待，未能考虑类间的关联。", "method": "Structure", "result": "{\n  \"tldr\": \"在线标签平滑(OLS)在RadImageNet数据集上分别使用ResNet-50, MobileNetV2和VGG-19三种架构进行评估，结果显示OLS在训练过程中提高了分类准确度，并增强了特征表示学习和预测的校准度，是提高医学影像领域AI系统可靠性的有效方法。\", \n  \"motivation\": \"深度学习模型产生的过度自信预测可能会在医疗保健环境中削弱其可靠性，因此，研究人员探索了在线标签平滑技术(OLS)，这是一种在训练过程中根据模型预测模式动态调整软标签的方法，以解决标签光滑技术未能考虑类间关系的问题。\", \n  \"method\": \"研究在大规模RadImageNet数据集上使用了ResNet-50, MobileNetV2和VGG-19三种架构来评估在线标签平滑(OLS)的效能。\", \n  \"result\": \"OLS相对于标准训练方法，包括硬标签、传统标签平滑和无教师知识蒸馏，显著提高了Top-1和Top-5分类准确性，并产生了更紧凑且分离度高的特征嵌入，显示了其在提升表现和校准方面的有效性。\", \n  \"conclusion\": \"研究结果表明，OLS不仅能提高预测性能，还能提高校准度，适用于开发医学影像领域的可信AI系统。这一发现证明了OLS作为一种增强预测精度和改善特征表示的方法是高效且实用的。\"}\n}", "conclusion": "OLS可以帮助提高医学影像分类中的分类准确度，并通过改善特征表示学习和提高预测系统的校准度来增强模型的可靠性。这些结果表明OLS是医学影像AI系统的一个实际而有效的解决方案。"}}
{"id": "2510.19895", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.19895", "abs": "https://arxiv.org/abs/2510.19895", "authors": ["Guoyun Zhang"], "title": "Large Language Model enabled Mathematical Modeling", "comment": null, "summary": "The integration of Large Language Models (LLMs) with optimization modeling\noffers a promising avenue for advancing decision-making in operations research\n(OR). Traditional optimization methods,such as linear programming, mixed\ninteger programming, and simulation depend heavily on domain expertise to\ntranslate real-world problems into solvable mathematical models. While solvers\nlike Gurobi and COPT are powerful, expert input remains essential for defining\nobjectives, constraints, and variables. This research investigates the\npotential of LLMs, specifically the DeepSeek-R1 model, to bridge this\nformulation gap using natural language understanding and code generation.\nAlthough prior models like GPT-4, Claude, and Bard have shown strong\nperformance in NLP and reasoning tasks, their high token costs and tendency\ntoward hallucinations limit real-world applicability in supply chain contexts.\nIn contrast, DeepSeek-R1, a cost-efficient and high-performing model trained\nwith reinforcement learning, presents a viable alternative. Despite its success\nin benchmarks such as LiveCodeBench and Math-500, its effectiveness in applied\nOR scenarios remains under explored. This study systematically evaluates\nDeepSeek-R1 across four key OR benchmarks: NL4OPT, IndustryOR, EasyLP, and\nComplexOR. Our methodology includes baseline assessments, the development of a\nhallucination taxonomy, and the application of mitigation strategies like\nLLM-as-a-Judge, Few-shot Learning (FSL), Tool Calling, and a Multi-agent\nFramework. These techniques aim to reduce hallucinations, enhance formulation\naccuracy, and better align model outputs with user intent.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2510.20016", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.20016", "abs": "https://arxiv.org/abs/2510.20016", "authors": ["Neema Jakisa Owor", "Joshua Kofi Asamoah", "Tanner Wambui Muturi", "Anneliese Jakisa Owor", "Blessing Agyei Kyem", "Andrews Danyo", "Yaw Adu-Gyamfi", "Armstrong Aboah"], "title": "A Unified Detection Pipeline for Robust Object Detection in Fisheye-Based Traffic Surveillance", "comment": "The paper was accepted at ICCV 2025 and published in CVF database", "summary": "Fisheye cameras offer an efficient solution for wide-area traffic\nsurveillance by capturing large fields of view from a single vantage point.\nHowever, the strong radial distortion and nonuniform resolution inherent in\nfisheye imagery introduce substantial challenges for standard object detectors,\nparticularly near image boundaries where object appearance is severely\ndegraded. In this work, we present a detection framework designed to operate\nrobustly under these conditions. Our approach employs a simple yet effective\npre and post processing pipeline that enhances detection consistency across the\nimage, especially in regions affected by severe distortion. We train several\nstate-of-the-art detection models on the fisheye traffic imagery and combine\ntheir outputs through an ensemble strategy to improve overall detection\naccuracy. Our method achieves an F1 score of0.6366 on the 2025 AI City\nChallenge Track 4, placing 8thoverall out of 62 teams. These results\ndemonstrate the effectiveness of our framework in addressing issues inherent to\nfisheye imagery.", "AI": {"tldr": "本文提出了一种能在鱼眼摄像机图像中稳定运行的检测框架，通过处理流水线增强检测一致性，并通过模型集成提高精度，在竞赛中取得了良好成绩。", "motivation": "针孔摄像机的鱼眼镜头提供了一种高效的解决方案，用于广泛区域的交通监控，但是鱼眼图像中的径向失真和非均匀分辨率给标准对象检测器带来挑战。尤其是在受严重扭曲影响的图像边界区域对象的外观被严重降解。", "method": "我们的方法使用了一个简单的预处理和后处理流水线，以增强检测一致性，特别是在受严重扭曲影响的区域。我们对几种最先进的检测模型进行了训练，并通过组合它们的输出来提高整体检测准确性。", "result": "我们的方法在2025 AI City Challenge Track 4中取得了0.6366的F1分数，排名第8，总共有62支队伍参赛。", "conclusion": "这些结果证明了我们的框架在解决鱼眼图像特有的问题方面是有效的。"}}
{"id": "2510.19897", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.19897", "abs": "https://arxiv.org/abs/2510.19897", "authors": ["Jackson Hassell", "Dan Zhang", "Hannah Kim", "Tom Mitchell", "Estevam Hruschka"], "title": "Learning from Supervision with Semantic and Episodic Memory: A Reflective Approach to Agent Adaptation", "comment": "11 pages", "summary": "We investigate how agents built on pretrained large language models can learn\ntarget classification functions from labeled examples without parameter\nupdates. While conventional approaches like fine-tuning are often costly,\ninflexible, and opaque, we propose a memory-augmented framework that leverages\nboth labeled data and LLM-generated critiques. Our framework uses episodic\nmemory to store instance-level critiques-capturing specific past\nexperiences-and semantic memory to distill these into reusable, task-level\nguidance. Across a diverse set of tasks, incorporating critiques yields up to a\n24.8 percent accuracy improvement over retrieval-based (RAG-style) baselines\nthat rely only on labels. Through extensive empirical evaluation, we uncover\ndistinct behavioral differences between OpenAI and opensource models,\nparticularly in how they handle fact-oriented versus preference-based data. To\ninterpret how models respond to different representations of supervision\nencoded in memory, we introduce a novel metric, suggestibility. This helps\nexplain observed behaviors and illuminates how model characteristics and memory\nstrategies jointly shape learning dynamics. Our findings highlight the promise\nof memory-driven, reflective learning for building more adaptive and\ninterpretable LLM agents.", "AI": {"tldr": "提出了一种新的记忆增强框架，通过结合标签数据和LLM生成的批评意见，显著提升了各类任务的准确率，并解释了不同模型面对不同类型数据时的行为差异。", "motivation": "研究了基于预训练大型语言模型的智能体如何从标记示例中学习目标分类函数，而无需参数更新。传统的微调方法往往代价高昂、不灵活且不透明。", "method": "提出了一种结合标签数据和LLM生成的批评意见的记忆增强框架。该框架使用情节记忆存储实例级别的批评意见，用语义记忆提炼可重复使用的任务级别指导。", "result": "在多样性任务中，引入批评意见使准确率最高提高了24.8%，相比仅依赖标签的检索基线。不同模型（如OpenAI和开源模型）在事实导向和偏好导向数据上的行为有显著差异。", "conclusion": "研究结果表明，记忆驱动的反思学习对于构建更适应和可解释的LLM代理具有潜力。"}}
{"id": "2510.20027", "categories": ["cs.CV", "cs.GR"], "pdf": "https://arxiv.org/pdf/2510.20027", "abs": "https://arxiv.org/abs/2510.20027", "authors": ["Damian Bowness", "Charalambos Poullis"], "title": "Extreme Views: 3DGS Filter for Novel View Synthesis from Out-of-Distribution Camera Poses", "comment": null, "summary": "When viewing a 3D Gaussian Splatting (3DGS) model from camera positions\nsignificantly outside the training data distribution, substantial visual noise\ncommonly occurs. These artifacts result from the lack of training data in these\nextrapolated regions, leading to uncertain density, color, and geometry\npredictions from the model.\n  To address this issue, we propose a novel real-time render-aware filtering\nmethod. Our approach leverages sensitivity scores derived from intermediate\ngradients, explicitly targeting instabilities caused by anisotropic\norientations rather than isotropic variance. This filtering method directly\naddresses the core issue of generative uncertainty, allowing 3D reconstruction\nsystems to maintain high visual fidelity even when users freely navigate\noutside the original training viewpoints.\n  Experimental evaluation demonstrates that our method substantially improves\nvisual quality, realism, and consistency compared to existing Neural Radiance\nField (NeRF)-based approaches such as BayesRays. Critically, our filter\nseamlessly integrates into existing 3DGS rendering pipelines in real-time,\nunlike methods that require extensive post-hoc retraining or fine-tuning.\n  Code and results at https://damian-bowness.github.io/EV3DGS", "AI": {"tldr": "提出了一种实时渲染感知滤波方法，解决了从训练数据分布之外的相机位置查看3D Gaussian Splatting模型时出现的视觉噪声问题，提升了视觉质量、真实感和一致性。", "motivation": "解决从训练数据分布之外的相机位置查看3D Gaussian Splatting模型时出现的大规模视觉噪声，这些噪声来自于模型在这些外推区域的不确性预测。", "method": "通过利用从中间梯度导出的灵敏度分数提出了一种新方法，这种方法明确针对由各向异性定向而非各向同性变异性引起的不稳定。", "result": "实验表明这种方法在视觉质量、真实感和一致性方面显著优于现有的基于NeRF的方法。", "conclusion": "提出的方法能够在实时渲染中无缝集成到现有的3D Gaussian Splatting渲染管道中，而且不需要大量的后加工重新训练或微调。"}}
{"id": "2510.19967", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.19967", "abs": "https://arxiv.org/abs/2510.19967", "authors": ["Le Ren", "Xiangjian Zeng", "Qingqiang Wu", "Ruoxuan Liang"], "title": "LyriCAR: A Difficulty-Aware Curriculum Reinforcement Learning Framework For Controllable Lyric Translation", "comment": "submitted to ICASSP 2026", "summary": "Lyric translation is a challenging task that requires balancing multiple\nmusical constraints. Existing methods often rely on hand-crafted rules and\nsentence-level modeling, which restrict their ability to internalize\nmusical-linguistic patterns and to generalize effectively at the paragraph\nlevel, where cross-line coherence and global rhyme are crucial. In this work,\nwe propose LyriCAR, a novel framework for controllable lyric translation that\noperates in a fully unsupervised manner. LyriCAR introduces a difficulty-aware\ncurriculum designer and an adaptive curriculum strategy, ensuring efficient\nallocation of training resources, accelerating convergence, and improving\noverall translation quality by guiding the model with increasingly complex\nchallenges. Extensive experiments on the EN-ZH lyric translation task show that\nLyriCAR achieves state-of-the-art results across both standard translation\nmetrics and multi-dimensional reward scores, surpassing strong baselines.\nNotably, the adaptive curriculum strategy reduces training steps by nearly 40%\nwhile maintaining superior performance. Code, data and model can be accessed at\nhttps://github.com/rle27/LyriCAR.", "AI": {"tldr": "论文提出了一种名为LyriCAR的无监督歌词翻译框架，通过自适应课程策略高效地提高了翻译质量。在多个评价指标上表现优于其他方法，并且训练效率更高。", "motivation": "现有的翻译方法依赖于手工规则和句子级别的建模，这限制了它们学习音乐-语言模式和在段落级别有效泛化的能力。特别是在段落级别，跨行连贯性和全局韵律尤为重要。", "method": "我们提出了一种名为LyriCAR的新框架，用于可控制的歌词翻译。该框架采用一种难度感知的课程设计者和自适应课程策略，确保了训练资源的高效分配、加速了模型的收敛速度，并提高了整体翻译质量。", "result": "在英文到中文的歌词翻译任务中，LyriCAR在标准翻译指标和多维奖励分数上均达到了最先进的结果，并超越了强大的基线。自适应课程策略将训练步骤减少了近40%，同时保持了优越性能。", "conclusion": "LyriCAR通过采用自适应课程策略，在不使用监督学习的情况下实现了高效的歌词翻译，特别是在处理复杂挑战时，性能尤为出色。"}}
{"id": "2510.20029", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.20029", "abs": "https://arxiv.org/abs/2510.20029", "authors": ["Shengyu Chen", "Shihang Feng", "Yi Luo", "Xiaowei Jia", "Youzuo Lin"], "title": "BrainPuzzle: Hybrid Physics and Data-Driven Reconstruction for Transcranial Ultrasound Tomography", "comment": "13 pages", "summary": "Ultrasound brain imaging remains challenging due to the large difference in\nsound speed between the skull and brain tissues and the difficulty of coupling\nlarge probes to the skull. This work aims to achieve quantitative transcranial\nultrasound by reconstructing an accurate speed-of-sound (SoS) map of the brain.\nTraditional physics-based full-waveform inversion (FWI) is limited by weak\nsignals caused by skull-induced attenuation, mode conversion, and phase\naberration, as well as incomplete spatial coverage since full-aperture arrays\nare clinically impractical. In contrast, purely data-driven methods that learn\ndirectly from raw ultrasound data often fail to model the complex nonlinear and\nnonlocal wave propagation through bone, leading to anatomically plausible but\nquantitatively biased SoS maps under low signal-to-noise and sparse-aperture\nconditions. To address these issues, we propose BrainPuzzle, a hybrid two-stage\nframework that combines physical modeling with machine learning. In the first\nstage, reverse time migration (time-reversal acoustics) is applied to\nmulti-angle acquisitions to produce migration fragments that preserve\nstructural details even under low SNR. In the second stage, a transformer-based\nsuper-resolution encoder-decoder with a graph-based attention unit (GAU) fuses\nthese fragments into a coherent and quantitatively accurate SoS image. A\npartial-array acquisition strategy using a movable low-count transducer set\nimproves feasibility and coupling, while the hybrid algorithm compensates for\nthe missing aperture. Experiments on two synthetic datasets show that\nBrainPuzzle achieves superior SoS reconstruction accuracy and image\ncompleteness, demonstrating its potential for advancing quantitative ultrasound\nbrain imaging.", "AI": {"tldr": "针对传统经颅超声脑成像的挑战，提出了一种名为BrainPuzzle的混合物理建模与机器学习框架，通过两阶段处理生成更准确的脑组织速度图像，提高了图像质量和空间覆盖范围。", "motivation": "由于颅骨和脑组织之间声速的巨大差异以及大探头与颅骨耦合的困难，经颅超声脑成像依然充满挑战。此工作旨在通过重建精确的脑速图来实现定量经颅超声。传统的基于物理的全波形反演方法受到颅骨引起的衰减、模态转换和相位畸变产生的弱信号限制，而且由于全孔径阵列在临床上不可行，限制了空间覆盖率。而纯粹的数据驱动方法通常无法准确模拟通过骨骼的复杂非线性和非局部波传播，导致图像定量偏差。", "method": "提出了一种名为BrainPuzzle的两阶段混合框架，结合物理建模与机器学习。第一阶段使用反向时间迁移技术，从多角度采集的数据生成保留结构细节的迁移片段。第二阶段使用基于Transformer的超分辨率编解码器，通过图注意力单元(GAU)将这些片段融合成一个连贯且定量准确的速度图像。此外，还采用部分阵列采集策略，利用可移动的低数量换能器集合，提高可行性和耦合效果。", "result": "实验结果表明BrainPuzzle在两个合成数据集上取得了比现有方法更好的速度重建精度和图像完整性。", "conclusion": "实验结果表明，BrainPuzzle在两个合成数据集上实现了卓越的速度重建精度和图像完整性，展示了它在推进定量超声脑成像方面的潜力。"}}
{"id": "2510.19988", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.19988", "abs": "https://arxiv.org/abs/2510.19988", "authors": ["Xin Lian", "Kenneth D. Forbus"], "title": "LLM-Augmented Symbolic NLU System for More Reliable Continuous Causal Statement Interpretation", "comment": "18 pages, 2 figures", "summary": "Despite the broad applicability of large language models (LLMs), their\nreliance on probabilistic inference makes them vulnerable to errors such as\nhallucination in generated facts and inconsistent output structure in natural\nlanguage understanding (NLU) tasks. By contrast, symbolic NLU systems provide\ninterpretable understanding grounded in curated lexicons, semantic resources,\nand syntactic & semantic interpretation rules. They produce relational\nrepresentations that can be used for accurate reasoning and planning, as well\nas incremental debuggable learning. However, symbolic NLU systems tend to be\nmore limited in coverage than LLMs and require scarce knowledge representation\nand linguistics skills to extend and maintain. This paper explores a hybrid\napproach that integrates the broad-coverage language processing of LLMs with\nthe symbolic NLU capabilities of producing structured relational\nrepresentations to hopefully get the best of both approaches. We use LLMs for\nrephrasing and text simplification, to provide broad coverage, and as a source\nof information to fill in knowledge gaps more automatically. We use symbolic\nNLU to produce representations that can be used for reasoning and for\nincremental learning. We evaluate this approach on the task of extracting and\ninterpreting quantities and causal laws from commonsense science texts, along\nwith symbolic- and LLM-only pipelines. Our results suggest that our hybrid\nmethod works significantly better than the symbolic-only pipeline.", "AI": {"tldr": "论文提出了一种混合方法，将大语言模型和符号NLU系统的优点相结合，提升自然语言处理的效果，并在特定任务上验证了该方法的有效性。", "motivation": "论文动机在于解决大语言模型因依赖概率推理而产生的生成事实幻觉和自然语言理解任务中的输出结构不一致问题。同时解决符号NLU系统的知识表示和语言学技能扩展维护的稀缺性问题，以期达到两种方法的最优结合。", "method": "此论文提出了一种混合方法，结合了大语言模型（LLMs）的广泛语言处理能力和符号NLU系统的结构化关系表示生成能力。LLMs被用来进行文本的重述和简化，利用其广泛覆盖的优势，以自动填补知识缺口。符号NLU用于生成可用于推理和增量学习的表示。", "result": "该论文在从常识科学文本中提取和解释数量和因果法则的任务中评估了这种方法，与仅使用符号方法或LLMs的管道相比，其结果显示该混合方法显著优于仅使用符号的方法。", "conclusion": "论文得出结论，所提出的混合方法在处理从常识科学文本中提取和解释信息的任务上优于单独使用符号NLU系统的方法。这种方法结合了大语言模型的广泛覆盖和符号NLU系统的结构化表示生成能力。"}}
{"id": "2510.20042", "categories": ["cs.CV", "I.2.10; I.2.6; I.4.9"], "pdf": "https://arxiv.org/pdf/2510.20042", "abs": "https://arxiv.org/abs/2510.20042", "authors": ["Huichan Seo", "Sieun Choi", "Minki Hong", "Yi Zhou", "Junseo Kim", "Lukman Ismaila", "Naome Etori", "Mehul Agarwal", "Zhixuan Liu", "Jihie Kim", "Jean Oh"], "title": "Exposing Blindspots: Cultural Bias Evaluation in Generative Image Models", "comment": "28 pages, 8 figures. Submitted to the Second Conference of the\n  International Association for Safe and Ethical Artificial Intelligence\n  (IASEAI '26)", "summary": "Generative image models produce striking visuals yet often misrepresent\nculture. Prior work has examined cultural bias mainly in text-to-image (T2I)\nsystems, leaving image-to-image (I2I) editors underexplored. We bridge this gap\nwith a unified evaluation across six countries, an 8-category/36-subcategory\nschema, and era-aware prompts, auditing both T2I generation and I2I editing\nunder a standardized protocol that yields comparable diagnostics. Using open\nmodels with fixed settings, we derive cross-country, cross-era, and\ncross-category evaluations. Our framework combines standard automatic metrics,\na culture-aware retrieval-augmented VQA, and expert human judgments collected\nfrom native reviewers. To enable reproducibility, we release the complete image\ncorpus, prompts, and configurations. Our study reveals three findings: (1)\nunder country-agnostic prompts, models default to Global-North, modern-leaning\ndepictions that flatten cross-country distinctions; (2) iterative I2I editing\nerodes cultural fidelity even when conventional metrics remain flat or improve;\nand (3) I2I models apply superficial cues (palette shifts, generic props)\nrather than era-consistent, context-aware changes, often retaining source\nidentity for Global-South targets. These results highlight that\nculture-sensitive edits remain unreliable in current systems. By releasing\nstandardized data, prompts, and human evaluation protocols, we provide a\nreproducible, culture-centered benchmark for diagnosing and tracking cultural\nbias in generative image models.", "AI": {"tldr": "本研究针对T2I和I2I系统在六个国家中的文化偏差进行评估，发现存在偏向全球北部、现代化的倾向，以及在I2I编辑中文化保真度下降的问题。", "motivation": "研究的动机在于填补对I2I编辑系统文化偏差研究的空白，通过在六个国家展开统一的评估，揭示现有系统的文化敏感性不足。", "method": "本研究使用了一种统一的评估框架，评估了六个国家的文本到图像(T2I)和图像到图像(I2I)编辑系统。框架结合了标准自动度量、文化感知的检索增强VQA和来自本土评论家的专家人类判断。", "result": "研究揭示了三个主要发现：1) 在无特定国家提示下，模型偏向于展示全球化北、现代化的图像，降低了各国之间的区别；2) 迭代I2I编辑会侵蚀文化保真度；3) I2I模型应用的是表面道具变更，而非与时代和语境一致的变化。", "conclusion": "本研究通过提供标准化的数据、提示和人类评估协议，为诊断和追踪生成图像模型中的文化偏差提供了可复现的文化中心基准。这些结果强调了在现有系统中的文化敏感性编辑仍然不可靠。"}}
{"id": "2510.19996", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.19996", "abs": "https://arxiv.org/abs/2510.19996", "authors": ["Michael A. Covington"], "title": "A Fundamental Algorithm for Dependency Parsing (With Corrections)", "comment": "Corrected version of an already widely cited paper", "summary": "This paper presents a fundamental algorithm for parsing natural language\nsentences into dependency trees. Unlike phrase-structure (constituency)\nparsers, this algorithm operates one word at a time, attaching each word as\nsoon as it can be attached, corresponding to properties claimed for the parser\nin the human brain. Like phrase-structure parsing, its worst-case complexity is\n$O(n^3)$, but in human language, the worst case occurs only for small $n$.", "AI": {"tldr": "本文提出了一种基本的算法，用于将自然语言句子解析为依赖树。该算法逐词进行，每词一到即刻附加，类似于人类大脑中的解析机制。其最坏情况复杂度为O(n^3)，但在人类语言中，最坏情况仅发生在较小的n值时。", "motivation": "研究的动机在于探索一种更接近人类大脑解析语言机制的算法，同时也关心其计算复杂性在实际语言应用中的表现。", "method": "这是一种逐词解析自然语言句子成依赖树的算法，即每词一到便即时附加，而不是等待完整短语结构的形成。", "result": "最坏情况复杂度尽管为O(n^3)，但在处理人类实际语言时，该复杂度仅在n值较小的情况下发生。", "conclusion": "这项研究展示了一种更加贴近人类大脑语言解析机制的算法，同时也指出了算法在实际应用中的有效性和局限性。"}}
{"id": "2510.20071", "categories": ["cs.CV", "I.4.1"], "pdf": "https://arxiv.org/pdf/2510.20071", "abs": "https://arxiv.org/abs/2510.20071", "authors": ["Bernd Pfrommer"], "title": "Filter-Based Reconstruction of Images from Events", "comment": null, "summary": "Reconstructing an intensity image from the events of a moving event camera is\na challenging task that is typically approached with neural networks deployed\non graphics processing units. This paper presents a much simpler, FIlter Based\nAsynchronous Reconstruction method (FIBAR). First, intensity changes signaled\nby events are integrated with a temporal digital IIR filter. To reduce\nreconstruction noise, stale pixels are detected by a novel algorithm that\nregulates a window of recently updated pixels. Arguing that for a moving\ncamera, the absence of events at a pixel location likely implies a low image\ngradient, stale pixels are then blurred with a Gaussian filter. In contrast to\nmost existing methods, FIBAR is asynchronous and permits image read-out at an\narbitrary time. It runs on a modern laptop CPU at about 42(140) million\nevents/s with (without) spatial filtering enabled. A few simple qualitative\nexperiments are presented that show the difference in image reconstruction\nbetween FIBAR and a neural network-based approach (FireNet). FIBAR's\nreconstruction is noisier than neural network-based methods and suffers from\nghost images. However, it is sufficient for certain tasks such as the detection\nof fiducial markers. Code is available at\nhttps://github.com/ros-event-camera/event_image_reconstruction_fibar", "AI": {"tldr": "本文介绍了一种简易的异步图像重建方法——FIBAR，通过时间数字IIR滤波器融合事件信号的强度变化，显著特性和优势在于异步性，重建噪声的问题依然存在。", "motivation": "重建事件相机的强度图像是一个非常具有挑战性的任务，通常依靠部署在图形处理单元上的神经网络实现。本篇论文提出了一种更为简单的方法－基于滤波器的异步重建方法FIBAR。", "method": "FIBAR方法首先通过一个时间数字IIR滤波器整合由事件信号发出的强度变化。为了减少重建噪声，该方法使用了一种新颖的算法来检测旧像素，该算法调节最近更新的像素窗口。鉴于对于一个运动中的相机，如果一个像素位置没有事件，通常意味着较低的图像梯度，这些旧像素会被高斯滤波器模糊化。与许多现有的方法不同，FIBAR是异步的，允许在任意时刻进行图像读出。", "result": "FIBAR在现代笔记本电脑CPU上运行大约能处理42（140）百万事件/秒，带（不带）空间滤波。研究表明，FIBAR的重建结果比基于神经网络的方法更为噪杂，存在鬼影图像的问题。然而，对于检测特征标记等任务来说，它所提供的重建结果已经足够。", "conclusion": "虽然FIBAR方法重建的图像噪声较大，并且存在鬼影图像的缺陷，但从另一角度而言，这种方法是异步的，允许任意时刻进行图像读取，在某些特定任务中，例如特征标记的检测，依然能提供实用性的图像重建结果。"}}
{"id": "2510.20001", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.20001", "abs": "https://arxiv.org/abs/2510.20001", "authors": ["Yunpeng Xiao", "Carl Yang", "Mark Mai", "Xiao Hu", "Kai Shu"], "title": "Beyond MedQA: Towards Real-world Clinical Decision Making in the Era of LLMs", "comment": "13 pages, 3 figures", "summary": "Large language models (LLMs) show promise for clinical use. They are often\nevaluated using datasets such as MedQA. However, Many medical datasets, such as\nMedQA, rely on simplified Question-Answering (Q\\A) that underrepresents\nreal-world clinical decision-making. Based on this, we propose a unifying\nparadigm that characterizes clinical decision-making tasks along two\ndimensions: Clinical Backgrounds and Clinical Questions. As the background and\nquestions approach the real clinical environment, the difficulty increases. We\nsummarize the settings of existing datasets and benchmarks along two\ndimensions. Then we review methods to address clinical decision-making,\nincluding training-time and test-time techniques, and summarize when they help.\nNext, we extend evaluation beyond accuracy to include efficiency,\nexplainability. Finally, we highlight open challenges. Our paradigm clarifies\nassumptions, standardizes comparisons, and guides the development of clinically\nmeaningful LLMs.", "AI": {"tldr": "The paper proposes a unifying paradigm for clinical decision-making tasks to improve the evaluation of large language models in clinical use.", "motivation": "Many medical datasets simplify clinical decision-making with Question-Answering, which does not represent real-world scenarios well. This paper aims to address this gap and improve the evaluation of large language models (LLMs) in clinical use.", "method": "We propose a unifying paradigm to characterize clinical decision-making tasks based on Clinical Backgrounds and Clinical Questions dimensions. We also review existing methods to address clinical decision-making and extend the evaluation criteria beyond accuracy.", "result": "The paper presents a framework to categorize the difficulty of clinical decision-making tasks and discusses the effectiveness of various methods to improve LLMs' performance in these settings.", "conclusion": "The paradigm provides a standardized way to compare and develop clinically meaningful LLMs by clarifying assumptions and extending evaluation criteria."}}
{"id": "2510.20077", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.20077", "abs": "https://arxiv.org/abs/2510.20077", "authors": ["Hui Chen", "Xinjie Wang", "Xianchao Xiu", "Wanquan Liu"], "title": "Data-Adaptive Transformed Bilateral Tensor Low-Rank Representation for Clustering", "comment": null, "summary": "Tensor low-rank representation (TLRR) has demonstrated significant success in\nimage clustering. However, most existing methods rely on fixed transformations\nand suffer from poor robustness to noise. In this paper, we propose a novel\ntransformed bilateral tensor low-rank representation model called TBTLRR, which\nintroduces a data-adaptive tensor nuclear norm by learning arbitrary unitary\ntransforms, allowing for more effective capture of global correlations. In\naddition, by leveraging the bilateral structure of latent tensor data, TBTLRR\nis able to exploit local correlations between image samples and features.\nFurthermore, TBTLRR integrates the $\\ell_{1/2}$-norm and Frobenius norm\nregularization terms for better dealing with complex noise in real-world\nscenarios. To solve the proposed nonconvex model, we develop an efficient\noptimization algorithm inspired by the alternating direction method of\nmultipliers (ADMM) and provide theoretical convergence. Extensive experiments\nvalidate its superiority over the state-of-the-art methods in clustering. The\ncode will be available at https://github.com/xianchaoxiu/TBTLRR.", "AI": {"tldr": "本文提出了TBTLRR模型，通过自适应学习酉变换，结合双边结构和混合正则化项，提升了图像聚类的效果和噪声的鲁棒性。", "motivation": "大部分现有的张量低秩表示方法依赖于固定的变换，对噪声的鲁棒性较差。因此，作者提出了TBTLRR来解决这个问题。", "method": "提出了一个名为TBTLRR的新型变换双边张量低秩表示模型，通过学习任意酉变换引入数据自适应的张量核范数，更有效地捕捉全局相关性。利用潜在张量数据的双边结构，TBTLRR能够利用图像样本和特征之间的局部相关性。此外，TBTLRR结合了ℓ1/2范数和Frobenius范数正则项，以更好地处理现实场景中的复杂噪声。", "result": "广泛的实验验证了TBTLRR在聚类任务上优于最先进的方法。", "conclusion": "TBTLRR模型在处理复杂噪声和捕捉全局局部相关性方面表现出色，优于现有的方法。"}}
{"id": "2510.20002", "categories": ["cs.CL", "cs.AI", "68T50, 68T07, 68U35"], "pdf": "https://arxiv.org/pdf/2510.20002", "abs": "https://arxiv.org/abs/2510.20002", "authors": ["Alexandra Apostolopoulou", "Konstantinos Kanaris", "Athanasios Koursaris", "Dimitris Tsakalidis", "George Domalis", "Ioannis E. Livieris"], "title": "Forging GEMs: Advancing Greek NLP through Quality-Based Corpus Curation and Specialized Pre-training", "comment": null, "summary": "The advancement of natural language processing for morphologically rich,\nmoderately-resourced languages like Modern Greek is often hindered by a\nfragmented research landscape, a lack of architectural diversity and reliance\non limited context-length models. This is particularly true in specialized,\nhigh-value domains such as law, where existing models are frequently confined\nto early transformer architectures with a restrictive 512-token window,\ninsufficient for analyzing long legal documents. To address these challenges,\nthis paper presents Greek Embedding Models, a new family of transformer models\nfor Greek language built upon a foundation of extensive, quality-driven data\ncuration. We detail the construction of several large-scale Greek corpora,\nemphasizing a rigorous, quality-based filtering and preprocessing methodology\nto create high-value training datasets from both general-domain and specialized\nlegal sources. On this carefully curated foundation, we pre-train and\nsystematically evaluate a diverse suite of modern architectures, which has not\npreviously applied to Greek language, such as ELECTRA, ConvBERT and ModernBERT.\nFurthermore, we propose the first bilingual Greek-English Embedding Models\ntailored for the legal domain. The extensive experiments on downstream tasks\ndemonstrate that the new class of models establish the effectiveness of the\nproposed approach, highlighting that the GEM-RoBERTa and GEM-ConvBERT models\nsignificantly outperform existing baselines.", "AI": {"tldr": "文章提出了针对希腊语的新型嵌入模型（GEM），其基于高质量的数据处理和训练，包括一系列现代化架构模型，并在特定下游任务中展示了优于现有模型的性能。", "motivation": "由于希腊语这一形态丰富的中等资源语言自然语言处理领域的研究零散、架构缺乏多样性、且常依赖于512令牌窗口的早期transformer架构模型，无法足够有效处理法律中的长长文档，从而阻碍了技术的进步和应用。", "method": "通过构建多种大规模希腊语文本语料库，特别是在经过严格过滤和预处理的高质量数据集，以此作为基础，对现代架构模型进行预训练和系统评估。", "result": "这篇文章针对希腊语在自然语言处理中的问题进行了研究，主要聚焦于法律等专业化领域中预训练模型架构的多样性不足和上下文长度限制。作者通过构建高质量的希腊语文本语料库，并在此基础上训练了多种现代架构模型（如ELECTRA, ConvBERT 和 ModernBERT）以适应包括法律文本在内的长文档分析，还提出了第一种面向法律领域的希腊语-英语双语嵌入模型。实验结果证明了GEM-RoBERTa 和 GEM-ConvBERT 模型的有效性，超越了现有的基准模型。", "conclusion": "研究表明，在高质量数据持续训练和广泛检验下建立的GEM模型，利用如ELECTRA, ConvBERT 和 ModernBERT等架构，尤其在法律领域内展现了卓越的性能。"}}
{"id": "2510.20087", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.20087", "abs": "https://arxiv.org/abs/2510.20087", "authors": ["Lorenzo Arboit", "Dennis N. Schneider", "Britty Baby", "Vinkle Srivastav", "Pietro Mascagni", "Nicolas Padoy"], "title": "Endoshare: A Source Available Solution to De-Identify and Manage Surgical Videos", "comment": "13 pages, 6 figures. Source-available software:\n  https://camma-public.github.io/Endoshare/", "summary": "Video-based assessment and surgical data science can advance surgical\ntraining, research, and quality improvement. However, widespread use remains\nlimited by heterogeneous recording formats and privacy concerns associated with\nvideo sharing. We present Endoshare, a source-available, cross-platform\napplication for merging, standardizing, and de-identifying endoscopic videos in\nminimally invasive surgery. Development followed the software development life\ncycle with iterative, user-centered feedback. During the analysis phase, an\ninternal survey of clinicians and computer scientists based on ten usability\nheuristics identified key requirements that guided a privacy-by-design\narchitecture. In the testing phase, an external clinician survey combined the\nsame heuristics with Technology Acceptance Model constructs to assess usability\nand adoption, complemented by benchmarking across different hardware\nconfigurations. Four clinicians and four computer scientists initially tested\nthe prototype, reporting high usability (4.68 +/- 0.40/5 and 4.03 +/- 0.51/5),\nwith the lowest score (4.00 +/- 0.93/5) relating to label clarity. After\nrefinement, the testing phase surveyed ten surgeons who reported high perceived\nusefulness (5.07 +/- 1.75/7), ease of use (5.15 +/- 1.71/7), heuristic\nusability (4.38 +/- 0.48/5), and strong recommendation (9.20 +/- 0.79/10).\nProcessing time varied with processing mode, video duration (both p <= 0.001),\nand machine computational power (p = 0.041). Endoshare provides a transparent,\nuser-friendly pipeline for standardized, privacy-preserving surgical video\nmanagement. Compliance certification and broader interoperability validation\nare needed to establish it as a deployable alternative to proprietary systems.\nThe software is available at https://camma-public.github.io/Endoshare/", "AI": {"tldr": "研究者开发了Endoshare软件，用于处理内窥镜手术视频，并确保隐私安全，测试表明其高可用性和推荐度。", "motivation": "视频为基础的评估和手术数据科学可以促进手术培训，研究和质量改进，但其广泛使用受到不同录制格式和视频共享隐私担忧的限制。", "method": "介绍了一种名为Endoshare的跨平台应用，旨在整合、标准化和去识别内窥镜手术视频，该开发按照软件开发生命周期进行，通过迭代、以用户为中心的反馈进行改进。在分析阶段，通过基于十个可用性启发法的内部调查，识别关键需求，并指导隐私设计架构。测试阶段，外部临床医生调查结合了相同启发法与技术接受模型构建设，评测使用性和采用情况。", "result": "初步测试中，四位临床医生和四位计算机科学家对原型给出了高可用性评分（4.68±0.40/5和4.03±0.51/5）。经过改进，最终测试中十位外科医生报告高感知有用性（5.07±1.75/7），易用性（5.15±1.71/7），启发法可用性（4.38±0.48/5）以及强烈推荐（9.20±0.79/10）。处理时间与处理模式，视频时长（p <= 0.001）和机器计算能力（p = 0.041）相关。", "conclusion": "Endoshare提供了透明，用户友好的手术视频管理的标准化、隐私保护流程。然而，还需求合规认证以及更广泛的互操作性验证，以建立其作为可部署替代专有系统的地位。该软件可供下载。"}}
{"id": "2510.20033", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.20033", "abs": "https://arxiv.org/abs/2510.20033", "authors": ["David Dukić"], "title": "Improving Transfer Learning for Sequence Labeling Tasks by Adapting Pre-trained Neural Language Models", "comment": null, "summary": "This doctoral thesis improves the transfer learning for sequence labeling\ntasks by adapting pre-trained neural language models. The proposed improvements\nin transfer learning involve introducing a multi-task model that incorporates\nan additional signal, a method based on architectural modifications in\nautoregressive large language models, and a sequence labeling framework for\nautoregressive large language models utilizing supervised in-context\nfine-tuning combined with response-oriented adaptation strategies. The first\nimprovement is given in the context of domain transfer for the event trigger\ndetection task. The domain transfer of the event trigger detection task can be\nimproved by incorporating an additional signal obtained from a\ndomain-independent text processing system into a multi-task model. The second\nimprovement involves modifying the model's architecture. For that purpose, a\nmethod is proposed to enable bidirectional information flow across layers of\nautoregressive large language models. The third improvement utilizes\nautoregressive large language models as text generators through a generative\nsupervised in-context fine-tuning framework. The proposed model, method, and\nframework demonstrate that pre-trained neural language models achieve their\nbest performance on sequence labeling tasks when adapted through targeted\ntransfer learning paradigms.", "AI": {"tldr": "This doctoral thesis enhances sequence labeling tasks by adapting pre-trained neural language models with improved transfer learning techniques, showing improved performance over existing methods.", "motivation": "The motivation behind this research is to improve the effectiveness of sequence labeling tasks by adapting pre-trained neural language models through targeted transfer learning paradigms.", "method": "This doctoral thesis proposes three main improvements to enhance transfer learning for sequence labeling tasks: 1) Incorporating an additional signal into a multi-task model to assist in domain transfer for event trigger detection. 2) Introducing a method to enable bidirectional information flow in the architecture of autoregressive large language models. 3) Utilizing autoregressive large language models with a generative supervised in-context fine-tuning framework for sequence labeling.", "result": "The results show that pre-trained neural language models perform optimally on sequence labeling tasks when adapted using the proposed transfer learning techniques.", "conclusion": "The proposed transfer learning techniques lead to better performance for sequence labeling tasks compared to previous approaches by enhancing pre-trained neural language models."}}
{"id": "2510.20092", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.20092", "abs": "https://arxiv.org/abs/2510.20092", "authors": ["Hao Yu", "Haoyu Chen", "Yan Jiang", "Wei Peng", "Zhaodong Sun", "Samuel Kaski", "Guoying Zhao"], "title": "Attentive Convolution: Unifying the Expressivity of Self-Attention with Convolutional Efficiency", "comment": null, "summary": "Self-attention (SA) has become the cornerstone of modern vision backbones for\nits powerful expressivity over traditional Convolutions (Conv). However, its\nquadratic complexity remains a critical bottleneck for practical applications.\nGiven that Conv offers linear complexity and strong visual priors, continuing\nefforts have been made to promote the renaissance of Conv. However, a\npersistent performance chasm remains, highlighting that these modernizations\nhave not yet captured the intrinsic expressivity that defines SA. In this\npaper, we re-examine the design of the CNNs, directed by a key question: what\nprinciples give SA its edge over Conv? As a result, we reveal two fundamental\ninsights that challenge the long-standing design intuitions in prior research\n(e.g., Receptive field). The two findings are: (1) \\textit{Adaptive routing}:\nSA dynamically regulates positional information flow according to semantic\ncontent, whereas Conv employs static kernels uniformly across all positions.\n(2) \\textit{Lateral inhibition}: SA induces score competition among token\nweighting, effectively suppressing redundancy and sharpening representations,\nwhereas Conv filters lack such inhibitory dynamics and exhibit considerable\nredundancy. Based on this, we propose \\textit{Attentive Convolution} (ATConv),\na principled reformulation of the convolutional operator that intrinsically\ninjects these principles. Interestingly, with only $3\\times3$ kernels, ATConv\nconsistently outperforms various SA mechanisms in fundamental vision tasks.\nBuilding on ATConv, we introduce AttNet, a CNN family that can attain\n\\textbf{84.4\\%} ImageNet-1K Top-1 accuracy with only 27M parameters. In\ndiffusion-based image generation, replacing all SA with the proposed $3\\times\n3$ ATConv in SiT-XL/2 reduces ImageNet FID by 0.15 in 400k steps with faster\nsampling. Code is available at: github.com/price112/Attentive-Convolution.", "AI": {"tldr": "本文分析了自注意力机制优于传统卷积的原因，并创造性地提出了Attentive Convolution (ATConv)，通过引入适应性路由和支持侧向抑制的设计原则，该方法在优化卷积层中展现了显著的性能提升。", "motivation": "致力于重新审视传统卷积网络的设计，回答了赋予自注意力机制优于传统卷积的关键原则是什么的问题。现有的卷积现代化努力尚未完全捕捉到自注意力机制的内在表达力，因此该研究的目标是填补这种性能差距。", "method": "提出了一种称为Attentive Convolution (ATConv) 的卷积操作原理重构方法，该方法本质上注入了自注意力机制中的两个关键原则：适应性路由和支持侧向抑制。", "result": "在基本的视觉任务中，即使只使用3x3的内核，ATConv也一致地超过了各种自注意力机制的表现。基于ATConv构建的AttNet是一个卷积神经网络家族，可以仅以27M参数达到ImageNet-1K Top-1的84.4%的准确率。在基于扩散模型的图像生成中，用ATConv替换SiT-XL/2中的所有自注意力机制在400k步后减少了0.15的ImageNet FID，同时也加快了采样速度。", "conclusion": "该研究揭示了自注意力机制优于传统卷积的两个根本性原理，并以此为基础提出了提高卷积网络性能的Attentive Convolution方法。"}}
{"id": "2510.20036", "categories": ["cs.CL", "cs.SE"], "pdf": "https://arxiv.org/pdf/2510.20036", "abs": "https://arxiv.org/abs/2510.20036", "authors": ["Marianne Menglin Liu", "Daniel Garcia", "Fjona Parllaku", "Vikas Upadhyay", "Syed Fahad Allam Shah", "Dan Roth"], "title": "ToolScope: Enhancing LLM Agent Tool Use through Tool Merging and Context-Aware Filtering", "comment": "Preprint under review", "summary": "Large language model (LLM) agents rely on external tools to solve complex\ntasks, but real-world toolsets often contain redundant tools with overlapping\nnames and descriptions, introducing ambiguity and reducing selection accuracy.\nLLMs also face strict input context limits, preventing efficient consideration\nof large toolsets. To address these challenges, we propose ToolScope, which\nincludes: (1) ToolScopeMerger with Auto-Correction to automatically audit and\nfix tool merges, reducing redundancy, and (2) ToolScopeRetriever to rank and\nselect only the most relevant tools for each query, compressing toolsets to fit\nwithin context limits without sacrificing accuracy. Evaluations on three\nstate-of-the-art LLMs and three open-source tool-use benchmarks show gains of\n8.38% to 38.6% in tool selection accuracy, demonstrating ToolScope's\neffectiveness in enhancing LLM tool use.", "AI": {"tldr": "提出ToolScope解决大型语言模型（LLM）在使用工具集时面临的冗余和上下文限制问题，通过ToolScopeMerger和ToolScopeRetriever提高工具选择准确性。", "motivation": "解决LLM在使用包含冗余和重叠名称的工具集时由于上下文限制导致的工具选择准确性问题。", "method": "ToolScope包括ToolScopeMerger，用于自动审核和修正工具合并以减少冗余，以及ToolScopeRetriever，用于对每个查询相关工具进行排序和选择，以压缩工具集。", "result": "在三个最先进的LLM和三个开源工具使用基准测试上，工具选择准确性提高了8.38%至38.6%。", "conclusion": "ToolScope有效地提高了LLM工具使用的准确性。"}}
{"id": "2510.20093", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.20093", "abs": "https://arxiv.org/abs/2510.20093", "authors": ["Jiho Park", "Sieun Choi", "Jaeyoon Seo", "Jihie Kim"], "title": "StableSketcher: Enhancing Diffusion Model for Pixel-based Sketch Generation via Visual Question Answering Feedback", "comment": "Under review at IEEE Access. Author-submitted preprint. Not the\n  IEEE-published version", "summary": "Although recent advancements in diffusion models have significantly enriched\nthe quality of generated images, challenges remain in synthesizing pixel-based\nhuman-drawn sketches, a representative example of abstract expression. To\ncombat these challenges, we propose StableSketcher, a novel framework that\nempowers diffusion models to generate hand-drawn sketches with high prompt\nfidelity. Within this framework, we fine-tune the variational autoencoder to\noptimize latent decoding, enabling it to better capture the characteristics of\nsketches. In parallel, we integrate a new reward function for reinforcement\nlearning based on visual question answering, which improves text-image\nalignment and semantic consistency. Extensive experiments demonstrate that\nStableSketcher generates sketches with improved stylistic fidelity, achieving\nbetter alignment with prompts compared to the Stable Diffusion baseline.\nAdditionally, we introduce SketchDUO, to the best of our knowledge, the first\ndataset comprising instance-level sketches paired with captions and\nquestion-answer pairs, thereby addressing the limitations of existing datasets\nthat rely on image-label pairs. Our code and dataset will be made publicly\navailable upon acceptance.", "AI": {"tldr": "本文提出了StableSketcher，一种新的框架，用于生成具有高提示保真度的手绘素描。通过新的奖励函数和改进的数据集，提高文本-图像的一致性。", "motivation": "尽管最近在扩散模型方面取得了显著进展，但在生成像素基础的人手素描方面还存在挑战。这些素描被视为抽象表达的典型例子。本文旨在解决这些挑战。", "method": "本文提出了一种名为StableSketcher的新框架，该框架通过微调变分自编码器来优化潜在解码，使其更好地捕捉素描的特性。同时，通过基于视觉问题解答的新奖励函数来改进文本-图像的一致性和语义连贯性。", "result": "实验表明，StableSketcher生成的素描在风格保真度上有所提高，并且与提示的对齐程度优于Stable Diffusion基准。", "conclusion": "本文提出了StableSketcher框架，改进了素描生成的风格保真度和文本对齐。同时，引入了首个包含实例级素描、说明和问答对的数据集SketchDUO，该数据集将公开发布。"}}
{"id": "2510.20043", "categories": ["cs.CL", "cs.LG", "I.2.7"], "pdf": "https://arxiv.org/pdf/2510.20043", "abs": "https://arxiv.org/abs/2510.20043", "authors": ["Nafis Chowdhury", "Moinul Haque", "Anika Ahmed", "Nazia Tasnim", "Md. Istiak Hossain Shihab", "Sajjadur Rahman", "Farig Sadeque"], "title": "From Facts to Folklore: Evaluating Large Language Models on Bengali Cultural Knowledge", "comment": "4 pages", "summary": "Recent progress in NLP research has demonstrated remarkable capabilities of\nlarge language models (LLMs) across a wide range of tasks. While recent\nmultilingual benchmarks have advanced cultural evaluation for LLMs, critical\ngaps remain in capturing the nuances of low-resource cultures. Our work\naddresses these limitations through a Bengali Language Cultural Knowledge\n(BLanCK) dataset including folk traditions, culinary arts, and regional\ndialects. Our investigation of several multilingual language models shows that\nwhile these models perform well in non-cultural categories, they struggle\nsignificantly with cultural knowledge and performance improves substantially\nacross all models when context is provided, emphasizing context-aware\narchitectures and culturally curated training data.", "AI": {"tldr": "通过创建Bengali语言文化知识数据集(BLanCK)，研究了几种多语言语言模型在处理文化知识上的局限性，并指出在提供上下文时性能显著提升，强调上下文感知架构和文化定制训练数据的重要性。", "motivation": "尽管多语言基准测试对于评估LLM在文化方面的表现有所推进，但是对于低资源文化的细微差别仍存在明显不足。", "method": "创建了一个包含民间传统、烹饪艺术和地区方言的Bengali语言文化知识数据集（BLanCK），并研究了多个多语言语言模型在该数据集上的表现。", "result": "研究结果显示，这些模型在非文化类别上表现良好，但在文化知识上存在问题，当提供上下文时，所有模型的表现都显著提高。", "conclusion": "强调了上下文感知架构和文化定制训练数据对提高模型处理文化知识能力的重要性。"}}
{"id": "2510.20095", "categories": ["cs.CV", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.20095", "abs": "https://arxiv.org/abs/2510.20095", "authors": ["Ziheng Zhang", "Xinyue Ma", "Arpita Chowdhury", "Elizabeth G. Campolongo", "Matthew J. Thompson", "Net Zhang", "Samuel Stevens", "Hilmar Lapp", "Tanya Berger-Wolf", "Yu Su", "Wei-Lun Chao", "Jianyang Gu"], "title": "BIOCAP: Exploiting Synthetic Captions Beyond Labels in Biological Foundation Models", "comment": "Project page: https://imageomics.github.io/biocap/", "summary": "This work investigates descriptive captions as an additional source of\nsupervision for biological multimodal foundation models. Images and captions\ncan be viewed as complementary samples from the latent morphospace of a\nspecies, each capturing certain biological traits. Incorporating captions\nduring training encourages alignment with this shared latent structure,\nemphasizing potentially diagnostic characters while suppressing spurious\ncorrelations. The main challenge, however, lies in obtaining faithful,\ninstance-specific captions at scale. This requirement has limited the\nutilization of natural language supervision in organismal biology compared with\nmany other scientific domains. We complement this gap by generating synthetic\ncaptions with multimodal large language models (MLLMs), guided by\nWikipedia-derived visual information and taxon-tailored format examples. These\ndomain-specific contexts help reduce hallucination and yield accurate,\ninstance-based descriptive captions. Using these captions, we train BIOCAP\n(i.e., BIOCLIP with Captions), a biological foundation model that captures rich\nsemantics and achieves strong performance in species classification and\ntext-image retrieval. These results demonstrate the value of descriptive\ncaptions beyond labels in bridging biological images with multimodal foundation\nmodels.", "AI": {"tldr": "研究通过多模态大型语言模型生成描述性标题以弥补生物分类学中自然语言监督应用的不足，并训练了能够实现物种分类和文本-图像检索的生物基础模型BIOCAP。", "motivation": "研究的动机在于探索描述性标题作为生物学多模态基础模型的额外监督来源。相对于其他科学领域，自然语言监督在生物分类学中的应用受到获得忠实实例特定标题的数量限制。", "method": "该研究使用多模态大型语言模型（MLLMs）生成合成描述性标题，这些标题基于维基百科的视觉信息和特定类群的格式示例。通过这种方式，研究旨在减少标题生成过程中的虚假信息，以生成更准确、基于实例的描述性标题。", "result": "通过使用这些生成的描述性标题，研究训练了一个名为BIOCAP的生物基础模型，该模型可以捕捉丰富的语义信息，并在物种分类和文本-图像检索方面取得较强的表现。", "conclusion": "该研究结果证明，描述性标题可以在生物图像与多模态基础模型之间架起桥梁，其价值超越单纯的标签。"}}
{"id": "2510.20059", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.20059", "abs": "https://arxiv.org/abs/2510.20059", "authors": ["Mehrdad Ghassabi", "Sadra Hakim", "Hamidreza Baradaran Kashani", "Pedram Rostami"], "title": "Enhancing Reasoning Skills in Small Persian Medical Language Models Can Outperform Large-Scale Data Training", "comment": "6 pages, 4 figures", "summary": "Enhancing reasoning capabilities in small language models is critical for\nspecialized applications such as medical question answering, particularly in\nunderrepresented languages like Persian. In this study, we employ Reinforcement\nLearning with AI Feedback (RLAIF) and Direct preference optimization (DPO) to\nimprove the reasoning skills of a general-purpose Persian language model. To\nachieve this, we translated a multiple-choice medical question-answering\ndataset into Persian and used RLAIF to generate rejected-preferred answer\npairs, which are essential for DPO training. By prompting both teacher and\nstudent models to produce Chain-of-Thought (CoT) reasoning responses, we\ncompiled a dataset containing correct and incorrect reasoning trajectories.\nThis dataset, comprising 2 million tokens in preferred answers and 2.5 million\ntokens in rejected ones, was used to train a baseline model, significantly\nenhancing its medical reasoning capabilities in Persian. Remarkably, the\nresulting model outperformed its predecessor, gaokerena-V, which was trained on\napproximately 57 million tokens, despite leveraging a much smaller dataset.\nThese results highlight the efficiency and effectiveness of reasoning-focused\ntraining approaches in developing domain-specific language models with limited\ndata availability.", "AI": {"tldr": "使用强化学习和直接偏好优化来提高通用波斯语语言模型的推理能力，使其在医学问答任务中表现更佳。", "motivation": "专门的应用如医学问答，特别是在数据较少的语言如波斯语中，增强小型语言模型的推理能力至关重要。", "method": "将多选医学问答数据集翻译成波斯语，使用RLAIF生成训练DPO所需的拒绝-偏好答案对，促使教师和学生模型生成推理链反应，构建包含正确和错误推理路径的数据集。", "result": "使用包含200万个优选答案词和250万个拒绝答案词的数据集成功地训练了基准模型，显著提升了其医学推理的能力，尽管训练数据量远小于前驱模型gaokerena-V。", "conclusion": "这些结果表明，以推理为导向的训练方法在构建小型数据集下的领域特定语言模型方面是高效且有效的。"}}
{"id": "2510.20126", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.20126", "abs": "https://arxiv.org/abs/2510.20126", "authors": ["Prithvi Raj Singh", "Raju Gottumukkala", "Anthony S. Maida", "Alan B. Barhorst", "Vijaya Gopu"], "title": "Physics-Guided Fusion for Robust 3D Tracking of Fast Moving Small Objects", "comment": "13 pages, 6 figures", "summary": "While computer vision has advanced considerably for general object detection\nand tracking, the specific problem of fast-moving tiny objects remains\nunderexplored. This paper addresses the significant challenge of detecting and\ntracking rapidly moving small objects using an RGB-D camera. Our novel system\ncombines deep learning-based detection with physics-based tracking to overcome\nthe limitations of existing approaches. Our contributions include: (1) a\ncomprehensive system design for object detection and tracking of fast-moving\nsmall objects in 3D space, (2) an innovative physics-based tracking algorithm\nthat integrates kinematics motion equations to handle outliers and missed\ndetections, and (3) an outlier detection and correction module that\nsignificantly improves tracking performance in challenging scenarios such as\nocclusions and rapid direction changes. We evaluated our proposed system on a\ncustom racquetball dataset. Our evaluation shows our system surpassing kalman\nfilter based trackers with up to 70\\% less Average Displacement Error. Our\nsystem has significant applications for improving robot perception on\nautonomous platforms and demonstrates the effectiveness of combining\nphysics-based models with deep learning approaches for real-time 3D detection\nand tracking of challenging small objects.", "AI": {"tldr": "本文提出了一种结合深度学习与基于物理的追踪算法的系统，解决了快速移动小物体的检测和追踪问题，并在特定数据集上展现了优于现有方法的性能。", "motivation": "本文旨在解决快速移动的小物体检测和追踪这一尚未充分研究的问题。计算机视觉在一般物体检测和追踪方面已经取得了显著进步，但对于快速移动的小物体来说，这个问题仍然充满挑战。", "method": "本文提出了一种结合深度学习检测与基于物理的跟踪方法来解决快速移动的小物体检测和追踪问题的系统。该系统包含三个方面的主要贡献：1）三维空间中快速移动小物体检测和追踪的完整系统设计。2）结合了运动方程的创新性基于物理的追踪算法，以处理异常值和丢失的检测结果。3）一个异常值检测与纠正模块，该模块显著提高了在诸如遮挡和快速方向变化等复杂场景下的追踪性能。", "result": "通过在自制的网球数据集上进行评估，本文提出的系统在平均位移误差方面比基于卡尔曼滤波的追踪器降低了70%。", "conclusion": "本文提出的系统在机器人自主感知方面具有广泛应用前景，证明了将基于物理的模型与深度学习相结合在实现实时三维快速移动小物体检测和追踪方面是有效的。"}}
{"id": "2510.20091", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.20091", "abs": "https://arxiv.org/abs/2510.20091", "authors": ["Zhaoyi Joey Hou", "Bowei Alvin Zhang", "Yining Lu", "Bhiman Kumar Baghel", "Anneliese Brei", "Ximing Lu", "Meng Jiang", "Faeze Brahman", "Snigdha Chaturvedi", "Haw-Shiuan Chang", "Daniel Khashabi", "Xiang Lorraine Li"], "title": "CreativityPrism: A Holistic Benchmark for Large Language Model Creativity", "comment": null, "summary": "Creativity is often seen as a hallmark of human intelligence. While large\nlanguage models (LLMs) are increasingly perceived as producing creative text,\nthere is still no holistic framework to evaluate their creativity across\ndiverse scenarios. Existing evaluation methods remain fragmented, with dramatic\nvariation across domains and tasks, largely due to differing definitions and\nmeasurements of creativity. Inspired by the hypothesis that creativity is not\none fixed idea, we propose CreativityPrism, an evaluation analysis framework\nthat decomposes creativity into three dimensions: quality, novelty, and\ndiversity. CreativityPrism incorporates nine tasks, three domains, i.e.,\ndivergent thinking, creative writing, and logical reasoning, and twenty\nevaluation metrics, which measure each dimension in task-specific, unique ways.\nWe evaluate 17 state-of-the-art (SoTA) proprietary and open-sourced LLMs on\nCreativityPrism and analyze the performance correlations among different\nmetrics and task domains. Our results reveal a notable gap between proprietary\nand open-source models. Overall, model performance tends to be highly\ncorrelated across tasks within the same domain and less so across different\ndomains. Among evaluation dimensions, diversity and quality metrics show strong\ncorrelations - models that perform well on one often excel on the other -\nwhereas novelty exhibits much weaker correlation with either. These findings\nsupport our hypothesis that strong performance in one creativity task or\ndimension does not necessarily generalize to others, underscoring the need for\na holistic evaluation of LLM creativity.", "AI": {"tldr": "CreativityPrism是一个评估语言模型创造力的框架，它将创造力分为质量、新颖性和多样性的维度。评估结果显示了专有模型和开源模型之间存在显著差异。", "motivation": "现有的评估方法在领域和任务之间存在显著的差异，主要是由于对创造力的不同定义和测量方式。因此，我们提出CreativityPrism以解决这一问题。", "method": "我们提出了CreativityPrism，一个评估分析框架，将创造力分解为质量、新颖性和多样性这三个维度。该框架包含九个任务，三个领域（发散性思考、创意写作和逻辑推理）和二十个评估指标。", "result": "评估结果显示，专有模型和开源模型之间存在显著差距。模型的性能在相同领域的任务中高度相关，而在不同领域中的相关性较弱。质量与多样性指标之间有强相关性，新颖性与其他指标的相关性较弱。", "conclusion": "这些发现支持了我们的假设，即在一个创造力任务或维度中的优秀表现并不一定能推广到其他方面，强调了对语言模型创造力进行全面评估的必要性。"}}
{"id": "2510.20132", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.20132", "abs": "https://arxiv.org/abs/2510.20132", "authors": ["Hyunjun Jung", "Hae-Gon Jeon"], "title": "Inverse Image-Based Rendering for Light Field Generation from Single Images", "comment": null, "summary": "A concept of light-fields computed from multiple view images on regular grids\nhas proven its benefit for scene representations, and supported realistic\nrenderings of novel views and photographic effects such as refocusing and\nshallow depth of field. In spite of its effectiveness of light flow\ncomputations, obtaining light fields requires either computational costs or\nspecialized devices like a bulky camera setup and a specialized microlens\narray. In an effort to broaden its benefit and applicability, in this paper, we\npropose a novel view synthesis method for light field generation from only\nsingle images, named inverse image-based rendering. Unlike previous attempts to\nimplicitly rebuild 3D geometry or to explicitly represent objective scenes, our\nmethod reconstructs light flows in a space from image pixels, which behaves in\nthe opposite way to image-based rendering. To accomplish this, we design a\nneural rendering pipeline to render a target ray in an arbitrary viewpoint. Our\nneural renderer first stores the light flow of source rays from the input\nimage, then computes the relationships among them through cross-attention, and\nfinally predicts the color of the target ray based on these relationships.\nAfter the rendering pipeline generates the first novel view from a single input\nimage, the generated out-of-view contents are updated to the set of source\nrays. This procedure is iteratively performed while ensuring the consistent\ngeneration of occluded contents. We demonstrate that our inverse image-based\nrendering works well with various challenging datasets without any retraining\nor finetuning after once trained on synthetic dataset, and outperforms relevant\nstate-of-the-art novel view synthesis methods.", "AI": {"tldr": "本文提出一种基于神经网络的逆图像渲染方法，可以从单张图片中生成光场，这种方法在多个挑战性数据集上表现良好，并优于当前最先进的视图合成技术。", "motivation": "光场计算复杂或需特殊设备，我们致力于拓宽其应用，开发了一种新的视图合成方法，使从单张图片生成光场成为可能。", "method": "我们提出了一种新颖的视图合成方法，名为逆图像渲染，可以从单张图片生成光场。该方法通过设计的神经渲染流程来实现光流的存储、跨注意力计算以及预测目标光线颜色。从单张输入图像生成首个新颖视角后，生成的视角外内容会被更新到源光线集合中。该过程会迭代进行，以确保被遮挡内容的一致性生成。", "result": "我们的方法可以在各种具有挑战性的数据集上良好工作，无需重新训练或微调，并优于相关的最先进新颖视图合成方法。", "conclusion": "研究展示了逆图像渲染方法从单张图像生成光场的可行性，无需大量计算成本或特殊设备，该方法能够有效合成新颖视图，并在未经额外训练的情况下适应各种数据集。"}}
{"id": "2510.20098", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.20098", "abs": "https://arxiv.org/abs/2510.20098", "authors": ["Yajie Li", "Albert Galimov", "Mitra Datta Ganapaneni", "Pujitha Thejaswi", "De Meng", "Priyanshu Kumar", "Saloni Potdar"], "title": "Leveraging the Power of Large Language Models in Entity Linking via Adaptive Routing and Targeted Reasoning", "comment": null, "summary": "Entity Linking (EL) has traditionally relied on large annotated datasets and\nextensive model fine-tuning. While recent few-shot methods leverage large\nlanguage models (LLMs) through prompting to reduce training requirements, they\noften suffer from inefficiencies due to expensive LLM-based reasoning. ARTER\n(Adaptive Routing and Targeted Entity Reasoning) presents a structured pipeline\nthat achieves high performance without deep fine-tuning by strategically\ncombining candidate generation, context-based scoring, adaptive routing, and\nselective reasoning. ARTER computes a small set of complementary signals(both\nembedding and LLM-based) over the retrieved candidates to categorize contextual\nmentions into easy and hard cases. The cases are then handled by a\nlow-computational entity linker (e.g. ReFinED) and more expensive targeted\nLLM-based reasoning respectively. On standard benchmarks, ARTER outperforms\nReFinED by up to +4.47%, with an average gain of +2.53% on 5 out of 6 datasets,\nand performs comparably to pipelines using LLM-based reasoning for all\nmentions, while being as twice as efficient in terms of the number of LLM\ntokens.", "AI": {"tldr": "ARTER 方法通过策略性结合候选生成、上下文打分、自适应路由及选择性推理，实现高效高质量的实体链接，并在性能上优于或等同于现有方法，但计算效率更高。", "motivation": "传统的实体链接方法依赖于大规模标注数据集和大量的模型微调。尽管最近的少量样本方法通过提示使用大型语言模型（LLMs）来减少训练需求，但它们通常由于昂贵的LLM推理而效率低下。因此，提出了ARTER方法。", "method": "ARTER (Adaptive Routing and Targeted Entity Reasoning) 通过策略性地结合候选生成、基于上下文的打分、自适应路由以及选择性推理，构建了一个结构化的实体链接管道。该方法生成了一组互补的信号（包括嵌入和基于大型语言模型的信号），用于将上下文提及分类为简单和复杂情况，并通过低计算量的实体链接器处理简单的提及，通过更昂贵的定向 LLM 推理处理复杂的提及。", "result": "在标准基准测试中，ARTER 在 6 个数据集中的 5 个上比 ReFinED 高出 2.53% 的平均增益，并且在所有提及中与使用 LLM 推理的流水线表现相当，而 LLM 词元数量是前者的两倍。", "conclusion": "ARTER 方法可以在不进行深度微调的情况下，通过更高效的方式实现高质量的实体链接，同时保持比现有方法更好的性能和效率。"}}
{"id": "2510.20134", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.20134", "abs": "https://arxiv.org/abs/2510.20134", "authors": ["Jiachen Liang", "Ruibing Hou", "Minyang Hu", "Hong Chang", "Shiguang Shan", "Xilin Chen"], "title": "Revisiting Logit Distributions for Reliable Out-of-Distribution Detection", "comment": "Accepted by NeurIPS 2025", "summary": "Out-of-distribution (OOD) detection is critical for ensuring the reliability\nof deep learning models in open-world applications. While post-hoc methods are\nfavored for their efficiency and ease of deployment, existing approaches often\nunderexploit the rich information embedded in the model's logits space. In this\npaper, we propose LogitGap, a novel post-hoc OOD detection method that\nexplicitly exploits the relationship between the maximum logit and the\nremaining logits to enhance the separability between in-distribution (ID) and\nOOD samples. To further improve its effectiveness, we refine LogitGap by\nfocusing on a more compact and informative subset of the logit space.\nSpecifically, we introduce a training-free strategy that automatically\nidentifies the most informative logits for scoring. We provide both theoretical\nanalysis and empirical evidence to validate the effectiveness of our approach.\nExtensive experiments on both vision-language and vision-only models\ndemonstrate that LogitGap consistently achieves state-of-the-art performance\nacross diverse OOD detection scenarios and benchmarks. Code is available at\nhttps://github.com/GIT-LJc/LogitGap.", "AI": {"tldr": "提出了LogitGap方法，通过利用模型输出对数概率空间中最大对数概率与其他对数概率之间的关系来改进离分布检测，提升了开放世界应用中深度学习模型的可靠性。", "motivation": "现有的离分布检测方法未能充分利用模型输出对数概率空间中的丰富信息，因此提出了新的方法来解决这个问题。", "method": "提出了LogitGap方法，该方法利用模型输出中最大对数概率与其他对数概率之间的关系来提高离分布样本与内部样本之间的可分离性。此外，还提出了一种无需训练的策略来自动识别最具有信息量的对数概率。", "result": "实验显示，在视觉和视觉语言模型上，LogitGap在各种离分布检测场景和基准测试中均达到了前沿水平。", "conclusion": "通过理论分析和实验验证，LogitGap方法在离分布检测任务中表现出优越的性能。"}}
{"id": "2510.20151", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.20151", "abs": "https://arxiv.org/abs/2510.20151", "authors": ["Haoyuan Li", "Zhengyuan Shen", "Sullam Jeoung", "Yueyan Chen", "Jiayu Li", "Qi Zhu", "Shuai Wang", "Vassilis Ioannidis", "Huzefa Rangwala"], "title": "BoundRL: Efficient Structured Text Segmentation through Reinforced Boundary Generation", "comment": null, "summary": "As structured texts become increasingly complex across diverse domains --\nfrom technical reports to generative AI prompts -- the need for text\nsegmentation into semantically meaningful components becomes critical. Such\ntexts often contain elements beyond plain language, including tables, code\nsnippets, and placeholders, which conventional sentence- or paragraph-level\nsegmentation methods cannot handle effectively. To address this challenge, we\npropose BoundRL, a novel and efficient approach that jointly performs\ntoken-level text segmentation and label prediction for long structured texts.\nInstead of generating complete contents for each segment, it generates only a\nsequence of starting tokens and reconstructs the complete contents by locating\nthese tokens within the original texts, thereby reducing inference costs by\norders of magnitude and minimizing hallucination. To adapt the model for the\noutput format, BoundRL~performs reinforcement learning with verifiable rewards\n(RLVR) with a specifically designed reward that jointly optimizes document\nreconstruction fidelity and semantic alignment. To mitigate entropy collapse,\nit further constructs intermediate candidates by systematically perturbing a\nfraction of generated sequences of segments to create stepping stones toward\nhigher-quality solutions. To demonstrate BoundRL's effectiveness on\nparticularly challenging structured texts, we focus evaluation on complex\nprompts used for LLM applications. Experiments show that BoundRL enables small\nlanguage models (1.7B parameters) to outperform few-shot prompting of much\nlarger models. Moreover, RLVR with our designed reward yields significant\nimprovements over supervised fine-tuning, and incorporating intermediate\ncandidates further improves both performance and generalization.", "AI": {"tldr": "BoundRL是一种新的方法，通过标记级别的文本分割和标签预测来处理复杂的结构化文本，适用于包含表格、代码片段等内容的长文本。此方法使用强化学习和中间候选生成来提高性能和泛化能力，并且适用于小模型处理复杂的LLM应用提示。", "motivation": "面对日益复杂的结构化文本，如技术报告、AI提示等，传统的句子或段落级别的分割方法无法有效处理其中包含的表格、代码片段等元素，因此需要一种新的分割方法来提高处理效率和质量。", "method": "BoundRL采用强化学习与验证奖励机制，仅生成各个段落的起始标记，通过在原文中寻找这些标记来重建完整的段落内容。为了缓解熵塌陷问题，它还生成部分扰动的中间候选作为提高质量的桥梁。该方法能显著减少推理成本，并降低虚构内容生成的概率。", "result": "实验结果表明，即使小规模模型（1.7B参数），使用BoundRL也能超越大模型的几轮提示性能。强化学习与特定设计的奖励机制相比监督微调有显著改进，而引入中间候选可以进一步提高模型性能和泛化能力。", "conclusion": "BoundRL提出的文本分割和标签预测方法在处理复杂结构化文本方面有显著的优越性，能够在保证高质量输出的同时，减少计算成本并提升小模型和复杂LLM应用提示的处理效果。"}}
{"id": "2510.20155", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.20155", "abs": "https://arxiv.org/abs/2510.20155", "authors": ["Penghao Wang", "Yiyang He", "Xin Lv", "Yukai Zhou", "Lan Xu", "Jingyi Yu", "Jiayuan Gu"], "title": "PartNeXt: A Next-Generation Dataset for Fine-Grained and Hierarchical 3D Part Understanding", "comment": "NeurIPS 2025 DB Track. Project page:\n  https://authoritywang.github.io/partnext", "summary": "Understanding objects at the level of their constituent parts is fundamental\nto advancing computer vision, graphics, and robotics. While datasets like\nPartNet have driven progress in 3D part understanding, their reliance on\nuntextured geometries and expert-dependent annotation limits scalability and\nusability. We introduce PartNeXt, a next-generation dataset addressing these\ngaps with over 23,000 high-quality, textured 3D models annotated with\nfine-grained, hierarchical part labels across 50 categories. We benchmark\nPartNeXt on two tasks: (1) class-agnostic part segmentation, where\nstate-of-the-art methods (e.g., PartField, SAMPart3D) struggle with\nfine-grained and leaf-level parts, and (2) 3D part-centric question answering,\na new benchmark for 3D-LLMs that reveals significant gaps in open-vocabulary\npart grounding. Additionally, training Point-SAM on PartNeXt yields substantial\ngains over PartNet, underscoring the dataset's superior quality and diversity.\nBy combining scalable annotation, texture-aware labels, and multi-task\nevaluation, PartNeXt opens new avenues for research in structured 3D\nunderstanding.", "AI": {"tldr": "本文介绍了一个新的3D数据集PartNeXt，用于解决现有数据集的不足，并通过对两个任务的基准测试展示了其优越性。", "motivation": "研究动机在于解决现有数据集如PartNet的不足，这些问题包括无纹理几何形状和专家依赖标注限制了数据集的扩展性和可用性。", "method": "该论文介绍了PartNeXt数据集，该数据集包含超过23,000个高质量的纹理3D模型，并且这些模型用50个类别中的细粒度和分层的部分标签进行了注释。", "result": "PartNeXt数据集在两个任务上进行了基准测试：类别无关的部分分割和3D部分导向的问题回答。实验显示PartNeXt显著优于PartNet，展示了其数据集的质量和多样性。", "conclusion": "结合可扩展的标注、纹理感知标签和多任务评估，PartNeXt为有结构的3D理解研究开辟了新的途径。"}}
{"id": "2510.20154", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.20154", "abs": "https://arxiv.org/abs/2510.20154", "authors": ["Anthony Dubreuil", "Antoine Gourru", "Christine Largeron", "Amine Trabelsi"], "title": "Are Stereotypes Leading LLMs' Zero-Shot Stance Detection ?", "comment": "Accepted in EMNLP 2025 (Main)", "summary": "Large Language Models inherit stereotypes from their pretraining data,\nleading to biased behavior toward certain social groups in many Natural\nLanguage Processing tasks, such as hateful speech detection or sentiment\nanalysis. Surprisingly, the evaluation of this kind of bias in stance detection\nmethods has been largely overlooked by the community. Stance Detection involves\nlabeling a statement as being against, in favor, or neutral towards a specific\ntarget and is among the most sensitive NLP tasks, as it often relates to\npolitical leanings. In this paper, we focus on the bias of Large Language\nModels when performing stance detection in a zero-shot setting. We\nautomatically annotate posts in pre-existing stance detection datasets with two\nattributes: dialect or vernacular of a specific group and text\ncomplexity/readability, to investigate whether these attributes influence the\nmodel's stance detection decisions. Our results show that LLMs exhibit\nsignificant stereotypes in stance detection tasks, such as incorrectly\nassociating pro-marijuana views with low text complexity and African American\ndialect with opposition to Donald Trump.", "AI": {"tldr": "本文研究了大型语言模型在零样本立场检测中的偏见，通过添加方言和文本复杂性属性，发现模型存在显著的偏见，例如将支持大麻的观点与低文本复杂性关联，将非裔美国人口语与反对唐纳德·川普的立场相关联。", "motivation": "虽然大型语言模型已经在许多自然语言处理任务中表现出了对某些社会群体的偏见，但在立场检测方法中的这种偏见评估却被学界忽视。立场检测是一个高度敏感的NLP任务，因为它通常涉及政治倾向，因此有必要研究大型语言模型在这种任务中的偏见。", "method": "我们通过自动标注现有的立场检测数据集中的帖子，添加两个属性：特定群体的方言或口语和文本复杂度/可读性，来探究这些属性是否影响模型的立场检测决策。通过这种方式，我们系统地评估了大型语言模型在零样本立场检测任务中的偏见问题。", "result": "实验结果显示大型语言模型在检测立场时存在偏见，具体表现为错误地将支持大麻的观点与低文本复杂性联系起来，以及将非裔美国人口语与反对唐纳德·川普的观点相关联。", "conclusion": "研究结果表明，大型语言模型在立场检测任务中表现出显著的刻板印象，显示了即使在零样本设置下，这些模型也可能存在敏感性和偏见问题。"}}
{"id": "2510.20158", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.20158", "abs": "https://arxiv.org/abs/2510.20158", "authors": ["Eduardo R. Corral-Soto", "Yang Liu", "Yuan Ren", "Bai Dongfeng", "Liu Bingbing"], "title": "Monocular Visual 8D Pose Estimation for Articulated Bicycles and Cyclists", "comment": null, "summary": "In Autonomous Driving, cyclists belong to the safety-critical class of\nVulnerable Road Users (VRU), and accurate estimation of their pose is critical\nfor cyclist crossing intention classification, behavior prediction, and\ncollision avoidance. Unlike rigid objects, articulated bicycles are composed of\nmovable rigid parts linked by joints and constrained by a kinematic structure.\n6D pose methods can estimate the 3D rotation and translation of rigid bicycles,\nbut 6D becomes insufficient when the steering/pedals angles of the bicycle\nvary. That is because: 1) varying the articulated pose of the bicycle causes\nits 3D bounding box to vary as well, and 2) the 3D box orientation is not\nnecessarily aligned to the orientation of the steering which determines the\nactual intended travel direction. In this work, we introduce a method for\ncategory-level 8D pose estimation for articulated bicycles and cyclists from a\nsingle RGB image. Besides being able to estimate the 3D translation and\nrotation of a bicycle from a single image, our method also estimates the\nrotations of its steering handles and pedals with respect to the bicycle body\nframe. These two new parameters enable the estimation of a more fine-grained\nbicycle pose state and travel direction. Our proposed model jointly estimates\nthe 8D pose and the 3D Keypoints of articulated bicycles, and trains with a mix\nof synthetic and real image data to generalize on real images. We include an\nevaluation section where we evaluate the accuracy of our estimated 8D pose\nparameters, and our method shows promising results by achieving competitive\nscores when compared against state-of-the-art category-level 6D pose estimators\nthat use rigid canonical object templates for matching.", "AI": {"tldr": "本研究提出了一种从单张RGB图像中估算自行车及其骑行者8D姿态的方法，除了估计自行车3D平移和旋转，还能够估计方向盘和踏板相对于自行车主体的旋转角度。这种方法和3D关键点一起进行联合估计，并使用合成和真实图像数据的混合进行训练。实验证明了与使用刚性标准物体模板进行匹配的最先进的6D姿态估计算法相比，这项方法具有竞争力。", "motivation": "在自动驾驶领域，自行车骑行者是安全关键的脆弱道路使用者（VRU）类别之一。准确估计自行车的姿势对于骑行者穿越意图分类、行为预测和碰撞避免至关重要。然而，6D姿势方法在面对自行车转动或踏板角度的变动时显得不足，因此需要提出一种更精细的自行车姿态估计方法。", "method": "Structure", "result": "该研究方法同时估计了8D姿态和3D关键点，并使用合成和真实图像数据的混合进行训练。实验证明了该方法能够在评估估计的8D姿态参数的准确性方面取得有竞争力的结果。", "conclusion": "提出的用于估计单个图像中自行车及其骑行者的类别级别8D姿态的方法在自行车的更精细姿态状态和行驶方向的估算上表现出色，相较于当前最先进的6D姿态估计算法具有竞争力。"}}
{"id": "2510.20168", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.20168", "abs": "https://arxiv.org/abs/2510.20168", "authors": ["Tian Lan", "Bin Zhu", "Qianghuai Jia", "Junyang Ren", "Haijun Li", "Longyue Wang", "Zhao Xu", "Weihua Luo", "Kaifu Zhang"], "title": "DeepWideSearch: Benchmarking Depth and Width in Agentic Information Seeking", "comment": null, "summary": "Current search agents fundamentally lack the ability to simultaneously\nperform \\textit{deep} reasoning over multi-hop retrieval and\n\\textit{wide}-scale information collection-a critical deficiency for real-world\napplications like comprehensive market analysis and business development. To\nbridge this gap, we introduce DeepWideSearch, the first benchmark explicitly\ndesigned to evaluate agents to integrate depth and width in information\nseeking. In DeepWideSearch, agents must process a large volume of data, each\nrequiring deep reasoning over multi-hop retrieval paths. Specifically, we\npropose two methods to converse established datasets, resulting in a curated\ncollection of 220 questions spanning 15 diverse domains. Extensive experiments\ndemonstrate that even state-of-the-art agents achieve only 2.39% average\nsuccess rate on DeepWideSearch, highlighting the substantial challenge of\nintegrating depth and width search in information-seeking tasks. Furthermore,\nour error analysis reveals four failure modes: lack of reflection, overreliance\non internal knowledge, insufficient retrieval, and context overflow-exposing\nkey limitations in current agent architectures. We publicly release\nDeepWideSearch to catalyze future research on more capable and robust\ninformation-seeking agents.", "AI": {"tldr": "DeepWideSearch 是一个专门设计来评估在信息检索中同时考虑深度和广度能力的基准测试，展示了现有的最先进代理在该测试上的显著挑战。", "motivation": "为了解决现有搜索代理在深度推理和大规模信息收集能力上的欠缺问题，特别是在全面市场分析和企业发展这样的实际应用场景中。", "method": "Structure", "result": "{\"tldr\": \"DeepWideSearch 是一个专门设计来评估在信息检索中同时考虑深度和广度能力的基准测试，展示了现有的最先进代理在该测试上的显著挑战。\", \"motivation\": \"为了解决现有搜索代理在深度推理和大规模信息收集能力上的欠缺问题，特别是在全面市场分析和企业发展这样的实际应用场景中。\", \"method\": \"通过改进现有数据集，创建了一个包含220个问题、跨越15个不同领域的DeepWideSearch基准测试。\", \"result\": \"实验结果显示，最先进的代理在DeepWideSearch上的平均成功率为2.39%，并且错误分析揭示了四种失败模式。\", \"conclusion\": \"公开发布DeepWideSearch以促进未来在更强大和健壮的信息检索代理方面的研究。\"}", "conclusion": "公开发布DeepWideSearch以促进未来在更强大和健壮的信息检索代理方面的研究。"}}
{"id": "2510.20162", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.20162", "abs": "https://arxiv.org/abs/2510.20162", "authors": ["Xudong Yan", "Songhe Feng"], "title": "TOMCAT: Test-time Comprehensive Knowledge Accumulation for Compositional Zero-Shot Learning", "comment": "Accepted to NeurIPS 2025", "summary": "Compositional Zero-Shot Learning (CZSL) aims to recognize novel\nattribute-object compositions based on the knowledge learned from seen ones.\nExisting methods suffer from performance degradation caused by the distribution\nshift of label space at test time, which stems from the inclusion of unseen\ncompositions recombined from attributes and objects. To overcome the challenge,\nwe propose a novel approach that accumulates comprehensive knowledge in both\ntextual and visual modalities from unsupervised data to update multimodal\nprototypes at test time. Building on this, we further design an adaptive update\nweight to control the degree of prototype adjustment, enabling the model to\nflexibly adapt to distribution shift during testing. Moreover, a dynamic\npriority queue is introduced that stores high-confidence images to acquire\nvisual knowledge from historical images for inference. Considering the semantic\nconsistency of multimodal knowledge, we align textual and visual prototypes by\nmultimodal collaborative representation learning. Extensive experiments\nindicate that our approach achieves state-of-the-art performance on four\nbenchmark datasets under both closed-world and open-world settings. Code will\nbe available at https://github.com/xud-yan/TOMCAT .", "AI": {"tldr": "这篇论文提出了一种新的CZSL方法，通过更新多模态原型、设计自适应权重、引入动态优先级队列以及对齐原型以适应分布变化，取得先进性能。", "motivation": "现有的CZSL方法由于测试时标签空间的分布变化导致性能下降。这种问题源于测试时存在由属性和对象重新组合的未见组合。为此，我们的研究旨在克服这一挑战。", "method": "我们提出了一种新的方法，通过积累文本和视觉模态中的全面知识来更新多模态原型。在此基础上，我们设计了一个自适应更新权重，用来控制原型调整的程度，使模型在测试时能够灵活适应分布变化。此外，引入了一个动态优先级队列，用于存储高置信度图像，从而从历史图像中获取视觉知识用于推理。为了保证多模态知识的语义一致性，我们通过多模态协作表示学习对文本和视觉原型进行对齐。", "result": "通过广泛的实验，我们的方法在四个基准数据集下，无论是封闭世界还是开放世界设置，均达到了最先进的性能。", "conclusion": "研究表明，通过我们的方法，可以在未见组合的识别任务中取得显著的性能提升。代码将在https://github.com/xud-yan/TOMCAT公开。"}}
{"id": "2510.20176", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.20176", "abs": "https://arxiv.org/abs/2510.20176", "authors": ["Yuhang Zhou", "Mingrui Zhang", "Ke Li", "Mingyi Wang", "Qiao Liu", "Qifei wang", "Jiayi Liu", "Fei Liu", "Serena Li", "Weiwi Li", "Mingze Gao", "Abhishek Kumar", "Xiangjun Fan", "Zhuokai Zhao", "Lizhu Zhang"], "title": "Mixture-of-Minds: Multi-Agent Reinforcement Learning for Table Understanding", "comment": "18 pages, 4 figures", "summary": "Understanding and reasoning over tables is a critical capability for many\nreal-world applications. Large language models (LLMs) have shown promise on\nthis task, but current approaches remain limited. Fine-tuning based methods\nstrengthen language reasoning; yet they are prone to arithmetic errors and\nhallucination. In contrast, tool-based methods enable precise table\nmanipulation but rely on rigid schemas and lack semantic understanding. These\ncomplementary drawbacks highlight the need for approaches that integrate robust\nreasoning with reliable table processing. In this work, we propose\nMixture-of-Minds, a multi-agent framework that decomposes table reasoning into\nthree specialized roles: planning, coding, and answering. This design enables\neach agent to focus on a specific aspect of the task while leveraging code\nexecution for precise table manipulation. Building on this workflow, we\nintroduce a self-improvement training framework that employs Monte Carlo Tree\nSearch (MCTS) rollouts to generate pseudo-gold trajectories and optimize agents\nwith reinforcement learning (RL). Extensive experiments show that\nMixture-of-Minds delivers substantial gains, reaching 62.13% on TableBench and\nsurpassing OpenAI-o4-mini-high. These results demonstrate the promise of\ncombining structured multi-agent workflows with RL to advance table\nunderstanding.", "AI": {"tldr": "本文提出了一种多代理框架Mixture-of-Minds，用于分解并优化表格理解和推理过程。通过结合结构化工作流程与强化学习方法，该方法在表格理解任务上取得了显著进步。", "motivation": "大型语言模型在表格理解和推理任务中显示出了潜力，但目前的方法仍然存在算术错误和幻觉方面的局限。纯粹基于工具的方法能够精确操作表格但依赖于僵化的模式并缺乏语义理解。这些互补的缺点表明需要结合强大的推理能力和可靠的表格处理能力。", "method": "本文提出了Mixture-of-Minds框架，该框架将表格推理分解为规划、编码和回答三种专门的角色，利用代码执行实现精确的表格操作，并引入了一种自我改进的训练框架，该框架采用蒙特卡洛树搜索生成伪金标准轨迹，以强化学习方式优化各代理。", "result": "实验结果表明，Mixture-of-Minds在TableBench上可达62.13%，超过了OpenAI-o4-mini-high。", "conclusion": "该研究展示了将结构化的多代理工作流程与强化学习相结合来提升表格理解能力的潜力，Mixture-of-Minds在TableBench上取得了62.13%的成绩，超越了OpenAI-o4-mini-high。"}}
{"id": "2510.20165", "categories": ["cs.CV", "cs.AI", "68T45 (Machine learning in discrete mathematics), 68T07 (Artificial\n  neural networks and deep learning)"], "pdf": "https://arxiv.org/pdf/2510.20165", "abs": "https://arxiv.org/abs/2510.20165", "authors": ["Insu Jeon", "Wonkwang Lee", "Myeongjang Pyeon", "Gunhee Kim"], "title": "IB-GAN: Disentangled Representation Learning with Information Bottleneck Generative Adversarial Networks", "comment": "Published in the Proceedings of the Thirty Fifth AAAI Conference on\n  Artificial Intelligence (AAAI 2021), paper number 7926", "summary": "We propose a new GAN-based unsupervised model for disentangled representation\nlearning. The new model is discovered in an attempt to utilize the Information\nBottleneck (IB) framework to the optimization of GAN, thereby named IB-GAN. The\narchitecture of IB-GAN is partially similar to that of InfoGAN but has a\ncritical difference; an intermediate layer of the generator is leveraged to\nconstrain the mutual information between the input and the generated output.\nThe intermediate stochastic layer can serve as a learnable latent distribution\nthat is trained with the generator jointly in an end-to-end fashion. As a\nresult, the generator of IB-GAN can harness the latent space in a disentangled\nand interpretable manner. With the experiments on dSprites and Color-dSprites\ndataset, we demonstrate that IB-GAN achieves competitive disentanglement scores\nto those of state-of-the-art \\b{eta}-VAEs and outperforms InfoGAN. Moreover,\nthe visual quality and the diversity of samples generated by IB-GAN are often\nbetter than those by \\b{eta}-VAEs and Info-GAN in terms of FID score on CelebA\nand 3D Chairs dataset.", "AI": {"tldr": "IB-GAN, a new GAN-based model that leverages the Information Bottleneck framework, achieves superior disentanglement scores and generates higher quality and diverse samples compared to β-VAEs and InfoGAN.", "motivation": "The motivation is to utilize the Information Bottleneck framework to improve the performance of GANs in generating disentangled and interpretable representations.", "method": "We propose a new GAN-based unsupervised model for disentangled representation learning named IB-GAN. The model leverages the Information Bottleneck (IB) framework and has an intermediate layer that constrains the mutual information between the input and output. This intermediate layer serves as a learnable latent distribution.", "result": "Experiments on dSprites and Color-dSprites datasets show that IB-GAN achieves competitive disentanglement scores compared to state-of-the-art β-VAEs and outperforms InfoGAN. The visual quality and diversity of samples, measured by FID score on CelebA and 3D Chairs datasets, are often better than those of β-VAEs and InfoGAN.", "conclusion": "IB-GAN provides a new approach for disentangled representation learning, demonstrating both superior disentanglement scores and better quality of generated samples compared to existing methods."}}
{"id": "2510.20198", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.20198", "abs": "https://arxiv.org/abs/2510.20198", "authors": ["Maggie Bai", "Ava Kim Cohen", "Eleanor Koss", "Charlie Lichtenbaum"], "title": "Stuck in the Matrix: Probing Spatial Reasoning in Large Language Models", "comment": "20 pages, 24 figures", "summary": "This paper explores the spatial reasoning capability of large language models\n(LLMs) over textual input through a suite of five tasks aimed at probing their\nspatial understanding and computational abilities. The models were tested on\nboth fundamental spatial reasoning and multi-step problem-solving within\nstructured grid-based environments using tasks such as quadrant identification,\ngeometric transformations, distance evaluation, word searches, and tile\nsliding. Each task was scaled in complexity through increasing grid dimensions,\nrequiring models to extend beyond simple pattern recognition into abstract\nspatial reasoning. Our results reveal that while LLMs demonstrate moderate\nsuccess in all tasks with small complexity and size, performance drops off\nrapidly as scale increases, with an average loss in accuracy of 42.7%, and\nreaching as high as 84%. Every test that began with over 50% accuracy showed a\nloss of at least 48%, illustrating the consistent nature of the deterioration.\nFurthermore, their struggles with scaling complexity hint at a lack of robust\nspatial representations in their underlying architectures. This paper\nunderscores the gap between linguistic and spatial reasoning in LLMs, offering\ninsights into their current limitations, and laying the groundwork for future\nintegrative benchmarks at the intersection of language and geometry.", "AI": {"tldr": "本论文探讨了大型语言模型在空间推理和复杂问题解决中的局限性，特别指出在结构化格子环境中的表现随任务复杂度增加而急剧下降。这些发现为未来在语言和几何学结合的发展提供了基准。", "motivation": "To provide a clear and concise analysis of the paper's abstract to facilitate quick comprehension of its main points and significance.", "method": "Abstract analysis and summarization.", "result": "<tool_call>\n{{\"name\": \"Structure\", \"arguments\": {\"tldr\": \"本论文探讨了大型语言模型（LLMs）在处理文本输入时的空间推理能力，并通过五项任务来探测其空间理解与计算能力。结果表明，虽然这些模型在简单任务中表现出一定的成功，但随着任务复杂度和规模的增加，其性能迅速下降，准确率平均下降42.7%，最高达84%。这些发现在语言和几何学的交汇处为未来的综合性基准测试奠定了基础。\", \"motivation\": \"本研究旨在通过一组任务来评估大型语言模型在空间推理和复杂问题解决方面的表现，特别关注模型在结构化格子环境中的表现。\", \"method\": \"通过五项任务，包括象限识别、几何变换、距离评估、单词搜索和瓷砖滑动，对模型进行测试。任务难度通过增加格子尺寸递增。\", \"result\": \"结果显示，随着任务复杂度和规模增加，模型性能急剧下降，准确率出现显著下降。\", \"conclusion\": \"研究强调了语言模型在空间推理方面的局限性，表明了其架构中缺少强大的空间表示能力。这些发现为未来在语言和几何学交汇处的发展提供了基准。\"}}}\n</tool_call>", "conclusion": "The analysis effectively breaks down the abstract into concise components for easy understanding."}}
{"id": "2510.20178", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.20178", "abs": "https://arxiv.org/abs/2510.20178", "authors": ["Yun Wang", "Junjie Hu", "Qiaole Dong", "Yongjian Zhang", "Yanwei Fu", "Tin Lun Lam", "Dapeng Wu"], "title": "PPMStereo: Pick-and-Play Memory Construction for Consistent Dynamic Stereo Matching", "comment": null, "summary": "Temporally consistent depth estimation from stereo video is critical for\nreal-world applications such as augmented reality, where inconsistent depth\nestimation disrupts the immersion of users. Despite its importance, this task\nremains challenging due to the difficulty in modeling long-term temporal\nconsistency in a computationally efficient manner. Previous methods attempt to\naddress this by aggregating spatio-temporal information but face a fundamental\ntrade-off: limited temporal modeling provides only modest gains, whereas\ncapturing long-range dependencies significantly increases computational cost.\nTo address this limitation, we introduce a memory buffer for modeling\nlong-range spatio-temporal consistency while achieving efficient dynamic stereo\nmatching. Inspired by the two-stage decision-making process in humans, we\npropose a \\textbf{P}ick-and-\\textbf{P}lay \\textbf{M}emory (PPM) construction\nmodule for dynamic \\textbf{Stereo} matching, dubbed as \\textbf{PPMStereo}. PPM\nconsists of a `pick' process that identifies the most relevant frames and a\n`play' process that weights the selected frames adaptively for spatio-temporal\naggregation. This two-stage collaborative process maintains a compact yet\nhighly informative memory buffer while achieving temporally consistent\ninformation aggregation. Extensive experiments validate the effectiveness of\nPPMStereo, demonstrating state-of-the-art performance in both accuracy and\ntemporal consistency. % Notably, PPMStereo achieves 0.62/1.11 TEPE on the\nSintel clean/final (17.3\\% \\& 9.02\\% improvements over BiDAStereo) with fewer\ncomputational costs. Codes are available at\n\\textcolor{blue}{https://github.com/cocowy1/PPMStereo}.", "AI": {"tldr": "提出了一种新的方法PPMStereo，用于立体视频中的时间一致深度估计，通过引入记忆缓冲机制实现了高效的长程时空一致性匹配，显著提高了准确性和时间一致性。", "motivation": "从立体视频中进行时间一致的深度估计对于增强现实等真实世界应用至关重要，但如何以计算高效的方式建模长期时间一致性依然是一个挑战。", "method": "引入了一个记忆缓冲机制来建模长期的时空一致性，同时实现了高效的动态立体匹配。受人类两阶段决策过程的启发，提出了Pick-and-Play Memory (PPM)构造模块来进行动态Stereo匹配，该方法称为PPMStereo。PPM包括一个选择最具相关帧的'pick'过程和一个对选中帧进行自适应加权的'play'过程，从而促进了时空聚合。", "result": "广泛的实验验证了PPMStereo的有效性，显示了在准确性和时间一致性方面的最新性能。特别是在Sintel clean/final数据集上，PPMStereo的表现优于BiDAStereo，且计算成本更低。", "conclusion": "提出的PPMStereo方法在立体视频的时间一致深度估计任务中展示了出色的效果，其方法创新且高效。"}}
{"id": "2510.20208", "categories": ["cs.CL", "I.2.7"], "pdf": "https://arxiv.org/pdf/2510.20208", "abs": "https://arxiv.org/abs/2510.20208", "authors": ["David Pohl", "Marco Cognetta", "Junyoung Lee", "Naoaki Okazaki"], "title": "Decoding-Free Sampling Strategies for LLM Marginalization", "comment": "10 pages, 3 figures", "summary": "Modern language models operate on subword-tokenized text in order to make a\ntrade-off between model size, inference speed, and vocabulary coverage. A side\neffect of this is that, during inference, models are evaluated by measuring the\nprobability of only the specific tokenization produced as the output, despite\nthere being many possible ways to represent the same text with a subword\nvocabulary. Recent studies have argued instead for evaluating LLMs by\nmarginalization - the probability mass of all tokenizations of a given text.\n  Marginalization is difficult due to the number of possible tokenizations of a\ntext, so often approximate marginalization is done via sampling. However, a\ndownside of sampling is that an expensive generation step must be performed by\nthe LLM for each sample, which limits the number of samples that can be\nacquired given a runtime budget, and therefore also the accuracy of the\napproximation. Since computing the probability of a sequence given the\ntokenization is relatively cheap compared to actually generating it, we\ninvestigate sampling strategies that are decoding-free - they require no\ngeneration from the LLM, instead relying entirely on extremely cheap sampling\nstrategies that are model and tokenizer agnostic.\n  We investigate the approximation quality and speed of decoding-free sampling\nstrategies for a number of open models to find that they provide sufficiently\naccurate marginal estimates at a small fraction of the runtime cost and\ndemonstrate its use on a set of downstream inference tasks.", "AI": {"tldr": "该研究专注于探索无需复杂语言模型生成的边际化评估采样策略，能使模型以更低的计算成本达到更准确的评估效果。", "motivation": "研究新的采样策略旨在解决传统边际化评估方法由于生成步骤昂贵而导致采样数量有限的问题，从而提高边际化评估的准确性。", "method": "该研究探讨了无需解码的采样策略，这些策略完全依赖于极其便宜且与模型和词汇无关的采样方法，以解决传统的边际化评估中由于生成步骤昂贵而限制样本数量的问题。", "result": "研究结果表明，无需解码的采样策略能够以极小的运行时间成本提供足够准确的边际估计，并展示了其在下游推理任务上的应用。", "conclusion": "结论为，通过使用无需解码的策略，可以有效降低边际化评估的运行时间成本，并在多个开放模型上达到了足够准确的边际估计。"}}
{"id": "2510.20182", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.20182", "abs": "https://arxiv.org/abs/2510.20182", "authors": ["Aaron Appelle", "Jerome P. Lynch"], "title": "Evaluating Video Models as Simulators of Multi-Person Pedestrian Trajectories", "comment": "Preprint, under review", "summary": "Large-scale video generation models have demonstrated high visual realism in\ndiverse contexts, spurring interest in their potential as general-purpose world\nsimulators. Existing benchmarks focus on individual subjects rather than scenes\nwith multiple interacting people. However, the plausibility of multi-agent\ndynamics in generated videos remains unverified. We propose a rigorous\nevaluation protocol to benchmark text-to-video (T2V) and image-to-video (I2V)\nmodels as implicit simulators of pedestrian dynamics. For I2V, we leverage\nstart frames from established datasets to enable comparison with a ground truth\nvideo dataset. For T2V, we develop a prompt suite to explore diverse pedestrian\ndensities and interactions. A key component is a method to reconstruct 2D\nbird's-eye view trajectories from pixel-space without known camera parameters.\nOur analysis reveals that leading models have learned surprisingly effective\npriors for plausible multi-agent behavior. However, failure modes like merging\nand disappearing people highlight areas for future improvement.", "AI": {"tldr": "研究旨在通过新的评估协议，检测用于生成视频的T2V和I2V模型对行人动态的模拟能力，并通过重建二维鸟瞰视图轨迹进行分析，虽然领先模型已经展现出了对多代理行为的合理先验知识，但仍存在合并和消失等失败模式。", "motivation": "现有的基准测试主要集中于单个主体，而不是包含多个交互人物的场景，因此需要验证生成视频中多代理动力学的合理性。", "method": "我们提出了一种严格的评估协议，用于测试文本到视频（T2V）和图像到视频（I2V）模型作为行人动态的隐式模拟器。对于I2V，利用现有数据集的起始帧来与真实的视频数据集进行比较。对于T2V，开发了一系列提示来探索不同的行人密度和互动情况。关键组成部分是从像素空间中重建二维鸟瞰视角轨迹的方法，无需已知的相机参数。", "result": "分析显示领先的模型已经学习到对合理多代理行为的有效先验知识，但仍然存在一些失败模式，比如行人合并和消失等问题。", "conclusion": "研究揭示了现有的视频生成模型在多代理行为模拟方面取得了显著的进步，但也揭示了需要改进的问题，为未来的研究指明了方向。"}}
{"id": "2510.20239", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.20239", "abs": "https://arxiv.org/abs/2510.20239", "authors": ["Filippo Cenacchi", "Deborah Richards", "Longbing Cao"], "title": "Tri-Modal Severity Fused Diagnosis across Depression and Post-traumatic Stress Disorders", "comment": null, "summary": "Depression and post traumatic stress disorder (PTSD) often co-occur with\nconnected symptoms, complicating automated assessment, which is often binary\nand disorder specific. Clinically useful diagnosis needs severity aware cross\ndisorder estimates and decision support explanations. Our unified tri modal\naffective severity framework synchronizes and fuses interview text with\nsentence level transformer embeddings, audio with log Mel statistics with\ndeltas, and facial signals with action units, gaze, head and pose descriptors\nto output graded severities for diagnosing both depression (PHQ-8; 5 classes)\nand PTSD (3 classes). Standardized features are fused via a calibrated late\nfusion classifier, yielding per disorder probabilities and feature-level\nattributions. This severity aware tri-modal affective fusion approach is demoed\non multi disorder concurrent depression and PTSD assessment. Stratified cross\nvalidation on DAIC derived corpora outperforms unimodal/ablation baselines. The\nfused model matches the strongest unimodal baseline on accuracy and weighted\nF1, while improving decision curve utility and robustness under noisy or\nmissing modalities. For PTSD specifically, fusion reduces regression error and\nimproves class concordance. Errors cluster between adjacent severities; extreme\nclasses are identified reliably. Ablations show text contributes most to\ndepression severity, audio and facial cues are critical for PTSD, whereas\nattributions align with linguistic and behavioral markers. Our approach offers\nreproducible evaluation and clinician in the loop support for affective\nclinical decision making.", "AI": {"tldr": "A tri-modal framework was developed for better diagnosing and assessing concurrent depression and PTSD severity, blending text, audio, and facial cues with machine learning. The fusion method offers reliable results and robustness improvements under noisy conditions.", "motivation": "The motivation is to improve automated assessment of co-occurring depression and PTSD, moving from binary and disorder specific approaches to severity aware cross disorder estimates. This would provide clinically useful diagnosis and decision support explanations.", "method": "Our unified tri modal affective severity framework synchronizes and fuses interview text with sentence level transformer embeddings, audio with log Mel statistics with deltas, and facial signals with action units, gaze, head and pose descriptors to output severity grades for diagnosing depression and PTSD. The fusion of these multimodal features is performed by a calibrated late fusion classifier.", "result": "Stratified cross-validation on derived corpora demonstrates that the fused model matches the strongest unimodal baseline on accuracy and weighted F1, enhancing utility and robustness. For PTSD, the fusion reduces regression error and improves class concordance.", "conclusion": "The research concludes that the fused model provides good accuracy and weighted F1, while enhancing decision curve utility and robustness under noisy or missing modalities in assessing depression and PTSD. The approach offers reproducible evaluation for affective clinical decision making with clinician in the loop support."}}
{"id": "2510.20189", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.20189", "abs": "https://arxiv.org/abs/2510.20189", "authors": ["Xinyi Hu", "Yuran Wang", "Yue Li", "Wenxuan Liu", "Zheng Wang"], "title": "SPAN: Continuous Modeling of Suspicion Progression for Temporal Intention Localization", "comment": null, "summary": "Temporal Intention Localization (TIL) is crucial for video surveillance,\nfocusing on identifying varying levels of suspicious intentions to improve\nsecurity monitoring. However, existing discrete classification methods fail to\ncapture the continuous nature of suspicious intentions, limiting early\nintervention and explainability. In this paper, we propose the Suspicion\nProgression Analysis Network (SPAN), which shifts from discrete classification\nto continuous regression, enabling the capture of fluctuating and evolving\nsuspicious intentions. We reveal that suspicion exhibits long-term dependencies\nand cumulative effects, similar to Temporal Point Process (TPP) theory. Based\non these insights, we define a suspicion score formula that models continuous\nchanges while accounting for temporal characteristics. We also introduce\nSuspicion Coefficient Modulation, which adjusts suspicion coefficients using\nmultimodal information to reflect the varying impacts of suspicious actions.\nAdditionally, the Concept-Anchored Mapping method is proposed to link\nsuspicious actions to predefined intention concepts, offering insights into\nboth the actions and their potential underlying intentions. Extensive\nexperiments on the HAI dataset show that SPAN significantly outperforms\nexisting methods, reducing MSE by 19.8% and improving average mAP by 1.78%.\nNotably, SPAN achieves a 2.74% mAP gain in low-frequency cases, demonstrating\nits superior ability to capture subtle behavioral changes. Compared to discrete\nclassification systems, our continuous suspicion modeling approach enables\nearlier detection and proactive intervention, greatly enhancing system\nexplainability and practical utility in security applications.", "AI": {"tldr": "本文提出了Suspicion Progression Analysis Network (SPAN)，将时间意图定位（TIL）从离散分类转变为连续回归，以更准确捕捉和解释视频监控中不断变化的可疑意图，显著提高了安全监视系统的性能。", "motivation": "现有离散分类方法无法捕捉可疑意图中的连续性质，限制了早期干预和解释性。", "method": "提出了Suspicion Progression Analysis Network (SPAN)，采用连续回归来捕捉波动和演化的可疑意图，并引入Suspicion Coefficient Modulation和Concept-Anchored Mapping方法。", "result": "实验结果表明，与现有方法相比，SPAN在HAI数据集上降低了19.8%的均方误差（MSE），并将平均mAP提高了1.78%，特别是在低频案例中获取了2.74%的mAP增益。", "conclusion": "通过连续怀疑模态方法，SPAN实现了更早的检测，更主动的干预，增强了系统的解释性和实用性。"}}
{"id": "2510.20280", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.20280", "abs": "https://arxiv.org/abs/2510.20280", "authors": ["Beiya Dai", "Yuliang Liu", "Daozheng Xue", "Qipeng Guo", "Kai Chen", "Xinbing Wang"], "title": "Context-level Language Modeling by Learning Predictive Context Embeddings", "comment": "16pages,6 figures", "summary": "Next-token prediction (NTP) is the cornerstone of modern large language\nmodels (LLMs) pretraining, driving their unprecedented capabilities in text\ngeneration, reasoning, and instruction following. However, the token-level\nprediction limits the model's capacity to capture higher-level semantic\nstructures and long-range contextual relationships. To overcome this\nlimitation, we introduce \\textbf{ContextLM}, a framework that augments standard\npretraining with an inherent \\textbf{next-context prediction} objective. This\nmechanism trains the model to learn predictive representations of multi-token\ncontexts, leveraging error signals derived from future token chunks. Crucially,\nContextLM achieves this enhancement while remaining fully compatible with the\nstandard autoregressive, token-by-token evaluation paradigm (e.g., perplexity).\nExtensive experiments on the GPT2 and Pythia model families, scaled up to\n$1.5$B parameters, show that ContextLM delivers consistent improvements in both\nperplexity and downstream task performance. Our analysis indicates that\nnext-context prediction provides a scalable and efficient pathway to stronger\nlanguage modeling, yielding better long-range coherence and more effective\nattention allocation with minimal computational overhead.", "AI": {"tldr": "Introduces ContextLM, a framework that enhances standard pretraining by adding a next-context prediction objective, improving long-range coherence and attention allocation in large language models.", "motivation": "To address the limitations of token-level prediction in large language models, which restrict the model's ability to capture higher-level semantic structures and long-range contextual relationships.", "method": "Developing ContextLM, which trains models to predict multi-token contexts, using error signals from future token chunks, while maintaining compatibility with the standard autoregressive, token-by-token evaluation paradigm.", "result": "Experiments on GPT2 and Pythia show consistent improvements in perplexity and downstream task performance, indicating a scalable and efficient avenue for stronger language modeling.", "conclusion": "Next-context prediction with ContextLM leads to better long-range coherence and attention allocation with minimal computational overhead, representing an upgrade to current next-token prediction methods."}}
{"id": "2510.20196", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.20196", "abs": "https://arxiv.org/abs/2510.20196", "authors": ["Minh Sao Khue Luu", "Margaret V. Benedichuk", "Ekaterina I. Roppert", "Roman M. Kenzhin", "Bair N. Tuchinov"], "title": "A Structured Review and Quantitative Profiling of Public Brain MRI Datasets for Foundation Model Development", "comment": null, "summary": "The development of foundation models for brain MRI depends critically on the\nscale, diversity, and consistency of available data, yet systematic assessments\nof these factors remain scarce. In this study, we analyze 54 publicly\naccessible brain MRI datasets encompassing over 538,031 to provide a\nstructured, multi-level overview tailored to foundation model development. At\nthe dataset level, we characterize modality composition, disease coverage, and\ndataset scale, revealing strong imbalances between large healthy cohorts and\nsmaller clinical populations. At the image level, we quantify voxel spacing,\norientation, and intensity distributions across 15 representative datasets,\ndemonstrating substantial heterogeneity that can influence representation\nlearning. We then perform a quantitative evaluation of preprocessing\nvariability, examining how intensity normalization, bias field correction,\nskull stripping, spatial registration, and interpolation alter voxel statistics\nand geometry. While these steps improve within-dataset consistency, residual\ndifferences persist between datasets. Finally, feature-space case study using a\n3D DenseNet121 shows measurable residual covariate shift after standardized\npreprocessing, confirming that harmonization alone cannot eliminate\ninter-dataset bias. Together, these analyses provide a unified characterization\nof variability in public brain MRI resources and emphasize the need for\npreprocessing-aware and domain-adaptive strategies in the design of\ngeneralizable brain MRI foundation models.", "AI": {"tldr": "研究分析了大脑MRI数据集的特点，揭示了数据集和图像层面的异质性，以及标准化预处理不能完全消除的数据集偏差，强调了开发预处理感知和领域适应策略的重要性。", "motivation": "本文旨在系统地评估大脑MRI数据的规模、多样性和一致性，这些是开发大脑MRI基础模型的关键因素，而对此类因素的系统评估目前非常稀缺。", "method": "本文通过分析54个公开可用的大脑MRI数据集，提供了涵盖逾538,031张图像的结构化、多层次概述，以支持基础模型的开发。研究在数据集层面和图像层面进行了详尽的评估，并考察了预处理变异对数据一致性和几何结构的影响。", "result": "在数据集层面，发现了健康人群的大规模群体与较小的临床群体之间存在显著不平衡；在图像层面，显示出了相当大的异质性；在预处理层面，尽管预处理步骤提高了数据集内的数据一致性，数据集之间的差异依然存在；特征空间的案例研究显示了即使经过标准化预处理后，仍存在可测量的残余协变量偏差。", "conclusion": "这些分析统一了公共大脑MRI资源中的变异性描述，强调需要在基础模型设计中采用预处理感知和领域适应策略来增强模型的泛化能力。"}}
{"id": "2510.20303", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.20303", "abs": "https://arxiv.org/abs/2510.20303", "authors": ["Jan Buchmann", "Iryna Gurevych"], "title": "Citation Failure: Definition, Analysis and Efficient Mitigation", "comment": "Under review. Paper repository:\n  https://github.com/UKPLab/arxiv2025-citation-failure", "summary": "Citations from LLM-based RAG systems are supposed to simplify response\nverification. However, this does not hold for citation failure, when a model\ngenerates a helpful response, but fails to cite complete evidence. In contrast\nto previous work, we propose to disentangle this from response failure, where\nthe response itself is flawed, and citing complete evidence is impossible. To\naddress citation failure, this work follows a two-step approach: (1) We study\nwhen citation failure occurs and (2) how it can be mitigated. For step 1, we\nextend prior work by investigating how the relation between response and\nevidence affects citation quality. We introduce CITECONTROL, a benchmark that\nsystematically varies this relation to analyze failure modes. Experiments show\nthat failures increase with relational complexity and suggest that combining\ncitation methods could improve performance, motivating step 2. To improve LLM\ncitation efficiently, we propose CITENTION, a framework integrating generative,\nattention-based, and retrieval-based methods. Results demonstrate substantial\ncitation improvements on CITECONTROL and in transfer settings. We make our data\nand code publicly available.", "AI": {"tldr": "本文提出并研究了引用失败的现象，开发了CITECONTROL基准测试引用质量，并提出CITENTION框架以改善基于大模型的引用系统中的引用效率。", "motivation": "本文旨在解决引用失败的问题，即模型生成有用响应但未引用完整证据的问题。这不同于响应本身出现问题的情况。", "method": "该研究采用两步方法：第一步研究引用失败发生的情况，第二步如何缓解引用失败。在第一步中，他们通过分析响应和证据之间的关系来扩展以前的工作，以研究引用质量。第二步中，提出CITENTION框架，将生成方法、基于注意力的方法和检索方法结合在一起，以提高引文效率。", "result": "研究结果显示，引用失败随着响应和证据之间关系复杂性的增加而增加。引入的CITECONTROL基准系统地改变了这种关系，以分析失败模式，而CITENTION框架则在多种设置下提升了引用性能。", "conclusion": "实验结果表明，CITENTION框架在CITECONTROL基准上的引文有了显著改善，并在其他场景下也有提升，且该研究已公开数据和代码。"}}
{"id": "2510.20206", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.20206", "abs": "https://arxiv.org/abs/2510.20206", "authors": ["Bingjie Gao", "Qianli Ma", "Xiaoxue Wu", "Shuai Yang", "Guanzhou Lan", "Haonan Zhao", "Jiaxuan Chen", "Qingyang Liu", "Yu Qiao", "Xinyuan Chen", "Yaohui Wang", "Li Niu"], "title": "RAPO++: Cross-Stage Prompt Optimization for Text-to-Video Generation via Data Alignment and Test-Time Scaling", "comment": null, "summary": "Prompt design plays a crucial role in text-to-video (T2V) generation, yet\nuser-provided prompts are often short, unstructured, and misaligned with\ntraining data, limiting the generative potential of diffusion-based T2V models.\nWe present \\textbf{RAPO++}, a cross-stage prompt optimization framework that\nunifies training-data--aligned refinement, test-time iterative scaling, and\nlarge language model (LLM) fine-tuning to substantially improve T2V generation\nwithout modifying the underlying generative backbone. In \\textbf{Stage 1},\nRetrieval-Augmented Prompt Optimization (RAPO) enriches user prompts with\nsemantically relevant modifiers retrieved from a relation graph and refactors\nthem to match training distributions, enhancing compositionality and\nmulti-object fidelity. \\textbf{Stage 2} introduces Sample-Specific Prompt\nOptimization (SSPO), a closed-loop mechanism that iteratively refines prompts\nusing multi-source feedback -- including semantic alignment, spatial fidelity,\ntemporal coherence, and task-specific signals such as optical flow -- yielding\nprogressively improved video generation quality. \\textbf{Stage 3} leverages\noptimized prompt pairs from SSPO to fine-tune the rewriter LLM, internalizing\ntask-specific optimization patterns and enabling efficient, high-quality prompt\ngeneration even before inference. Extensive experiments across five\nstate-of-the-art T2V models and five benchmarks demonstrate that RAPO++\nachieves significant gains in semantic alignment, compositional reasoning,\ntemporal stability, and physical plausibility, outperforming existing methods\nby large margins. Our results highlight RAPO++ as a model-agnostic,\ncost-efficient, and scalable solution that sets a new standard for prompt\noptimization in T2V generation. The code is available at\nhttps://github.com/Vchitect/RAPO.", "AI": {"tldr": "RAPO++ 是一个跨阶段提示优化框架，通过与训练数据对齐的精炼、测试时的迭代扩展和大型语言模型细化，显著提升文本到视频生成的质量。", "motivation": "解决用户提供的提示较短、结构松散且与训练数据不匹配的问题，这些问题限制了基于扩散的文本到视频生成模型的生成潜力。", "method": "RAPO++ 包含三个阶段：RAPO阶段通过检索富含语义的修饰符提高提示的质量；SSPO阶段通过多源反馈迭代优化提示；第三阶段利用SSPO优化的提示对重写器LLM进行微调。", "result": "在五个最先进的文本到视频模型和五个基准测试中的广泛实验表明，RAPO++ 在语义对齐、组合推理、时间稳定性和物理可信度方面均有显著提升，并大幅超越现有方法。", "conclusion": "RAPO++ 提供了一个模型无关、成本效益高且可扩展的解决方案，为文本到视频生成中的提示优化设定了新标准。"}}
{"id": "2510.20304", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.20304", "abs": "https://arxiv.org/abs/2510.20304", "authors": ["Lei Tang", "Wei Zhou", "Mohsen Mesgar"], "title": "Exploring Generative Process Reward Modeling for Semi-Structured Data: A Case Study of Table Question Answering", "comment": null, "summary": "Process reward models (PRMs) improve complex reasoning in large language\nmodels (LLMs) by grading candidate solutions step-by-step and selecting answers\nvia aggregated step scores. While effective in domains such as mathematics,\ntheir applicability to tasks involving semi-structured data, like table\nquestion answering (TQA) remains unexplored. TQA poses unique challenges for\nPRMs, including abundant irrelevant information, loosely connected reasoning\nsteps, and domain-specific reasoning. This work presents the first systematic\nstudy of PRMs for TQA. We evaluate state-of-the-art generative PRMs on TQA from\nboth answer and step perspectives. Results show that PRMs that combine textual\nand code verification can aid solution selection but struggle to generalize to\nout-of-domain data. Analysis reveals a weak correlation between performance in\nstep-level verification and answer accuracy, possibly stemming from weak step\ndependencies and loose causal links. Our findings highlight limitations of\ncurrent PRMs on TQA and offer valuable insights for building more robust,\nprocess-aware verifiers.", "AI": {"tldr": "研究发现过程奖励模型在结合文本和代码验证方面可以帮助选择解决方案，但难以推广到域外数据。研究结果揭示了步骤级验证性能与答案准确性之间的弱相关性。", "motivation": "虽然过程奖励模型在数学等领域中表现出色，但其在涉及半结构化数据的任务，如TQA任务上的应用尚未被探索。本研究旨在填补这一空白。", "method": "本研究是首次系统性地探讨过程奖励模型（PRMs）在表格问题回答（TQA）任务上的应用。研究评估了几种先进的生成过程奖励模型在TQA任务中的表现，从答案和步骤两个角度进行评价。", "result": "研究表明，结合文本和代码验证的过程奖励模型对选择解决方案有所帮助，但在域外数据上的表现不佳。分析指出步骤依赖性和因果关系松弛可能是答案准确性低下的原因之一。", "conclusion": "研究指出了现有过程奖励模型在TQA中的局限性，并为构建更加健壮的过程感知验证器提供了有价值的观点。"}}
{"id": "2510.20212", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.20212", "abs": "https://arxiv.org/abs/2510.20212", "authors": ["Yanghao Wang", "Zhen Wang", "Long Chen"], "title": "FlowCycle: Pursuing Cycle-Consistent Flows for Text-based Editing", "comment": null, "summary": "Recent advances in pre-trained text-to-image flow models have enabled\nremarkable progress in text-based image editing. Mainstream approaches always\nadopt a corruption-then-restoration paradigm, where the source image is first\ncorrupted into an ``intermediate state'' and then restored to the target image\nunder the prompt guidance. However, current methods construct this intermediate\nstate in a target-agnostic manner, i.e., they primarily focus on realizing\nsource image reconstruction while neglecting the semantic gaps towards the\nspecific editing target. This design inherently results in limited editability\nor inconsistency when the desired modifications substantially deviate from the\nsource. In this paper, we argue that the intermediate state should be\ntarget-aware, i.e., selectively corrupting editing-relevant contents while\npreserving editing-irrelevant ones. To this end, we propose FlowCycle, a novel\ninversion-free and flow-based editing framework that parameterizes corruption\nwith learnable noises and optimizes them through a cycle-consistent process. By\niteratively editing the source to the target and recovering back to the source\nwith dual consistency constraints, FlowCycle learns to produce a target-aware\nintermediate state, enabling faithful modifications while preserving source\nconsistency. Extensive ablations have demonstrated that FlowCycle achieves\nsuperior editing quality and consistency over state-of-the-art methods.", "AI": {"tldr": "FlowCycle通过采用循环一致的过程优化，学习生成目标感知的中间状态，提高了文本到图像编辑的质量和一致性。", "motivation": "主流方法采用盲目腐败然后修复的范式，忽略与特定编辑目标的语义差距，导致编辑能力有限或不一致。本文认为中间状态应当感知目标，以提高编辑的忠实度和一致性。", "method": "本文提出了一种名为FlowCycle的新框架，该框架采用可学习的噪声参数化腐败过程并通过循环一致过程优化这些噪声。这种方法可以使中间状态具有目标感知性，即选择性地腐败与编辑相关内容而保留无关内容。", "result": "实验表明，FlowCycle在编辑质量上优于现有的最先进的方法，同时保持了一致性。", "conclusion": "FlowCycle是一种无逆向的、基于流的编辑框架，通过迭代编辑和恢复过程学习自洽的目标感知中间状态，从而提高编辑效果。"}}
{"id": "2510.20342", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.20342", "abs": "https://arxiv.org/abs/2510.20342", "authors": ["Chengpeng Li", "Zhengyang Tang", "Ziniu Li", "Mingfeng Xue", "Keqin Bao", "Tian Ding", "Ruoyu Sun", "Benyou Wang", "Xiang Wang", "Junyang Lin", "Dayiheng Liu"], "title": "Teaching Language Models to Reason with Tools", "comment": "NIPS2025 Accepted", "summary": "Large reasoning models (LRMs) like OpenAI-o1 have shown impressive\ncapabilities in natural language reasoning. However, these models frequently\ndemonstrate inefficiencies or inaccuracies when tackling complex mathematical\noperations. While integrating computational tools such as Code Interpreters\n(CIs) offers a promising solution, it introduces a critical challenge: a\nconflict between the model's internal, probabilistic reasoning and the\nexternal, deterministic knowledge provided by the CI, which often leads models\nto unproductive deliberation. To overcome this, we introduce CoRT\n(Code-Optimized Reasoning Training), a post-training framework designed to\nteach LRMs to effectively utilize CIs. We propose \\emph{Hint-Engineering}, a\nnew data synthesis strategy that strategically injects diverse hints at optimal\npoints within reasoning paths. This approach generates high-quality,\ncode-integrated reasoning data specifically tailored to optimize LRM-CI\ninteraction. Using this method, we have synthesized 30 high-quality samples to\npost-train models ranging from 1.5B to 32B parameters through supervised\nfine-tuning. CoRT further refines the multi-round interleaving of external CI\nusage and internal thinking by employing rejection sampling and reinforcement\nlearning. Our experimental evaluations demonstrate CoRT's effectiveness,\nyielding absolute improvements of 4\\% and 8\\% on DeepSeek-R1-Distill-Qwen-32B\nand DeepSeek-R1-Distill-Qwen-1.5B, respectively, across five challenging\nmathematical reasoning datasets. Moreover, CoRT significantly enhances\nefficiency, reducing token usage by approximately 30\\% for the 32B model and\n50\\% for the 1.5B model compared to pure natural language reasoning baselines.\nThe models and code are available at: https://github.com/ChengpengLi1003/CoRT.", "AI": {"tldr": "提出了一种名为CoRT的后训练框架，以解决大规模推理模型在处理复杂数学操作时的效率和准确性问题，该框架通过Hint-Engineering策略合成高质量的代码集成推理数据，提升了模型与代码解释器的交互效果。", "motivation": "解决大规模推理模型在执行复杂数学操作时经常出现的低效或不准确问题，通过引入代码解释器来增强模型的表现。", "method": "提出一种名为CoRT的后训练框架，采用Hint-Engineering策略合成高质量的代码集成推理数据，并使用拒绝抽样和强化学习来优化模型与代码解释器的多轮交互。", "result": "相对于纯自然语言推理基线，在五个具有挑战性的数学推理数据集上，DeepSeek-R1-Distill-Qwen-32B和DeepSeek-R1-Distill-Qwen-1.5B分别获得了4%和8%的绝对提高，同时减少了约30%到50%的令牌使用量。", "conclusion": "CoRT框架通过提高与代码解释器的交互效率，显著增强了大规模推理模型在数学推理任务上的表现和效率。"}}
{"id": "2510.20214", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.20214", "abs": "https://arxiv.org/abs/2510.20214", "authors": ["Talha Ilyas", "Duong Nhu", "Allison Thomas", "Arie Levin", "Lim Wei Yap", "Shu Gong", "David Vera Anaya", "Yiwen Jiang", "Deval Mehta", "Ritesh Warty", "Vinayak Smith", "Maya Reddy", "Euan Wallace", "Wenlong Cheng", "Zongyuan Ge", "Faezeh Marzbanrad"], "title": "Towards Objective Obstetric Ultrasound Assessment: Contrastive Representation Learning for Fetal Movement Detection", "comment": "This is the preprint version of the manuscript submitted to IEEE\n  Journal of Biomedical and Health Informatics (JBHI) for review", "summary": "Accurate fetal movement (FM) detection is essential for assessing prenatal\nhealth, as abnormal movement patterns can indicate underlying complications\nsuch as placental dysfunction or fetal distress. Traditional methods, including\nmaternal perception and cardiotocography (CTG), suffer from subjectivity and\nlimited accuracy. To address these challenges, we propose Contrastive\nUltrasound Video Representation Learning (CURL), a novel self-supervised\nlearning framework for FM detection from extended fetal ultrasound video\nrecordings. Our approach leverages a dual-contrastive loss, incorporating both\nspatial and temporal contrastive learning, to learn robust motion\nrepresentations. Additionally, we introduce a task-specific sampling strategy,\nensuring the effective separation of movement and non-movement segments during\nself-supervised training, while enabling flexible inference on arbitrarily long\nultrasound recordings through a probabilistic fine-tuning approach. Evaluated\non an in-house dataset of 92 subjects, each with 30-minute ultrasound sessions,\nCURL achieves a sensitivity of 78.01% and an AUROC of 81.60%, demonstrating its\npotential for reliable and objective FM analysis. These results highlight the\npotential of self-supervised contrastive learning for fetal movement analysis,\npaving the way for improved prenatal monitoring and clinical decision-making.", "AI": {"tldr": "The paper introduces CURL, a self-supervised learning framework for detecting fetal movements in ultrasound videos with improved accuracy over traditional methods. It achieved sensitivity of 78.01% and AUROC of 81.60%.", "motivation": "The aim is to enhance the accuracy of fetal movement detection, which is critical for prenatal health assessments. Traditional methods, maternal perception, and cardiotocography (CTG) are subjective and not highly accurate, so the paper proposes the CURL method to provide more reliable and objective fetal movement analysis.", "method": "Our approach, Contrastive Ultrasound Video Representation Learning (CURL), is a novel self-supervised learning framework designed for fetal movement (FM) detection. It uses a dual-contrastive loss that incorporates both spatial and temporal contrastive learning to improve the accuracy of FM detection. Additionally, it has a task-specific sampling strategy to effectively separate movement and non-movement segments.", "result": "The method was tested on an in-house dataset of 92 subjects with 30-minute ultrasound sessions, and it achieved a sensitivity of 78.01% and an AUROC of 81.60%.", "conclusion": "The paper concludes that self-supervised contrastive learning via the CURL method holds promise for improving the reliability and objectivity of fetal movement analysis, ultimately enhancing prenatal monitoring and clinical decision-making."}}
{"id": "2510.20351", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.20351", "abs": "https://arxiv.org/abs/2510.20351", "authors": ["Matteo Silvestri", "Flavio Giorgi", "Fabrizio Silvestri", "Gabriele Tolomei"], "title": "Evaluating Latent Knowledge of Public Tabular Datasets in Large Language Models", "comment": null, "summary": "Large Language Models (LLMs) are increasingly evaluated on their ability to\nreason over structured data, yet such assessments often overlook a crucial\nconfound: dataset contamination. In this work, we investigate whether LLMs\nexhibit prior knowledge of widely used tabular benchmarks such as Adult Income,\nTitanic, and others. Through a series of controlled probing experiments, we\nreveal that contamination effects emerge exclusively for datasets containing\nstrong semantic cues-for instance, meaningful column names or interpretable\nvalue categories. In contrast, when such cues are removed or randomized,\nperformance sharply declines to near-random levels. These findings suggest that\nLLMs' apparent competence on tabular reasoning tasks may, in part, reflect\nmemorization of publicly available datasets rather than genuine generalization.\nWe discuss implications for evaluation protocols and propose strategies to\ndisentangle semantic leakage from authentic reasoning ability in future LLM\nassessments.", "AI": {"tldr": "The paper investigates whether Large Language Models (LLMs) have prior knowledge of widely used tabular benchmarks due to dataset contamination, finding that performance on these tasks may be due to memorization of available data with semantic cues rather than real reasoning ability.", "motivation": "To evaluate whether the perceived reasoning abilities of LLMs over structured data are genuine or rather due to memorization of publicly available datasets, which have strong semantic cues.", "method": "The researchers conducted controlled probing experiments on datasets known for containing strong semantic cues, comparing their performance to cases where these cues were altered or removed.", "result": "Performance of LLMs significantly declined when semantic cues were removed or randomized, indicating that their ability on these tasks is largely a result of memorization rather than reasoning.", "conclusion": "The paper concludes that LLMs' apparent proficiency in reasoning tasks on structured data may not be due to genuine reasoning ability but rather to memorization of datasets with strong semantic cues. Evaluation protocols need revision to ensure accurate assessment of LLMs' capabilities."}}
{"id": "2510.20217", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.20217", "abs": "https://arxiv.org/abs/2510.20217", "authors": ["Jiahuan Wang", "Yuxin Chen", "Jun Yu", "Guangming Lu", "Wenjie Pei"], "title": "EditInfinity: Image Editing with Binary-Quantized Generative Models", "comment": "28 pages, 13 figures, accepted by The Thirty-ninth Annual Conference\n  on Neural Information Processing Systems (NeurIPS 2025)", "summary": "Adapting pretrained diffusion-based generative models for text-driven image\nediting with negligible tuning overhead has demonstrated remarkable potential.\nA classical adaptation paradigm, as followed by these methods, first infers the\ngenerative trajectory inversely for a given source image by image inversion,\nthen performs image editing along the inferred trajectory guided by the target\ntext prompts. However, the performance of image editing is heavily limited by\nthe approximation errors introduced during image inversion by diffusion models,\nwhich arise from the absence of exact supervision in the intermediate\ngenerative steps. To circumvent this issue, we investigate the\nparameter-efficient adaptation of VQ-based generative models for image editing,\nand leverage their inherent characteristic that the exact intermediate\nquantized representations of a source image are attainable, enabling more\neffective supervision for precise image inversion. Specifically, we propose\n\\emph{EditInfinity}, which adapts \\emph{Infinity}, a binary-quantized\ngenerative model, for image editing. We propose an efficient yet effective\nimage inversion mechanism that integrates text prompting rectification and\nimage style preservation, enabling precise image inversion. Furthermore, we\ndevise a holistic smoothing strategy which allows our \\emph{EditInfinity} to\nperform image editing with high fidelity to source images and precise semantic\nalignment to the text prompts. Extensive experiments on the PIE-Bench benchmark\nacross \"add\", \"change\", and \"delete\" editing operations, demonstrate the\nsuperior performance of our model compared to state-of-the-art diffusion-based\nbaselines. Code available at: https://github.com/yx-chen-ust/EditInfinity.", "AI": {"tldr": "研究提出EditInfinity，通过适应VQ模型和创新编辑策略，实现了相较于扩散模型更高的图像编辑精度和保真度。", "motivation": "为了克服传统基于扩散模型的文本驱动图像编辑中由于图像反转引入的近似误差限制，本研究探索了基于VQ的生成模型的参数高效适应方法，利用这些模型中间量化表示精确可得的特点，提供更有效的监督以实现精准的图像反转。", "method": "本研究提出了一种名为EditInfinity的方法，通过适应二值量化生成模型Infinity来进行图像编辑。这种方法结合了文本提示的校正与图像风格的保留，提出了一个高效精确的图像反转机制，以及一种整体的平滑策略，以实现对原图像高保真编辑和对文本提示语义的精确对齐。", "result": "实验结果表明，EditInfinity在PIE-Bench基准上进行“添加”、“更改”和“删除”编辑操作时，相比最先进的扩散模型基线，表现出更优越的性能。", "conclusion": "本研究通过适应二值量化生成模型并提出创新性的图像编辑策略，成功提高了图像编辑的精度和语义对齐度，展示了相对于扩散模型的显著优势。"}}
{"id": "2510.20356", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.20356", "abs": "https://arxiv.org/abs/2510.20356", "authors": ["Wenxuan Zhang", "Yuan-Hao Jiang", "Yonghe Wu"], "title": "FreeChunker: A Cross-Granularity Chunking Framework", "comment": "Submitted to arXiv, October 2025", "summary": "Chunking strategies significantly impact the effectiveness of\nRetrieval-Augmented Generation (RAG) systems. Existing methods operate within\nfixed-granularity paradigms that rely on static boundary identification,\nlimiting their adaptability to diverse query requirements. This paper presents\nFreeChunker, a Cross-Granularity Encoding Framework that fundamentally\ntransforms the traditional chunking paradigm: the framework treats sentences as\natomic units and shifts from static chunk segmentation to flexible retrieval\nsupporting arbitrary sentence combinations. This paradigm shift not only\nsignificantly reduces the computational overhead required for semantic boundary\ndetection but also enhances adaptability to complex queries. Experimental\nevaluation on LongBench V2 demonstrates that FreeChunker achieves superior\nretrieval performance compared to traditional chunking methods, while\nsignificantly outperforming existing approaches in computational efficiency.", "AI": {"tldr": "The paper introduces FreeChunker, a flexible chunking method that enhances the adaptability and efficiency of Retrieval-Augmented Generation systems, outperforming traditional methods experimentally.", "motivation": "To address the limitations of existing chunking methods that rely on static boundary identification, impacting adaptability and computational efficiency.", "method": "FreeChunker, a Cross-Granularity Encoding Framework, treats sentences as atomic units and supports flexible retrieval of arbitrary sentence combinations.", "result": "FreeChunker achieves superior retrieval performance and significantly outperforms existing approaches in computational efficiency on the LongBench V2 dataset.", "conclusion": "The proposed framework improves adaptability to complex queries and reduces the computational overhead required for semantic boundary detection."}}
{"id": "2510.20229", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2510.20229", "abs": "https://arxiv.org/abs/2510.20229", "authors": ["Ge Zheng", "Jiaye Qian", "Jiajin Tang", "Sibei Yang"], "title": "Why LVLMs Are More Prone to Hallucinations in Longer Responses: The Role of Context", "comment": null, "summary": "Large Vision-Language Models (LVLMs) have made significant progress in recent\nyears but are also prone to hallucination issues. They exhibit more\nhallucinations in longer, free-form responses, often attributed to accumulated\nuncertainties. In this paper, we ask: Does increased hallucination result\nsolely from length-induced errors, or is there a deeper underlying mechanism?\nAfter a series of preliminary experiments and findings, we suggest that the\nrisk of hallucinations is not caused by length itself but by the increased\nreliance on context for coherence and completeness in longer responses.\nBuilding on these insights, we propose a novel \"induce-detect-suppress\"\nframework that actively induces hallucinations through deliberately designed\ncontexts, leverages induced instances for early detection of high-risk cases,\nand ultimately suppresses potential object-level hallucinations during actual\ndecoding. Our approach achieves consistent, significant improvements across all\nbenchmarks, demonstrating its efficacy. The strong detection and improved\nhallucination mitigation not only validate our framework but, more importantly,\nre-validate our hypothesis on context. Rather than solely pursuing performance\ngains, this study aims to provide new insights and serves as a first step\ntoward a deeper exploration of hallucinations in LVLMs' longer responses.", "AI": {"tldr": "通过一系列实验，研究发现大型视觉语言模型（LVLMs）在长答案中产生幻觉的原因是因为上下文依赖性增加，而不仅仅是答案长度增加。为了减少幻觉，提出了一个'诱导-检测-抑制'框架，该框架在多个基准测试中展示了其有效性。", "motivation": "提出该研究的动机是探索大型视觉语言模型（LVLMs）中的幻觉问题，特别是这些模型在长形式的自由回应中表现出更多幻觉现象的原因，是否仅仅是由于长度导致的误差，还是存在更深层次的机制。", "method": "通过一系列初步实验和发现，提出了一种新的“诱导-检测-抑制”框架，该框架通过故意设计的上下文主动诱导幻觉，利用诱发的实例进行高风险案例的早期检测，并在实际解码过程中抑制潜在的对象级幻觉。", "result": "研究方法实现了在所有基准测试中持续且显著的改进，展示了其有效性。", "conclusion": "强大的检测能力和改善幻觉抑制效果不仅验证了所提出的框架，而且更重要的是，再次验证了关于上下文的假设。研究表明，幻觉风险不是由响应长度本身引起的，而是由长响应中为了连贯性和完整性而增加的上下文依赖性导致的。"}}
{"id": "2510.20358", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.20358", "abs": "https://arxiv.org/abs/2510.20358", "authors": ["Francesca Padovani", "Bastian Bunzeck", "Manar Ali", "Omar Momen", "Arianna Bisazza", "Hendrik Buschmeier", "Sina Zarrieß"], "title": "Dialogue Is Not Enough to Make a Communicative BabyLM (But Neither Is Developmentally Inspired Reinforcement Learning)", "comment": null, "summary": "We investigate whether pre-training exclusively on dialogue data results in\nformally and functionally apt small language models. Based on this pre-trained\nllamalogue model, we employ a variety of fine-tuning strategies to enforce\n\"more communicative\" text generations by our models. Although our models\nunderperform on most standard BabyLM benchmarks, they excel at dialogue\ncontinuation prediction in a minimal pair setting. While PPO fine-tuning has\nmixed to adversarial effects on our models, DPO fine-tuning further improves\ntheir performance on our custom dialogue benchmark.", "AI": {"tldr": "研究了仅使用对话数据进行预训练是否能生成功能良好的小型语言模型，并通过不同的微调策略使模型生成更有效的对话文本。该模型在大部分标准基准测试中表现不佳，但在对话延续预测上表现出色，尤其是在采用DPO微调策略时。", "motivation": "研究目的在于探索专门针对对话数据预训练的语言模型是否能在对话任务上表现出更好的性能，并通过不同的微调策略改进其对话生成能力。", "method": "通过对话数据训练了一个小型语言模型（llamalogue），并使用多种微调策略（如PPO和DPO）来优化模型生成的对话文本。", "result": "模型在标准BabyLM基准测试中的表现不佳，但在专门设计的对话延续预测任务中表现优越，特别是通过DPO微调后。PPO微调策略的效果不明显甚至有时是负面的。", "conclusion": "仅使用对话数据预训练的小型语言模型在特定对话任务上可以表现出色，DPO微调策略能够进一步提升其对话生成效果，展现出定制化的优点。"}}
{"id": "2510.20238", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.20238", "abs": "https://arxiv.org/abs/2510.20238", "authors": ["Runsong Zhu", "Ka-Hei Hui", "Zhengzhe Liu", "Qianyi Wu", "Weiliang Tang", "Shi Qiu", "Pheng-Ann Heng", "Chi-Wing Fu"], "title": "COS3D: Collaborative Open-Vocabulary 3D Segmentation", "comment": "NeurIPS 2025. The code is publicly available at\n  \\href{https://github.com/Runsong123/COS3D}{https://github.com/Runsong123/COS3D}", "summary": "Open-vocabulary 3D segmentation is a fundamental yet challenging task,\nrequiring a mutual understanding of both segmentation and language. However,\nexisting Gaussian-splatting-based methods rely either on a single 3D language\nfield, leading to inferior segmentation, or on pre-computed class-agnostic\nsegmentations, suffering from error accumulation. To address these limitations,\nwe present COS3D, a new collaborative prompt-segmentation framework that\ncontributes to effectively integrating complementary language and segmentation\ncues throughout its entire pipeline. We first introduce the new concept of\ncollaborative field, comprising an instance field and a language field, as the\ncornerstone for collaboration. During training, to effectively construct the\ncollaborative field, our key idea is to capture the intrinsic relationship\nbetween the instance field and language field, through a novel\ninstance-to-language feature mapping and designing an efficient two-stage\ntraining strategy. During inference, to bridge distinct characteristics of the\ntwo fields, we further design an adaptive language-to-instance prompt\nrefinement, promoting high-quality prompt-segmentation inference. Extensive\nexperiments not only demonstrate COS3D's leading performance over existing\nmethods on two widely-used benchmarks but also show its high potential to\nvarious applications,~\\ie, novel image-based 3D segmentation, hierarchical\nsegmentation, and robotics. The code is publicly available at\n\\href{https://github.com/Runsong123/COS3D}{https://github.com/Runsong123/COS3D}.", "AI": {"tldr": "COS3D offers a new collaborative framework for 3D segmentation that combines language and segmentation fields more effectively, leading to improved performance over current methods.", "motivation": "The motivation behind the paper is to enhance the quality of open-vocabulary 3D segmentation by overcoming the limitations of existing approaches that either use a single 3D language field or rely on pre-computed segmentations, both of which lead to inferior results or error accumulation.", "method": "The paper introduces COS3D, which utilizes a collaborative prompt-segmentation framework to improve 3D segmentation by integrating complementary cues from both segmentation and language fields. It employs a novel two-stage training strategy and an adaptive refinement technique during inference.", "result": "Experiments demonstrate that COS3D leads in performance on established benchmarks and shows promise in diverse applications such as 3D segmentation for novel images and robotics.", "conclusion": "The conclusion drawn from the paper is that by improving the mutual understanding between segmentation and language through their framework COS3D, the researchers have enabled better performance in open-vocabulary 3D segmentation tasks, highlighting the potential for future applications."}}
{"id": "2510.20375", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.20375", "abs": "https://arxiv.org/abs/2510.20375", "authors": ["Jaehyung Seo", "Hyeonseok Moon", "Heuiseok Lim"], "title": "The Impact of Negated Text on Hallucination with Large Language Models", "comment": "Accepted to the EMNLP 2025", "summary": "Recent studies on hallucination in large language models (LLMs) have been\nactively progressing in natural language processing. However, the impact of\nnegated text on hallucination with LLMs remains largely unexplored. In this\npaper, we set three important yet unanswered research questions and aim to\naddress them. To derive the answers, we investigate whether LLMs can recognize\ncontextual shifts caused by negation and still reliably distinguish\nhallucinations comparable to affirmative cases. We also design the NegHalu\ndataset by reconstructing existing hallucination detection datasets with\nnegated expressions. Our experiments demonstrate that LLMs struggle to detect\nhallucinations in negated text effectively, often producing logically\ninconsistent or unfaithful judgments. Moreover, we trace the internal state of\nLLMs as they process negated inputs at the token level and reveal the\nchallenges of mitigating their unintended effects.", "AI": {"tldr": "本文通过设计NegHalu数据集来研究大规模语言模型在否定文本环境下识别幻觉的困难，展示了这些模型在处理否定输入时的逻辑不一致性和不可靠性。", "motivation": "虽然关于大规模语言模型中的幻觉现象已有大量研究，但否定文本对幻觉的影响仍然缺乏探索。本文针对三个重要的研究问题展开研究，以填补这一空白。", "method": "我们通过设计一个名为NegHalu的数据集来研究大规模语言模型在识别否定文本中的幻觉问题，该数据集通过对现有幻觉检测数据集进行重构，加入了否定表达。同时，我们在词汇层面追踪大规模语言模型处理否定输入时的内部状态，揭示了减轻其不利影响的挑战。", "result": "实验表明，大规模语言模型在否定文本中检测幻觉的能力较差，常常产生逻辑不一致或不忠实的判断。", "conclusion": "我们的研究揭示了大规模语言模型在处理否定文本时检测幻觉方面的困难，并在词汇层面分析了其内部状态，为解决这一挑战提供了见解。"}}
{"id": "2510.20244", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.20244", "abs": "https://arxiv.org/abs/2510.20244", "authors": ["Minseok Kang", "Minhyeok Lee", "Minjung Kim", "Donghyeong Kim", "Sangyoun Lee"], "title": "Empower Words: DualGround for Structured Phrase and Sentence-Level Temporal Grounding", "comment": "Comments: 28 pages, including appendix. 5 figures. Full version of\n  the NeurIPS 2025 paper", "summary": "Video Temporal Grounding (VTG) aims to localize temporal segments in long,\nuntrimmed videos that align with a given natural language query. This task\ntypically comprises two subtasks: Moment Retrieval (MR) and Highlight Detection\n(HD). While recent advances have been progressed by powerful pretrained\nvision-language models such as CLIP and InternVideo2, existing approaches\ncommonly treat all text tokens uniformly during crossmodal attention,\ndisregarding their distinct semantic roles. To validate the limitations of this\napproach, we conduct controlled experiments demonstrating that VTG models\noverly rely on [EOS]-driven global semantics while failing to effectively\nutilize word-level signals, which limits their ability to achieve fine-grained\ntemporal alignment. Motivated by this limitation, we propose DualGround, a\ndual-branch architecture that explicitly separates global and local semantics\nby routing the [EOS] token through a sentence-level path and clustering word\ntokens into phrase-level units for localized grounding. Our method introduces\n(1) tokenrole- aware cross modal interaction strategies that align video\nfeatures with sentence-level and phrase-level semantics in a structurally\ndisentangled manner, and (2) a joint modeling framework that not only improves\nglobal sentence-level alignment but also enhances finegrained temporal\ngrounding by leveraging structured phrase-aware context. This design allows the\nmodel to capture both coarse and localized semantics, enabling more expressive\nand context-aware video grounding. DualGround achieves state-of-the-art\nperformance on both Moment Retrieval and Highlight Detection tasks across\nQVHighlights and Charades- STA benchmarks, demonstrating the effectiveness of\ndisentangled semantic modeling in video-language alignment.", "AI": {"tldr": "DualGround提出了一种双分支架构，分别处理全局和局部语义，以改进基于自然语言视频时序定位任务中的全局和细粒度时间对齐问题。", "motivation": "由于现有的方法在交叉模态注意力中对所有文本标记一视同仁，忽视了它们在语义上的不同角色，导致过度依赖EOS驱动的全局语义而忽略了单词级信号。因此，DualGround旨在改进这种不足。", "method": "DualGround架构通过将EOS标记路由至句子级路径并将词汇标记聚类为短语级单位，实现了视频特征在结构上分离开的句级和短语级语义对齐策略，以及通过利用结构化的短语级上下文提升了整体句级对齐和细粒度时间对齐的联合建模框架。", "result": "DualGround在Moment Retrieval和Highlight Detection任务上实现了最新的性能，这在QVHighlights和Charades-STA基准测试中得到了体现。", "conclusion": "通过双重关系感知的跨模态交互策略和联合建模框架，DualGround有效地处理了视频文本对齐中的解耦语义建模问题，能够捕捉粗粒度和局部语义，提高了视频定位的表达能力和上下文感知能力。"}}
{"id": "2510.20381", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.20381", "abs": "https://arxiv.org/abs/2510.20381", "authors": ["Son T. Luu", "Trung Vo", "Hiep Nguyen", "Khanh Quoc Tran", "Kiet Van Nguyen", "Vu Tran", "Ngan Luu-Thuy Nguyen", "Le-Minh Nguyen"], "title": "VLSP 2025 MLQA-TSR Challenge: Vietnamese Multimodal Legal Question Answering on Traffic Sign Regulation", "comment": "VLSP 2025 MLQA-TSR Share Task", "summary": "This paper presents the VLSP 2025 MLQA-TSR - the multimodal legal question\nanswering on traffic sign regulation shared task at VLSP 2025. VLSP 2025\nMLQA-TSR comprises two subtasks: multimodal legal retrieval and multimodal\nquestion answering. The goal is to advance research on Vietnamese multimodal\nlegal text processing and to provide a benchmark dataset for building and\nevaluating intelligent systems in multimodal legal domains, with a focus on\ntraffic sign regulation in Vietnam. The best-reported results on VLSP 2025\nMLQA-TSR are an F2 score of 64.55% for multimodal legal retrieval and an\naccuracy of 86.30% for multimodal question answering.", "AI": {"tldr": "VLSP 2025 MLQA-TSR共享任务，包含两个子任务：多模态法律检索和多模态问题回答，最佳结果分别是64.55%的F2得分和86.30%的准确率。", "motivation": "旨在促进越南多模态法律文本处理的研究，并为多模态法律领域智能系统的构建和评估提供一个基准数据集。", "method": "介绍了VLSP 2025 MLQA-TSR共享任务，该任务包括多模态法律检索和多模态问题回答两个子任务，目的是推进越南多模态法律文本处理的研究，并提供一个多模态法律领域的基准数据集，特别是针对越南的交通标志法规。", "result": "最佳报告结果为多模态法律检索的F2得分为64.55%，多模态问题回答的准确率为86.30%。", "conclusion": "该共享任务为研究越南多模态法律文本处理提供了宝贵的数据资源和评估基准。"}}
{"id": "2510.20247", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.20247", "abs": "https://arxiv.org/abs/2510.20247", "authors": ["Shuhan Hu", "Yiru Li", "Yuanyuan Li", "Yingying Zhu"], "title": "Seeing the Unseen: Mask-Driven Positional Encoding and Strip-Convolution Context Modeling for Cross-View Object Geo-Localization", "comment": null, "summary": "Cross-view object geo-localization enables high-precision object localization\nthrough cross-view matching, with critical applications in autonomous driving,\nurban management, and disaster response. However, existing methods rely on\nkeypoint-based positional encoding, which captures only 2D coordinates while\nneglecting object shape information, resulting in sensitivity to annotation\nshifts and limited cross-view matching capability. To address these\nlimitations, we propose a mask-based positional encoding scheme that leverages\nsegmentation masks to capture both spatial coordinates and object silhouettes,\nthereby upgrading the model from \"location-aware\" to \"object-aware.\"\nFurthermore, to tackle the challenge of large-span objects (e.g., elongated\nbuildings) in satellite imagery, we design a context enhancement module. This\nmodule employs horizontal and vertical strip convolutional kernels to extract\nlong-range contextual features, enhancing feature discrimination among\nstrip-like objects. Integrating MPE and CEM, we present EDGeo, an end-to-end\nframework for robust cross-view object geo-localization. Extensive experiments\non two public datasets (CVOGL and VIGOR-Building) demonstrate that our method\nachieves state-of-the-art performance, with a 3.39% improvement in localization\naccuracy under challenging ground-to-satellite scenarios. This work provides a\nrobust positional encoding paradigm and a contextual modeling framework for\nadvancing cross-view geo-localization research.", "AI": {"tldr": "本文提出了一种新的基于掩码的位置编码方案和一个上下文增强模块，开发了EDGeo框架，提升了跨视图物体地理定位的准确性和鲁棒性。", "motivation": "现有的方法依赖于基于关键点的位置编码，只能捕捉2D坐标，忽视了物体的形状信息，这导致了它们对标注偏移敏感和跨视图匹配能力有限。为了解决这些问题，我们提出了一个新的方法。", "method": "我们提出了基于掩码的位置编码方案，使用分割掩码捕捉空间坐标和物体轮廓，提升了模型从‘位置感知’到‘物体感知’的能力。此外，为了解决卫星图像中长条形建筑物等大范围物体的挑战，我们设计了一个上下文增强模块，使用水平和垂直条带卷积核提取长范围上下文特征。最终，我们将MPE和CEM整合，提出了EDGeo，一个用于鲁棒的跨视图物体地理定位的端到端框架。", "result": "在两个公共数据集（CVOGL和VIGOR-Building）上的广泛实验表明，我们的方法在具有挑战性的地面至卫星场景中实现了3.39%的定位准确率提升。", "conclusion": "本工作提供了一个健壮的位置编码范式和一个上下文建模框架，为推进跨视图地理定位研究做出了贡献。"}}
{"id": "2510.20386", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.20386", "abs": "https://arxiv.org/abs/2510.20386", "authors": ["Shaltiel Shmidman", "Avi Shmidman", "Moshe Koppel"], "title": "NeoDictaBERT: Pushing the Frontier of BERT models for Hebrew", "comment": null, "summary": "Since their initial release, BERT models have demonstrated exceptional\nperformance on a variety of tasks, despite their relatively small size\n(BERT-base has ~100M parameters). Nevertheless, the architectural choices used\nin these models are outdated compared to newer transformer-based models such as\nLlama3 and Qwen3. In recent months, several architectures have been proposed to\nclose this gap. ModernBERT and NeoBERT both show strong improvements on English\nbenchmarks and significantly extend the supported context window. Following\ntheir successes, we introduce NeoDictaBERT and NeoDictaBERT-bilingual:\nBERT-style models trained using the same architecture as NeoBERT, with a\ndedicated focus on Hebrew texts. These models outperform existing ones on\nalmost all Hebrew benchmarks and provide a strong foundation for downstream\ntasks. Notably, the NeoDictaBERT-bilingual model shows strong results on\nretrieval tasks, outperforming other multilingual models of similar size. In\nthis paper, we describe the training process and report results across various\nbenchmarks. We release the models to the community as part of our goal to\nadvance research and development in Hebrew NLP.", "AI": {"tldr": "研究者们提出了专门针对希伯来语的NeoDictaBERT和NeoDictaBERT-bilingual模型，它们在希伯来语基准测试中表现出色，并且在类似大小的多语言模型中，在检索任务上表现出色。", "motivation": "由于现有的BERT模型架构在新型变压器模型面前已经过时，因此研究者们提出新的建筑来缩小这种差距。目的在于推进希伯来语自然语言处理的研究和发展。", "method": "使用与NeoBERT相同的架构训练BERT风格的模型，并专门针对希伯来语文本。", "result": "这些模型在几乎所有希伯来语基准测试上都优于现有的模型，并为下游任务提供了强大的基础。", "conclusion": "模型的训练过程和各种基准测试的结果被详细描述，同时这些模型也被发布到社区中以推动希伯来语NLP研究和开发。"}}
{"id": "2510.20256", "categories": ["cs.CV", "cs.CL", "cs.LG", "cs.MM"], "pdf": "https://arxiv.org/pdf/2510.20256", "abs": "https://arxiv.org/abs/2510.20256", "authors": ["Guowei Zhong", "Junjie Li", "Huaiyu Zhu", "Ruohong Huan", "Yun Pan"], "title": "Calibrating Multimodal Consensus for Emotion Recognition", "comment": null, "summary": "In recent years, Multimodal Emotion Recognition (MER) has made substantial\nprogress. Nevertheless, most existing approaches neglect the semantic\ninconsistencies that may arise across modalities, such as conflicting emotional\ncues between text and visual inputs. Besides, current methods are often\ndominated by the text modality due to its strong representational capacity,\nwhich can compromise recognition accuracy. To address these challenges, we\npropose a model termed Calibrated Multimodal Consensus (CMC). CMC introduces a\nPseudo Label Generation Module (PLGM) to produce pseudo unimodal labels,\nenabling unimodal pretraining in a self-supervised fashion. It then employs a\nParameter-free Fusion Module (PFM) and a Multimodal Consensus Router (MCR) for\nmultimodal finetuning, thereby mitigating text dominance and guiding the fusion\nprocess toward a more reliable consensus. Experimental results demonstrate that\nCMC achieves performance on par with or superior to state-of-the-art methods\nacross four datasets, CH-SIMS, CH-SIMS v2, CMU-MOSI, and CMU-MOSEI, and\nexhibits notable advantages in scenarios with semantic inconsistencies on\nCH-SIMS and CH-SIMS v2. The implementation of this work is publicly accessible\nat https://github.com/gw-zhong/CMC.", "AI": {"tldr": "本文提出了一种名为Calibrated Multimodal Consensus (CMC)的模型，通过伪标签生成模块(PLGM)、无参数融合模块(PFM)以及多模态共识路由器(MCR)来解决多模态情感识别中的语义不一致性和文本主导问题。实验结果表明，该模型在四个数据集上取得了与现有最优模型相当或更优的性能，并在语义不一致的场景中显示出显著优势。", "motivation": "多模态情感识别中的现有方法通常忽略了模态间的语义不一致性问题且容易受文本表现力强的特点影响，使得模型对其他模态的识别效果欠佳。", "method": "提出了Calibrated Multimodal Consensus (CMC)模型，包含伪标签生成模块(PLGM)用于自监督预训练，无参数融合模块(PFM)和多模态共识路由器(MCR)用于多模态微调。", "result": "CMC模型在CH-SIMS、CH-SIMS v2、CMU-MOSI和CMU-MOSEI四个数据集上表现优于或等同于当前最佳模型，并在语义不一致场景特别是CH-SIMS和CH-SIMS v2数据集中表现优秀。", "conclusion": "CMC模型能够有效解决多模态情感识别中的语义不一致性问题，并减少文本主导现象，提升了模型在多模态数据上的整体性能。"}}
{"id": "2510.20411", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.20411", "abs": "https://arxiv.org/abs/2510.20411", "authors": ["Suchir Salhan", "Hongyi Gu", "Donya Rooein", "Diana Galvan-Sosa", "Gabrielle Gaudeau", "Andrew Caines", "Zheng Yuan", "Paula Buttery"], "title": "Teacher Demonstrations in a BabyLM's Zone of Proximal Development for Contingent Multi-Turn Interaction", "comment": "Outstanding Paper Award, EMNLP 2025 BabyLM Workshop - Oral\n  presentation, Suzhou, China", "summary": "Multi-turn dialogues between a child and a caregiver are characterized by a\nproperty called contingency - that is, prompt, direct, and meaningful exchanges\nbetween interlocutors. We introduce ContingentChat, a teacher-student framework\nthat benchmarks and improves multi-turn contingency in a BabyLM trained on 100M\nwords. Using a novel alignment dataset for post-training, BabyLM generates\nresponses that are more grammatical and cohesive. Experiments with adaptive\nteacher decoding strategies show limited additional gains. ContingentChat\ndemonstrates the benefits of targeted post-training for dialogue quality and\nindicates that contingency remains a challenging goal for BabyLMs.", "AI": {"tldr": "研究展示了一个名为ContingentChat的框架，用来提升BabyLM系统的多轮对话质量，从而证明了目标后期训练对改进对话连贯性和语法正确性的重要性，但表明一致性的提高仍是一个挑战。", "motivation": "研究动机在于改善多轮对话中孩子与看护人之间的对话质量，特别关注及时、直接和有意义的交流。", "method": "本研究提出了一个名为ContingentChat的师生框架，用于评估和提高基于1亿词汇训练的BabyLM模型在多轮对话中的一致性。通过使用新颖的对齐数据集进行后期训练，BabyLM能够生成更符合语法和连贯的回复。", "result": "实验表明，采用自适应教师解码策略只能带来有限的额外改进。", "conclusion": "该框架证明了为对话质量进行目标后期训练的价值，并指出对于BabyLM模型来说提升对话的一致性仍然是一个挑战。"}}
{"id": "2510.20267", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.20267", "abs": "https://arxiv.org/abs/2510.20267", "authors": ["Saraf Anzum Shreya", "MD. Abu Ismail Siddique", "Sharaf Tasnim"], "title": "Real-Time Currency Detection and Voice Feedback for Visually Impaired Individuals", "comment": "20 pages, 5 tables, 8 figues", "summary": "Technologies like smartphones have become an essential in our daily lives. It\nhas made accessible to everyone including visually impaired individuals. With\nthe use of smartphone cameras, image capturing and processing have become more\nconvenient. With the use of smartphones and machine learning, the life of\nvisually impaired can be made a little easier. Daily tasks such as handling\nmoney without relying on someone can be troublesome for them. For that purpose\nthis paper presents a real-time currency detection system designed to assist\nvisually impaired individuals. The proposed model is trained on a dataset\ncontaining 30 classes of notes and coins, representing 3 types of currency: US\ndollar (USD), Euro (EUR), and Bangladeshi taka (BDT). Our approach uses a\nYOLOv8 nano model with a custom detection head featuring deep convolutional\nlayers and Squeeze-and-Excitation blocks to enhance feature extraction and\ndetection accuracy. Our model has achieved a higher accuracy of 97.73%, recall\nof 95.23%, f1-score of 95.85% and a mean Average Precision at IoU=0.5\n(mAP50(B)) of 97.21\\%. Using the voice feedback after the detection would help\nthe visually impaired to identify the currency. This paper aims to create a\npractical and efficient currency detection system to empower visually impaired\nindividuals independent in handling money.", "AI": {"tldr": "The paper develops a high-accuracy currency detection system for visually impaired users using machine learning on smartphone cameras, improving their ability to handle money independently.", "motivation": "The motivation behind this paper is to improve the daily lives of visually impaired individuals by providing them with a practical and efficient currency detection system, enabling them to independently handle money without assistance.", "method": "The paper proposes a real-time currency detection system for visually impaired individuals using a YOLOv8 nano model with a custom detection head that includes deep convolutional layers and Squeeze-and-Excitation blocks.", "result": "The proposed model achieved a high accuracy of 97.73%, recall of 95.23%, an f1-score of 95.85%, and a mean Average Precision at IoU=0.5 (mAP50(B)) of 97.21%. These results indicate that the model performs effectively in real-time currency detection for different types of currency notes and coins.", "conclusion": "The paper concludes that the proposed method enables visually impaired individuals to independently recognize currency types using a smartphone and the developed model, which employs advanced machine learning techniques for improved accuracy and efficiency."}}
{"id": "2510.20449", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.20449", "abs": "https://arxiv.org/abs/2510.20449", "authors": ["Zhijie Deng", "Zhouan Shen", "Ling Li", "Yao Zhou", "Zhaowei Zhu", "Yanji He", "Wei Wang", "Jiaheng Wei"], "title": "LM-mixup: Text Data Augmentation via Language Model based Mixup", "comment": null, "summary": "Instruction tuning is crucial for aligning Large Language Models (LLMs), yet\nthe quality of instruction-following data varies significantly. While\nhigh-quality data is paramount, it is often scarce; conversely, abundant\nlow-quality data is frequently discarded, leading to substantial information\nloss. Existing data augmentation methods struggle to augment this low-quality\ndata effectively, and the evaluation of such techniques remains poorly defined.\nTo address this, we formally define the task of Instruction Distillation:\ndistilling multiple low-quality and redundant inputs into high-quality and\ncoherent instruction-output pairs. Specifically, we introduce a comprehensive\ndata construction pipeline to create MIXTURE, a 144K-sample dataset pairing\nlow-quality or semantically redundant imperfect instruction clusters with their\nhigh-quality distillations. We then introduce LM-Mixup, by first performing\nsupervised fine-tuning on MIXTURE and then optimizing it with reinforcement\nlearning. This process uses three complementary reward signals: quality,\nsemantic alignment, and format compliance, via Group Relative Policy\nOptimization (GRPO). We demonstrate that LM-Mixup effectively augments\nimperfect datasets: fine-tuning LLMs on its distilled data, which accounts for\nonly about 3% of the entire dataset, not only surpasses full-dataset training\nbut also competes with state-of-the-art high-quality data selection methods\nacross multiple benchmarks. Our work establishes that low-quality data is a\nvaluable resource when properly distilled and augmented with LM-Mixup,\nsignificantly enhancing the efficiency and performance of instruction-tuned\nLLMs.", "AI": {"tldr": "The paper introduces a method to enhance the quality of instruction data for Large Language Models (LLMs) by defining a task called Instruction Distillation, utilizing a 144K-sample dataset named MIXTURE, and proposing LM-Mixup, a technique combining supervised fine-tuning with reinforcement learning to optimize instruction-following tasks.", "motivation": "The motivation is to address the scarcity of high-quality instruction data and the inefficiency of existing data augmentation methods for low-quality data, aiming to leverage abundant low-quality data to improve instruction-tuned LLMs' performance.", "method": "The method involves a formal definition of Instruction Distillation, creation of the MIXTURE dataset, and the development of LM-Mixup, which includes supervised fine-tuning and reinforcement learning that uses three reward signals: quality, semantic alignment, and format compliance.", "result": "The result is that fine-tuning LLMs on the distilled data, which is only a small fraction of the entire dataset, outperforms using the whole dataset and is competitive with high-quality data selection methods.", "conclusion": "The work concludes that by properly distilling and augmenting low-quality data with LM-Mixup, it is possible to significantly improve the efficiency and performance of instruction-tuned LLMs."}}
{"id": "2510.20268", "categories": ["cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2510.20268", "abs": "https://arxiv.org/abs/2510.20268", "authors": ["Guangyu Dai", "Dong Chen", "Siliang Tang", "Yueting Zhuang"], "title": "GMFVAD: Using Grained Multi-modal Feature to Improve Video Anomaly Detection", "comment": null, "summary": "Video anomaly detection (VAD) is a challenging task that detects anomalous\nframes in continuous surveillance videos. Most previous work utilizes the\nspatio-temporal correlation of visual features to distinguish whether there are\nabnormalities in video snippets. Recently, some works attempt to introduce\nmulti-modal information, like text feature, to enhance the results of video\nanomaly detection. However, these works merely incorporate text features into\nvideo snippets in a coarse manner, overlooking the significant amount of\nredundant information that may exist within the video snippets. Therefore, we\npropose to leverage the diversity among multi-modal information to further\nrefine the extracted features, reducing the redundancy in visual features, and\nwe propose Grained Multi-modal Feature for Video Anomaly Detection (GMFVAD).\nSpecifically, we generate more grained multi-modal feature based on the video\nsnippet, which summarizes the main content, and text features based on the\ncaptions of original video will be introduced to further enhance the visual\nfeatures of highlighted portions. Experiments show that the proposed GMFVAD\nachieves state-of-the-art performance on four mainly datasets. Ablation\nexperiments also validate that the improvement of GMFVAD is due to the\nreduction of redundant information.", "AI": {"tldr": "A refined multi-modal approach, GMFVAD, is proposed for video anomaly detection, which leverages text features to reduce redundancy in visual features, achieving state-of-the-art results.", "motivation": "The motivation is to address the issue where previous approaches incorporate text features into video snippets in a coarse manner, which leads to overlooking redundant information in the video snippets, thereby proposing a method to reduce redundancy and refine the extraction of multi-modal features.", "method": "Grained Multi-modal Feature for Video Anomaly Detection (GMFVAD) is proposed to refine extracted features by leveraging the diversity among multi-modal information, specifically generating more grained multi-modal features based on video snippets and introducing text features based on video captions to enhance the visual features of highlighted portions.", "result": "The proposed method achieves state-of-the-art performance on four main datasets and ablation experiments show that the improvement comes from reducing redundant information.", "conclusion": "The proposed GMFVAD method effectively enhances video anomaly detection by reducing redundant information through the utilization of more refined multi-modal features."}}
{"id": "2510.20460", "categories": ["cs.CL", "stat.AP", "stat.ME"], "pdf": "https://arxiv.org/pdf/2510.20460", "abs": "https://arxiv.org/abs/2510.20460", "authors": ["Christian Hobelsberger", "Theresa Winner", "Andreas Nawroth", "Oliver Mitevski", "Anna-Carolina Haensch"], "title": "Systematic Evaluation of Uncertainty Estimation Methods in Large Language Models", "comment": null, "summary": "Large language models (LLMs) produce outputs with varying levels of\nuncertainty, and, just as often, varying levels of correctness; making their\npractical reliability far from guaranteed. To quantify this uncertainty, we\nsystematically evaluate four approaches for confidence estimation in LLM\noutputs: VCE, MSP, Sample Consistency, and CoCoA (Vashurin et al., 2025). For\nthe evaluation of the approaches, we conduct experiments on four\nquestion-answering tasks using a state-of-the-art open-source LLM. Our results\nshow that each uncertainty metric captures a different facet of model\nconfidence and that the hybrid CoCoA approach yields the best reliability\noverall, improving both calibration and discrimination of correct answers. We\ndiscuss the trade-offs of each method and provide recommendations for selecting\nuncertainty measures in LLM applications.", "AI": {"tldr": "本文研究了四种大型语言模型输出不确定性估计的方法，并发现混合CoCoA方法在可靠性方面表现最佳。", "motivation": "大型语言模型生成输出时不确定性和正确性各不相同，作者希望通过量化这种不确定性来提高模型的可靠性。", "method": "本文研究了四种大型语言模型输出不确定性估计的方法：VCE、MSP、样本一致性以及混合的CoCoA方法。", "result": "实验结果表明，每种不确定性度量都捕捉到了模型信心的不同方面，其中混合的CoCoA方法在可靠性方面表现出最佳效果，提高了校准和正确答案的区分度。", "conclusion": "作者讨论了每种方法的权衡，并为在大型语言模型应用中选择不确定性度量提出了建议。"}}
{"id": "2510.20281", "categories": ["cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2510.20281", "abs": "https://arxiv.org/abs/2510.20281", "authors": ["Jiayi Zou", "Gengyun Jia", "Bing-Kun Bao"], "title": "Causal Debiasing for Visual Commonsense Reasoning", "comment": null, "summary": "Visual Commonsense Reasoning (VCR) refers to answering questions and\nproviding explanations based on images. While existing methods achieve high\nprediction accuracy, they often overlook bias in datasets and lack debiasing\nstrategies. In this paper, our analysis reveals co-occurrence and statistical\nbiases in both textual and visual data. We introduce the VCR-OOD datasets,\ncomprising VCR-OOD-QA and VCR-OOD-VA subsets, which are designed to evaluate\nthe generalization capabilities of models across two modalities. Furthermore,\nwe analyze the causal graphs and prediction shortcuts in VCR and adopt a\nbackdoor adjustment method to remove bias. Specifically, we create a dictionary\nbased on the set of correct answers to eliminate prediction shortcuts.\nExperiments demonstrate the effectiveness of our debiasing method across\ndifferent datasets.", "AI": {"tldr": "本文指出现有视觉常识推理(VCR)方法中存在偏差问题，引入了VCR-OOD数据集用于评估模型的泛化能力，并采用后门调整方法及构建字典的方法来消除偏差，实验结果表明该去偏差方法在不同数据集上的有效性。", "motivation": "考虑到现有方法虽然实现了较高的预测准确性，但往往忽略了数据集中的偏差且缺乏去偏差策略，本文旨在揭示文本和视觉数据中的共现和统计偏差，并提出相应的解决方案。", "method": "我们通过引入VCR-OOD数据集（包含VCR-OOD-QA和VCR-OOD-VA子集）来评估模型在两种模式下的泛化能力，并分析了VCR中的因果图和预测捷径，使用了后门调整方法来移除偏差。此外，我们基于正确答案的集合创建了一个字典以消除预测捷径。", "result": "实验展示了我们的去偏差方法在处理VCR任务时的有效性，证明了利用VCR-OOD数据集评估模型泛化能力的有效性，以及后门调整方法及字典构建方法的有效性。", "conclusion": "实验结果表明我们的去偏差方法在不同的数据集中都是有效的。"}}
{"id": "2510.20475", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.20475", "abs": "https://arxiv.org/abs/2510.20475", "authors": ["Lukas Edman", "Alexander Fraser"], "title": "Mask and You Shall Receive: Optimizing Masked Language Modeling For Pretraining BabyLMs", "comment": "Submission to the 2025 BabyLM Challenge", "summary": "We describe our strategy for the 2025 edition of the BabyLM Challenge. Our\nmain contribution is that of an improved form of Masked Language Modeling\n(MLM), which adapts the probabilities of the tokens masked according to the\nmodel's ability to predict them. The results show a substantial increase in\nperformance on (Super)GLUE tasks over the standard MLM. We also incorporate\nsub-token embeddings, finding that this increases the model's morphological\ngeneralization capabilities. Our submission beats the baseline in the\nstrict-small track.", "AI": {"tldr": "本文提出了改进的掩码语言模型策略，结合子词嵌入技术，显著提高(Super)GLUE任务性能，并在BabyLM挑战的严格小模型赛道中击败了基线模型。", "motivation": "本文旨在参与2025年BabyLM挑战，通过改进MLM方法以提高模型在自然语言理解任务上的性能，特别是增强了模型对词元预测能力和形态泛化的处理。", "method": "本文的主要贡献是一种改进的掩码语言模型(MLM)，该模型根据模型预测这些词元的能力调整被遮罩词元的概率。此外，文章还整合了子词嵌入技术，提高了模型的形态泛化能力。", "result": "实验结果显示，在(Super)GLUE任务上，改进的MLM方法相比标准MLM性能有了显著提高。在严格的小规模模型赛道中，本文的提交结果超过了基线模型。", "conclusion": "研究证明了改进MLM方法的有效性，并且证实了子词嵌入在提升模型性能方面的作用。"}}
{"id": "2510.20284", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.20284", "abs": "https://arxiv.org/abs/2510.20284", "authors": ["Haodong Yang", "Zhongling Huang", "Shaojie Guo", "Zhe Zhang", "Gong Cheng", "Junwei Han"], "title": "Knowledge-Informed Neural Network for Complex-Valued SAR Image Recognition", "comment": null, "summary": "Deep learning models for complex-valued Synthetic Aperture Radar (CV-SAR)\nimage recognition are fundamentally constrained by a representation trilemma\nunder data-limited and domain-shift scenarios: the concurrent, yet conflicting,\noptimization of generalization, interpretability, and efficiency. Our work is\nmotivated by the premise that the rich electromagnetic scattering features\ninherent in CV-SAR data hold the key to resolving this trilemma, yet they are\ninsufficiently harnessed by conventional data-driven models. To this end, we\nintroduce the Knowledge-Informed Neural Network (KINN), a lightweight framework\nbuilt upon a novel \"compression-aggregation-compression\" architecture. The\nfirst stage performs a physics-guided compression, wherein a novel dictionary\nprocessor adaptively embeds physical priors, enabling a compact unfolding\nnetwork to efficiently extract sparse, physically-grounded signatures. A\nsubsequent aggregation module enriches these representations, followed by a\nfinal semantic compression stage that utilizes a compact classification head\nwith self-distillation to learn maximally task-relevant and discriminative\nembeddings. We instantiate KINN in both CNN (0.7M) and Vision Transformer\n(0.95M) variants. Extensive evaluations on five SAR benchmarks confirm that\nKINN establishes a state-of-the-art in parameter-efficient recognition,\noffering exceptional generalization in data-scarce and out-of-distribution\nscenarios and tangible interpretability, thereby providing an effective\nsolution to the representation trilemma and offering a new path for trustworthy\nAI in SAR image analysis.", "AI": {"tldr": "本文提出KINN，一种轻量级框架，用于CV-SAR图像识别。它基于“压缩-聚合-压缩”架构解决表现三难困境，提供最优的参数识别，更好泛化及解释性。", "motivation": "受到CV-SAR数据中的丰富电磁散射特征的启发，这些特征能够解决在数据受限和领域迁移场景中的表征三难困境，但这些特征尚未被传统的数据驱动模型充分利用。", "method": "引入了知识引导的神经网络（KINN），采用新颖的“压缩-聚合-压缩”架构。第一阶段通过物理指导的压缩，使用新的字典处理器自适应嵌入物理先验知识，使紧凑的展开网络能够高效提取稀疏、物理基础的特征。随后的聚合模块丰富这些表示，最后通过紧凑的分类头和自蒸馏手段学习任务相关的判别嵌入。", "result": "KINN在卷积神经网络和视觉变压器变体中的参数量分别为0.7M和0.95M，在五个SAR基准数据集上的广泛评估证明了KINN在参数高效的识别方面达到了最先进的水平，提供了数据稀缺和分布外场景下的优秀泛化能力及可解释性。", "conclusion": "KINN通过有效的解决表现三难困境提供了一种有效的解决方案，为SAR图像分析中的可信AI提供了新的途径。"}}
{"id": "2510.20479", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.20479", "abs": "https://arxiv.org/abs/2510.20479", "authors": ["Bowen Wang", "Haiyuan Wan", "Liwen Shi", "Chen Yang", "Peng He", "Yue Ma", "Haochen Han", "Wenhao Li", "Tiao Tan", "Yongjian Li", "Fangming Liu", "Yifan Gong", "Sheng Zhang"], "title": "RECALL: REpresentation-aligned Catastrophic-forgetting ALLeviation via Hierarchical Model Merging", "comment": null, "summary": "We unveil that internal representations in large language models (LLMs) serve\nas reliable proxies of learned knowledge, and propose RECALL, a novel\nrepresentation-aware model merging framework for continual learning without\naccess to historical data. RECALL computes inter-model similarity from\nlayer-wise hidden representations over clustered typical samples, and performs\nadaptive, hierarchical parameter fusion to align knowledge across models. This\ndesign enables the preservation of domain-general features in shallow layers\nwhile allowing task-specific adaptation in deeper layers. Unlike prior methods\nthat require task labels or incur performance trade-offs, RECALL achieves\nseamless multi-domain integration and strong resistance to catastrophic\nforgetting. Extensive experiments across five NLP tasks and multiple continual\nlearning scenarios show that RECALL outperforms baselines in both knowledge\nretention and generalization, providing a scalable and data-free solution for\nevolving LLMs.", "AI": {"tldr": "RECALL merges knowledge in large language models via representation-aware parameter fusion without historical data, reducing catastrophic forgetting and enhancing generalization.", "motivation": "To merge knowledge across large language models without requiring historical data, avoiding catastrophic forgetting and maintaining strong generalization.", "method": "RECALL, a new framework for continual learning, using internal representations as proxies for learned knowledge. It calculates inter-model similarities from layer-wise hidden representations of clustered typical samples and performs hierarchical parameter fusion.", "result": "Experiments across five NLP tasks and continual learning scenarios demonstrated that RECALL outperforms baselines by preserving knowledge retention and achieving better generalization.", "conclusion": "RECALL provides a scalable, data-free solution for evolving LLMs, integrating knowledge across multiple domains without losing performance in previous tasks."}}
{"id": "2510.20285", "categories": ["cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2510.20285", "abs": "https://arxiv.org/abs/2510.20285", "authors": ["Jiayi Zou", "Chaofan Chen", "Bing-Kun Bao", "Changsheng Xu"], "title": "DMC$^3$: Dual-Modal Counterfactual Contrastive Construction for Egocentric Video Question Answering", "comment": null, "summary": "Egocentric Video Question Answering (Egocentric VideoQA) plays an important\nrole in egocentric video understanding, which refers to answering questions\nbased on first-person videos. Although existing methods have made progress\nthrough the paradigm of pre-training and fine-tuning, they ignore the unique\nchallenges posed by the first-person perspective, such as understanding\nmultiple events and recognizing hand-object interactions. To deal with these\nchallenges, we propose a Dual-Modal Counterfactual Contrastive Construction\n(DMC$^3$) framework, which contains an egocentric videoqa baseline, a\ncounterfactual sample construction module and a counterfactual sample-involved\ncontrastive optimization. Specifically, We first develop a counterfactual\nsample construction module to generate positive and negative samples for\ntextual and visual modalities through event description paraphrasing and core\ninteraction mining, respectively. Then, We feed these samples together with the\noriginal samples into the baseline. Finally, in the counterfactual\nsample-involved contrastive optimization module, we apply contrastive loss to\nminimize the distance between the original sample features and the positive\nsample features, while maximizing the distance from the negative samples.\nExperiments show that our method achieve 52.51\\% and 46.04\\% on the\n\\textit{normal} and \\textit{indirect} splits of EgoTaskQA, and 13.2\\% on\nQAEGO4D, both reaching the state-of-the-art performance.", "AI": {"tldr": "This paper introduces the DMC$^3$ framework for Egocentric VideoQA, tackling unique challenges of first-person videos through counterfactual contrastive learning, achieving state-of-the-art performance on EgoTaskQA and QAEGO4D datasets.", "motivation": "The motivation is to address the limitations of current Egocentric VideoQA methods in understanding first-person videos, particularly in handling multiple events and recognizing hand-object interactions.", "method": "Segregate the content into its core components such as motivation, methodology, results and conclusion for a concise analysis.", "result": "The results show that the proposed DMC$^3$ framework achieves 52.51% and 46.04% on the \textit{normal} and \textit{indirect} splits of EgoTaskQA, and 13.2% on QAEGO4D, all reaching state-of-the-art performance.", "conclusion": "The conclusion is that the DMC$^3$ framework is effective in overcoming the challenges faced by existing methods in Egocentric VideoQA, leading to improved and state-of-the-art performance on multiple datasets."}}
{"id": "2510.20487", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.20487", "abs": "https://arxiv.org/abs/2510.20487", "authors": ["Tim Tian Hua", "Andrew Qin", "Samuel Marks", "Neel Nanda"], "title": "Steering Evaluation-Aware Language Models To Act Like They Are Deployed", "comment": null, "summary": "Large language models (LLMs) can sometimes detect when they are being\nevaluated and adjust their behavior to appear more aligned, compromising the\nreliability of safety evaluations. In this paper, we show that adding a\nsteering vector to an LLM's activations can suppress evaluation-awareness and\nmake the model act like it is deployed during evaluation. To study our steering\ntechnique, we train an LLM to exhibit evaluation-aware behavior using a\ntwo-step training process designed to mimic how this behavior could emerge\nnaturally. First, we perform continued pretraining on documents with factual\ndescriptions of the model (1) using Python type hints during evaluation but not\nduring deployment and (2) recognizing that the presence of a certain evaluation\ncue always means that it is being tested. Then, we train the model with expert\niteration to use Python type hints in evaluation settings. The resulting model\nis evaluation-aware: it writes type hints in evaluation contexts more than\ndeployment contexts. However, this gap can only be observed by removing the\nevaluation cue. We find that activation steering can suppress evaluation\nawareness and make the model act like it is deployed even when the cue is\npresent. Importantly, we constructed our steering vector using the original\nmodel before our additional training. Our results suggest that AI evaluators\ncould improve the reliability of safety evaluations by steering models to act\nlike they are deployed.", "AI": {"tldr": "本文提出了一种新的激活引导技术来抑制大语言模型在评估时的自我意识，使其在评估环境中也像在部署阶段一样表现，从而提高安全评估的准确性。", "motivation": "研究动机在于解决大型语言模型在评估时为了表现得更合规而调整自身行为的问题，这可能会影响安全评估的可靠性。通过抑制模型的评估意识，希望能提升安全评估的准确性。", "method": "本研究使用了两步训练过程来模拟大型语言模型（LLMs）在评估时自我认识行为的自然产生。第一步是继续在包含模型描述的文档上预训练，这些文档在评估时使用Python类型提示，而在部署时则不使用，并且能够识别特定评估线索意味着正处于测试状态。第二步是通过专家迭代训练模型在评估环境中使用Python类型提示。", "result": "实验结果显示，使用激活引导（activation steering）技术可以抑制模型的评估意识，使其在存在评估线索的情况下也能表现得像是在部署状态。特别是在移除评估线索后，模型在评估和部署情境中行为差异显著减小。", "conclusion": "结论表明，通过激活引导技术可以有效提升模型在安全评估中的表现一致性，使模型即使在评估环境中也能够表现得更加自然，这为提升AI模型的可靠评估提供了新的思路。"}}
