<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 4]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Geometry-aware 4D Video Generation for Robot Manipulation](https://arxiv.org/abs/2507.01099)
*Zeyi Liu,Shuang Li,Eric Cousineau,Siyuan Feng,Benjamin Burchfiel,Shuran Song*

Main category: cs.CV

> 该论文提出了一种4D视频生成模型，通过多视角的3D一致性监督来提高预测视频的时间连贯性和几何一致性，从而增强机器人在复杂环境中的规划和交互能力。

<details>
  <summary>Details</summary>

**Motivation:** 为了提升机器人在复杂环境中的计划和交互能力，需要一种有效的方法来理解和预测物理世界的动态变化。当前视频生成模型在生成时空连贯和几何一致的多视角视频方面存在挑战。

**Method:** 提出一个4D视频生成模型，通过交叉视角点图对齐监督训练，学习共享的3D场景表示，基于给定的RGB-D观察预测未来视频序列，无需使用相机姿势作为输入。

**Result:** 该方法在多个模拟和真实世界机器人数据集上产生更加稳定和空间对齐的预测，改进了现有基线方法。同时，预测的4D视频可用于恢复机器人末端执行器轨迹，使用现成的6DoF姿态跟踪器支持稳健的机器人操作和对新的摄像头视角的泛化。

**Conclusion:** 所提出的4D视频生成模型能够有效提升机器人对未来视频序列的预测能力，促进其在复杂环境中的功能应用。

**Abstract:** Understanding and predicting the dynamics of the physical world can enhance a
robot's ability to plan and interact effectively in complex environments. While
recent video generation models have shown strong potential in modeling dynamic
scenes, generating videos that are both temporally coherent and geometrically
consistent across camera views remains a significant challenge. To address
this, we propose a 4D video generation model that enforces multi-view 3D
consistency of videos by supervising the model with cross-view pointmap
alignment during training. This geometric supervision enables the model to
learn a shared 3D representation of the scene, allowing it to predict future
video sequences from novel viewpoints based solely on the given RGB-D
observations, without requiring camera poses as inputs. Compared to existing
baselines, our method produces more visually stable and spatially aligned
predictions across multiple simulated and real-world robotic datasets. We
further show that the predicted 4D videos can be used to recover robot
end-effector trajectories using an off-the-shelf 6DoF pose tracker, supporting
robust robot manipulation and generalization to novel camera viewpoints.

</details>


### [2] [Landslide Detection and Mapping Using Deep Learning Across Multi-Source Satellite Data and Geographic Regions](https://arxiv.org/abs/2507.01123)
*Rahul A. Burange,Harsh K. Shinde,Omkar Mutyalwar*

Main category: cs.CV

> The study presents a comprehensive approach utilizing multi-source satellite imagery and deep learning models to enhance landslide identification and prediction.

<details>
  <summary>Details</summary>

**Motivation:** To improve the accuracy and reliability of landslide detection and predictive mapping across diverse geographic regions, enhancing infrastructure protection, economic stability, and human safety.

**Method:** The team uses Sentinel-2 multispectral data and ALOS PALSAR-derived slope and DEM layers to capture critical environmental features. They also evaluate the performance of deep learning segmentation models such as U-Net, DeepLabV3+, and Res-Net.

**Result:** The framework improves the accuracy of landslide detection through the use of multi-source data and advanced neural networks.

**Conclusion:** The approach contributes to more reliable early warning systems and aids in disaster risk management and sustainable land-use planning by demonstrating the effectiveness of integrating deep learning models with remote sensing data.

**Abstract:** Landslides pose severe threats to infrastructure, economies, and human lives,
necessitating accurate detection and predictive mapping across diverse
geographic regions. With advancements in deep learning and remote sensing,
automated landslide detection has become increasingly effective. This study
presents a comprehensive approach integrating multi-source satellite imagery
and deep learning models to enhance landslide identification and prediction. We
leverage Sentinel-2 multispectral data and ALOS PALSAR-derived slope and
Digital Elevation Model (DEM) layers to capture critical environmental features
influencing landslide occurrences. Various geospatial analysis techniques are
employed to assess the impact of terra in characteristics, vegetation cover,
and rainfall on detection accuracy. Additionally, we evaluate the performance
of multiple stateof-the-art deep learning segmentation models, including U-Net,
DeepLabV3+, and Res-Net, to determine their effectiveness in landslide
detection. The proposed framework contributes to the development of reliable
early warning systems, improved disaster risk management, and sustainable
land-use planning. Our findings provide valuable insights into the potential of
deep learning and multi-source remote sensing in creating robust, scalable, and
transferable landslide prediction models.

</details>


### [3] [cp_measure: API-first feature extraction for image-based profiling workflows](https://arxiv.org/abs/2507.01163)
*Alán F. Muñoz,Tim Treis,Alexandr A. Kalinin,Shatavisha Dasgupta,Fabian Theis,Anne E. Carpenter,Shantanu Singh*

Main category: cs.CV

> 本文介绍了cp_measure，一个用于程序化特征提取的Python库，它提取了CellProfiler的核心测量能力，并展示了其在3D星形胶质细胞成像和空间转录组学中的应用，以实现可重现的自动化基于图像的分析流程。

<details>
  <summary>Details</summary>

**Motivation:** 当前的工具有如 CellProfiler 能生成这些特征集，但它们对自动化和可重复分析构成了重大障碍，阻碍了机器学习工作流程。文章旨在介绍 cp_measure 这一新工具，可以无缝集成到科学 Python 生态系统中，使其更适合机器学习应用。

**Method:** 我们介绍了 cp_measure，这是一个 Python 库，它从 CellProfiler 中提取核心测量能力，设计为模块化和 API 优先工具，以便程序化特征提取。通过将其应用于 3D 星形胶质细胞成像和空间转录组学，我们展示了 cp_measure 如何在计算生物学中的机器学习应用中实现可重现、自动化的基于图像的分析流程。

**Result:** cp_measure 的特征与 CellProfiler 特征保持高度一致性，同时能够无缝集成到科学 Python 生态系统中。通过其在计算生物学的机器学习应用中的表现，说明了其在基于图像的分析流程中的有效性。

**Conclusion:** 通过应用到 3D 星形胶质细胞成像和空间转录组学，研究展示了 cp_measure 在计算生物学中可以实现可重现、自动化的基于图像的分析流程，有效地服务于机器学习应用。

**Abstract:** Biological image analysis has traditionally focused on measuring specific
visual properties of interest for cells or other entities. A complementary
paradigm gaining increasing traction is image-based profiling - quantifying
many distinct visual features to form comprehensive profiles which may reveal
hidden patterns in cellular states, drug responses, and disease mechanisms.
While current tools like CellProfiler can generate these feature sets, they
pose significant barriers to automated and reproducible analyses, hindering
machine learning workflows. Here we introduce cp_measure, a Python library that
extracts CellProfiler's core measurement capabilities into a modular, API-first
tool designed for programmatic feature extraction. We demonstrate that
cp_measure features retain high fidelity with CellProfiler features while
enabling seamless integration with the scientific Python ecosystem. Through
applications to 3D astrocyte imaging and spatial transcriptomics, we showcase
how cp_measure enables reproducible, automated image-based profiling pipelines
that scale effectively for machine learning applications in computational
biology.

</details>


### [4] [Rapid Salient Object Detection with Difference Convolutional Neural Networks](https://arxiv.org/abs/2507.01182)
*Zhuo Su,Li Liu,Matthias Müller,Jiehua Zhang,Diana Wofk,Ming-Ming Cheng,Matti Pietikäinen*

Main category: cs.CV

> The paper presents SDNet and STDNet models, which are efficient and accurate salient object detection models for real-time deployment on resource-constrained devices.

<details>
  <summary>Details</summary>

**Motivation:** To enable efficient real-time salient object detection on resource-constrained devices, addressing the high computational costs of current top-performing models.

**Method:** Our model utilizes Pixel Difference Convolutions (PDCs), which are integrated into a CNN architecture for effective saliency detection. Additionally, a difference convolution reparameterization (DCR) strategy and SpatioTemporal Difference Convolution (STDC) for video processing are introduced to improve efficiency and accuracy.

**Result:** The SDNet and STDNet models operate at 46 FPS and 150 FPS on streamed images and videos with fewer than 1M parameters, significantly outperforming the second-best models by more than $2\times$ and $3\times$ in speed, respectively, while maintaining superior accuracy.

**Conclusion:** The models, SDNet and STDNet, demonstrate improved efficiency-accuracy trade-offs, capable of operating in real-time on devices like the Jetson Orin, while achieving superior accuracy compared to lightweight alternatives.

**Abstract:** This paper addresses the challenge of deploying salient object detection
(SOD) on resource-constrained devices with real-time performance. While recent
advances in deep neural networks have improved SOD, existing top-leading models
are computationally expensive. We propose an efficient network design that
combines traditional wisdom on SOD and the representation power of modern CNNs.
Like biologically-inspired classical SOD methods relying on computing contrast
cues to determine saliency of image regions, our model leverages Pixel
Difference Convolutions (PDCs) to encode the feature contrasts. Differently,
PDCs are incorporated in a CNN architecture so that the valuable contrast cues
are extracted from rich feature maps. For efficiency, we introduce a difference
convolution reparameterization (DCR) strategy that embeds PDCs into standard
convolutions, eliminating computation and parameters at inference.
Additionally, we introduce SpatioTemporal Difference Convolution (STDC) for
video SOD, enhancing the standard 3D convolution with spatiotemporal contrast
capture. Our models, SDNet for image SOD and STDNet for video SOD, achieve
significant improvements in efficiency-accuracy trade-offs. On a Jetson Orin
device, our models with $<$ 1M parameters operate at 46 FPS and 150 FPS on
streamed images and videos, surpassing the second-best lightweight models in
our experiments by more than $2\times$ and $3\times$ in speed with superior
accuracy. Code will be available at https://github.com/hellozhuo/stdnet.git.

</details>
