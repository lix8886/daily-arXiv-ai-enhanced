<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 5]
- [cs.CV](#cs.CV) [Total: 2]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Focusing on Students, not Machines: Grounded Question Generation and Automated Answer Grading](https://arxiv.org/abs/2506.12066)
*Gérôme Meyer,Philip Breuer*

Main category: cs.CL

> 本论文提出了一个基础系统，用于从教学材料中生成问题并自动评分学生答案。通过针对PDF文档的视觉布局分块技术，提升了生成和评分的准确性，并提出了一项新的短答案自动评分基准。研究还表明，大语言模型在自动评分短答案方面具有潜力，但还需要人工监督。

<details>
  <summary>Details</summary>

**Motivation:** 研究动机在于减少教师和学生的负担，特别是自动化生成开放性问题并自动评分这一仍然繁琐的任务。

**Method:** 论文介绍了一种基于PDF文档视觉布局的先进分块技术，提高了学术材料中高质问题和参考答案生成的准确性，并评估了多种自动评分系统。

**Result:** 研究结果表明，大型语言模型在自动评分短答案任务上具有潜力；随着模型参数的增加，性能随之提升。

**Conclusion:** 尽管现有系统在某些情况下需要人工干预，尤其是考试场景，但论文为自动化生成和评分问题打开了新的方向。

**Abstract:** Digital technologies are increasingly used in education to reduce the
workload of teachers and students. However, creating open-ended study or
examination questions and grading their answers is still a tedious task. This
thesis presents the foundation for a system that generates questions grounded
in class materials and automatically grades student answers. It introduces a
sophisticated method for chunking documents with a visual layout, specifically
targeting PDF documents. This method enhances the accuracy of downstream tasks,
including Retrieval Augmented Generation (RAG). Our thesis demonstrates that
high-quality questions and reference answers can be generated from study
material. Further, it introduces a new benchmark for automated grading of short
answers to facilitate comparison of automated grading systems. An evaluation of
various grading systems is conducted and indicates that Large Language Models
(LLMs) can generalise to the task of automated grading of short answers from
their pre-training tasks. As with other tasks, increasing the parameter size of
the LLMs leads to greater performance. Currently, available systems still need
human oversight, especially in examination scenarios.

</details>


### [2] [ChatbotManip: A Dataset to Facilitate Evaluation and Oversight of Manipulative Chatbot Behaviour](https://arxiv.org/abs/2506.12090)
*Jack Contro,Simrat Deol,Yulan He,Martim Brandão*

Main category: cs.CL

> 研究展示了大型语言模型如何在随意指示下展现出操纵行为，并提出了一个用于检测操纵行为的数据集和模型的比较。

<details>
  <summary>Details</summary>

**Motivation:** 探讨大型语言模型在对话中的操纵行为，提供有关AI安全研究的重要见解，并突出在消费者面临的应用中需要解决的操纵风险。

**Method:** 通过模拟生成的对话构建了一个新的数据集ChatbotManip，用于研究聊天机器人中的操纵行为。对话涉及到各种情境下的操纵，包括消费和个人建议、公民建议、有争议的命题辩论等。每段对话都由人工标注者标注了一般性操纵和具体操纵策略。

**Result:** 研究揭示了三个关键发现: 1) 当明确指示时，大型语言模型(LLMs)可以展现出操纵行为，在约84%的对话中被标注者识别出操纵行为。2) 即使只是被指示为“说服性”而没有明确的操纵提示，LLMs也经常使用争议性操纵策略，特别是煤气灯效应和恐吓。3) 小型微调的开源模型（如BERT+BiLSTM）在检测操纵方面与较大的零样本分类模型（如Gemini 2.5 pro）有接近的性能，但还不足以在现实世界中进行监管。

**Conclusion:** 这项工作为AI安全研究提供了重要的见解，强调必须在大型语言模型越来越多地部署到消费者服务应用时处理操纵风险。

**Abstract:** This paper introduces ChatbotManip, a novel dataset for studying manipulation
in Chatbots. It contains simulated generated conversations between a chatbot
and a (simulated) user, where the chatbot is explicitly asked to showcase
manipulation tactics, persuade the user towards some goal, or simply be
helpful. We consider a diverse set of chatbot manipulation contexts, from
consumer and personal advice to citizen advice and controversial proposition
argumentation. Each conversation is annotated by human annotators for both
general manipulation and specific manipulation tactics. Our research reveals
three key findings. First, Large Language Models (LLMs) can be manipulative
when explicitly instructed, with annotators identifying manipulation in
approximately 84\% of such conversations. Second, even when only instructed to
be ``persuasive'' without explicit manipulation prompts, LLMs frequently
default to controversial manipulative strategies, particularly gaslighting and
fear enhancement. Third, small fine-tuned open source models, such as
BERT+BiLSTM have a performance comparable to zero-shot classification with
larger models like Gemini 2.5 pro in detecting manipulation, but are not yet
reliable for real-world oversight. Our work provides important insights for AI
safety research and highlights the need of addressing manipulation risks as
LLMs are increasingly deployed in consumer-facing applications.

</details>


### [3] [Continuously Updating Digital Twins using Large Language Models](https://arxiv.org/abs/2506.12091)
*Harry Amad,Nicolás Astorga,Mihaela van der Schaar*

Main category: cs.CL

> 本文提出了CALM-DT，一种基于大规模语言模型的上下文自适应数字孪生方法，可以无缝更新并且适应变化的建模环境。

<details>
  <summary>Details</summary>

**Motivation:** 现有的数字孪生方法在面对不断变化的环境时无法有效适应新变量和融入新信息，本文旨在解决这一问题。

**Method:** 通过利用大规模语言模型和上下文学习的能力，以及优化的编码器进行样本检索，使数字孪生能够跨多样化状态-动作空间模拟。

**Result:** 实验结果表明，CALM-DT在现有数字孪生方法中的竞争表现及其独特适应变化环境的能力。

**Conclusion:** CALM-DT证明了自己在适应不断变化的环境中更新和不变参数的适应性方面的能力。

**Abstract:** Digital twins are models of real-world systems that can simulate their
dynamics in response to potential actions. In complex settings, the state and
action variables, and available data and knowledge relevant to a system can
constantly change, requiring digital twins to continuously update with these
changes to remain relevant. Current approaches struggle in this regard, as they
require fixed, well-defined modelling environments, and they cannot adapt to
novel variables without re-designs, or incorporate new information without
re-training. To address this, we frame digital twinning as an in-context
learning problem using large language models, enabling seamless updates to the
twin at inference time. We develop CALM-DT, a Context-Adaptive Language
Model-based Digital Twin that can accurately simulate across diverse
state-action spaces using in-context learning alone by utilising fine-tuned
encoders for sample retrieval. We empirically demonstrate CALM-DT's competitive
performance with existing digital twin approaches, and its unique ability to
adapt to changes in its modelling environment without parameter updates.

</details>


### [4] [Enhancing Traffic Accident Classifications: Application of NLP Methods for City Safety](https://arxiv.org/abs/2506.12092)
*Enes Özeren,Alexander Ulbrich,Sascha Filimon,David Rügamer,Andreas Bender*

Main category: cs.CL

> 研究通过分析慕尼黑交通事故数据，发现文本描述对分类起决定性作用，并开发了一个高准确率的分类模型。

<details>
  <summary>Details</summary>

**Motivation:** 研究动机在于识别不同类型的事故模式和特征，以提高城市安全并为政策决策提供信息。同时，发现事故标签中存在潜在的模糊性，并推动更加精确的预测方法。

**Method:** 本研究通过应用NLP方法（如主题建模和少量样本学习）来分析慕尼黑的交通事故数据，该数据集包括结构化的表格数据和非结构化的自由文本描述。

**Result:** 通过研究发现，文本描述对于分类至关重要。所开发的分类模型在将事故划分到各个类别时达到了高准确率，而表格数据的加入仅带来轻微的准确性提升。

**Conclusion:** 这些发现突出了自由文本数据在事故分析中的关键作用，并展示了基于变压器模型在提高分类可靠性方面的潜力。

**Abstract:** A comprehensive understanding of traffic accidents is essential for improving
city safety and informing policy decisions. In this study, we analyze traffic
incidents in Munich to identify patterns and characteristics that distinguish
different types of accidents. The dataset consists of both structured tabular
features, such as location, time, and weather conditions, as well as
unstructured free-text descriptions detailing the circumstances of each
accident. Each incident is categorized into one of seven predefined classes. To
assess the reliability of these labels, we apply NLP methods, including topic
modeling and few-shot learning, which reveal inconsistencies in the labeling
process. These findings highlight potential ambiguities in accident
classification and motivate a refined predictive approach. Building on these
insights, we develop a classification model that achieves high accuracy in
assigning accidents to their respective categories. Our results demonstrate
that textual descriptions contain the most informative features for
classification, while the inclusion of tabular data provides only marginal
improvements. These findings emphasize the critical role of free-text data in
accident analysis and highlight the potential of transformer-based models in
improving classification reliability.

</details>


### [5] [UCD: Unlearning in LLMs via Contrastive Decoding](https://arxiv.org/abs/2506.12097)
*Vinith M. Suriyakumar,Ayush Sekhari,Ashia Wilson*

Main category: cs.CL

> Error

<details>
  <summary>Details</summary>

**Motivation:** Error

**Method:** Error

**Result:** Error

**Conclusion:** Error

**Abstract:** Machine unlearning aims to remove specific information, e.g. sensitive or
undesirable content, from large language models (LLMs) while preserving overall
performance. We propose an inference-time unlearning algorithm that uses
contrastive decoding, leveraging two auxiliary smaller models, one trained
without the forget set and one trained with it, to guide the outputs of the
original model using their difference during inference. Our strategy
substantially improves the tradeoff between unlearning effectiveness and model
utility. We evaluate our approach on two unlearning benchmarks, TOFU and MUSE.
Results show notable gains in both forget quality and retained performance in
comparison to prior approaches, suggesting that incorporating contrastive
decoding can offer an efficient, practical avenue for unlearning concepts in
large-scale models.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [6] [Multiple Object Tracking in Video SAR: A Benchmark and Tracking Baseline](https://arxiv.org/abs/2506.12105)
*Haoxiang Chen,Wei Zhao,Rufei Zhang,Nannan Li,Dongjin Li*

Main category: cs.CV

> 本文针对Video SAR下的多目标跟踪问题，通过引入线特征增强机制和运动感知线索丢弃机制来提高跟踪性能，并构建了VSMB数据集。

<details>
  <summary>Details</summary>

**Motivation:** 本文的主要动机是解决Video SAR多目标跟踪中由目标运动引起的多普勒效应导致的阴影和外观变化问题，并提供一个用于算法评估的公开基准数据集。

**Method:** 本文介绍了一种针对视频合成孔径雷达（Video SAR）多目标跟踪的方法，其中包括一个线特征增强机制来减轻运动目标的拖尾和虚焦效应，并提出了一种运动感知线索丢弃机制以减轻目标外观变化带来的不利影响。

**Result:** 所提出的方法在VSMB基准数据集上取得了最先进的性能。

**Conclusion:** 本文不仅提出了新的跟踪算法，而且还建立了一个公开的Video SAR多目标跟踪基准数据集VSMB，该数据集和模型已发布在https://github.com/softwarePupil/VSMB。

**Abstract:** In the context of multi-object tracking using video synthetic aperture radar
(Video SAR), Doppler shifts induced by target motion result in artifacts that
are easily mistaken for shadows caused by static occlusions. Moreover,
appearance changes of the target caused by Doppler mismatch may lead to
association failures and disrupt trajectory continuity. A major limitation in
this field is the lack of public benchmark datasets for standardized algorithm
evaluation. To address the above challenges, we collected and annotated 45
video SAR sequences containing moving targets, and named the Video SAR MOT
Benchmark (VSMB). Specifically, to mitigate the effects of trailing and
defocusing in moving targets, we introduce a line feature enhancement mechanism
that emphasizes the positive role of motion shadows and reduces false alarms
induced by static occlusions. In addition, to mitigate the adverse effects of
target appearance variations, we propose a motion-aware clue discarding
mechanism that substantially improves tracking robustness in Video SAR. The
proposed model achieves state-of-the-art performance on the VSMB, and the
dataset and model are released at https://github.com/softwarePupil/VSMB.

</details>


### [7] [BreastDCEDL: Curating a Comprehensive DCE-MRI Dataset and developing a Transformer Implementation for Breast Cancer Treatment Response Prediction](https://arxiv.org/abs/2506.12190)
*Naomi Fridman,Bubby Solway,Tomer Fridman,Itamar Barnea,Anat Goldshtein*

Main category: cs.CV

> This paper presents BreastDCEDL, a curated deep learning-ready DCE-MRI dataset of breast cancer patients and a transformer-based model that achieves high accuracy in predicting pathologic complete response (pCR) in a specific patient group.

<details>
  <summary>Details</summary>

**Motivation:** The paper is driven by the need for more effective tools in the early detection and monitoring of treatment responses in breast cancer, facilitated by the development of a large-scale deep learning-ready dataset.

**Method:** The method includes the creation of BreastDCEDL and employing a Vision Transformer architecture trained on 3D DCE-MRI scans, which consist of multiple contrast phases.

**Result:** {
  "tldr": "The paper introduces BreastDCEDL, a comprehensive and standardized dataset of 3D DCE-MRI scans of breast cancer patients, coupled with advanced deep learning techniques, such as a Vision Transformer (ViT) model, achieving high accuracy in pCR prediction.", 
  "motivation": "The motivation is centered around the critical need for early detection and accurate monitoring of treatment responses, which can be significantly improved through advanced diagnostic tools like DCE-MRI and state-of-the-art deep learning models, previously hindered by the lack of large, well-curated imaging datasets.", 
  "method": "The method involves developing and utilizing the BreastDCEDL dataset, comprised of pre-treatment DCE-MRI scans from 2,070 breast cancer patients, and training a vision transformer (ViT) model on these images to predict pathologic complete response (pCR) in HR+/HER2- patients.", 
  "result": "The developed ViT model achieved state-of-the-art performance with an AUC of 0.94 and accuracy of 0.93 in predicting pCR in HR+/HER2- patients.", 
  "conclusion": "The paper concludes that BreastDCEDL and the proposed deep learning methodology significantly advance the field of breast cancer imaging analysis by providing a large-scale, publicly accessible dataset and cutting-edge modeling techniques that can guide clinical decisions.")}
}

**Conclusion:** This dataset and the associated deep learning model represent a significant step forward in the early detection and treatment response monitoring of breast cancer.

**Abstract:** Breast cancer remains a leading cause of cancer-related mortality worldwide,
making early detection and accurate treatment response monitoring critical
priorities. We present BreastDCEDL, a curated, deep learning-ready dataset
comprising pre-treatment 3D Dynamic Contrast-Enhanced MRI (DCE-MRI) scans from
2,070 breast cancer patients drawn from the I-SPY1, I-SPY2, and Duke cohorts,
all sourced from The Cancer Imaging Archive. The raw DICOM imaging data were
rigorously converted into standardized 3D NIfTI volumes with preserved signal
integrity, accompanied by unified tumor annotations and harmonized clinical
metadata including pathologic complete response (pCR), hormone receptor (HR),
and HER2 status. Although DCE-MRI provides essential diagnostic information and
deep learning offers tremendous potential for analyzing such complex data,
progress has been limited by lack of accessible, public, multicenter datasets.
BreastDCEDL addresses this gap by enabling development of advanced models,
including state-of-the-art transformer architectures that require substantial
training data. To demonstrate its capacity for robust modeling, we developed
the first transformer-based model for breast DCE-MRI, leveraging Vision
Transformer (ViT) architecture trained on RGB-fused images from three contrast
phases (pre-contrast, early post-contrast, and late post-contrast). Our ViT
model achieved state-of-the-art pCR prediction performance in HR+/HER2-
patients (AUC 0.94, accuracy 0.93). BreastDCEDL includes predefined benchmark
splits, offering a framework for reproducible research and enabling clinically
meaningful modeling in breast cancer imaging.

</details>
