<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 12]
- [cs.CV](#cs.CV) [Total: 16]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Modeling Open-World Cognition as On-Demand Synthesis of Probabilistic Models](https://arxiv.org/abs/2507.12547)
*Lionel Wong,Katherine M. Collins,Lance Ying,Cedegao E. Zhang,Adrian Weller,Tobias Gersternberg,Timothy O'Donnell,Alexander K. Lew,Jacob D. Andreas,Joshua B. Tenenbaum,Tyler Brooke-Wilson*

Main category: cs.CL

> 研究提出一种结合分布式和符号表示方法，构建专有心理模型以应对新情景的“模型合成架构”，并通过特定实验验证了其优于单纯语言模型的表现。

<details>
  <summary>Details</summary>

**Motivation:** 研究者们旨在探索人们如何在面对新情况时，调动广泛背景知识中的相关信息进行连贯推理的核心机制。

**Method:** 提出了一种名为“模型合成架构”(MSA)的计算实现方式，使用语言模型来实现全局相关性检索和模型合成，使用概率程序来实现特制的、连贯的世界模型。

**Result:** 研究结果表明，MSA在处理一种新颖推理数据集（以体育比喻构成的故事段落）时，能更准确地预测人类的决策，比仅依赖语言模型的方法表现更好。

**Conclusion:** 研究结果表明，MSA能够模拟人们在全球相关变量上的局部连贯推理能力，为理解和复制人在开放领域中的推理提供了一条途径。

**Abstract:** When faced with novel situations, people are able to marshal relevant
considerations from a wide range of background knowledge and put these to use
in inferences and predictions. What permits us to draw in globally relevant
information and reason over it coherently? Here, we explore the hypothesis that
people use a combination of distributed and symbolic representations to
construct bespoke mental models tailored to novel situations. We propose a
computational implementation of this idea -- a ``Model Synthesis Architecture''
(MSA) -- using language models to implement global relevance-based retrieval
and model synthesis and probabilistic programs to implement bespoke, coherent
world models. We evaluate our MSA as a model of human judgments on a novel
reasoning dataset. The dataset -- built around a `Model Olympics` domain of
sports vignettes -- tests models' capacity for human-like, open-ended reasoning
by requiring (i) judgments about novel causal structures described in language;
(ii) drawing on large bodies of background knowledge; and (iii) doing both in
light of observations that introduce arbitrary novel variables. Our MSA
approach captures human judgments better than language model-only baselines,
under both direct and chain-of-thought generations from the LM that supports
model synthesis. These results suggest that MSAs can be implemented in a way
that mirrors people's ability to deliver locally coherent reasoning over
globally relevant variables, offering a path to understanding and replicating
human reasoning in open-ended domains.

</details>


### [2] [Is This Just Fantasy? Language Model Representations Reflect Human Judgments of Event Plausibility](https://arxiv.org/abs/2507.12553)
*Michael A. Lepori,Jennifer Hu,Ishita Dasgupta,Roma Patel,Thomas Serre,Ellie Pavlick*

Main category: cs.CL

> 本研究指出语言模型具备比之前认为更可靠的句子语气分类能力，并开发了可以映射人类语气分类行为的语气差异向量，丰富了对模型语言理解机制及人类分类行为的认识。

<details>
  <summary>Details</summary>

**Motivation:** 研究动机是探讨语言模型在区分句子语气类别方面的能力，回应近期质疑其分类能力的研究。

**Method:** 研究通过识别语言模型中的语气差异向量来分析模型对句子语气类别的理解，并将其与人类分类行为进行比较。

**Result:** 该研究旨在识别出语言模型中能够区分句子语气类别（如可能、不可能或完全无意义等）的线性表示形式——语气差异向量。通过分析语气差异向量，研究人员发现语言模型对于语气类别的分类比之前认为的要更可靠。随着模型能力的提升（如训练步数、层深度及参数数量的增长），语气差异向量呈现出一致的发现顺序。更进一步，这些向量可以用来模仿人类对语气类别的精细分类行为，通过将人类参与者的评估与沿语气差异向量的投影相关联，为理解人们如何区别语气类别提供了新视角。这项研究利用机制解释技术，提供了关于语言模型语气分类的新见解，并可能对人类的语气分类理解有所贡献。

**Conclusion:** 研究得出结论，语言模型能够执行语气分类，其分级行为可以通过语气差异向量类比人类行为，这可能促进了对模型以及人类语气分类行为的理解。

**Abstract:** Language models (LMs) are used for a diverse range of tasks, from question
answering to writing fantastical stories. In order to reliably accomplish these
tasks, LMs must be able to discern the modal category of a sentence (i.e.,
whether it describes something that is possible, impossible, completely
nonsensical, etc.). However, recent studies have called into question the
ability of LMs to categorize sentences according to modality (Michaelov et al.,
2025; Kauf et al., 2023). In this work, we identify linear representations that
discriminate between modal categories within a variety of LMs, or modal
difference vectors. Analysis of modal difference vectors reveals that LMs have
access to more reliable modal categorization judgments than previously
reported. Furthermore, we find that modal difference vectors emerge in a
consistent order as models become more competent (i.e., through training steps,
layers, and parameter count). Notably, we find that modal difference vectors
identified within LM activations can be used to model fine-grained human
categorization behavior. This potentially provides a novel view into how human
participants distinguish between modal categories, which we explore by
correlating projections along modal difference vectors with human participants'
ratings of interpretable features. In summary, we derive new insights into LM
modal categorization using techniques from mechanistic interpretability, with
the potential to inform our understanding of modal categorization in humans.

</details>


### [3] [The first open machine translation system for the Chechen language](https://arxiv.org/abs/2507.12672)
*Abu-Viskhan A. Umishov,Vladislav A. Grigorian*

Main category: cs.CL

> 研究团队发布了第一个用于切森语和俄语之间翻译的开源模型，并评估了其性能，同时创建了相关的数据集和编码器以支持研究。

<details>
  <summary>Details</summary>

**Motivation:** 鉴于切森语是一个脆弱的语言，我们需要为这个语言的保护和保留作出努力。通过创建一个开放源代码的翻译模型，不仅可以支持切森语的保存，也可以推进多语言翻译系统的发展。

**Method:** 我们介绍了第一个用于切森语和俄语之间翻译的开源模型，并收集了用于训练和评估该模型的数据集。我们探讨了将新语言纳入NLLB-200多语言翻译系统中的微调能力。

**Result:** 我们的模型在BLEU / ChrF++评分上的表现分别为8.34 / 34.69（从俄语到切森语）和20.89 / 44.55（从切森语到俄语）。

**Conclusion:** 除了发布翻译模型外，我们还提供了平行词、短语和句子语料库以及适应切森语的多语言句子编码器的分布。

**Abstract:** We introduce the first open-source model for translation between the
vulnerable Chechen language and Russian, and the dataset collected to train and
evaluate it. We explore fine-tuning capabilities for including a new language
into a large language model system for multilingual translation NLLB-200. The
BLEU / ChrF++ scores for our model are 8.34 / 34.69 and 20.89 / 44.55 for
translation from Russian to Chechen and reverse direction, respectively. The
release of the translation models is accompanied by the distribution of
parallel words, phrases and sentences corpora and multilingual sentence encoder
adapted to the Chechen language.

</details>


### [4] [Improving Drug Identification in Overdose Death Surveillance using Large Language Models](https://arxiv.org/abs/2507.12679)
*Arthur J. Funnell,Panayiotis Petousis,Fabrice Harel-Canada,Ruby Romero,Alex A. T. Bui,Adam Koncsol,Hritika Chaturvedi,Chelsea Shover,David Goodman-Meza*

Main category: cs.CL

> 研究通过使用自然语言处理（NLP）模型，特别是基于ClinicalBERT的训练模型，实现了针对药物过量死亡案件中自由文本报告信息的高度准确分类，从而极大提高了监视工作流效率。

<details>
  <summary>Details</summary>

**Motivation:** 研究动机是解决由芬太尼等药物引发的吸毒致死率上升的问题，以及关键过量数据在自由文本法医报告中的隐藏导致延误和信息丢失的问题。此研究旨在利用NLP模型自动化和提高过量死亡监视效果。

**Method:** 本研究使用了来自美国多个司法管辖区的35,433份2020年死亡记录数据集进行模型训练和内部测试，并使用2023-2024年的3,335份记录数据集进行外部验证。评估了多种NLP方法对未结构化死亡证书文本中的特定药物涉及情况的分类效果，包括传统的单标签和多标签分类器，以及微调的编码器仅有模型如BERT和BioClinicalBERT，和当前的解码器仅有大型语言模型如Qwen 3和Llama 3。

**Result:** 在内部测试集中，微调的BioClinicalBERT模型取得了近完美的性能，表现优于传统的机器学习、通用领域BERT模型及各种解码器仅有大型语言模型。外部验证确认了其健壮性，表明NLP模型可以准确分类自由文本报告中的过量死亡案例。

**Conclusion:** NLP模型，尤其是微调的临床变体如BioClinicalBERT，为从自由文本报告中分类过量死亡提供了高精度且可扩展的解决方案。这些方法可以显著加速监视工作流，克服手工ICD-10编码限制，支持接近实时地检测新兴物质使用趋势。

**Abstract:** The rising rate of drug-related deaths in the United States, largely driven
by fentanyl, requires timely and accurate surveillance. However, critical
overdose data are often buried in free-text coroner reports, leading to delays
and information loss when coded into ICD (International Classification of
Disease)-10 classifications. Natural language processing (NLP) models may
automate and enhance overdose surveillance, but prior applications have been
limited. A dataset of 35,433 death records from multiple U.S. jurisdictions in
2020 was used for model training and internal testing. External validation was
conducted using a novel separate dataset of 3,335 records from 2023-2024.
Multiple NLP approaches were evaluated for classifying specific drug
involvement from unstructured death certificate text. These included
traditional single- and multi-label classifiers, as well as fine-tuned
encoder-only language models such as Bidirectional Encoder Representations from
Transformers (BERT) and BioClinicalBERT, and contemporary decoder-only large
language models such as Qwen 3 and Llama 3. Model performance was assessed
using macro-averaged F1 scores, and 95% confidence intervals were calculated to
quantify uncertainty. Fine-tuned BioClinicalBERT models achieved near-perfect
performance, with macro F1 scores >=0.998 on the internal test set. External
validation confirmed robustness (macro F1=0.966), outperforming conventional
machine learning, general-domain BERT models, and various decoder-only large
language models. NLP models, particularly fine-tuned clinical variants like
BioClinicalBERT, offer a highly accurate and scalable solution for overdose
death classification from free-text reports. These methods can significantly
accelerate surveillance workflows, overcoming the limitations of manual ICD-10
coding and supporting near real-time detection of emerging substance use
trends.

</details>


### [5] [AdaptiSent: Context-Aware Adaptive Attention for Multimodal Aspect-Based Sentiment Analysis](https://arxiv.org/abs/2507.12695)
*S M Rafiuddin,Sadia Kamal,Mohammed Rakib,Arunkumar Bagavathi,Atriya Sen*

Main category: cs.CL

> AdaptiSent is an advanced MABSA framework that uses adaptive cross-modal attention to improve sentiment and aspect term extraction from text and images, showing superior performance over existing models.

<details>
  <summary>Details</summary>

**Motivation:** To improve sentiment classification and aspect term extraction by leveraging the interaction between textual cues and visual context in multimodal datasets.

**Method:** Our model, AdaptiSent, uses adaptive cross-modal attention mechanisms for Multimodal Aspect-Based Sentiment Analysis (MABSA). It integrates dynamic modality weighting and context-adaptive attention to enhance sentiment and aspect term extraction from text and images.

**Result:** AdaptiSent outperforms existing models in precision, recall, and F1 score on standard Twitter datasets, particularly in identifying nuanced inter-modal relationships.

**Conclusion:** AdaptiSent sets a new standard for MABSA methods, showcasing superior performance, especially in dealing with complex multimodal data sets.

**Abstract:** We introduce AdaptiSent, a new framework for Multimodal Aspect-Based
Sentiment Analysis (MABSA) that uses adaptive cross-modal attention mechanisms
to improve sentiment classification and aspect term extraction from both text
and images. Our model integrates dynamic modality weighting and
context-adaptive attention, enhancing the extraction of sentiment and
aspect-related information by focusing on how textual cues and visual context
interact. We tested our approach against several baselines, including
traditional text-based models and other multimodal methods. Results from
standard Twitter datasets show that AdaptiSent surpasses existing models in
precision, recall, and F1 score, and is particularly effective in identifying
nuanced inter-modal relationships that are crucial for accurate sentiment and
aspect term extraction. This effectiveness comes from the model's ability to
adjust its focus dynamically based on the context's relevance, improving the
depth and accuracy of sentiment analysis across various multimodal data sets.
AdaptiSent sets a new standard for MABSA, significantly outperforming current
methods, especially in understanding complex multimodal information.

</details>


### [6] [AudioJudge: Understanding What Works in Large Audio Model Based Speech Evaluation](https://arxiv.org/abs/2507.12705)
*Potsawee Manakul,Woody Haosheng Gan,Michael J. Ryan,Ali Sartaz Khan,Warit Sirichotedumrong,Kunat Pipatanakul,William Held,Diyi Yang*

Main category: cs.CL

> A study on the use of a Large Audio Model (LAM) as AudioJudge finds that it can serve as a unified framework for assessing various audio characteristics and simulating human preferences effectively, with a multi-aspect ensemble reaching up to 0.91 Spearman correlation with human preferences, but showing biases in noisy conditions.

<details>
  <summary>Details</summary>

**Motivation:** The paper's motivation stems from the need to address the limitations in current speech evaluation methodologies, particularly the challenges in aligning automated evaluations with human auditory preferences and the necessity for specialized systems for different audio features.

**Method:** Current speech evaluation methods are challenged by the need for specialized systems for individual audio characteristics and low correlation with human preferences. This paper studies the use of Large Audio Model (LAM) as a 'Judge', focusing on its ability to provide a unified evaluation framework across various audio tasks, including pronunciation, speaking rate, speaker identification, speech quality, and simulation of human preferences. It explores different prompting strategies and introduces an ensemble method for multi-aspect evaluation.

**Result:** The study finds that combining audio concatenation with in-context learning significantly improves the performance of AudioJudge in audio characteristic detection and human preference simulation. A multi-aspect ensemble approach, comprising specialized judges for different speech evaluation facets, achieves 0.91 Spearman correlation with human preferences. However, LAMs exhibit verbosity and positional biases under noisy conditions.

**Conclusion:** The paper concludes that a Large Audio Model trained with specific prompt engineering strategies can effectively serve as a unified evaluation framework for automated benchmarking against human preferences, although there are identified biases that warrant further research.

**Abstract:** Current speech evaluation suffers from two critical limitations: the need and
difficulty of designing specialized systems targeting individual audio
characteristics, and poor correlation between automatic evaluation methods and
human preferences. This work presents a systematic study of Large Audio Model
(LAM) as a Judge, AudioJudge, investigating whether it can provide a unified
evaluation framework that addresses both challenges. We systematically explore
AudioJudge across audio characteristic detection tasks, including
pronunciation, speaking rate, speaker identification and speech quality, and
system-level human preference simulation for automated benchmarking. We
investigate different prompt engineering strategies, finding that audio
concatenation combined with in-context learning significantly improves
performance across both audio characteristic detection and human preference
simulation tasks. We further introduce a multi-aspect ensemble AudioJudge to
enable general-purpose multi-aspect audio evaluation. This method decomposes
speech assessment into specialized judges for lexical content, speech quality,
and paralinguistic features, achieving up to 0.91 Spearman correlation with
human preferences on our system ranking benchmark. Robustness analysis reveals
that while LAMs maintain strong performance under acoustic noise, they exhibit
significant verbosity and positional biases that require careful mitigation.

</details>


### [7] [FLEXITOKENS: Flexible Tokenization for Evolving Language Models](https://arxiv.org/abs/2507.12720)
*Abraham Toluase Owodunni,Orevaoghene Ahia,Sachin Kumar*

Main category: cs.CL

> 研究开发了具备可学习分词器的字节级语言模型，提出了FLEXITOKENS训练目标，于多种任务上相比传统方法提升了性能和适应性。

<details>
  <summary>Details</summary>

**Motivation:** 语言模型在通过简单微调适应新的数据分布时面临挑战，主要是因为子词分词器在适应过程中通常保持不变，这种不变性导致分词效率低下，特别是在处理数据分布之外的领域、未见过的语言或脚本时。

**Method:** 本研究开发了字节级语言模型，并采用可学习的分词器以使分词过程具有适应性。模型中包含了预测输入字节序列分界线的子模块，从而将其编码为可变长度的片段。研究中提出了FLEXITOKENS，这是一种简化的训练目标，能显著提高适应性。

**Result:** 在多个多语言基准测试、形态多样化的任务和领域中进行评估，FLEXITOKENS显著减少了分词过度碎片化的问题，并且在下游任务性能上比传统的子词和其他基于梯度的分词器提升了高达10%。

**Conclusion:** 研究结果表明，使用FLEXITOKENS技术的语言模型能够在多种应用场景下更有效地进行分词，显示出比现有模型更好的适应性及性能提升。

**Abstract:** Language models (LMs) are challenging to adapt to new data distributions by
simple finetuning. This is due to the rigidity of their subword tokenizers,
which typically remain unchanged during adaptation. This inflexibility often
leads to inefficient tokenization, causing overfragmentation of
out-of-distribution domains, unseen languages, or scripts. In this work, we
develop byte-level LMs with learnable tokenizers to make tokenization adaptive.
Our models include a submodule that learns to predict boundaries between the
input byte sequence, encoding it into variable-length segments. Existing
tokenizer-free methods train this boundary predictor using an auxiliary loss
that enforces a fixed compression rate across the training corpus, introducing
a new kind of rigidity. We propose FLEXITOKENS, a simplified training objective
that enables significantly greater flexibility during adaptation. Evaluating
across multiple multilingual benchmarks, morphologically diverse tasks, and
domains, we demonstrate that FLEXITOKENS consistently reduces token
over-fragmentation and achieves up to 10\% improvements on downstream task
performance compared to subword and other gradient-based tokenizers. Code and
data for our experiments will be released at
https://github.com/owos/flexitokens

</details>


### [8] [TransEvalnia: Reasoning-based Evaluation and Ranking of Translations](https://arxiv.org/abs/2507.12724)
*Richard Sproat,Tianyu Zhao,Llion Jones*

Main category: cs.CL

> 本研究介绍了一个名为TransEvalnia的基于推理和细粒度评估的翻译评价系统，该系统在多个语言对的测试中表现出色，并且其评分与人类评分的相关度很高。

<details>
  <summary>Details</summary>

**Motivation:** TransEvalnia旨在解决现有翻译评价系统的局限，通过提供基于多维质量指标的详细评估和排名来改进翻译质量评价。

**Method:** 本文提出了一种基于提示的翻译评估和排名系统TransEvalnia，该系统使用推理来执行其评估和排名工作。此系统基于Multidimensional Quality Metrics的子集进行细粒度的评估，并提供每个翻译维度的数值评分和整体翻译评分。为了进行评估，TransEvalnia使用了Anthropic的Claude-3.5-Sonnet和Qwen-2.5-72B-Instruct作为评估的大规模语言模型。

**Result:** TransEvalnia在英语-日语数据和WMT共享任务的多种语言对中，与现有的MT-Ranker相比表现持平或更优。实验表明，由Sonnet等大规模语言模型给出的评分与人类评分的相关性良好。此外，作者还发现了系统对翻译展示顺序的敏感性，并提出了缓解这一位置偏差的方法。

**Conclusion:** 研究结果显示，TransEvalnia系统作为一种基于语言模型的推理方法，相较于现有的优于或持平于与MT-Ranker的性能。敏感性分析表明系统对顺序有反应，提出了对策。该系统的全部数据、评估和推理情况、以及代码已公开，证明了其可靠性和透明度。

**Abstract:** We present TransEvalnia, a prompting-based translation evaluation and ranking
system that uses reasoning in performing its evaluations and ranking. This
system presents fine-grained evaluations based on a subset of the
Multidimensional Quality Metrics (https://themqm.org/), returns an assessment
of which translation it deems the best, and provides numerical scores for the
various dimensions and for the overall translation. We show that TransEvalnia
performs as well as or better than the state-of-the-art MT-Ranker (Moosa et al.
2024) on our own English-Japanese data as well as several language pairs from
various WMT shared tasks. Using Anthropic's Claude-3.5-Sonnet and
Qwen-2.5-72B-Instruct as the evaluation LLMs, we show that the evaluations
returned are deemed highly acceptable to human raters, and that the scores
assigned to the translations by Sonnet, as well as other LLMs, correlate well
with scores assigned by the human raters. We also note the sensitivity of our
system -- as well as MT-Ranker -- to the order in which the translations are
presented, and we propose methods to address this position bias. All data,
including the system's evaluation and reasoning, human assessments, as well as
code is released.

</details>


### [9] [Strategy Adaptation in Large Language Model Werewolf Agents](https://arxiv.org/abs/2507.12732)
*Fuya Nakamori,Yin Jou Huang,Fei Cheng*

Main category: cs.CL

> This paper presents a method for Werewolf agents to adapt their strategies based on game context and player roles, showing improved performance over agents with static strategies.

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to overcome the limitation of prior works where Werewolf agents using prompt engineering cannot adapt to changing situations.

**Method:** This paper proposes a method for Werewolf agents to improve their performance by switching between predefined strategies based on the attitudes of other players and the context of conversations.

**Result:** The research compares agents with the proposed strategy adaptation method against baseline agents using implicit or fixed strategies, demonstrating the effectiveness of the new approach.

**Conclusion:** The conclusion is that Werewolf agents which adapt strategies explicitly based on game context and estimated roles of other players perform better than those with implicit or fixed strategies.

**Abstract:** This study proposes a method to improve the performance of Werewolf agents by
switching between predefined strategies based on the attitudes of other players
and the context of conversations. While prior works of Werewolf agents using
prompt engineering have employed methods where effective strategies are
implicitly defined, they cannot adapt to changing situations. In this research,
we propose a method that explicitly selects an appropriate strategy based on
the game context and the estimated roles of other players. We compare the
strategy adaptation Werewolf agents with baseline agents using implicit or
fixed strategies and verify the effectiveness of our proposed method.

</details>


### [10] [Logit Arithmetic Elicits Long Reasoning Capabilities Without Training](https://arxiv.org/abs/2507.12759)
*Yunxiang Zhang,Muhammad Khalifa,Lechen Zhang,Xin Liu,Ayoung Lee,Xinliang Frederick Zhang,Farima Fatahi Bayat,Lu Wang*

Main category: cs.CL

> 研究提出了解码时利用logits算术调整大模型进行长期推理的方法ThinkLogit，及通过偏好优化训练引导模型的方法ThinkLogit-DPO，实验结果表明它们在无需额外训练的情况下显著提升了大模型的长期推理能力。

<details>
  <summary>Details</summary>

**Motivation:** 研究目的在于探索无需额外训练即可激发大模型执行长链推理的能力。

**Method:** 提出了一种解码时的策略ThinkLogit，利用logits算术来调整大模型用于长期推理，使用一个小得多的模型作为引导模型。同时提出ThinkLogit-DPO，通过偏好优化训练引导模型，使用从目标模型和引导模型采样的正确/错误推理对来提升性能。

**Result:** 实验结果显示，相较于Qwen2.5-32B基础模型，在R1-Distill-Qwen-1.5B引导下，ThinkLogit和ThinkLogit-DPO在四个数学数据集上分别实现了26%和29%的通过率绝对提升，表明该方法在最小或无需额外训练情况下激活大模型长期推理能力的有效性。

**Conclusion:** 提出了计算效率高的方法，可以以最小的或没有额外训练的方式提高大型模型的长链推理能力。

**Abstract:** Large reasoning models (LRMs) can do complex reasoning via long
chain-of-thought (CoT) involving cognitive strategies such as backtracking and
self-correction. Recent studies suggest that some models inherently possess
these long reasoning abilities, which may be unlocked via extra training. Our
work first investigates whether we can elicit such behavior without any
training. To this end, we propose a decoding-time approach, ThinkLogit, which
utilizes logits arithmetic (Liu et al., 2024) to tune a target large LM for
long reasoning using a substantially smaller model as guider. We then show that
we can further boost performance by training the guider model with preference
optimization over correct/incorrect reasoning pairs sampled from both the
target and guider model -- a setup we refer to as ThinkLogit-DPO. Our
experiments demonstrate that ThinkLogit and ThinkLogit-DPO achieve a relative
improvement in pass@1 by 26% and 29%, respectively, over four mathematical
datasets using the Qwen2.5-32B when guided by R1-Distill-Qwen-1.5B -- a model
21x smaller. Lastly, we show that ThinkLogit can transfer long reasoning skills
acquired through reinforcement learning, improving pass@1 by 13% relative
compared to the Qwen2.5-32B base model. Our work presents a
computationally-efficient method to elicit long reasoning in large models with
minimal or no additional training.

</details>


### [11] [Synergy: End-to-end Concept Model](https://arxiv.org/abs/2507.12769)
*Keli Zheng,Zerong Xie*

Main category: cs.CL

> Synergy模型通过学习路由机制无缝连接不同抽象层级，展示了字节级标记化的高效性，无需传统标记化即可获得较好性能，特别是在去除位置编码后，其抽象部分表现更优，为未来语言模型指明了方向。

<details>
  <summary>Details</summary>

**Motivation:** 针对现有标记化方法在词元数量和模型性能之间的平衡问题，本论文提出了无标记化方法，致力于创建一个更加高效和灵活的语言模型。

**Method:** 本研究通过训练一种采用学习路由机制的字节级语言模型，旨在无标记化状态下连接不同的抽象层次。主要侧重于模型中自发学习标记化的能力，基于此特征进行性能比较。

**Result:** 本文提出了一种名为Synergy的语言模型，该模型通过一种学习路由机制在不同抽象层次之间进行无缝连接。研究集中于低层次的语言抽象，将模型作为字节级语言模型进行训练。模型自发地学习字节的标记化过程，相比Byte-level Byte Pair Encoder (BBPE)标记器，它生成的词元更少但性能相当。与Llama3相比，在相同模型规模和训练数据集大小的情况下，Synergy显示出优势。进一步的研究表明，移除位置编码后，模型的中层部分（即更高抽象部分）表现更好，显示出位置独立的概念的出现。这些发现证明了无标记化架构的可行性，为构建更稳健和灵活的流水线铺平了道路。

**Conclusion:** 研究表明，Synergy不仅在抽象层面上优化了语言模型的表现，而且无需依赖传统的标记化过程，这可能标志着自然语言处理方法的一个重要进步，推动更加高效、灵活的处理方式的发展。

**Abstract:** In this paper, we present Synergy, a language model that bridges different
levels of abstraction in an end-to-end fashion through a learned routing
mechanism. Focusing on low-level linguistic abstraction, we trained our model
as a byte-level language model. Our model spontaneously learns to tokenize
bytes, producing fewer concept tokens than Byte-level Byte Pair Encoder (BBPE)
tokenizers while keeping comparable performance. By comparing with Llama3, we
observed an advantage of Synergy under the same model scale and training
dataset size. Further studies show that the middle part (the higher abstraction
part) of our model performs better when positional encodings are removed,
suggesting the emergence of position-independent concepts. These findings
demonstrate the feasibility of tokenizer-free architectures, paving the way for
more robust and flexible pipelines.

</details>


### [12] [Learning Robust Negation Text Representations](https://arxiv.org/abs/2507.12782)
*Thinh Hung Truong,Karin Verspoor,Trevor Cohn,Timothy Baldwin*

Main category: cs.CL

> 提出了一种通过对比学习提升文本编码器否定理解能力的方法。

<details>
  <summary>Details</summary>

**Motivation:** 尽管自动回归语言模型快速发展，文本编码器在需要丰富上下文表示的文本理解任务中仍发挥重要作用。然而，这类方法未能很好地捕捉否定的重要性，影响了许多依赖文本嵌入的下游应用。该研究的目的是提升文本编码器对否定的识别能力。

**Method:** 通过从大型语言模型中使用多样化的否定和修饰语模式提取数据，采用标准的对比学习策略微调了一个强大的基于BERT的模型，以提高文本编码器的否定鲁棒性。

**Result:** 观察到在保持一般基准测试上的竞争力的同时，该方法显著提高了否定理解能力。此外，该方法还能被应用于大型语言模型，并提升了其在否定基准测试上的表现。

**Conclusion:** 通过对比学习策略，可以显著提升文本编码器对否定的识别能力，并且该策略可以应用于大型语言模型以提高其在否定识别上的性能。

**Abstract:** Despite rapid adoption of autoregressive large language models, smaller text
encoders still play an important role in text understanding tasks that require
rich contextualized representations. Negation is an important semantic function
that is still not properly captured by such methods, affecting many downstream
applications relying on text embeddings. We propose a strategy to improve
negation robustness of text encoders, by distilling data from large language
models using diverse patterns of negation and hedging. We adopt a standard
contrastive learning strategy to finetune a strong BERT-based model, and
observe large improvement in negation understanding capabilities while
maintaining competitive performance on general benchmarks. In addition, we also
show that our method can be adapted to LLMs, leading to improved performance on
negation benchmarks.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [13] [Spatially Grounded Explanations in Vision Language Models for Document Visual Question Answering](https://arxiv.org/abs/2507.12490)
*Maximiliano Hormazábal Lagos,Héctor Cerezo-Costas,Dimosthenis Karatzas*

Main category: cs.CV

> EaGERS is a novel, training-free pipeline for enhancing the DocVQA performance of vision-language models by incorporating reasoning and grounding mechanisms, which improves model accuracy and interpretability.

<details>
  <summary>Details</summary>

**Motivation:** The motivation for EaGERS is to develop a flexible, training-free tool for enhancing the performance of VQA models on tasks like DocVQA while maintaining or improving transparency and reproducibility in the model output.

**Method:** EaGERS, a fully training-free and model-agnostic pipeline, works by generating natural language rationales through a vision language model, grounding these rationales to spatial sub-regions using multimodal embedding similarities with majority voting over a grid, and restricting the generation of responses to only the relevant regions of a masked image.

**Result:** The best configuration of EaGERS outperforms the base model on DocVQA's exact match accuracy and Average Normalized Levenshtein Similarity metrics, while also improving transparency and reproducibility without the need for further model fine-tuning.

**Conclusion:** EaGERS offers an effective and flexible method for enhancing VQA performance, especially on specialized datasets like DocVQA, by integrating rational explanations and spatial reasoning, and does so without requiring additional model training.

**Abstract:** We introduce EaGERS, a fully training-free and model-agnostic pipeline that
(1) generates natural language rationales via a vision language model, (2)
grounds these rationales to spatial sub-regions by computing multimodal
embedding similarities over a configurable grid with majority voting, and (3)
restricts the generation of responses only from the relevant regions selected
in the masked image. Experiments on the DocVQA dataset demonstrate that our
best configuration not only outperforms the base model on exact match accuracy
and Average Normalized Levenshtein Similarity metrics but also enhances
transparency and reproducibility in DocVQA without additional model
fine-tuning.

</details>


### [14] [MindJourney: Test-Time Scaling with World Models for Spatial Reasoning](https://arxiv.org/abs/2507.12508)
*Yuncong Yang,Jiageng Liu,Zheyuan Zhang,Siyuan Zhou,Reuben Tan,Jianwei Yang,Yilun Du,Chuang Gan*

Main category: cs.CV

> This paper presents MindJourney, a framework that couples a vision-language model with a world model to improve 3D spatial reasoning without fine-tuning, resulting in an 8% boost in performance on benchmark tests.

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to enhance vision-language models' ability to reason in 3D spaces, which is crucial for human-like tasks such as navigation but is lacking in current state-of-the-art models.

**Method:** The method uses a test-time scaling framework called MindJourney that pairs a vision-language model with a controllable world model based on video diffusion to enable 3D spatial reasoning. The world model synthesizes views at each step of a camera trajectory proposed by the VLM.

**Result:** The result is a significant performance boost (over 8%) on the SAT spatial reasoning benchmark without the need for fine-tuning, indicating the effectiveness of the approach.

**Conclusion:** The conclusion is that integrating vision-language models with world models via test-time scaling is an effective and simple way to enhance 3D reasoning capabilities, outweighing methods that train VLMs through reinforcement learning for test-time inference.

**Abstract:** Spatial reasoning in 3D space is central to human cognition and indispensable
for embodied tasks such as navigation and manipulation. However,
state-of-the-art vision-language models (VLMs) struggle frequently with tasks
as simple as anticipating how a scene will look after an egocentric motion:
they perceive 2D images but lack an internal model of 3D dynamics. We therefore
propose MindJourney, a test-time scaling framework that grants a VLM with this
missing capability by coupling it to a controllable world model based on video
diffusion. The VLM iteratively sketches a concise camera trajectory, while the
world model synthesizes the corresponding view at each step. The VLM then
reasons over this multi-view evidence gathered during the interactive
exploration. Without any fine-tuning, our MindJourney achieves over an average
8% performance boost on the representative spatial reasoning benchmark SAT,
showing that pairing VLMs with world models for test-time scaling offers a
simple, plug-and-play route to robust 3D reasoning. Meanwhile, our method also
improves upon the test-time inference VLMs trained through reinforcement
learning, which demonstrates the potential of our method that utilizes world
models for test-time scaling.

</details>


### [15] [Mono-InternVL-1.5: Towards Cheaper and Faster Monolithic Multimodal Large Language Models](https://arxiv.org/abs/2507.12566)
*Gen Luo,Wenhan Dou,Wenhao Li,Zhaokai Wang,Xue Yang,Changyao Tian,Hao Li,Weiyun Wang,Wenhai Wang,Xizhou Zhu,Yu Qiao,Jifeng Dai*

Main category: cs.CV

> 本文提出Mono-InternVL和Mono-InternVL-1.5，这两者是先进的单片多模态大型语言模型，能够在视觉理解上表现出色并有效减少训练和推理成本。

<details>
  <summary>Details</summary>

**Motivation:** 现有的单片MLLM结构和预训练策略常常遭遇不稳定优化和灾难性遗忘的问题。为解决这些问题，本文提出了一个新的方法和改进版模型。

**Method:** 通过将新的视觉参数空间嵌入到已预训练的语言模型中，使得可以利用增量调优从嘈杂的数据中稳定地学习视觉知识。提出了Mono-InternVL，一个集成了视觉专家的先进单片MLLM，并设计了名为EViP的内源视觉预训练方法，通过渐进式学习最大化其视觉能力。进一步提出了Mono-InternVL-1.5，配备改进后的EViP++，引入额外的视觉注意力专家并以更高效的方式重组预训练过程。

**Result:** 实验结果表明，Mono-InternVL在15个基准测试中的12个上优于现有的单片MLLM，Mono-InternVL-1.5与模块化同行相比，性能相似但延迟减少高达69%。

**Conclusion:** 提出的Mono-InternVL和Mono-InternVL-1.5单片MLLM，在视觉理解和性能方面取得了显著成果，并在成本方面有所降低。

**Abstract:** This paper focuses on monolithic Multimodal Large Language Models (MLLMs),
which integrate visual encoding and language decoding into a single model.
Existing structures and pre-training strategies for monolithic MLLMs often
suffer from unstable optimization and catastrophic forgetting. To address these
challenges, our key idea is to embed a new visual parameter space into a
pre-trained LLM, enabling stable learning of visual knowledge from noisy data
via delta tuning. Based on this principle, we first introduce Mono-InternVL, an
advanced monolithic MLLM that incorporates a set of visual experts through a
multimodal mixture-of-experts architecture. In addition, we design an
innovative Endogenous Visual Pre-training (EViP) for Mono-InternVL to maximize
its visual capabilities via progressive learning. Mono-InternVL achieves
competitive performance against existing MLLMs but also leads to relatively
expensive data cost. Therefore, we further present Mono-InternVL-1.5, a cheaper
and stronger monolithic MLLM equipped with an improved EViP (EViP++). EViP++
introduces additional visual attention experts to Mono-InternVL-1.5 and
re-organizes the pre-training process in an efficient manner. During inference,
it includes a fused CUDA kernel to speed up its MoE operations. With these
designs, Mono-InternVL-1.5 significantly reduces training and inference costs,
while still maintaining competitive performance with Mono-InternVL. To evaluate
our approach, we conduct extensive experiments across 15 benchmarks. Results
demonstrate that Mono-InternVL outperforms existing monolithic MLLMs on 12 out
of 15 benchmarks, e.g., +114-point improvement over Emu3 on OCRBench. Compared
to its modular counterpart, i.e., InternVL-1.5, Mono-InternVL-1.5 achieves
similar multimodal performance while reducing first-token latency by up to 69%.
Code and models are released at https://github.com/OpenGVLab/Mono-InternVL.

</details>


### [16] [Best Practices for Large-Scale, Pixel-Wise Crop Mapping and Transfer Learning Workflows](https://arxiv.org/abs/2507.12590)
*Judy Long,Tao Liu,Sean Alexander Woznicki,Miljana Marković,Oskar Marko,Molly Sears*

Main category: cs.CV

> 本研究首次全面回顾了大尺度基于像素的作物制图工作流程，比较了多种预处理方法、监督分类模型，并研究了迁移学习技术在不同领域偏移情况下的表现。主要发现是变压器模型搭配精细区间预处理在监督和可迁移的工作流中表现最优，而随机森林在传统监督学习和直接迁移相似领域中训练速度快且性能良好。当标记样本充足时，监督训练通常提供更精确和泛化能力强的结果；当样本不足时，匹配领域偏移的迁移学习成为有效替代方案。

<details>
  <summary>Details</summary>

**Motivation:** 研究旨在确定最优的基于像素的作物制图工作流程，尤其是比较传统的监督方法和新兴的迁移学习方法。

**Method:** 研究比较了六种广泛采用的卫星图像预处理方法和十一种监督像素分类模型，评估了不同训练样本量和变量组合的影响，并在不同规模的农业站点评估了最佳方法。

**Result:** 研究发现了精细区间预处理与变压器模型组合最优、随机森林在传统监督学习中表现良好、迁移学习技术提高工作流适应性等主要结论。

**Conclusion:** 研究强调了工作流选择高度依赖于标记样本的可用性，并提出在样本不足的情况下，适配领域偏移的迁移学习是一种可行的替代方法。

**Abstract:** Crop mapping involves identifying and classifying crop types using spatial
data, primarily derived from remote sensing imagery. This study presents the
first comprehensive review of large-scale, pixel-wise crop mapping workflows,
encompassing both conventional supervised methods and emerging transfer
learning approaches. To identify the optimal supervised crop mapping workflows,
we conducted systematic experiments, comparing six widely adopted satellite
image-based preprocessing methods, alongside eleven supervised pixel-wise
classification models. Additionally, we assessed the synergistic impact of
varied training sample sizes and variable combinations. Moreover, we identified
optimal transfer learning techniques for different magnitudes of domain shift.
The evaluation of best methods was conducted across five diverse agricultural
sites. Landsat 8 served as the primary satellite data source. Labels come from
CDL trusted pixels and field surveys.
  Our findings reveal three key insights. First, fine-scale interval
preprocessing paired with Transformer models consistently delivered optimal
performance for both supervised and transferable workflows. RF offered rapid
training and competitive performance in conventional supervised learning and
direct transfer to similar domains. Second, transfer learning techniques
enhanced workflow adaptability, with UDA being effective for homogeneous crop
classes while fine-tuning remains robust across diverse scenarios. Finally,
workflow choice depends heavily on the availability of labeled samples. With a
sufficient sample size, supervised training typically delivers more accurate
and generalizable results. Below a certain threshold, transfer learning that
matches the level of domain shift is a viable alternative to achieve crop
mapping. Repository:
Best-Practices-for-Large-Scale-Pixel-Wise-Crop-Mapping-and-Transfer-Learning-Workflows

</details>


### [17] [CT-ScanGaze: A Dataset and Baselines for 3D Volumetric Scanpath Modeling](https://arxiv.org/abs/2507.12591)
*Trong-Thang Pham,Akash Awasthi,Saba Khan,Esteban Duran Marti,Tien-Phat Nguyen,Khoa Vo,Minh Tran,Ngoc Son Nguyen,Cuong Tran Van,Yuki Ikebe,Anh Totti Nguyen,Anh Nguyen,Zhigang Deng,Carol C. Wu,Hien Van Nguyen,Ngan Le*

Main category: cs.CV

> 研究提出了CT-ScanGaze——首个公开的CT眼动数据集和CT-Searcher——一种特别处理CT体积并生成类似放射科医生的3D注视序列的3D扫描路径预测器，为3D扫描路径预测提供了新的解决方案和评估框架。

<details>
  <summary>Details</summary>

**Motivation:** 鉴于缺乏公开的眼动追踪数据集以及CT体积的三维复杂性，研究旨在促进对放射科医生在CT阅读期间眼动的理解，这有助于开发有效的可解释性计算机辅助诊断系统。

**Method:** 通过构建首个公开的CT眼动追踪数据集CT-ScanGaze和开发CT-Searcher——一种专门处理CT体积并生成类似放射科医生的3D注视序列的3D扫描路径预测器，研究解决了现有的2D扫描路径预测器的局限性。此外，还开发了一种将现有的2D注视数据集转换为3D注视数据的预训练流程，以增强模型的深度学习效果。

**Result:** 通过定性和定量的评估，研究展示了该方法在CT-ScanGaze上的有效性，并为医学成像中的3D扫描路径预测提供了一个全面的评估框架。

**Conclusion:** 研究结果表明了开发3D扫描路径预测器在促进理解和解释医学图像中的作用，同时展示了其作为一种新的工具在放射科学中的潜力。

**Abstract:** Understanding radiologists' eye movement during Computed Tomography (CT)
reading is crucial for developing effective interpretable computer-aided
diagnosis systems. However, CT research in this area has been limited by the
lack of publicly available eye-tracking datasets and the three-dimensional
complexity of CT volumes. To address these challenges, we present the first
publicly available eye gaze dataset on CT, called CT-ScanGaze. Then, we
introduce CT-Searcher, a novel 3D scanpath predictor designed specifically to
process CT volumes and generate radiologist-like 3D fixation sequences,
overcoming the limitations of current scanpath predictors that only handle 2D
inputs. Since deep learning models benefit from a pretraining step, we develop
a pipeline that converts existing 2D gaze datasets into 3D gaze data to
pretrain CT-Searcher. Through both qualitative and quantitative evaluations on
CT-ScanGaze, we demonstrate the effectiveness of our approach and provide a
comprehensive assessment framework for 3D scanpath prediction in medical
imaging.

</details>


### [18] [MS-DGCNN++: A Multi-Scale Fusion Dynamic Graph Neural Network with Biological Knowledge Integration for LiDAR Tree Species Classification](https://arxiv.org/abs/2507.12602)
*Said Ohamouddou,Abdellatif El Afia,Hanaa El Afia,Raddouane Chiheb*

Main category: cs.CV

> 提出了MS-DGCNN++, 一种在不同尺度上捕获语义信息的层次多尺度融合动态图卷积网络，提高了森林环境中树种分类的准确性，且泛化到标准3D物体识别任务中。

<details>
  <summary>Details</summary>

**Motivation:** 现有的多尺度动态图卷积神经网络方法使用并行多尺度处理，无法捕捉到树结构不同层级之间的语义关系。

**Method:** MS-DGCNN++使用在局部、枝干和冠层尺度上的语义特征提取以及跨尺度信息传播，包括局部尺度的标准几何特征、枝干尺度的归一化相对向量以及冠层尺度的距离信息。

**Result:** 在STPCTLS数据集上准确率达到94.96%，在FOR-species20K数据集上准确率为67.25%，并提高了标准3D物体识别的准确度。

**Conclusion:** 展示了MS-DGCNN++不仅适用于树种分类，也能在标准3D物体识别任务中表现出色，具备良好的应用前景。

**Abstract:** Tree species classification from terrestrial LiDAR point clouds is
challenging because of the complex multi-scale geometric structures in forest
environments. Existing approaches using multi-scale dynamic graph convolutional
neural networks (MS-DGCNN) employ parallel multi-scale processing, which fails
to capture the semantic relationships between the hierarchical levels of the
tree architecture. We present MS-DGCNN++, a hierarchical multiscale fusion
dynamic graph convolutional network that uses semantically meaningful feature
extraction at local, branch, and canopy scales with cross-scale information
propagation. Our method employs scale-specific feature engineering, including
standard geometric features for the local scale, normalized relative vectors
for the branch scale, and distance information for the canopy scale. This
hierarchical approach replaces uniform parallel processing with semantically
differentiated representations that are aligned with the natural tree
structure. Under the same proposed tree species data augmentation strategy for
all experiments, MS-DGCNN++ achieved an accuracy of 94.96 \% on STPCTLS,
outperforming DGCNN, MS-DGCNN, and the state-of-the-art model PPT. On
FOR-species20K, it achieves 67.25\% accuracy (6.1\% improvement compared to
MS-DGCNN). For standard 3D object recognition, our method outperformed DGCNN
and MS-DGCNN with overall accuracies of 93.15\% on ModelNet40 and 94.05\% on
ModelNet10. With lower parameters and reduced complexity compared to
state-of-the-art transformer approaches, our method is suitable for
resource-constrained applications while maintaining a competitive accuracy.
Beyond tree classification, the method generalizes to standard 3D object
recognition, establishing it as a versatile solution for diverse point cloud
processing applications. The implementation code is publicly available at
https://github.com/said-ohamouddou/MS-DGCNN2.

</details>


### [19] [Predicting Soccer Penalty Kick Direction Using Human Action Recognition](https://arxiv.org/abs/2507.12617)
*David Freire-Obregón,Oliverio J. Santana,Javier Lorenzo-Navarro,Daniel Hernández-Sosa,Modesto Castrillón-Santana*

Main category: cs.CV

> 该研究通过创建一个用于预测射门方向的新足球点球数据集，展示了在体育预测任务中利用深度学习模型进行动作预测的潜力。

<details>
  <summary>Details</summary>

**Motivation:** 动作预测在人类动作识别（HAR）领域成为一个显著话题，但其在实际体育场景中的应用受到了合适注释数据集的限制。本研究旨在通过提供新的注释数据集来解决这一问题。

**Method:** 本研究提出了一种新的手动注释的足球点球数据集，用于基于射门前的球员动作来预测射门方向。研究中采用了深度学习分类器整合了基于人体动作识别的特征嵌入及上下文元数据来进行基准测试。

**Result:** 研究评估了来自七个架构系列（MViTv2, MViTv1, SlowFast, Slow, X3D, I3D, C2D）的二十二种骨干模型，预测射门方向（左或右）的准确率高达63.9%，超过了实际守门员的决策水平。

**Conclusion:** 这些结果显示了该数据集在预测性动作识别中的价值，并验证了我们模型作为体育预测任务普遍适用性方法的潜力。

**Abstract:** Action anticipation has become a prominent topic in Human Action Recognition
(HAR). However, its application to real-world sports scenarios remains limited
by the availability of suitable annotated datasets. This work presents a novel
dataset of manually annotated soccer penalty kicks to predict shot direction
based on pre-kick player movements. We propose a deep learning classifier to
benchmark this dataset that integrates HAR-based feature embeddings with
contextual metadata. We evaluate twenty-two backbone models across seven
architecture families (MViTv2, MViTv1, SlowFast, Slow, X3D, I3D, C2D),
achieving up to 63.9% accuracy in predicting shot direction (left or right),
outperforming the real goalkeepers' decisions. These results demonstrate the
dataset's value for anticipatory action recognition and validate our model's
potential as a generalizable approach for sports-based predictive tasks.

</details>


### [20] [Funnel-HOI: Top-Down Perception for Zero-Shot HOI Detection](https://arxiv.org/abs/2507.12628)
*Sandipan Sarma,Agney Talwarr,Arijit Sur*

Main category: cs.CV

> 我们构建了一个名为Funnel-HOI的自顶向下框架，通过新颖的非对称协同注意力机制在编码阶段强化交互表示，并设计了一种新的损失函数，从而提升人类物体交互检测的性能，特别对于未见过和稀有的交互类别有显著提升。

<details>
  <summary>Details</summary>

**Motivation:** 目前HOI检测主要集中在设计改进的解码器以学习纠缠或解纠缠的交互解释。我们主张HOI特定的线索必须在编码阶段就预见，以获得更强的场景解释。因此，我们引入了一种自顶向下的框架，灵感来自于人类在场景理解时首先掌握明确概念然后关联抽象概念的认知方式。

**Method:** 我们提出了一个名为Funnel-HOI的自顶向下框架，该框架首先检测图像中的物体（明确的概念），然后识别这些物体相关的动作（抽象的概念）。一种新颖的非对称协同注意机制利用多模态信息（包括零样本学习能力）来挖掘这些线索，从而在编码阶段获得更强的交互表示。此外，还设计了一种新的损失函数，该函数考虑了物体和动作之间的相关性，并更好地控制了错误分类的惩罚，从而指导交互分类器的学习。

**Result:** 实验结果显示，我们在HICO-DET和V-COCO数据集上实现了最高性能，对于未见过和稀有的交互类别分别提升了12.4%和8.4%的性能。

**Conclusion:** 在HICO-DET和V-COCO数据集上的广泛实验表明，我们的方法在全监督和六种零样本场景下均表现出最先进的性能，尤其是在未见过和稀有的交互类别上的提升更加显著。

**Abstract:** Human-object interaction detection (HOID) refers to localizing interactive
human-object pairs in images and identifying the interactions. Since there
could be an exponential number of object-action combinations, labeled data is
limited - leading to a long-tail distribution problem. Recently, zero-shot
learning emerged as a solution, with end-to-end transformer-based object
detectors adapted for HOID becoming successful frameworks. However, their
primary focus is designing improved decoders for learning entangled or
disentangled interpretations of interactions. We advocate that HOI-specific
cues must be anticipated at the encoder stage itself to obtain a stronger scene
interpretation. Consequently, we build a top-down framework named Funnel-HOI
inspired by the human tendency to grasp well-defined concepts first and then
associate them with abstract concepts during scene understanding. We first
probe an image for the presence of objects (well-defined concepts) and then
probe for actions (abstract concepts) associated with them. A novel asymmetric
co-attention mechanism mines these cues utilizing multimodal information
(incorporating zero-shot capabilities) and yields stronger interaction
representations at the encoder level. Furthermore, a novel loss is devised that
considers objectaction relatedness and regulates misclassification penalty
better than existing loss functions for guiding the interaction classifier.
Extensive experiments on the HICO-DET and V-COCO datasets across
fully-supervised and six zero-shot settings reveal our state-of-the-art
performance, with up to 12.4% and 8.4% gains for unseen and rare HOI
categories, respectively.

</details>


### [21] [Reconstruct, Inpaint, Finetune: Dynamic Novel-view Synthesis from Monocular Videos](https://arxiv.org/abs/2507.12646)
*Kaihua Chen,Tarasha Khurana,Deva Ramanan*

Main category: cs.CV

> A new approach for synthesizing novel views of dynamic scenes from monocular videos using 3D reconstruction and 2D diffusion models for inpainting, which outperforms previous methods.

<details>
  <summary>Details</summary>

**Motivation:** The goal is to improve novel-view synthesis for dynamic scenes from monocular videos, especially addressing the limitations of prior methods that either require costly test-time optimization or fail to preserve scene geometry in a feed-forward model.

**Method:** Our method involves three key components: reconstructing the dynamic 3D scene from covisible pixels, using a feed-forward 2D video diffusion model for inpainting hidden pixels, and training a video inpainting diffusion model (CogNVS) in a self-supervised manner using 2D videos.

**Result:** The approach proposed, CogNVS, achieves superior performance in novel-view synthesis of dynamic scenes from monocular videos compared to almost all previous methods.

**Conclusion:** Our introduced approach, CogNVS, demonstrates significant advancements in the synthesis of novel views for dynamic scenes captured by monocular videos, showcasing a blend of 3D scene reconstruction and 2D diffusion-based inpainting.

**Abstract:** We explore novel-view synthesis for dynamic scenes from monocular videos.
Prior approaches rely on costly test-time optimization of 4D representations or
do not preserve scene geometry when trained in a feed-forward manner. Our
approach is based on three key insights: (1) covisible pixels (that are visible
in both the input and target views) can be rendered by first reconstructing the
dynamic 3D scene and rendering the reconstruction from the novel-views and (2)
hidden pixels in novel views can be "inpainted" with feed-forward 2D video
diffusion models. Notably, our video inpainting diffusion model (CogNVS) can be
self-supervised from 2D videos, allowing us to train it on a large corpus of
in-the-wild videos. This in turn allows for (3) CogNVS to be applied zero-shot
to novel test videos via test-time finetuning. We empirically verify that
CogNVS outperforms almost all prior art for novel-view synthesis of dynamic
scenes from monocular videos.

</details>


### [22] [Integrated Oculomics and Lipidomics Reveal Microvascular Metabolic Signatures Associated with Cardiovascular Health in a Healthy Cohort](https://arxiv.org/abs/2507.12663)
*Inamullah,Ernesto Elias Vidal Rosas,Imran Razzak,Shoaib Jameel*

Main category: cs.CV

> 本文提出一种成像组学框架，结合视网膜血管特征与脂质组学，首次在大样本健康人群中研究了这些特征之间的相关性，发现对心血管疾病早期检测和预防有重要价值。

<details>
  <summary>Details</summary>

**Motivation:** 现有的心血管疾病（CVD）风险分层方法常常未能早期检测到无症状变化。该研究结合视网膜微血管特征与血清脂质组学数据，以识别无症状的心血管风险生物标志物，超越常规血脂分析。

**Method:** 介绍了结合深度学习图像处理提取的视网膜微血管特征与血清脂质组学数据的创新性成像组学框架，利用自动化图像分析工具量化解剖视网膜表型，通过超高效液相色谱-电喷雾电离-高分辨率质谱（UHPLC-ESI-HRMS）进行血清脂质分析。

**Result:** 在健康人群中首次进行了大规模的、协变量调整和分层的相关性分析，发现了视网膜表型（如平均动脉宽度、血管密度）与脂质亚类（如三酰甘油（TAGs）、二酰甘油（DAGs）、鞘脂类（Cers））之间存在强的、与年龄和性别无关的相关性。

**Conclusion:** 研究表明，将详细的血管结构表型与特定脂质物种联系起来，可以填补对CVD早期发病机制理解的空白，为非侵入性生物标志物的识别提供了新视角，并有可能支持CVD的早期检测、个性化预防和治疗。

**Abstract:** Cardiovascular disease (CVD) remains the leading global cause of mortality,
yet current risk stratification methods often fail to detect early, subclinical
changes. Previous studies have generally not integrated retinal
microvasculature characteristics with comprehensive serum lipidomic profiles as
potential indicators of CVD risk. In this study, an innovative imaging omics
framework was introduced, combining retinal microvascular traits derived
through deep learning based image processing with serum lipidomic data to
highlight asymptomatic biomarkers of cardiovascular risk beyond the
conventional lipid panel. This represents the first large scale, covariate
adjusted and stratified correlation analysis conducted in a healthy population,
which is essential for identifying early indicators of disease. Retinal
phenotypes were quantified using automated image analysis tools, while serum
lipid profiling was performed by Ultra High Performance Liquid Chromatography
Electrospray ionization High resolution mass spectrometry (UHPLC ESI HRMS).
Strong, age- and sex-independent correlations were established, particularly
between average artery width, vessel density, and lipid subclasses such as
triacylglycerols (TAGs), diacylglycerols (DAGs), and ceramides (Cers). These
associations suggest a converging mechanism of microvascular remodeling under
metabolic stress. By linking detailed
  vascular structural phenotypes to specific lipid species, this study fills a
critical gap in the understanding of early CVD pathogenesis. This integration
not only offers a novel perspective on microvascular metabolic associations but
also presents a significant opportunity for the identification of robust,
non-invasive biomarkers. Ultimately, these findings may support improved early
detection, targeted prevention, and personalized approaches in cardiovascular
healthcare.

</details>


### [23] [FORTRESS: Function-composition Optimized Real-Time Resilient Structural Segmentation via Kolmogorov-Arnold Enhanced Spatial Attention Networks](https://arxiv.org/abs/2507.12675)
*Christina Thrainer,Md Meftahul Ferdaus,Mahdi Abdelguerfi,Christian Guetl,Steven Sloan,Kendall N. Niles,Ken Pathak*

Main category: cs.CV

> FORTRESS, a new architecture for structural defect segmentation in civil infrastructure, balances accuracy and computational efficiency through depthwise separable convolutions and adaptive Kolmogorov-Arnold Network integration, achieving state-of-the-art performance with remarkable efficiency.

<details>
  <summary>Details</summary>

**Motivation:** The primary motivation is to tackle the challenge of achieving high accuracy in automated structural defect segmentation while ensuring computational efficiency for real-time deployment in civil infrastructure.

**Method:** The method, dubbed FORTRESS, employs a combination of depthwise separable convolutions and adaptive Kolmogorov-Arnold Network integration. It includes a systematic depthwise separable convolution framework that reduces parameters, adaptive TiKAN integration, and multi-scale attention fusion to enhance segmentation performance.

**Result:** FORTRESS achieves 91% parameter reduction, 91% computational complexity reduction, and a 3x improvement in inference speed. On benchmark datasets, it outperforms existing methods with an F1-score of 0.771 and a mean IoU of 0.677.

**Conclusion:** FORTRESS establishes itself as a robust solution for real-time and efficient structural defect segmentation in civil infrastructure, particularly in resource-constrained settings where both accuracy and computational efficiency are critical.

**Abstract:** Automated structural defect segmentation in civil infrastructure faces a
critical challenge: achieving high accuracy while maintaining computational
efficiency for real-time deployment. This paper presents FORTRESS
(Function-composition Optimized Real-Time Resilient Structural Segmentation), a
new architecture that balances accuracy and speed by using a special method
that combines depthwise separable convolutions with adaptive Kolmogorov-Arnold
Network integration. FORTRESS incorporates three key innovations: a systematic
depthwise separable convolution framework achieving a 3.6x parameter reduction
per layer, adaptive TiKAN integration that selectively applies function
composition transformations only when computationally beneficial, and
multi-scale attention fusion combining spatial, channel, and KAN-enhanced
features across decoder levels. The architecture achieves remarkable efficiency
gains with 91% parameter reduction (31M to 2.9M), 91% computational complexity
reduction (13.7 to 1.17 GFLOPs), and 3x inference speed improvement while
delivering superior segmentation performance. Evaluation on benchmark
infrastructure datasets demonstrates state-of-the-art results with an F1- score
of 0.771 and a mean IoU of 0.677, significantly outperforming existing methods
including U-Net, SA-UNet, and U- KAN. The dual optimization strategy proves
essential for optimal performance, establishing FORTRESS as a robust solution
for practical structural defect segmentation in resource-constrained
environments where both accuracy and computational efficiency are paramount.
Comprehensive architectural specifications are provided in the Supplemental
Material. Source code is available at URL:
https://github.com/faeyelab/fortress-paper-code.

</details>


### [24] [NeuraLeaf: Neural Parametric Leaf Models with Shape and Deformation Disentanglement](https://arxiv.org/abs/2507.12714)
*Yang Yang,Dongni Mao,Hiroaki Santo,Yasuyuki Matsushita,Fumio Okura*

Main category: cs.CV

> NeuraLeaf 是一种用于植物建模和重构的 3D 神经参数模型，解决了植物叶片形状多样和变形灵活所带来的独特挑战。

<details>
  <summary>Details</summary>

**Motivation:** 研究 3D 叶子模型对于农业和计算机图形学非常重要，但叶子的形状多样和容易变形给模型建立带来了挑战。

**Method:** Structure

**Result:** {"tldr": "NeuraLeaf 是一种用于植物建模和重构的 3D 神经参数模型，解决了植物叶片形状多样和变形灵活所带来的独特挑战。", "motivation": "研究 3D 叶子模型对于农业和计算机图形学非常重要，但叶子的形状多样和容易变形给模型建立带来了挑战。", "method": "NeuraLeaf 将叶子的几何结构分解为 2D 基础形状和 3D 变形。2D 基础形状可以从丰富的 2D 叶子图像数据集中进行学习，同时学习与几何结构对齐的纹理；3D 变形则通过一个新的无骨架蒙皮模型进行建模，并创建了一个名为 DeformLeaf 的 3D 叶子数据集。", "result": "NeuraLeaf 可以生成一系列具有变形的叶子形状，对 3D 观察结果比如深度图和点云具有精确的模型拟合能力。", "conclusion": "NeuraLeaf 是一种能成功用于植物建模和重构的有效 3D 神经参数模型。"}

**Conclusion:** NeuraLeaf 是一种能成功用于植物建模和重构的有效 3D 神经参数模型。

**Abstract:** We develop a neural parametric model for 3D leaves for plant modeling and
reconstruction that are essential for agriculture and computer graphics. While
neural parametric models are actively studied for humans and animals, plant
leaves present unique challenges due to their diverse shapes and flexible
deformation. To this problem, we introduce a neural parametric model for
leaves, NeuraLeaf. Capitalizing on the fact that flattened leaf shapes can be
approximated as a 2D plane, NeuraLeaf disentangles the leaves' geometry into
their 2D base shapes and 3D deformations. This representation allows learning
from rich sources of 2D leaf image datasets for the base shapes, and also has
the advantage of simultaneously learning textures aligned with the geometry. To
model the 3D deformation, we propose a novel skeleton-free skinning model and
create a newly captured 3D leaf dataset called DeformLeaf. We show that
NeuraLeaf successfully generates a wide range of leaf shapes with deformation,
resulting in accurate model fitting to 3D observations like depth maps and
point clouds. Our implementation and dataset are available at
https://neuraleaf-yang.github.io/.

</details>


### [25] [SOD-YOLO: Enhancing YOLO-Based Detection of Small Objects in UAV Imagery](https://arxiv.org/abs/2507.12727)
*Peijun Wang,Jinhua Zhao*

Main category: cs.CV

> 本研究介绍了SOD-YOLO，这是基于YOLOv8的一种改进的模型，用于提升小物体检测的性能。

<details>
  <summary>Details</summary>

**Motivation:** 小物体检测在目标检测领域是一个艰巨的挑战。本研究旨在通过提出一种增强的YOLOv8模型（SOD-YOLO）来处理这一难题。

**Method:** 本研究提出了一个基于YOLOv8增强的小物体检测模型SOD-YOLO。该模型在颈部集成了一个多尺度特征融合的ASF机制，添加了一个小型物体检测层P2以提供更高分辨率的特征图，并采用了Soft-NMS来优化置信得分从而提高真阳性保留。

**Result:** 实验结果显示，SOD-YOLO在VisDrone2019-DET数据集上的mAP$_{50:95}$和mAP$_{50}$分别比基准模型提高了36.1%和20.6%，显著提升了检测性能。

**Conclusion:** SOD-YOLO通过结合多尺度特征增强、精确的小尺度特征层改进以及精良的后处理实现了对于小型物体检测性能的有效提升，该项目适用于无人机场景下的图像分析。

**Abstract:** Small object detection remains a challenging problem in the field of object
detection. To address this challenge, we propose an enhanced YOLOv8-based
model, SOD-YOLO. This model integrates an ASF mechanism in the neck to enhance
multi-scale feature fusion, adds a Small Object Detection Layer (named P2) to
provide higher-resolution feature maps for better small object detection, and
employs Soft-NMS to refine confidence scores and retain true positives.
Experimental results demonstrate that SOD-YOLO significantly improves detection
performance, achieving a 36.1% increase in mAP$_{50:95}$ and 20.6% increase in
mAP$_{50}$ on the VisDrone2019-DET dataset compared to the baseline model.
These enhancements make SOD-YOLO a practical and efficient solution for small
object detection in UAV imagery. Our source code, hyper-parameters, and model
weights are available at https://github.com/iamwangxiaobai/SOD-YOLO.

</details>


### [26] [A Privacy-Preserving Semantic-Segmentation Method Using Domain-Adaptation Technique](https://arxiv.org/abs/2507.12730)
*Homare Sueyoshi,Kiyoshi Nishikawa,Hitoshi Kiya*

Main category: cs.CV

> 提出了一种隐私保护的语义分割方法，使用感知加密技术应用于模型训练和测试图像，该方法能保持与未加密模型几乎相同的准确率，通过ViT的嵌入结构调整域适应技术实现，实验验证了使用Segmentation Transformer时的分割准确性。

<details>
  <summary>Details</summary>

**Motivation:** 保护隐私的同时并不过分牺牲模型的准确率，在图像语义分割任务上维持高效性能。

**Method:** 利用Vision Transformer (ViT)的嵌入结构进行域适应，为训练和测试图像实施感知加密技术。

**Result:** 实验中，所提方法与未加密模型相比，保持了几乎相同的语义分割准确性。

**Conclusion:** 所提出的方法在保护图像隐私的同时，能够有效地维持高准确性的语义分割性能。

**Abstract:** We propose a privacy-preserving semantic-segmentation method for applying
perceptual encryption to images used for model training in addition to test
images. This method also provides almost the same accuracy as models without
any encryption. The above performance is achieved using a domain-adaptation
technique on the embedding structure of the Vision Transformer (ViT). The
effectiveness of the proposed method was experimentally confirmed in terms of
the accuracy of semantic segmentation when using a powerful
semantic-segmentation model with ViT called Segmentation Transformer.

</details>


### [27] [Transformer-based Spatial Grounding: A Comprehensive Survey](https://arxiv.org/abs/2507.12739)
*Ijazul Haq,Muhammad Saqib,Yingjie Zhang*

Main category: cs.CV

> 本文综述了2018年至2025年间基于变压器的空间定位方法，包括主导模型架构、常用数据集及评价指标，并为研究者提供了关键方法趋势和最佳实践的见解，以促进稳健可靠的空间定位模型的开发。

<details>
  <summary>Details</summary>

**Motivation:** 虽然基于变压器的空间定位方法在多模态表示和跨模态对齐方面取得了快速进展，但该领域缺乏对当前方法、数据集使用、评估度量和工业适用性的全面总结。本文旨在提供系统性文献综述。

**Method:** Structure

**Result:** {
  "tldr": "本文综述了2018年至2025年间基于变压器的空间定位方法，包括主导模型架构、常用数据集及评价指标，并为研究者提供了关键方法趋势和最佳实践的见解，以促进稳健可靠的空间定位模型的开发。", 
  "motivation": "虽然基于变压器的空间定位方法在多模态表示和跨模态对齐方面取得了快速进展，但该领域缺乏对当前方法、数据集使用、评估度量和工业适用性的全面总结。本文旨在提供系统性文献综述。", 
  "method": "本文进行了2018年至2025年间基于变压器的空间定位方法的系统性文献回顾，识别研究中的主导模型架构、数据集、评估度量并指出关键方法趋势和最佳实践。", 
  "result": "本研究确定了主导模型架构、常用数据集及评价指标，并强调了关键方法趋势和最佳实践。", 
  "conclusion": "本文分析为研究人员和从业者提供了关键的见解和结构化的指导，促进开发出更成熟、可靠和工业应用的空间定位模型。"}
}

**Conclusion:** 本文分析为研究人员和从业者提供了关键的见解和结构化的指导，促进开发出更成熟、可靠和工业应用的空间定位模型。

**Abstract:** Spatial grounding, the process of associating natural language expressions
with corresponding image regions, has rapidly advanced due to the introduction
of transformer-based models, significantly enhancing multimodal representation
and cross-modal alignment. Despite this progress, the field lacks a
comprehensive synthesis of current methodologies, dataset usage, evaluation
metrics, and industrial applicability. This paper presents a systematic
literature review of transformer-based spatial grounding approaches from 2018
to 2025. Our analysis identifies dominant model architectures, prevalent
datasets, and widely adopted evaluation metrics, alongside highlighting key
methodological trends and best practices. This study provides essential
insights and structured guidance for researchers and practitioners,
facilitating the development of robust, reliable, and industry-ready
transformer-based spatial grounding models.

</details>


### [28] [Domain-Enhanced Dual-Branch Model for Efficient and Interpretable Accident Anticipation](https://arxiv.org/abs/2507.12755)
*Yanchen Guan,Haicheng Liao,Chengyue Wang,Bonan Wang,Jiaxun Zhang,Jia Hu,Zhenning Li*

Main category: cs.CV

> 本文提出了一种新的交通事故预测方法，该方法集成视觉和文本数据，使用多模态模型并进行了有效的Prompt设计，显示出优于当前最佳性能的表现。

<details>
  <summary>Details</summary>

**Motivation:** 为了支持现代自动驾驶技术，本研究致力于开发一种精确且计算效率高的交通事故预测系统，以便在事故发生时能及时干预和减少损失。

**Method:** 本研究提出了一种基于双分支架构的事故预测框架，该框架能有效整合行车记录仪视频的视觉信息与交通事故报告的结构化文本数据。此外，我们还提出了一种特征聚合方法，通过大型模型（如GPT-4o和Long-CLIP）来实现多模态输入的无缝集成，并辅以针对性的Prompt工程技术，以生成可操作的反馈和标准化的事故档案。

**Result:** 我们在几个基准数据集（DAD、CCD 和 A3D）上的综合评估验证了该方法具有更高的预测准确性、改进的响应速度、更低的计算开销和更高的可解释性。

**Conclusion:** 该研究的方法在交通事故预测方面展示出的新基准性能证明了其在自动驾驶领域的应用潜力。

**Abstract:** Developing precise and computationally efficient traffic accident
anticipation system is crucial for contemporary autonomous driving
technologies, enabling timely intervention and loss prevention. In this paper,
we propose an accident anticipation framework employing a dual-branch
architecture that effectively integrates visual information from dashcam videos
with structured textual data derived from accident reports. Furthermore, we
introduce a feature aggregation method that facilitates seamless integration of
multimodal inputs through large models (GPT-4o, Long-CLIP), complemented by
targeted prompt engineering strategies to produce actionable feedback and
standardized accident archives. Comprehensive evaluations conducted on
benchmark datasets (DAD, CCD, and A3D) validate the superior predictive
accuracy, enhanced responsiveness, reduced computational overhead, and improved
interpretability of our approach, thus establishing a new benchmark for
state-of-the-art performance in traffic accident anticipation.

</details>
