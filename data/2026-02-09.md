<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 7]
- [cs.CV](#cs.CV) [Total: 3]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Recontextualizing Famous Quotes for Brand Slogan Generation](https://arxiv.org/abs/2602.06049)
*Ziao Yang,Zizhang Chen,Lei Zhang,Hongfu Liu*

Main category: cs.CL

> 本文研究了如何生成新颖、有创意且富含洞察力的标语。我们提出的方法利用名人名言，并将其重新定位，用于标语生成。实验表明这种方法可以生成更好的标语。

<details>
  <summary>Details</summary>

**Motivation:** 当前的方法在生成标语时，经常产生风格重复的标语，缺乏明确的品牌形象，并且看起来过于机器生成。因此，我们需要一个新的方式来平衡标语的新颖性和熟悉度。

**Method:** 我们提出了一种新的标语生成范式，利用名人名言进行标语生成，其中包含名言匹配、结构分解、词汇替换和混音生成等可解释子任务。

**Result:** 我们的方法在标语生成的多样化、新颖性、情感影响和人类偏好方面有了微小的提升。

**Conclusion:** 广泛的自动和人工评估表明，与三个最先进的大语言模型基线相比，我们的方法在多样性、新颖性、情感影响和人类偏好方面表现出微小的改进。

**Abstract:** Slogans are concise and memorable catchphrases that play a crucial role in advertising by conveying brand identity and shaping public perception. However, advertising fatigue reduces the effectiveness of repeated slogans, creating a growing demand for novel, creative, and insightful slogan generation. While recent work leverages large language models (LLMs) for this task, existing approaches often produce stylistically redundant outputs that lack a clear brand persona and appear overtly machine-generated. We argue that effective slogans should balance novelty with familiarity and propose a new paradigm that recontextualizes persona-related famous quotes for slogan generation. Well-known quotes naturally align with slogan-length text, employ rich rhetorical devices, and offer depth and insight, making them a powerful resource for creative generation. Technically, we introduce a modular framework that decomposes slogan generation into interpretable subtasks, including quote matching, structural decomposition, vocabulary replacement, and remix generation. Extensive automatic and human evaluations demonstrate marginal improvements in diversity, novelty, emotional impact, and human preference over three state-of-the-art LLM baselines.

</details>


### [2] [Relevance-aware Multi-context Contrastive Decoding for Retrieval-augmented Visual Question Answering](https://arxiv.org/abs/2602.06050)
*Jongha Kim,Byungoh Ko,Jeehye Na,Jinsung Yoon,Hyunwoo J. Kim*

Main category: cs.CL

> RMCD是一个新的解码方法，它可以改善RAG在LVLM中的表现。通过加权相关上下文，RMCD优化了信息的聚集并减少了无关上下文的负面影响，提升了性能。

<details>
  <summary>Details</summary>

**Motivation:** 尽管LVLM在各种任务上表现出色，但它们缺乏对特定实体的详细知识。为了改进这一点，研究者提出了RMCD来优化RAG的解码方法，使其能更有效地利用多个相关上下文并减少无关上下文的负面影响。

**Method:** RMCD是一种新颖的解码方法，它通过对每个上下文预测的输出进行加权，结合多个上下文的有用信息的同时也能抑制无关上下文的负面影响。

**Result:** 实验表明，RMCD在多个LVLM中以及三个知识密集型的视觉问答基准测试中均表现优异，优于其他解码方法。此外，RMCD对检索结果具有鲁棒性，在最弱和最强的检索结果下均表现出色。

**Conclusion:** 研究表明，RMCD不仅性能出众，而且易于实施，无需额外训练，只需替换LVLM的解码方法即可使用，扩展了LVLM在知识密集型视觉问答任务上的应用潜力。

**Abstract:** Despite the remarkable capabilities of Large Vision Language Models (LVLMs), they still lack detailed knowledge about specific entities. Retrieval-augmented Generation (RAG) is a widely adopted solution that enhances LVLMs by providing additional contexts from an external Knowledge Base. However, we observe that previous decoding methods for RAG are sub-optimal as they fail to sufficiently leverage multiple relevant contexts and suppress the negative effects of irrelevant contexts. To this end, we propose Relevance-aware Multi-context Contrastive Decoding (RMCD), a novel decoding method for RAG. RMCD outputs a final prediction by combining outputs predicted with each context, where each output is weighted based on its relevance to the question. By doing so, RMCD effectively aggregates useful information from multiple relevant contexts while also counteracting the negative effects of irrelevant ones. Experiments show that RMCD consistently outperforms other decoding methods across multiple LVLMs, achieving the best performance on three knowledge-intensive visual question-answering benchmarks. Also, RMCD can be simply applied by replacing the decoding method of LVLMs without additional training. Analyses also show that RMCD is robust to the retrieval results, consistently performing the best across the weakest to the strongest retrieval results. Code is available at https://github.com/mlvlab/RMCD.

</details>


### [3] [CAST: Character-and-Scene Episodic Memory for Agents](https://arxiv.org/abs/2602.06051)
*Kexin Ma,Bojun Li,Yuhua Tang,Ruochun Jin,Liting Sun*

Main category: cs.CL

> 论文提出了一种基于角色和场景的记忆架构（CAST），以解决代理记忆系统在表示和检索连贯事件上的困难，并展示了该方法在多个数据集上表现出色。

<details>
  <summary>Details</summary>

**Motivation:** 大多数代理记忆系统只强调语义回忆，将经验视为结构（如键值、向量或图），这使得它们在表示和检索连贯事件方面存在困难，因此提出了CAST方法。

**Method:** CAST方法结合了以角色和场景为基础的记忆架构，通过建立3D场景（时间/地点/主题）并将其组织成角色概况来表示情景记忆，并补充以基于图的语义记忆，构成了一种稳健的双记忆设计。

**Result:** 实验表明，CAST在各种数据集上平均提高了8.11%的F1分数和10.21%的J（LLM-as-a-Judge）分数，特别是在开放性和时间敏感性的对话问题上。

**Conclusion:** CAST作为一种结合情景记忆和语义记忆的设计，在处理各种任务时，尤其是在开放性和时间敏感性的对话问题上，表现出更高的准确性和更好的性能。

**Abstract:** Episodic memory is a central component of human memory, which refers to the ability to recall coherent events grounded in who, when, and where. However, most agent memory systems only emphasize semantic recall and treat experience as structures such as key-value, vector, or graph, which makes them struggle to represent and retrieve coherent events. To address this challenge, we propose a Character-and-Scene based memory architecture(CAST) inspired by dramatic theory. Specifically, CAST constructs 3D scenes (time/place/topic) and organizes them into character profiles that summarize the events of a character to represent episodic memory. Moreover, CAST complements this episodic memory with a graph-based semantic memory, which yields a robust dual memory design. Experiments demonstrate that CAST has averagely improved 8.11% F1 and 10.21% J(LLM-as-a-Judge) than baselines on various datasets, especially on open and time-sensitive conversational questions.

</details>


### [4] [Rethinking Memory Mechanisms of Foundation Agents in the Second Half](https://arxiv.org/abs/2602.06052)
*Wei-Chieh Huang,Weizhi Zhang,Yueqing Liang,Yuanchen Bei,Yankai Chen,Tao Feng,Xinyu Pan,Zhen Tan,Yu Wang,Tianxin Wei,Shanglin Wu,Ruiyao Xu,Liangwei Yang,Rui Yang,Wooseong Yang,Chin-Yuan Yeh,Hanrong Zhang,Haozhen Zhang,Siqi Zhu,Henry Peng Zou,Wanjia Zhao,Song Wang,Wujiang Xu,Zixuan Ke,Zheng Hui,Dawei Li,Yaozu Wu,Langzhou He,Chen Wang,Xiongxiao Xu,Baixiang Huang,Juntao Tan,Shelby Heinecke,Huan Wang,Caiming Xiong,Ahmed A. Metwally,Jun Yan,Chen-Yu Lee,Hanqing Zeng,Yinglong Xia,Xiaokai Wei,Ali Payani,Yu Wang,Haitong Ma,Wenya Wang,Chengguang Wang,Yu Zhang,Xin Wang,Yongfeng Zhang,Jiaxuan You,Hanghang Tong,Xiao Luo,Yizhou Sun,Wei Wang,Julian McAuley,James Zou,Jiawei Han,Philip S. Yu,Kai Shu*

Main category: cs.CL

> 本文分析了智能体记忆的三个维度，提出记忆作为长期、动态和用户依赖环境中的关键解决方法，并回顾了评估记忆效用的基准和未来挑战。

<details>
  <summary>Details</summary>

**Motivation:** 随着人工智能研究范式的转变，研究焦点从模型创新转向严格的实际应用评估。在这种背景下，文章旨在解决智能体在长期、动态和用户依赖环境中的实用性问题。

**Method:** 本文通过三个维度（记忆基质、认知机制、记忆主体）对基础智能体的记忆进行了统一的分析，并研究了在不同的智能体拓扑结构下记忆是如何被实例化和操作的。此外，文章还回顾了评估记忆效用的基准测试和指标，并概述了各种开放性的挑战和未来研究方向。

**Result:** 文章提出了记忆作为填补智能体实用性和满足长期、动态和用户依赖环境的关键解决方案，并且提供了关于记忆基质、认知机制和记忆主体的全面视角。

**Conclusion:** 本文强调了记忆在实现智能体实用价值中的核心作用，并指出了未来研究的方向和开放性挑战。

**Abstract:** The research of artificial intelligence is undergoing a paradigm shift from prioritizing model innovations over benchmark scores towards emphasizing problem definition and rigorous real-world evaluation. As the field enters the "second half," the central challenge becomes real utility in long-horizon, dynamic, and user-dependent environments, where agents face context explosion and must continuously accumulate, manage, and selectively reuse large volumes of information across extended interactions. Memory, with hundreds of papers released this year, therefore emerges as the critical solution to fill the utility gap. In this survey, we provide a unified view of foundation agent memory along three dimensions: memory substrate (internal and external), cognitive mechanism (episodic, semantic, sensory, working, and procedural), and memory subject (agent- and user-centric). We then analyze how memory is instantiated and operated under different agent topologies and highlight learning policies over memory operations. Finally, we review evaluation benchmarks and metrics for assessing memory utility, and outline various open challenges and future directions.

</details>


### [5] [PersonaPlex: Voice and Role Control for Full Duplex Conversational Speech Models](https://arxiv.org/abs/2602.06053)
*Rajarshi Roy,Jonathan Raiman,Sang-gil Lee,Teodor-Dumitru Ene,Robert Kirby,Sungwon Kim,Jaehyeon Kim,Bryan Catanzaro*

Main category: cs.CL

> 研究介绍了一种新型双工对话语音模型PersonaPlex，通过结合角色条件和语音克隆技术，实现了更强的角色遵从性、语音相似性和自然对话反应，超越了现有模型。

<details>
  <summary>Details</summary>

**Motivation:** 当前的双工语音模型受限于固定角色和声音，难以满足结构化角色驱动的现实应用和个人化交互需求。该研究旨在解决这一限制。

**Method:** 该研究介绍了PersonaPlex，一种双工对话语音模型，结合了角色引导的系统提示和语音克隆技术。PersonaPlex使用基于开源大型语言模型（LLM）和文本到语音（TTS）模型生成的合成数据进行训练。

**Result:** 实验结果表明，PersonaPlex在角色条件行为、语音条件语音、自然对话反应、角色遵从性、说话者相似性、延迟和自然度方面均取得优异表现，超过了现有的双工语音模型和大型语言模型驱动的语音系统。

**Conclusion:** 该研究展示了PersonaPlex作为一种能够处理多角色服务场景的创新对话语音模型的潜力，为未来的研究和发展打下了坚实的基础。

**Abstract:** Recent advances in duplex speech models have enabled natural, low-latency speech-to-speech interactions. However, existing models are restricted to a fixed role and voice, limiting their ability to support structured, role-driven real-world applications and personalized interactions. In this work, we introduce PersonaPlex, a duplex conversational speech model that incorporates hybrid system prompts, combining role conditioning with text prompts and voice cloning with speech samples. PersonaPlex is trained on a large-scale synthetic dataset of paired prompts and user-agent conversations, generated with open-source large language models (LLM) and text-to-speech (TTS) models. To evaluate role conditioning in real-world settings, we extend the Full-Duplex-Bench benchmark beyond a single assistant role to multi-role customer service scenarios. Experiments show that PersonaPlex achieves strong role-conditioned behavior, voice-conditioned speech, and natural conversational responsiveness, surpassing state-of-the-art duplex speech models and hybrid large language model-based speech systems in role adherence, speaker similarity, latency, and naturalness.

</details>


### [6] [What Is Novel? A Knowledge-Driven Framework for Bias-Aware Literature Originality Evaluation](https://arxiv.org/abs/2602.06054)
*Abeer Mostafa,Thi Huyen Nguyen,Zahra Ahmadi*

Main category: cs.CL

> 本文提出了一种基于文献的新颖性评估框架，采用大约8万个顶级AI会议上的新颖性注释评审来微调大型语言模型，改进了研究新颖性评估的客观性和一致性。

<details>
  <summary>Details</summary>

**Motivation:** 评估研究的创新性是同行评审的核心但高度主观方面，通常基于隐含的判断和对先前工作的不完整比较。

**Method:** 提出了一种基于文献的创新评估框架，该框架从同行评审报告中显式学习人类如何判断创新，并将这些判断与现有研究进行结构化比较。

**Result:** 该系统能够提取给定手稿的思想、方法和主张的结构化表示，检索语义相关的论文，并构建相似性图以实现对先前工作的细粒度概念级比较。模型产生校准后的创新分数和类似人类的解释性评估，减少高估并提升一致性。

**Conclusion:** 这一方法能够更好地量化和客观评估研究的创新性，改进现有方法的评估结果一致性和准确性。

**Abstract:** Assessing research novelty is a core yet highly subjective aspect of peer review, typically based on implicit judgment and incomplete comparison to prior work. We introduce a literature-aware novelty assessment framework that explicitly learns how humans judge novelty from peer-review reports and grounds these judgments in structured comparison to existing research. Using nearly 80K novelty-annotated reviews from top-tier AI conferences, we fine-tune a large language model to capture reviewer-aligned novelty evaluation behavior. For a given manuscript, the system extracts structured representations of its ideas, methods, and claims, retrieves semantically related papers, and constructs a similarity graph that enables fine-grained, concept-level comparison to prior work. Conditioning on this structured evidence, the model produces calibrated novelty scores and human-like explanatory assessments, reducing overestimation and improving consistency relative to existing approaches.

</details>


### [7] [Quantifying and Attributing Polarization to Annotator Groups](https://arxiv.org/abs/2602.06055)
*Dimitris Tsirmpas,John Pavlopoulos*

Main category: cs.CL

> 提出了新的注解一致度量及统计验证方法，发现仇恨言论任务中标注员群体表现出明显的极化现象，特别在种族差异上。研发了一个开源Python库，评估了出稳健注解结果所需最低人数。

<details>
  <summary>Details</summary>

**Motivation:** 现存的标注协议度量不适合进行组间分析，对组规模的不平衡敏感，并且局限于单标签设置。这些限制使得它们无法充分应对诸如毒性检测和仇恨言论检测等主观任务。

**Method:** 提出了一种可量化的指标，搭配统计显著性测试，用于归因不同标注员群体的极化现象。该指标支持跨不同数据集和任务的重型不平衡社会人口统计和意识形态子群之间的直接比较，同时允许在多标签设置下进行分析。

**Result:** 将该指标应用于三个仇恨言论数据集和一个毒性检测数据集，发现：(1) 标注员的种族在仇恨言论任务中表现出强烈的并且持续的极化。(2) 宗教信仰标注员之间意见基本一致，但与其他群体存在分歧，这一趋势随着无宗教信仰者的增加而逐渐消失，并最终逆转。(3) 教育程度较低的标注员更主观，而受教育程度较高的标注员在彼此之间更倾向于广泛同意。

**Conclusion:** 总体来说，研究结果反映了当前针对各种次群体在标注模式上的发现。最后，估算了获得稳健结果所需的最小标注员数量，并提供了一个开源Python库来实现这一指标。

**Abstract:** Current annotation agreement metrics are not well-suited for inter-group analysis, are sensitive to group size imbalances and restricted to single-annotation settings. These restrictions render them insufficient for many subjective tasks such as toxicity and hate-speech detection. For this reason, we introduce a quantifiable metric, paired with a statistical significance test, that attributes polarization to various annotator groups. Our metric enables direct comparisons between heavily imbalanced sociodemographic and ideological subgroups across different datasets and tasks, while also enabling analysis on multi-label settings. We apply this metric to three datasets on hate speech, and one on toxicity detection, discovering that: (1) Polarization is strongly and persistently attributed to annotator race, especially on the hate speech task. (2) Religious annotators do not fundamentally disagree with each other, but do with other annotators, a trend that is gradually diminished and then reversed with irreligious annotators. (3) Less educated annotators are more subjective, while educated ones tend to broadly agree more between themselves. Overall, our results reflect current findings around annotation patterns for various subgroups. Finally, we estimate the minimum number of annotators needed to obtain robust results, and provide an open-source Python library that implements our metric.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [8] [From Blurry to Believable: Enhancing Low-quality Talking Heads with 3D Generative Priors](https://arxiv.org/abs/2602.06122)
*Ding-Jiun Huang,Yuanhao Wang,Shao-Ji Yuan,Albert Mosella-Montoro,Francisco Vicente Carrasco,Cheng Zhang,Fernando De la Torre*

Main category: cs.CV

> SuperHead is a novel framework that enhances low-resolution 3D head avatars by synthesizing high-quality geometry and textures, ensuring 3D and temporal consistency, and preserving identity. It outperforms baseline methods in visual quality.

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to create high-fidelity, animatable 3D talking heads from low-quality image or video sources, overcoming the challenge of synthesizing high-quality geometry and textures while maintaining 3D and temporal consistency during animation and preserving the subject's identity.

**Method:** The method introduces SuperHead, a framework that enhances low-resolution 3D head avatars through a novel dynamics-aware 3D inversion scheme that leverages pre-trained 3D generative models to produce a high-resolution 3D Gaussian Splatting (3DGS) head model, which is then rigged to a parametric head model for animation.

**Result:** The result of the paper demonstrates that the SuperHead framework generates avatars with fine-grained facial details under dynamic motions, significantly outperforming baseline methods in terms of visual quality.

**Conclusion:** The conclusion is that SuperHead provides a robust solution for generating high-fidelity, animatable 3D head avatars from low-resolution inputs, supported by the use of 3D generative models and a dynamics-aware inversion scheme.

**Abstract:** Creating high-fidelity, animatable 3D talking heads is crucial for immersive applications, yet often hindered by the prevalence of low-quality image or video sources, which yield poor 3D reconstructions. In this paper, we introduce SuperHead, a novel framework for enhancing low-resolution, animatable 3D head avatars. The core challenge lies in synthesizing high-quality geometry and textures, while ensuring both 3D and temporal consistency during animation and preserving subject identity. Despite recent progress in image, video and 3D-based super-resolution (SR), existing SR techniques are ill-equipped to handle dynamic 3D inputs. To address this, SuperHead leverages the rich priors from pre-trained 3D generative models via a novel dynamics-aware 3D inversion scheme. This process optimizes the latent representation of the generative model to produce a super-resolved 3D Gaussian Splatting (3DGS) head model, which is subsequently rigged to an underlying parametric head model (e.g., FLAME) for animation. The inversion is jointly supervised using a sparse collection of upscaled 2D face renderings and corresponding depth maps, captured from diverse facial expressions and camera viewpoints, to ensure realism under dynamic facial motions. Experiments demonstrate that SuperHead generates avatars with fine-grained facial details under dynamic motions, significantly outperforming baseline methods in visual quality.

</details>


### [9] [EgoAVU: Egocentric Audio-Visual Understanding](https://arxiv.org/abs/2602.06139)
*Ashish Seth,Xinhao Mei,Changsheng Zhao,Varun Nagaraja,Ernie Chang,Gregory P. Meyer,Gael Le Lan,Yunyang Xiong,Vikas Chandra,Yangyang Shi,Dinesh Manocha,Zhipeng Cai*

Main category: cs.CV

> 论文介绍了EgoAVU，一个用于生成以自我为中心视频的音视频内容的数据引擎。研究表明现有MLLMs偏向于视觉信号，通过微调在EgoAVU-Instruct上可以显著提高MLLMs对于音视频联合理解的能力。

<details>
  <summary>Details</summary>

**Motivation:** 解决由于难以获取具有连贯多模态信息的文本标签，多模态大型语言模型 (MLLMs) 在理解和处理以自我为中心的视频中两种模态的问题。

**Method:** 引入EgoAVU，这是一个可扩展的数据引擎，可以自动生成以自我为中心的音视频叙述、问题和答案。EgoAVU通过跨模态相关性建模来丰富人类叙述的多模态上下文，并生成音视频叙述。基于标记的视频过滤和模块化、基于图的策划确保了数据的多样性和质量。

**Result:** EgoAVU-Bench显示了现有MLLMs的局限性，它们严重偏向于视觉信号，往往忽视音频线索或无法将音频与视觉来源对应。在EgoAVU-Instruct上对MLLMs进行微调可以显著提高性能，提升幅度最高达113%，并可迁移到其他基准测试，性能提升最高可达28%。

**Conclusion:** 通过在EgoAVU-Instruct上对MLLMs进行微调，可以有效缓解现有MLLMs对视觉信号依赖过重，忽视音频线索或无法将音频与视觉来源对应的问题，使得在EgoAVU-Bench上的性能最高提升113%，也使得在其他诸如EgoTempo和EgoIllusion等基准测试上的相对性能提升达到28%。

**Abstract:** Understanding egocentric videos plays a vital role for embodied intelligence. Recent multi-modal large language models (MLLMs) can accept both visual and audio inputs. However, due to the challenge of obtaining text labels with coherent joint-modality information, whether MLLMs can jointly understand both modalities in egocentric videos remains under-explored. To address this problem, we introduce EgoAVU, a scalable data engine to automatically generate egocentric audio-visual narrations, questions, and answers. EgoAVU enriches human narrations with multimodal context and generates audio-visual narrations through cross-modal correlation modeling. Token-based video filtering and modular, graph-based curation ensure both data diversity and quality. Leveraging EgoAVU, we construct EgoAVU-Instruct, a large-scale training dataset of 3M samples, and EgoAVU-Bench, a manually verified evaluation split covering diverse tasks. EgoAVU-Bench clearly reveals the limitations of existing MLLMs: they bias heavily toward visual signals, often neglecting audio cues or failing to correspond audio with the visual source. Finetuning MLLMs on EgoAVU-Instruct effectively addresses this issue, enabling up to 113% performance improvement on EgoAVU-Bench. Such benefits also transfer to other benchmarks such as EgoTempo and EgoIllusion, achieving up to 28% relative performance gain. Code will be released to the community.

</details>


### [10] [MGP-KAD: Multimodal Geometric Priors and Kolmogorov-Arnold Decoder for Single-View 3D Reconstruction in Complex Scenes](https://arxiv.org/abs/2602.06158)
*Luoxi Zhang,Chun Xie,Itaru Kitahara*

Main category: cs.CV

> MGP-KAD, 一种新颖的多模态特性融合框架，通过整合RGB和几何先验来提升单视角3D重建精度，在复杂的现实场景中达到了最先进的性能。

<details>
  <summary>Details</summary>

**Motivation:** 提升单视角在复杂真实场景下的3D重建精度，解决噪声、对象多样性以及数据集不足的问题。

**Method:** 提出MGP-KAD框架。该框架通过把RGB和几何先验融合，并且引入了基于KAN的混合解码器来处理复杂的多模态输入，解决了传统的线性解码器的限制。

**Result:** {
  "tldr": "MGP-KAD, 一种新颖的多模态特性融合框架，通过整合RGB和几何先验来提升单视角3D重建精度，在复杂的现实场景中达到了最先进的性能。",
  "motivation": "提升单视角在复杂真实场景下的3D重建精度，解决噪声、对象多样性以及数据集不足的问题。",
  "method": "提出MGP-KAD框架。该框架通过把RGB和几何先验融合，并且引入了基于KAN的混合解码器来处理复杂的多模态输入，解决了传统的线性解码器的限制。",
  "result": "在Pix3D数据集上的大量实验表明，MGP-KAD框架实现了最先进的性能，显著提高了几何完整性、平滑度和细节保留度。",
  "conclusion": "MGP-KAD框架提供了一种稳健且有效的解决方案，进一步推动了复杂场景中的单视角3D重建技术。回答完毕。若无其他询问，则等待进一步指令。请知悉。该结论概述了研究的主要贡献，适合学术交流和文献引用。",
  "name": "Structure",
  "arguments": "{...}同上述字段值..."}

**Conclusion:** MGP-KAD框架提供了一种稳健且有效的解决方案，进一步推动了复杂场景中的单视角3D重建技术。

**Abstract:** Single-view 3D reconstruction in complex real-world scenes is challenging due to noise, object diversity, and limited dataset availability. To address these challenges, we propose MGP-KAD, a novel multimodal feature fusion framework that integrates RGB and geometric prior to enhance reconstruction accuracy. The geometric prior is generated by sampling and clustering ground-truth object data, producing class-level features that dynamically adjust during training to improve geometric understanding. Additionally, we introduce a hybrid decoder based on Kolmogorov-Arnold Networks (KAN) to overcome the limitations of traditional linear decoders in processing complex multimodal inputs. Extensive experiments on the Pix3D dataset demonstrate that MGP-KAD achieves state-of-the-art (SOTA) performance, significantly improving geometric integrity, smoothness, and detail preservation. Our work provides a robust and effective solution for advancing single-view 3D reconstruction in complex scenes.

</details>
