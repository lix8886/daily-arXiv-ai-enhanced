{"id": "2506.19939", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.19939", "abs": "https://arxiv.org/abs/2506.19939", "authors": ["Aryan Singh Dalal", "Sidharth Rai", "Rahul Singh", "Treman Singh Kaloya", "Rahul Harsha Cheppally", "Ajay Sharda"], "title": "Computer Vision based Automated Quantification of Agricultural Sprayers Boom Displacement", "comment": "Under publication process for COMPAG", "summary": "Application rate errors when using self-propelled agricultural sprayers for\nagricultural production remain a concern. Among other factors, spray boom\ninstability is one of the major contributors to application errors. Spray\nbooms' width of 38m, combined with 30 kph driving speeds, varying terrain, and\nmachine dynamics when maneuvering complex field boundaries, make controls of\nthese booms very complex. However, there is no quantitative knowledge on the\nextent of boom movement to systematically develop a solution that might include\nboom designs and responsive boom control systems. Therefore, this study was\nconducted to develop an automated computer vision system to quantify the boom\nmovement of various agricultural sprayers. A computer vision system was\ndeveloped to track a target on the edge of the sprayer boom in real time. YOLO\nV7, V8, and V11 neural network models were trained to track the boom's\nmovements in field operations to quantify effective displacement in the\nvertical and transverse directions. An inclinometer sensor was mounted on the\nboom to capture boom angles and validate the neural network model output. The\nresults showed that the model could detect the target with more than 90 percent\naccuracy, and distance estimates of the target on the boom were within 0.026 m\nof the inclinometer sensor data. This system can quantify the boom movement on\nthe current sprayer and potentially on any other sprayer with minor\nmodifications. The data can be used to make design improvements to make sprayer\nbooms more stable and achieve greater application accuracy.", "AI": {"tldr": "本研究开发了一种计算机视觉系统来量化农业喷洒机喷洒臂的移动，以高精度检测和估算目标位置，并可用于改进喷洒臂的设计，提高应用精度。", "motivation": "存在使用自走式农业喷洒机进行农业作业时喷洒臂不稳定的现象，没有定量知识来制定解决喷洒臂移动问题的策略，因此本研究旨在开发一种量化喷洒臂移动的计算机视觉系统。", "method": "开发了一套自动化的计算机视觉系统来实时追踪喷洒器边缘的目标，使用了YOLO V7、V8 和 V11 神经网络模型来在田间操作中量化垂直和横向的有效位移，并利用安装在喷洒臂上的倾角传感器来捕捉喷洒臂的角度并验证神经网络模型的输出结果。", "result": "结果表明模型可以以超过90%的准确率识别目标，并且对喷洒臂上目标的距离估算值与倾角传感器数据相差0.026米以内。", "conclusion": "该系统能够量化目前喷洒器上的喷洒臂移动，并且在进行少量的修改后，可能适用于任何其他喷洒器。该系统收集的数据可以帮助进行设计改进以提高喷洒器的稳定性，并实现更高的应用精度。"}}
{"id": "2506.19955", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.19955", "abs": "https://arxiv.org/abs/2506.19955", "authors": ["Yiming Ma", "Victor Sanchez", "Tanaya Guha"], "title": "EBC-ZIP: Improving Blockwise Crowd Counting with Zero-Inflated Poisson Regression", "comment": null, "summary": "Density map estimation has become the mainstream paradigm in crowd counting.\nHowever, most existing methods overlook the extreme sparsity of ground-truth\ndensity maps. In real-world crowd scenes, the vast majority of spatial regions\n(often over 95%) contain no people, leading to heavily imbalanced count\ndistributions. Ignoring this imbalance can bias models toward overestimating\ndense regions and underperforming in sparse areas. Furthermore, most loss\nfunctions used in density estimation are majorly based on MSE and implicitly\nassume Gaussian distributions, which are ill-suited for modeling discrete,\nnon-negative count data. In this paper, we propose EBC-ZIP, a crowd counting\nframework that models the spatial distribution of counts using a Zero-Inflated\nPoisson (ZIP) regression formulation. Our approach replaces the traditional\nregression loss with the negative log-likelihood of the ZIP distribution,\nenabling better handling of zero-heavy distributions while preserving count\naccuracy. Built upon the recently proposed Enhanced Block Classification (EBC)\nframework, EBC-ZIP inherits EBC's advantages in preserving the discreteness of\ntargets and ensuring training stability, while further improving performance\nthrough a more principled probabilistic loss. We also evaluate EBC-ZIP with\nbackbones of varying computational complexity to assess its scalability.\nExtensive experiments on four crowd counting benchmarks demonstrate that\nEBC-ZIP consistently outperforms EBC and achieves state-of-the-art results.", "AI": {"tldr": "本论文提出了一种名为EBC-ZIP的群计数框架，该框架利用零膨胀泊松（ZIP）回归来处理计数数据的空间分布，从而更好地应对密集和稀疏区域的不平衡问题，并实现了更优的性能。", "motivation": "现有的群计数方法往往忽视了真实世界场景中人群分布的高度不平衡问题，导致模型在稀疏区域表现不佳。同时，多数方法依赖于假设高斯分布的MSE损失函数，这并不适合离散、非负的计数数据。", "method": "论文提出的方法EBC-ZIP替代了传统的回归损失函数，采用了ZIP分布的负对数似然函数作为损失函数，以更好地处理包含大量零值的数据。方法构建在增强块分类（EBC）框架之上，不仅能更好地保持目标的离散性并且确保了训练的稳定性。", "result": "实验结果显示EBC-ZIP在四种群计数基准数据集上表现优于EBC，并达到了当前最优结果。", "conclusion": "通过采用ZIP回归模型，EBC-ZIP成功地处理了不平衡和离散问题，在群计数任务中表现出了优越的性能。"}}
{"id": "2506.20066", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.20066", "abs": "https://arxiv.org/abs/2506.20066", "authors": ["Hsiang-Wei Huang", "Wenhao Chai", "Kuang-Ming Chen", "Cheng-Yen Yang", "Jenq-Neng Hwang"], "title": "ToSA: Token Merging with Spatial Awareness", "comment": "Accepted by IROS 2025", "summary": "Token merging has emerged as an effective strategy to accelerate Vision\nTransformers (ViT) by reducing computational costs. However, existing methods\nprimarily rely on the visual token's feature similarity for token merging,\noverlooking the potential of integrating spatial information, which can serve\nas a reliable criterion for token merging in the early layers of ViT, where the\nvisual tokens only possess weak visual information. In this paper, we propose\nToSA, a novel token merging method that combines both semantic and spatial\nawareness to guide the token merging process. ToSA leverages the depth image as\ninput to generate pseudo spatial tokens, which serve as auxiliary spatial\ninformation for the visual token merging process. With the introduced spatial\nawareness, ToSA achieves a more informed merging strategy that better preserves\ncritical scene structure. Experimental results demonstrate that ToSA\noutperforms previous token merging methods across multiple benchmarks on visual\nand embodied question answering while largely reducing the runtime of the ViT,\nmaking it an efficient solution for ViT acceleration. The code will be\navailable at: https://github.com/hsiangwei0903/ToSA", "AI": {"tldr": "本文介绍了新型令牌合并方法ToSA，该方法通过引入深度图像生成伪空间令牌来提高令牌合并的有效性，并在多个基准测试中表现优异，显著减少ViT的运行时间。", "motivation": "尽管现有的令牌合并方法依赖视觉令牌的特征相似性来降低计算成本，但这些方法忽视了整合空间信息在早期层中作为令牌合并可靠标准的潜力。", "method": "通过引入深度图像作为输入生成伪空间令牌，ToSA结合了空间意识与语义意识来指导令牌合并过程。", "result": "本文提出了一种结合语义和空间意识的新颖令牌合并方法ToSA，该方法使用深度图像作为输入生成伪空间令牌，从而辅助视觉令牌合并过程，保留关键场景结构，提高合并策略的有效性。实验结果显示，ToSA在多个视觉和体感问题解答基准测试中优于先前的令牌合并方法，并显著减少了ViT的运行时间。", "conclusion": "ToSA在提高合并策略的有效性同时，保留了关键的场景结构，并在多个视觉和体感问题解答基准测试中优于先前的方法，证明了其作为一种ViT加速的高效解决方案的潜力。"}}
{"id": "2506.20103", "categories": ["cs.CV", "cs.AI", "I.4"], "pdf": "https://arxiv.org/pdf/2506.20103", "abs": "https://arxiv.org/abs/2506.20103", "authors": ["Jiahao Lin", "Weixuan Peng", "Bojia Zi", "Yifeng Gao", "Xianbiao Qi", "Xingjun Ma", "Yu-Gang Jiang"], "title": "BrokenVideos: A Benchmark Dataset for Fine-Grained Artifact Localization in AI-Generated Videos", "comment": "7 page,4 figures,2 tables", "summary": "Recent advances in deep generative models have led to significant progress in\nvideo generation, yet the fidelity of AI-generated videos remains limited.\nSynthesized content often exhibits visual artifacts such as temporally\ninconsistent motion, physically implausible trajectories, unnatural object\ndeformations, and local blurring that undermine realism and user trust.\nAccurate detection and spatial localization of these artifacts are crucial for\nboth automated quality control and for guiding the development of improved\ngenerative models. However, the research community currently lacks a\ncomprehensive benchmark specifically designed for artifact localization in AI\ngenerated videos. Existing datasets either restrict themselves to video or\nframe level detection or lack the fine-grained spatial annotations necessary\nfor evaluating localization methods. To address this gap, we introduce\nBrokenVideos, a benchmark dataset of 3,254 AI-generated videos with\nmeticulously annotated, pixel-level masks highlighting regions of visual\ncorruption. Each annotation is validated through detailed human inspection to\nensure high quality ground truth. Our experiments show that training state of\nthe art artifact detection models and multi modal large language models (MLLMs)\non BrokenVideos significantly improves their ability to localize corrupted\nregions. Through extensive evaluation, we demonstrate that BrokenVideos\nestablishes a critical foundation for benchmarking and advancing research on\nartifact localization in generative video models. The dataset is available at:\nhttps://broken-video-detection-datetsets.github.io/Broken-Video-Detection-Datasets.github.io/.", "AI": {"tldr": "论文介绍了一个新的名为BrokenVideos的数据集，用于提高对AI生成视频中视觉缺陷定位的检测性能。", "motivation": "现有的数据集仅限于视频或帧级别的检测，或者缺乏评估定位方法所需的精细的空间注释。该数据集的引入是为了填补在人工智能生成视频中缺陷定位基准评价的空白。", "method": "该论文引入了一个名为BrokenVideos的新基准数据集，包含3,254个人工智能生成的视频，并且每个视频都有细致的人工审核标注的像素级遮罩，用于高精度的评价视觉缺陷定位方法。", "result": "实验表明，在BrokenVideos数据集上训练先进的人工智能缺陷检测模型和多模态大型语言模型，能够显著提高它们在定位受损区域方面的性能。", "conclusion": "BrokenVideos数据集为评估和改进生成视频模型中的缺陷定位研究提供了一个关键的基础。"}}
{"id": "2506.19952", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.19952", "abs": "https://arxiv.org/abs/2506.19952", "authors": ["Deepon Halder", "Thanmay Jayakumar", "Raj Dabre"], "title": "CycleDistill: Bootstrapping Machine Translation using LLMs with Cyclical Distillation", "comment": null, "summary": "Large language models (LLMs), despite their ability to perform few-shot\nmachine translation (MT), often lag behind dedicated MT systems trained on\nparallel corpora, which are crucial for high quality machine translation (MT).\nHowever, parallel corpora are often scarce or non-existent for low-resource\nlanguages. In this paper, we propose CycleDistill, a bootstrapping approach\nleveraging LLMs and few-shot translation to obtain high-quality MT systems.\nCycleDistill involves iteratively generating synthetic parallel corpora from\nmonolingual corpora via zero- or few-shot MT, which is then used to fine-tune\nthe model that was used for generating said data for MT. CycleDistill does not\nneed parallel corpora beyond 1 to 4 few-shot examples, and in our experiments\nfocusing on three Indian languages, by relying solely on monolingual corpora,\nit can achieve high-quality machine translation, improving upon a few-shot\nbaseline model by over 20-30 chrF points on average in the first iteration. We\nalso study the effect of leveraging softmax activations during the distillation\nprocess and observe mild improvements in translation quality.", "AI": {"tldr": "CycleDistill is an iterative approach that creates synthetic parallel corpora from monolingual data to improve machine translation quality, especially useful for low-resource languages.", "motivation": "To overcome the limitations of LLMs in few-shot machine translation and the scarcity of parallel corpora for low-resource languages.", "method": "CycleDistill, a bootstrapping method that uses LLMs and few-shot translation to create synthetic parallel corpora from monolingual data for fine-tuning.", "result": "In experiments with three Indian languages, CycleDistill improved upon a few-shot baseline model by over 20-30 chrF points in the first iteration without needing extensive parallel corpora.", "conclusion": "CycleDistill can generate high-quality MT systems using only monolingual corpora and a few initial examples, making it effective for low-resource languages."}}
{"id": "2506.20134", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.20134", "abs": "https://arxiv.org/abs/2506.20134", "authors": ["Ningwei Xie", "Zizi Tian", "Lei Yang", "Xiao-Ping Zhang", "Meng Guo", "Jie Li"], "title": "From 2D to 3D Cognition: A Brief Survey of General World Models", "comment": null, "summary": "World models have garnered increasing attention in the development of\nartificial general intelligence (AGI), serving as computational frameworks for\nlearning representations of the external world and forecasting future states.\nWhile early efforts focused on 2D visual perception and simulation, recent\n3D-aware generative world models have demonstrated the ability to synthesize\ngeometrically consistent, interactive 3D environments, marking a shift toward\n3D spatial cognition. Despite rapid progress, the field lacks systematic\nanalysis to categorize emerging techniques and clarify their roles in advancing\n3D cognitive world models. This survey addresses this need by introducing a\nconceptual framework, providing a structured and forward-looking review of\nworld models transitioning from 2D perception to 3D cognition. Within this\nframework, we highlight two key technological drivers, particularly advances in\n3D representations and the incorporation of world knowledge, as fundamental\npillars. Building on these, we dissect three core cognitive capabilities that\nunderpin 3D world modeling: 3D physical scene generation, 3D spatial reasoning,\nand 3D spatial interaction. We further examine the deployment of these\ncapabilities in real-world applications, including embodied AI, autonomous\ndriving, digital twin, and gaming/VR. Finally, we identify challenges across\ndata, modeling, and deployment, and outline future directions for advancing\nmore robust and generalizable 3D world models.", "AI": {"tldr": "本文综述了从2D向3D世界模型转变，并分析了3D表示和世界知识在支持更强大的认知能力中的作用。", "motivation": "本文旨在填补领域内对新兴技术的系统分析空白，明确这些技术在推进3D认知世界模型发展中的作用。", "method": "本文采用了概念框架的方法，对从2D感知到3D认知的世界模型转变进行了有结构和前瞻性的回顾。", "result": "通过概念框架，本文辨别并分析了两大技术推动因素以及三大核心认知能力，并探讨了这些能力在实际应用中的情况。", "conclusion": "本文指出3D表示法的进步和世界知识的融合是关键的技术推动力，并剖析了支撑3D世界建模的三大核心认知能力：3D物理场景生成、3D空间推理和3D空间交互。另外，还探讨了这些能力在实际应用中的部署，并指出了在数据、建模和部署方面的挑战，以及未来提高更强大和通用的3D世界模型的方向。"}}
{"id": "2506.19967", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.19967", "abs": "https://arxiv.org/abs/2506.19967", "authors": ["Travis Thompson", "Seung-Hwan Lim", "Paul Liu", "Ruoying He", "Dongkuan Xu"], "title": "Inference Scaled GraphRAG: Improving Multi Hop Question Answering on Knowledge Graphs", "comment": null, "summary": "Large Language Models (LLMs) have achieved impressive capabilities in\nlanguage understanding and generation, yet they continue to underperform on\nknowledge-intensive reasoning tasks due to limited access to structured context\nand multi-hop information. Retrieval-Augmented Generation (RAG) partially\nmitigates this by grounding generation in retrieved context, but conventional\nRAG and GraphRAG methods often fail to capture relational structure across\nnodes in knowledge graphs. We introduce Inference-Scaled GraphRAG, a novel\nframework that enhances LLM-based graph reasoning by applying inference-time\ncompute scaling. Our method combines sequential scaling with deep\nchain-of-thought graph traversal, and parallel scaling with majority voting\nover sampled trajectories within an interleaved reasoning-execution loop.\nExperiments on the GRBench benchmark demonstrate that our approach\nsignificantly improves multi-hop question answering performance, achieving\nsubstantial gains over both traditional GraphRAG and prior graph traversal\nbaselines. These findings suggest that inference-time scaling is a practical\nand architecture-agnostic solution for structured knowledge reasoning with LLMs", "AI": {"tldr": "我们提出了一种名为Inference-Scaled GraphRAG的新框架，通过在推理时进行计算扩展来增强基于大规模语言模型（LLMs）的图推理能力。", "motivation": "本论文旨在解决当前大规模语言模型（LLMs）在知识密集型推理任务上的不足，特别是LLMs在利用结构化上下文和多跳信息时的局限性。", "method": "我们的方法结合了推理时的计算扩展，包括顺序扩展与深入的链式思考图遍历，以及并行扩展与投票机制来处理采样的轨迹。这些工作是在一个交替的推理-执行循环中进行的。", "result": "实验结果显示，相较于传统的GraphRAG和先前的图遍历基线方法，我们提出的方法在多跳问题回答任务上显著提升了性能。", "conclusion": "本研究的发现表明，推理时的计算扩展是一种有效且与架构无关的解决方案，适用于基于LLMs的结构化知识推理任务。"}}
{"id": "2506.20151", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.20151", "abs": "https://arxiv.org/abs/2506.20151", "authors": ["Haipeng Fan", "Shiyuan Zhang", "Baohunesitu", "Zihang Guo", "Huaiwen Zhang"], "title": "EAR: Erasing Concepts from Unified Autoregressive Models", "comment": "11 pages, 7 figures, 1 tables", "summary": "Autoregressive (AR) models have achieved unified and strong performance\nacross both visual understanding and image generation tasks. However, removing\nundesired concepts from AR models while maintaining overall generation quality\nremains an open challenge. In this paper, we propose Erasure Autoregressive\nModel (EAR), a fine-tuning method for effective and utility-preserving concept\nerasure in AR models. Specifically, we introduce Windowed Gradient Accumulation\n(WGA) strategy to align patch-level decoding with erasure objectives, and\nThresholded Loss Masking (TLM) strategy to protect content unrelated to the\ntarget concept during fine-tuning. Furthermore, we propose a novel benchmark,\nErase Concept Generator and Visual Filter (ECGVF), aim at provide a more\nrigorous and comprehensive foundation for evaluating concept erasure in AR\nmodels. Specifically, we first employ structured templates across diverse large\nlanguage models (LLMs) to pre-generate a large-scale corpus of\ntarget-replacement concept prompt pairs. Subsequently, we generate images from\nthese prompts and subject them to rigorous filtering via a visual classifier to\nensure concept fidelity and alignment. Extensive experimental results conducted\non the ECGVF benchmark with the AR model Janus-Pro demonstrate that EAR\nachieves marked improvements in both erasure effectiveness and model utility\npreservation. Code is available at: https://github.com/immc-lab/ear/", "AI": {"tldr": "提出EAR方法用于有效且保留性能的概念擦除，引入WGA和TLM策略来对齐解码与擦除目标并保护无关内容。同时开发ECGVF基准用于概念擦除的评估。实验结果证明其有效性。", "motivation": "尽管自回归(AR)模型在视觉理解和图像生成任务中取得了统一和强大的表现，但如何在保持生成质量的同时从AR模型中移除不需要的概念仍然是一个开放的挑战。", "method": "我们提出了Erasure Autoregressive Model (EAR)，一种针对概念擦除的有效微调方法。特别地，我们引入了分窗梯度累积(windowed gradient accumulation, WGA)策略来对齐补丁级别解码和擦除目标，以及阈值损失掩码(thresholded loss masking, TLM)策略来保护微调期间与目标概念无关的内容。", "result": "在AR模型Janus-Pro上开展的ECGVF基准测试表明，EAR在擦除效果和模型实用性保持方面都取得了显著的改进。", "conclusion": "实验结果证明了我们提出的方法Erase Autoregressive Model (EAR)及其相应的评估基准Erase Concept Generator and Visual Filter (ECGVF)的有效性，能够在保持模型原有性能的同时有效擦除不需要的概念。"}}
{"id": "2506.19998", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.19998", "abs": "https://arxiv.org/abs/2506.19998", "authors": ["Xinyi Ni", "Haonan Jian", "Qiuyang Wang", "Vedanshi Chetan Shah", "Pengyu Hong"], "title": "Doc2Agent: Scalable Generation of Tool-Using Agents from API Documentation", "comment": null, "summary": "REST APIs play important roles in enriching the action space of web agents,\nyet most API-based agents rely on curated and uniform toolsets that do not\nreflect the complexity of real-world APIs. Building tool-using agents for\narbitrary domains remains a major challenge, as it requires reading\nunstructured API documentation, testing APIs and inferring correct parameters.\nWe propose Doc2Agent, a scalable pipeline to build agents that can call\nPython-based tools generated from API documentation. Doc2Agent generates\nexecutable tools from API documentations and iteratively refines them using a\ncode agent. We evaluate our approach on real-world APIs, WebArena APIs, and\nresearch APIs, producing validated tools. We achieved a 55\\% relative\nperformance improvement with 90\\% lower cost compared to direct API calling on\nWebArena benchmark. A domain-specific agent built for glycomaterial science\nfurther demonstrates the pipeline's adaptability to complex, knowledge-rich\ntasks. Doc2Agent offers a generalizable solution for building tool agents from\nunstructured API documentation at scale.", "AI": {"tldr": "我们提出了Doc2Agent这一方案，它可以解决从非结构化API文档构建代理所遇到的挑战，显著提高了效率并降低了成本。", "motivation": "当前，大多数基于API的代理依赖于经过策划和统一的工具集，这些工具集不能反映现实世界API的复杂性，因此很难构建能够适应任意领域的工具使用代理。", "method": "我们提出了Doc2Agent，这是一个可扩展的流水线，用于从API文档生成可用于Python工具的代理，并通过代码代理迭代地优化这些工具。", "result": "相对于直接调用API的方法，我们的方法在WebArena基准测试中达到了55％的相对性能提升，且成本减少了90％。此外，一种专门针对糖材料科学领域的代理也展示了该流水线对复杂、知识密集任务的适应性。", "conclusion": "Doc2Agent提供了一个通用的解决方案，适用于从非结构化API文档构建代理，解决复杂任务的能力证明了它的实用性和广泛适用性。"}}
{"id": "2506.20152", "categories": ["cs.CV", "cs.AI", "eess.IV"], "pdf": "https://arxiv.org/pdf/2506.20152", "abs": "https://arxiv.org/abs/2506.20152", "authors": ["Deepak Ghimire", "Kilho Lee", "Seong-heum Kim"], "title": "Loss-Aware Automatic Selection of Structured Pruning Criteria for Deep Neural Network Acceleration", "comment": null, "summary": "Structured pruning is a well-established technique for compressing neural\nnetworks, making it suitable for deployment in resource-limited edge devices.\nThis paper presents an efficient Loss-Aware Automatic Selection of Structured\nPruning Criteria (LAASP) for slimming and accelerating deep neural networks.\nThe majority of pruning methodologies employ a sequential process consisting of\nthree stages: 1) training, 2) pruning, and 3) fine-tuning, whereas the proposed\npruning technique adopts a pruning-while-training approach that eliminates the\nfirst stage and integrates the second and third stages into a single cycle. The\nautomatic selection of magnitude or similarity-based filter pruning criteria\nfrom a specified pool of criteria and the specific pruning layer at each\npruning iteration is guided by the network's overall loss on a small subset of\nthe training data. To mitigate the abrupt accuracy drop due to pruning, the\nnetwork is retrained briefly after each reduction of a predefined number of\nfloating-point operations (FLOPs). The optimal pruning rates for each layer in\nthe network are automatically determined, eliminating the need for manual\nallocation of fixed or variable pruning rates for each layer. Experiments on\nthe VGGNet and ResNet models on the CIFAR-10 and ImageNet benchmark datasets\ndemonstrate the effectiveness of the proposed method. In particular, the\nResNet56 and ResNet110 models on the CIFAR-10 dataset significantly improve the\ntop-1 accuracy compared to state-of-the-art methods while reducing the network\nFLOPs by 52\\%. Furthermore, the ResNet50 model on the ImageNet dataset reduces\nFLOPs by more than 42\\% with a negligible 0.33\\% drop in top-5 accuracy. The\nsource code of this paper is publicly available online -\nhttps://github.com/ghimiredhikura/laasp.", "AI": {"tldr": "本文提出了一种新的结构化修剪方法LAASP，可以自动选择最优修剪标准并在训练过程中进行修剪，减少了修剪导致的准确率下降，并且在CIFAR-10和ImageNet数据集上实验结果表明，该方法能够有效提升模型的准确率并减少计算量。", "motivation": "结构化修剪是一种知名的压缩神经网络的技术，使其适用于在资源有限的边缘设备部署。本研究的目标是通过提出一种新型的自适应修剪技术，提高现有方法在模型压缩和加速方面的效果。", "method": "本文提出了一种损失感知的自动选择结构化修剪标准（LAASP）技术，用于加速和简化深度神经网络。与传统的修剪方法（训练-修剪-微调三个阶段）不同，LAASP技术在训练过程中进行修剪，将修剪和微调结合在一个循环中。自动选择修剪标准基于网络在一小部分训练数据上的整体损失，同时在网络修剪一定数量的浮点操作后进行短期重新训练，以减少由于修剪造成的准确性下降。", "result": "实验显示，LAASP方法在CIFAR-10和ImageNet数据集上测试的VGGNet和ResNet模型效果显著。特别是对于CIFAR-10数据集上的ResNet56和ResNet110，其top-1准确率相比最先进的方法有明显提升，并减少了52%的网络浮点操作。在ImageNet数据集上，ResNet50模型的浮点操作减少了超过42%，而top-5准确率仅下降了0.33%。", "conclusion": "本研究通过自动选择最优的修剪标准并对修剪后模型进行短期重新训练，实现了资源高效的模型压缩和加速，实验结果显示该方法在多个基准数据集上达到甚至超过了现有技术水平的准确率并减少了计算量，证明了此方法的有效性。"}}
{"id": "2506.19999", "categories": ["cs.LG", "cs.CL", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2506.19999", "abs": "https://arxiv.org/abs/2506.19999", "authors": ["Francesco Ignazio Re", "Andreas Opedal", "Glib Manaiev", "Mario Giulianelli", "Ryan Cotterell"], "title": "A Spatio-Temporal Point Process for Fine-Grained Modeling of Reading Behavior", "comment": "ACL 2025", "summary": "Reading is a process that unfolds across space and time, alternating between\nfixations where a reader focuses on a specific point in space, and saccades\nwhere a reader rapidly shifts their focus to a new point. An ansatz of\npsycholinguistics is that modeling a reader's fixations and saccades yields\ninsight into their online sentence processing. However, standard approaches to\nsuch modeling rely on aggregated eye-tracking measurements and models that\nimpose strong assumptions, ignoring much of the spatio-temporal dynamics that\noccur during reading. In this paper, we propose a more general probabilistic\nmodel of reading behavior, based on a marked spatio-temporal point process,\nthat captures not only how long fixations last, but also where they land in\nspace and when they take place in time. The saccades are modeled using a Hawkes\nprocess, which captures how each fixation excites the probability of a new\nfixation occurring near it in time and space. The duration time of fixation\nevents is modeled as a function of fixation-specific predictors convolved\nacross time, thus capturing spillover effects. Empirically, our Hawkes process\nmodel exhibits a better fit to human saccades than baselines. With respect to\nfixation durations, we observe that incorporating contextual surprisal as a\npredictor results in only a marginal improvement in the model's predictive\naccuracy. This finding suggests that surprisal theory struggles to explain\nfine-grained eye movements.", "AI": {"tldr": "本文提出了一种新的模型，用于更好地理解阅读过程中的人的行为，特别是他们的目光固定和快速移动（扫视）模式，发现惊喜理论不足以解释细粒度的眼动行为。", "motivation": "动机是提供一个更通用的阅读行为概率模型，而不是基于标准方法中假设强烈的聚合眼动测量方式，该模型能够捕捉阅读过程中在时间和空间中发生的更复杂的动力学行为。", "method": "提出了一种基于标记时空点过程的阅读行为概率模型，该模型不仅捕捉了注视持续时间，还捕捉了注视在空间和时间上如何进行。使用霍克斯过程对扫视进行建模，霍克斯过程捕捉了每次注视如何激发附近时间点和空间点新的注视发生的概率。注视事件的持续时间建模为注视特异性预测随时间卷积的结果，因此捕捉了溢出效应。", "result": "实证研究表明，他们所提出的霍克斯过程模型对人类扫视的拟合优于基线模型。至于注视持续时间，将上下文惊喜作为一个预测因素，却只带来了模型预测准确性的细微改善，这表明惊喜理论难以解释控制粒度的眼动行为。", "conclusion": "研究表明，惊喜理论难以完全解释细粒度的眼动行为，新的模型在对阅读过程中人的扫视行为进行描述时表现较好。"}}
{"id": "2506.20155", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.20155", "abs": "https://arxiv.org/abs/2506.20155", "authors": ["Avadhoot Jadhav", "Ashutosh Srivastava", "Abhinav Java", "Silky Singh", "Tarun Ram Menta", "Surgan Jandial", "Balaji Krishnamurthy"], "title": "Towards Efficient Exemplar Based Image Editing with Multimodal VLMs", "comment": "Accepted at ECCV 2024 (AI4VA Workshop)", "summary": "Text-to-Image Diffusion models have enabled a wide array of image editing\napplications. However, capturing all types of edits through text alone can be\nchallenging and cumbersome. The ambiguous nature of certain image edits is\nbetter expressed through an exemplar pair, i.e., a pair of images depicting an\nimage before and after an edit respectively. In this work, we tackle\nexemplar-based image editing -- the task of transferring an edit from an\nexemplar pair to a content image(s), by leveraging pretrained text-to-image\ndiffusion models and multimodal VLMs. Even though our end-to-end pipeline is\noptimization-free, our experiments demonstrate that it still outperforms\nbaselines on multiple types of edits while being ~4x faster.", "AI": {"tldr": "该研究提出了一个基于示例的图像编辑方法，利用文本到图像扩散模型和多模态模型，实现高效且高质量的图像编辑。", "motivation": "文本描述所有类型的图像编辑可能具有挑战性。某些图像编辑的模糊性更适合通过示例对的形式表达。", "method": "本研究通过利用预训练的文本到图像扩散模型和多模态视觉语言模型，解决了基于示例的图像编辑问题，该问题旨在将示例对中的编辑转移到内容图像上。", "result": "尽管该研究提出的端到端流程是无优化的，但实验证明其在多种类型的编辑上超越了基线方法，且速度提高了约4倍。", "conclusion": "本研究表明，结合文本到图像扩散模型和多模态视觉语言模型可以有效地进行基于示例的图像编辑，提高编辑质量的同时显著减少处理时间。"}}
