<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 52]
- [cs.CV](#cs.CV) [Total: 56]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Inconsistent Affective Reaction: Sentiment of Perception and Opinion in Urban Environments](https://arxiv.org/abs/2510.07359)
*Jingfei Huang,Han Tu*

Main category: cs.CL

> 通过结合社交媒体和街景图像，研究提出了一种新的方法来识别和阐明北京市的感知与意见情感反应不一致现象，并分析了这些情感反应的变化与城市元素的关系。

<details>
  <summary>Details</summary>

**Motivation:** 传统的多维情感分析方法在解释城市情感方面遇到挑战，而社交媒体数据提供了丰富的感知与意见情感信息，可以为此提供新的研究视角。

**Method:** 构建了包含百度和腾讯街景图片及微博文本数据的数据集，并开发了一种结合目标检测和自然语言处理的情感反应指数，使用回归分析、图像分割和词频分析等方法进行情感分类分析。

**Result:** 感知情感趋势图显示正向情绪分布更均匀，而意见情感趋势图则显示了更为极端的变化，不匹配图谱表明了在多年间城市区域感知与意见情感之间的重大差异，情感反应的变化与密集建筑和行人存在等因素有显著关系。

**Conclusion:** 情感感知与意见之间的不一致揭示了城市环境的变化，特别是疫情前后的变化，这对于城市更新策略的制定提供了方向和解释。

**Abstract:** The ascension of social media platforms has transformed our understanding of
urban environments, giving rise to nuanced variations in sentiment reaction
embedded within human perception and opinion, and challenging existing
multidimensional sentiment analysis approaches in urban studies. This study
presents novel methodologies for identifying and elucidating sentiment
inconsistency, constructing a dataset encompassing 140,750 Baidu and Tencent
Street view images to measure perceptions, and 984,024 Weibo social media text
posts to measure opinions. A reaction index is developed, integrating object
detection and natural language processing techniques to classify sentiment in
Beijing Second Ring for 2016 and 2022. Classified sentiment reaction is
analysed and visualized using regression analysis, image segmentation, and word
frequency based on land-use distribution to discern underlying factors. The
perception affective reaction trend map reveals a shift toward more evenly
distributed positive sentiment, while the opinion affective reaction trend map
shows more extreme changes. Our mismatch map indicates significant disparities
between the sentiments of human perception and opinion of urban areas over the
years. Changes in sentiment reactions have significant relationships with
elements such as dense buildings and pedestrian presence. Our inconsistent maps
present perception and opinion sentiments before and after the pandemic and
offer potential explanations and directions for environmental management, in
formulating strategies for urban renewal.

</details>


### [2] [Haystack Engineering: Context Engineering for Heterogeneous and Agentic Long-Context Evaluation](https://arxiv.org/abs/2510.07414)
*Mufei Li,Dongqi Fu,Limei Wang,Si Zhang,Hanqing Zeng,Kaan Sancak,Ruizhong Qiu,Haoyu Wang,Xiaoxin He,Xavier Bresson,Yinglong Xia,Chonglin Sun,Pan Li*

Main category: cs.CL

> 介绍了HaystackCraft，一个新的“针在haystack中”基准测试，用于评估LLMs在嘈杂长上下文中的表现，实验结果揭示了在机构设置下的持续推理挑战。

<details>
  <summary>Details</summary>

**Motivation:** 当前，现代长上下文大型语言模型（LLMs）在合成的“针在haystack中”基准测试中表现良好，但这些测试忽视了来自有偏检索和机构工作流的嘈杂上下文。认为需要haystack工程来构建忠实反映关键现实因素（如异构有偏检索器产生的干扰和机构工作流中的级联错误）嘈杂长上下文，以测试模型的长上下文鲁棒性。

**Method:** 通过HaystackCraft，一个新的基于全英文维基百科超链接网络的多跳问题构建的“针在 haystack中”基准测试，研究异构检索策略（例如稀疏、密集、混合和基于图的）如何影响干扰成分、haystack顺序以及下游LLM性能。此外，在模拟机构操作的动态、LLM依赖环境下，模型可以优化查询，反思之前的推理并决定何时停止。

**Result:** 实验结果指出，(1) 更强的密集检索器可以引入更具挑战性的干扰，而基于图的重新排序同时提高了检索有效性并缓解了更多的有害干扰；(2) 在机构测试中，即使像Gemini 2.5 Pro和GPT-5这样的先进模型也会因自我生成的干扰或困难进行早期停止而出现级联失败。

**Conclusion:** 实验结果表明了在机构长上下文推理中持久存在的挑战，并建立了HaystackCraft作为未来研究的有价值测试平台。

**Abstract:** Modern long-context large language models (LLMs) perform well on synthetic
"needle-in-a-haystack" (NIAH) benchmarks, but such tests overlook how noisy
contexts arise from biased retrieval and agentic workflows. We argue that
haystack engineering is necessary to construct noisy long contexts that
faithfully capture key real-world factors -- distraction from heterogeneous
biased retrievers and cascading errors in agentic workflows -- to test models'
long-context robustness. We instantiate it through HaystackCraft, a new NIAH
benchmark built on the full English Wikipedia hyperlink network with multi-hop
questions. HaystackCraft evaluates how heterogeneous retrieval strategies
(e.g., sparse, dense, hybrid, and graph-based) affect distractor composition,
haystack ordering, and downstream LLM performance. HaystackCraft further
extends NIAH to dynamic, LLM-dependent settings that simulate agentic
operations, where models refine queries, reflect on their past reasonings, and
decide when to stop. Experiments with 15 long-context models show that (1)
while stronger dense retrievers can introduce more challenging distractors,
graph-based reranking simultaneously improves retrieval effectiveness and
mitigates more harmful distractors; (2) in agentic tests, even advanced models
like Gemini 2.5 Pro and GPT-5 suffer cascading failures from self-generated
distractors or struggle to perform early stops. These results highlight
persistent challenges in agentic long-context reasoning and establish
HaystackCraft as a valuable testbed for future progress.

</details>


### [3] [Lemma Dilemma: On Lemma Generation Without Domain- or Language-Specific Training Data](https://arxiv.org/abs/2510.07434)
*Olia Toporkov,Alan Akbik,Rodrigo Agerri*

Main category: cs.CL

> 研究了大型语言模型(LLMs)在上下文引理化任务中未经过优化前的表现，并与传统的监督方法进行了比较，发现LLMs只需要少量示例即可在大多数语言中实现最先进的结果。

<details>
  <summary>Details</summary>

**Motivation:** 此前没有关于大型语言模型在上下文引语化任务中有效性的证据，研究其在没有监督训练数据的目标域或语言中的性能。

**Method:** 比较了监督方法（包括仅对编码器优化和跨语言方法）与直接使用LLMs通过上下文生成引语的方法。研究覆盖了12种不同形态学复杂度的语言。

**Result:** 发现即使编码器在经过非目标域数据优化后仍具有竞争力，但未经优化的当前LLM在大多数语言中通过上下文直接生成引语达到了最先进的结果。

**Conclusion:** 在没有目标领域或语言的监督训练数据情况下，LLMs通过上下文生成引语的研究表明，在提供了一些例子后，它们可以达到最先进的性能，凸显了LLMs在未见过的数据集上的适应性。

**Abstract:** Lemmatization is the task of transforming all words in a given text to their
dictionary forms. While large language models (LLMs) have demonstrated their
ability to achieve competitive results across a wide range of NLP tasks, there
is no prior evidence of how effective they are in the contextual lemmatization
task. In this paper, we empirically investigate the capacity of the latest
generation of LLMs to perform in-context lemmatization, comparing it to the
traditional fully supervised approach. In particular, we consider the setting
in which supervised training data is not available for a target domain or
language, comparing (i) encoder-only supervised approaches, fine-tuned
out-of-domain, and (ii) cross-lingual methods, against direct in-context lemma
generation with LLMs. Our experimental investigation across 12 languages of
different morphological complexity finds that, while encoders remain
competitive in out-of-domain settings when fine-tuned on gold data, current
LLMs reach state-of-the-art results for most languages by directly generating
lemmas in-context without prior fine-tuning, provided just with a few examples.
Data and code available upon publication:
https://github.com/oltoporkov/lemma-dilemma

</details>


### [4] [LASER: An LLM-based ASR Scoring and Evaluation Rubric](https://arxiv.org/abs/2510.07437)
*Amruta Parulekar,Preethi Jyothi*

Main category: cs.CL

> 提出LASER评分标准，用于更公平地评估语音识别结果，避免对不影响语义的细节惩罚过重。使用先进LLM Gemini 2.5 Pro达到了与人类标注的高度相关性，并且证明了方法在其他印度语言的有效性。

<details>
  <summary>Details</summary>

**Motivation:** 传统的语音识别评估指标如词错误率（WER）倾向于不公平地惩罚那些不会显著改变句子语义的形态和句法细微差别。

**Method:** 引入了一种基于LLM的评分标准LASER，该标准利用了先进LLM的上下文学习能力，通过包含详细示例的提示进行学习。此外，还展示了如何通过对从参考文本和ASR预测中导出的词对示例进行微调，使较小的LLM（如Llama 3）能够准确预测应施加何种处罚。

**Result:** 使用Gemini 2.5 Pro的Hindi LASER得分与人类标注的高度相关性达到了94%，且Hindi示例也有效地用于分析其他印度语言（如Marathi、Kannada和Malayalam）中的错误。此外，Llama 3 LLM在预测应适用何种惩罚时的准确率接近89%。

**Conclusion:** 该研究提出了一种新的评估语音识别系统性能的方法，采用基于LLM和上下文学习的评分标准LASER，展示了如何有效地衡量那些不改变句子语义的细微差别，并且证明了这种方法在多个印度语言中的有效性。

**Abstract:** Standard ASR evaluation metrics like Word Error Rate (WER) tend to unfairly
penalize morphological and syntactic nuances that do not significantly alter
sentence semantics. We introduce an LLM-based scoring rubric LASER that
leverages state-of-the-art LLMs' in-context learning abilities to learn from
prompts with detailed examples. Hindi LASER scores using Gemini 2.5 Pro
achieved a very high correlation score of 94% with human annotations. Hindi
examples in the prompt were also effective in analyzing errors in other Indian
languages such as Marathi, Kannada and Malayalam. We also demonstrate how a
smaller LLM like Llama 3 can be finetuned on word-pair examples derived from
reference and ASR predictions to predict what kind of penalty should be applied
with close to 89% accuracy.

</details>


### [5] [Meaningful Pose-Based Sign Language Evaluation](https://arxiv.org/abs/2510.07453)
*Zifan Jiang,Colin Leong,Amit Moryossef,Anne Göhring,Annette Rios,Oliver Cory,Maksym Ivashechkin,Neha Tarigopula,Biao Zhang,Rico Sennrich,Sarah Ebling*

Main category: cs.CL

> 本文研究了针对人类骨骼姿态形式的手语表达有意义的评估方法，希望通过评估工具包促进手语翻译或生成系统的开发。

<details>
  <summary>Details</summary>

**Motivation:** 本文旨在为手语翻译或生成系统的开发和评估提供实践性和可重复性的方式。

**Method:** 本文通过自动元评估手势级检索和跨不同手语的文本到姿态翻译的人类相关性研究，对手势语表达（以人类骨骼姿态的形式）进行有意义的评估。研究涵盖了基于关键点距离、嵌入式以及反向翻译的度量方法。

**Result:** 研究表明不同度量方法在不同场景下的权衡，并提供了一个开源的姿态评估工具包，有助于实践性和可重复性的手语翻译或生成系统的开发和评估。

**Conclusion:** 研究成果和发布的开源姿态评估工具包为手语翻译或生成系统的开发和评估提供了一个实践性和可重复性的方法。

**Abstract:** We present a comprehensive study on meaningfully evaluating sign language
utterances in the form of human skeletal poses. The study covers keypoint
distance-based, embedding-based, and back-translation-based metrics. We show
tradeoffs between different metrics in different scenarios through automatic
meta-evaluation of sign-level retrieval and a human correlation study of
text-to-pose translation across different sign languages. Our findings and the
open-source pose-evaluation toolkit provide a practical and reproducible way of
developing and evaluating sign language translation or generation systems.

</details>


### [6] [Populism Meets AI: Advancing Populism Research with LLMs](https://arxiv.org/abs/2510.07458)
*Eduardo Ryô Tamaki,Yujin J. Jung,Julia Chatterley,Grant Mitchell,Semir Dzebo,Cristóbal Sandoval,Levente Littvay,Kirk A. Hawkins*

Main category: cs.CL

> 提出了一种类似人工编码员训练过程的指南和锚定指导的连贯思考（CoT）提示方法，以解决跨语言、语境和大型语料库中民粹主义观念内容测量的问题。实验结果显示，这种方法使LLM能够达到专家人工编码员的分类准确性。

<details>
  <summary>Details</summary>

**Motivation:** 针对基于文本分析的传统策略在跨语言、语境和大型语料库中扩展成本高昂、耗时且困难的问题，提出了一种新方法来解决民粹主义观念内容测量的挑战。

**Method:** 通过使用一个指南和锚定指导的连贯思考（CoT）提示方法，该方法模仿了人工编码员的培训过程。借助Global Populism Database (GPD)，这是一个全球领导人演讲的详尽数据集，并已根据民粹主义的程度进行了注释。

**Result:** 实验结果表明，该方法使LLM达到了与专家人工编码员相似的分类准确性，证明了这种方法在处理民粹主义复杂和语境敏感方面的能力。

**Conclusion:** 实验结果表明，通过域特定的提示策略，大语言模型可以达到与专家人工编码员相当的分类准确性，显示了模型处理民粹主义复杂、语境敏感方面的能力。

**Abstract:** Measuring the ideational content of populism remains a challenge. Traditional
strategies based on textual analysis have been critical for building the
field's foundations and providing a valid, objective indicator of populist
framing. Yet these approaches are costly, time consuming, and difficult to
scale across languages, contexts, and large corpora. Here we present the
results from a rubric and anchor guided chain of thought (CoT) prompting
approach that mirrors human coder training. By leveraging the Global Populism
Database (GPD), a comprehensive dataset of global leaders' speeches annotated
for degrees of populism, we replicate the process used to train human coders by
prompting the LLM with an adapted version of the same documentation to guide
the model's reasoning. We then test multiple proprietary and open weight models
by replicating scores in the GPD. Our findings reveal that this domain specific
prompting strategy enables the LLM to achieve classification accuracy on par
with expert human coders, demonstrating its ability to navigate the nuanced,
context sensitive aspects of populism.

</details>


### [7] [MAPRO: Recasting Multi-Agent Prompt Optimization as Maximum a Posteriori Inference](https://arxiv.org/abs/2510.07475)
*Zheyuan Zhang,Lin Ge,Hongjiang Li,Weicheng Zhu,Chuxu Zhang,Yanfang Ye*

Main category: cs.CL

> 研究提出了MAPRO，一个高效的多代理系统提示优化框架，该框架通过拓扑感知精炼机制在最大后验概率推理框架下实现逐步收敛，以解决搜索空间过大和责任分配难以明确的问题。实验结果表明，该方法在性能上优于手动设计和自动化替代方案，并提供了未来工作的指导。

<details>
  <summary>Details</summary>

**Motivation:** 大语言模型（LLMs）及其驱动的代理在多样任务中表现出色。然而，尽管多方努力，多代理系统（MAS）的自动提示设计仍然缺乏系统性方法。这是因为搜索空间爆炸性增长和模糊责任分配所造成的障碍。因此，作者提出了MAPRO来解决这些问题。

**Method:** 本文提出了一个名为MAPRO（Multi-Agent PRompt Optimization）的四阶段框架，该框架首先将多代理系统的提示优化问题建模为最大后验概率（MAP）推理问题，并利用语言引导的max-product信念传播算法求解。为了应对信用分配问题，并允许系统迭代更新，MAPRO引入了一种拓扑感知的精炼机制，通过集成执行反馈和下游责备来选择性地更新代理提示。

**Result:** 在各种任务的基准测试中，MAPRO达到了最先进的性能水平，持续超越手动设计的基线和最近的自动化替代方案。

**Conclusion:** 除了性能优势外，基于MAP的公式还为构建更可靠、更系统的未来多代理系统提供了通用指导。

**Abstract:** Large language models (LLMs) have demonstrated remarkable capabilities across
diverse tasks, and LLM-based agents further extend these abilities to various
practical workflows. While recent progress shows that multi-agent systems (MAS)
can outperform single agents by coordinating specialized roles, designing
effective MAS remains difficult due to prompt sensitivity and the compounded
instability MAS creates. To cope with the challenge, recent efforts in
automated prompt design have reduced manual effort. However, multi-agent prompt
optimization remains largely unexplored. Challenges like exponentially
expanding search space and ambiguous credit assignment together make systematic
design intractable without principled methods. Therefore, we introduce
M}ulti-Agent PRompt Optimization (MAPRO), a four-stage framework that first
formulates MAS prompt optimization as a Maximum a Posteriori (MAP) inference
problem and solves it using a language-guided variant of max-product belief
propagation algorithm. To address credit assignment and updates the system
iteratively, MAPRO employs a topology-aware refinement mechanism that
integrates execution feedback and downstream blames to selectively update agent
prompts. Through this process, MAPRO progressively converges to a coordinated
set of agent-specific prompt policies. Across benchmarks in various tasks,
MAPRO achieves state-of-the-art performance, consistently surpassing manually
engineered baselines and recent automated alternatives. Beyond performance, our
MAP-based formulation also delivers general guidelines for building more
reliable and principled multi-agent systems in the future

</details>


### [8] [AsyncSpade: Efficient Test-Time Scaling with Asynchronous Sparse Decoding](https://arxiv.org/abs/2510.07486)
*Shuqing Luo,Yilin Guan,Pingzhi Li,Hanrui Wang,Tianlong Chen*

Main category: cs.CL

> 本文提出AsyncSpade框架，通过异步计算和轻量级时间回归模块，在不牺牲模型性能的情况下提高解码效率，尤其适用于长链式思考和高并发场景，显著降低了TPOT。

<details>
  <summary>Details</summary>

**Motivation:** 解决LLM在长链式思考任务中因线性KV缓存增长与查询感知稀疏解码方法导致的内存瓶颈及高并发场景下的延迟问题。

**Method:** 提出了一种异步框架AsyncSpade，包含两个核心组件：一种轻量级的时间回归模块，用于预测下一词的查询状态；一个异步分层框架，用于解耦KV缓存过滤与自回归解码过程，通过异步机制实现令牌级KV选择与前向推理计算的重叠。

**Result:** 在A100节点上进行的实验结果显示，相比当前最佳基准（Quest）和完全注意力机制，AsyncSpade在Qwen3-8B和Qwen3-32B模型上TPOT分别降低了20%和至少50%，同时在各种测试时间缩放基准上保持或超过了其准确性。

**Conclusion:** AsyncSpade通过消除序列依赖，首次实现了理论最优的TPOT，显著提升了长链推理和高并发场景下的解码效率，证明了其在LLM服务中的优越性。

**Abstract:** Test-time scaling (TTS) boosts LLM reasoning via long chain-of-thought (CoT),
but the linear KV-cache growth amplifies the memory-bound bottleneck of LLM
decoding. Query-aware page-level sparse decoding can achieve state-of-the-art
performance under constrained FLOPs budgets, but is limited by both
sequential-dependent page filtering and coarse-grained token selection,
hampering serving efficiency and model performance on TTS tasks under high
concurrency and long CoT scenarios (consuming even higher runtime than the
forward pipeline itself). In this paper, we first find that the current-step
query state can be accurately approximated in a unified manner from a short
window of recent queries, enabling training-free query-aware sparsity without
waiting in the decoding loop. We propose AsyncSpade, an asynchronous framework
for efficient TTS built on two core components: (1) a novel light-weight
temporal-regressive module that predicts the next-token query state; (2) an
asynchronous and disaggregated framework that decouples the KV cache filtering
from the auto-regressive decoding loop, overlapping the token-level KV
selection with the forward inference computation through asynchronism. To our
knowledge, AsyncSpade is the first to eliminate the sequential dependence
without sacrificing model performance. We validate the effectiveness of
AsyncSpade on common LLM serving setups with an A100 node, where AsyncSpade
fully overlaps KV-cache operations with the inference pipeline, achieving
theoretical optimal time-per-output-token (TPOT). Specifically, AsyncSpade
delivers over 20% reduction on TPOT compared to SoTA baseline (i.e. Quest) and
at least 50% TPOT reduction compared to full attention on Qwen3-8B and
Qwen3-32B models, while matching or surpassing their accuracy on various TTS
benchmarks (AIME-24/25, GPQA-Diamond, MATH-500).

</details>


### [9] [Can Lessons From Human Teams Be Applied to Multi-Agent Systems? The Role of Structure, Diversity, and Interaction Dynamics](https://arxiv.org/abs/2510.07488)
*Rasika Muralidharan,Jaewoon Kwak,Jisun An*

Main category: cs.CL

> 研究了大型语言模型驱动的多智能体系统的团队结构、多样性和互动，发现扁平化优于分层，多样性影响复杂。

<details>
  <summary>Details</summary>

**Motivation:** 研究大型语言模型驱动的多智能体系统的团队动态。

**Method:** 提出了一种基于人类团队科学的多智能体框架，用于研究团队科学的核心方面：结构、多样性以及互动动态。

**Result:** 实验结果表明，扁平化的团队比分层团队表现更好；多样性对团队有复杂的影响。

**Conclusion:** 智能体对团队表现过于自信，但对协作的挑战，包括有限的交流协调仍有一定认识。

**Abstract:** Multi-Agent Systems (MAS) with Large Language Model (LLM)-powered agents are
gaining attention, yet fewer studies explore their team dynamics. Inspired by
human team science, we propose a multi-agent framework to examine core aspects
of team science: structure, diversity, and interaction dynamics. We evaluate
team performance across four tasks: CommonsenseQA, StrategyQA, Social IQa, and
Latent Implicit Hate, spanning commonsense and social reasoning. Our results
show that flat teams tend to perform better than hierarchical ones, while
diversity has a nuanced impact. Interviews suggest agents are overconfident
about their team performance, yet post-task reflections reveal both
appreciation for collaboration and challenges in integration, including limited
conversational coordination.

</details>


### [10] [Can Speech LLMs Think while Listening?](https://arxiv.org/abs/2510.07497)
*Yi-Jen Shih,Desh Raj,Chunyang Wu,Wei Zhou,SK Bong,Yashesh Gaur,Jay Mahadeokar,Ozlem Kalinli,Mike Seltzer*

Main category: cs.CL

> The research shows that fine-tuning multi-stream speech LLMs with CoT increases their accuracy substantially and proposes a method to start reasoning before query completion, reducing latency without accuracy loss. DPO is applied to further optimize accuracy and latency.

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind this research is to address the challenge that speech LLMs face when it comes to handling complex reasoning tasks, aiming to improve their accuracy and reduce latency in spoken interactions.

**Method:** In this study, CoT fine-tuning is applied to multi-stream speech LLMs to enhance their reasoning capabilities. An entropy-based 'question completeness' metric is introduced to allow the model to begin reasoning before the user's query is complete. Additionally, Direct Preference Optimization (DPO) on preference data generated via rejection sampling is used to optimize accuracy and latency trade-offs.

**Result:** The study demonstrates that CoT fine-tuning increases the accuracy of speech LLMs by an average of 2.4x. The 'question completeness' metric achieves a 4% accuracy improvement without changing latency. By employing DPO, the study also manages to reduce latency by 70% while maintaining accuracy.

**Conclusion:** The conclusion is that by applying CoT fine-tuning to speech LLMs and utilizing advanced techniques such as the 'question completeness' metric and DPO, there is a significant improvement in the accuracy and efficiency of these models in handling reasoning tasks.

**Abstract:** Recent advances in speech large language models (speech LLMs) have enabled
seamless spoken interactions, but these systems still struggle with complex
reasoning tasks. Previously, chain-of-thought (CoT) prompting or fine-tuning
has been to shown to significantly improve the reasoning abilities of
text-based LLMs. In this work, we investigate the effect of CoT fine-tuning for
multi-stream speech LLMs, demonstrating that reasoning in text space improves
the accuracy of speech LLMs by 2.4x, on average, over a suite of spoken
reasoning tasks. Beyond accuracy, the latency of the spoken response is a
crucial factor for interacting with voice-based agents. Inspired by the human
behavior of "thinking while listening," we propose methods to reduce the
additional latency from reasoning by allowing the model to start reasoning
before the user query has ended. To achieve this, we introduce an entropy-based
metric, "question completeness," which acts as an indicator to guide the model
on the optimal time to start reasoning. This method provides greater control
over the accuracy-latency trade-off compared with heuristic-based approaches
and, under equivalent latency conditions, yields a 4% accuracy gain on
ARC-Easy. Finally, we use Direct Preference Optimization (DPO) on preference
data created using rejection sampling to push the accuracy-latency pareto
frontier further, resulting in a 70% reduction in latency without loss in
accuracy.

</details>


### [11] [When Thoughts Meet Facts: Reusable Reasoning for Long-Context LMs](https://arxiv.org/abs/2510.07499)
*Soyeong Jeong,Taehee Jung,Sung Ju Hwang,Joo-Kyung Kim,Dongyeop Kang*

Main category: cs.CL

> 本文介绍了思想模板增强的长上下文语言模型框架（ToTAL），该方法通过思想模板和迭代更新策略提高模型的推理能力。

<details>
  <summary>Details</summary>

**Motivation:** 近年来，长上下文语言模型尽管可以处理大量的标记，但仅仅增加文档并不能解决证据如何连接的问题。为此，提出了思想模板来解决这一问题。

**Method:** 使用思想模板将推理重新表述为可重用的思想缓存，这些缓存源自先前的问题解决轨迹，并提出了一种迭代更新策略以通过自然语言反馈优化这些模板。

**Result:** 该方法在各种基准测试和LCLM系列中，相比强基线方法在基于检索和无检索设置下都取得了显著提高。

**Conclusion:** 证明了经过优化的思想模板可以被提炼进更小型的开源模型中，展示了其广泛适用性和透明推理复用。

**Abstract:** Recent Long-Context Language Models (LCLMs) can process hundreds of thousands
of tokens in a single prompt, enabling new opportunities for
knowledge-intensive multi-hop reasoning by integrating large sets of retrieved
documents or, in some cases, directly all necessary information. However,
simply feeding more documents into the context window fails to capture how
evidence should be connected. We address this gap with thought templates, which
recast reasoning as reusable thought caches, derived from prior problem solving
traces, structuring how evidence is combined and guiding multi-hop inference
with factual documents. To keep these templates effective, we propose an update
strategy that iteratively refines templates derived from training data through
natural-language feedback. Across diverse benchmarks and LCLM families, our
approach delivers consistent gains over strong baselines in both
retrieval-based and retrieval-free settings. Furthermore, we show that
optimized templates can be distilled into smaller open-source models,
demonstrating its broad applicability and transparent reasoning reuse. We refer
to our framework as Thought Template Augmented LCLMs (ToTAL).

</details>


### [12] [ParsTranslit: Truly Versatile Tajik-Farsi Transliteration](https://arxiv.org/abs/2510.07520)
*Rayyan Merchant,Kevin Tang*

Main category: cs.CL

> 本文在一个新的更大范围的数据集上，提出了一个先进的塔吉克-波斯语转写模型，避免了先前模型的数据局限性，提高了实际应用中的灵活性和性能。

<details>
  <summary>Details</summary>

**Motivation:** 鉴于之前的努力未能使用其他数据集限制了模型仅适用于某些领域的文本，本文旨在解决这一问题，提出一个更通用的转写系统。

**Method:** 本文提出了一种最新状态的序列到序列模型，用于塔吉克语-波斯语的转写，并训练了所有可用的数据集。此外，还提供了两个自己的数据集。

**Result:** 本文模型在Farsi到Tajik和Tajik到Farsi的转写中分别达到了chrF++和Normalised CER得分为87.91和0.05，92.28和0.04。

**Conclusion:** 研究结果提供了跨领域任务的清晰理解，设定了全面的领先基准。

**Abstract:** As a digraphic language, the Persian language utilizes two written standards:
Perso-Arabic in Afghanistan and Iran, and Tajik-Cyrillic in Tajikistan. Despite
the significant similarity between the dialects of each country, script
differences prevent simple one-to-one mapping, hindering written communication
and interaction between Tajikistan and its Persian-speaking ``siblings''. To
overcome this, previously-published efforts have investigated machine
transliteration models to convert between the two scripts. Unfortunately, most
efforts did not use datasets other than those they created, limiting these
models to certain domains of text such as archaic poetry or word lists. A truly
usable transliteration system must be capable of handling varied domains,
meaning that suck models lack the versatility required for real-world usage.
The contrast in domain between data also obscures the task's true difficulty.
We present a new state-of-the-art sequence-to-sequence model for Tajik-Farsi
transliteration trained across all available datasets, and present two datasets
of our own. Our results across domains provide clearer understanding of the
task, and set comprehensive comparable leading benchmarks. Overall, our model
achieves chrF++ and Normalized CER scores of 87.91 and 0.05 from Farsi to Tajik
and 92.28 and 0.04 from Tajik to Farsi. Our model, data, and code are available
at https://anonymous.4open.science/r/ParsTranslit-FB30/.

</details>


### [13] [OWL: Overcoming Window Length-Dependence in Speculative Decoding for Long-Context Inputs](https://arxiv.org/abs/2510.07535)
*Jaeseong Lee,seung-won hwang,Aurick Qiao,Gabriele Oliaro,Ye Wang,Samyam Rajbhandari*

Main category: cs.CL

> 论文提出了一种新的长上下文推测解码方法，解决了现有方法在长上下文任务中的性能退化问题。

<details>
  <summary>Details</summary>

**Motivation:** 当前推测解码方法在长上下文任务中效果不佳，此论文希望通过引入新的长上下文基准（LongSpecBench）和OWL模型来解决这一问题。

**Method:** 该论文引入了OWL模型并通过三个创新点来改进长上下文推测解码性能：1) 一个基于LSTM的起草者，仅依赖于最后一个标记的状态，使其能够适应各种长度；2) 在验证者中使用特殊标记[SPEC]以产生更丰富的表示形式；3) 结合树状和非树状解码方法的混合算法。

**Result:** OWL模型在长上下文输入下实现了比EAGLE3高出约5倍的接受长度。

**Conclusion:** 通过引入新的长上下文基准（LongSpecBench）和OWL模型，实现了在长上下文任务中的推测解码性能提升，有助于推动未来的研究方向。

**Abstract:** Speculative decoding promises faster inference for large language models
(LLMs), yet existing methods fail to generalize to real-world settings.
Benchmarks typically assume short contexts (e.g., 2K tokens), whereas practical
workloads involve long contexts. We find current approaches degrade severely
with long contexts; for instance, EAGLE3 even slows down the generation speed
by 0.81x. We address these limitations by releasing a new long-context
benchmark (LongSpecBench) and introducing a novel model (OWL). OWL achieves
about 5x higher acceptance length than EAGLE3 on long-context inputs through
three innovations: (1) an LSTM-based drafter conditioned only on the last-token
state, making it generalize to various lengths, (2) a special token [SPEC] in
the verifier that produces richer representation for drafter, and (3) a hybrid
algorithm combining both tree and non-tree decoding methods. We release all
code and datasets to advance future research.

</details>


### [14] [Deploying Tiny LVLM Judges for Real-World Evaluation of Chart Models: Lessons Learned and Best Practices](https://arxiv.org/abs/2510.07545)
*Md Tahmid Rahman Laskar,Mohammed Saidul Islam,Ridwan Mahbub,Mizanur Rahman,Amran Bhuiyan,Israt Jahan,Mir Tafseer Nayeem,Shafiq Joty,Enamul Hoque,Jimmy Huang*

Main category: cs.CL

> 本文讨论了如何通过多标准提示技术和领域自适应迁移学习来改善小规模视觉语言模型在图表理解任务中的性能，以实现在资源受限环境下的有效应用。

<details>
  <summary>Details</summary>

**Motivation:** 尽管具有70亿参数的大规模视觉语言模型（LVLM）在图表理解任务中表现出了希望，但小规模模型（参数小于等于20亿）的表现仍然不佳。这限制了在资源受限环境中它们的真实世界使用。为了应对这一问题，研究人员希望通过减少模型大小同时提高系统性能来解决这一问题。

**Method:** 本文提出两种方法来确保评估的成本效益：多标准提示和领域自适应迁移学习。多标准提示将不同的评估标准合并为单个查询。领域自适应迁移学习则是在一个图表数据集上的合成评判数据上微调一个20亿参数的LVLM，以创建ChartJudge模型。

**Result:** 实验显示，多标准提示暴露了结构上的脆弱点，导致7B模型（包括专门的LVLM评判者如LLaVA-Critic）的性能大幅下降。此外，研究表明，小规模LVLM（ChartJudge）可以从一个数据集转移到另一个数据集，使其成为一个更专业的模型。

**Conclusion:** 通过对图表类型和查询复杂度的细粒度分析，提供了关于模型大小、提示设计和可移植性之间权衡的实用性见解，这使得图表推理任务的大规模、低成本评估成为可能。

**Abstract:** Large Vision-Language Models (LVLMs) with only 7B parameters have shown
promise as automated judges in chart comprehension tasks. However, tiny models
(<=2B parameters) still perform poorly as judges, limiting their real-world use
in resource-constrained settings. To address this, we propose two approaches to
ensure cost-efficient evaluation: (i) multi-criteria prompting, which combines
separate evaluation criteria into a single query, and (ii) domain-adaptive
transfer learning, in which we fine-tune a 2B-parameter LVLM on synthetic
judgments in a chart dataset to create the ChartJudge. Experiments show that
multi-criteria prompting exposes robustness gaps, which led to a huge drop in
performance for 7B models, including specialized LVLM judges like LLaVA-Critic.
In addition, we find that our tiny LVLM (ChartJudge) can effectively transfer
knowledge from one dataset to another to make it a more specialized model. Our
fine-grained analysis across chart types and query complexities offers
actionable insights into trade-offs between model size, prompt design, and
transferability, enabling scalable, low-cost evaluation for chart reasoning
tasks. Our code and the data will be made publicly available.

</details>


### [15] [Multi-Task Pre-Finetuning of Lightweight Transformer Encoders for Text Classification and NER](https://arxiv.org/abs/2510.07566)
*Junyi Zhu,Savas Ozkan,Andrea Maracani,Sinan Mutlu,Cho Jung Min,Mete Ozay*

Main category: cs.CL

> 本文提出了一种新的多任务预微调框架，用于优化轻量级BERT样编码器在移动设备上的NLP应用，该框架解决了优化冲突问题，并在NER和文本分类任务上取得了显著效果。

<details>
  <summary>Details</summary>

**Motivation:** 研究动机是探索策略，以提高轻量级BERT样编码器在跨多种应用的同时保持低内存和计算效率的能力，特别是在命名实体识别（NER）和文本分类两个基本NLP任务家族中。

**Method:** 本研究提出了一种基于任务主导LoRA模块的多任务预微调框架，以解决传统多任务预微调中的优化冲突问题，从而实现在单一共享编码器基础上的模块化适配器。

**Result:** 实验结果表明，在21个下游任务上，NER任务平均改善了+0.8%，文本分类任务平均改善了+8.8%，显示了该方法对移动NLP应用的有效性。

**Conclusion:** 研究结论是，所提出的方法不仅达到了独立预微调的性能，还满足了实际部署的约束，有效地提高了移动平台上的NLP模型适应性。

**Abstract:** Deploying natural language processing (NLP) models on mobile platforms
requires models that can adapt across diverse applications while remaining
efficient in memory and computation. We investigate pre-finetuning strategies
to enhance the adaptability of lightweight BERT-like encoders for two
fundamental NLP task families: named entity recognition (NER) and text
classification. While pre-finetuning improves downstream performance for each
task family individually, we find that na\"ive multi-task pre-finetuning
introduces conflicting optimization signals that degrade overall performance.
To address this, we propose a simple yet effective multi-task pre-finetuning
framework based on task-primary LoRA modules, which enables a single shared
encoder backbone with modular adapters. Our approach achieves performance
comparable to individual pre-finetuning while meeting practical deployment
constraint. Experiments on 21 downstream tasks show average improvements of
+0.8% for NER and +8.8% for text classification, demonstrating the
effectiveness of our method for versatile mobile NLP applications.

</details>


### [16] [Linguistic Patterns in Pandemic-Related Content: A Comparative Analysis of COVID-19, Constraint, and Monkeypox Datasets](https://arxiv.org/abs/2510.07579)
*Mkululi Sikosana,Sean Maudsley-Barton,Oluwaseun Ajao*

Main category: cs.CL

> 本研究通过计算语言学分析明确了与新冠疫情相关的误导信息和正确信息之间在语言使用上的区别，尤其是可读性和情感化语言的使用。

<details>
  <summary>Details</summary>

**Motivation:** 研究动机在于通过分析不同类型的新冠疫情相关内容的语言特征，揭示健康误导信息与事实性信息之间的差异，以期为识别新冠疫情中的误导性信息提供语言学依据，并对公共卫生信息传播策略和网络媒体环境下的危机沟通理论模型提供理论支持。

**Method:** 本研究利用计算语言学分析了与疫情相关的在线话语，以探讨语言如何将健康误导信息与事实信息区分开来。研究基于三个语料库：7588条关于新冠疫情的错误叙述、10700条一般新冠疫情内容和5787条关于猴痘的帖子，识别了可读性、修辞标记和说服性语言使用的显著差异。

**Result:** 研究结果表明，新冠疫情的误导信息可读性得分明显较低，并且包含恐惧相关或说服性词汇的频率是其他数据集的两倍多。误导性信息极少使用感叹号，与具有更多情感化风格的猴痘内容形成对比。

**Conclusion:** 研究认为，误导信息倾向于使用带有情感线索的复杂修辞风格，这种组合可能增强了其被感知的可信度。同时研究也承认存在一些局限性，包括对传统可读性指数的依赖和对说服性词汇的狭隘选择，这促使未来的研究采用更纵向的设计和更广泛的感性词汇以及基于平台的方法来提升研究的准确性。

**Abstract:** This study conducts a computational linguistic analysis of pandemic-related
online discourse to examine how language distinguishes health misinformation
from factual communication. Drawing on three corpora: COVID-19 false narratives
(n = 7588), general COVID-19 content (n = 10700), and Monkeypox-related posts
(n = 5787), we identify significant differences in readability, rhetorical
markers, and persuasive language use. COVID-19 misinformation exhibited
markedly lower readability scores and contained over twice the frequency of
fear-related or persuasive terms compared to the other datasets. It also showed
minimal use of exclamation marks, contrasting with the more emotive style of
Monkeypox content. These patterns suggest that misinformation employs a
deliberately complex rhetorical style embedded with emotional cues, a
combination that may enhance its perceived credibility. Our findings contribute
to the growing body of work on digital health misinformation by highlighting
linguistic indicators that may aid detection efforts. They also inform public
health messaging strategies and theoretical models of crisis communication in
networked media environments. At the same time, the study acknowledges
limitations, including reliance on traditional readability indices, use of a
deliberately narrow persuasive lexicon, and reliance on static aggregate
analysis. Future research should therefore incorporate longitudinal designs,
broader emotion lexicons, and platform-sensitive approaches to strengthen
robustness.

</details>


### [17] [IASC: Interactive Agentic System for ConLangs](https://arxiv.org/abs/2510.07591)
*Chihiro Taguchi,Richard Sproat*

Main category: cs.CL

> 使用LLM开发的构造语系统展示了开发人造语言的潜力，并展示了LLM对语言理解的能力以及这种系统在未来可能在语言翻译任务中的应用前景。

<details>
  <summary>Details</summary>

**Motivation:** 研究的动机是通过该系统创建人造语言应具有娱乐性，并探索LLM对语言的理解，而非针对特定语言或语言现象的认知。此外，系统还可用于从高资源语言到低资源语言的翻译。

**Method:** 该系统使用大型语言模型（LLM）作为辅助工具开发构造语。系统是模块化的，首先通过代理方法为语言创建目标音系，该方法在每一步中通过反馈修改其输出。接下来，将英语句子‘翻译’为形态句法标记，然后构建一个词汇表，最后创建正字法并编写简要语法手册。

**Result:** 研究结果展示了不同LLM在不同语言规格上的能力差异，对常见模式的处理比对罕见模式的处理要容易得多。此外，虽然目前从高资源到低资源语言的翻译结果大多负面，但研究也提供了证据表明改进后的系统可能带来实质性改进。

**Conclusion:** 系统展示了LLM在语言能力上的瓶颈，同时表明这种方法在未来改进后可能对语言翻译任务有帮助，特别是对于高资源语言到低资源语言的翻译。

**Abstract:** We present a system that uses LLMs as a tool in the development of
Constructed Languages. The system is modular in that one first creates a target
phonology for the language using an agentic approach that refines its output at
each step with commentary feedback on its previous attempt. Next, a set of
sentences is 'translated' from their English original into a morphosyntactic
markup that reflects the word order and morphosyntactic feature specifications
of the desired target language, with affixes represented as morphosyntactic
feature bundles. From this translated corpus, a lexicon is constructed using
the phonological model and the set of morphemes (stems and affixes) extracted
from the 'translated' sentences. The system is then instructed to provide an
orthography for the language, using an existing script such as Latin or
Cyrillic. Finally, the system writes a brief grammatical handbook of the
language. The system can also translate further sentences into the target
language.
  Our goal is twofold. First, we hope that these tools will be fun to use for
creating artificially constructed languages. Second, we are interested in
exploring what LLMs 'know' about language-not what they know about any
particular language or linguistic phenomenon, but how much they know about and
understand language and linguistic concepts. As we shall see, there is a fairly
wide gulf in capabilities both among different LLMs and among different
linguistic specifications, with it being notably easier for systems to deal
with more common patterns than rarer ones. An additional avenue that we explore
is the application of our approach to translating from high-resource into
low-resource languages. While the results so far are mostly negative, we
provide some evidence that an improved version of the present system could
afford some real gains in such tasks.
  https://github.com/SakanaAI/IASC

</details>


### [18] [Vocabulary embeddings organize linguistic structure early in language model training](https://arxiv.org/abs/2510.07613)
*Isabel Papadimitriou,Jacob Prince*

Main category: cs.CL

> 研究了语言模型在训练过程中输入词汇表示的结构，揭示了高频词汇和低频词汇在训练过程中的不同演化过程。

<details>
  <summary>Details</summary>

**Motivation:** 研究语言模型的输入词汇表示是如何被结构化的，以及这种结构在训练过程中如何和何时发生变化。

**Method:** 使用表示相似性分析，进行了一系列实验，将两个开源模型（Pythia 12B 和 OLMo 7B）的输入嵌入和输出嵌入的几何结构与语义、句法和频率相关的度量相关联。

**Result:** 发现一：在训练过程中，词汇嵌入几何结构快速地与一系列语义和句法特征高度相关；发现二：高频词汇和功能词汇的嵌入（如“the”、“of”）比词汇和低频词汇更快收敛到其最终向量，且低频词汇在某种程度上保持了与随机初始化的对齐。

**Conclusion:** 这些发现有助于了解输入嵌入如何围绕语言结构组织的动态轨迹，揭示了词汇频率和功能的不同作用。进一步的研究可能探索词汇几何结构的演化如何促进模型训练中的特定能力提升。

**Abstract:** Large language models (LLMs) work by manipulating the geometry of input
embedding vectors over multiple layers. Here, we ask: how are the input
vocabulary representations of language models structured, and how and when does
this structure evolve over training? To answer this question, we use
representational similarity analysis, running a suite of experiments that
correlate the geometric structure of the input embeddings and output embeddings
of two open-source models (Pythia 12B and OLMo 7B) with semantic, syntactic,
and frequency-based metrics over the course of training. Our key findings are
as follows: 1) During training, the vocabulary embedding geometry quickly
converges to high correlations with a suite of semantic and syntactic features;
2) Embeddings of high-frequency and function words (e.g., "the," "of") converge
to their final vectors faster than lexical and low-frequency words, which
retain some alignment with the bias in their random initializations. These
findings help map the dynamic trajectory by which input embeddings organize
around linguistic structure, revealing distinct roles for word frequency and
function. Our findings motivate a deeper study of how the evolution of
vocabulary geometry may facilitate specific capability gains during model
training.

</details>


### [19] [Toward Reliable Clinical Coding with Language Models: Verification and Lightweight Adaptation](https://arxiv.org/abs/2510.07629)
*Zhangdie Yuan,Han-Chin Shing,Mitch Strong,Chaitanya Shivade*

Main category: cs.CL

> 研究展示轻量级干预措施可以提升LLMs在临床编码任务中的准确性，强调了临床代码验证的有效性，并发布了新的门诊临床记录数据集。

<details>
  <summary>Details</summary>

**Motivation:** 临床编码的准确性对于医疗文档、计费和决策至关重要。尽管之前的工作表明现成的LLMs在此任务中表现不佳，但也发现单纯的精确匹配度量标准往往忽视了预测编码在层级上接近却错误的问题。

**Method:** 通过轻量级干预措施，包括提示工程和小规模微调来提高准确性，而不需借助计算昂贵的搜索方法。为了处理层级接近的错误编码问题，引入了临床代码验证作为独立任务和管道组件。

**Result:** 揭示了这样的层级不一致占了LLMs错误的很大比例。验证作为一种步骤显示出改进LLM医疗编码的有效性和可靠性。释放了一个经过专家双重标注的门诊临床记录数据集，该数据集包含ICD-10编码，用以缓解现有数据集的不足，如证据不完整和MIMIC的住院偏见。

**Conclusion:** 引入了临床代码验证作为一种有效和可靠的步骤，能够提高基于LLM的医疗编码的准确性。同时，发布了门诊临床记录的专家双重标注数据集，改进了现有数据集的不足。

**Abstract:** Accurate clinical coding is essential for healthcare documentation, billing,
and decision-making. While prior work shows that off-the-shelf LLMs struggle
with this task, evaluations based on exact match metrics often overlook errors
where predicted codes are hierarchically close but incorrect. Our analysis
reveals that such hierarchical misalignments account for a substantial portion
of LLM failures. We show that lightweight interventions, including prompt
engineering and small-scale fine-tuning, can improve accuracy without the
computational overhead of search-based methods. To address hierarchically
near-miss errors, we introduce clinical code verification as both a standalone
task and a pipeline component. To mitigate the limitations in existing
datasets, such as incomplete evidence and inpatient bias in MIMIC, we release
an expert double-annotated benchmark of outpatient clinical notes with ICD-10
codes. Our results highlight verification as an effective and reliable step
toward improving LLM-based medical coding.

</details>


### [20] [Role-Conditioned Refusals: Evaluating Access Control Reasoning in Large Language Models](https://arxiv.org/abs/2510.07642)
*Đorđe Klisura,Joseph Khoury,Ashish Kundu,Ram Krishnan,Anthony Rios*

Main category: cs.CL

> 研究考察了大型语言模型根据角色权限拒绝或回答的能力，使用增强的SQL数据集进行评估，并比较了三种权限控制的设计策略。

<details>
  <summary>Details</summary>

**Motivation:** 大型语言模型在保持角色边界方面存在挑战，它们可能会产生不受限制的响应。本研究旨在评估和改进大型语言模型遵守访问控制策略的能力。

**Method:** 本研究通过创建一个结合了Spider和BIRD数据集的新型数据集，这个数据集加入了基于PostgreSQL角色的真实权限策略，来评估大型语言模型在不同设计下的表现。研究比较了三种设计：1. 零样本或少样本提示；2. 两步生成-验证流水线，该流水线通过验证SQL语句是否符合策略来过滤内容；3. 使用LoRA进行微调的模型，其目标是直接学习权限感知能力。

**Result:** 研究发现，使用明确验证的两步框架可以提高拒绝的精确度和减少不正确的许可情况，而微调模型则在安全性和实用性之间取得了更好的平衡。不过，当面对更多的角色和访问策略时，所有系统的性能下降。

**Conclusion:** 研究结果显示，明确的验证过程（两步框架方法）可以提高拒绝的精确度，减少错误许可的情况。微调方法则在安全性与实用性之间取得了更好的平衡。但是，更长更复杂的策略会降低所有系统的可靠性。

**Abstract:** Access control is a cornerstone of secure computing, yet large language
models often blur role boundaries by producing unrestricted responses. We study
role-conditioned refusals, focusing on the LLM's ability to adhere to access
control policies by answering when authorized and refusing when not. To
evaluate this behavior, we created a novel dataset that extends the Spider and
BIRD text-to-SQL datasets, both of which have been modified with realistic
PostgreSQL role-based policies at the table and column levels. We compare three
designs: (i) zero or few-shot prompting, (ii) a two-step generator-verifier
pipeline that checks SQL against policy, and (iii) LoRA fine-tuned models that
learn permission awareness directly. Across multiple model families, explicit
verification (the two-step framework) improves refusal precision and lowers
false permits. At the same time, fine-tuning achieves a stronger balance
between safety and utility (i.e., when considering execution accuracy). Longer
and more complex policies consistently reduce the reliability of all systems.
We release RBAC-augmented datasets and code.

</details>


### [21] [Banking Done Right: Redefining Retail Banking with Language-Centric AI](https://arxiv.org/abs/2510.07645)
*Xin Jie Chua,Jeraelyn Ming Li Tan,Jia Xuan Tan,Soon Chang Poh,Yi Xian Goh,Debbie Hui Tian Choong,Chee Mun Foong,Sze Jue Yang,Chee Seng Chan*

Main category: cs.CL

> 论文展示了Ryt AI框架如何通过自然语言对话界面使客户执行核心金融交易，作为全球首个获得监管批准的对话式AI银行界面，该框架通过四个LLM代理和安全措施提供了可靠的金融服务。

<details>
  <summary>Details</summary>

**Motivation:** Ryt AI是全球首个获得监管机构批准的部署项目，它使得对话式AI作为主要的银行界面，与之前仅限于咨询或支持角色的助手形成对比。该框架通过确定性的护栏、人类互动环节确认和无状态审计架构提供了多层次的安全和合规保障。

**Method:** 本论文介绍了一种名为Ryt AI的LLM原生代理框架，该框架通过自然语言对话支持客户执行核心金融交易。Ryt AI完全由内部开发，由ILMU（内部开发的闭源LLM）驱动，它通过四个LLM代理（Guardrails、Intent、Payment和FAQ）取代了僵化的多屏工作流，每个代理都附加了特定任务的LoRA适配器，以确保一致的行为同时保持较低的开销。

**Result:** 结果证明，监管批准的自然语言界面可以可靠地支持在严格治理下进行的核心金融操作。

**Conclusion:** 研究结论表明，经过监管机构批准的自然语言界面可以支持严格的治理下的核心金融操作，实现了银行业务的正确执行。

**Abstract:** This paper presents Ryt AI, an LLM-native agentic framework that powers Ryt
Bank to enable customers to execute core financial transactions through natural
language conversation. This represents the first global regulator-approved
deployment worldwide where conversational AI functions as the primary banking
interface, in contrast to prior assistants that have been limited to advisory
or support roles. Built entirely in-house, Ryt AI is powered by ILMU, a
closed-source LLM developed internally, and replaces rigid multi-screen
workflows with a single dialogue orchestrated by four LLM-powered agents
(Guardrails, Intent, Payment, and FAQ). Each agent attaches a task-specific
LoRA adapter to ILMU, which is hosted within the bank's infrastructure to
ensure consistent behavior with minimal overhead. Deterministic guardrails,
human-in-the-loop confirmation, and a stateless audit architecture provide
defense-in-depth for security and compliance. The result is Banking Done Right:
demonstrating that regulator-approved natural-language interfaces can reliably
support core financial operations under strict governance.

</details>


### [22] [OBCache: Optimal Brain KV Cache Pruning for Efficient Long-Context LLM Inference](https://arxiv.org/abs/2510.07651)
*Yuzhe Gu,Xiyu Liang,Jiaojiao Zhao,Enmao Diao*

Main category: cs.CL

> 提出了OBCache框架，以缓存替换问题为基础，通过关注令牌对注意力输出的影响来提高大型语言模型在长上下文应用中的性能。

<details>
  <summary>Details</summary>

**Motivation:** 大型语言模型（LLMs）拥有扩展的上下文窗口，但这些模型的缓存会给内存带来显著的负担。现有的缓存替换方法通常基于积累的注意力权重对令牌进行启发式排序，但这没有考虑令牌对注意力输出的真实影响。

**Method:** 构建了一个名为OBCache的框架，该框架将缓存替换视为分层结构化剪枝问题。它基于最优脑损伤（OBD）理论，通过测量剪枝令牌引起的注意力输出扰动来量化令牌的重要性。得分考虑了注意力权重、值状态和注意力输出的信息。

**Result:** 实验结果表明，使用OBCache得分替换现有工作的启发式得分可以提高长上下文的准确性。

**Conclusion:** OBCache通过提供输出感知信号增强了现有的缓存替换策略，从而提升了长上下文的应用效果。

**Abstract:** Large language models (LLMs) with extended context windows enable powerful
downstream applications but impose significant memory overhead, as caching all
key-value (KV) states scales linearly with sequence length and batch size.
Existing cache eviction methods address this by exploiting attention sparsity,
yet they typically rank tokens heuristically using accumulated attention
weights without considering their true impact on attention outputs. We propose
Optimal Brain Cache (OBCache), a principled framework that formulates cache
eviction as a layer-wise structured pruning problem. Building upon the Optimal
Brain Damage (OBD) theory, OBCache quantifies token saliency by measuring the
perturbation in attention outputs induced by pruning tokens, with closed-form
scores derived for isolated keys, isolated values, and joint key-value pairs.
Our scores account not only for attention weights but also for information from
value states and attention outputs, thereby enhancing existing eviction
strategies with output-aware signals. Experiments on LLaMA and Qwen models
demonstrate that replacing the heuristic scores in existing works, which
estimate token saliency across different query positions, with OBCache's
output-aware scores consistently improves long-context accuracy.

</details>


### [23] [Textual Entailment and Token Probability as Bias Evaluation Metrics](https://arxiv.org/abs/2510.07662)
*Virginia K. Felkner,Allison Lim,Jonathan May*

Main category: cs.CL

> 研究测试了NLI作为语言模型偏见度量的替代方法，发现NLI和TP评估存在显著差异，建议将两者与其他下游评价方法结合使用。

<details>
  <summary>Details</summary>

**Motivation:** 当前，语言模型中的社会偏见通常是通过TP指标来测量的，虽然TP指标的适用性广泛，但它们与现实世界的语言模型用途和危害之间的距离受到了批评。本研究旨在探索替代偏见度量指标的合理性。

**Method:** 本研究测试了自然语言推理（NLI）作为一种更真实的语言模型偏见度量的替代方法，并将其与基于标记概率（TP）的方法进行了比较。

**Result:** 研究显示，NLI和TP偏见评估行为存在显著差异，NLI度量更可能检测到欠矫正的案例，但似乎对反刻板语言的措辞更加脆弱和敏感。

**Conclusion:** 结论是，TP和NLI都不是所有情况下更好的偏见度量，建议结合TP、NLI和下游偏见评估以确保对语言模型进行全面评价。

**Abstract:** Measurement of social bias in language models is typically by token
probability (TP) metrics, which are broadly applicable but have been criticized
for their distance from real-world langugage model use cases and harms. In this
work, we test natural language inference (NLI) as a more realistic alternative
bias metric. We show that, curiously, NLI and TP bias evaluation behave
substantially differently, with very low correlation among different NLI
metrics and between NLI and TP metrics. We find that NLI metrics are more
likely to detect "underdebiased" cases. However, NLI metrics seem to be more
brittle and sensitive to wording of counterstereotypical sentences than TP
approaches. We conclude that neither token probability nor natural language
inference is a "better" bias metric in all cases, and we recommend a
combination of TP, NLI, and downstream bias evaluations to ensure comprehensive
evaluation of language models.
  Content Warning: This paper contains examples of anti-LGBTQ+ stereotypes.

</details>


### [24] [Stress-Testing Model Specs Reveals Character Differences among Language Models](https://arxiv.org/abs/2510.07686)
*Jifan Zhang,Henry Sleight,Andi Peng,John Schulman,Esin Durmus*

Main category: cs.CL

> The paper introduces a method to stress-test AI model specifications, revealing over 70,000 instances of behavioral inconsistencies across twelve major LLMs.

<details>
  <summary>Details</summary>

**Motivation:** To address the critical challenges faced by AI constitutions and model specifications, including internal conflicts and insufficient coverage.

**Method:** Developing a systematic and automatic approach to generate scenarios that force LLMs to choose between conflicting value-based principles and measuring behavioral disagreement across models.

**Result:** Identified over 70,000 cases of significant behavioral divergence among tested models, indicating underlying problems in their specifications.

**Conclusion:** The method effectively highlights issues such as contradictions and interpretive ambiguities, providing valuable insights for refining AI model specifications.

**Abstract:** Large language models (LLMs) are increasingly trained from AI constitutions
and model specifications that establish behavioral guidelines and ethical
principles. However, these specifications face critical challenges, including
internal conflicts between principles and insufficient coverage of nuanced
scenarios. We present a systematic methodology for stress-testing model
character specifications, automatically identifying numerous cases of principle
contradictions and interpretive ambiguities in current model specs.
  We stress test current model specs by generating scenarios that force
explicit tradeoffs between competing value-based principles. Using a
comprehensive taxonomy we generate diverse value tradeoff scenarios where
models must choose between pairs of legitimate principles that cannot be
simultaneously satisfied. We evaluate responses from twelve frontier LLMs
across major providers (Anthropic, OpenAI, Google, xAI) and measure behavioral
disagreement through value classification scores. Among these scenarios, we
identify over 70,000 cases exhibiting significant behavioral divergence.
Empirically, we show this high divergence in model behavior strongly predicts
underlying problems in model specifications. Through qualitative analysis, we
provide numerous example issues in current model specs such as direct
contradiction and interpretive ambiguities of several principles. Additionally,
our generated dataset also reveals both clear misalignment cases and
false-positive refusals across all of the frontier models we study. Lastly, we
also provide value prioritization patterns and differences of these models.

</details>


### [25] [Large Language Models Meet Virtual Cell: A Survey](https://arxiv.org/abs/2510.07706)
*Krinos Li,Xianglu Xiao,Shenglong Deng,Lucas He,Zijun Zhong,Yuanjie Zou,Zhonghao Zhan,Zheng Hui,Weiye Bao,Guang Yang*

Main category: cs.CL

> 本文综述了大型语言模型在虚拟细胞建模中的应用，提出了两种范式，并概述了三个核心任务及面临的挑战。

<details>
  <summary>Details</summary>

**Motivation:** 随着大型语言模型的进步，它们在细胞生物学领域展现出新的潜力，能够用于构建能够表示、预测和推理细胞状态和行为的“虚拟细胞”。因此，对这一领域进行全面的综述显得尤为重要。

**Method:** 本文提出了一种统一的分类法，将现有方法分为两种范式：作为直接细胞建模的Oracle模式和作为复杂科学任务协调者的Agent模式。

**Result:** 研究归纳了细胞表示、扰动预测、基因调节推断三个核心任务，并分析了相关模型、数据集以及评估基准。

**Conclusion:** 随着大型语言模型应用于细胞建模，研究指出了扩展性、泛化能力和可解释性等关键挑战。

**Abstract:** Large language models (LLMs) are transforming cellular biology by enabling
the development of "virtual cells"--computational systems that represent,
predict, and reason about cellular states and behaviors. This work provides a
comprehensive review of LLMs for virtual cell modeling. We propose a unified
taxonomy that organizes existing methods into two paradigms: LLMs as Oracles,
for direct cellular modeling, and LLMs as Agents, for orchestrating complex
scientific tasks. We identify three core tasks--cellular representation,
perturbation prediction, and gene regulation inference--and review their
associated models, datasets, evaluation benchmarks, as well as the critical
challenges in scalability, generalizability, and interpretability.

</details>


### [26] [Causality Guided Representation Learning for Cross-Style Hate Speech Detection](https://arxiv.org/abs/2510.07707)
*Chengshuai Zhao,Shu Wan,Paras Sheth,Karan Patwa,K. Selçuk Candan,Huan Liu*

Main category: cs.CL

> 论文提出CADET框架来解决已有的仇恨言论检测模型在处理隐性仇恨言论时遇到的挑战，通过因果图分解仇恨言论并控制混杂因素以提高检测效果。

<details>
  <summary>Details</summary>

**Motivation:** 现有的仇恨言论检测模型主要依赖于表面层面的语言线索，在面对复杂的修辞手法和不同的平台特点时，缺乏泛化能力。为了应对这一挑战，作者假设仇恨言论的生成过程可以建模为一个因果图，该图涉及关键因素：上下文环境、创作者动机、目标、和风格。

**Method:** CADET是一种因果表征学习框架，该框架将仇恨言论分解为可解释的潜在因素，并控制混杂因素，从而将真正的仇恨意图与表面上的语言线索区分开来。此外，CADET允许通过在潜在空间中对风格进行干预来进行反事实推理，引导模型稳健地识别各种形式的仇恨言论。

**Result:** CADET在全面的实验中表现出色，证明了因果先验在推进可泛化的仇恨言论检测中的潜力。

**Conclusion:** 该论文提出的方法CADT展示了利用因果关系进行仇恨言论检测的有效性，相较于现有方法，它不仅提升了模型的检测能力，还增强了模型在不同风格和环境下的鲁棒性。

**Abstract:** The proliferation of online hate speech poses a significant threat to the
harmony of the web. While explicit hate is easily recognized through overt
slurs, implicit hate speech is often conveyed through sarcasm, irony,
stereotypes, or coded language -- making it harder to detect. Existing hate
speech detection models, which predominantly rely on surface-level linguistic
cues, fail to generalize effectively across diverse stylistic variations.
Moreover, hate speech spread on different platforms often targets distinct
groups and adopts unique styles, potentially inducing spurious correlations
between them and labels, further challenging current detection approaches.
Motivated by these observations, we hypothesize that the generation of hate
speech can be modeled as a causal graph involving key factors: contextual
environment, creator motivation, target, and style. Guided by this graph, we
propose CADET, a causal representation learning framework that disentangles
hate speech into interpretable latent factors and then controls confounders,
thereby isolating genuine hate intent from superficial linguistic cues.
Furthermore, CADET allows counterfactual reasoning by intervening on style
within the latent space, naturally guiding the model to robustly identify hate
speech in varying forms. CADET demonstrates superior performance in
comprehensive experiments, highlighting the potential of causal priors in
advancing generalizable hate speech detection.

</details>


### [27] [MemWeaver: A Hierarchical Memory from Textual Interactive Behaviors for Personalized Generation](https://arxiv.org/abs/2510.07713)
*Shuo Yu,Mingyue Cheng,Daoyu Wang,Qi Liu,Zirui Liu,Ze Guo,Xiaoyu Tao*

Main category: cs.CL

> 本文提出MemWeaver框架，将用户的文本历史编织成一个分层记忆，以实现深度个性化生成，并在LaMP基准上验证了有效。

<details>
  <summary>Details</summary>

**Motivation:** 文章动机是用户互联网互动方式的变化从隐式反馈转向显式文本反馈，传统的个性化策略对待用户历史方式较浅，未能捕捉深层的时序和语义结构。因此提出MemWeaver框架来更深地挖掘用户的个性化信息。

**Method:** 本文提出了MemWeaver框架，将用户的完整文本历史编织成一个分层记忆，以支持深度个性化的生成。该记忆的核心创新在于能够捕捉兴趣的时间演变和不同活动之间的语义关系。MemWeaver构建了两种互补的记忆组件：行为记忆和认知记忆，它们在不同的抽象层次上整合时间信息和语义信息，共同为大型语言模型提供了一个统一的用户表示，以支持具体行为和抽象特性的推理。

**Result:** 实验结果在语言模型个性化（LaMP）基准上验证了MemWeaver的有效性。

**Conclusion:** 通过MemWeaver框架，作者证明了正在改进的记忆形式和双组分记忆架构能够提高个性化语言模型的性能。

**Abstract:** The primary form of user-internet engagement is shifting from leveraging
implicit feedback signals, such as browsing and clicks, to harnessing the rich
explicit feedback provided by textual interactive behaviors. This shift unlocks
a rich source of user textual history, presenting a profound opportunity for a
deeper form of personalization. However, prevailing approaches offer only a
shallow form of personalization, as they treat user history as a flat list of
texts for retrieval and fail to model the rich temporal and semantic structures
reflecting dynamic nature of user interests. In this work, we propose
\textbf{MemWeaver}, a framework that weaves the user's entire textual history
into a hierarchical memory to power deeply personalized generation. The core
innovation of our memory lies in its ability to capture both the temporal
evolution of interests and the semantic relationships between different
activities. To achieve this, MemWeaver builds two complementary memory
components that both integrate temporal and semantic information, but at
different levels of abstraction: behavioral memory, which captures specific
user actions, and cognitive memory, which represents long-term preferences.
This dual-component memory serves as a unified representation of the user,
allowing large language models (LLMs) to reason over both concrete behaviors
and abstracted traits. Experiments on the Language Model Personalization (LaMP)
benchmark validate the efficacy of MemWeaver. Our code is
available\footnote{https://github.com/fishsure/MemWeaver}.

</details>


### [28] [SUBQRAG: sub-question driven dynamic graph rag](https://arxiv.org/abs/2510.07718)
*Jiaoyang Li,Junhao Ruan,Shengwei Tang,Saihan Chen,Kaiyan Chang,Yuan Ge,Tong Xiao,Jingbo Zhu*

Main category: cs.CL

> 提出了一种叫做SubQRAG的框架，通过将复杂问题分解成子问题链来增强多跳问题回答中的深层数理能力。实验表明，这种方法在精确匹配度上显著优于现有方法。

<details>
  <summary>Details</summary>

**Motivation:** 图检索增强生成（Graph RAG）虽然能够有效构建知识图谱，但其宽泛视角的方法往往缺乏处理复杂多跳问题需要的深层次结构化推理，导致证据不完整和错误累积。为了应对这些局限，提出了SubQRAG。

**Method:** SubQRAG, 一种基于子问题驱动的框架，通过将复杂问题分解成有序的可验证子问题链来增强推理深度。对于每个子问题，它会从图中检索相关三元组。当现有图不足时，系统会通过实时从源文档中提取新的三元组来动态扩展图。用于推理过程的所有三元组将被整合到'图记忆'中，形成最终答案生成的结构化且可追踪的证据路径。

**Result:** 在三个多跳问题回答基准测试上的实验表明，SubQRAG 在精确匹配得分方面取得了一致且显著的改进。

**Conclusion:** SubQRAG通过分解复杂问题并实时扩展知识图谱，实现了多跳问题回答中的更深层次推理，尤其在精确匹配评分上显著提升。

**Abstract:** Graph Retrieval-Augmented Generation (Graph RAG) effectively builds a
knowledge graph (KG) to connect disparate facts across a large document corpus.
However, this broad-view approach often lacks the deep structured reasoning
needed for complex multi-hop question answering (QA), leading to incomplete
evidence and error accumulation. To address these limitations, we propose
SubQRAG, a sub-question-driven framework that enhances reasoning depth. SubQRAG
decomposes a complex question into an ordered chain of verifiable
sub-questions. For each sub-question, it retrieves relevant triples from the
graph. When the existing graph is insufficient, the system dynamically expands
it by extracting new triples from source documents in real time. All triples
used in the reasoning process are aggregated into a "graph memory," forming a
structured and traceable evidence path for final answer generation. Experiments
on three multi-hop QA benchmarks demonstrate that SubQRAG achieves consistent
and significant improvements, especially in Exact Match scores.

</details>


### [29] [Multilingual Knowledge Graph Completion via Efficient Multilingual Knowledge Sharing](https://arxiv.org/abs/2510.07736)
*Cunli Mao,Xiaofei Gao,Ran Song,Shizhu He,Shengxiang Gao,Kang Liu,Zhengtao Yu*

Main category: cs.CL

> Proposed a novel MKGC framework with KL-GMoE and IER, achieving notable performance improvements by sharing multilingual knowledge, outperforming existing SOTA MKGC methods in extensive experiments.

<details>
  <summary>Details</summary>

**Motivation:** To address the underutilization of LLMs' multilingual capabilities and the lack of cross-lingual knowledge shareability in current MKGC research.

**Method:** Large language models (LLMs) based Multilingual Knowledge Graph Completion (MKGC) utilizing multilingual shared knowledge. The approach consists of two key components: Knowledge-level Grouped Mixture of Experts (KL-GMoE) and Iterative Entity Reranking (IER).

**Result:** Experimental evaluation on a multilingual KG dataset (containing 5 languages) showed performance improvements of 5.47%, 3.27%, and 1.01% in Hits@1, Hits@3, and Hits@10 metrics, respectively, compared to the state-of-the-art MKGC method.

**Conclusion:** The proposed MKGC framework, leveraging multilingual shared knowledge through KL-GMoE and IER, significantly enhances the performance of multilingual KG completion, especially in handling unseen and unbalanced languages.

**Abstract:** Large language models (LLMs) based Multilingual Knowledge Graph Completion
(MKGC) aim to predict missing facts by leveraging LLMs' multilingual
understanding capabilities, improving the completeness of multilingual
knowledge graphs (KGs). However, existing MKGC research underutilizes the
multilingual capabilities of LLMs and ignores the shareability of cross-lingual
knowledge. In this paper, we propose a novel MKGC framework that leverages
multilingual shared knowledge to significantly enhance performance through two
components: Knowledge-level Grouped Mixture of Experts (KL-GMoE) and Iterative
Entity Reranking (IER). KL-GMoE efficiently models shared knowledge, while IER
significantly enhances its utilization. To evaluate our framework, we
constructed a mKG dataset containing 5 languages and conducted comprehensive
comparative experiments with existing state-of-the-art (SOTA) MKGC method. The
experimental results demonstrate that our framework achieves improvements of
5.47%, 3.27%, and 1.01% in the Hits@1, Hits@3, and Hits@10 metrics,
respectively, compared with SOTA MKGC method. Further experimental analysis
revealed the properties of knowledge sharing in settings of unseen and
unbalanced languages. We have released the dataset and code for our work on
https://github.com/gaoxiaofei07/KL-GMoE.

</details>


### [30] [ToolExpander: Extending the Frontiers of Tool-Using Reinforcement Learning to Weak LLMs](https://arxiv.org/abs/2510.07737)
*Fu Chen,Peng Wang,Xiyin Li,Wen Li,Shichi Lei,Dongdong Xiang*

Main category: cs.CL

> 为了解决GRPO训练大规模语言模型时的问题，提出ToolExpander框架，通过两种创新改进面向工具的强化学习，实验表明该方法显著提高小规模模型的性能和稳定性。

<details>
  <summary>Details</summary>

**Motivation:** 解决大规模语言模型在使用组相对策略优化（GRPO）时遇到的问题，尤其是小规模架构中难以产生准确响应的问题，这些问题会影响性能提升和模型训练的稳定性。

**Method:** ToolExpander框架通过两种创新来改进面向工具的强化学习：1) 动态多轮硬采样，使用高质量的少样本演示替换训练中难以处理的样本，并结合指数学习率衰减策略来减少振荡；2) 自举证明思考，增强的GRPO框架，消除了KL散度并调整了剪辑系数，鼓励模型自主生成和分析少样本示例。

**Result:** 实验结果表明，ToolExpander显著提高了大规模语言模型的工具使用能力，特别是在较弱的小规模模型中，提高了训练稳定性和整体性能。

**Conclusion:** ToolExpander框架在提高小规模语言模型工具使用能力和训练稳定性方面显示出显著效果，从而解决GRPO训练中的常见问题。

**Abstract:** Training Large Language Models (LLMs) with Group Relative Policy Optimization
(GRPO) encounters a significant challenge: models often fail to produce
accurate responses, particularly in small-scale architectures. This limitation
not only diminishes performance improvements and undermines the potential of
GRPO but also frequently leads to mid-training collapse, adversely affecting
stability and final efficacy. To address these issues, we propose ToolExpander,
a novel framework that advances tool-oriented reinforcement learning for
resource-constrained LLMs through two key innovations:(1) Dynamic Multi-Round
Hard Sampling, which dynamically substitutes challenging samples(those without
correct outputs over 10 rollouts) with high-quality few-shot demonstrations
during training, coupled with an exponential learning rate decay strategy to
mitigate oscillations;(2) Self-Exemplifying Thinking, an enhanced GRPO
framework that eliminates KL divergence and incorporates adjusted clipping
coefficients, encouraging models to autonomously generate and analyze few-shot
examples via a minimal additional reward (0.01).Experimental results
demonstrate that ToolExpander significantly enhances tool-using capabilities in
LLMs, especially in weaker small-scale models, improving both training
stability and overall performance.

</details>


### [31] [OpenRubrics: Towards Scalable Synthetic Rubric Generation for Reward Modeling and LLM Alignment](https://arxiv.org/abs/2510.07743)
*Tianci Liu,Ran Xu,Tony Yu,Ilgee Hong,Carl Yang,Tuo Zhao,Haoyu Wang*

Main category: cs.CL

> 本文提出了OpenRubrics和对比式评分生成CRG，以产生可靠且可扩展的rubric，并借此改进奖励模型，显著提高了模型性能。

<details>
  <summary>Details</summary>

**Motivation:** 当前的奖励模型大多依赖于标量或成对的判断，无法捕捉人类偏好的多方面性。因此，研究基于评分标准奖励模型（RaR）来捕捉多个维度的响应质量的多方面挑战，生产可靠且可扩展的评分标准是一项关键任务。

**Method:** 我们提出了OpenRubrics，这是一个多样化的、大规模的(prompt, rubric)对集合，用于训练rubric生成和基于rubric的奖励模型。我们引入了对比式评分生成(CRG)，通过对比优选和被拒的回答来衍生出明确的规则和隐含的原则。此外，我们通过拒绝采样来确保偏好标签的一致性，从而提高可靠性。

**Result:** 在多个奖励模型基准测试中，我们的基于rubric的奖励模型Rubric-RM超过了尺寸匹配的强大基线模型，提高了6.8%。这些改进也应用到了指令遵循和生物医学基准测试的策略模型上。

**Conclusion:** 研究表明，评分标准提供了可扩展的一致信号，缩小了代价高昂的人类评价和自动奖励建模之间的差距，从而为LLM对其对齐提供了一个新的以原则为导向的范例。

**Abstract:** Reward modeling lies at the core of reinforcement learning from human
feedback (RLHF), yet most existing reward models rely on scalar or pairwise
judgments that fail to capture the multifaceted nature of human preferences.
Recent studies have explored rubrics-as-rewards (RaR) that uses structured
natural language criteria that capture multiple dimensions of response quality.
However, producing rubrics that are both reliable and scalable remains a key
challenge. In this work, we introduce OpenRubrics, a diverse, large-scale
collection of (prompt, rubric) pairs for training rubric-generation and
rubric-based reward models. To elicit discriminative and comprehensive
evaluation signals, we introduce Contrastive Rubric Generation (CRG), which
derives both hard rules (explicit constraints) and principles (implicit
qualities) by contrasting preferred and rejected responses. We further improve
reliability by enforcing preference-label consistency via rejection sampling to
remove noisy rubrics. Across multiple reward-modeling benchmarks, our
rubric-based reward model, Rubric-RM, surpasses strong size-matched baselines
by 6.8%. These gains transfer to policy models on instruction-following and
biomedical benchmarks. Our results show that rubrics provide scalable alignment
signals that narrow the gap between costly human evaluation and automated
reward modeling, enabling a new principle-driven paradigm for LLM alignment.

</details>


### [32] [Parallel Test-Time Scaling for Latent Reasoning Models](https://arxiv.org/abs/2510.07745)
*Runyang You,Yongqi Li,Meng Liu,Wenjie Wang,Liqiang Nie,Wenjie Li*

Main category: cs.CL

> 论文提出了一种用于潜在推理模型的并行测试时间扩展方法，引入了新的采样策略和聚合技术，从而使得潜在推理模型也能受益于并行 TTS。

<details>
  <summary>Details</summary>

**Motivation:** 由于缺乏在连续空间中的采样机制和高级轨迹聚合的概率信号，现有的隐式推理模型能否像显式的链式思考模型一样通过并行测试时间扩展（TTS）获益仍然是开放问题。本文旨在解决这些问题，并使潜在推理模型也能从并行TTS中受益。

**Method:** 此论文引入了两种基于不确定性的采样策略：蒙特卡洛 dropout 和加性高斯噪声，以解决在连续空间中缺少采样机制的问题。同时设计了一个基于对比目标训练的潜在奖励模型（LatentRM）来评估和指导潜在推理。

**Result:** 广泛的实验表明，两种采样策略都能有效地扩展计算资源，并展现出不同的探索动态特性。LatentRM 能够实现有效的轨迹选择。

**Conclusion:** 该论文的研究为在连续空间中实现可扩展推理开辟了新的研究方向。

**Abstract:** Parallel test-time scaling (TTS) is a pivotal approach for enhancing large
language models (LLMs), typically by sampling multiple token-based
chains-of-thought in parallel and aggregating outcomes through voting or
search. Recent advances in latent reasoning, where intermediate reasoning
unfolds in continuous vector spaces, offer a more efficient alternative to
explicit Chain-of-Thought, yet whether such latent models can similarly benefit
from parallel TTS remains open, mainly due to the absence of sampling
mechanisms in continuous space, and the lack of probabilistic signals for
advanced trajectory aggregation. \ This work enables parallel TTS for latent
reasoning models by addressing the above issues. For sampling, we introduce two
uncertainty-inspired stochastic strategies: Monte Carlo Dropout and Additive
Gaussian Noise. For aggregation, we design a Latent Reward Model (LatentRM)
trained with step-wise contrastive objective to score and guide latent
reasoning. Extensive experiments and visualization analyses show that both
sampling strategies scale effectively with compute and exhibit distinct
exploration dynamics, while LatentRM enables effective trajectory selection.
Together, our explorations open a new direction for scalable inference in
continuous spaces. Code released at https://github.com/YRYangang/LatentTTS.

</details>


### [33] [Test-Time Reasoners Are Strategic Multiple-Choice Test-Takers](https://arxiv.org/abs/2510.07761)
*Nishant Balepur,Atrey Desai,Rachel Rudinger*

Main category: cs.CL

> 研究探讨了具备推理能力的大语言模型在解决多项选择题时的策略，尤其是在仅使用选项的情况下，发现其成功率并未显著下降，并且这些模型能够通过可信度测试，表明它们采用了较少有问题的策略。

<details>
  <summary>Details</summary>

**Motivation:** 尽管已有研究表明，不使用推理的大语言模型在仅使用选项的情况下也能在多项选择题上取得成功，但这种成功的可靠性受到质疑。这项研究旨在通过分析具备推理能力的大语言模型的推理路径，探究它们在仅使用选项时采用的策略是否为浅层策略。

**Method:** 通过让具备推理能力的大语言模型在全输入和仅选项输入下解决多项选择题，来研究它们的解题策略。

**Result:** 具备推理能力的模型在解决多项选择题时，即使在仅使用选项的情况下也能取得一定程度的成功。这种情况下，推理路径的长度对其成功率影响不大，并且这些模型能够通过可信度测试，表明它们采用了较少有问题的策略，例如推断丢失的问题。

**Conclusion:** 研究挑战了部分输入成功总是缺陷的观点，并讨论了如何通过推理路径来区分有问题的数据和较少有问题的推理。

**Abstract:** Large language models (LLMs) now give reasoning before answering, excelling
in tasks like multiple-choice question answering (MCQA). Yet, a concern is that
LLMs do not solve MCQs as intended, as work finds LLMs sans reasoning succeed
in MCQA without using the question, i.e., choices-only. Such partial-input
success is often deemed problematic, but reasoning traces could reveal if these
strategies are truly shallow in choices-only settings. To study these
strategies, reasoning LLMs solve MCQs in full and choices-only inputs;
test-time reasoning often boosts accuracy on full and in choices-only half the
time. While possibly due to shallow shortcuts, choices-only success is barely
affected by the length of reasoning traces, and after finding traces pass
faithfulness tests, we show they use less problematic strategies like inferring
missing questions. In all, we challenge claims that partial-input success is
always a flaw, so we discuss how reasoning traces could separate problematic
data from less problematic reasoning.

</details>


### [34] [ToolLibGen: Scalable Automatic Tool Creation and Aggregation for LLM Reasoning](https://arxiv.org/abs/2510.07768)
*Murong Yue,Zhiwei Liu,Liangwei Yang,Jianguo Zhang,Zuxin Liu,Haolin Chen,Ziyu Yao,Silvio Savarese,Caiming Xiong,Shelby Heinecke,Huan Wang*

Main category: cs.CL

> 本文提出的方法可以自动将大量针对特定问题的工具整合成少数量多功能的聚合工具，提高工具检索准确性和整体推理性能。

<details>
  <summary>Details</summary>

**Motivation:** 广泛采用工具增强推理受到领域特定工具稀缺的限制。特别是，对于推理任务如物理问答，合适的特定工具往往缺失。现有工作面临生成工具数量增加带来的存储和检索挑战。

**Method:** 本文提出了一种系统的方法，可以将无结构的工具集合自动重构为结构化的工具库。系统首先生成特定任务的离散工具，并将它们聚类为语义连贯的主题。在每个聚类中，引入了一个多代理框架来整合分散的功能：代码代理重构代码以提取共享逻辑并创建多功能的聚合工具，而审核代理确保这些聚合工具保持原有完整功能。

**Result:** 实验结果表明，本文方法在多种推理任务中显著提高了工具检索准确性和整体推理性能。此外，随着特定问题数量的增加，本文方法相较于基线方法展示出增强的可扩展性。

**Conclusion:** 本文提出的方法实现将无结构工具集自动重构为结构化的工具库，显著改善了工具检索性能和推理性能，并在可扩展性方面优于现有方法。

**Abstract:** Large Language Models (LLMs) equipped with external tools have demonstrated
enhanced performance on complex reasoning tasks. The widespread adoption of
this tool-augmented reasoning is hindered by the scarcity of domain-specific
tools. For instance, in domains such as physics question answering, suitable
and specialized tools are often missing. Recent work has explored automating
tool creation by extracting reusable functions from Chain-of-Thought (CoT)
reasoning traces; however, these approaches face a critical scalability
bottleneck. As the number of generated tools grows, storing them in an
unstructured collection leads to significant retrieval challenges, including an
expanding search space and ambiguity between function-related tools. To address
this, we propose a systematic approach to automatically refactor an
unstructured collection of tools into a structured tool library. Our system
first generates discrete, task-specific tools and clusters them into
semantically coherent topics. Within each cluster, we introduce a multi-agent
framework to consolidate scattered functionalities: a code agent refactors code
to extract shared logic and creates versatile, aggregated tools, while a
reviewing agent ensures that these aggregated tools maintain the complete
functional capabilities of the original set. This process transforms numerous
question-specific tools into a smaller set of powerful, aggregated tools
without loss of functionality. Experimental results demonstrate that our
approach significantly improves tool retrieval accuracy and overall reasoning
performance across multiple reasoning tasks. Furthermore, our method shows
enhanced scalability compared with baselines as the number of question-specific
increases.

</details>


### [35] [Curing Miracle Steps in LLM Mathematical Reasoning with Rubric Rewards](https://arxiv.org/abs/2510.07774)
*Youliang Yuan,Qiuyang Mang,Jingbang Chen,Hong Wan,Xiaoyuan Liu,Junjielong Xu,Jen-tse Huang,Wenxuan Wang,Wenxiang Jiao,Pinjia He*

Main category: cs.CL

> 本文指出针对数学理性的大模型利用基于结果的奖励方法训练会高估其推理能力，因此提出了基于评分标准的RRM过程奖励方法，提高了模型的准确性和可靠性。

<details>
  <summary>Details</summary>

**Motivation:** 传统的基于结果的奖励训练方法容易导致奖励作弊，会导致模型的推理能力被高估。通过高频率的假阳性，解决方案通过不正当的推理过程也能得出正确答案。

**Method:** 系统性分析结合人工验证，提出了一种针对解决方案过程的评分奖励模型（RRM），该模型会根据问题特定的评分标准来评估整个推理过程，并给予精确的奖励。通过集成到强化学习管道中，这种方法可以在四个数学基准测试中持续优于仅基于结果的监督方法。

**Result:** 采用RRM奖励模型训练的大模型显著提高了准确性，减少了奇迹步骤的发生，具体提高了AIME2024的通过率，并减少71%的奇迹步骤。

**Conclusion:** 大模型估算在数学推理上的真实性与准确性，决定性因素在于奖励过程而非只是奖励结果，采用RRM模型能提高模型的准确度和可靠性。

**Abstract:** Large language models for mathematical reasoning are typically trained with
outcome-based rewards, which credit only the final answer. In our experiments,
we observe that this paradigm is highly susceptible to reward hacking, leading
to a substantial overestimation of a model's reasoning ability. This is
evidenced by a high incidence of false positives - solutions that reach the
correct final answer through an unsound reasoning process. Through a systematic
analysis with human verification, we establish a taxonomy of these failure
modes, identifying patterns like Miracle Steps - abrupt jumps to a correct
output without a valid preceding derivation. Probing experiments suggest a
strong association between these Miracle Steps and memorization, where the
model appears to recall the answer directly rather than deriving it. To
mitigate this systemic issue, we introduce the Rubric Reward Model (RRM), a
process-oriented reward function that evaluates the entire reasoning trajectory
against problem-specific rubrics. The generative RRM provides fine-grained,
calibrated rewards (0-1) that explicitly penalize logical flaws and encourage
rigorous deduction. When integrated into a reinforcement learning pipeline,
RRM-based training consistently outperforms outcome-only supervision across
four math benchmarks. Notably, it boosts Verified Pass@1024 on AIME2024 from
26.7% to 62.6% and reduces the incidence of Miracle Steps by 71%. Our work
demonstrates that rewarding the solution process is crucial for building models
that are not only more accurate but also more reliable.

</details>


### [36] [The Unintended Trade-off of AI Alignment:Balancing Hallucination Mitigation and Safety in LLMs](https://arxiv.org/abs/2510.07775)
*Omar Mahmoud,Ali Khalil,Buddhika Laknath Semage,Thommen George Karimpanal,Santu Rana*

Main category: cs.CL

> The paper tackles the issue of improving truthfulness negatively impacting safety alignment in large language models by proposing a method to disentangle refusal and hallucination components.

<details>
  <summary>Details</summary>

**Motivation:** The paper aims to investigate the trade-off between enhancing truthfulness and safety alignment in large language models, which occurs due to overlapping components encoding both hallucination and refusal information.

**Method:** Our approach involves using sparse autoencoders to disentangle refusal-related features from hallucination features and preserving refusal behavior during fine-tuning through subspace orthogonalization.

**Result:** The evaluation on commonsense reasoning tasks and harmful benchmarks shows that the method preserves refusal behavior and task utility, effectively managing the trade-off.

**Conclusion:** The proposed method successfully mitigates the trade-off between truthfulness and safety, preserving refusal behavior and task utility.

**Abstract:** Hallucination in large language models (LLMs) has been widely studied in
recent years, with progress in both detection and mitigation aimed at improving
truthfulness. Yet, a critical side effect remains largely overlooked: enhancing
truthfulness can negatively impact safety alignment. In this paper, we
investigate this trade-off and show that increasing factual accuracy often
comes at the cost of weakened refusal behavior. Our analysis reveals that this
arises from overlapping components in the model that simultaneously encode
hallucination and refusal information, leading alignment methods to suppress
factual knowledge unintentionally. We further examine how fine-tuning on benign
datasets, even when curated for safety, can degrade alignment for the same
reason. To address this, we propose a method that disentangles refusal-related
features from hallucination features using sparse autoencoders, and preserves
refusal behavior during fine-tuning through subspace orthogonalization. This
approach prevents hallucinations from increasing while maintaining safety
alignment.We evaluate our method on commonsense reasoning tasks and harmful
benchmarks (AdvBench and StrongReject). Results demonstrate that our approach
preserves refusal behavior and task utility, mitigating the trade-off between
truthfulness and safety.

</details>


### [37] [Instance Relation Learning Network with Label Knowledge Propagation for Few-shot Multi-label Intent Detection](https://arxiv.org/abs/2510.07776)
*Shiman Zhao,Shangyuan Li,Wei Chen,Tengjiao Wang,Jiahui Yao,Jiabin Zheng,Kam Fai Wong*

Main category: cs.CL

> Proposes a multi-label joint learning method to enhance few-shot multi-label intent detection, demonstrating significant improvements over existing methods in 1-shot scenarios.

<details>
  <summary>Details</summary>

**Motivation:** solve the error propagation issue in previous two-stage pipeline methods and improve performance in few-shot multi-label intent detection.

**Method:** multi-label joint learning method for few-shot MID in an end-to-end manner, which constructs an instance relation learning network with label knowledge propagation to eliminate error propagation.

**Result:** outperform strong baselines by an average of 9.54% AUC and 11.19% Macro-F1 in 1-shot scenarios.

**Conclusion:** the proposed multi-label joint learning method with instance relation learning network and label knowledge propagation can effectively improve performance in few-shot multi-label intent detection tasks.

**Abstract:** Few-shot Multi-label Intent Detection (MID) is crucial for dialogue systems,
aiming to detect multiple intents of utterances in low-resource dialogue
domains. Previous studies focus on a two-stage pipeline. They first learn
representations of utterances with multiple labels and then use a
threshold-based strategy to identify multi-label results. However, these
methods rely on representation classification and ignore instance relations,
leading to error propagation. To solve the above issues, we propose a
multi-label joint learning method for few-shot MID in an end-to-end manner,
which constructs an instance relation learning network with label knowledge
propagation to eliminate error propagation. Concretely, we learn the
interaction relations between instances with class information to propagate
label knowledge between a few labeled (support set) and unlabeled (query set)
instances. With label knowledge propagation, the relation strength between
instances directly indicates whether two utterances belong to the same intent
for multi-label prediction. Besides, a dual relation-enhanced loss is developed
to optimize support- and query-level relation strength to improve performance.
Experiments show that we outperform strong baselines by an average of 9.54% AUC
and 11.19% Macro-F1 in 1-shot scenarios.

</details>


### [38] [Drift No More? Context Equilibria in Multi-Turn LLM Interactions](https://arxiv.org/abs/2510.07777)
*Vardhan Dongre,Ryan A. Rossi,Viet Dac Lai,David Seunghyun Yoon,Dilek Hakkani-Tür,Trung Bui*

Main category: cs.CL

> 通过递归模型，研究了大型语言模型在多轮次任务中的上下文漂移，发现可通过简单干预控制漂移，无需担心性能不可避免地衰退。

<details>
  <summary>Details</summary>

**Motivation:** 当前，大语言模型虽然在单轮交互任务中表现出色，但现实部署要求可持续且多轮次交互，这需要模型在对话中保持和调整用户目标和上下文，而上下文漂移是一个挑战。

**Method:** 通过将多轮次交互中的上下文漂移形式化为测试模型与目标一致参考模型之间基于时间步长的KL散度，并提出一个递归模型来解释其演变，该模型认为这是一个具有恢复力和可控干预的受限随机过程。

**Result:** 实验使用了开放权重的大型语言模型，作为用户模拟器，显示了在长周期改写任务和现实用户代理模拟中的稳定平衡而不是失控退化，简单的提示干预能有效减少差异。

**Conclusion:** 研究表明，多轮次交互中的上下文漂移可以被理解为一个可控的平衡现象，而不是不可避免的衰减，这为研究和缓解长时间互动中的上下文漂移提供了基础。

**Abstract:** Large Language Models (LLMs) excel at single-turn tasks such as instruction
following and summarization, yet real-world deployments require sustained
multi-turn interactions where user goals and conversational context persist and
evolve. A recurring challenge in this setting is context drift: the gradual
divergence of a model's outputs from goal-consistent behavior across turns.
Unlike single-turn errors, drift unfolds temporally and is poorly captured by
static evaluation metrics. In this work, we present a study of context drift in
multi-turn interactions and propose a simple dynamical framework to interpret
its behavior. We formalize drift as the turn-wise KL divergence between the
token-level predictive distributions of the test model and a goal-consistent
reference model, and propose a recurrence model that interprets its evolution
as a bounded stochastic process with restoring forces and controllable
interventions. We instantiate this framework in both synthetic long-horizon
rewriting tasks and realistic user-agent simulations such as in $\tau$-Bench,
measuring drift for several open-weight LLMs that are used as user simulators.
Our experiments consistently reveal stable, noise-limited equilibria rather
than runaway degradation, and demonstrate that simple reminder interventions
reliably reduce divergence in line with theoretical predictions. Together,
these results suggest that multi-turn drift can be understood as a controllable
equilibrium phenomenon rather than as inevitable decay, providing a foundation
for studying and mitigating context drift in extended interactions.

</details>


### [39] [RCPU: Rotation-Constrained Error Compensation for Structured Pruning of a Large Language Model](https://arxiv.org/abs/2510.07782)
*Shuichiro Haruta,Kazunori Matsumoto,Zhi Li,Yanan Wang,Mori Kurokawa*

Main category: cs.CL

> The authors propose a rotation-constrained compensation method that combines a variance-aware importance score and geometry-preserving updates to effectively reduce errors caused by structured pruning of large language models, achieving superior perplexity and accuracy compared to baseline models on various benchmarks.

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind this paper is to address the issue of overfitting to a limited calibration set during the pruning process of LLMs, which can lead to output mismatches due to the reduced size of the calibration data as compared to the extensive training data.

**Method:** In this paper, the authors introduce a rotation-constrained compensation method to mitigate errors caused by structured pruning in large language models (LLMs). They preserve the geometry of the output representations while modifying the pruned subspace. Additionally, they design a variance-aware importance score to maintain dimensions with large variance, which are crucial for the principal directions of output.

**Result:** The experimental results show that the proposed method achieves better perplexity and task accuracy on WikiText-2 and multiple language understanding benchmarks when compared to existing pruning methods for the LLaMA-7B model.

**Conclusion:** The paper concludes that the proposed rotation-constrained compensation method is successful in compensating for the errors introduced by pruning large language models, thereby improving the overall performance of the pruned models on language understanding tasks.

**Abstract:** In this paper, we propose a rotation-constrained compensation method to
address the errors introduced by structured pruning of large language models
(LLMs). LLMs are trained on massive datasets and accumulate rich semantic
knowledge in their representation space. In contrast, pruning is typically
carried out with only a small amount of calibration data, which makes output
mismatches unavoidable. Although direct least-squares fitting can reduce such
errors, it tends to overfit to the limited calibration set, destructively
modifying pretrained weights. To overcome this difficulty, we update the pruned
parameters under a rotation constraint. This constrained update preserves the
geometry of output representations (i.e., norms and inner products) and
simultaneously re-aligns the pruned subspace with the original outputs.
Furthermore, in rotation-constrained compensation, removing components that
strongly contribute to the principal directions of the output makes error
recovery difficult. Since input dimensions with large variance strongly affect
these principal directions, we design a variance-aware importance score that
ensures such dimensions are preferentially kept in the pruned model. By
combining this scoring rule with rotation-constrained updates, the proposed
method effectively compensates errors while retaining the components likely to
be more important in a geometry-preserving manner. In the experiments, we apply
the proposed method to LLaMA-7B and evaluate it on WikiText-2 and multiple
language understanding benchmarks. The results demonstrate consistently better
perplexity and task accuracy compared with existing baselines.

</details>


### [40] [LLM4Cell: A Survey of Large Language and Agentic Models for Single-Cell Biology](https://arxiv.org/abs/2510.07793)
*Sajib Acharjee Dip,Adrika Zafor,Bikash Kumar Paul,Uddip Acharjee Shuvo,Muhit Islam Emon,Xuan Wang,Liqing Zhang*

Main category: cs.CL

> LLM4Cell是首个关于单细胞研究的基础和代理模型的统一综述，旨在整合语言驱动的单细胞智能，分析模型在解释性、标准化和可信赖模型开发方面的开放挑战。

<details>
  <summary>Details</summary>

**Motivation:** 研究动机：为了应对当前在单细胞生物学领域中，进展在数据模态、架构和评估标准方面分散的问题，提出LLM4Cell作为第一个整合语言驱动的单细胞智能的综述，分析模型在解释性、标准化和可信赖模型开发方面的开放挑战。

**Method:** 内容概述：大型语言模型(LLMs)和新兴的代理框架正在通过实现自然语言推理、生成注释和多模态数据集成，对单细胞生物学产生变革性的影响。然而，这些进展在整个数据模态、架构和评估标准方面仍然分散。LLM4Cell是首个涵盖58个单细胞研究基础和代理模型的统一综述，涵盖了RNA、ATAC、多组学和空间模态。

**Result:** 研究成果：论文通过分析超过40个公开数据集，研究了模型跨10个领域维度的适配性，覆盖了生物学基础性、多组学协同、公平性、隐私和可解释性。

**Conclusion:** 研究结论：LLM4Cell链接了数据集、模型和评估领域，提供了首个关于语言驱动的单细胞智能的整合视野，并概述了在解释性、标准化和可信模型开发中的开放挑战。

**Abstract:** Large language models (LLMs) and emerging agentic frameworks are beginning to
transform single-cell biology by enabling natural-language reasoning,
generative annotation, and multimodal data integration. However, progress
remains fragmented across data modalities, architectures, and evaluation
standards. LLM4Cell presents the first unified survey of 58 foundation and
agentic models developed for single-cell research, spanning RNA, ATAC,
multi-omic, and spatial modalities. We categorize these methods into five
families-foundation, text-bridge, spatial, multimodal, epigenomic, and
agentic-and map them to eight key analytical tasks including annotation,
trajectory and perturbation modeling, and drug-response prediction. Drawing on
over 40 public datasets, we analyze benchmark suitability, data diversity, and
ethical or scalability constraints, and evaluate models across 10 domain
dimensions covering biological grounding, multi-omics alignment, fairness,
privacy, and explainability. By linking datasets, models, and evaluation
domains, LLM4Cell provides the first integrated view of language-driven
single-cell intelligence and outlines open challenges in interpretability,
standardization, and trustworthy model development.

</details>


### [41] [HiPRAG: Hierarchical Process Rewards for Efficient Agentic Retrieval Augmented Generation](https://arxiv.org/abs/2510.07794)
*Peilin Wu,Mian Zhang,Kun Wan,Wentian Zhao,Kaiyu He,Xinya Du,Zhiyu Chen*

Main category: cs.CL

> 本文介绍了一种新的训练方法HiPRAG，解决了代理RAG中搜索行为效率低下问题，并展示了其在提高搜索效率和准确性方面的有效性。

<details>
  <summary>Details</summary>

**Motivation:** 当前的训练方法依靠基于结果的奖励，缺乏解决代理RAG中搜索行为效率低下问题所需的精细控制。这些问题包括过度搜索和不足搜索，导致不必要的开销和不可靠的结果。

**Method:** 本文提出了HiPRAG，一种用于高效代理RAG的训练方法。该方法引入了一个细粒度、基于知识的过程奖励，通过将智能体的推理轨迹分解成离散、可解析的步骤来实时评估每个搜索决定的必要性。在此基础上，应用了一个分层奖励函数，该函数根据最优搜索和非搜索步骤的比例提供额外奖励，同时使用常见的结果和格式奖励。

**Result:** 在七种不同的问答基准测试中，Qwen2.5和Llama-3.2模型的实验表明，该方法实现了平均准确率65.4%（3B）和67.2%（7B），降低了过度搜索率至2.3%，同时减少了不足搜索率。实验和分析还显示，HiPRAG展示了良好的泛化能力，适用于广泛的强化学习算法、模型家族、大小和类型。

**Conclusion:** 本文工作展示了细粒度控制通过强化学习优化搜索代理推理效率和最优性的潜力和重要性。这些结果证明了优化推理过程本身的重要性，而不仅仅是最终结果。

**Abstract:** Agentic RAG is a powerful technique for incorporating external information
that LLMs lack, enabling better problem solving and question answering.
However, suboptimal search behaviors exist widely, such as over-search
(retrieving information already known) and under-search (failing to search when
necessary), which leads to unnecessary overhead and unreliable outputs. Current
training methods, which typically rely on outcome-based rewards in a RL
framework, lack the fine-grained control needed to address these
inefficiencies. To overcome this, we introduce Hierarchical Process Rewards for
Efficient agentic RAG (HiPRAG), a training methodology that incorporates a
fine-grained, knowledge-grounded process reward into the RL training. Our
approach evaluates the necessity of each search decision on-the-fly by
decomposing the agent's reasoning trajectory into discrete, parsable steps. We
then apply a hierarchical reward function that provides an additional bonus
based on the proportion of optimal search and non-search steps, on top of
commonly used outcome and format rewards. Experiments on the Qwen2.5 and
Llama-3.2 models across seven diverse QA benchmarks show that our method
achieves average accuracies of 65.4% (3B) and 67.2% (7B). This is accomplished
while improving search efficiency, reducing the over-search rate to just 2.3%
and concurrently lowering the under-search rate. These results demonstrate the
efficacy of optimizing the reasoning process itself, not just the final
outcome. Further experiments and analysis demonstrate that HiPRAG shows good
generalizability across a wide range of RL algorithms, model families, sizes,
and types. This work demonstrates the importance and potential of fine-grained
control through RL, for improving the efficiency and optimality of reasoning
for search agents.

</details>


### [42] [Dynamic Generation of Multi-LLM Agents Communication Topologies with Graph Diffusion Models](https://arxiv.org/abs/2510.07799)
*Eric Hanchen Jiang,Guancheng Wan,Sophia Yin,Mengting Li,Yuchen Wu,Xiao Liang,Xinfeng Li,Yizhou Sun,Wei Wang,Kai-Wei Chang,Ying Nian Wu*

Main category: cs.CL

> 本文解决了多智能体系统中通信拓扑设计复杂的问题，提出了GTD框架，该框架能够生成适应性强、效率高的通信拓扑。

<details>
  <summary>Details</summary>

**Motivation:** 现有的多智能体系统的通信拓扑设计主要依赖于静态或人为设计的拓扑结构，这种做法无法灵活应对不同的任务需求，导致要么过度消耗令牌，要么在处理复杂任务时出现性能瓶颈。因此，作者提出了GTD框架来解决这一问题。

**Method:** 该论文提出了一种名为Guided Topology Diffusion (GTD)的新框架。通过迭代构造过程并利用轻量级代理模型预测多目标奖励，GTD可以在实时无梯度优化下生成任务自适应的通信拓扑。

**Result:** 实验结果显示，GTD框架能够生成高度任务自适应的、稀疏且高效的通信拓扑结构，并且在大规模语言模型代理间的合作中明显优于现有方法。

**Conclusion:** 提出的GTD框架显著提升了LLM驱动的多智能体系统中的通信效率。实验验证了该方法的有效性和优越性，适用于生成复杂的、任务自适应的通信拓扑。

**Abstract:** The efficiency of multi-agent systems driven by large language models (LLMs)
largely hinges on their communication topology. However, designing an optimal
topology is a non-trivial challenge, as it requires balancing competing
objectives such as task performance, communication cost, and robustness.
Existing frameworks often rely on static or hand-crafted topologies, which
inherently fail to adapt to diverse task requirements, leading to either
excessive token consumption for simple problems or performance bottlenecks for
complex ones. To address this challenge, we introduce a novel generative
framework called \textit{Guided Topology Diffusion (GTD)}. Inspired by
conditional discrete graph diffusion models, GTD formulates topology synthesis
as an iterative construction process. At each step, the generation is steered
by a lightweight proxy model that predicts multi-objective rewards (e.g.,
accuracy, utility, cost), enabling real-time, gradient-free optimization
towards task-adaptive topologies. This iterative, guided synthesis process
distinguishes GTD from single-step generative frameworks, enabling it to better
navigate complex design trade-offs. We validated GTD across multiple
benchmarks, and experiments show that this framework can generate highly
task-adaptive, sparse, and efficient communication topologies, significantly
outperforming existing methods in LLM agent collaboration.

</details>


### [43] [Multilingual Generative Retrieval via Cross-lingual Semantic Compression](https://arxiv.org/abs/2510.07812)
*Yuxin Huang,Simeng Wu,Ran Song,Yan Xiang,Yantuan Xian,Shengxiang Gao,Zhengtao Yu*

Main category: cs.CL

> 本文提出了一种针对跨语言标识符错位和膨胀问题的多语言生成检索新框架 MGR-CSC，该框架通过语义等价关键词统合成共享原子来对齐语义并压缩标识空间，提升检索精度并减少文档标识符长度。

<details>
  <summary>Details</summary>

**Motivation:** 生成式信息检索在单语场景中表现出色，但在多语言检索中仍存在跨语言标识符错位和标识符膨胀两大挑战。为了应对这些挑战，从而设计了该方法。

**Method:** 提出了一种通过跨语言语义压缩（MGR-CSC）实现多语言生成检索的新框架。该框架将语义等价的多语言关键词统合成共享原子来对齐语义并压缩标志空间，并提出了一种动态多步约束解码策略。

**Result:** 实验结果表明，MGR-CSC 在 mMarco100k 和 mNQ320k 上分别提高了 6.83% 和 4.77% 的检索精度，同时分别将文档标识符长度减少至 74.51% 和 78.2%。

**Conclusion:** MGR-CSC 通过一致分配标识符提高了跨语言对齐，并通过减少冗余提高了解码效率。

**Abstract:** Generative Information Retrieval is an emerging retrieval paradigm that
exhibits remarkable performance in monolingual scenarios.However, applying
these methods to multilingual retrieval still encounters two primary
challenges, cross-lingual identifier misalignment and identifier inflation. To
address these limitations, we propose Multilingual Generative Retrieval via
Cross-lingual Semantic Compression (MGR-CSC), a novel framework that unifies
semantically equivalent multilingual keywords into shared atoms to align
semantics and compresses the identifier space, and we propose a dynamic
multi-step constrained decoding strategy during retrieval. MGR-CSC improves
cross-lingual alignment by assigning consistent identifiers and enhances
decoding efficiency by reducing redundancy. Experiments demonstrate that
MGR-CSC achieves outstanding retrieval accuracy, improving by 6.83% on
mMarco100k and 4.77% on mNQ320k, while reducing document identifiers length by
74.51% and 78.2%, respectively.

</details>


### [44] [AdaSwitch: Adaptive Switching Generation for Knowledge Distillation](https://arxiv.org/abs/2510.07842)
*Jingyu Peng,Maolin Wang,Hengyi Cai,Yuchen Li,Kai Zhang,Shuaiqiang Wang,Dawei Yin,Xiangyu Zhao*

Main category: cs.CL

> 研究提出了一种名为 AdaSwitch 的方法，它可以同时保持训练生成的一致性和监督的质量，从而提高小语言模型的性能。实验证明了这种方法的有效性和实用性。

<details>
  <summary>Details</summary>

**Motivation:** 小语言模型(SLMs)在延迟和计算约束严格的应用中非常重要，但要实现高性能仍然是具有挑战性的。现有的知识蒸馏方法存在权衡：离线策略蒸馏提供高质量的监督但引入了训练推理不匹配，而在线方法保持一致性但依赖低质量的学生输出。AdaSwitch 的动机是解决这些权衡问题。

**Method:** AdaSwitch 是一种新颖的方法，它能够在令牌级别动态组合在线和离线策略生成。AdaSwitch 允许学生模型首先探索自己的预测，然后基于实时质量评估选择性地整合教师指导。这个方法同时保持了一致性和监督质量。

**Result:** 在三个数据集和两对教师-学生大型语言模型上的实验表明，AdaSwitch 能够一致地提高准确性，提供了一种实用且有效的方法来蒸馏小语言模型，并且计算开销在可接受范围内。

**Conclusion:** AdaSwitch 为蒸馏小语言模型提供了一种同时保持训练生成一致性和高质量监督的方法，该方法表现出色并且计算开销可控。

**Abstract:** Small language models (SLMs) are crucial for applications with strict latency
and computational constraints, yet achieving high performance remains
challenging. Knowledge distillation (KD) can transfer capabilities from large
teacher models, but existing methods involve trade-offs: off-policy
distillation provides high-quality supervision but introduces a
training-inference mismatch, while on-policy approaches maintain consistency
but rely on low-quality student outputs. To address these issues, we propose
AdaSwitch, a novel approach that dynamically combines on-policy and off-policy
generation at the token level. AdaSwitch allows the student to first explore
its own predictions and then selectively integrate teacher guidance based on
real-time quality assessment. This approach simultaneously preserves
consistency and maintains supervision quality. Experiments on three datasets
with two teacher-student LLM pairs demonstrate that AdaSwitch consistently
improves accuracy, offering a practical and effective method for distilling
SLMs with acceptable additional overhead.

</details>


### [45] [Ready to Translate, Not to Represent? Bias and Performance Gaps in Multilingual LLMs Across Language Families and Domains](https://arxiv.org/abs/2510.07877)
*Md. Faiyaz Abdullah Sayeedi,Md. Mahbub Alam,Subhey Sadi Rahman,Md. Adnanul Islam,Jannatul Ferdous Deepti,Tasnim Mohiuddin,Md Mofijul Islam,Swakkhar Shatabda*

Main category: cs.CL

> 介绍Translation Tangles框架和数据集以评估开源LLMs在翻译质量和公平性方面的表现，并提出一种混合偏差检测方法。

<details>
  <summary>Details</summary>

**Motivation:** 尽管LLMs在机器翻译方面表现出色，但在不同语言家族和专业领域中的性能表现参差不齐，并且它们在训练数据中编码并放大不同偏见的问题严重，尤其是在资源较少的语言中。为了应对这些问题，我们设计了此框架和数据集。

**Method:** 我们提出了Translation Tangles，一个统一的框架和数据集，用于评估开源LLMs的翻译质量和公平性。我们的方法涉及24种双向语言对的跨多个领域的基准测试，并采用不同的评估指标。此外，我们提出了一种混合偏差检测管道，结合了基于规则的启发式方法、语义相似性过滤以及LLM验证。

**Result:** 创建了一个包含1,439个翻译-参考对的高质量偏差标注数据集，并在GitHub上公开了代码和数据集。

**Conclusion:** Translation Tangles框架和数据集为评估LLMs在翻译质量和公平性方面的表现提供了一个基准测试平台，并有助于解决LLMs的偏差问题。

**Abstract:** The rise of Large Language Models (LLMs) has redefined Machine Translation
(MT), enabling context-aware and fluent translations across hundreds of
languages and textual domains. Despite their remarkable capabilities, LLMs
often exhibit uneven performance across language families and specialized
domains. Moreover, recent evidence reveals that these models can encode and
amplify different biases present in their training data, posing serious
concerns for fairness, especially in low-resource languages. To address these
gaps, we introduce Translation Tangles, a unified framework and dataset for
evaluating the translation quality and fairness of open-source LLMs. Our
approach benchmarks 24 bidirectional language pairs across multiple domains
using different metrics. We further propose a hybrid bias detection pipeline
that integrates rule-based heuristics, semantic similarity filtering, and
LLM-based validation. We also introduce a high-quality, bias-annotated dataset
based on human evaluations of 1,439 translation-reference pairs. The code and
dataset are accessible on GitHub:
https://github.com/faiyazabdullah/TranslationTangles

</details>


### [46] [Do LLMs Really Need 10+ Thoughts for "Find the Time 1000 Days Later"? Towards Structural Understanding of LLM Overthinking](https://arxiv.org/abs/2510.07880)
*Xinliang Frederick Zhang,Anhad Mohananey,Alexandra Chronopoulou,Pinelopi Papalampidi,Somit Gupta,Tsendsuren Munkhdalai,Lu Wang,Shyam Upadhyay*

Main category: cs.CL

> 本文通过引入TRACE方法分析长链思维模型中的过度思考问题，发现过度验证和过度探索是过度思考的主要原因，并提出了基于效用的过度思考定义。

<details>
  <summary>Details</summary>

**Motivation:** 先前的工作虽然探讨了缓解过度思考的方法，但对于过度思考的根本原因理解不足。这项研究旨在通过引入TRACE分析器来弥补这一缺口，从而更深入地理解LLMs的内部运作。

**Method:** 该研究引入了一种系统化的LLMs思维过程细粒度分析器TRACE。首先，通过基准测试来确认过度思考的问题，然后通过分解思维过程为最小完整的子思维，并推断子思维之间的讨论关系，构建细粒度的思想进展图，从而识别出类似主题查询的常见思维模式。

**Result:** 研究揭示了开放权重思维模型的两种主要模式 - 探索者和晚期着陆。发现过度验证和过度探索是LLMs过度思考的主要驱动因素。

**Conclusion:** 基于思想结构，该研究提出了一个基于效用的过度思考定义，超越了基于长度的度量，为理解LLMs的思想进程提供了更深入的认知，并为有原则地管理过度思考提供了实用指南。

**Abstract:** Models employing long chain-of-thought (CoT) reasoning have shown superior
performance on complex reasoning tasks. Yet, this capability introduces a
critical and often overlooked inefficiency -- overthinking -- models often
engage in unnecessarily extensive reasoning even for simple queries, incurring
significant computations without accuracy improvements. While prior work has
explored solutions to mitigate overthinking, a fundamental gap remains in our
understanding of its underlying causes. Most existing analyses are limited to
superficial, profiling-based observations, failing to delve into LLMs' inner
workings. This study introduces a systematic, fine-grained analyzer of LLMs'
thought process to bridge the gap, TRACE. We first benchmark the overthinking
issue, confirming that long-thinking models are five to twenty times slower on
simple tasks with no substantial gains. We then use TRACE to first decompose
the thought process into minimally complete sub-thoughts. Next, by inferring
discourse relationships among sub-thoughts, we construct granular thought
progression graphs and subsequently identify common thinking patterns for
topically similar queries. Our analysis reveals two major patterns for
open-weight thinking models -- Explorer and Late Landing. This finding provides
evidence that over-verification and over-exploration are the primary drivers of
overthinking in LLMs. Grounded in thought structures, we propose a
utility-based definition of overthinking, which moves beyond length-based
metrics. This revised definition offers a more insightful understanding of
LLMs' thought progression, as well as practical guidelines for principled
overthinking management.

</details>


### [47] [CS3-Bench: Evaluating and Enhancing Speech-to-Speech LLMs for Mandarin-English Code-Switching](https://arxiv.org/abs/2510.07881)
*Heyang Liu,Yuhao Wang,Ziyang Cheng,Ronghua Wu,Qunshan Gu,Yanfeng Wang,Yu Wang*

Main category: cs.CL

> 研究人员发现多模态大型语言模型在处理代码切换任务时表现较差。使用Chain of Recognition和Keyword Highlighting技术显著提高了模型的知识准确性和开放对话理解率，且降低了次要语言中的发音错误率。

<details>
  <summary>Details</summary>

**Motivation:** 动机是解决现有模型在多语言交互，特别是代码切换语音到语音交互中存在的语言对齐问题，这些问题可能导致理解或产生错误。

**Method:** 研究中使用了7种主要的多模态大型语言模型，并且通过实验发现它们在处理知识密集型和开放性对话任务时存在不足。为了改进这些模型的能力，作者提出了Chain of Recognition和Keyword Highlighting两种方法。

**Result:** {
  "tldr": "该研究发现多模态大型语言模型在语言对齐方面存在不足，特别是在代码切换的语音到语音的任务中表现较差。通过引入Chain of Recognition和Keyword Highlighting技术，他们显著提高了知识准确性与开放对话理解率，同时减少了次要语言中的发音错误。", 
  "motivation": "尽管自然单语交互已经实现，但现有模型在语言对齐能力上存在缺陷，特别是在处理代码切换的任务时。", 
  "method": "实验基于7种主流模型，并使用Chain of Recognition（CoR）和Keyword Highlighting（KH）两种方法来改进语言对齐能力。", 
  "result": "知识准确性从25.14%提高到46.13%，开放对话理解率从64.5%提升到86.5%，并且减少了次要语言中的发音错误。", 
  "conclusion": "通过引入新的数据构建和训练方法，本文提出的解决方案显著提高了多模态大规模语言模型的语言对齐能力。"}
}

**Conclusion:** 通过引入新的数据构建和训练方法，特别是在Chain of Recognition和Keyword Highlighting技术的帮助下，多模态语言模型在处理代码切换任务时的语言对齐能力得到了显著提升。

**Abstract:** The advancement of multimodal large language models has accelerated the
development of speech-to-speech interaction systems. While natural monolingual
interaction has been achieved, we find existing models exhibit deficiencies in
language alignment. In our proposed Code-Switching Speech-to-Speech Benchmark
(CS3-Bench), experiments on 7 mainstream models demonstrate a relative
performance drop of up to 66% in knowledge-intensive question answering and
varying degrees of misunderstanding in open-ended conversations. Starting from
a model with severe performance deterioration, we propose both data
constructions and training approaches to improve the language alignment
capabilities, specifically employing Chain of Recognition (CoR) to enhance
understanding and Keyword Highlighting (KH) to guide generation. Our approach
improves the knowledge accuracy from 25.14% to 46.13%, with open-ended
understanding rate from 64.5% to 86.5%, and significantly reduces pronunciation
errors in the secondary language. CS3-Bench is available at
https://huggingface.co/datasets/VocalNet/CS3-Bench.

</details>


### [48] [Contrastive Weak-to-strong Generalization](https://arxiv.org/abs/2510.07884)
*Houcheng Jiang,Junfeng Fang,Jiaxin Wu,Tianyu Zhang,Chen Gao,Yong Li,Xiang Wang,Xiangnan He,Yang Deng*

Main category: cs.CL

> 提出ConG框架，通过对比解码方式，改进从弱到强的大语言模型推广，提高模型泛化能力和鲁棒性。

<details>
  <summary>Details</summary>

**Motivation:** 解决弱模型输出中的噪声和偏差带来的传统弱到强推广方法的局限性。其目标是提高大语言模型的强模型训练的稳健性和泛化能力，尤其是在没有人工反馈或明确奖励建模的情况下。

**Method:** 提出Contrastive Weak-to-Strong Generalization (ConG)框架，利用对比解码在预对齐和后对齐的弱模型之间生成更高质量的样本。这种方法通过对比解码来提高弱模型到强模型的迁移能力，减少噪声并提高鲁棒性。

**Result:** 实验结果显示，ConG方法在降低噪声和提高强模型训练的质量方面取得了显著改进，展示了其在弱到强推广领域中的潜力。

**Conclusion:** 实验证明了ConG在不同模型家族中的普遍适用性和有效性，为弱到强推广的进步提供了新途径，也为通往AGI提供了一条可行的路径。

**Abstract:** Weak-to-strong generalization provides a promising paradigm for scaling large
language models (LLMs) by training stronger models on samples from aligned
weaker ones, without requiring human feedback or explicit reward modeling.
However, its robustness and generalization are hindered by the noise and biases
in weak-model outputs, which limit its applicability in practice. To address
this challenge, we leverage implicit rewards, which approximate explicit
rewards through log-likelihood ratios, and reveal their structural equivalence
with Contrastive Decoding (CD), a decoding strategy shown to reduce noise in
LLM generation. Building on this connection, we propose Contrastive
Weak-to-Strong Generalization (ConG), a framework that employs contrastive
decoding between pre- and post-alignment weak models to generate higher-quality
samples. This approach enables more reliable capability transfer, denoising,
and improved robustness, substantially mitigating the limitations of
traditional weak-to-strong methods. Empirical results across different model
families confirm consistent improvements, demonstrating the generality and
effectiveness of ConG. Taken together, our findings highlight the potential of
ConG to advance weak-to-strong generalization and provide a promising pathway
toward AGI.

</details>


### [49] [Standard-to-Dialect Transfer Trends Differ across Text and Speech: A Case Study on Intent and Topic Classification in German Dialects](https://arxiv.org/abs/2510.07890)
*Verena Blaschke,Miriam Winkler,Barbara Plank*

Main category: cs.CL

> 研究比较了文本模型、语音模型和级联系统在标准到非标准德语方言转换中的效果，首次发布了方言音频意图分类数据集。

<details>
  <summary>Details</summary>

**Motivation:** 跨方言转换的研究主要集中在文本数据上，但方言主要是口头交流的，非标准拼写会在文本处理中造成问题。

**Method:** 

**Result:** 

**Conclusion:** 纯语音系统的设置在方言数据上表现最好，而纯文本系统的设置在标准数据上效果最佳。级联系统在非标准数据上的表现相当好，如果转录系统产生了标准化的输出。

**Abstract:** Research on cross-dialectal transfer from a standard to a non-standard
dialect variety has typically focused on text data. However, dialects are
primarily spoken, and non-standard spellings are known to cause issues in text
processing. We compare standard-to-dialect transfer in three settings: text
models, speech models, and cascaded systems where speech first gets
automatically transcribed and then further processed by a text model. In our
experiments, we focus on German and multiple German dialects in the context of
written and spoken intent and topic classification. To that end, we release the
first dialectal audio intent classification dataset. We find that the
speech-only setup provides the best results on the dialect data while the
text-only setup works best on the standard data. While the cascaded systems lag
behind the text-only models for German, they perform relatively well on the
dialectal data if the transcription system generates normalized, standard-like
output.

</details>


### [50] [Metric Calculating Benchmark: Code-Verifiable Complicate Instruction Following Benchmark for Large Language Models](https://arxiv.org/abs/2510.07892)
*Hyeonseok Moon,Seongtae Hong,Jaehyung Seo,Heuiseok Lim*

Main category: cs.CL

> 本文提出了MCBench，一种可以客观、确定和可编程验证的基准测试，用于评估大语言模型的逐步执行和指令理解能力。

<details>
  <summary>Details</summary>

**Motivation:** 要设计出具有挑战性的基准测试工具，以便客观地验证大模型的性能，特别是对逐步执行能力和指令理解能力的评估。

**Method:** 介绍了一个名为MCBench的基准测试工具，该工具用于评估大模型是否能按步骤执行字符串匹配NLP任务，区别于依赖主观判断或通用推理的先前基准测试。

**Result:** MCBench能够作为衡量最先进大模型能力的有效且客观工具。

**Conclusion:** MCBench被证明是一个有效的、客观的评估最先进语言模型能力的工具。

**Abstract:** Recent frontier-level LLMs have saturated many previously difficult
benchmarks, leaving little room for further differentiation. This progress
highlights the need for challenging benchmarks that provide objective
verification. In this paper, we introduce MCBench, a benchmark designed to
evaluate whether LLMs can execute string-matching NLP metrics by strictly
following step-by-step instructions. Unlike prior benchmarks that depend on
subjective judgments or general reasoning, MCBench offers an objective,
deterministic and codeverifiable evaluation. This setup allows us to
systematically test whether LLMs can maintain accurate step-by-step execution,
including instruction adherence, numerical computation, and long-range
consistency in handling intermediate results. To ensure objective evaluation of
these abilities, we provide a parallel reference code that can evaluate the
accuracy of LLM output. We provide three evaluative metrics and three benchmark
variants designed to measure the detailed instruction understanding capability
of LLMs. Our analyses show that MCBench serves as an effective and objective
tool for evaluating the capabilities of cutting-edge LLMs.

</details>


### [51] [ACE: Attribution-Controlled Knowledge Editing for Multi-hop Factual Recall](https://arxiv.org/abs/2510.07896)
*Jiayu Yang,Yuxuan Fan,Songning Lai,Shengen Wu,Jiaqi Tang,Chun Kang,Zhijiang Guo,Yutao Yue*

Main category: cs.CL

> 本文揭示了多步事实记忆中的知识编辑问题，提出ACE框架，利用神经元级别的归因而识别和编辑关键通路，大幅提升了性能表现。

<details>
  <summary>Details</summary>

**Motivation:** 大型语言模型（LLMs）需要高效的KE来更新事实信息，但是现有的方法在多步事实回溯中表现大大衰退。尤其是在编辑涉及推理链中的中间隐含主题时，这种情况尤为严重。

**Method:** 通过因果分析，我们揭示了这种局限性源于对链式知识在神经元层面动态表示和利用方式的忽视。我们发现，在多步推理过程中，隐含主体充当查询神经元，这些查询神经元依次激活跨变压器层的相关值神经元，以向最终答案累积信息，这是先前知识编辑工作所忽视的动态先验。基于这一见解，我们提出了ACE：归因控制知识编辑框架，它利用神经元级别归因而识别和编辑这些关键的查询-值通路。

**Result:** ACE在GPT-J和Qwen3-8B上分别优于现有尖端技术9.44%和37.46%。进一步分析还揭示了Qwen3中更精细的激活模式，表明值神经元的语义可解释性是由查询驱动的累积所编排的。

**Conclusion:** 这些发现建立了一条基于对内部推理机制的原理性理解来推进KE能力的新途径。

**Abstract:** Large Language Models (LLMs) require efficient knowledge editing (KE) to
update factual information, yet existing methods exhibit significant
performance decay in multi-hop factual recall. This failure is particularly
acute when edits involve intermediate implicit subjects within reasoning
chains. Through causal analysis, we reveal that this limitation stems from an
oversight of how chained knowledge is dynamically represented and utilized at
the neuron level. We discover that during multi hop reasoning, implicit
subjects function as query neurons, which sequentially activate corresponding
value neurons across transformer layers to accumulate information toward the
final answer, a dynamic prior KE work has overlooked. Guided by this insight,
we propose ACE: Attribution-Controlled Knowledge Editing for Multi-hop Factual
Recall, a framework that leverages neuron-level attribution to identify and
edit these critical query-value (Q-V) pathways. ACE provides a mechanistically
grounded solution for multi-hop KE, empirically outperforming state-of-the-art
methods by 9.44% on GPT-J and 37.46% on Qwen3-8B. Our analysis further reveals
more fine-grained activation patterns in Qwen3 and demonstrates that the
semantic interpretability of value neurons is orchestrated by query-driven
accumulation. These findings establish a new pathway for advancing KE
capabilities based on the principled understanding of internal reasoning
mechanisms.

</details>


### [52] [Towards Human-Like Grading: A Unified LLM-Enhanced Framework for Subjective Question Evaluation](https://arxiv.org/abs/2510.07912)
*Fanwei Zhua,Jiaxuan He,Xiaoxiao Chen,Zulong Chen,Quan Lu,Chenrui Mei*

Main category: cs.CL

> 研究提出了一种基于大语言模型的统一自动评分框架，可以对各类主观题进行评估，并表明在多个数据集上性能优于其他系统。

<details>
  <summary>Details</summary>

**Motivation:** 目前的自动评分系统主要关注特定类型的主观题，缺乏对综合性考试中多种题型的支持能力，这成为了考试评估中的一个挑战。

**Method:** 本研究提出了一种统一的大语言模型（LLM）增强的自动评分框架，旨在为所有类型的主观题提供类似人类的评估。该框架集成了四个互补模块，包括基础文本匹配模块、关键知识点比对模块、伪问题生成模块以及模拟人类评估的模块。

**Result:** 实验结果表明，该框架在通用数据集和专业数据集上，相对于传统的和基于LLM的基线系统，在多种评分指标上都表现出色。

**Conclusion:** 该系统已经在一家大型电子商务企业的实际训练和认证考试中成功部署。

**Abstract:** Automatic grading of subjective questions remains a significant challenge in
examination assessment due to the diversity in question formats and the
open-ended nature of student responses. Existing works primarily focus on a
specific type of subjective question and lack the generality to support
comprehensive exams that contain diverse question types. In this paper, we
propose a unified Large Language Model (LLM)-enhanced auto-grading framework
that provides human-like evaluation for all types of subjective questions
across various domains. Our framework integrates four complementary modules to
holistically evaluate student answers. In addition to a basic text matching
module that provides a foundational assessment of content similarity, we
leverage the powerful reasoning and generative capabilities of LLMs to: (1)
compare key knowledge points extracted from both student and reference answers,
(2) generate a pseudo-question from the student answer to assess its relevance
to the original question, and (3) simulate human evaluation by identifying
content-related and non-content strengths and weaknesses. Extensive experiments
on both general-purpose and domain-specific datasets show that our framework
consistently outperforms traditional and LLM-based baselines across multiple
grading metrics. Moreover, the proposed system has been successfully deployed
in real-world training and certification exams at a major e-commerce
enterprise.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [53] [Enhancing Maritime Object Detection in Real-Time with RT-DETR and Data Augmentation](https://arxiv.org/abs/2510.07346)
*Nader Nemati*

Main category: cs.CV

> The paper proposes a real-time maritime object detection system using RT-DETR, enhanced by augmented synthetic images and special strategies to balance synthetic and real data. It maintains real-time detection of small maritime objects and offers a full Python pipeline.

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind this paper is to improve maritime object detection which is inherently complicated due to the small size of targets in the maritime environment and the insufficiency of labeled real RGB data.

**Method:** This paper presents a real-time maritime object detection system based on RT-DETR, which is enhanced by the use of augmented synthetic images. It employs multi-scale feature fusion, uncertainty-minimizing query selection, and a smart weight strategy for balancing synthetic and real training samples to minimize the domain gap.

**Result:** The system presents improved detection of small, low-contrast vessels, and it maintains real-time performance under practical limits. The component analysis provides insights into each module's contribution and their interactions.

**Conclusion:** The study concludes that the proposed method using RT-DETR with enhancements can offer superior maritime object detection capabilities, while the full Python pipeline is provided as a robust design that balances speed and accuracy.

**Abstract:** Maritime object detection faces essential challenges due to the small target
size and limitations of labeled real RGB data. This paper will present a
real-time object detection system based on RT-DETR, enhanced by employing
augmented synthetic images while strictly evaluating on real data. This study
employs RT-DETR for the maritime environment by combining multi-scale feature
fusion, uncertainty-minimizing query selection, and smart weight between
synthetic and real training samples. The fusion module in DETR enhances the
detection of small, low-contrast vessels, query selection focuses on the most
reliable proposals, and the weighting strategy helps reduce the visual gap
between synthetic and real domains. This design preserves DETR's refined
end-to-end set prediction while allowing users to adjust between speed and
accuracy at inference time. Data augmentation techniques were also used to
balance the different classes of the dataset to improve the robustness and
accuracy of the model. Regarding this study, a full Python robust maritime
detection pipeline is delivered that maintains real-time performance even under
practical limits. It also verifies how each module contributes, and how the
system handles failures in extreme lighting or sea conditions. This study also
includes a component analysis to quantify the contribution of each
architectural module and explore its interactions.

</details>


### [54] [DynamicEval: Rethinking Evaluation for Dynamic Text-to-Video Synthesis](https://arxiv.org/abs/2510.07441)
*Nithin C. Babu,Aniruddha Mahapatra,Harsh Rangwani,Rajiv Soundararajan,Kuldeep Kulkarni*

Main category: cs.CV

> 介绍了DynamicEval，一种系统地强调摄像机动态场景并包含45000个人工注释的基准。提出新的背景和前景一致性度量，提高了衡量文本到视频模型性能的准确性。

<details>
  <summary>Details</summary>

**Motivation:** 现有的文本到视频(T2V)评估基准，如VBench和EvalCrafter，存在两个局限性。首先，这些基准强调以主题为中心的提示或静态摄像机场景，但对于生成电影镜头至关重要的摄像机动态几乎没有涉及。其次，这些基准通常将视频级别的得分汇总为单一的模型级别得分，以排名生成模型，这忽略了对给定提示生成的候选视频进行视频级别评估的重要性。为了解决这些不足，引入了DynamicEval，一个系统地强调摄像机动态的提示集，并包含来自十种T2V模型生成的3000个视频中成对的45000个人工注释。

**Method:** DynamicEval 评估视频质量的两个关键维度：背景场景一致性以及前景对象一致性。针对背景场景一致性，基于 Vbench 运动平滑度指标获取可解释的误差图。发现 Vbench 指标在人类判断上表现出良好的一致性，但在摄像机和前景物体运动产生的遮挡/非遮挡情况下表现不佳。在此基础上，提出一种新的背景一致性度量，利用物体误差图在一定程度上纠正两个失败情况。第二个创新是引入前景物体一致性度量，跟踪每个物体实例内部点及其邻居，以评估对象的保真度。

**Result:** 广泛的实验表明，提出的新指标在与人类偏好的相关性方面在视频级别和模型级别都实现了更强的相关性（提高了2%以上），确立了DynamicEval作为评估动态摄像机运动下T2V模型的更全面基准。

**Conclusion:** 通过提出一种新的背景一致性度量和前景物体一致性度量，DynamicEval 作为评估文本到视频模型的基准更加全面，尤其是在动态摄像机运动方面。

**Abstract:** Existing text-to-video (T2V) evaluation benchmarks, such as VBench and
EvalCrafter, suffer from two limitations. (i) While the emphasis is on
subject-centric prompts or static camera scenes, camera motion essential for
producing cinematic shots and existing metrics under dynamic motion are largely
unexplored. (ii) These benchmarks typically aggregate video-level scores into a
single model-level score for ranking generative models. Such aggregation,
however, overlook video-level evaluation, which is vital to selecting the
better video among the candidate videos generated for a given prompt. To
address these gaps, we introduce DynamicEval, a benchmark consisting of
systematically curated prompts emphasizing dynamic camera motion, paired with
45k human annotations on video pairs from 3k videos generated by ten T2V
models. DynamicEval evaluates two key dimensions of video quality: background
scene consistency and foreground object consistency. For background scene
consistency, we obtain the interpretable error maps based on the Vbench motion
smoothness metric. We observe that while the Vbench motion smoothness metric
shows promising alignment with human judgments, it fails in two cases:
occlusions/disocclusions arising from camera and foreground object movements.
Building on this, we propose a new background consistency metric that leverages
object error maps to correct two failure cases in a principled manner. Our
second innovation is the introduction of a foreground consistency metric that
tracks points and their neighbors within each object instance to assess object
fidelity. Extensive experiments demonstrate that our proposed metrics achieve
stronger correlations with human preferences at both the video level and the
model level (an improvement of more than 2% points), establishing DynamicEval
as a more comprehensive benchmark for evaluating T2V models under dynamic
camera motion.

</details>


### [55] [Provably Accelerated Imaging with Restarted Inertia and Score-based Image Priors](https://arxiv.org/abs/2510.07470)
*Marien Renaud,Julien Hermant,Deliang Wei,Yu Sun*

Main category: cs.CV

> RISP combines fast convergence and high-quality reconstruction in imaging problems, outperforming traditional RED methods both theoretically and practically.

<details>
  <summary>Details</summary>

**Motivation:** To address the common issue in existing imaging inverse problem solving algorithms where fast convergence and high-quality reconstruction trade off against each other.

**Method:** Restarted Inertia with Score-based Priors (RISP) is proposed as an improvement over existing RED methods by incorporating a restarting inertia for quicker convergence without sacrificing reconstruction quality.

**Result:** Proven faster stationary-point convergence than RED without the need for the image prior to be convex, along with experimental demonstrations of both speed and quality improvements.

**Conclusion:** RISP offers a balance of fast convergence and high-quality image recovery, supported by theoretical proofs and empirical evidence.

**Abstract:** Fast convergence and high-quality image recovery are two essential features
of algorithms for solving ill-posed imaging inverse problems. Existing methods,
such as regularization by denoising (RED), often focus on designing
sophisticated image priors to improve reconstruction quality, while leaving
convergence acceleration to heuristics. To bridge the gap, we propose Restarted
Inertia with Score-based Priors (RISP) as a principled extension of RED. RISP
incorporates a restarting inertia for fast convergence, while still allowing
score-based image priors for high-quality reconstruction. We prove that RISP
attains a faster stationary-point convergence rate than RED, without requiring
the convexity of the image prior. We further derive and analyze the associated
continuous-time dynamical system, offering insight into the connection between
RISP and the heavy-ball ordinary differential equation (ODE). Experiments
across a range of imaging inverse problems demonstrate that RISP enables fast
convergence while achieving high-quality reconstructions.

</details>


### [56] [A Denoising Framework for Real-World Ultra-Low Dose Lung CT Images Based on an Image Purification Strategy](https://arxiv.org/abs/2510.07492)
*Guoliang Gong,Man Yu*

Main category: cs.CV

> 本文提出了基于图像净化策略（IP）的超低剂量CT（uLDCT）降噪框架，并引入了频率域流匹配（FFM）模型来进行有效降噪，提升了uLDCT图像解剖结构的完整性。

<details>
  <summary>Details</summary>

**Motivation:** 超低剂量CT (uLDCT) 虽然显著减少了辐射暴露，但引入了严重的噪声和伪影，并导致uLDCT和正常剂量CT (NDCT) 图像间的空间错位。因此现有降噪网络无法直接应用于uLDCT图像，从而提出了本研究来解决uLDCT图像降噪挑战。

**Method:** 本文提出了一种基于图像净化策略（IP）的超低剂量CT（uLDCT）降噪框架。IP策略用于生成结构对齐的uLDCT-NDCT图像对，为网络训练提供了高质量的数据基础。在此基础上，提出了频率域流匹配（FFM）模型，协同IP策略以极佳地保持了降噪图像的解剖结构完整性。

**Result:** 实验结果表明，IP策略显著提升了多种主流降噪模型在uLDCT任务中的性能。而结合IP策略的FFM模型达到了最先进的（SOTA）结构保留效果。

**Conclusion:** 本研究为解决现实世界中uLDCT图像降噪的数据不匹配问题提供了有效方案。代码和数据集可在https://github.com/MonkeyDadLufy/flow-matching 找到。

**Abstract:** Ultra-low dose CT (uLDCT) significantly reduces radiation exposure but
introduces severe noise and artifacts. It also leads to substantial spatial
misalignment between uLDCT and normal dose CT (NDCT) image pairs. This poses
challenges for directly applying existing denoising networks trained on
synthetic noise or aligned data. To address this core challenge in uLDCT
denoising, this paper proposes an innovative denoising framework based on an
Image Purification (IP) strategy. First, we construct a real clinical uLDCT
lung dataset. Then, we propose an Image Purification strategy that generates
structurally aligned uLDCT-NDCT image pairs, providing a high-quality data
foundation for network training. Building upon this, we propose a
Frequency-domain Flow Matching (FFM) model, which works synergistically with
the IP strategy to excellently preserve the anatomical structure integrity of
denoised images. Experiments on the real clinical dataset demonstrate that our
IP strategy significantly enhances the performance of multiple mainstream
denoising models on the uLDCT task. Notably, our proposed FFM model combined
with the IP strategy achieves state-of-the-art (SOTA) results in anatomical
structure preservation. This study provides an effective solution to the data
mismatch problem in real-world uLDCT denoising. Code and dataset are available
at https://github.com/MonkeyDadLufy/flow-matching.

</details>


### [57] [D2RA: Dual Domain Regeneration Attack](https://arxiv.org/abs/2510.07538)
*Pragati Shuddhodhan Meshram,Varun Chandrasekaran*

Main category: cs.CV

> 研究提出了D2RA，一种无须训练且适用于单一图像的方法，用以在资源受限的对抗环境中移除或削弱生成模型中的水印，展示了现有水印技术的不足之处。

<details>
  <summary>Details</summary>

**Motivation:** 随着生成模型使用的增加，确保内容归属和来源的水印方法变得尤为重要。而最新的语义水印方案虽然通过在潜伏或频率表示中嵌入信号提高了鲁棒性，但我们发现它们在资源受限的对抗环境中仍然存在漏洞。

**Method:** 我们提出了D2RA方法，这是一种无需训练、单一图像攻击的方法，可以在不访问底层模型的情况下移除或削弱水印。通过将带有水印的图像投影到互补表示中的自然先验上，D2RA能够抑制水印信号同时保持视觉保真度。

**Result:** 实验结果显示，我们的方法在多种水印方案下始终能够降低水印的可检测性，揭示了现有设计的基本弱点。

**Conclusion:** 我们的研究揭示了现有水印设计的基本弱点，并提供了一种有效的方法来削弱或移除这些水印，凸显了在对抗性设置下水印技术面临的挑战。

**Abstract:** The growing use of generative models has intensified the need for
watermarking methods that ensure content attribution and provenance. While
recent semantic watermarking schemes improve robustness by embedding signals in
latent or frequency representations, we show they remain vulnerable even under
resource-constrained adversarial settings. We present D2RA, a training-free,
single-image attack that removes or weakens watermarks without access to the
underlying model. By projecting watermarked images onto natural priors across
complementary representations, D2RA suppresses watermark signals while
preserving visual fidelity. Experiments across diverse watermarking schemes
demonstrate that our approach consistently reduces watermark detectability,
revealing fundamental weaknesses in current designs. Our code is available at
https://github.com/Pragati-Meshram/DAWN.

</details>


### [58] [PickStyle: Video-to-Video Style Transfer with Context-Style Adapters](https://arxiv.org/abs/2510.07546)
*Soroush Mehraban,Vida Adeli,Jacob Rommann,Babak Taati,Kyryl Truskovskyi*

Main category: cs.CV

> 该研究解决了视频风格迁移问题，提出了PickStyle框架，不仅利用样式适配器实现了高效率的风格迁移，还通过新型无分类器指导技术（CS-CFG）确保了生成视频的上下文保持与风格正确转移。

<details>
  <summary>Details</summary>

**Motivation:** 针对视频风格迁移任务，特别是在缺乏成对视频数据进行监督的情况下，需要开发一种有效的解决方案来实现内容的保真和风格的迁移。

**Method:** 提出PickStyle框架，通过在预训练的视频扩散模型中添加样式适配器，并利用配对的静态图像数据进行训练来解决视频风格迁移问题。PickStyle在条件模块的自注意力层中插入低秩适配器，增强了动作风格迁移的效率，同时保持了视频内容和风格的一致性。为了弥合静态图像监督与动态视频之间的差距，通过应用模拟摄像机运动的共享增强技术，从配对图像中构建合成训练片段。此外，引入了背景样式无分类器指导（CS-CFG），这是一种新型的无分类器指导因子，能够确保生成的视频在保持上下文的同时有效转移风格。

**Result:** 

**Conclusion:** 实验结果表明，该方法能实现时间连贯的、风格保留的、且内容保持的视频翻译，无论是定性还是定量评估都优于现有的基线方法。

**Abstract:** We address the task of video style transfer with diffusion models, where the
goal is to preserve the context of an input video while rendering it in a
target style specified by a text prompt. A major challenge is the lack of
paired video data for supervision. We propose PickStyle, a video-to-video style
transfer framework that augments pretrained video diffusion backbones with
style adapters and benefits from paired still image data with source-style
correspondences for training. PickStyle inserts low-rank adapters into the
self-attention layers of conditioning modules, enabling efficient
specialization for motion-style transfer while maintaining strong alignment
between video content and style. To bridge the gap between static image
supervision and dynamic video, we construct synthetic training clips from
paired images by applying shared augmentations that simulate camera motion,
ensuring temporal priors are preserved. In addition, we introduce Context-Style
Classifier-Free Guidance (CS-CFG), a novel factorization of classifier-free
guidance into independent text (style) and video (context) directions. CS-CFG
ensures that context is preserved in generated video while the style is
effectively transferred. Experiments across benchmarks show that our approach
achieves temporally coherent, style-faithful, and content-preserving video
translations, outperforming existing baselines both qualitatively and
quantitatively.

</details>


### [59] [TRAVL: A Recipe for Making Video-Language Models Better Judges of Physics Implausibility](https://arxiv.org/abs/2510.07550)
*Saman Motamed,Minghao Chen,Luc Van Gool,Iro Laina*

Main category: cs.CV

> 研究提出TRAVL和ImplausiBench，用于改进视频-语言模型中的物理合理性评估，展示了未来在改进视觉-时间理解方面的工作方向。

<details>
  <summary>Details</summary>

**Motivation:** 尽管现代视频生成模型提供了惊人的视觉保真度，但它们产生的序列经常违反直觉上的物理定律。此研究旨在探索如何通过视频-语言模型来更可靠地评估视频中的物理现实性。

**Method:** 探索使用视频-语言模型（VLM）来作为物理合理性的判断者，并介绍了TRA VL，一个结合了平衡训练数据集与轨迹感知注意力模块的微调配方，以改进VLM中的运动编码和分辨能力。

**Result:** 通过引入ImplausiBench，一个包含300个视频（150个真实，150个生成）的基准，该研究更加严格地评估了物理推理能力的表现。结果表明TRAVL在物理合理性的判断上有显著提升。

**Conclusion:** TRAVL和ImplausiBench一起提供了一个完整的框架，用于探究和改进跨模态模型中的物理合理性，突显了视觉-时间理解中的挑战和未被充分探索的领域。

**Abstract:** Despite impressive visual fidelity, modern video generative models frequently
produce sequences that violate intuitive physical laws, such as objects
floating, teleporting, or morphing in ways that defy causality. While humans
can easily detect such implausibilities, there remains no robust method for
quantitatively assessing physical realism in video. In this work, we explore
whether Video-Language Models (VLMs) can be trained to serve as reliable judges
of physical plausibility. We find that existing VLMs struggle to identify
physics violations, exposing fundamental limitations in their temporal and
causal reasoning. To address this, we introduce TRAVL, a fine-tuning recipe
that combines a balanced training dataset with a trajectory-aware attention
module to improve motion encoding and discrimination in VLMs. To evaluate
physical reasoning more rigorously, we propose ImplausiBench, a benchmark of
300 videos (150 real, 150 generated) that removes linguistic biases and
isolates visual-temporal understanding. Performance is reported both with
gold-standard human judgments and stricter LLM-as-judge metrics. Together,
TRAVL and ImplausiBench offer a unified framework for probing and improving
physical plausibility in multimodal models, shedding light on a challenging and
underexplored aspect of visual-temporal understanding.

</details>


### [60] [Label Semantics for Robust Hyperspectral Image Classification](https://arxiv.org/abs/2510.07556)
*Rafin Hassan,Zarin Tasnim Roshni,Rafiqul Bari,Alimul Islam,Nabeel Mohammed,Moshiur Farazi,Shafin Rahman*

Main category: cs.CV

> S3FN利用特定类别的文本描述增强HSI分类，提升了性能。

<details>
  <summary>Details</summary>

**Motivation:** 光谱图像（HSI）分类面临着样本量有限和高维光谱数据导致的过度拟合问题。现有模型大多为单模态，仅依赖光谱-空间数据来学习高维嵌入空间中的决策边界。

**Method:** 提出了一种通用的语义光谱-空间融合网络（S3FN），利用上下文相关的、特定类别的文本描述来补充HSI分类模型的训练。S3FN使用大型语言模型为每个类别标签生成详细的文本描述，并利用预训练的文本编码器如BERT或RoBERTa将其嵌入向量空间，以捕捉独特的类别语义。

**Result:** 通过在三个多样的HSI基准数据集上评估模型，展示了显著的性能提升。

**Conclusion:** 研究展示了文本语义与光谱-空间数据之间的协同作用，为语义增强的HSI分类模型的发展铺平了道路。

**Abstract:** Hyperspectral imaging (HSI) classification is a critical tool with widespread
applications across diverse fields such as agriculture, environmental
monitoring, medicine, and materials science. Due to the limited availability of
high-quality training samples and the high dimensionality of spectral data, HSI
classification models are prone to overfitting and often face challenges in
balancing accuracy and computational complexity. Furthermore, most of HSI
classification models are monomodal, where it solely relies on spectral-spatial
data to learn decision boundaries in the high dimensional embedding space. To
address this, we propose a general-purpose Semantic Spectral-Spatial Fusion
Network (S3FN) that uses contextual, class specific textual descriptions to
complement the training of an HSI classification model. Specifically, S3FN
leverages LLMs to generate comprehensive textual descriptions for each class
label that captures their unique characteristics and spectral behaviors. These
descriptions are then embedded into a vector space using a pre-trained text
encoder such as BERT or RoBERTa to extract meaningful label semantics which in
turn leads to a better feature-label alignment for improved classification
performance. To demonstrate the effectiveness of our approach, we evaluate our
model on three diverse HSI benchmark datasets - Hyperspectral Wood,
HyperspectralBlueberries, and DeepHS-Fruit and report significant performance
boost. Our results highlight the synergy between textual semantics and
spectral-spatial data, paving the way for further advancements in semantically
augmented HSI classification models. Codes are be available in:
https://github.com/milab-nsu/S3FN

</details>


### [61] [Cross-Modal Attention Guided Unlearning in Vision-Language Models](https://arxiv.org/abs/2510.07567)
*Karuna Bhaila,Aneesh Komanduri,Minh-Hao Van,Xintao Wu*

Main category: cs.CV

> 本文提出了一种轻量级的视觉语言模型的卸载框架CAGUL，该方法利用跨模态注意力机制，在不改变预训练模型参数的情况下，有效防止模型在推理过程中泄露敏感信息。

<details>
  <summary>Details</summary>

**Motivation:** 视觉语言模型在处理多模态理解任务时具有强大的能力，但同时存在泄露训练过程中学到的敏感信息的风险。针对这一问题，本文旨在探索视觉语言模型尤其是VQA任务的卸载方法。

**Method:** 本文提出了一种名为CAGUL（Cross-Modal Attention Guided Unlearning）的轻量级视觉语言模型的卸载框架，该框架利用跨模态注意力来编码不需要的视觉记号，从而防止敏感信息泄露，同时保持预训练模型的行为。不同于计算成本高的微调方法，CAGUL侧重于对外部模块的利用，将卸载信息编码进与查询相关且重要性较低的视觉记号中。

**Result:** 实验结果显示，与微调基线方法相比，CAGUL在不改变预训练模型参数或不需重新训练的情况下，性能表现相当或优于基线方法，是一个既实用又有效的视觉语言模型卸载解决方案。

**Conclusion:** CAGUL框架是一种去除视觉和语言模型中隐私信息的有效方法，具有轻量且高效的特点，非常适合解决视觉语言模型在推理时泄露敏感信息的问题。

**Abstract:** Vision-Language Models (VLMs) have demonstrated immense capabilities in
multi-modal understanding and inference tasks such as Visual Question Answering
(VQA), which requires models to infer outputs based on visual and textual
context simultaneously. Such inference abilities of large-scale pretrained
models are often attributed to the massive scale of pre-training data collected
across several domains. However, the models may memorize private and/or
sensitive information during training and regurgitate it in inference.
Recently, machine unlearning has been leveraged to address the leakage of
private data in LLMs. VLMs add a layer of complexity to this process, as the
visual context in the query may also contain sensitive information in addition
to the text. To address this issue, we explore unlearning for vision-language
models, specifically for the VQA task. We explore the role of visual tokens for
output generation in VLMs using cross-modal attention and utilize it to
formulate Cross-Modal Attention Guided Unlearning (CAGUL), a lightweight and
efficient VLM unlearning framework. In contrast to computationally expensive
model finetuning methods, CAGUL utilizes external modules to encode unlearning
information in visual tokens of low importance for relevant queries. We find
that the transformed visual tokens not only prevent leakage but also retain
reference model behavior. Experimental results show that our method performs
better or on par with finetuning-based baselines without altering the
pre-trained model parameters or incurring retraining costs, making it a
practical and effective unlearning solution for VLMs.

</details>


### [62] [MaizeStandCounting (MaSC): Automated and Accurate Maize Stand Counting from UAV Imagery Using Image Processing and Deep Learning](https://arxiv.org/abs/2510.07580)
*Dewi Endah Kharismawati,Toni Kazic*

Main category: cs.CV

> MaSC是一个利用轻量级YOLOv9模型进行玉米幼苗计数的算法，通过无人机捕获的RGB图像进行操作。该算法在研究和生产环境中展示出了准确和实时计数的性能。

<details>
  <summary>Details</summary>

**Motivation:** 手动计数玉米幼苗耗时且容易出错，尤其是在大面积或可变性田地上。为了提高农业管理和研究的效率，需要一个快速、准确、低成本的方法来计数玉米幼苗。

**Method:** MaizeStandCounting (MaSC)使用轻量级YOLOv9模型从RGB图像中检测玉米幼苗，这些图像是由低成本无人机捕获并经过经济硬件处理的。该算法有两种模式：1) 马赛克图像被分割成补丁；2) 通过使用单应性矩阵对齐的原始视频帧。MaSC能够区分玉米和杂草及其他植被，并基于检测的空间分布进行行和范围分割，从而获得精确的行数计数。

**Result:** 在田野实地的手动计数基准测试中，对于马赛克图像而言，相关系数达到了0.616，而对于原始视频帧来讲，相关系数达到了0.906。MaSC可以在60.63秒内处理83个全分辨率帧，包括推理和后处理，展示了其实时运行的能力。

**Conclusion:** MaSC使用低硬件预算和低成本无人机图像实现了精确的玉米幼苗计数，具有可扩展性和高准确性，适用于研究和生产环境中的自动化玉米幼苗计数。

**Abstract:** Accurate maize stand counts are essential for crop management and research,
informing yield prediction, planting density optimization, and early detection
of germination issues. Manual counting is labor-intensive, slow, and
error-prone, especially across large or variable fields. We present
MaizeStandCounting (MaSC), a robust algorithm for automated maize seedling
stand counting from RGB imagery captured by low-cost UAVs and processed on
affordable hardware. MaSC operates in two modes: (1) mosaic images divided into
patches, and (2) raw video frames aligned using homography matrices. Both modes
use a lightweight YOLOv9 model trained to detect maize seedlings from V2-V10
growth stages. MaSC distinguishes maize from weeds and other vegetation, then
performs row and range segmentation based on the spatial distribution of
detections to produce precise row-wise stand counts. Evaluation against
in-field manual counts from our 2024 summer nursery showed strong agreement
with ground truth (R^2= 0.616 for mosaics, R^2 = 0.906 for raw frames). MaSC
processed 83 full-resolution frames in 60.63 s, including inference and
post-processing, highlighting its potential for real-time operation. These
results demonstrate MaSC's effectiveness as a scalable, low-cost, and accurate
tool for automated maize stand counting in both research and production
environments.

</details>


### [63] [Quick-CapsNet (QCN): A fast alternative to Capsule Networks](https://arxiv.org/abs/2510.07600)
*Pouya Shiri,Ramin Sharifi,Amirali Baniasadi*

Main category: cs.CV

> The paper presents Quick-CapsNet (QCN), a faster version of CapsNet that reduces the number of capsules but incurs a minor accuracy loss, achieving 5x faster inference on several datasets with an enhanced decoder for better performance.

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind the research is to address the slow training and testing issues of CapsNet, facilitating its use in applications requiring real-time performance.

**Method:** The method introduces Quick-CapsNet (QCN), a variant of CapsNet that reduces the number of capsules to speed up the network while incurring a minor drop in accuracy. An enhanced decoder is also employed to further improve the performance of QCN.

**Result:** QCN achieves a 5x faster inference on MNIST, F-MNIST, SVHN, and Cifar-10 datasets compared to the original CapsNet, with a slight decrease in accuracy.

**Conclusion:** The paper concludes that QCN provides a balance between speed and accuracy, making it suitable for real-time applications where faster inference is desired.

**Abstract:** The basic computational unit in Capsule Network (CapsNet) is a capsule (vs.
neurons in Convolutional Neural Networks (CNNs)). A capsule is a set of
neurons, which form a vector. CapsNet is used for supervised classification of
data and has achieved state-of-the-art accuracy on MNIST digit recognition
dataset, outperforming conventional CNNs in detecting overlapping digits.
Moreover, CapsNet shows higher robustness towards affine transformation when
compared to CNNs for MNIST datasets. One of the drawbacks of CapsNet, however,
is slow training and testing. This can be a bottleneck for applications that
require a fast network, especially during inference. In this work, we introduce
Quick-CapsNet (QCN) as a fast alternative to CapsNet, which can be a starting
point to develop CapsNet for fast real-time applications. QCN builds on
producing a fewer number of capsules, which results in a faster network. QCN
achieves this at the cost of marginal loss in accuracy. Inference is 5x faster
on MNIST, F-MNIST, SVHN and Cifar-10 datasets. We also further enhanced QCN by
employing a more powerful decoder instead of the default decoder to further
improve QCN.

</details>


### [64] [Rectified-CFG++ for Flow Based Models](https://arxiv.org/abs/2510.07631)
*Shreshth Saini,Shashank Gupta,Alan C. Bovik*

Main category: cs.CV

> 本文介绍了一种改进的Rectified-CFG++方法，适用于文本条件的大规模扩散模型，在多个数据集上均显示出优于标准CFG的性能。

<details>
  <summary>Details</summary>

**Motivation:** 这项研究的动机在于解决分类器无关指导（CFG）在应用到校正流（RF）基础模型时会引发严重离流形漂移的问题，导致视觉伪影，文本对齐不良和脆弱行为。

**Method:** 我们提出了Rectified-CFG++, 一种自适应的预测-校正引导方法，结合了校正流的确定性效率和几何感知条件规则。每个推理步骤首先执行条件RF更新，将样本固定在学习的传输路径附近，然后应用加权的条件校正，插值条件和非条件速度场。

**Result:** 实验结果表明，Rectified-CFG++在大规模文本到图像模型上（如Flux，Stable Diffusion 3/3.5，Lumina）在基准数据集如MS-COCO，LAION-Aesthetic和T2I-CompBench上均优于标准的CFG。

**Conclusion:** Rectified-CFG++通过确保速度场的边缘一致性，在数据流形的有界管状邻域内保持轨迹，从而在广泛的引导强度范围内保证了稳定性，并表现出比标准CFG更好的性能。

**Abstract:** Classifier-free guidance (CFG) is the workhorse for steering large diffusion
models toward text-conditioned targets, yet its native application to rectified
flow (RF) based models provokes severe off-manifold drift, yielding visual
artifacts, text misalignment, and brittle behaviour. We present
Rectified-CFG++, an adaptive predictor-corrector guidance that couples the
deterministic efficiency of rectified flows with a geometry-aware conditioning
rule. Each inference step first executes a conditional RF update that anchors
the sample near the learned transport path, then applies a weighted conditional
correction that interpolates between conditional and unconditional velocity
fields. We prove that the resulting velocity field is marginally consistent and
that its trajectories remain within a bounded tubular neighbourhood of the data
manifold, ensuring stability across a wide range of guidance strengths.
Extensive experiments on large-scale text-to-image models (Flux, Stable
Diffusion 3/3.5, Lumina) show that Rectified-CFG++ consistently outperforms
standard CFG on benchmark datasets such as MS-COCO, LAION-Aesthetic, and
T2I-CompBench. Project page: https://rectified-cfgpp.github.io/

</details>


### [65] [PIT-QMM: A Large Multimodal Model For No-Reference Point Cloud Quality Assessment](https://arxiv.org/abs/2510.07636)
*Shashank Gupta,Gregoire Phillips,Alan C. Bovik*

Main category: cs.CV

> A new method, PIT-QMM, uses text, images, and point clouds to assess the quality of 3D point clouds automatically without needing a reference image, outperforming current methods.

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to explore the potential of Large Multimodal Models in the domain of 3D assets, specifically for No-Reference Point Cloud Quality Assessment, an area where the progress enabled by such models hasn't been fully explored yet.

**Method:** Content describes the development of PIT-QMM, a novel Large Multimodal Model for No-Reference Point Cloud Quality Assessment that utilizes text, 2D projections, and 3D point clouds to predict perceptual quality scores in absence of a reference.

**Result:** The method outperformed the state-of-the-art on popular benchmarks with fewer training iterations. It also demonstrated the capability to enable distortion localization and identification.

**Conclusion:** The conclusion is that PIT-QMM shows significant improvements in predicting the perceptual quality of point clouds without a reference, and opens new possibilities for model explainability and interactivity.

**Abstract:** Large Multimodal Models (LMMs) have recently enabled considerable advances in
the realm of image and video quality assessment, but this progress has yet to
be fully explored in the domain of 3D assets. We are interested in using these
models to conduct No-Reference Point Cloud Quality Assessment (NR-PCQA), where
the aim is to automatically evaluate the perceptual quality of a point cloud in
absence of a reference. We begin with the observation that different modalities
of data - text descriptions, 2D projections, and 3D point cloud views - provide
complementary information about point cloud quality. We then construct PIT-QMM,
a novel LMM for NR-PCQA that is capable of consuming text, images and point
clouds end-to-end to predict quality scores. Extensive experimentation shows
that our proposed method outperforms the state-of-the-art by significant
margins on popular benchmarks with fewer training iterations. We also
demonstrate that our framework enables distortion localization and
identification, which paves a new way forward for model explainability and
interactivity. Code and datasets are available at
https://www.github.com/shngt/pit-qmm.

</details>


### [66] [Dual-Stream Alignment for Action Segmentation](https://arxiv.org/abs/2510.07652)
*Harshala Gammulle,Clinton Fookes,Sridha Sridharan,Simon Denman*

Main category: cs.CV

> 本文提出Dual-Stream Alignment Network (DSA Net)，结合帧流和动作流以提高动作分割表现。通过Temporal Context (TC)模块融合互补信息，提出Dual-Stream Alignment Loss，提升了动作和过渡动作的识别。在多个基准数据集上的评估中，DSA Net展现了优越性能，达到当前最佳水平。

<details>
  <summary>Details</summary>

**Motivation:** 尽管现有的动作分割方法大多集中在单流方法上，本文提出了一个双流方法，结合动作特征来增强动作分割效果，旨在通过帧流和动作流的协作提升分割性能。

**Method:** Structure

**Result:** {
  "tldr": "本文提出Dual-Stream Alignment Network (DSA Net)，结合帧流和动作流以提高动作分割表现。通过Temporal Context (TC)模块融合互补信息，提出Dual-Stream Alignment Loss，提升了动作和过渡动作的识别。在多个基准数据集上的评估中，DSA Net展现了优越性能，达到当前最佳水平。", 
  "motivation": "尽管现有的动作分割方法大多集中在单流方法上，本文提出了一个双流方法，结合动作特征来增强动作分割效果，旨在通过帧流和动作流的协作提升分割性能。", 
  "method": "本文提出采用Dual-Stream Alignment Network (DSA Net)，创新性地引入了Temporal Context (TC)块，使用跨注意力机制和量子动作导向调制(Q-ActGM)来融合流间的互补信息。同时还提出了Dual-Stream Alignment Loss，包含关联一致性、跨层对比、循环一致性重构损失三个部分。", 
  "result": "在GTEA、Breakfast、50Salads和EgoProcel等多样化的基准数据集上进行了评估，展示了模型的有效性和每个组件的作用。实验结果显示，DSA Net达到了状态-of-the-art的表现，优于现有的方法。", 
  "conclusion": "本文提出的双流网络框架通过融合帧和动作信息，不仅提高了动作分割性能，还在多个公共基准数据集上达到了当前最佳水平，同时推动了量子-经典混合机器学习框架在动作分割中的应用。"]}

**Conclusion:** 本文提出的双流网络框架通过融合帧和动作信息，不仅提高了动作分割性能，还在多个公共基准数据集上达到了当前最佳水平，同时推动了量子-经典混合机器学习框架在动作分割中的应用。

**Abstract:** Action segmentation is a challenging yet active research area that involves
identifying when and where specific actions occur in continuous video streams.
Most existing work has focused on single-stream approaches that model the
spatio-temporal aspects of frame sequences. However, recent research has
shifted toward two-stream methods that learn action-wise features to enhance
action segmentation performance. In this work, we propose the Dual-Stream
Alignment Network (DSA Net) and investigate the impact of incorporating a
second stream of learned action features to guide segmentation by capturing
both action and action-transition cues. Communication between the two streams
is facilitated by a Temporal Context (TC) block, which fuses complementary
information using cross-attention and Quantum-based Action-Guided Modulation
(Q-ActGM), enhancing the expressive power of the fused features. To the best of
our knowledge, this is the first study to introduce a hybrid quantum-classical
machine learning framework for action segmentation. Our primary objective is
for the two streams (frame-wise and action-wise) to learn a shared feature
space through feature alignment. This is encouraged by the proposed Dual-Stream
Alignment Loss, which comprises three components: relational consistency,
cross-level contrastive, and cycle-consistency reconstruction losses. Following
prior work, we evaluate DSA Net on several diverse benchmark datasets: GTEA,
Breakfast, 50Salads, and EgoProcel. We further demonstrate the effectiveness of
each component through extensive ablation studies. Notably, DSA Net achieves
state-of-the-art performance, significantly outperforming existing

</details>


### [67] [Once Is Enough: Lightweight DiT-Based Video Virtual Try-On via One-Time Garment Appearance Injection](https://arxiv.org/abs/2510.07654)
*Yanjie Pan,Qingdong He,Lidong Wang,Bo Peng,Mingmin Chi*

Main category: cs.CV

> 该研究提出了一种新的视频虚拟试穿方法OIE，通过第一帧衣物替换和利用姿态及掩码信息顺序生成后续帧，解决了现有方法参数量大和缺乏时间特性的问题。

<details>
  <summary>Details</summary>

**Motivation:** 现有的双分支架构基于U-Net的扩散模型已经取得了显著的成功，但将其适应到基于扩散Transformer的模型中仍具有挑战性。主要是因为引入来自衣物参考分支的潜在空间特征需要添加或修改骨干网络，导致训练参数量大增。此外，衣物的潜在空间特征缺乏内在的时间特性，这也需要额外的学习。

**Method:** 该研究提出了一种名为OIE（一次即足够）的新方法，这是一种基于第一帧衣物替换的虚拟试穿策略。具体来说，使用基于图像的衣物转移模型替换了初始帧的衣物，然后在编辑过的初始帧的内容控制下，利用姿态和掩码信息引导视频生成模型的时间先验，顺序合成其余帧。

**Result:** 实验表明，该方法在参数效率和计算效率方面表现出色，同时仍能保持高约束条件下的领先性能。

**Conclusion:** 该研究提出的方法在保持高性能的同时，显著提高了参数和计算效率，为视频虚拟试衣提供了一个有前景的解决方案。

**Abstract:** Video virtual try-on aims to replace the clothing of a person in a video with
a target garment. Current dual-branch architectures have achieved significant
success in diffusion models based on the U-Net; however, adapting them to
diffusion models built upon the Diffusion Transformer remains challenging.
Initially, introducing latent space features from the garment reference branch
requires adding or modifying the backbone network, leading to a large number of
trainable parameters. Subsequently, the latent space features of garments lack
inherent temporal characteristics and thus require additional learning. To
address these challenges, we propose a novel approach, OIE (Once is Enough), a
virtual try-on strategy based on first-frame clothing replacement:
specifically, we employ an image-based clothing transfer model to replace the
clothing in the initial frame, and then, under the content control of the
edited first frame, utilize pose and mask information to guide the temporal
prior of the video generation model in synthesizing the remaining frames
sequentially. Experiments show that our method achieves superior parameter
efficiency and computational efficiency while still maintaining leading
performance under these constraints.

</details>


### [68] [MONKEY: Masking ON KEY-Value Activation Adapter for Personalization](https://arxiv.org/abs/2510.07656)
*James Baker*

Main category: cs.CV

> Error

<details>
  <summary>Details</summary>

**Motivation:** Error

**Method:** Error

**Result:** Error

**Conclusion:** Error

**Abstract:** Personalizing diffusion models allows users to generate new images that
incorporate a given subject, allowing more control than a text prompt. These
models often suffer somewhat when they end up just recreating the subject
image, and ignoring the text prompt. We observe that one popular method for
personalization, the IP-Adapter automatically generates masks that we
definitively segment the subject from the background during inference. We
propose to use this automatically generated mask on a second pass to mask the
image tokens, thus restricting them to the subject, not the background,
allowing the text prompt to attend to the rest of the image. For text prompts
describing locations and places, this produces images that accurately depict
the subject while definitively matching the prompt. We compare our method to a
few other test time personalization methods, and find our method displays high
prompt and source image alignment.

</details>


### [69] [Automatic Text Box Placement for Supporting Typographic Design](https://arxiv.org/abs/2510.07665)
*Jun Muraoka,Daichi Haraguchi,Naoto Inoue,Wataru Shimoda,Kota Yamaguchi,Seiichi Uchida*

Main category: cs.CV

> 研究对比了不同模型在不完整布局中的文本框自动放置效果，标准Transformer表现最优，但所有模型在处理小型文本和密集布局时均有局限。

<details>
  <summary>Details</summary>

**Motivation:** 在广告和网页设计的布局设计中，平衡视觉吸引力和沟通效率至关重要。

**Method:** 本研究比较了标准的Transformer模型、小型视觉语言模型(Phi3.5-vision)、大型预训练视觉语言模型(Gemini)以及扩展的处理多图像的Transformer模型，在不完整布局设计中自动放置文本框的效果。

**Result:** 在Crello数据集上的评估显示，标准的Transformer模型通常优于视觉语言模型方法，尤其是在包含更丰富外观信息的情况下。然而，所有方法在处理非常小的文本或充满元素的布局时都遇到了挑战。

**Conclusion:** 这些发现强调了任务特定架构的优势，并且为自动化布局设计的进一步改进指明了方向。

**Abstract:** In layout design for advertisements and web pages, balancing visual appeal
and communication efficiency is crucial. This study examines automated text box
placement in incomplete layouts, comparing a standard Transformer-based method,
a small Vision and Language Model (Phi3.5-vision), a large pretrained VLM
(Gemini), and an extended Transformer that processes multiple images.
Evaluations on the Crello dataset show the standard Transformer-based models
generally outperform VLM-based approaches, particularly when incorporating
richer appearance information. However, all methods face challenges with very
small text or densely populated layouts. These findings highlight the benefits
of task-specific architectures and suggest avenues for further improvement in
automated layout design.

</details>


### [70] [TCIP: Threshold-Controlled Iterative Pyramid Network for Deformable Medical Image Registration](https://arxiv.org/abs/2510.07666)
*Heming Wu,Di Wang,Tai Ma,Peng Zhao,Yubin Xiao,Zhongke Wu,Xing-Ce Wang,Chuang Li,Xuan Wu,You Zhou*

Main category: cs.CV

> 本文提出TCIP模型，结合FERM和TCI策略，有效减少了解剖结构对齐误差，自适应迭代次数，提升了图像配准的精度。

<details>
  <summary>Details</summary>

**Motivation:** 现有的金字塔网络在医学图像配准中表现出色，但其解码器结构容易积累解剖结构误差，并且无法根据不同的图像自适应确定迭代次数，这会导致过早终止或不必要的迭代，降低配准精度。

**Method:** 本文提出Feature-Enhanced Residual Module (FERM) 和双阶段阈值控制迭代（TCI）策略，用于解决现有金字塔网络解码器结构容易积累解剖结构对齐误差以及无法自适应地确定迭代次数的问题。

**Result:** 在三个公共的脑MRI数据集和一个腹部CT数据集上的大量实验表明，TCIP在精度上超越了现有的最先进的注册网络，同时保持了相当的推理速度和紧凑的模型参数大小。

**Conclusion:** 实验结果表明，TCIP在医学图像配准中表现优越，功能增强残差模块和双阶段阈值控制迭代策略有效提高了对齐精度。

**Abstract:** Although pyramid networks have demonstrated superior performance in
deformable medical image registration, their decoder architectures are
inherently prone to propagating and accumulating anatomical structure
misalignments. Moreover, most existing models do not adaptively determine the
number of iterations for optimization under varying deformation requirements
across images, resulting in either premature termination or excessive
iterations that degrades registration accuracy. To effectively mitigate the
accumulation of anatomical misalignments, we propose the Feature-Enhanced
Residual Module (FERM) as the core component of each decoding layer in the
pyramid network. FERM comprises three sequential blocks that extract anatomical
semantic features, learn to suppress irrelevant features, and estimate the
final deformation field, respectively. To adaptively determine the number of
iterations for varying images, we propose the dual-stage Threshold-Controlled
Iterative (TCI) strategy. In the first stage, TCI assesses registration
stability and with asserted stability, it continues with the second stage to
evaluate convergence. We coin the model that integrates FERM and TCI as
Threshold-Controlled Iterative Pyramid (TCIP). Extensive experiments on three
public brain MRI datasets and one abdomen CT dataset demonstrate that TCIP
outperforms the state-of-the-art (SOTA) registration networks in terms of
accuracy, while maintaining comparable inference speed and a compact model
parameter size. Finally, we assess the generalizability of FERM and TCI by
integrating them with existing registration networks and further conduct
ablation studies to validate the effectiveness of these two proposed methods.

</details>


### [71] [Controllable Video Synthesis via Variational Inference](https://arxiv.org/abs/2510.07670)
*Haoyi Duan,Yunzhi Zhang,Yilun Du,Jiajun Wu*

Main category: cs.CV

> 本文提出了一种视频合成方法，通过变分推理，使用多个生成模型共同完成任务，以提供高度可控的生成同时保持样本的多样性，实验显示该方法优于先前的工作。

<details>
  <summary>Details</summary>

**Motivation:** 许多视频工作流程受益于混合用户控制的使用，控制的粒度从精确的4D对象轨迹和相机路径到粗略的文字提示不等，而现有的视频生成模型通常针对固定输入格式进行训练。我们开发的方法解决了这一需求，为指定元素提供高度控制的同时保持对未指定元素的多样性。

**Method:** 我们开发了一种将任务视为变分推断的视频合成方法，通过使用多个视频生成模型来共同满足所有任务约束。为了应对优化挑战，我们将问题分解为通过逐步最小化KL散度来优化一系列退火分布，并提出了一种上下文条件因子化技术，以减少解空间中的模式，从而避免局部最优解。

**Result:** 实验表明，与以前的工作相比，我们的方法生成的样本具有改进的可控性，多样性以及3D一致性。

**Conclusion:** 本研究提出的方法在可控性，多样性和3D一致性方面都优于先前工作。此方法展示了在视频合成领域结合深入的用户控制和多样化表现的可能性。

**Abstract:** Many video workflows benefit from a mixture of user controls with varying
granularity, from exact 4D object trajectories and camera paths to coarse text
prompts, while existing video generative models are typically trained for fixed
input formats. We develop a video synthesis method that addresses this need and
generates samples with high controllability for specified elements while
maintaining diversity for under-specified ones. We cast the task as variational
inference to approximate a composed distribution, leveraging multiple video
generation backbones to account for all task constraints collectively. To
address the optimization challenge, we break down the problem into step-wise KL
divergence minimization over an annealed sequence of distributions, and further
propose a context-conditioned factorization technique that reduces modes in the
solution space to circumvent local optima. Experiments suggest that our method
produces samples with improved controllability, diversity, and 3D consistency
compared to prior works.

</details>


### [72] [Hybrid CNN-BYOL Approach for Fault Detection in Induction Motors Using Thermal Images](https://arxiv.org/abs/2510.07692)
*Tangin Amir Smrity,MD Zahin Muntaqim Hasan Muhammad Kafi,Abu Saleh Musa Miah,Najmul Hassan,Yuichi Okuyama,Nobuyoshi Asai,Taro Suzuki,Jungpil Shin*

Main category: cs.CV

> A hybrid BYOL-CNN method is proposed to detect faults in induction motors by classifying thermal images, with the newly introduced BYOL-IMNet achieving 99.89% accuracy and a rapid inference time, surpassing existing models.

<details>
  <summary>Details</summary>

**Motivation:** The purpose of this study is to develop a method that can accurately and swiftly detect faults in induction motors to prevent overheating, wasted energy, and service failure, thereby extending the motor's lifespan and ensuring safety.

**Method:** This paper presents a hybrid method that integrates Bootstrap Your Own Latent (BYOL) with Convolutional Neural Networks (CNNs) to classify thermal images of induction motors for fault detection. It explores multiple deep learning models for the BYOL technique, including ResNet-50, DenseNet-121, DenseNet-169, EfficientNetB0, VGG16, and MobileNetV2. Additionally, it introduces a new CNN model, BYOL-IMNet, tailored for fault classification in thermal images with high performance and light weight.

**Result:** Experiments show that BYOL-IMNet achieves a remarkable test accuracy of 99.89% with an inference speed of 5.7 ms per image, outperforming current state-of-the-art models.

**Conclusion:** The proposed BYOL-IMNet hybrid method exhibits exceptional performance in fault detection for induction motors by combining BYOL and CNNs. It offers a robust solution for online motor condition monitoring in industrial environments.

**Abstract:** Induction motors (IMs) are indispensable in industrial and daily life, but
they are susceptible to various faults that can lead to overheating, wasted
energy consumption, and service failure. Early detection of faults is essential
to protect the motor and prolong its lifespan. This paper presents a hybrid
method that integrates BYOL with CNNs for classifying thermal images of
induction motors for fault detection. The thermal dataset used in this work
includes different operating states of the motor, such as normal operation,
overload, and faults. We employed multiple deep learning (DL) models for the
BYOL technique, ranging from popular architectures such as ResNet-50,
DenseNet-121, DenseNet-169, EfficientNetB0, VGG16, and MobileNetV2.
Additionally, we introduced a new high-performance yet lightweight CNN model
named BYOL-IMNet, which comprises four custom-designed blocks tailored for
fault classification in thermal images. Our experimental results demonstrate
that the proposed BYOL-IMNet achieves 99.89\% test accuracy and an inference
time of 5.7 ms per image, outperforming state-of-the-art models. This study
highlights the promising performance of the CNN-BYOL hybrid method in enhancing
accuracy for detecting faults in induction motors, offering a robust
methodology for online monitoring in industrial settings.

</details>


### [73] [Mutual Learning for Hashing: Unlocking Strong Hash Functions from Weak Supervision](https://arxiv.org/abs/2510.07703)
*Xiaoxu Ma,Runhao Li,Zhenyu Weng*

Main category: cs.CV

> 该论文提出了一个名为MLH的框架，通过互相学习和跨分支交互的方法，解决了基于中心方法在学习哈希函数时局部相似性信息利用不足的问题，在多个基准数据集上表现优于最先进的哈希方法。

<details>
  <summary>Details</summary>

**Motivation:** 该论文本旨在解决基于中心的方法在建模全局结构时往往未能充分利用重要局部相似性信息的问题，提出了一种名为互学习哈希（MLH）的新框架。

**Method:** 该论文提出了一种名为互学习哈希（MLH）的新框架，包含一个强的基于中心的分支和一个较弱的基于成对的分支。通过互相学习来增强基于中心分支的局部相似性。同时还引入了一种名为哈希专家混合模块的新模块，以增强跨分支的有效交互。

**Result:** {

**Conclusion:** 该论文得出结论，MLH方法在多个基准数据集上能超越其他最先进的哈希方法。这个框架通过引入哈希专家混合模块，增强了跨分支的有效交互和学习，从而改善了局部相似性的利用并提高了整体性能。

**Abstract:** Deep hashing has been widely adopted for large-scale image retrieval, with
numerous strategies proposed to optimize hash function learning. Pairwise-based
methods are effective in learning hash functions that preserve local similarity
relationships, whereas center-based methods typically achieve superior
performance by more effectively capturing global data distributions. However,
the strength of center-based methods in modeling global structures often comes
at the expense of underutilizing important local similarity information. To
address this limitation, we propose Mutual Learning for Hashing (MLH), a novel
weak-to-strong framework that enhances a center-based hashing branch by
transferring knowledge from a weaker pairwise-based branch. MLH consists of two
branches: a strong center-based branch and a weaker pairwise-based branch.
Through an iterative mutual learning process, the center-based branch leverages
local similarity cues learned by the pairwise-based branch. Furthermore,
inspired by the mixture-of-experts paradigm, we introduce a novel
mixture-of-hash-experts module that enables effective cross-branch interaction,
further enhancing the performance of both branches. Extensive experiments
demonstrate that MLH consistently outperforms state-of-the-art hashing methods
across multiple benchmark datasets.

</details>


### [74] [RePainter: Empowering E-commerce Object Removal via Spatial-matting Reinforcement Learning](https://arxiv.org/abs/2510.07721)
*Zipeng Guo,Lichen Ma,Xiaolong Fu,Gaojing Zhou,Lan Yang,Yuchen Zhou,Linkai Liu,Yu He,Ximan Liu,Shiping Dong,Jingling Fu,Zhen Chen,Yu Shi,Junshi Huang,Jason Li,Chao Gou*

Main category: cs.CV

> 针对电子商务平台产品图片中的扰动元素问题，提出Repainter，利用强化学习技术改进图像修复效果，减少了广告和水印等元素对视觉效果的影响。

<details>
  <summary>Details</summary>

**Motivation:** 在电子商务平台中，产品图片对于提高用户参与度及广告效果至关重要，但水印和促销文字等侵入元素仍然是提供清晰吸引人的产品视觉的主要障碍。虽然基于扩散的图像修复方法已经进步，但在实际商业应用中仍然面临物体去除不可靠和领域特定适应性有限的挑战。

**Method:** Repainter, 一个结合空间抠图轨迹细化与组相对策略优化（GRPO）的强化学习框架，调整注意力机制强调背景上下文，生成高奖励样本并减少不必要的物体插入。

**Result:** 通过引入综合奖励机制来平衡全局、局部和语义约束，减少视觉伪影并防止奖励欺骗，实验结果表明Repainter显著优于最先进的方法，在具有复杂构成的挑战性场景中尤其如此。

**Conclusion:** Repainter通过强化学习方法显著改进了电子商务领域的图像修复效果，特别是在解决复杂背景下的物体移除问题上展现了优势。

**Abstract:** In web data, product images are central to boosting user engagement and
advertising efficacy on e-commerce platforms, yet the intrusive elements such
as watermarks and promotional text remain major obstacles to delivering clear
and appealing product visuals. Although diffusion-based inpainting methods have
advanced, they still face challenges in commercial settings due to unreliable
object removal and limited domain-specific adaptation. To tackle these
challenges, we propose Repainter, a reinforcement learning framework that
integrates spatial-matting trajectory refinement with Group Relative Policy
Optimization (GRPO). Our approach modulates attention mechanisms to emphasize
background context, generating higher-reward samples and reducing unwanted
object insertion. We also introduce a composite reward mechanism that balances
global, local, and semantic constraints, effectively reducing visual artifacts
and reward hacking. Additionally, we contribute EcomPaint-100K, a high-quality,
large-scale e-commerce inpainting dataset, and a standardized benchmark
EcomPaint-Bench for fair evaluation. Extensive experiments demonstrate that
Repainter significantly outperforms state-of-the-art methods, especially in
challenging scenes with intricate compositions. We will release our code and
weights upon acceptance.

</details>


### [75] [SyncHuman: Synchronizing 2D and 3D Generative Models for Single-view Human Reconstruction](https://arxiv.org/abs/2510.07723)
*Wenyue Chen,Peng Li,Wangguandong Zheng,Chengfeng Zhao,Mengfei Li,Yaolong Zhu,Zhiyang Dou,Ronggang Wang,Yuan Liu*

Main category: cs.CV

> 本文介绍了一种名为SyncHuman的框架，能够从单视角图像重建高质量的着装人体网格，即使在面对挑战性姿态时，依然表现出色。

<details>
  <summary>Details</summary>

**Motivation:** 传统的方法依赖于SMPL估计和受SMPL条件限制的图像生成模型来产生新视角，但这些方法面临3D先验估计不准确的问题，并且难以处理复杂的人体姿态和重建精细的细节。

**Method:** 本文提出了SyncHuman框架，首次结合了2D多视角生成模型和3D原生生成模型。2D多视角生成模型擅长捕捉细小的2D细节但难以保持结构一致性，而3D原生生成模型在生成粗糙但结构一致的3D形状方面更有优势。通过结合这两种方法的优势，作者开发了更有效的生成框架。

**Result:** 实验结果表明，SyncHuman框架实现了鲁棒且逼真的3D人体重建，即使对于具有挑战性的姿态图像，也能表现良好。本文的方法在几何准确性和视觉保真度上均优于基线方法。

**Conclusion:** SyncHuman证明了结合2D多视角生成模型和3D原生生成模型的潜力，不仅在几何准确性上表现优越，在视觉保真度上也领先于基线方法，为未来3D生成模型研究开辟了新的方向。

**Abstract:** Photorealistic 3D full-body human reconstruction from a single image is a
critical yet challenging task for applications in films and video games due to
inherent ambiguities and severe self-occlusions. While recent approaches
leverage SMPL estimation and SMPL-conditioned image generative models to
hallucinate novel views, they suffer from inaccurate 3D priors estimated from
SMPL meshes and have difficulty in handling difficult human poses and
reconstructing fine details. In this paper, we propose SyncHuman, a novel
framework that combines 2D multiview generative model and 3D native generative
model for the first time, enabling high-quality clothed human mesh
reconstruction from single-view images even under challenging human poses.
Multiview generative model excels at capturing fine 2D details but struggles
with structural consistency, whereas 3D native generative model generates
coarse yet structurally consistent 3D shapes. By integrating the complementary
strengths of these two approaches, we develop a more effective generation
framework. Specifically, we first jointly fine-tune the multiview generative
model and the 3D native generative model with proposed pixel-aligned 2D-3D
synchronization attention to produce geometrically aligned 3D shapes and 2D
multiview images. To further improve details, we introduce a feature injection
mechanism that lifts fine details from 2D multiview images onto the aligned 3D
shapes, enabling accurate and high-fidelity reconstruction. Extensive
experiments demonstrate that SyncHuman achieves robust and photo-realistic 3D
human reconstruction, even for images with challenging poses. Our method
outperforms baseline methods in geometric accuracy and visual fidelity,
demonstrating a promising direction for future 3D generation models.

</details>


### [76] [ComGS: Efficient 3D Object-Scene Composition via Surface Octahedral Probes](https://arxiv.org/abs/2510.07729)
*Jian Gao,Mengqi Yuan,Yifei Zeng,Chang Zeng,Zhihao Li,Zhenyu Chen,Weichao Qiu,Xiao-Xiao Long,Hao Zhu,Xun Cao,Yao Yao*

Main category: cs.CV

> 本文提出ComGS框架，通过Surface Octahedral Probes(SOPs)提高高斯渲染效率并解决真实3D物体与场景组合中出现的不一致性问题。

<details>
  <summary>Details</summary>

**Motivation:** 现有高斯散射技术在渲染真实3D对象时存在镶嵌不一致的问题，需要开发新的方法来解决这些挑战。

**Method:** 采用Surface Octahedral Probes (SOPs)存储光照和遮挡信息，提高渲染效率并简化复杂场景的光照估计。

**Result:** ComGS实现了高质量，实时渲染，帧速率达到28 FPS，且编辑时间为36秒，视觉效果良好。

**Conclusion:** ComGS是一种新的3D对象-场景组合框架，显著提高了渲染质量和速度，解决了现有方法中存在的问题。

**Abstract:** Gaussian Splatting (GS) enables immersive rendering, but realistic 3D
object-scene composition remains challenging. Baked appearance and shadow
information in GS radiance fields cause inconsistencies when combining objects
and scenes. Addressing this requires relightable object reconstruction and
scene lighting estimation. For relightable object reconstruction, existing
Gaussian-based inverse rendering methods often rely on ray tracing, leading to
low efficiency. We introduce Surface Octahedral Probes (SOPs), which store
lighting and occlusion information and allow efficient 3D querying via
interpolation, avoiding expensive ray tracing. SOPs provide at least a 2x
speedup in reconstruction and enable real-time shadow computation in Gaussian
scenes. For lighting estimation, existing Gaussian-based inverse rendering
methods struggle to model intricate light transport and often fail in complex
scenes, while learning-based methods predict lighting from a single image and
are viewpoint-sensitive. We observe that 3D object-scene composition primarily
concerns the object's appearance and nearby shadows. Thus, we simplify the
challenging task of full scene lighting estimation by focusing on the
environment lighting at the object's placement. Specifically, we capture a 360
degrees reconstructed radiance field of the scene at the location and fine-tune
a diffusion model to complete the lighting. Building on these advances, we
propose ComGS, a novel 3D object-scene composition framework. Our method
achieves high-quality, real-time rendering at around 28 FPS, produces visually
harmonious results with vivid shadows, and requires only 36 seconds for
editing. Code and dataset are available at
https://nju-3dv.github.io/projects/ComGS/.

</details>


### [77] [UltraLED: Learning to See Everything in Ultra-High Dynamic Range Scenes](https://arxiv.org/abs/2510.07741)
*Yuang Meng,Xin Jin,Lina Lei,Chun-Le Guo,Chongyi Li*

Main category: cs.CV

> 本文介绍了一种仅使用单个短曝光RAW图像来处理超高动态范围场景的方法，避免了鬼影和运动模糊，提升了暗区细节的恢复效果。

<details>
  <summary>Details</summary>

**Motivation:** 夜间场景中由于光线变化较大，高动态范围场景的明亮和暗区之间的曝光差异显著。现有的基于RGB的包围曝光方法存在伪影和对齐问题。

**Method:** 提出了一种两阶段框架UltraLED，首先通过比率图执行曝光校正来平衡动态范围，然后使用亮度感知的RAW图像去噪器来增强暗区细节的恢复。

**Result:** 通过使用9档包围曝光管线合成现实场景的超高动态范围图像，实验表明UltraLED方法显著优于现有的单帧处理方法。

**Conclusion:** 实验表明，UltraLED显著优于现有的单帧方法。代码和数据集已公开。

**Abstract:** Ultra-high dynamic range (UHDR) scenes exhibit significant exposure
disparities between bright and dark regions. Such conditions are commonly
encountered in nighttime scenes with light sources. Even with standard exposure
settings, a bimodal intensity distribution with boundary peaks often emerges,
making it difficult to preserve both highlight and shadow details
simultaneously. RGB-based bracketing methods can capture details at both ends
using short-long exposure pairs, but are susceptible to misalignment and
ghosting artifacts. We found that a short-exposure image already retains
sufficient highlight detail. The main challenge of UHDR reconstruction lies in
denoising and recovering information in dark regions. In comparison to the RGB
images, RAW images, thanks to their higher bit depth and more predictable noise
characteristics, offer greater potential for addressing this challenge. This
raises a key question: can we learn to see everything in UHDR scenes using only
a single short-exposure RAW image? In this study, we rely solely on a single
short-exposure frame, which inherently avoids ghosting and motion blur, making
it particularly robust in dynamic scenes. To achieve that, we introduce
UltraLED, a two-stage framework that performs exposure correction via a ratio
map to balance dynamic range, followed by a brightness-aware RAW denoiser to
enhance detail recovery in dark regions. To support this setting, we design a
9-stop bracketing pipeline to synthesize realistic UHDR images and contribute a
corresponding dataset based on diverse scenes, using only the shortest exposure
as input for reconstruction. Extensive experiments show that UltraLED
significantly outperforms existing single-frame approaches. Our code and
dataset are made publicly available at
https://srameo.github.io/projects/ultraled.

</details>


### [78] [DEGS: Deformable Event-based 3D Gaussian Splatting from RGB and Event Stream](https://arxiv.org/abs/2510.07752)
*Junhao He,Jiaxu Wang,Jia Li,Mingyuan Sun,Qiang Zhang,Jiahang Cao,Ziyi Zhang,Yi Gu,Jingkai Sun,Renjing Xu*

Main category: cs.CV

> 本文提出了一种结合RGB图像和事件流数据来优化动态3DGS的新框架。通过使用运动先验指导形变场的优化，解决了动态3D高斯散斑重建中帧间大运动带来的不确定性问题。

<details>
  <summary>Details</summary>

**Motivation:** 由于帧间大运动增加了动态3D高斯散斑重建的不确定性，而RGB图像和事件数据模态间的巨大差异使联合优化变得具有挑战性。因此，本文旨在通过结合RGB和事件流数据来解决这个问题。

**Method:** 本文提出了一种新的框架，该框架结合了RGB图像和事件流数据，用于动态3D高斯散斑重建。该方法首先使用提出的LoCM无监督微调框架来提取事件流中的运动先验，然后通过几何感知数据关联方法建立事件-高斯运动对应关系。

**Result:** 实验结果表明，相较于现有基于图像和基于事件的方法，该方法在合成场景和真实场景中都表现出色，证明了在事件数据的帮助下可以有效地优化动态3DGS。

**Conclusion:** 本文提出了一种联合优化动态3DGS的新方法，结合了RGB图像和高帧率事件流数据，并通过实验验证了其有效性。

**Abstract:** Reconstructing Dynamic 3D Gaussian Splatting (3DGS) from low-framerate RGB
videos is challenging. This is because large inter-frame motions will increase
the uncertainty of the solution space. For example, one pixel in the first
frame might have more choices to reach the corresponding pixel in the second
frame. Event cameras can asynchronously capture rapid visual changes and are
robust to motion blur, but they do not provide color information. Intuitively,
the event stream can provide deterministic constraints for the inter-frame
large motion by the event trajectories. Hence, combining
low-temporal-resolution images with high-framerate event streams can address
this challenge. However, it is challenging to jointly optimize Dynamic 3DGS
using both RGB and event modalities due to the significant discrepancy between
these two data modalities. This paper introduces a novel framework that jointly
optimizes dynamic 3DGS from the two modalities. The key idea is to adopt event
motion priors to guide the optimization of the deformation fields. First, we
extract the motion priors encoded in event streams by using the proposed LoCM
unsupervised fine-tuning framework to adapt an event flow estimator to a
certain unseen scene. Then, we present the geometry-aware data association
method to build the event-Gaussian motion correspondence, which is the primary
foundation of the pipeline, accompanied by two useful strategies, namely motion
decomposition and inter-frame pseudo-label. Extensive experiments show that our
method outperforms existing image and event-based approaches across synthetic
and real scenes and prove that our method can effectively optimize dynamic 3DGS
with the help of event data.

</details>


### [79] [Demystifying Deep Learning-based Brain Tumor Segmentation with 3D UNets and Explainable AI (XAI): A Comparative Analysis](https://arxiv.org/abs/2510.07785)
*Ming Jie Ong,Sze Yinn Ung,Sim Kuan Goh,Jimmy Y. Zhong*

Main category: cs.CV

> 研究使用可解释的人工智能技术提高MRI图像中脑瘤分割的准确性，应用了UNet、ResUNet、和AttUNet三种模型，其中ResUNet表现最佳并推荐用于临床评估。

<details>
  <summary>Details</summary>

**Motivation:** 该研究的动机是通过XAI技术提升脑瘤图像分割的准确性，辅助医生的临床诊断。

**Method:** 研究评估了三种深度学习模型：UNet、ResUNet、和AttUNet，使用了Grad-CAM和注意力可视化技术，基于BraTS2020数据集进行模型训练和评估。

**Result:** ResUNet在Dice和Jaccard相似性分数以及准确率、召回率和F1分数上表现出色。

**Conclusion:** 研究结论推荐使用ResUNet进行脑瘤自动化分割，以提高未来的临床评估准确性。

**Abstract:** The current study investigated the use of Explainable Artificial Intelligence
(XAI) to improve the accuracy of brain tumor segmentation in MRI images, with
the goal of assisting physicians in clinical decision-making. The study focused
on applying UNet models for brain tumor segmentation and using the XAI
techniques of Gradient-weighted Class Activation Mapping (Grad-CAM) and
attention-based visualization to enhance the understanding of these models.
Three deep learning models - UNet, Residual UNet (ResUNet), and Attention UNet
(AttUNet) - were evaluated to identify the best-performing model. XAI was
employed with the aims of clarifying model decisions and increasing physicians'
trust in these models. We compared the performance of two UNet variants
(ResUNet and AttUNet) with the conventional UNet in segmenting brain tumors
from the BraTS2020 public dataset and analyzed model predictions with Grad-CAM
and attention-based visualization. Using the latest computer hardware, we
trained and validated each model using the Adam optimizer and assessed their
performance with respect to: (i) training, validation, and inference times,
(ii) segmentation similarity coefficients and loss functions, and (iii)
classification performance. Notably, during the final testing phase, ResUNet
outperformed the other models with respect to Dice and Jaccard similarity
scores, as well as accuracy, recall, and F1 scores. Grad-CAM provided
visuospatial insights into the tumor subregions each UNet model focused on
while attention-based visualization provided valuable insights into the working
mechanisms of AttUNet's attention modules. These results demonstrated ResUNet
as the best-performing model and we conclude by recommending its use for
automated brain tumor segmentation in future clinical assessments. Our source
code and checkpoint are available at
https://github.com/ethanong98/MultiModel-XAI-Brats2020

</details>


### [80] [GTR-Bench: Evaluating Geo-Temporal Reasoning in Vision-Language Models](https://arxiv.org/abs/2510.07791)
*Qinghongbing Xie,Zhaoyuan Xia,Feng Zhu,Lijun Gong,Ziyue Li,Rui Zhao,Long Zeng*

Main category: cs.CV

> 介绍了GTR-Bench来评估VLMs在地理时空推理中的能力，展示了它们与人类表现之间的差距，并指出了当前模型的缺陷。

<details>
  <summary>Details</summary>

**Motivation:** 现有的时空基准主要侧重于基于图像/视频上下文的以自我为中心的视角推理，或基于图形上下文的地理视角推理，这无法评估VLMs在结合图像/视频和图形上下文时的地理时空智能。这项研究是为了填补这一空白。

**Method:** 引入了Geo-Temporal Reasoning基准(GTR-Bench)，这是一个在大规模摄像头网络中对移动目标进行地理时间推理的新挑战。该基准提出了在地图与视频间进行多次视角转换、跨多个非重叠视场的视频进行联合推理，以及对任何视频上下文未观察到的时空区域进行推理的要求。

**Result:** 在GTR-Bench上对10多个流行的VLMs进行评估表明，即使是最先进的Gemini-2.5-Pro模型（准确率为34.9%），也显著落后于人类（准确率为78.61%）在地理时空推理上的表现。

**Conclusion:** 这项研究揭示了当前模型在地理时空推理上的三个主要缺陷：不平衡使用时空上下文、弱时间预测能力，以及理解和对齐地图数据与多视角视频输入能力的不足。GTR-Bench为时空智能的研究和应用提供了有价值的见解和新的机会。

**Abstract:** Recently spatial-temporal intelligence of Visual-Language Models (VLMs) has
attracted much attention due to its importance for Autonomous Driving, Embodied
AI and General Artificial Intelligence. Existing spatial-temporal benchmarks
mainly focus on egocentric perspective reasoning with images/video context, or
geographic perspective reasoning with graphics context (eg. a map), thus fail
to assess VLMs' geographic spatial-temporal intelligence with both images/video
and graphics context, which is important for areas like traffic management and
emergency response. To address the gaps, we introduce Geo-Temporal Reasoning
benchmark (GTR-Bench), a novel challenge for geographic temporal reasoning of
moving targets in a large-scale camera network. GTR-Bench is more challenging
as it requires multiple perspective switches between maps and videos, joint
reasoning across multiple videos with non-overlapping fields of view, and
inference over spatial-temporal regions that are unobserved by any video
context. Evaluations of more than 10 popular VLMs on GTR-Bench demonstrate that
even the best proprietary model, Gemini-2.5-Pro (34.9%), significantly lags
behind human performance (78.61%) on geo-temporal reasoning. Moreover, our
comprehensive analysis on GTR-Bench reveals three primary deficiencies of
current models for geo-temporal reasoning. (1) VLMs' reasoning is impaired by
an imbalanced utilization of spatial-temporal context. (2) VLMs are weak in
temporal forecasting, which leads to worse performance on temporal-emphasized
tasks than on spatial-emphasized tasks. (3) VLMs lack the proficiency to
comprehend or align the map data with multi-view video inputs. We believe
GTR-Bench offers valuable insights and opens up new opportunities for research
and applications in spatial-temporal intelligence. Benchmark and code will be
released at https://github.com/X-Luffy/GTR-Bench.

</details>


### [81] [FMANet: A Novel Dual-Phase Optical Flow Approach with Fusion Motion Attention Network for Robust Micro-expression Recognition](https://arxiv.org/abs/2510.07810)
*Luu Tu Nguyen,Vu Tram Anh Khuong,Thi Bich Phuong Man,Thi Duyen Ngo,Thanh Ha Le*

Main category: cs.CV

> 本文提出了一种新的光流表示方法MM-COF及相应的神经网络架构FMANet，实验证明此方法能有效提升微表情识别性能。

<details>
  <summary>Details</summary>

**Motivation:** 现有的方法仅在微表情开始和顶峰帧之间计算光流，忽略了峰点到结束阶段的重要运动信息。为了改进这一点，本文提出了更全面的光流表示方法和识别网络模型。

**Method:** 本文提出了一种新的运动表示方法——幅度调制组合光流（MM-COF），该方法结合了微表情的两个阶段的运动动力学，进而在识别网络中使用。此外，还设计了一个端到端的神经网络架构FMANet，该架构将两个阶段的分析和幅度调制纳入可学习模块，使网络能自适应地融合运动线索并专注于面部关键区域进行分类。

**Result:** 实验结果表明，在MMEW、SMIC、CASME-II和SAMM四个数据集上，所提出的MM-COF表示方法和FMANet网络优于现有方法。

**Conclusion:** 论文表明，利用可学习的双阶段框架能够提升微表情识别的性能。

**Abstract:** Facial micro-expressions, characterized by their subtle and brief nature, are
valuable indicators of genuine emotions. Despite their significance in
psychology, security, and behavioral analysis, micro-expression recognition
remains challenging due to the difficulty of capturing subtle facial movements.
Optical flow has been widely employed as an input modality for this task due to
its effectiveness. However, most existing methods compute optical flow only
between the onset and apex frames, thereby overlooking essential motion
information in the apex-to-offset phase. To address this limitation, we first
introduce a comprehensive motion representation, termed Magnitude-Modulated
Combined Optical Flow (MM-COF), which integrates motion dynamics from both
micro-expression phases into a unified descriptor suitable for direct use in
recognition networks. Building upon this principle, we then propose FMANet, a
novel end-to-end neural network architecture that internalizes the dual-phase
analysis and magnitude modulation into learnable modules. This allows the
network to adaptively fuse motion cues and focus on salient facial regions for
classification. Experimental evaluations on the MMEW, SMIC, CASME-II, and SAMM
datasets, widely recognized as standard benchmarks, demonstrate that our
proposed MM-COF representation and FMANet outperforms existing methods,
underscoring the potential of a learnable, dual-phase framework in advancing
micro-expression recognition.

</details>


### [82] [An End-to-End Room Geometry Constrained Depth Estimation Framework for Indoor Panorama Images](https://arxiv.org/abs/2510.07817)
*Kanglin Ning,Ruzhao Chen,Penghong Wang,Xingtao Wang,Ruiqin Xiong,Xiaopeng Fan*

Main category: cs.CV

> Error

<details>
  <summary>Details</summary>

**Motivation:** Error

**Method:** Error

**Result:** Error

**Conclusion:** Error

**Abstract:** Predicting spherical pixel depth from monocular $360^{\circ}$ indoor
panoramas is critical for many vision applications. However, existing methods
focus on pixel-level accuracy, causing oversmoothed room corners and noise
sensitivity. In this paper, we propose a depth estimation framework based on
room geometry constraints, which extracts room geometry information through
layout prediction and integrates those information into the depth estimation
process through background segmentation mechanism. At the model level, our
framework comprises a shared feature encoder followed by task-specific decoders
for layout estimation, depth estimation, and background segmentation. The
shared encoder extracts multi-scale features, which are subsequently processed
by individual decoders to generate initial predictions: a depth map, a room
layout map, and a background segmentation map. Furthermore, our framework
incorporates two strategies: a room geometry-based background depth resolving
strategy and a background-segmentation-guided fusion mechanism. The proposed
room-geometry-based background depth resolving strategy leverages the room
layout and the depth decoder's output to generate the corresponding background
depth map. Then, a background-segmentation-guided fusion strategy derives
fusion weights for the background and coarse depth maps from the segmentation
decoder's predictions. Extensive experimental results on the Stanford2D3D,
Matterport3D and Structured3D datasets show that our proposed methods can
achieve significantly superior performance than current open-source methods.
Our code is available at https://github.com/emiyaning/RGCNet.

</details>


### [83] [Enhancing Visual Prompting through Expanded Transformation Space and Overfitting Mitigation](https://arxiv.org/abs/2510.07823)
*Shohei Enomoto*

Main category: cs.CV

> 研究提出了ACAVP方法，通过引入仿射变换和颜色变换以及数据增强技术，提高了视觉提示技术在图像分类任务中的准确性和鲁棒性。

<details>
  <summary>Details</summary>

**Motivation:** 该研究旨在提升视觉提示方法的准确性和泛化能力，解决其表达能力有限和容易过拟合的问题，提出了一种新的方法 ACAVP，以提高图像分类任务的性能。

**Method:** ACAVP (Affine, Color, and Additive Visual Prompting) 是一种增强视觉提示表达能力的方法，通过引入仿射变换和颜色变换来创建任务特定的提示区域并强调与任务相关的视觉特征，解决了传统视觉提示方法表达能力有限和过拟合的问题。此外，还使用了 TrivialAugment 数据增强技术来缓解过拟合问题，这对现有方法也有明显提升。

**Result:** 在十二个不同的图像分类数据集上，使用两种不同的模型架构，ACAVP 达到了视觉提示方法中的最先进准确率，超过了线性探测的平均准确率，并且在保持最小计算开销的同时，展示出对分布偏移的良好鲁棒性。

**Conclusion:** 通过使用仿射变换、颜色变换和数据增强技术，ACAVP 方法能够有效提升图像分类任务的性能，并展示出对分布偏移的良好鲁棒性，同时保持较小的计算开销。这表明适当的数据增强对于视觉提示技术训练是普遍有益的。

**Abstract:** Visual prompting (VP) has emerged as a promising parameter-efficient
fine-tuning approach for adapting pre-trained vision models to downstream tasks
without modifying model parameters. Despite offering advantages like negligible
computational overhead and compatibility with black-box models, conventional VP
methods typically achieve lower accuracy than other adaptation approaches. Our
analysis reveals two critical limitations: the restricted expressivity of
simple additive transformation and a tendency toward overfitting when the
parameter count increases. To address these challenges, we propose ACAVP
(Affine, Color, and Additive Visual Prompting), which enhances VP's expressive
power by introducing complementary transformation operations: affine
transformation for creating task-specific prompt regions while preserving
original image information, and color transformation for emphasizing
task-relevant visual features. Additionally, we identify that overfitting is a
critical issue in VP training and introduce TrivialAugment as an effective data
augmentation, which not only benefits our approach but also significantly
improves existing VP methods, with performance gains of up to 12 percentage
points on certain datasets. This demonstrates that appropriate data
augmentation is universally beneficial for VP training. Extensive experiments
across twelve diverse image classification datasets with two different model
architectures demonstrate that ACAVP achieves state-of-the-art accuracy among
VP methods, surpasses linear probing in average accuracy, and exhibits superior
robustness to distribution shifts, all while maintaining minimal computational
overhead during inference.

</details>


### [84] [MMHOI: Modeling Complex 3D Multi-Human Multi-Object Interactions](https://arxiv.org/abs/2510.07828)
*Kaen Kogashi,Anoop Cherian,Meng-Yu Jennifer Kuo*

Main category: cs.CV

> 本文提出了一种新的MOHOI数据集和相应的MMHOI-Net模型，该模型利用结构化的双补丁表示和动作识别技术实现了多人类物体交互的精确建模和预测。

<details>
  <summary>Details</summary>

**Motivation:** 由于现有的3D人体物体交互数据集仅涵盖了复杂交互行为的一部分，本文提出了一个大规模数据集MMHOI，以填补这一空白。MMHOI提供了12种日常场景的图像，并拥有完整的人体和物体3D形状和姿态注释，以及78个动作类别和14个交互特定体部分的标签，为下一代HOI研究提供了全面的测试平台。

**Method:** 本文介绍了一种名为MMHOI-Net的端到端变压器神经网络，该网络能够同时估计人类和物体的三维几何形状，它们之间的相互作用以及相关动作。其中，关键创新在于采用了一个结构化双补丁表示法来建模物体及其交互，并结合了动作识别来提高交互预测的准确性。

**Result:** 实验表明，作者提出的方法在多个数据集上达到了最先进的多人类-物体交互建模性能，不仅在准确性方面表现优异，在重建质量方面也表现出色。

**Conclusion:** 本文提出了一种新的大规模多人类多物体交互数据集MMHOI和一个名为MMHOI-Net的模型，在多个数据集上达到了最先进的多人类-物体交互建模性能。

**Abstract:** Real-world scenes often feature multiple humans interacting with multiple
objects in ways that are causal, goal-oriented, or cooperative. Yet existing 3D
human-object interaction (HOI) benchmarks consider only a fraction of these
complex interactions. To close this gap, we present MMHOI -- a large-scale,
Multi-human Multi-object Interaction dataset consisting of images from 12
everyday scenarios. MMHOI offers complete 3D shape and pose annotations for
every person and object, along with labels for 78 action categories and 14
interaction-specific body parts, providing a comprehensive testbed for
next-generation HOI research. Building on MMHOI, we present MMHOI-Net, an
end-to-end transformer-based neural network for jointly estimating human-object
3D geometries, their interactions, and associated actions. A key innovation in
our framework is a structured dual-patch representation for modeling objects
and their interactions, combined with action recognition to enhance the
interaction prediction. Experiments on MMHOI and the recently proposed CORE4D
datasets demonstrate that our approach achieves state-of-the-art performance in
multi-HOI modeling, excelling in both accuracy and reconstruction quality.

</details>


### [85] [PrismGS: Physically-Grounded Anti-Aliasing for High-Fidelity Large-Scale 3D Gaussian Splatting](https://arxiv.org/abs/2510.07830)
*Houqiang Zhong,Zhenglong Wu,Sihua Fu,Zihan Zheng,Xin Jin,Xiaoyun Zhang,Li Song,Qiang Hu*

Main category: cs.CV

> PrismGS方法改进了3D Gaussian Splatting在大规模城市环境中的渲染效果，通过金字塔多尺度监督和显式大小正则化减少了锯齿边缘和光影闪烁，提升了4K渲染质量。

<details>
  <summary>Details</summary>

**Motivation:** 尽管'分而治之'的管道处理方法已经解决了大规模场景的可伸缩性问题，但对于抗锯齿和优化稳定性问题尚不能有效解决。这些问题是由于高斯原始几何体与城市几何体的多尺度性质不匹配而导致的。为了解决这些问题，我们需要改进3D高斯的渲染行为，以适应大规模城市环境中的渲染。

**Method:** PrismGS采用了一种基于物理的正则化框架，以改进3D高斯粒子在大规模城市环境下的渲染效果。该方法结合了两种协同的正则化手段：金字塔多尺度监督和显式的大小正则化。前者通过监督渲染结果与预过滤图像金字塔的一致性来学习抗锯齿表示，后者通过对3D高斯粒子的尺寸设置物理合理性的下限来防止视图依赖异常形态的形成，从而提高几何表面的稳定性和减少锯齿边缘。

**Result:** 实验表明，PrismGS在MatrixCity, Mill-19和UrbanScene3D数据集上表现优越，相比CityGaussian方法有了大约1.5 dB的PSNR增益，在苛刻的4K渲染条件下仍能保持高质量和鲁棒性。

**Conclusion:** PrismGS方法能有效提高3D高斯渲染的稳定性和质量，特别在大规模城市环境中。其性能优越，兼容现有的渲染管道，且在4K等高分辨率渲染条件下具有显著的效果。

**Abstract:** 3D Gaussian Splatting (3DGS) has recently enabled real-time photorealistic
rendering in compact scenes, but scaling to large urban environments introduces
severe aliasing artifacts and optimization instability, especially under
high-resolution (e.g., 4K) rendering. These artifacts, manifesting as
flickering textures and jagged edges, arise from the mismatch between Gaussian
primitives and the multi-scale nature of urban geometry. While existing
``divide-and-conquer'' pipelines address scalability, they fail to resolve this
fidelity gap. In this paper, we propose PrismGS, a physically-grounded
regularization framework that improves the intrinsic rendering behavior of 3D
Gaussians. PrismGS integrates two synergistic regularizers. The first is
pyramidal multi-scale supervision, which enforces consistency by supervising
the rendering against a pre-filtered image pyramid. This compels the model to
learn an inherently anti-aliased representation that remains coherent across
different viewing scales, directly mitigating flickering textures. This is
complemented by an explicit size regularization that imposes a
physically-grounded lower bound on the dimensions of the 3D Gaussians. This
prevents the formation of degenerate, view-dependent primitives, leading to
more stable and plausible geometric surfaces and reducing jagged edges. Our
method is plug-and-play and compatible with existing pipelines. Extensive
experiments on MatrixCity, Mill-19, and UrbanScene3D demonstrate that PrismGS
achieves state-of-the-art performance, yielding significant PSNR gains around
1.5 dB against CityGaussian, while maintaining its superior quality and
robustness under demanding 4K rendering.

</details>


### [86] [IsoSignVid2Aud: Sign Language Video to Audio Conversion without Text Intermediaries](https://arxiv.org/abs/2510.07837)
*Harsh Kavediya,Vighnesh Nayak,Bheeshm Sharma,Balamurugan Palaniappan*

Main category: cs.CV

> 本文提出了一种新的端到端框架IsoSignVid2Aud，用于翻译连续的但非语法的手语视频序列到语音，实验显示良好的准确性和音频质量。

<details>
  <summary>Details</summary>

**Motivation:** 手语到口语的音频翻译对于连通听力和言语障碍人士和其他人非常重要。我们考虑具有独立手势序列的手语视频，而非连续的语法规则手势，此类视频在教育应用和手势提示界面有用。

**Method:** IsoSignVid2Aud采用了一种端到端的框架，该框架结合了基于I3D的特征提取模块、专门的特征转换网络和音频生成管道，使用一种新颖的非极大值抑制(NMS)算法来检测非语法连续序列中的手势。

**Result:** 实验结果表明，在ASL-Citizen-1500和WLASL-100数据集上，Top-1准确率分别为72.01%和78.67%，音频质量指标（PESQ：2.67，STOI：0.73）表明输出的语音具有可理解性。

**Conclusion:** 本研究证明了IsoSignVid2Aud框架在手语视频转换为口语音频方面的有效性，特别是在直接通信中避免了多阶段翻译系统的延迟和级联错误。

**Abstract:** Sign language to spoken language audio translation is important to connect
the hearing- and speech-challenged humans with others. We consider sign
language videos with isolated sign sequences rather than continuous grammatical
signing. Such videos are useful in educational applications and sign prompt
interfaces. Towards this, we propose IsoSignVid2Aud, a novel end-to-end
framework that translates sign language videos with a sequence of possibly
non-grammatic continuous signs to speech without requiring intermediate text
representation, providing immediate communication benefits while avoiding the
latency and cascading errors inherent in multi-stage translation systems. Our
approach combines an I3D-based feature extraction module with a specialized
feature transformation network and an audio generation pipeline, utilizing a
novel Non-Maximal Suppression (NMS) algorithm for the temporal detection of
signs in non-grammatic continuous sequences. Experimental results demonstrate
competitive performance on ASL-Citizen-1500 and WLASL-100 datasets with Top-1
accuracies of 72.01\% and 78.67\%, respectively, and audio quality metrics
(PESQ: 2.67, STOI: 0.73) indicating intelligible speech output. Code is
available at: https://github.com/BheeshmSharma/IsoSignVid2Aud_AIMLsystems-2025.

</details>


### [87] [AlignGS: Aligning Geometry and Semantics for Robust Indoor Reconstruction from Sparse Views](https://arxiv.org/abs/2510.07839)
*Yijie Gao,Houqiang Zhong,Tianchi Zhu,Zhengxue Cheng,Qiang Hu,Li Song*

Main category: cs.CV

> 论文提出了一种全新的方法_aligngs，该方法可以同时优化3D场景的几何形状和语义信息，从而更好地从有限视角重建出更完整且连贯的3D模型。

<details>
  <summary>Details</summary>

**Motivation:** 现有的方法往往将语义视为已经形成的、可能是有缺陷的几何模型上的被动特征。该论文提出，为了实现健壮的稀疏视图重建，语义理解应该成为积极的指导力量。

**Method:** AlignGS框架采用了一种新颖的协同端到端优化几何和语义的方法，通过从2D基础模型中提炼丰富的先验知识，使用一系列新的语义到几何的指导机制（包括深度一致性和多方面法线规则化）直接规整3D表示。

**Result:** 实验结果表明，该方法在新视图合成方面达到了最先进的结果，并产生了具有更高几何精度的重建模型。结果验证了利用语义先验作为几何规整器可以生成更具连贯性和完整性的3D模型。

**Conclusion:** 研究结果表明，通过从有限视角输入中利用语义先验作为几何规整器，能够生成更连贯和完整的3D模型。这种方法对于提出内场景的3D模型重建有重要的意义。

**Abstract:** The demand for semantically rich 3D models of indoor scenes is rapidly
growing, driven by applications in augmented reality, virtual reality, and
robotics. However, creating them from sparse views remains a challenge due to
geometric ambiguity. Existing methods often treat semantics as a passive
feature painted on an already-formed, and potentially flawed, geometry. We
posit that for robust sparse-view reconstruction, semantic understanding
instead be an active, guiding force. This paper introduces AlignGS, a novel
framework that actualizes this vision by pioneering a synergistic, end-to-end
optimization of geometry and semantics. Our method distills rich priors from 2D
foundation models and uses them to directly regularize the 3D representation
through a set of novel semantic-to-geometry guidance mechanisms, including
depth consistency and multi-faceted normal regularization. Extensive
evaluations on standard benchmarks demonstrate that our approach achieves
state-of-the-art results in novel view synthesis and produces reconstructions
with superior geometric accuracy. The results validate that leveraging semantic
priors as a geometric regularizer leads to more coherent and complete 3D models
from limited input views. Our code is avaliable at
https://github.com/MediaX-SJTU/AlignGS .

</details>


### [88] [Self-Supervised Learning Strategies for a Platform to Test the Toxicity of New Chemicals and Materials](https://arxiv.org/abs/2510.07853)
*Thomas Lautenschlager,Nils Friederich,Angelo Jovin Yamachui Sitcheu,Katja Nau,Gaëlle Hayot,Thomas Dickmeis,Ralf Mikut*

Main category: cs.CV

> 该论文探讨了利用自监督学习在毒性测试中识别化合物诱导变化的方法，并通过EmbryoNet数据集证明了这种方法的有效性，同时讨论了在TOXBOX项目中毒性测试设备中集成机器学习模型的可能性。

<details>
  <summary>Details</summary>

**Motivation:** 研究的动机在于解决高通量毒性测试中的关键挑战，特别是利用机器学习模型进行自动化评估的挑战。通过探索自监督学习方法的应用，研究希望找到一种有效的方法来识别毒物诱导的变化。

**Method:** 该论文利用自监督学习来生成能够区分不同化合物作用模式的表现形式。采用的方法基于EmbryoNet数据集，这个数据包含由不同的化学成分引起的十个斑马鱼胚胎表型的变化。

**Result:** 研究表明，通过自监督学习学习到的表现形式能够有效地识别出不同化合物的毒性作用模式。这为解决高通量毒性测试中的关键问题提供了前景。

**Conclusion:** 论文结论指出，自监督学习方法在区分不同化合物毒性作用模式方面是有潜力的，这为进一步整合机器学习模型到毒性测试设备提供了可能性。

**Abstract:** High-throughput toxicity testing offers a fast and cost-effective way to test
large amounts of compounds. A key component for such systems is the automated
evaluation via machine learning models. In this paper, we address critical
challenges in this domain and demonstrate how representations learned via
self-supervised learning can effectively identify toxicant-induced changes. We
provide a proof-of-concept that utilizes the publicly available EmbryoNet
dataset, which contains ten zebrafish embryo phenotypes elicited by various
chemical compounds targeting different processes in early embryonic
development. Our analysis shows that the learned representations using
self-supervised learning are suitable for effectively distinguishing between
the modes-of-action of different compounds. Finally, we discuss the integration
of machine learning models in a physical toxicity testing device in the context
of the TOXBOX project.

</details>


### [89] [XYZCylinder: Feedforward Reconstruction for Driving Scenes Based on A Unified Cylinder Lifting Method](https://arxiv.org/abs/2510.07856)
*Haochen Yu,Qiankun Liu,Hongyuan Liu,Jianfei Jiang,Juntao Lyu,Jiansheng Chen,Huimin Ma*

Main category: cs.CV

> XYZCylinder model is developed to improve driving scene reconstruction by adapting to different camera configurations and using a cylinder lifting method for better 2D to 3D feature transformation.

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of existing feedforward reconstruction paradigms regarding generalization and accuracy in driving scenes, mainly caused by fixed view transformations and complex scenes.

**Method:** The paper proposes XYZCylinder, a feedforward model that uses a unified cylinder lifting method involving camera modeling (Unified Cylinder Camera Modeling - UCCM) and feature lifting (Cylinder Plane Feature Group - CPFG) to improve generalization capability and reconstruction accuracy for driving scenes.

**Result:** The model achieves state-of-the-art performance under various evaluation settings and can generalize to other driving scenes without additional training.

**Conclusion:** The proposed XYZCylinder model demonstrates significant improvements in both generalization capability and reconstruction accuracy for driving scenes, showcasing its applicability to a wide range of scenarios.

**Abstract:** Recently, more attention has been paid to feedforward reconstruction
paradigms, which mainly learn a fixed view transformation implicitly and
reconstruct the scene with a single representation. However, their
generalization capability and reconstruction accuracy are still limited while
reconstructing driving scenes, which results from two aspects: (1) The fixed
view transformation fails when the camera configuration changes, limiting the
generalization capability across different driving scenes equipped with
different camera configurations. (2) The small overlapping regions between
sparse views of the $360^\circ$ panorama and the complexity of driving scenes
increase the learning difficulty, reducing the reconstruction accuracy. To
handle these difficulties, we propose \textbf{XYZCylinder}, a feedforward model
based on a unified cylinder lifting method which involves camera modeling and
feature lifting. Specifically, to improve the generalization capability, we
design a Unified Cylinder Camera Modeling (UCCM) strategy, which avoids the
learning of viewpoint-dependent spatial correspondence and unifies different
camera configurations with adjustable parameters. To improve the reconstruction
accuracy, we propose a hybrid representation with several dedicated modules
based on newly designed Cylinder Plane Feature Group (CPFG) to lift 2D image
features to 3D space. Experimental results show that XYZCylinder achieves
state-of-the-art performance under different evaluation settings, and can be
generalized to other driving scenes in a zero-shot manner. Project page:
\href{https://yuyuyu223.github.io/XYZCYlinder-projectpage/}{here}.

</details>


### [90] [MARC: Memory-Augmented RL Token Compression for Efficient Video Understanding](https://arxiv.org/abs/2510.07915)
*Peiran Wu,Zhuorui Yu,Yunze Liu,Chi-Hao Wu,Enmin Zhou,Junxiao Shen*

Main category: cs.CV

> MARC is a novel approach that effectively condenses visual tokens in videos with reinforcement learning and memory augmentation, resulting in reduced computational resources and improved efficiency without notable performance loss.

<details>
  <summary>Details</summary>

**Motivation:** The primary motivation is to address the computational inefficiencies encountered by visual language models (VLMs) in handling videos characterized by high frame rates and extended durations, especially in resource-limited settings.

**Method:** Memory-Augmented Reinforcement Learning-based Token Compression (MARC) is introduced, forming a retrieve-then-compress strategy that utilizes the Visual Memory Retriever (VMR) for selecting key clips and Compression Group Relative Policy Optimization (C-GRPO) for enhancing student model reasoning through teacher model distillation.

**Result:** MARC accomplishes near-par performance to benchmarks with a significant reduction in visual tokens (95%), a decrease in GPU memory usage (72%), and latency (23.9%).

**Conclusion:** The findings indicate that MARC has the potential to enable more efficient video understanding, particularly suitable for applications like real-time video questioning and answering, surveillance, and autonomous driving.

**Abstract:** The rapid progress of large language models (LLMs) has laid the foundation
for multimodal models. However, visual language models (VLMs) still face heavy
computational costs when extended from images to videos due to high frame rates
and long durations. Token compression is a promising solution, yet most
existing training-free methods cause information loss and performance
degradation. To overcome this, we propose \textbf{Memory-Augmented
Reinforcement Learning-based Token Compression (MARC)}, which integrates
structured retrieval and RL-based distillation. MARC adopts a
\textit{retrieve-then-compress} strategy using a \textbf{Visual Memory
Retriever (VMR)} to select key clips and a \textbf{Compression Group Relative
Policy Optimization (C-GRPO)} framework to distil reasoning ability from a
teacher to a student model. Experiments on six video benchmarks show that MARC
achieves near-baseline accuracy using only one frame's tokens -- reducing
visual tokens by \textbf{95\%}, GPU memory by \textbf{72\%}, and latency by
\textbf{23.9\%}. This demonstrates its potential for efficient, real-time video
understanding in resource-constrained settings such as video QA, surveillance,
and autonomous driving.

</details>


### [91] [ASBench: Image Anomalies Synthesis Benchmark for Anomaly Detection](https://arxiv.org/abs/2510.07927)
*Qunyi Zhang,Songan Zhang,Jinbao Wang,Xiaoning Lei,Guoyang Xie,Guannan Jiang,Zhichao Lu*

Main category: cs.CV

> 该论文提出了ASBench，这是首个专门用于评估异常合成方法的综合基准框架，该框架可以帮助我们理解现有的异常合成方法的优势和劣势，并为未来的研究指明方向。

<details>
  <summary>Details</summary>

**Motivation:** 现有的异常检测研究将异常合成视为一个辅助组件，很少有系统的异常合成算法评估。并且当前的研究忽视了某些特定的异常合成因素，例如将其影响与检测分离，合成数据的定量分析以及不同场景下的适应性。

**Method:** 提出ASBench，这是一个专门用于评估异常合成方法的综合基准框架，包含四个关键评估维度：(i) 通用性能跨不同数据集和管道(ii) 合成数据与真实数据的比例(iii) 合成图像的内在指标与异常检测性能指标间的相关性，(iv) 混合异常合成方法的策略。

**Result:** 通过广泛的实验，ASBench不仅揭示了当前异常合成方法的局限性，还为未来的异常合成研究提供了实用见解。

**Conclusion:** 通过ASBench，研究人员可以更好地评估和改进现有的异常合成方法，从而有利于制造业的质量控制。

**Abstract:** Anomaly detection plays a pivotal role in manufacturing quality control, yet
its application is constrained by limited abnormal samples and high manual
annotation costs. While anomaly synthesis offers a promising solution, existing
studies predominantly treat anomaly synthesis as an auxiliary component within
anomaly detection frameworks, lacking systematic evaluation of anomaly
synthesis algorithms. Current research also overlook crucial factors specific
to anomaly synthesis, such as decoupling its impact from detection,
quantitative analysis of synthetic data and adaptability across different
scenarios. To address these limitations, we propose ASBench, the first
comprehensive benchmarking framework dedicated to evaluating anomaly synthesis
methods. Our framework introduces four critical evaluation dimensions: (i) the
generalization performance across different datasets and pipelines (ii) the
ratio of synthetic to real data (iii) the correlation between intrinsic metrics
of synthesis images and anomaly detection performance metrics , and (iv)
strategies for hybrid anomaly synthesis methods. Through extensive experiments,
ASBench not only reveals limitations in current anomaly synthesis methods but
also provides actionable insights for future research directions in anomaly
synthesis

</details>


### [92] [TTOM: Test-Time Optimization and Memorization for Compositional Video Generation](https://arxiv.org/abs/2510.07940)
*Leigang Qu,Ziyang Wang,Na Zheng,Wenjie Wang,Liqiang Nie,Tat-Seng Chua*

Main category: cs.CV

> The paper presents TTOM, a method to improve compositional video generation by optimizing VFM outputs with spatiotemporal layouts during inference, achieving better text-image alignment and showcasing good performance on key benchmarks.

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind this work is to enhance the compositional abilities of VFMs, which are currently weak in handling scenarios that involve motion, numeracy, and spatial relations. TTOM aims to improve the alignment of generated visuals with the intended text descriptions during inference.

**Method:** Video Foundation Models (VFMs) face challenges in compositional scenarios like motion and spatial relation. The paper introduces Test-Time Optimization and Memorization (TTOM), a training-free framework that optimizes VFM outputs with spatiotemporal layouts during inference for improved text-image alignment. TTOM uses a general layout-attention objective to optimize new parameters. It also supports video generation in a streaming setting, using a memory mechanism to manage historical optimization contexts.

**Result:** TTOM exhibits powerful transferability and generalization, effectively disentangling compositional world knowledge. It shows good performance in cross-modal alignment for compositional video generation, as evidenced by results on T2V-CompBench and Vbench benchmarks.

**Conclusion:** TTOM is a practical and efficient framework for achieving better cross-modal alignment in compositional video generation. It demonstrates scalability and effectiveness in optimizing VFM outputs with minimal intervention, making it a promising approach for enhancing video generation capabilities.

**Abstract:** Video Foundation Models (VFMs) exhibit remarkable visual generation
performance, but struggle in compositional scenarios (e.g., motion, numeracy,
and spatial relation). In this work, we introduce Test-Time Optimization and
Memorization (TTOM), a training-free framework that aligns VFM outputs with
spatiotemporal layouts during inference for better text-image alignment. Rather
than direct intervention to latents or attention per-sample in existing work,
we integrate and optimize new parameters guided by a general layout-attention
objective. Furthermore, we formulate video generation within a streaming
setting, and maintain historical optimization contexts with a parametric memory
mechanism that supports flexible operations, such as insert, read, update, and
delete. Notably, we found that TTOM disentangles compositional world knowledge,
showing powerful transferability and generalization. Experimental results on
the T2V-CompBench and Vbench benchmarks establish TTOM as an effective,
practical, scalable, and efficient framework to achieve cross-modal alignment
for compositional video generation on the fly.

</details>


### [93] [CVD-STORM: Cross-View Video Diffusion with Spatial-Temporal Reconstruction Model for Autonomous Driving](https://arxiv.org/abs/2510.07944)
*Tianrui Zhang,Yichen Liu,Zilin Guo,Yuxin Guo,Jingcheng Ni,Chenjing Ding,Dan Xu,Lewei Lu,Zehuan Wu*

Main category: cs.CV

> Error

<details>
  <summary>Details</summary>

**Motivation:** Error

**Method:** Error

**Result:** Error

**Conclusion:** Error

**Abstract:** Generative models have been widely applied to world modeling for environment
simulation and future state prediction. With advancements in autonomous
driving, there is a growing demand not only for high-fidelity video generation
under various controls, but also for producing diverse and meaningful
information such as depth estimation. To address this, we propose CVD-STORM, a
cross-view video diffusion model utilizing a spatial-temporal reconstruction
Variational Autoencoder (VAE) that generates long-term, multi-view videos with
4D reconstruction capabilities under various control inputs. Our approach first
fine-tunes the VAE with an auxiliary 4D reconstruction task, enhancing its
ability to encode 3D structures and temporal dynamics. Subsequently, we
integrate this VAE into the video diffusion process to significantly improve
generation quality. Experimental results demonstrate that our model achieves
substantial improvements in both FID and FVD metrics. Additionally, the
jointly-trained Gaussian Splatting Decoder effectively reconstructs dynamic
scenes, providing valuable geometric information for comprehensive scene
understanding.

</details>


### [94] [A Large-scale Dataset for Robust Complex Anime Scene Text Detection](https://arxiv.org/abs/2510.07951)
*Ziyi Dong,Yurui Zhang,Changmao Li,Naomi Rue Golding,Qing Long*

Main category: cs.CV

> This paper presents AnimeText, a specialized text detection dataset for anime scenes, which demonstrates superior performance in text detection tasks in anime compared to existing datasets.

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind this work is to address the gap in current text detection datasets when applied to anime scenes, which are characterized by diverse text styles, irregular arrangements, and complex visual elements that are not typically encountered in natural or document scenes.

**Method:** The paper introduces AnimeText, a dataset specifically designed for anime scenes. It consists of hierarchical annotations and hard negative samples, tailored for the unique characteristics of text within anime, such as diverse styles, irregular arrangements, and complex visual elements.

**Result:** Cross-dataset evaluations reveal that models trained on AnimeText outperform those trained on traditional datasets in text detection tasks specific to anime scenes.

**Conclusion:** The paper concludes that AnimeText, due to its specific design for anime scenes, provides a valuable resource for training models that can more effectively detect text in complex anime environments compared to models trained on traditional text detection datasets.

**Abstract:** Current text detection datasets primarily target natural or document scenes,
where text typically appear in regular font and shapes, monotonous colors, and
orderly layouts. The text usually arranged along straight or curved lines.
However, these characteristics differ significantly from anime scenes, where
text is often diverse in style, irregularly arranged, and easily confused with
complex visual elements such as symbols and decorative patterns. Text in anime
scene also includes a large number of handwritten and stylized fonts. Motivated
by this gap, we introduce AnimeText, a large-scale dataset containing 735K
images and 4.2M annotated text blocks. It features hierarchical annotations and
hard negative samples tailored for anime scenarios. %Cross-dataset evaluations
using state-of-the-art methods demonstrate that models trained on AnimeText
achieve superior performance in anime text detection tasks compared to existing
datasets. To evaluate the robustness of AnimeText in complex anime scenes, we
conducted cross-dataset benchmarking using state-of-the-art text detection
methods. Experimental results demonstrate that models trained on AnimeText
outperform those trained on existing datasets in anime scene text detection
tasks. AnimeText on HuggingFace:
https://huggingface.co/datasets/deepghs/AnimeText

</details>


### [95] [SimCast: Enhancing Precipitation Nowcasting with Short-to-Long Term Knowledge Distillation](https://arxiv.org/abs/2510.07953)
*Yifang Yin,Shengkai Chen,Yiyao Li,Lu Wang,Ruibing Jin,Wei Cui,Shili Xiang*

Main category: cs.CV

> 

<details>
  <summary>Details</summary>

**Motivation:** 

**Method:** 

**Result:** {
  "tldr": "The paper proposes SimCast, a novel training pipeline for precipitation nowcasting using a short-to-long term knowledge distillation technique and a weighted MSE loss. It then integrates SimCast into a diffusion-based framework named CasCast to improve prediction accuracy, achieving high CSI scores on three benchmark datasets.",
  "motivation": "The motivation is to address the challenges in accurate precipitation nowcasting, which is crucial for societal needs like disaster management and agriculture, by improving upon existing non-autoregressive approaches.",
  "method": "The method involves developing SimCast, a training pipeline that employs a short-to-long term knowledge distillation technique and a weighted MSE loss to prioritize heavy rainfall regions, and integrating it into a diffusion-based framework named CasCast.",
  "result": "Experimental results show that the proposed framework outperforms existing approaches by a significant margin, achieving mean CSI scores of 0.452 on SEVIR, 0.474 on HKO-7, and 0.361 on MeteoNet.",
  "conclusion": "The conclusion is that the proposed pipeline and framework enhance the accuracy of precipitation nowcasting, making it more effective for various applications without increasing computational overhead in inference.")

**Conclusion:** 

**Abstract:** Precipitation nowcasting predicts future radar sequences based on current
observations, which is a highly challenging task driven by the inherent
complexity of the Earth system. Accurate nowcasting is of utmost importance for
addressing various societal needs, including disaster management, agriculture,
transportation, and energy optimization. As a complementary to existing
non-autoregressive nowcasting approaches, we investigate the impact of
prediction horizons on nowcasting models and propose SimCast, a novel training
pipeline featuring a short-to-long term knowledge distillation technique
coupled with a weighted MSE loss to prioritize heavy rainfall regions. Improved
nowcasting predictions can be obtained without introducing additional overhead
during inference. As SimCast generates deterministic predictions, we further
integrate it into a diffusion-based framework named CasCast, leveraging the
strengths from probabilistic models to overcome limitations such as blurriness
and distribution shift in deterministic outputs. Extensive experimental results
on three benchmark datasets validate the effectiveness of the proposed
framework, achieving mean CSI scores of 0.452 on SEVIR, 0.474 on HKO-7, and
0.361 on MeteoNet, which outperforms existing approaches by a significant
margin.

</details>


### [96] [Latent Harmony: Synergistic Unified UHD Image Restoration via Latent Space Regularization and Controllable Refinement](https://arxiv.org/abs/2510.07961)
*Yidi Liu,Xueyang Fu,Jie Huang,Jie Xiao,Dong Li,Wenlong Zhang,Lei Bai,Zheng-Jun Zha*

Main category: cs.CV

> 本文提出了一种名为Latent Harmony的两阶段框架，用于改善UHD图像的恢复，通过引入视觉语义约束和HF-LoRA来克服VAE的高斯约束对高频重构建的限制，实现了性能和效率的提升。

<details>
  <summary>Details</summary>

**Motivation:** UHD图像恢复面临计算效率和高频细节保留之间的权衡。尽管变分自编码器（VAEs）通过潜空间处理提高了效率，但由于其高斯约束往往会丢弃特定退化的高频信息，从而损害重建保真度。本文旨在克服这一问题。

**Method:** 本文提出了一种名为Latent Harmony的两阶段框架，旨在改善UHD图像的恢复。第一阶段提出了LH-VAE，通过视觉语义约束和渐进退化扰动增强语义鲁棒性，同时使用潜变量等价性加强高频重建。第二阶段通过HF-LoRA，对经过改进的VAE与恢复模型共同训练，HF-LoRA包括对高频对齐损失导向的编码器LoRA和对感知导向损失驱动的解码器LoRA，二者通过选择性梯度传播交替优化以保持预训练的潜变量结构。

**Result:** 实验显示，Latent Harmony在UHD和标准分辨率任务上均取得了最先进的性能，有效平衡了效率、感知质量和重建准确性。

**Conclusion:** Latent Harmony通过联合正则化潜空间和强制高频感知重建，成功改善了UHD图像恢复的性能，同时保持了计算效率，表现出色。

**Abstract:** Ultra-High Definition (UHD) image restoration faces a trade-off between
computational efficiency and high-frequency detail retention. While Variational
Autoencoders (VAEs) improve efficiency via latent-space processing, their
Gaussian constraint often discards degradation-specific high-frequency
information, hurting reconstruction fidelity. To overcome this, we propose
Latent Harmony, a two-stage framework that redefines VAEs for UHD restoration
by jointly regularizing the latent space and enforcing high-frequency-aware
reconstruction.In Stage One, we introduce LH-VAE, which enhances semantic
robustness through visual semantic constraints and progressive degradation
perturbations, while latent equivariance strengthens high-frequency
reconstruction.Stage Two jointly trains this refined VAE with a restoration
model using High-Frequency Low-Rank Adaptation (HF-LoRA): an encoder LoRA
guided by a fidelity-oriented high-frequency alignment loss to recover
authentic details, and a decoder LoRA driven by a perception-oriented loss to
synthesize realistic textures. Both LoRA modules are trained via alternating
optimization with selective gradient propagation to preserve the pretrained
latent structure.At inference, a tunable parameter {\alpha} enables flexible
fidelity-perception trade-offs.Experiments show Latent Harmony achieves
state-of-the-art performance across UHD and standard-resolution tasks,
effectively balancing efficiency, perceptual quality, and reconstruction
accuracy.

</details>


### [97] [The impact of abstract and object tags on image privacy classification](https://arxiv.org/abs/2510.07976)
*Darya Baranouskaya,Andrea Cavallaro*

Main category: cs.CV

> 在图像隐私任务中，当标签数量有限时，抽象标签比物体标签更有效；标签数量较多时，两者的效用相近。

<details>
  <summary>Details</summary>

**Motivation:** 物体标签表示具体的实体，是许多计算机视觉任务的核心，而抽象标签捕捉高层次的信息，对于需要上下文、可能是主观场景理解的任务是相关的。我们希望探索哪种类型的标签更适合用于图像隐私任务，并指导未来更准确的图像隐私分类器的研究。

**Method:** 我们探索了在依赖背景且具有主观性的图像隐私任务中，使用哪种类型的标签更为合适。虽然物体标签通常用于隐私分类，但我们发现当标签数量受到限制时，抽象标签更加有效。另一方面，当每个图像可用的标签数量较大时，与物体相关的标签同样是有用的。

**Result:** 研究表明，在标签预算有限的情况下，抽象标签比物体标签在图像隐私识别上更有效。然而，当标签数量较多时，物体标签也能发挥同样重要的作用。

**Conclusion:** 这些发现可以指导未来在开发更加准确的图像隐私分类器方面的研究，考虑到标签类型和数量的作用。

**Abstract:** Object tags denote concrete entities and are central to many computer vision
tasks, whereas abstract tags capture higher-level information, which is
relevant for tasks that require a contextual, potentially subjective scene
understanding. Object and abstract tags extracted from images also facilitate
interpretability. In this paper, we explore which type of tags is more suitable
for the context-dependent and inherently subjective task of image privacy.
While object tags are generally used for privacy classification, we show that
abstract tags are more effective when the tag budget is limited. Conversely,
when a larger number of tags per image is available, object-related information
is as useful. We believe that these findings will guide future research in
developing more accurate image privacy classifiers, informed by the role of tag
types and quantity.

</details>


### [98] [Is Architectural Complexity Always the Answer? A Case Study on SwinIR vs. an Efficient CNN](https://arxiv.org/abs/2510.07984)
*Chandresh Sutariya,Nitin Singh*

Main category: cs.CV

> 本文探讨了高性能模型SwinIR与轻量级CNN在低光照图像处理中的表现，结果表明轻量级模型在计算成本更低的情况下，仍能提供接近顶尖的PSNR性能。

<details>
  <summary>Details</summary>

**Motivation:** 尽管大型Transformer模型如SwinIR在性能上表现优异，但其高计算成本限制了实际应用。因此，探讨高性能和低计算成本之间权衡的重要性。

**Method:** 通过比较最新的SwinIR模型和一个标准的轻量级卷积神经网络(CNN)在低光照图像中的表现，研究性能与效率之间的关键权衡。

**Result:** 实验结果显示，基于Transformer的SwinIR模型达到最高性能，PSNR为39.03 dB，而轻量级CNN以较小的计算成本达到了37.4 dB的PSNR，且训练收敛需要的周期数仅为10，比SwinIR少很多。

**Conclusion:** 标准CNN能在显著降低计算成本的情况下提供接近顶尖的效果，这表明它在资源受限的实际场景中是一个值得考虑的选择。

**Abstract:** The simultaneous restoration of high-frequency details and suppression of
severe noise in low-light imagery presents a significant and persistent
challenge in computer vision. While large-scale Transformer models like SwinIR
have set the state of the art in performance, their high computational cost can
be a barrier for practical applications. This paper investigates the critical
trade-off between performance and efficiency by comparing the state-of-the-art
SwinIR model against a standard, lightweight Convolutional Neural Network (CNN)
on this challenging task. Our experimental results reveal a nuanced but
important finding. While the Transformer-based SwinIR model achieves a higher
peak performance, with a Peak Signal-to-Noise Ratio (PSNR) of 39.03 dB, the
lightweight CNN delivers a surprisingly competitive PSNR of 37.4 dB. Crucially,
the CNN reached this performance after converging in only 10 epochs of
training, whereas the more complex SwinIR model required 132 epochs. This
efficiency is further underscored by the model's size; the CNN is over 55 times
smaller than SwinIR. This work demonstrates that a standard CNN can provide a
near state-of-the-art result with significantly lower computational overhead,
presenting a compelling case for its use in real-world scenarios where resource
constraints are a primary concern.

</details>


### [99] [GraphEnet: Event-driven Human Pose Estimation with a Graph Neural Network](https://arxiv.org/abs/2510.07990)
*Gaurvi Goyal,Pham Cong Thuong,Arren Glover,Masayoshi Mizuno,Chiara Bartolozzi*

Main category: cs.CV

> 本论文提出一种基于图神经网络的GraphEnet方法，用于估计事件相机数据中的2D人体姿态，适用于资源受限的设备。

<details>
  <summary>Details</summary>

**Motivation:** 论文旨在利用事件相机的低延迟和低能耗特性，结合图神经网络技术，来改善资源受限环境（如便携式电子设备和移动机器人）中的人体姿态估计问题。

**Method:** 本论文提出了一种基于图神经网络（Graph Neural Network）的方法GraphEnet，该方法利用了事件相机输出的稀疏特性，并采用基于线的中间事件表示来估计单人2D人体姿态，频率较高。该架构引入了一种新的偏移向量学习范式以及基于置信度的汇聚策略。

**Result:** 该工作是首次将图神经网络应用于事件数据进行人体姿态估计的研究，展现了此方法在资源受限应用中的潜力。

**Conclusion:** GraphEnet通过采用基于图神经网络的创新设计，在利用事件相机进行高频率的人体姿态估计方面取得了进展，为资源受限的设备提供了有益的解决方案。

**Abstract:** Human Pose Estimation is a crucial module in human-machine interaction
applications and, especially since the rise in deep learning technology, robust
methods are available to consumers using RGB cameras and commercial GPUs. On
the other hand, event-based cameras have gained popularity in the vision
research community for their low latency and low energy advantages that make
them ideal for applications where those resources are constrained like portable
electronics and mobile robots. In this work we propose a Graph Neural Network,
GraphEnet, that leverages the sparse nature of event camera output, with an
intermediate line based event representation, to estimate 2D Human Pose of a
single person at a high frequency. The architecture incorporates a novel offset
vector learning paradigm with confidence based pooling to estimate the human
pose. This is the first work that applies Graph Neural Networks to event data
for Human Pose Estimation. The code is open-source at
https://github.com/event-driven-robotics/GraphEnet-NeVi-ICCV2025.

</details>


### [100] [CIR-CoT: Towards Interpretable Composed Image Retrieval via End-to-End Chain-of-Thought Reasoning](https://arxiv.org/abs/2510.08003)
*Weihuang Lin,Yiwei Ma,Jiayi Ji,Xiaoshuai Sun,Rongrong Ji*

Main category: cs.CV

> Error

<details>
  <summary>Details</summary>

**Motivation:** Error

**Method:** Error

**Result:** Error

**Conclusion:** Error

**Abstract:** Composed Image Retrieval (CIR), which aims to find a target image from a
reference image and a modification text, presents the core challenge of
performing unified reasoning across visual and semantic modalities. While
current approaches based on Vision-Language Models (VLMs, e.g., CLIP) and more
recent Multimodal Large Language Models (MLLMs, e.g., Qwen-VL) have shown
progress, they predominantly function as ``black boxes." This inherent opacity
not only prevents users from understanding the retrieval rationale but also
restricts the models' ability to follow complex, fine-grained instructions. To
overcome these limitations, we introduce CIR-CoT, the first end-to-end
retrieval-oriented MLLM designed to integrate explicit Chain-of-Thought (CoT)
reasoning. By compelling the model to first generate an interpretable reasoning
chain, CIR-CoT enhances its ability to capture crucial cross-modal
interactions, leading to more accurate retrieval while making its decision
process transparent. Since existing datasets like FashionIQ and CIRR lack the
necessary reasoning data, a key contribution of our work is the creation of
structured CoT annotations using a three-stage process involving a caption,
reasoning, and conclusion. Our model is then fine-tuned to produce this
structured output before encoding its final retrieval intent into a dedicated
embedding. Comprehensive experiments show that CIR-CoT achieves highly
competitive performance on in-domain datasets (FashionIQ, CIRR) and
demonstrates remarkable generalization on the out-of-domain CIRCO dataset,
establishing a new path toward more effective and trustworthy retrieval
systems.

</details>


### [101] [RayFusion: Ray Fusion Enhanced Collaborative Visual Perception](https://arxiv.org/abs/2510.08017)
*Shaohong Wang,Bin Lu,Xinyu Xiao,Hanzhi Zhong,Bowen Pang,Tong Wang,Zhiyu Xiang,Hangguan Shan,Eryun Liu*

Main category: cs.CV

> RayFusion is a proposed method that uses ray occupancy information from collaborators to enhance depth estimation and performance in camera-based collaborative perception systems for autonomous driving.

<details>
  <summary>Details</summary>

**Motivation:** To address the issue of inaccurate depth estimation in camera-based perception systems for autonomous driving, which often stems from the lack of explicit depth information.

**Method:** The paper proposes RayFusion, a ray-based fusion method that uses occupancy information from collaborators to improve the depth estimation accuracy in camera-based collaborative perception systems.

**Result:** Experiments show that RayFusion outperforms current state-of-the-art methods, enhancing the performance of collaborative visual perception systems.

**Conclusion:** RayFusion, by leveraging ray occupancy information from collaborators, improves the performance of collaborative visual perception systems, suggesting a promising direction for future research in autonomous driving.

**Abstract:** Collaborative visual perception methods have gained widespread attention in
the autonomous driving community in recent years due to their ability to
address sensor limitation problems. However, the absence of explicit depth
information often makes it difficult for camera-based perception systems, e.g.,
3D object detection, to generate accurate predictions. To alleviate the
ambiguity in depth estimation, we propose RayFusion, a ray-based fusion method
for collaborative visual perception. Using ray occupancy information from
collaborators, RayFusion reduces redundancy and false positive predictions
along camera rays, enhancing the detection performance of purely camera-based
collaborative perception systems. Comprehensive experiments show that our
method consistently outperforms existing state-of-the-art models, substantially
advancing the performance of collaborative visual perception. The code is
available at https://github.com/wangsh0111/RayFusion.

</details>


### [102] [RASALoRE: Region Aware Spatial Attention with Location-based Random Embeddings for Weakly Supervised Anomaly Detection in Brain MRI Scans](https://arxiv.org/abs/2510.08052)
*Bheeshm Sharma,Karthikeyan Jaganathan,Balamurugan Palaniappan*

Main category: cs.CV

> 该研究提出了一种基于区域感知空间注意力和位置嵌入的两阶段弱监督异常检测方法，用于脑MRI图像的异常检测，显示出优秀的检测性能及较低的计算复杂度。

<details>
  <summary>Details</summary>

**Motivation:** 研究动机在于利用弱监督学习在只能够获得切片级别标签而无法获得精确像素级异常标注时，依然能够实现快速且准确的脑异常检测。

**Method:** 该论文提出了一种名为RASALoRE的新型两阶段弱监督异常检测框架。第一阶段使用判别式双提示微调机制生成基于切片级别标签的伪弱掩模，作为粗略定位线索。第二阶段使用带有基于位置的随机嵌入的区域感知空间注意力机制的分割网络，使模型能够有效关注异常区域。

**Result:** 该方法在BraTS20、BraTS21、BraTS23和MSD等数据集上进行的广泛评估表明，它不仅性能优越，而且显著减少了计算复杂度。

**Conclusion:** 实验结果表明RASALoRE不仅在脑MRI异常检测任务中达到了最先进的性能，而且由于其模型参数数量少于800万，其计算复杂度显著降低。

**Abstract:** Weakly Supervised Anomaly detection (WSAD) in brain MRI scans is an important
challenge useful to obtain quick and accurate detection of brain anomalies when
precise pixel-level anomaly annotations are unavailable and only weak labels
(e.g., slice-level) are available. In this work, we propose RASALoRE: Region
Aware Spatial Attention with Location-based Random Embeddings, a novel
two-stage WSAD framework. In the first stage, we introduce a Discriminative
Dual Prompt Tuning (DDPT) mechanism that generates high-quality pseudo weak
masks based on slice-level labels, serving as coarse localization cues. In the
second stage, we propose a segmentation network with a region-aware spatial
attention mechanism that relies on fixed location-based random embeddings. This
design enables the model to effectively focus on anomalous regions. Our
approach achieves state-of-the-art anomaly detection performance, significantly
outperforming existing WSAD methods while utilizing less than 8 million
parameters. Extensive evaluations on the BraTS20, BraTS21, BraTS23, and MSD
datasets demonstrate a substantial performance improvement coupled with a
significant reduction in computational complexity. Code is available at:
https://github.com/BheeshmSharma/RASALoRE-BMVC-2025/.

</details>


### [103] [RetouchLLM: Training-free White-box Image Retouching](https://arxiv.org/abs/2510.08054)
*Moon Ye-Bin,Roy Miles,Tae-Hyun Oh,Ismail Elezi,Jiankang Deng*

Main category: cs.CV

> RetouchLLM是一种无需训练数据的白盒图像修复系统，它通过可视化评判模块和代码生成模块，可以直接对高分辨率图像进行可解释的修复，增强了用户交互的灵活性和图像修复的适应性。

<details>
  <summary>Details</summary>

**Motivation:** 现有的学习型方法需要大量配对数据，并且作为一个黑盒操作，使得修复过程不透明，限制了其适应性以处理多样化的、用户或图像特定的调整需求。

**Method:** 提出了一种无需训练的白盒图像修复系统RetouchLLM，该系统不依赖大规模配对数据，直接对高分辨率图像进行基于代码的可解释修复。框架以类似于人类多步骤修复的方式逐步增强图像，包含一个可视化评判模块和一个代码生成模块。

**Result:** 实验表明，该方法在多种修复风格上具有良好的泛化能力，基于自然语言的用户交互使得调整更符合用户意图，更加可解释和可控。

**Conclusion:** RetouchLLM作为无需训练的框架，为高分辨率图像的基于代码的修复提供了可行性，支持多样化的调整路径，实现了人机之间可解释和可控的修复过程。

**Abstract:** Image retouching not only enhances visual quality but also serves as a means
of expressing personal preferences and emotions. However, existing
learning-based approaches require large-scale paired data and operate as black
boxes, making the retouching process opaque and limiting their adaptability to
handle diverse, user- or image-specific adjustments. In this work, we propose
RetouchLLM, a training-free white-box image retouching system, which requires
no training data and performs interpretable, code-based retouching directly on
high-resolution images. Our framework progressively enhances the image in a
manner similar to how humans perform multi-step retouching, allowing
exploration of diverse adjustment paths. It comprises of two main modules: a
visual critic that identifies differences between the input and reference
images, and a code generator that produces executable codes. Experiments
demonstrate that our approach generalizes well across diverse retouching
styles, while natural language-based user interaction enables interpretable and
controllable adjustments tailored to user intent.

</details>


### [104] [A class-driven hierarchical ResNet for classification of multispectral remote sensing images](https://arxiv.org/abs/2510.08060)
*Giulio Weikmann,Gianmarco Perantoni,Lorenzo Bruzzone*

Main category: cs.CV

> 本文提出了一种改进ResNet的层次残差神经网络，能够有效地对多光谱图像时间序列进行分类，并具备处理有限训练样本的能力。

<details>
  <summary>Details</summary>

**Motivation:** 该方法旨在提高不同详细程度的分类识别能力，并训练一个模块化的架构，可以作为骨干网络用于引入新的特定类别和考虑有限训练样本的额外任务。

**Method:** 本文提出了一种多时态类驱动的层次残差神经网络（ResNet），用于对不同语义层次的多光谱图像时间序列进行分类。网络架构是基于修改的ResNet，引入了额外的分支来进行不同层次的分类，并利用层次惩罚图来避免分类中的不一致层次转换。

**Result:** 实验结果在2019年获取的亚马逊森林两个地块上12个每月合成的Sentinel 2图像中得到，表明层次方法在不同层次上的泛化能力和对新目标区域的微分类准确分类中都能表现出较好的性能。

**Conclusion:** 提出的模块化网络具有内在的适应能力，可通过对网络进行微调实现，展示了一种能够有效处理不同详细程度类别分类的方法。

**Abstract:** This work presents a multitemporal class-driven hierarchical Residual Neural
Network (ResNet) designed for modelling the classification of Time Series (TS)
of multispectral images at different semantical class levels. The architecture
consists of a modification of the ResNet where we introduce additional branches
to perform the classification at the different hierarchy levels and leverage on
hierarchy-penalty maps to discourage incoherent hierarchical transitions within
the classification. In this way, we improve the discrimination capabilities of
classes at different levels of semantic details and train a modular
architecture that can be used as a backbone network for introducing new
specific classes and additional tasks considering limited training samples
available. We exploit the class-hierarchy labels to train efficiently the
different layers of the architecture, allowing the first layers to train faster
on the first levels of the hierarchy modeling general classes (i.e., the
macro-classes) and the intermediate classes, while using the last ones to
discriminate more specific classes (i.e., the micro-classes). In this way, the
targets are constrained in following the hierarchy defined, improving the
classification of classes at the most detailed level. The proposed modular
network has intrinsic adaptation capability that can be obtained through fine
tuning. The experimental results, obtained on two tiles of the Amazonian Forest
on 12 monthly composites of Sentinel 2 images acquired during 2019, demonstrate
the effectiveness of the hierarchical approach in both generalizing over
different hierarchical levels and learning discriminant features for an
accurate classification at the micro-class level on a new target area, with a
better representation of the minoritarian classes.

</details>


### [105] [Towards Real-World Deepfake Detection: A Diverse In-the-wild Dataset of Forgery Faces](https://arxiv.org/abs/2510.08067)
*Junyu Shi,Minghui Li,Junguo Zuo,Zhifei Yu,Yipeng Lin,Shengshan Hu,Ziqi Zhou,Yechao Zhang,Wei Wan,Yinzhe Xu,Leo Yu Zhang*

Main category: cs.CV

> 论文介绍了RedFace数据集，该数据集通过使用9个商业在线平台整合最新的深伪技术来弥补现有深伪检测数据集的局限性，可以帮助实现更有效的深伪检测应用。

<details>
  <summary>Details</summary>

**Motivation:** 现有的深伪检测评估和基准测试往往由于缺乏具体性、受限的深伪多样性以及限制性的操作技术，无法实现有效的应用。为了针对这些限制，研究人员提出了RedFace数据集，以弥合学术评估和现实需求之间的差距。

**Method:** 本研究介绍了RedFace数据集，这是一个专为检测深伪(Deepfake)伪造而设计的面部数据集，包含超过60,000张伪造图像和1,000个篡改过的视频。与以往的基准测试不同，RedFace数据集利用了9个商业在线平台来整合实际存在的最新深伪技术，模拟真实的黑盒场景。

**Result:** 研究表明，现有的深伪检测方案在真实世界应用中的实用性有限，而通过使用RedFace数据集可以有效提升深伪检测的准确性。

**Conclusion:** 实验结果表明，现有深伪检测方案对比实际应用存在局限性。RedFace数据集的使用能够更准确地模拟真实世界的深伪情况，研究中还详细分析了RedFace数据集对于检测性能的影响，并指出其明显优于常规数据集的优势。

**Abstract:** Deepfakes, leveraging advanced AIGC (Artificial Intelligence-Generated
Content) techniques, create hyper-realistic synthetic images and videos of
human faces, posing a significant threat to the authenticity of social media.
While this real-world threat is increasingly prevalent, existing academic
evaluations and benchmarks for detecting deepfake forgery often fall short to
achieve effective application for their lack of specificity, limited deepfake
diversity, restricted manipulation techniques.To address these limitations, we
introduce RedFace (Real-world-oriented Deepfake Face), a specialized facial
deepfake dataset, comprising over 60,000 forged images and 1,000 manipulated
videos derived from authentic facial features, to bridge the gap between
academic evaluations and real-world necessity. Unlike prior benchmarks, which
typically rely on academic methods to generate deepfakes, RedFace utilizes 9
commercial online platforms to integrate the latest deepfake technologies found
"in the wild", effectively simulating real-world black-box scenarios.Moreover,
RedFace's deepfakes are synthesized using bespoke algorithms, allowing it to
capture diverse and evolving methods used by real-world deepfake creators.
Extensive experimental results on RedFace (including cross-domain,
intra-domain, and real-world social network dissemination simulations) verify
the limited practicality of existing deepfake detection schemes against
real-world applications. We further perform a detailed analysis of the RedFace
dataset, elucidating the reason of its impact on detection performance compared
to conventional datasets. Our dataset is available at:
https://github.com/kikyou-220/RedFace.

</details>


### [106] [Physics-Driven Spatiotemporal Modeling for AI-Generated Video Detection](https://arxiv.org/abs/2510.08073)
*Shuhai Zhang,ZiHao Lian,Jiahao Yang,Daiyuan Li,Guoxuan Pang,Feng Liu,Bo Han,Shutao Li,Mingkui Tan*

Main category: cs.CV

> 该研究提出了一种基于物理原理的AI生成视频检测方法NSG-VD，该方法基于NSG特征的MMD实现，实验表明它在检测AI生成视频方面优于现有方法。

<details>
  <summary>Details</summary>

**Motivation:** 面对高度现实的AI生成视频（如Sora），需要开发可靠的检测手段来辨别。但在建模高维时空动态和识别违反物理定律的细微异常方面存在重大挑战。因此，研究提出了一种新的检测手段，改善这一现状。

**Method:** 提出了基于概率流守恒原理的物理驱动AI生成视频检测范式。具体而言，提出了一个称为标准化时空梯度（NSG）的统计指标，该指标量化了空间概率梯度与时间密度变化的比例，明确捕捉到了与自然视频动态的偏差。利用预训练的扩散模型，通过空间梯度近似和感知时间建模的方法开发了NSG估计器，无需复杂的运动分解即可保持物理约束。进而，提出了一个基于NSG的视频检测方法（NSG-VD），该方法通过计算测试视频与真实视频NSG特征之间的最大平均差异（MMD）作为检测指标。最后，推导出了真实视频和生成视频间NSG特征距离的上界，证明了由于分布偏移，生成视频展示出放大的偏差。

**Result:** 实验显示NSG-VD方法在召回率上比现有方法高出16%，F1分数上高出10.75%，证明了其检测性能的优越性。

**Conclusion:** 实验结果验证了NSG-VD在召回率和F1分数上分别比最先进的基线高出16.00%和10.75%，证明了NSG-VD的优越性能。源代码在https://github.com/ZSHsh98/NSG-VD提供。

**Abstract:** AI-generated videos have achieved near-perfect visual realism (e.g., Sora),
urgently necessitating reliable detection mechanisms. However, detecting such
videos faces significant challenges in modeling high-dimensional spatiotemporal
dynamics and identifying subtle anomalies that violate physical laws. In this
paper, we propose a physics-driven AI-generated video detection paradigm based
on probability flow conservation principles. Specifically, we propose a
statistic called Normalized Spatiotemporal Gradient (NSG), which quantifies the
ratio of spatial probability gradients to temporal density changes, explicitly
capturing deviations from natural video dynamics. Leveraging pre-trained
diffusion models, we develop an NSG estimator through spatial gradients
approximation and motion-aware temporal modeling without complex motion
decomposition while preserving physical constraints. Building on this, we
propose an NSG-based video detection method (NSG-VD) that computes the Maximum
Mean Discrepancy (MMD) between NSG features of the test and real videos as a
detection metric. Last, we derive an upper bound of NSG feature distances
between real and generated videos, proving that generated videos exhibit
amplified discrepancies due to distributional shifts. Extensive experiments
confirm that NSG-VD outperforms state-of-the-art baselines by 16.00% in Recall
and 10.75% in F1-Score, validating the superior performance of NSG-VD. The
source code is available at https://github.com/ZSHsh98/NSG-VD.

</details>


### [107] [DarkHash: A Data-Free Backdoor Attack Against Deep Hashing](https://arxiv.org/abs/2510.08094)
*Ziqi Zhou,Menghao Deng,Yufei Song,Hangtao Zhang,Wei Wan,Shengshan Hu,Minghui Li,Leo Yu Zhang,Dezhong Yao*

Main category: cs.CV

> 本文提出DarkHash，一种数据自由后门攻击技术，能有效植入后门功能而不影响原有的深度哈希模型性能，并展示了其在不同模型和哈希技术上的优越性。

<details>
  <summary>Details</summary>

**Motivation:** 现有研究依赖于访问训练数据来植入后门，但在实际应用中，获取此类数据（如身份信息）是受限的。因此，探索一种无需访问训练数据，并且能够维持原有检索准确度的后门植入方法成为亟待解决的问题。

**Method:** 我们提出DarkHash，这是一种无需访问训练数据的新型数据自由后门攻击方法。通过利用双重语义引导的方法，在不降低原有检索准确度的情况下，仅使用替代数据集微调受害者模型中的特定层以嵌入后门功能。此外，我们设计了一种拓扑对齐损失，优化被污染样本及其邻居向目标样本靠近，从而进一步增强攻击效果。

**Result:** 通过在四个图像数据集、五种模型架构以及两种哈希方法上进行的实验，验证了DarkHash的高度有效性，超过了现有的最先进后门攻击方法。防御实验表明DarkHash能够抵抗现有的主流后门防御方法。

**Conclusion:** DarkHash首次提出了无需训练数据的后门攻击框架，实现了高效且准确的后门嵌入，并在不同程度上抵抗现有的后门防御策略。

**Abstract:** Benefiting from its superior feature learning capabilities and efficiency,
deep hashing has achieved remarkable success in large-scale image retrieval.
Recent studies have demonstrated the vulnerability of deep hashing models to
backdoor attacks. Although these studies have shown promising attack results,
they rely on access to the training dataset to implant the backdoor. In the
real world, obtaining such data (e.g., identity information) is often
prohibited due to privacy protection and intellectual property concerns.
Embedding backdoors into deep hashing models without access to the training
data, while maintaining retrieval accuracy for the original task, presents a
novel and challenging problem. In this paper, we propose DarkHash, the first
data-free backdoor attack against deep hashing. Specifically, we design a novel
shadow backdoor attack framework with dual-semantic guidance. It embeds
backdoor functionality and maintains original retrieval accuracy by fine-tuning
only specific layers of the victim model using a surrogate dataset. We consider
leveraging the relationship between individual samples and their neighbors to
enhance backdoor attacks during training. By designing a topological alignment
loss, we optimize both individual and neighboring poisoned samples toward the
target sample, further enhancing the attack capability. Experimental results on
four image datasets, five model architectures, and two hashing methods
demonstrate the high effectiveness of DarkHash, outperforming existing
state-of-the-art backdoor attack methods. Defense experiments show that
DarkHash can withstand existing mainstream backdoor defense methods.

</details>


### [108] [Efficient Label Refinement for Face Parsing Under Extreme Poses Using 3D Gaussian Splatting](https://arxiv.org/abs/2510.08096)
*Ankit Gahlawat,Anirban Mukherjee,Dinesh Babu Jayagopi*

Main category: cs.CV

> The paper introduces a label refinement pipeline leveraging 3D Gaussian Splatting to improve face parsing accuracy in extreme poses. It enables better parsing performance on challenging poses with minimal manual intervention.

<details>
  <summary>Details</summary>

**Motivation:** The goal is to tackle the difficulty of achieving accurate face parsing in extreme viewing angles due to lack of labeled data in those poses, and to provide a solution that is both scalable and effective for real-world applications without needing a large amount of manual labeling.

**Method:** Our method involves generating accurate segmentation masks using a novel label refinement pipeline that employs 3D Gaussian Splatting (3DGS). This pipeline fits two 3DGS models jointly, one to RGB images and the other to initial segmentation maps, thus enforcing multiview consistency through shared geometry to synthesize pose-diverse training data.

**Result:** Experiments, including human evaluations, prove that this technique surpasses state-of-the-art methods in terms of face parsing accuracy, especially for atypical poses. Achievement of these results rely on pose-diverse training data generated from initial noisy predictions.

**Conclusion:** The approach leads to improved accuracy on challenging head poses while retaining high performance on standard views, indicating that by using minimal post-processing and no ground-truth 3D annotations, the proposed method outperforms existing state-of-the-art methods.

**Abstract:** Accurate face parsing under extreme viewing angles remains a significant
challenge due to limited labeled data in such poses. Manual annotation is
costly and often impractical at scale. We propose a novel label refinement
pipeline that leverages 3D Gaussian Splatting (3DGS) to generate accurate
segmentation masks from noisy multiview predictions. By jointly fitting two
3DGS models, one to RGB images and one to their initial segmentation maps, our
method enforces multiview consistency through shared geometry, enabling the
synthesis of pose-diverse training data with only minimal post-processing.
Fine-tuning a face parsing model on this refined dataset significantly improves
accuracy on challenging head poses, while maintaining strong performance on
standard views. Extensive experiments, including human evaluations, demonstrate
that our approach achieves superior results compared to state-of-the-art
methods, despite requiring no ground-truth 3D annotations and using only a
small set of initial images. Our method offers a scalable and effective
solution for improving face parsing robustness in real-world settings.

</details>
