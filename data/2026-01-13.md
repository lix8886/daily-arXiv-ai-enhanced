<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 7]
- [cs.CV](#cs.CV) [Total: 4]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [TeleMem: Building Long-Term and Multimodal Memory for Agentic AI](https://arxiv.org/abs/2601.06037)
*Chunliang Chen,Ming Guan,Xiao Lin,Jiaxu Li,Qiyi Wang,Xiangyu Chen,Jixiang Luo,Changzhi Sun,Dell Zhang,Xuelong Li*

Main category: cs.CL

> TeleMem是一个统一的长期和多模态记忆系统，它通过叙事动态提取来维护连贯的用户档案，并引入了结构化写入管道以提高效率。实验结果表明，TeleMem在ZH-4O长角色扮演游戏基准测试中超越了Mem0基准，准确率提高了19%，令牌使用减少了43%，速度提高了2.1倍。

<details>
  <summary>Details</summary>

**Motivation:** 大型语言模型在许多自然语言任务中表现出色，但在长时间对话中难以保持，因为它们对较长对话历史的关注有限。检索增强生成（RAG）缓解了这个问题，但缺乏可靠的机制来更新或精炼存储的记忆，导致基于模式的幻觉、低效的写入操作以及对多模态推理的支持不足。

**Method:** 提出了一种名为TeleMem的统一长期和多模态记忆系统，该系统通过叙事动态提取来维护连贯的用户档案，确保仅保存基于对话的信息。TeleMem 还引入了一个结构化的写入管道，该管道对记忆条目进行批量处理、检索、聚类和整合，显著提高了存储效率，减少了令牌使用量，并加速了记忆操作。此外，结合ReAct样式的推理，系统配备了一个闭合环路观察、思考和行动的过程，使系统能够准确理解长期上下文中的复杂视频内容。

**Result:** 实验结果显示，TeleMem在ZH-4O长角色扮演游戏基准测试中超越了Mem0基准，准确率提高了19%，令牌使用减少了43%，速度提高了2.1倍。

**Conclusion:** 实验结果表明，TeleMem能够显著提升长对话中的记忆管理效率和性能，尤其是在多模态理解和长角色扮演游戏等场景中表现突出。

**Abstract:** Large language models (LLMs) excel at many NLP tasks but struggle to sustain long-term interactions due to limited attention over extended dialogue histories. Retrieval-augmented generation (RAG) mitigates this issue but lacks reliable mechanisms for updating or refining stored memories, leading to schema-driven hallucinations, inefficient write operations, and minimal support for multimodal reasoning.To address these challenges, we propose TeleMem, a unified long-term and multimodal memory system that maintains coherent user profiles through narrative dynamic extraction, ensuring that only dialogue-grounded information is preserved. TeleMem further introduces a structured writing pipeline that batches, retrieves, clusters, and consolidates memory entries, substantially improving storage efficiency, reducing token usage, and accelerating memory operations. Additionally, a multimodal memory module combined with ReAct-style reasoning equips the system with a closed-loop observe, think, and act process that enables accurate understanding of complex video content in long-term contexts. Experimental results show that TeleMem surpasses the state-of-the-art Mem0 baseline with 19% higher accuracy, 43% fewer tokens, and a 2.1x speedup on the ZH-4O long-term role-play gaming benchmark.

</details>


### [2] [Operation Veja: Fixing Fundamental Concepts Missing from Modern Roleplaying Training Paradigms](https://arxiv.org/abs/2601.06039)
*Yueze Liu,Ajay Nagi Reddy Kumdam,Ronit Kanjilal,Hao Yang,Yichi Zhang*

Main category: cs.CL

> 本文提出了VEJA框架，该框架通过强调价值观、经验、判断和能力四个核心概念来解决现有角色扮演模型中的系统性限制。实证研究表明，VEJA框架下的数据集质量显著优于现有方法。

<details>
  <summary>Details</summary>

**Motivation:** 由于现有角色扮演模型难以捕捉到真实和引人入胜的角色，我们认为训练范式未能反映角色内在世界的动态交互。我们提出VEJA框架来解决这个问题。

**Method:** 我们提出了一种新的数据整理范式——VEJA框架，该框架强调价值观、经验、判断和能力四个核心概念，以解决现有角色扮演模型中的系统性限制。

**Result:** 通过一个试点研究，我们将基于VEJA的手动整理的数据集与最先进的合成数据集进行了比较，结果显示我们的方法在质量上有显著的优势。

**Conclusion:** 研究结果表明，通过采用概念上更为严谨的数据整理方法，如VEJA框架，可以创建出具有更大深度和连续叙事性的角色扮演代理。

**Abstract:** Modern roleplaying models are increasingly sophisticated, yet they consistently struggle to capture the essence of believable, engaging characters. We argue this failure stems from training paradigms that overlook the dynamic interplay of a character's internal world. Current approaches, including Retrieval-Augmented Generation (RAG), fact-based priming, literature-based learning, and synthetic data generation, exhibit recurring limitations in modeling the deliberative, value-conflicted reasoning that defines human interaction. In this paper, we identify four core concepts essential for character authenticity: Values, Experiences, Judgments, and Abilities (VEJA). We propose the VEJA framework as a new paradigm for data curation that addresses these systemic limitations. To illustrate the qualitative ceiling enabled by our framework, we present a pilot study comparing a manually curated, VEJA-grounded dataset against a state-of-the-art synthetic baseline. Using an LLM-as-judge evaluation, our findings demonstrate a significant quality gap, suggesting that a shift toward conceptually grounded data curation, as embodied by VEJA, is necessary for creating roleplaying agents with genuine depth and narrative continuity. The full dataset is available at https://github.com/HyouinKyoumaIRL/Operation-Veja

</details>


### [3] [Lexical and Statistical Analysis of Bangla Newspaper and Literature: A Corpus-Driven Study on Diversity, Readability, and NLP Adaptation](https://arxiv.org/abs/2601.06041)
*Pramit Bhattacharyya,Arnab Bhattacharya*

Main category: cs.CL

> 该论文通过对孟加拉文学和新闻文本的综合语料库驱动分析，研究了它们的词汇多样性、结构复杂性和可读性。结果显示文学语料库在词汇丰富性和结构变化上显著优于新闻语料库，并且将文学数据与新闻数据结合可以提高模型在下游任务上的性能。

<details>
  <summary>Details</summary>

**Motivation:** 研究孟加拉文学和新闻文本的词汇多样性、结构复杂性和可读性，评估文学和新闻语料库在语言特性上的差异以及它们对模型性能的影响。

**Method:** 采用了类型-标记比率(TTR)、单次出现词比率(HLR)、二元词组多样性、平均音节数和单词长度、以及齐普夫定律等关键语言特征来比较文学和新闻语料库，此外还构建了n元语言模型并测量困惑度，进行了可读性评估。

**Result:** 文学语料库在词汇丰富性和结构变化上比新闻语料库更有优势，具有更高的困惑度，符合齐普夫定律，更高的熵值和更低的冗余度，表明文学文本更复杂。

**Conclusion:** 该研究表明，将文学语料加入新闻语料可以提高模型在下游任务中的表现，并且文学语料在词汇多样性和复杂性方面优于新闻语料。

**Abstract:** In this paper, we present a comprehensive corpus-driven analysis of Bangla literary and newspaper texts to investigate their lexical diversity, structural complexity and readability. We undertook Vacaspati and IndicCorp, which are the most extensive literature and newspaper-only corpora for Bangla. We examine key linguistic properties, including the type-token ratio (TTR), hapax legomena ratio (HLR), Bigram diversity, average syllable and word lengths, and adherence to Zipfs Law, for both newspaper (IndicCorp) and literary corpora (Vacaspati).For all the features, such as Bigram Diversity and HLR, despite its smaller size, the literary corpus exhibits significantly higher lexical richness and structural variation. Additionally, we tried to understand the diversity of corpora by building n-gram models and measuring perplexity. Our findings reveal that literary corpora have higher perplexity than newspaper corpora, even for similar sentence sizes. This trend can also be observed for the English newspaper and literature corpus, indicating its generalizability. We also examined how the perfor- mance of models on downstream tasks is influenced by the inclusion of literary data alongside newspaper data. Our findings suggest that inte- grating literary data with newspapers improves the performance of models on various downstream tasks. We have also demonstrated that a literary corpus adheres more closely to global word distribution proper- ties, such as Zipfs law, than a newspaper corpus or a merged corpus of both literary and newspaper texts. Literature corpora also have higher entropy and lower redundancy values compared to a newspaper corpus. We also further assess the readability using Flesch and Coleman-Liau in- dices, showing that literary texts are more complex.

</details>


### [4] [Reinforcement Learning for Chain of Thought Compression with One-Domain-to-All Generalization](https://arxiv.org/abs/2601.06052)
*Hanyu Li,Jiangshan Duo,Bofei Gao,Hailin Zhang,Sujian Li,Xiaotie Deng,Liang Zhao*

Main category: cs.CL

> 我们提出了样本级别的、软性强化学习方法来压缩语言模型的推理计算，这种方法在实验中显示可以有效减少响应长度同时保持或提高准确性。

<details>
  <summary>Details</summary>

**Motivation:** 大型语言模型中的链式思考推理往往导致计算成本和延迟的增加，但仅仅依靠全局静态控制可能存在惩罚必要推理的风险。因此，为了减少计算成本和提高模型效率，我们需要一种新的压缩方法。

**Method:** 我们提出了一种基于样本级别的、软性强化学习的压缩方法，该方法仅在模型已经解决并且已经生成简洁推理的问题上，对低效的长推理进行惩罚。

**Result:** 实验表明，该方法可以使平均响应长度减少20-40%，同时保持或提高准确性。并且该压缩方法表现出跨领域良好的泛化能力。

**Conclusion:** 我们展示了在训练后能稳定准确度-压缩-准确度的课程训练方法，可以最终产生更准确和简洁推理的模型，并认为这种压缩方法应成为开发有效推理模型的标准阶段。

**Abstract:** Chain-of-thought reasoning in large language models often creates an "overthinking trap," leading to excessive computational cost and latency for unreliable accuracy gains. Prior work has typically relied on global, static controls that risk penalizing necessary reasoning. We introduce a sample-level, soft reinforcement learning compression method that penalizes inefficiently long rollouts, but only on problems where the model has already mastered and already produced a more concise rollout. Our experiments show that this method reduces average response length by 20-40% with comparable or higher accuracy. Crucially, the compression exhibits strong cross-domain generalization; a model trained on math spontaneously shortens responses on unseen tasks like code, instruction following, and general knowledge QA, with stable or improved accuracy. We demonstrate a stable post-training curriculum (accuracy-compression-accuracy) that can ultimately produce models that are more accurate and reason more concisely, arguing that such compression method should be a standard phase in developing efficient reasoning models.

</details>


### [5] [A Multi-Stage Workflow for the Review of Marketing Content with Reasoning Large Language Models](https://arxiv.org/abs/2601.06054)
*Alberto Purpura,Emily Chen,Swapnil Shinde*

Main category: cs.CL

> 本文提出了一种不依赖外部知识表示的创新方法，用于自动识别文本内容中的合规性问题。通过比较不同微调策略和奖励函数的有效性，优化语言模型在审查营销内容是否符合特定要求方面的应用。

<details>
  <summary>Details</summary>

**Motivation:** 大型语言模型（LLMs）已经在解决复杂问题方面显示出有希望的结果。本文旨在通过提出和评估一种利用微调推理LLMs的能力来辅助审查营销内容是否符合特定要求的多阶段工作流程，进一步优化语言模型的应用。

**Method:** 我们提出了一种不依赖外部知识表示的多阶段工作流程，通过微调推理语言模型来辅助审查营销内容是否符合特定要求。我们比较了几种微调策略的有效性，包括监督微调(SFT)和相对策略优化(GRPO)，并且评估了在输出最终响应前训练小型语言模型生成推理标记的有效性。此外，我们还评估了不同奖励函数的选择和组合对使用GRPO训练模型性能的影响。

**Result:** 本研究比较了不同微调策略的有效性，评估了生成推理标记的小型语言模型的表现，并分析了不同奖励函数的选择和组合对模型性能的影响。

**Conclusion:** 我们的方法提供了一种创新的方式来自动识别文本内容中的合规性问题，这有助于优化营销内容的审查流程。通过比较不同微调策略和奖励函数的有效性，我们的工作为未来的工作提供了参考。

**Abstract:** Reasoning Large Language Models (LLMs) have shown promising results when tasked with solving complex problems. In this paper, we propose and evaluate a multi-stage workflow that leverages the capabilities of fine-tuned reasoning LLMs to assist in the review process of marketing content, making sure they comply with a given list of requirements. The contributions of this paper are the following: (i) we present a novel approach -- that does not rely on any external knowledge representation -- for the automatic identification of compliance issues in textual content; (ii) compare the effectiveness of different fine-tuning strategies like Supervised Fine-Tuning (SFT) and Group Relative Policy Optimization (GRPO) in training models to solve this problem; (iii) we evaluate the effectiveness of training small LLMs to generate reasoning tokens before providing their final response; (iv) we evaluate how the choice and combinations of different reward functions affects the performance of a model trained with GRPO.

</details>


### [6] [AzeroS: Extending LLM to Speech with Self-Generated Instruction-Free Tuning](https://arxiv.org/abs/2601.06086)
*Yiwen Shao,Wei Liu,Jiahong Li,Tianzi Wang,Kun Wei,Meng Yu,Dong Yu*

Main category: cs.CL

> 研究提出了AZeroS模型，这是一种仅在两个轻量级投影模块上进行训练的语音-LLM模型，能够在未见过的任务上实现最佳泛化效果，并展现出优越的性能。

<details>
  <summary>Details</summary>

**Motivation:** 现有方法在将大型语言模型（LLMs）扩展到语音领域时，需要花费大量时间收集特定任务的指令微调数据，并且训练出的模型在未见过的任务上泛化能力差。为了克服这些限制，提出了AZeroS模型。

**Method:** 通过使用自动生成的指令无监督微调方法（SIFT），模型利用文本表示的语音作为输入来生成监督信号，这消除了收集特定任务的问题-答案对的需要，并在理论上为未见过的任务提供了最好的泛化能力。AZeroS 模型是在从公共语料库导出的语音-文本对上训练的，只更新了两个轻量级的投影模块，而保持 LLM 和音频编码器不变。

**Result:** 尽管训练成本低且数据规模适中，AZeroS 模型在语义和副语言基准测试中表现出了最先进的性能，包括VoiceBench、AIR-Bench Foundation (Speech) 和AIR-Bench Chat (Speech)。

**Conclusion:** AZeroS模型通过自动生成指令无监督微调的SIFT范式，实现了对未见过任务的最佳泛化，并展示了在语音理解和生成方面的先进性能。

**Abstract:** Extending large language models (LLMs) to the speech domain has recently gained significant attention. A typical approach connects a pretrained LLM with an audio encoder through a projection module and trains the resulting model on large-scale, task-specific instruction-tuning datasets. However, curating such instruction-tuning data for specific requirements is time-consuming, and models trained in this manner often generalize poorly to unseen tasks. In this work, we first formulate that the strongest generalization of a speech-LLM is achieved when it is trained with Self-Generated Instruction-Free Tuning (SIFT), in which supervision signals are generated by a frozen LLM using textual representations of speech as input. Our proposed SIFT paradigm eliminates the need for collecting task-specific question-answer pairs and yields the theoretically best generalization to unseen tasks. Building upon this paradigm, we introduce AZeroS (Auden Zero-instruction-tuned Speech-LLM), which is trained on speech-text pairs derived from publicly available corpora, including approximately 25,000 hours of speech with ASR transcripts and 3,000 hours of speech with paralinguistic labels. Built upon Qwen2.5-7B-Instruct, the model updates only two lightweight projection modules (23.8 million parameters each), while keeping both the LLM and audio encoders frozen. Despite the minimal training cost and modest data scale, AZeroS achieves state-of-the-art performance on both semantic and paralinguistic benchmarks, including VoiceBench, AIR-Bench Foundation (Speech), and AIR-Bench Chat (Speech).

</details>


### [7] [Is Sanskrit the most token-efficient language? A quantitative study using GPT, Gemini, and SentencePiece](https://arxiv.org/abs/2601.06142)
*Anshul Kumar*

Main category: cs.CL

> 研究结果显示梵文在token数量上比英文或印地语更紧凑，使用SentencePiece（SPM）、GPT和Gemini等tokenizer测试了梵文、英文和印地语的token效率和成本，发现GPT-o200k base和Gemini在减少token数量偏差方面有所改善，但仍未完全解决梵文的紧凑性问题。这一研究成果有助于未来tokenizer设计的改进，并显示了梵文在节省成本方面的重要潜力。

<details>
  <summary>Details</summary>

**Motivation:** 研究探讨了梵文由于其形态和语法规则，可能比其他语言更有效地表达意义。在此之前，没有任何研究对这一假设进行过量化验证。因此，本研究旨在填补这一空白，并提供对梵文在tokenizer效率方面潜在价值的见解。

**Method:** 使用了包含701个巴格瓦提歌平行诗句的数据集，该数据集包含三种语言——梵文、英文和印地语以及梵文的转写文本。测试了包括SentencePiece (SPM), 早期GPT模型以及Gemini和GPT的最新一代tokenizer。使用了token数、每token字符数（token效率）和每字符token数（token成本）等指标。

**Result:** 在无偏SentencePiece（SPM）基准下，梵文的token计数比英文/印地语约少2倍。梵文注释的英文/印地语翻译导致token计数增加了大约20倍。使用GPT-o200k base和Gemini的最新版本虽然在一定程度上减少了偏差，但仍未完全捕捉到梵文的紧凑性。

**Conclusion:** 研究表明梵文相对于英语和印地语可能更紧凑，使用不同的tokenizer会在token数量上有显著差异，从而影响计算成本和推理效率。该研究为改善未来的tokenizer设计提供了基础，并显示了梵文在进行紧凑编码方面的潜力，有助于节省成本并加快训练和推理的速度。

**Abstract:** Tokens are the basic units of Large Language Models (LLMs). LLMs rely on tokenizers to segment text into these tokens, and tokenization is the primary determinant of computational and inference cost. Sanskrit, one of the oldest languages, is hypothesized to express more meaning per token due to its morphology and grammar rules; however, no prior work has quantified this. We use a dataset of 701 parallel verses of the Bhagavad Gita, which comprises three languages-Sanskrit, English, and Hindi along with transliteration of Sanskrit into English. We test tokenizers including SentencePiece (SPM), older GPT models, and the latest generation tokenizers from Gemini and GPT. We use metrics of token count, characters per token (token efficiency), and tokens per character (token cost). Results show a ~2x difference in token counts between Sanskrit and English/Hindi under the unbiased SPM baseline. English/Hindi translations of Sanskrit commentary resulted in an approximately 20x increase in token count. GPT o200k base (latest, used by GPT-4o) and Gemini (latest) reduce bias by a significant degree compared to GPT cl100k base (used until GPT-4), but still fail to fully capture Sanskrit's compactness. This matters because there might be a penalty bias for non-English users, which inflates the token count. This research provides a foundation for improving future tokenizer design and shows the potential of Sanskrit for highly compact encoding, saving on cost while speeding up training and inference. The code and dataset are available at https://github.com/anshulkr713/sanskrit-token-efficiency

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [8] [HyperTopo-Adapters: Geometry- and Topology-Aware Segmentation of Leaf Lesions on Frozen Encoders](https://arxiv.org/abs/2601.06067)
*Chimdi Walter Ndubuisi,Toni Kazic*

Main category: cs.CV

> 研究提出了一种名为HyperTopo-Adapters的新方法用于叶斑分割，改进了边界和拓扑度量，同时保持了Dice和IoU的竞争力。

<details>
  <summary>Details</summary>

**Motivation:** 背景是现有的像素级损失在欧几里得潜在特征中对叶斑分割中的小合并、分裂或虚假洞等生物意义描述的弱惩罚。

**Method:** 研究提出了一种名为HyperTopo-Adapters的轻量化且参数高效的方法，该方法基于冻结的视觉编码器之上进行训练，在特征嵌入中使用了超双曲+欧几里得+球形流形（H + E + S）来鼓励分层分离（H），局部线性细节（E）和全局封闭（S）。

**Result:** 通过引入Warm-ups、超双曲对比性项和拓扑先验，研究人员能够在边界和拓扑度量方面取得显著改进（减少Delta beta_1孔隙误差9%），同时保持Dice交并比（IoU）的竞争性。

**Conclusion:** 这种研究是一个具有诊断性质的研究，它提供了一个公开可重复的训练/评估套件，它将几何/拓扑先验与失败模式隔离，以引导更强健且具有保留拓扑结构的能力的架构。

**Abstract:** Leaf-lesion segmentation is topology-sensitive: small merges, splits, or false holes can be biologically meaningful descriptors of biochemical pathways, yet they are weakly penalized by standard pixel-wise losses in Euclidean latents. I explore HyperTopo-Adapters, a lightweight, parameter-efficient head trained on top of a frozen vision encoder, which embeds features on a product manifold -- hyperbolic + Euclidean + spherical (H + E + S) -- to encourage hierarchical separation (H), local linear detail (E), and global closure (S). A topology prior complements Dice/BCE in two forms: (i) persistent-homology (PH) distance for evaluation and selection, and (ii) a differentiable surrogate that combines a soft Euler-characteristic match with total variation regularization for stable training. I introduce warm-ups for both the hyperbolic contrastive term and the topology prior, per-sample evaluation of structure-aware metrics (Boundary-F1, Betti errors, PD distance), and a min-PD within top-K Dice rule for checkpoint selection. On a Kaggle leaf-lesion dataset (N=2,940), early results show consistent gains in boundary and topology metrics (reducing Delta beta_1 hole error by 9%) while Dice/IoU remain competitive. The study is diagnostic by design: I report controlled ablations (curvature learning, latent dimensions, contrastive temperature, surrogate settings), and ongoing tests varying encoder strength (ResNet-50, DeepLabV3, DINOv2/v3), input resolution, PH weight, and partial unfreezing of late blocks. The contribution is an open, reproducible train/eval suite (available at https://github.com/ChimdiWalter/HyperTopo-Adapters) that isolates geometric/topological priors and surfaces failure modes to guide stronger, topology-preserving architectures.

</details>


### [9] [OptFormer: Optical Flow-Guided Attention and Phase Space Reconstruction for SST Forecasting](https://arxiv.org/abs/2601.06078)
*Yin Wang,Chunlin Gong,Zhuozhen Xu,Lehan Zhang,Xiang Wu*

Main category: cs.CV

> 提出了一种名为OptFormer的新模型，用于提高海面温度预测的准确性。该模型结合了相空间重构与运动感知注意机制来提高预测性能。

<details>
  <summary>Details</summary>

**Motivation:** 海面温度预测对气候模型和灾害预报至关重要，但预测难点在于其非线性时空动态和长期预测范围。

**Method:** OptFormer模型整合了相空间重构与基于光流的运动感知注意力机制，以捕捉动态区域并有效捕捉长期时间依赖性。

**Result:** 在NOAA SST数据集上的实验表明，OptFormer在1:1训练到预测设置下表现优异，优于现有基线模型，特别是在准确性与鲁棒性方面。

**Conclusion:** 研究表明，OptFormer模型在海面温度长期预测上表现出很好的性能。

**Abstract:** Sea Surface Temperature (SST) prediction plays a vital role in climate modeling and disaster forecasting. However, it remains challenging due to its nonlinear spatiotemporal dynamics and extended prediction horizons. To address this, we propose OptFormer, a novel encoder-decoder model that integrates phase-space reconstruction with a motion-aware attention mechanism guided by optical flow. Unlike conventional attention, our approach leverages inter-frame motion cues to highlight relative changes in the spatial field, allowing the model to focus on dynamic regions and capture long-range temporal dependencies more effectively. Experiments on NOAA SST datasets across multiple spatial scales demonstrate that OptFormer achieves superior performance under a 1:1 training-to-prediction setting, significantly outperforming existing baselines in accuracy and robustness.

</details>


### [10] [Semantic Event Graphs for Long-Form Video Question Answering](https://arxiv.org/abs/2601.06097)
*Aradhya Dixit,Tianxi Liang*

Main category: cs.CV

> 本文提出了一种名为Semantic Event Graphs (SEG)的轻量级符号接口，该接口通过将视频转化为紧凑的时间交互日志来改进长视频问答的效率。在不牺牲准确率的情况下，SEG显著减少了所需的计算资源。

<details>
  <summary>Details</summary>

**Motivation:** 当前的视觉语言模型在处理小时级视频内容时，由于计算资源和令牌预算的限制而面临挑战。本文旨在提供一种解决方案，以提高长视频问答的效率，同时保持其准确性。

**Method:** SEG首先使用YOLOv11检测并跟踪视频中的对象，然后将对象间的接近模式转换为开始和结束的事件，并构建一个Temporal Scene Graph (TSG)。在推理时，使用查询感知修剪模块来识别相关实体和事件，并生成答案。

**Result:** 在五个YouTube视频和自动产生的长时域问题上，SEG取得了65.0%的准确率，仅使用了3.47k个令牌，与全日志基线性能相当但减少了91.4%的令牌使用。

**Conclusion:** 本文证实，符号化的时间图可以作为一种有效的插件式记忆层，用于现有的视觉语言模型，以保持远程推理能力，同时显著提高长视频问答的令牌和成本效率。

**Abstract:** Long-form video question answering remains challenging for modern vision-language models, which struggle to reason over hour-scale footage without exceeding practical token and compute budgets. Existing systems typically downsample frames or feed dense visual embeddings to large-context language models, trading off temporal coverage against cost. We propose Semantic Event Graphs (SEG), a lightweight symbolic interface between video and language that replaces raw frames with compact temporal interaction logs. Our pipeline detects and tracks objects with YOLOv11, converts proximity patterns into START/END human-object events, and organizes them into a Temporal Scene Graph (TSG). At inference time, a query-aware pruning module identifies anchor entities and lexically relevant events, returning only a small subgraph which is verbalized and passed to Gemini 2.5 Flash for answer generation. On five YouTube videos (300-500 interactions each) and 120 automatically generated long-horizon questions, SEG achieves 65.0% accuracy using only 3.47k tokens per query, closely matching a full-log baseline (62.5% at 40.39k tokens) while reducing token usage by 91.4%. A short-context baseline restricted to the last 30 seconds collapses to 2.5% accuracy, underscoring the need for explicit temporal memory. These results show that symbolic temporal graphs can serve as an effective, plug-and-play memory layer for off-the-shelf vision-language models, preserving long-range reasoning ability while making long-form video question answering substantially more token- and cost-efficient. Code, logs, and event-extraction tools will be released for reproducibility.

</details>


### [11] [COVR:Collaborative Optimization of VLMs and RL Agent for Visual-Based Control](https://arxiv.org/abs/2601.06122)
*Canming Xia,Peixi Peng,Guang Tan,Zhan Su,Haoran Xu,Zhenxian Liu,Luntong Li*

Main category: cs.CV

> 该研究提出了一个名为COVR的框架，用于通过视觉语言模型（VLM）和强化学习（RL）之间的交互数据来进行彼此增强，从而改善视觉控制任务中的学习效率和性能。

<details>
  <summary>Details</summary>

**Motivation:** 现有的研究表明视觉语言模型（VLM）可以辅助视觉强化学习（RL），但他们通常只关注从VLM到RL的知识迁移，忽略了RL生成的交互数据对VLM的潜在增强作用。因此，这项研究旨在解决这个问题，通过提出一个新的方法来增强VLM和RL策略之间的相互作用。

**Method:** 该论文提出了一个名为COVR的协作优化框架，该框架通过精细调整视觉语言模型（VLM）和强化学习（RL）策略之间的交互数据来相互增强这两种模型。特别地，COVR利用RL生成的数据来提高VLM的语义推理能力，并且利用增强后的VLM通过动作优先级来进一步指导RL策略的学习。为了提高精细调整的效率，该论文引入了两个关键模块：(1) 基于探索程度的自适应阈值，保留有价值的探索样本的探索驱动动态滤波模块；(2) 通过RL产生的回报信号量化采样动作的不一致性，提高训练稳定性的回报感知自适应损失权重模块。此外，作者还设计了一种渐进式精细调整策略来减少资源消耗。

**Result:** 通过一系列实验，展示了COVR框架在不同有挑战性的视觉控制任务中表现出强大的性能。

**Conclusion:** 实验结果显示，在多个具有挑战性的视觉控制任务中，COVR达到了很强的性能表现。

**Abstract:** Visual reinforcement learning (RL) suffers from poor sample efficiency due to high-dimensional observations in complex tasks. While existing works have shown that vision-language models (VLMs) can assist RL, they often focus on knowledge distillation from the VLM to RL, overlooking the potential of RL-generated interaction data to enhance the VLM. To address this, we propose COVR, a collaborative optimization framework that enables the mutual enhancement of the VLM and RL policies. Specifically, COVR fine-tunes the VLM with RL-generated data to enhance the semantic reasoning ability consistent with the target task, and uses the enhanced VLM to further guide policy learning via action priors. To improve fine-tuning efficiency, we introduce two key modules: (1) an Exploration-Driven Dynamic Filter module that preserves valuable exploration samples using adaptive thresholds based on the degree of exploration, and (2) a Return-Aware Adaptive Loss Weight module that improves the stability of training by quantifying the inconsistency of sampling actions via return signals of RL. We further design a progressive fine-tuning strategy to reduce resource consumption. Extensive experiments show that COVR achieves strong performance across various challenging visual control tasks.

</details>
