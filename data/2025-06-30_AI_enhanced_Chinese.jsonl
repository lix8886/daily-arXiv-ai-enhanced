{"id": "2506.21555", "categories": ["cs.CL", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.21555", "abs": "https://arxiv.org/abs/2506.21555", "authors": ["Jiahong Li", "Yiwen Shao", "Jianheng Zhuo", "Chenda Li", "Liliang Tang", "Dong Yu", "Yanmin Qian"], "title": "Efficient Multilingual ASR Finetuning via LoRA Language Experts", "comment": "Accepted in Interspeech 2025", "summary": "Recent advancements in deep learning have significantly enhanced multilingual\nautomatic speech recognition (ASR) due to the development of advanced model\narchitectures and available large-scale multilingual datasets. Despite that,\nmultilingual ASR still suffers from the curse of multilinguality in that\ndifferent languages tend to interfere with each other, making it difficult for\nthe ASR model to identify multiple languages effectively while sharing model\ncapacity across them. This paper proposes an efficient finetuning framework for\ncustomized multilingual ASR via prepared LoRA language experts based on\nWhisper. Through LoRA expert fusion or knowledge distillation, our approach\nachieves better recognition performance on target languages than standard\nfine-tuning methods. Experimental results demonstrate that the proposed models\nyield approximately 10\\% and 15\\% relative performance gains in language-aware\nand language-agnostic scenarios, respectively.", "AI": {"tldr": "本文聚焦于多语种ASR中不同语言的干扰问题，提供了一种基于LoRA语言专家的高效微调方法，实验显示在特定语言和语言不可知场景下性能显著提升。", "motivation": "尽管深度学习的进展提高了多语种自动语音识别（ASR）的效果，但不同语言之间的相互干扰使得ASR模型在共享模型容量的同时难以有效识别多种语言。", "method": "本论文提出了一种基于LoRA语言专家的高效微调框架，通过LoRA专家融合或知识蒸馏的方法来提升多语种语音识别的效果。", "result": "实验结果表明，在语言感知和语言不可知的场景下，所提出的模型分别实现了约10%和15%的相对性能提升。", "conclusion": "提出的微调框架通过LoRA专家和知识蒸馏技术，相对于标准微调方法，在目标语言上获得了更好的识别性能。"}}
{"id": "2506.21556", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.21556", "abs": "https://arxiv.org/abs/2506.21556", "authors": ["Hyeongcheol Park", "MinHyuk Jang", "Ha Dam Baek", "Gyusam Chang", "Jiyoung Seo", "Jiwan Park", "Hogun Park", "Sangpil Kim"], "title": "VAT-KG: Knowledge-Intensive Multimodal Knowledge Graph Dataset for Retrieval-Augmented Generation", "comment": "Project Page: https://vatkg.github.io/", "summary": "Multimodal Knowledge Graphs (MMKGs), which represent explicit knowledge\nacross multiple modalities, play a pivotal role by complementing the implicit\nknowledge of Multimodal Large Language Models (MLLMs) and enabling more\ngrounded reasoning via Retrieval Augmented Generation (RAG). However, existing\nMMKGs are generally limited in scope: they are often constructed by augmenting\npre-existing knowledge graphs, which restricts their knowledge, resulting in\noutdated or incomplete knowledge coverage, and they often support only a narrow\nrange of modalities, such as text and visual information. These limitations\nreduce their extensibility and applicability to a broad range of multimodal\ntasks, particularly as the field shifts toward richer modalities such as video\nand audio in recent MLLMs. Therefore, we propose the Visual-Audio-Text\nKnowledge Graph (VAT-KG), the first concept-centric and knowledge-intensive\nmultimodal knowledge graph that covers visual, audio, and text information,\nwhere each triplet is linked to multimodal data and enriched with detailed\ndescriptions of concepts. Specifically, our construction pipeline ensures\ncross-modal knowledge alignment between multimodal data and fine-grained\nsemantics through a series of stringent filtering and alignment steps, enabling\nthe automatic generation of MMKGs from any multimodal dataset. We further\nintroduce a novel multimodal RAG framework that retrieves detailed\nconcept-level knowledge in response to queries from arbitrary modalities.\nExperiments on question answering tasks across various modalities demonstrate\nthe effectiveness of VAT-KG in supporting MLLMs, highlighting its practical\nvalue in unifying and leveraging multimodal knowledge.", "AI": {"tldr": "提出Visual-Audio-Text知识图谱（VAT-KG），这是一种覆盖视觉、音频和文本信息的概念中心型多模态知识图谱，通过严格的过滤和对齐步骤自动从任何多模态数据集中生成。实验表明VAT-KG在跨模态问答任务中有效支持多模态大语言模型。", "motivation": "现有的多模态知识图谱受限于范围和模态，限制了知识的完整性和丰富性，尤其是在视频和音频等更丰富的模态日益重要的情况下。因此提出VAT-KG来扩展知识图谱的范围和适用性。", "method": "采用一系列严格的过滤和对齐步骤，构建跨模态知识对齐的管道，自动从任何多模态数据集中生成VAT-KG。", "result": "实验表明，VAT-KG在跨模态的问答任务中有效，证明了其在统一和利用多模态知识上的实用价值。", "conclusion": "VAT-KG通过涵盖视觉、音频和文本信息并支持从任意模态检索详细的概念级别知识，证明了其在支持多模态大语言模型中的有效性。"}}
{"id": "2506.21557", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.21557", "abs": "https://arxiv.org/abs/2506.21557", "authors": ["Kaiying Yan", "Moyang Liu", "Yukun Liu", "Ruibo Fu", "Zhengqi Wen", "Jianhua Tao", "Xuefei Liu"], "title": "Debunk and Infer: Multimodal Fake News Detection via Diffusion-Generated Evidence and LLM Reasoning", "comment": null, "summary": "The rapid spread of fake news across multimedia platforms presents serious\nchallenges to information credibility. In this paper, we propose a\nDebunk-and-Infer framework for Fake News Detection(DIFND) that leverages\ndebunking knowledge to enhance both the performance and interpretability of\nfake news detection. DIFND integrates the generative strength of conditional\ndiffusion models with the collaborative reasoning capabilities of multimodal\nlarge language models (MLLMs). Specifically, debunk diffusion is employed to\ngenerate refuting or authenticating evidence based on the multimodal content of\nnews videos, enriching the evaluation process with diverse yet semantically\naligned synthetic samples. To improve inference, we propose a chain-of-debunk\nstrategy where a multi-agent MLLM system produces logic-grounded,\nmultimodal-aware reasoning content and final veracity judgment. By jointly\nmodeling multimodal features, generative debunking cues, and reasoning-rich\nverification within a unified architecture, DIFND achieves notable improvements\nin detection accuracy. Extensive experiments on the FakeSV and FVC datasets\nshow that DIFND not only outperforms existing approaches but also delivers\ntrustworthy decisions.", "AI": {"tldr": "论文提出了一种结合反证知识改进假新闻检测方法的框架DIFND，在FakeSV和FVC数据集上其实验结果超越了现有技术方案并提供了可信的决策结果。", "motivation": "假新闻在多媒体平台上的快速传播对信息可信度构成了严重挑战。该论文旨在利用反击知识来提高假新闻检测的表现和可解释性。", "method": "提出了一种称作Debunk-and-Infer框架用于假新闻检测(DIFND)，该框架结合了条件扩散模型的生成能力和多模态大语言模型（MLLMs）的协作推理能力。具体而言，使用反证扩散生成基于新闻视频多模态内容的反驳或认证证据，丰富评估过程。为提高推理，提出了一种多重反证策略，其中多代理MLLM系统产生逻辑基础且多模态感知的推理内容并对最终真实性进行判断。", "result": "在假新闻检测的准确性方面取得了显著提升。在FakeSV和FVC数据集上的广泛实验证明，DIFND不仅超越了现有方法，而且提供了值得信赖的决策。", "conclusion": "DIFND通过结合多模态特征、生成性反证线索和丰富的推理验证，在统一架构中实现了假新闻检测的显著改进。实验表明其在提高检测准确性的同时，还提供了可信的决策。"}}
{"id": "2506.21558", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.21558", "abs": "https://arxiv.org/abs/2506.21558", "authors": ["FutureSearch", ":", "Jack Wildman", "Nikos I. Bosse", "Daniel Hnyk", "Peter Mühlbacher", "Finn Hambly", "Jon Evans", "Dan Schwarz", "Lawrence Phillips"], "title": "Bench to the Future: A Pastcasting Benchmark for Forecasting Agents", "comment": null, "summary": "Forecasting is a challenging task that offers a clearly measurable way to\nstudy AI systems. Forecasting requires a large amount of research on the\ninternet, and evaluations require time for events to happen, making the\ndevelopment of forecasting benchmarks challenging. To date, no forecasting\nbenchmark provides a realistic, hermetic, and repeatable environment for LLM\nforecasters. We introduce Bench To the Future (BTF), a \"pastcasting\" benchmark\nwith hundreds of high-quality questions for which the resolution is already\nknown. Each question is accompanied by a large offline corpus of tens of\nthousands of relevant web pages, enabling a way to elicit realistic \"forecasts\"\non past events from LLMs. Results suggest that our pastcasting environment can\nproduce results comparable to those based on forecasts using the internet on\nat-the-time unresolved questions. We show results benchmarking agent and\nchain-of-thought forecasting approaches using several LLMs, including the\nrecently-released Claude 4 models, and demonstrate BTF's ability to track\nsteady forecasting capability progress over time. We intend this to be a living\nbenchmark, with new questions added continually to account for increasing\ntraining data cutoff dates. We invite researchers to contact us at\nhello@futuresearch.ai to utilize our benchmark or tooling for their own\nresearch.", "AI": {"tldr": "BTF是一个新的“回溯”评估基准，通过高质量的已知结果问题和相关网页集合，为大模型提供了一个现实的、封闭的且可重复的预测测试环境，旨在成为持续更新的活跃基准。", "motivation": "当前没有一个预测基准能够提供一个现实的、封闭的且可重复的环境供大模型进行预测，而预测又需要大量的在线研究以及时间来验证结果，这使得开发预测基准极具挑战性。", "method": "介绍了一个名为Bench To the Future (BTF) 的“回溯”基准测试，包含数百个已知结果的高质量问题，每个问题都伴随着成千上万的相关网页的离线语料库，使大规模语言模型（LLMs）能够对过去事件进行实际的“预测”。", "result": "结果显示，该回溯环境所产生的结果与基于尚未解决的问题并使用互联网进行预测所产生的结果相当。使用几个大模型，包括最近发布的Claude 4模型，进行代理预测和链式思考预测方法的基准测试，并展示了BTF追踪预测能力随时间发展的能力。", "conclusion": "BTF能够作为一个活的基准，会持续更新新的问题，以适应不断增加的数据训练截止日期。"}}
{"id": "2506.21656", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.21656", "abs": "https://arxiv.org/abs/2506.21656", "authors": ["Yifan Shen", "Yuanzhe Liu", "Jingyuan Zhu", "Xu Cao", "Xiaofeng Zhang", "Yixiao He", "Wenming Ye", "James Matthew Rehg", "Ismini Lourentzou"], "title": "Fine-Grained Preference Optimization Improves Spatial Reasoning in VLMs", "comment": "29 pages", "summary": "Current Vision-Language Models (VLMs) struggle with fine-grained spatial\nreasoning, particularly when multi-step logic and precise spatial alignment are\nrequired. In this work, we introduce SpatialReasoner-R1, a vision-language\nreasoning model designed to address these limitations. To construct\nhigh-quality supervision for spatial reasoning, we design a Multi-Model Monte\nCarlo Tree Search (M3CTS) method that generates diverse, logically consistent\nLong Chain-of-Thought (LongCoT) reasoning trajectories. In addition, we propose\nfine-grained Direct Preference Optimization (fDPO), which introduces\nsegment-specific preference granularity for descriptive grounding and logical\nreasoning, guided by a spatial reward mechanism that evaluates candidate\nresponses based on visual consistency, spatial grounding, and logical\ncoherence. Experimental results demonstrate that fDPO achieves an average\nimprovement of 4.1% over standard DPO across spatial quality tasks, and a 9.0%\ngain in spatial quantity tasks. SpatialReasoner-R1, trained with fDPO, sets a\nnew SoTA on SPATIALRGPT-Bench, outperforming the strongest baseline by 9.8% in\naverage accuracy, while maintaining competitive performance on general\nvision-language tasks.", "AI": {"tldr": "The paper presents SpatialReasoner-R1 and innovative fDPO and M3CTS techniques, improving spatial reasoning in VLMs with significant gains over existing methods.", "motivation": "The main motivation is to enhance current Vision-Language Models (VLMs) by addressing their issues with fine-grained spatial reasoning, especially in situations involving multi-step logic and detailed spatial alignment.", "method": "The paper introduces the SpatialReasoner-R1 model and utilizes a novel Multi-Model Monte Carlo Tree Search (M3CTS) method to generate high-quality supervision for spatial reasoning. A fine-grained Direct Preference Optimization (fDPO) technique is also proposed, which provides segment-specific preference granularity for descriptive grounding and logical reasoning.", "result": "The experimental outcomes reveal that SpatialReasoner-R1, utilizing fDPO, enhances spatial quality task performance by an average of 4.1%, while achieving a 9.0% increase in spatial quantity tasks, and surpasses the current best model by 9.8% on the SPATIALRGPT-Bench.", "conclusion": "SpatialReasoner-R1, incorporating fDPO, establishes a new state-of-the-art level of performance on the SPATIALRGPT-Bench, showing a major improvement over the previous best model."}}
{"id": "2506.21559", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.21559", "abs": "https://arxiv.org/abs/2506.21559", "authors": ["Junze Chen", "Cheng Yang", "Shujie Li", "Zhiqiang Zhang", "Yawen Li", "Junping Du", "Chuan Shi"], "title": "GraphLAMA: Enabling Efficient Adaptation of Graph Language Models with Limited Annotations", "comment": null, "summary": "Large language models (LLMs) have demonstrated their strong capabilities in\nvarious domains, and have been recently integrated for graph analysis as graph\nlanguage models (GLMs). With LLMs as the predictor, some GLMs can interpret\nunseen tasks described by natural language, and learn from a few examples in\nthe prompts without parameter tuning, known as in-context learning (ICL).\nAnother subset of GLMs utilizes abundant training labels to enhance model\nperformance, known as instruction tuning. However, we argue that ICL on graphs\nhas effectiveness issues due to fixed parameters and efficiency issues due to\nlong context. Meanwhile, the large amount of labeled data required for\ninstruction tuning can be difficult to obtain in real-world scenarios. To this\nend, we aim to introduce an extra parameter adaptation stage that can\nefficiently tailor GLMs to an unseen graph and task with only a few labeled\nexamples, in exchange for better prediction accuracy and faster inference\nspeed. For implementation, in this paper we propose GraphLAMA method, with its\nmodel backbone and learning schemes specialized for efficient tuning and\ninference. Specifically, for model backbone, we use a graph neural network\n(GNN) with several well-designed components to transform nodes into the\nrepresentation space of LLM tokens. Task instructions can then be represented\nas a mixture of node and language tokens. In the pre-training stage, model\nparameters except the LLM will be trained with different tasks to capture\ngeneral knowledge. In the adaptation stage, only a few pre-trained parameters\nwill be updated based on few-shot examples. Extensive experiments on\nfew/zero-shot node classification and summary generation show that our proposed\nGraphLAMA achieves state-of-the-art performance with 4.91% absolution\nimprovement in accuracy. Compared with ICL, our inference speed can be 10 times\nfaster under 5-shot setting.", "AI": {"tldr": "The paper introduces GraphLAMA, a method using a GNN backbone and efficient adaptation mechanism for GLMs, offering state-of-the-art performance and faster inference on graph tasks with few labeled examples compared to existing ICL and instruction tuning approaches.", "motivation": "The authors address the limitations of in-context learning (ICL) and instruction tuning in the context of graph analysis, which include poor effectiveness, low efficiency, and the need for large labeled datasets. They aim to improve the adaptation of GLMs to unseen tasks with minimal labeled data.", "method": "The paper proposes GraphLAMA, a method designed to efficiently tailor GLMs to new graph tasks using few labeled examples. The method utilizes a GNN backbone to transform nodes into LLM token representation space, allowing task instructions to be encoded as a mix of node and language tokens. Training involves a pre-training phase on general tasks and an efficient adaptation phase using few-shot examples.", "result": "Experiments on few/zero-shot node classification and summary generation tasks demonstrate that GraphLAMA achieves state-of-the-art performance, with a 4.91% absolute improvement in accuracy. Compared to in-context learning, GraphLAMA offers a 10x speedup in inference under a 5-shot setting.", "conclusion": "The GraphLAMA method is shown to be effective in improving prediction accuracy and inference speed in graph tasks with few labeled examples. This makes it a viable solution for real-world scenarios where extensive labeled data is not available."}}
{"id": "2506.21681", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.21681", "abs": "https://arxiv.org/abs/2506.21681", "authors": ["Hakan Çapuk", "Andrew Bond", "Muhammed Burak Kızıl", "Emir Göçen", "Erkut Erdem", "Aykut Erdem"], "title": "TanDiT: Tangent-Plane Diffusion Transformer for High-Quality 360° Panorama Generation", "comment": null, "summary": "Recent advances in image generation have led to remarkable improvements in\nsynthesizing perspective images. However, these models still struggle with\npanoramic image generation due to unique challenges, including varying levels\nof geometric distortion and the requirement for seamless loop-consistency. To\naddress these issues while leveraging the strengths of the existing models, we\nintroduce TanDiT, a method that synthesizes panoramic scenes by generating\ngrids of tangent-plane images covering the entire 360$^\\circ$ view. Unlike\nprevious methods relying on multiple diffusion branches, TanDiT utilizes a\nunified diffusion model trained to produce these tangent-plane images\nsimultaneously within a single denoising iteration. Furthermore, we propose a\nmodel-agnostic post-processing step specifically designed to enhance global\ncoherence across the generated panoramas. To accurately assess panoramic image\nquality, we also present two specialized metrics, TangentIS and TangentFID, and\nprovide a comprehensive benchmark comprising captioned panoramic datasets and\nstandardized evaluation scripts. Extensive experiments demonstrate that our\nmethod generalizes effectively beyond its training data, robustly interprets\ndetailed and complex text prompts, and seamlessly integrates with various\ngenerative models to yield high-quality, diverse panoramic images.", "AI": {"tldr": "TanDiT提出了通过生成切平面图像来合成全景图像的新方法，并通过新的评估指标TangentIS和TangentFID，实现了高质量的全景图像生成。", "motivation": "当前的图像生成模型在处理全景图像生成时，面临着几何畸变变化和无缝循环一致性的独特挑战，而该方法旨在克服这些问题的同时，利用现有模型的优势。", "method": "一种名为TanDiT的方法被提出，用于通过生成覆盖整个360度视角的切平面图像网格来合成全景场景，不同于依赖多个扩散分支的先前方法，TanDiT使用单一统一扩散模型同时生成这些切平面图像。此外，作者提出了一种模型无关的后处理步骤旨在增强生成全景图像的全局一致性。", "result": "实验表明，该方法能够超越其训练数据泛化，准确解释详细的复杂文本提示，并与其他生成模型无缝集成，生成高质量多样的全景图像。", "conclusion": "引入了评估全景图像质量的两个专门指标TangentIS和TangentFID，以及包含标注过的全景数据集和标准化评估脚本的全面基准。"}}
