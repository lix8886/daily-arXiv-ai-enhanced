<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 5]
- [cs.CV](#cs.CV) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Talking to Yourself: Defying Forgetting in Large Language Models](https://arxiv.org/abs/2602.20162)
*Yutao Sun,Mingshuai Chen,Tiancheng Zhao,Phillip Miao,Zilun Zhang,Haozhan Shen,Ruizhe Zhu,Jianwei Yin*

Main category: cs.CL

> SA-SFT是一种不需要外部数据或额外调整的轻量级自我增强方法，它通过生成任务数据前的自我对话数据来缓解灾难性遗忘并提高领域内性能。

<details>
  <summary>Details</summary>

**Motivation:** 面临的挑战是，当对大型语言模型（LLMs）进行窄化和任务特定数据的微调时，灾难性遗忘成为一个重大的挑战，这通常会损害它们的通用知识和推理能力。

**Method:** 我们提出了SA-SFT，一种轻量级的自我增强流程。在这个过程中，LLM在微调之前生成自我对话语料，然后将这些自动生成的数据与任务数据混合，而不需要修改优化或训练计划。

**Result:** 在50种评估场景中，SA-SFT在40种情况下取得了最佳结果，超过了常见的基准方法，如层冻结和外部数据混合，且保持了与原始模型相当的性能。

**Conclusion:** 总的来说，我们的结果表明，自我增强提供了一种简单且有效的方法，可以实现大型语言模型的稳健适应，而不会导致灾难性遗忘。

**Abstract:** Catastrophic forgetting remains a major challenge when fine-tuning large language models (LLMs) on narrow, task-specific data, often degrading their general knowledge and reasoning abilities. We propose SA-SFT, a lightweight self-augmentation routine in which an LLM generates self-dialogues prior to fine-tuning, and the resulting self-authored data are mixed with task data without modifying optimization or training schedules.
  Despite requiring no external data or additional tuning, SA-SFT consistently mitigates catastrophic forgetting while improving in-domain performance. Across 50 evaluation scenarios, it maintains performance comparable to the original model and achieves the best results in 40 cases, outperforming common baselines such as layer freezing and external data mixing. Guided by these empirical findings, we further present a theoretical analysis suggesting that forgetting can partly stem from style-induced parameter drift, and that self-alignment through self-generated data provides an effective means to counteract this effect. Overall, our results indicate that self-augmentation offers a simple and effective mechanism for robust LLM adaptation without incurring catastrophic forgetting.

</details>


### [2] [Benchmarking Distilled Language Models: Performance and Efficiency in Resource-Constrained Settings](https://arxiv.org/abs/2602.20164)
*Sachin Gopal Wani,Eric Page,Ajay Dholakia,David Ellison*

Main category: cs.CL

> 本论文通过定量分析，验证了知识蒸馏技术在提高小型语言模型计算效率与性能方面的卓越性，认为其应被作为AI研发的首要策略。

<details>
  <summary>Details</summary>

**Motivation:** 研究动机在于评估知识蒸馏是否能有效提高小型语言模型在计算效率和性能方面的表现，使其更适合资源受限的环境。

**Method:** 本研究通过对比蒸馏模型、原生模型及专有模型在性能和计算成本方面的表现，对其进行定量分析。

**Result:** 研究发现，蒸馏产生了一个更加优异的性能-计算比率曲线，一个蒸馏8B模型比训练原生模型的计算效率高出2000倍以上。

**Conclusion:** 这些发现验证了知识蒸馏不仅是压缩技术，更是构建一流的、高访问性的AI系统的首要策略。

**Abstract:** Knowledge distillation offers a transformative pathway to developing powerful, yet efficient, small language models (SLMs) suitable for resource-constrained environments. In this paper, we benchmark the performance and computational cost of distilled models against their vanilla and proprietary counterparts, providing a quantitative analysis of their efficiency. Our results demonstrate that distillation creates a superior performance-tocompute curve. We find that creating a distilled 8B model is over 2,000 times more compute-efficient than training its vanilla counterpart, while achieving reasoning capabilities on par with, or even exceeding, standard models ten times its size. These findings validate distillation not just as a compression technique, but as a primary strategy for building state-of-the-art, accessible AI

</details>


### [3] [ConceptRM: The Quest to Mitigate Alert Fatigue through Consensus-Based Purity-Driven Data Cleaning for Reflection Modelling](https://arxiv.org/abs/2602.20166)
*Yongda Yu,Lei Zhang,Xinxin Guo,Minghui Yu,Zhengqi Zhuang,Guoping Rong,Haifeng Shen,Zhengfeng Li,Boge Wang,Guoan Zhang,Bangyu Xiang,Xiaobin Xu*

Main category: cs.CL

> 本文提出了一种名为ConceptRM的方法，旨在解决智能代理生成的大量警报（大多数是错误的）导致的疲劳问题。与现有方法相比，ConceptRM在最小化手动标注成本的同时，提高了拦截错误警报的准确性。

<details>
  <summary>Details</summary>

**Motivation:** 由于噪声数据的清理成本高昂，因此本研究旨在开发一种新的方法，以在最小的标注成本下改善虚假警报的拦截。

**Method:** ConceptRM 提出了一种新的方法，通过少量专家标注作为锚点，创建具有不同噪声比的扰动数据集，并利用协同教学训练多个不同的模型进行协作学习。通过分析这些模型的共识决策，有效识别来自噪声数据集中的可靠负样本。

**Result:** 实验结果表明，与几个最先进的LLM基线相比，ConceptRM在领域内数据集上最多可提高53.31%，在领域外数据集上最多可提高41.67%的拦截效果。

**Conclusion:** 总体而言，ConceptRM通过处理噪声数据集且只需少量标注，显著提高拦截虚假警报的效果。

**Abstract:** In many applications involving intelligent agents, the overwhelming volume of alerts (mostly false) generated by the agents may desensitize users and cause them to overlook critical issues, leading to the so-called ''alert fatigue''. A common strategy is to train a reflection model as a filter to intercept false alerts with labelled data collected from user verification feedback. However, a key challenge is the noisy nature of such data as it is often collected in production environments. As cleaning noise via manual annotation incurs high costs, this paper proposes a novel method ConceptRM for constructing a high-quality corpus to train a reflection model capable of effectively intercepting false alerts. With only a small amount of expert annotations as anchors, ConceptRM creates perturbed datasets with varying noise ratios and utilizes co-teaching to train multiple distinct models for collaborative learning. By analyzing the consensus decisions of these models, it effectively identifies reliable negative samples from a noisy dataset. Experimental results demonstrate that ConceptRM significantly enhances the interception of false alerts with minimal annotation cost, outperforming several state-of-the-art LLM baselines by up to 53.31% on in-domain datasets and 41.67% on out-of-domain datasets.

</details>


### [4] [InterviewSim: A Scalable Framework for Interview-Grounded Personality Simulation](https://arxiv.org/abs/2602.20294)
*Yu Li,Pranav Narayanan Venkit,Yada Pruksachatkun,Chien-Sheng Wu*

Main category: cs.CL

> 本文提出了一个基于真实采访数据的多维度评估框架，用于大规模个性模拟评估，并展示了其在评估和研究个性模拟中的优势。

<details>
  <summary>Details</summary>

**Motivation:** 现有的评估方法依赖于人口统计学调查、个性问卷或简短的人工智能面试作为代理，但缺乏对个体实际所说内容的直接评估。我们通过一个基于采访的大规模个性模拟评估框架来弥补这一空白。

**Method:** 我们提出一个基于真实采访数据的多维度评估框架，包括四个互补指标：内容相似度、事实一致性、个性一致性和知识保留度。

**Result:** 通过系统比较，我们证明了基于真实采访数据的方法显著优于仅依赖生物图形资料或模型参数知识的方法。此外，我们揭示了如何最好地利用采访数据的权衡：检索增强方法在捕捉个人风格和响应质量方面表现出色，而基于时间顺序的方法则在保持事实一致性和知识保留方面更胜一筹。

**Conclusion:** 我们的评估框架使得可以根据应用需求进行有原则的方法选择，而我们的实证发现为推进个性模拟研究提供了行动指南。

**Abstract:** Simulating real personalities with large language models requires grounding generation in authentic personal data. Existing evaluation approaches rely on demographic surveys, personality questionnaires, or short AI-led interviews as proxies, but lack direct assessment against what individuals actually said. We address this gap with an interview-grounded evaluation framework for personality simulation at a large scale. We extract over 671,000 question-answer pairs from 23,000 verified interview transcripts across 1,000 public personalities, each with an average of 11.5 hours of interview content. We propose a multi-dimensional evaluation framework with four complementary metrics measuring content similarity, factual consistency, personality alignment, and factual knowledge retention. Through systematic comparison, we demonstrate that methods grounded in real interview data substantially outperform those relying solely on biographical profiles or the model's parametric knowledge. We further reveal a trade-off in how interview data is best utilized: retrieval-augmented methods excel at capturing personality style and response quality, while chronological-based methods better preserve factual consistency and knowledge retention. Our evaluation framework enables principled method selection based on application requirements, and our empirical findings provide actionable insights for advancing personality simulation research.

</details>


### [5] [What Makes a Good Query? Measuring the Impact of Human-Confusing Linguistic Features on LLM Performance](https://arxiv.org/abs/2602.20300)
*William Watson,Nicole Cho,Sumitra Ganesh,Manuela Veloso*

Main category: cs.CL

> 本文研究表明，查询的形式可以影响LLM产生幻觉的可能性，发现了一些与幻觉风险相关的特征。

<details>
  <summary>Details</summary>

**Motivation:** 大多数情况下，大型语言模型（LLM）的幻觉被视作模型或其解码策略的缺陷。本文旨在通过经典语言学的角度来证明查询的形式可以塑造生成的回答。

**Method:** 通过构建一个包含子句复杂度、词汇稀有度、指示代词、否定、可答性以及意图基准等22维查询特征向量来对这一见解进行操作化处理。使用369,837个真实查询进行大规模分析，以揭示某些特征是否使幻觉更有可能发生。

**Result:** 研究发现了一些一致的“风险景观”：某些特征如深度子句嵌套和欠指定与更高的幻觉倾向有关，而清晰的意图基准和可答性则与较低的幻觉率有关。其他特征，包括领域特异性，则显示出混合影响，依赖于数据集和模型。

**Conclusion:** 研究结果建立了一个可观察到与幻觉风险相关的查询特征表示方式，为指导查询重写和未来的干预研究铺平了道路。

**Abstract:** Large Language Model (LLM) hallucinations are usually treated as defects of the model or its decoding strategy. Drawing on classical linguistics, we argue that a query's form can also shape a listener's (and model's) response. We operationalize this insight by constructing a 22-dimension query feature vector covering clause complexity, lexical rarity, and anaphora, negation, answerability, and intention grounding, all known to affect human comprehension. Using 369,837 real-world queries, we ask: Are there certain types of queries that make hallucination more likely? A large-scale analysis reveals a consistent "risk landscape": certain features such as deep clause nesting and underspecification align with higher hallucination propensity. In contrast, clear intention grounding and answerability align with lower hallucination rates. Others, including domain specificity, show mixed, dataset- and model-dependent effects. Thus, these findings establish an empirically observable query-feature representation correlated with hallucination risk, paving the way for guided query rewriting and future intervention studies.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [6] [VISION-ICE: Video-based Interpretation and Spatial Identification of Arrhythmia Origins via Neural Networks in Intracardiac Echocardiography](https://arxiv.org/abs/2602.20165)
*Dorsa EPMoghaddam,Feng Gao,Drew Bernard,Kavya Sinha,Mehdi Razavi,Behnaam Aazhang*

Main category: cs.CV

> This study introduces an AI-based framework that leverages ICE to improve the speed and accuracy of arrhythmia localization, achieving a 66.2% accuracy rate using deep learning techniques.

<details>
  <summary>Details</summary>

**Motivation:** High-density mapping techniques and traditional CT/MRI are time-consuming and resource-intensive for localizing arrhythmias. The study aims to provide a faster and more accurate localization approach by utilizing AI and ICE during electrophysiology treatments.

**Method:** Building an AI-enabled framework that uses intracardiac echocardiography (ICE) to identify areas of arrhythmias, formulated as a three-class classification task. A 3D Convolutional Neural Network was trained and tested using ten-fold cross-validation.

**Result:** The model achieved a mean accuracy of 66.2% on four previously unseen patients, which is significantly better than the 33.3% random baseline.

**Conclusion:** The study demonstrates the feasibility of using ICE videos and deep learning to automate arrhythmia localization, which could facilitate faster and more precise electrophysiological interventions. Further research will concentrate on enhancing model robustness and generalizability by expanding the dataset.

**Abstract:** Contemporary high-density mapping techniques and preoperative CT/MRI remain time and resource intensive in localizing arrhythmias. AI has been validated as a clinical decision aid in providing accurate, rapid real-time analysis of echocardiographic images. Building on this, we propose an AI-enabled framework that leverages intracardiac echocardiography (ICE), a routine part of electrophysiology procedures, to guide clinicians toward areas of arrhythmogenesis and potentially reduce procedural time. Arrhythmia source localization is formulated as a three-class classification task, distinguishing normal sinus rhythm, left-sided, and right-sided arrhythmias, based on ICE video data. We developed a 3D Convolutional Neural Network trained to discriminate among the three aforementioned classes. In ten-fold cross-validation, the model achieved a mean accuracy of 66.2% when evaluated on four previously unseen patients (substantially outperforming the 33.3% random baseline). These results demonstrate the feasibility and clinical promise of using ICE videos combined with deep learning for automated arrhythmia localization. Leveraging ICE imaging could enable faster, more targeted electrophysiological interventions and reduce the procedural burden of cardiac ablation. Future work will focus on expanding the dataset to improve model robustness and generalizability across diverse patient populations.

</details>
