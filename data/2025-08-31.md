<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 34]
- [cs.CV](#cs.CV) [Total: 30]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Social Bias in Multilingual Language Models: A Survey](https://arxiv.org/abs/2508.20201)
*Lance Calvin Lim Gamboa,Yue Feng,Mark Lee*

Main category: cs.CL

> 本文通过系统回顾发现多语言预训练模型存在社会偏见，提出了未来研究方向以增强文献的包容性、跨文化适宜性和技术先进性。

<details>
  <summary>Details</summary>

**Motivation:** 研究的动机在于分析在多语言和非英语环境下存在的偏见评估及缓解方法，发现研究领域的局限性，并为未来的多元语言偏见研究提供建议。

**Method:** 本文通过系统的文献回顾方法，分析了将偏见评估和缓解方法扩展到多种语言和非英语环境的研究。

**Result:** 研究揭示了该领域在一些建设性设计方案上的不足，包括偏好的语言选择、缺乏多语言缓解实验等，同时记录了跨语言和文化适配偏见基准时常见问题和解决方法。

**Conclusion:** 研究表明，预训练的多语言模型也表现出与处理英语文本的模型相同的社会偏见，并提出了未来研究的方向，以增强多语言偏见文献的包容性、跨文化适宜性和与最先进的NLP进展的一致性。

**Abstract:** Pretrained multilingual models exhibit the same social bias as models
processing English texts. This systematic review analyzes emerging research
that extends bias evaluation and mitigation approaches into multilingual and
non-English contexts. We examine these studies with respect to linguistic
diversity, cultural awareness, and their choice of evaluation metrics and
mitigation techniques. Our survey illuminates gaps in the field's dominant
methodological design choices (e.g., preference for certain languages, scarcity
of multilingual mitigation experiments) while cataloging common issues
encountered and solutions implemented in adapting bias benchmarks across
languages and cultures. Drawing from the implications of our findings, we chart
directions for future research that can reinforce the multilingual bias
literature's inclusivity, cross-cultural appropriateness, and alignment with
state-of-the-art NLP advancements.

</details>


### [2] [Prompting Strategies for Language Model-Based Item Generation in K-12 Education: Bridging the Gap Between Small and Large Language Models](https://arxiv.org/abs/2508.20217)
*Mohammad Amini,Babak Ahmadi,Xiaomeng Xiong,Yilin Zhang,Christopher Qiao*

Main category: cs.CL

> 研究通过对比模型和评估不同提示策略，旨在使用语言模型自动生成多项选择题，以减少人工开发成本和不一致性。

<details>
  <summary>Details</summary>

**Motivation:** 本研究动机是探索使用语言模型自动生成多项选择题（MCQs）用于形态评估，旨在减少人工开发测试的成本和不一致性。

**Method:** 本研究采用两阶段方法。首先，比较微调的中型模型（Gemma，2B）和未微调的大模型（GPT-3.5，175B）。其次，评估七种结构性提示策略，包括零样本、少样本、链式思维、角色扮演、顺序提示及其组合。

**Result:** 研究结果表明，结构性提示尤其是结合链式思维和顺序设计的策略显著提高了Gemma的输出质量。Gemma产生的项目通常比GPT-3.5的零样本响应更符合构建和教学要求，提示设计在中型模型的表现中起关键作用。

**Conclusion:** 本研究展示了结构性提示和有效的微调可以提高中型模型在数据有限条件下的自动生成性能。强调结合自动化指标、专家判断和大模型模拟的重要性，以确保与评估目标的一致性。提出的流程提供了一种实践且可扩展的方式来开发和验证K-12语言评估项目。

**Abstract:** This study explores automatic generation (AIG) using language models to
create multiple choice questions (MCQs) for morphological assessment, aiming to
reduce the cost and inconsistency of manual test development. The study used a
two-fold approach. First, we compared a fine-tuned medium model (Gemma, 2B)
with a larger untuned one (GPT-3.5, 175B). Second, we evaluated seven
structured prompting strategies, including zero-shot, few-shot,
chain-of-thought, role-based, sequential, and combinations. Generated items
were assessed using automated metrics and expert scoring across five
dimensions. We also used GPT-4.1, trained on expert-rated samples, to simulate
human scoring at scale. Results show that structured prompting, especially
strategies combining chain-of-thought and sequential design, significantly
improved Gemma's outputs. Gemma generally produced more construct-aligned and
instructionally appropriate items than GPT-3.5's zero-shot responses, with
prompt design playing a key role in mid-size model performance. This study
demonstrates that structured prompting and efficient fine-tuning can enhance
midsized models for AIG under limited data conditions. We highlight the value
of combining automated metrics, expert judgment, and large-model simulation to
ensure alignment with assessment goals. The proposed workflow offers a
practical and scalable way to develop and validate language assessment items
for K-12.

</details>


### [3] [Integrating SystemC TLM into FMI 3.0 Co-Simulations with an Open-Source Approach](https://arxiv.org/abs/2508.20223)
*Andrei Mihai Albu,Giovanni Pollo,Alessio Burrello,Daniele Jahier Pagliari,Cristian Tesconi,Alessandra Neri,Dario Soldi,Fabio Autieri,Sara Vinco*

Main category: cs.CL

> 本文讨论了将SystemC TLM模型集成到基于FMI的协同仿真中的方法，解决了集成中的技术挑战，并通过实例展示了该方法的有效性。

<details>
  <summary>Details</summary>

**Motivation:** 由于网络物理系统的复杂性不断增加，特别是在汽车应用中，有效建模和跨域协同仿真技术的需求也日益增长。尽管SystemC事务级建模（TLM）能够支持有效的硬件/软件协同设计，但其与其他工程领域模型的有限互操作性带来了集成挑战。

**Method:** 本文提出了一种完全开源的方法，将SystemC事务级建模（TLM）模型集成到基于功能模拟接口（FMI）的协同仿真工作流中。通过将SystemC TLM组件封装为FMI 3.0协作仿真功能模拟单元（FMU），该方法促进了在异构仿真环境中的无缝、标准化集成。

**Result:** 该研究介绍了一种轻量级的开源工具链，解决了关键的技术挑战，例如时间同步和数据交换，并通过代表性案例研究证明了该集成方法的可行性和有效性。

**Conclusion:** 该研究提出了一种全新的方法，不仅证明了SystemC TLM模型与现有基于FMI的协同仿真框架可以无缝集成，还提供了一个解决实际工程挑战的工具链。

**Abstract:** The growing complexity of cyber-physical systems, particularly in automotive
applications, has increased the demand for efficient modeling and cross-domain
co-simulation techniques. While SystemC Transaction-Level Modeling (TLM)
enables effective hardware/software co-design, its limited interoperability
with models from other engineering domains poses integration challenges. This
paper presents a fully open-source methodology for integrating SystemC TLM
models into Functional Mock-up Interface (FMI)-based co-simulation workflows.
By encapsulating SystemC TLM components as FMI 3.0 Co Simulation Functional
Mock-up Units (FMUs), the proposed approach facilitates seamless, standardized
integration across heterogeneous simulation environments. We introduce a
lightweight open-source toolchain, address key technical challenges such as
time synchronization and data exchange, and demonstrate the feasibility and
effectiveness of the integration through representative case studies.

</details>


### [4] [Can Compact Language Models Search Like Agents? Distillation-Guided Policy Optimization for Preserving Agentic RAG Capabilities](https://arxiv.org/abs/2508.20324)
*Rikuto Kotoge,Mai Nishimura,Jiaxin Ma*

Main category: cs.CL

> 为了克服紧凑型语言模型中推理能力的问题，提出了一种新的策略优化方法（DGPO），实验表明，DGPO使模型在资源受限条件下实现更高级的行为搜索表现。

<details>
  <summary>Details</summary>

**Motivation:** 研究动机是解决紧凑型语言模型（如：0.5B参数）在推理能力上的不足，导致奖励稀疏和训练不稳定的问题。

**Method:** 研究团队提出了Distillation-Guided Policy Optimization（DGPO）方法，通过冷启动初始化教师演示以及持续的教师指导来解决策略优化中的问题。

**Result:** 实验结果表明，DGPO使紧凑型模型能够实现复杂的行为搜索，即使在某些情况下，其表现超过了较大的教师模型。这意味着DGPO能够在计算资源受限的环境中实现机构化RAG。

**Conclusion:** 结论指出，DGPO有效地解决了小型语言模型在训练中的问题，使得即使在计算资源有限的情况下，也能实现高级的机构化RAG行为。

**Abstract:** Reinforcement Learning has emerged as a post-training approach to elicit
agentic RAG behaviors such as search and planning from language models.
However, compact language models (e.g., 0.5B parameters) struggle due to poor
reasoning ability, resulting in sparse rewards and unstable training. To
overcome these difficulties, we propose Distillation-Guided Policy Optimization
(DGPO), which addresses the challenges through cold-start initialization from
teacher demonstrations and continuous teacher guidance during policy
optimization. To systematically evaluate our approach, we introduce Agentic RAG
Capabilities (ARC), a fine-grained metric analyzing reasoning, search
coordination, and response synthesis. Comprehensive experiments demonstrate
that DGPO enables compact models to achieve sophisticated agentic search
behaviors, even outperforming the larger teacher model in some cases. DGPO
makes agentic RAG feasible in computing resource-constrained environments.

</details>


### [5] [GUARD: Guideline Upholding Test through Adaptive Role-play and Jailbreak Diagnostics for LLMs](https://arxiv.org/abs/2508.20325)
*Haibo Jin,Ruoxi Chen,Peiyan Zhang,Andy Zhou,Yang Zhang,Haohan Wang*

Main category: cs.CL

> 本文提出了一种名为GUARD的测试方法，旨在通过自动化生成判别问题来评估大语言模型对政府发布伦理准则的遵守情况，验证和提高语言模型的可靠性和安全性。

<details>
  <summary>Details</summary>

**Motivation:** 随着大语言模型在各个领域的广泛应用，其潜在生成有害内容的能力引发了社会和监管的广泛关注。虽然政府发布了伦理准则，但这些准则通常是对开发者和测试者的高层次要求，缺乏可操作的测试问题来验证大语言模型的合规性。

**Method:** 本文提出了一种名为GUARD的测试方法，用于将政府发布的伦理准则转化为具体的测试问题，以验证大语言模型是否遵循这些准则。GUARD通过自动化生成违反准则的问题来实现这一目标，并对回答进行评估。此外，GUARD引入了“越狱诊断”的概念，通过创建可能引发不道德或违规回答的场景来强化测试，这被命名为GUARD-JD。

**Result:** 该方法在七个大语言模型上进行了实证验证，测试了它们在三个政府发布的准则下的遵守情况，并进行了越狱诊断。结果显示了各模型的合规性，并指出可能存在的违规情况。

**Conclusion:** GUARD方法通过将其转化为具体的测试问题，有效地填补了这一空白，为检验大语言模型的合规性提供了工具，并展示了在视觉语言模型中的潜在应用。

**Abstract:** As Large Language Models become increasingly integral to various domains,
their potential to generate harmful responses has prompted significant societal
and regulatory concerns. In response, governments have issued ethics guidelines
to promote the development of trustworthy AI. However, these guidelines are
typically high-level demands for developers and testers, leaving a gap in
translating them into actionable testing questions to verify LLM compliance.
  To address this challenge, we introduce GUARD (\textbf{G}uideline
\textbf{U}pholding Test through \textbf{A}daptive \textbf{R}ole-play and
Jailbreak \textbf{D}iagnostics), a testing method designed to operationalize
guidelines into specific guideline-violating questions that assess LLM
adherence. To implement this, GUARD uses automated generation of
guideline-violating questions based on government-issued guidelines, thereby
testing whether responses comply with these guidelines. When responses directly
violate guidelines, GUARD reports inconsistencies. Furthermore, for responses
that do not directly violate guidelines, GUARD integrates the concept of
``jailbreaks'' to diagnostics, named GUARD-JD, which creates scenarios that
provoke unethical or guideline-violating responses, effectively identifying
potential scenarios that could bypass built-in safety mechanisms. Our method
finally culminates in a compliance report, delineating the extent of adherence
and highlighting any violations.
  We have empirically validated the effectiveness of GUARD on seven LLMs,
including Vicuna-13B, LongChat-7B, Llama2-7B, Llama-3-8B, GPT-3.5, GPT-4,
GPT-4o, and Claude-3.7, by testing compliance under three government-issued
guidelines and conducting jailbreak diagnostics. Additionally, GUARD-JD can
transfer jailbreak diagnostics to vision-language models, demonstrating its
usage in promoting reliable LLM-based applications.

</details>


### [6] [Joint Enhancement of Relational Reasoning for Long-Context LLMs](https://arxiv.org/abs/2508.20351)
*Zhirui Chen,Wei Shen,Jiashui Huang,Ling Shao*

Main category: cs.CL

> JERR框架采用策略性文本分块，构建DAG，使用MCTS处理复杂路径，以改进LLMs的长文本理解和推理精度，并提高了模型的透明度，实验中获得优异表现。

<details>
  <summary>Details</summary>

**Motivation:** 面对大语言模型（LLMs）在长文本上下文理解和复杂任务处理方面的局限性，JERR框架旨在通过图推理增强LLMs的长文本处理能力，提高透明度并减少幻觉问题。

**Method:** JERR框架整合了三个关键组件：摘要提取，图构建，和关系推理。首先，通过策略性分块文本以总结和理解信息。其次，构建有向无环图(DAG)来解决冗余问题，确保逻辑一致性和清晰度。最后，使用蒙特卡洛树搜索(MCTS)帮助模型处理复杂的推理路径，确保更准确和可解释的结果。

**Result:** 实验结果表明，JERR在ROUGE和F1评估指标上超越了所有基线，在LLM-Rater评价中得到了最高分。

**Conclusion:** JERR框架为解决大语言模型在长文本理解和复杂推理任务上的问题提供了新颖的方法，通过增强LLMs在这些任务中的处理能力，提升了其可靠性和透明度。

**Abstract:** Despite significant progress, large language models (LLMs) still struggle
with long contexts due to memory limitations and their inability to tackle
complex and long-context tasks. Additionally, LLMs often suffer from a lack of
transparency and are prone to producing hallucinations. To address these
challenges, we propose \textbf{JERR}, a novel framework designed to enhance
long-context comprehension via graph-based reasoning in LLMs. JERR integrates
three key components: synopsis extraction, graph construction, and relational
reasoning. First, synopsis is extracted by chunking text strategically,
allowing the model to summarize and understand information more efficiently.
Second, we build a directed acyclic graph (DAG) to resolve redundancy, ensuring
logical consistency and clarity. Finally, we incorporate Monte Carlo Tree
Search (MCTS) to help the model navigate complex reasoning paths, ensuring more
accurate and interpretable outputs. This framework provides a novel solution
that enables LLMs to handle extended contexts and complex reasoning tasks with
improved reliability and transparency. Experimental results show that JERR
consistently outperforms all baselines on the ROUGE and F1 metrics, achieving
the highest scores on the LLM-Rater evaluation.

</details>


### [7] [Graph-R1: Unleashing LLM Reasoning with NP-Hard Graph Problems](https://arxiv.org/abs/2508.20373)
*Yuyao Wang,Bowen Liu,Jianheng Tang,Nuo Chen,Yuhan Li,Qifan Zhang,Jia Li*

Main category: cs.CL

> 该研究提出了一种基于NP-hard图问题的新颖合成训练语料库，用于开发大型语言模型的长链式推理能力，通过两阶段后训练框架显著增强了推理深度与效率。模型Graph-R1-7B在多个领域展示了强大的泛化能力。

<details>
  <summary>Details</summary>

**Motivation:** 解决现有大型语言模型在长链式推理行为上所需的高质量数据集成本高昂且人工编制的问题，探索具有可扩展性的替代方案。

**Method:** 采用NP-hard图问题作为训练语料库，设计了包括细粒度奖励设计的强化学习在内的两阶段长链式推理监督微调框架。

**Result:** 旗舰模型Graph-R1-7B在数学、编程、STEM、逻辑推理等多个领域展示了较强的泛化能力，并且在NP-hard图问题上比QwQ-32B模型更具准确性与推理效率。

**Conclusion:** NP-hard图问题作为合成训练语料库是提升大型语言模型长链式推理能力的一个有效且可扩展的方法，为大型语言模型训练开创了新的领域。

**Abstract:** Reasoning Large Language Models (RLLMs) have recently achieved remarkable
progress on complex reasoning tasks, largely enabled by their long
chain-of-thought (Long CoT) capabilities. However, developing these Long CoT
behaviors relies heavily on post-training with high-quality datasets, which are
typically costly and human-curated (e.g., mathematics and code), leaving
scalable alternatives unexplored. In this work, we introduce NP-hard (NPH)
graph problems as a novel synthetic training corpus, as they inherently require
deep reasoning, extensive exploration, and reflective strategies, which are
core characteristics of Long CoT reasoning. Building on this insight, we
develop a two-stage post-training framework: (i) Long CoT Supervised
Fine-Tuning (SFT) on rejection-sampled NPH graph instances, which substantially
enhances reasoning depth, and (ii) Reinforcement Learning (RL) with a
fine-grained reward design, which sharpens reasoning efficiency. Our flagship
model, Graph-R1-7B, demonstrates strong generalization across mathematics,
coding, STEM, and logic, and surpasses QwQ-32B on NPH graph problems in both
accuracy and reasoning efficiency. These results position NPH graph problems as
an effective and scalable resource for advancing Long CoT reasoning in LLMs,
opening a new frontier for LLM post-training. Our implementation is available
at https://github.com/Graph-Reasoner/Graph-R1, with models and datasets hosted
in our Hugging Face collection HKUST-DSAIL/Graph-R1.

</details>


### [8] [CAPE: Context-Aware Personality Evaluation Framework for Large Language Models](https://arxiv.org/abs/2508.20385)
*Jivnesh Sandhan,Fei Cheng,Tushar Sandhan,Yugo Murawaki*

Main category: cs.CL

> 本文提出了首个上下文感知性格评估框架（CAPE），用于评估大规模语言模型的一致性。实验表明，在对话历史的基础上，一些模型在响应一致性上表现出显著的提升，但同时也产生了性格的变化。

<details>
  <summary>Details</summary>

**Motivation:** 尽管传统的心理测量测试被应用于评估大规模语言模型的行为特征，现有的研究都采用一种无上下文的方法单独回答每个问题以避免上下文影响。这忽略了现实世界应用中对话历史对响应的影响。本文提出了一种名为CAPE的框架以弥补这一差距。

**Method:** 我们提出了首个用于大规模语言模型的上下文感知性格评估（CAPE）框架，该框架将之前的对话互动纳入考量。为了全面分析上下文的影响，我们引入了新的度量标准来量化LLM响应的一致性，这是人类行为的基本特征。

**Result:** 在7个大规模语言模型上的详尽实验显示，对话历史通过上下文学习增强了响应的一致性但也带来了性格转变。GPT-3.5-Turbo和GPT-4-Turbo显示出极端的偏差。GPT模型响应来自其内在性格特征和先前的互动，而Gemini-1.5-Flash和Llama-8B则严重依赖于之前的互动。

**Conclusion:** 将提出的框架应用于角色扮演代理（RPAs）显示，依赖上下文的性格变化提升了响应的一致性，并且与人类的判断更趋一致。本文研究结果表明，上下文因素在评估LLM性格特征中的重要性。代码和数据集均可在https://github.com/jivnesh/CAPE上公开获取。

**Abstract:** Psychometric tests, traditionally used to assess humans, are now being
applied to Large Language Models (LLMs) to evaluate their behavioral traits.
However, existing studies follow a context-free approach, answering each
question in isolation to avoid contextual influence. We term this the Disney
World test, an artificial setting that ignores real-world applications, where
conversational history shapes responses. To bridge this gap, we propose the
first Context-Aware Personality Evaluation (CAPE) framework for LLMs,
incorporating prior conversational interactions. To thoroughly analyze the
influence of context, we introduce novel metrics to quantify the consistency of
LLM responses, a fundamental trait in human behavior.
  Our exhaustive experiments on 7 LLMs reveal that conversational history
enhances response consistency via in-context learning but also induces
personality shifts, with GPT-3.5-Turbo and GPT-4-Turbo exhibiting extreme
deviations. While GPT models are robust to question ordering, Gemini-1.5-Flash
and Llama-8B display significant sensitivity. Moreover, GPT models response
stem from their intrinsic personality traits as well as prior interactions,
whereas Gemini-1.5-Flash and Llama--8B heavily depend on prior interactions.
Finally, applying our framework to Role Playing Agents (RPAs) shows
context-dependent personality shifts improve response consistency and better
align with human judgments. Our code and datasets are publicly available at:
https://github.com/jivnesh/CAPE

</details>


### [9] [Measuring Reasoning Utility in LLMs via Conditional Entropy Reduction](https://arxiv.org/abs/2508.20395)
*Xu Guo*

Main category: cs.CL

> 本文针对如何通过预测推理实用性来增强语言模型正确性进行了探究，发现条件熵减少与正确答案相关，而错误答案往往伴随条件熵保持不变或增加，此外还不正确的推理路径通常更长。

<details>
  <summary>Details</summary>

**Motivation:** 本文试图研究推理实用性如何贡献于最终答案的正确性。这是因为自回归生成的随机性质使得生成更多上下文并不一定能增加答案的信心。如果能在生成过程中预测推理步骤是否有用，就能提前停止或剪枝无效步骤，从而避免对最终决策的干扰。

**Method:** 我们使用MATH数据集进行研究，通过Qwen2.5-32B和GPT-4o生成推理链，并使用单独的模型Qwen3-8B来量化这些推理链对最终准确性的有用性。具体来说，我们在每一步推理过程中使用条件熵（词汇表上期望的负对数似然）测量模型对答案片段Y的不确定性。

**Result:** 研究结果表明：条件熵在推理步骤中减少与正确答案存在显著相关性；而条件熵保持平缓或增加则往往导致错误答案。同时发现，不正确的推理路径往往比正确的推理路径更长，这表明更长的推理并不一定会产生更好的结果。

**Conclusion:** 这些发现为未来设计能够检测并避免无效推理的高效推理流水线奠定了基础。

**Abstract:** Recent advancements in large language models (LLMs) often rely on generating
intermediate reasoning steps to enhance accuracy. However, little work has
examined how reasoning utility contributes to the final answer's correctness.
Due to the stochastic nature of autoregressive generation, generating more
context does not guarantee increased confidence in the answer. If we could
predict, during generation, whether a reasoning step will be useful, we could
stop early or prune ineffective steps, avoiding distractions in the final
decision.
  We present an oracle study on MATH dataset, using Qwen2.5-32B and GPT-4o to
generate reasoning chains, and then employing a separate model (Qwen3-8B) to
quantify the utility of these chains for final accuracy. Specifically, we
measure the model's uncertainty on the answer span Y at each reasoning step
using conditional entropy (expected negative log-likelihood over the
vocabulary) with context expanding step by step. Our results show a clear
pattern: conditional entropy that decreases over steps is strongly associated
with correct answers, whereas flat or increasing entropy often results in wrong
answers. We also corroborate that incorrect reasoning paths tend to be longer
than correct ones, suggesting that longer reasoning does not necessarily yield
better outcomes. These findings serve as a foundation to inspire future work on
designing efficient reasoning pipelines that detect and avoid unproductive
reasoning early.

</details>


### [10] [UI-Bench: A Benchmark for Evaluating Design Capabilities of AI Text-to-App Tools](https://arxiv.org/abs/2508.20410)
*Sam Jung,Agustin Garcinuno,Spencer Mateega*

Main category: cs.CL

> 开发了UI-Bench作为首个大规模基准测试，用于通过专家配对比较的方式评估十个AI文本转应用工具的视觉表现。

<details>
  <summary>Details</summary>

**Motivation:** 目前没有公开的基准测试来严谨验证AI文本转应用工具的质量声明，因此作者开发了UI-Bench。

**Method:** 引入UI-Bench，这是一个大规模基准测试，通过专家配对比较评估了十个AI文本转应用工具在视觉卓越性方面的表现。

**Result:** UI-Bench 覆盖了十个工具，三十个提示，三百个生成的网站，和4000多个专家判断，它对系统进行了真正的技能模型排名，提供了校准过的置信区间。

**Conclusion:** UI-Bench 建立了一个可重现的标准来推动AI驱动的网页设计的进步，并公开了提示集，开源评估框架，和公共排行榜。

**Abstract:** AI text-to-app tools promise high quality applications and websites in
minutes, yet no public benchmark rigorously verifies those claims. We introduce
UI-Bench, the first large-scale benchmark that evaluates visual excellence
across competing AI text-to-app tools through expert pairwise comparison.
Spanning 10 tools, 30 prompts, 300 generated sites, and \textit{4000+} expert
judgments, UI-Bench ranks systems with a TrueSkill-derived model that yields
calibrated confidence intervals. UI-Bench establishes a reproducible standard
for advancing AI-driven web design. We release (i) the complete prompt set,
(ii) an open-source evaluation framework, and (iii) a public leaderboard. The
generated sites rated by participants will be released soon. View the UI-Bench
leaderboard at https://uibench.ai/leaderboard.

</details>


### [11] [DentalBench: Benchmarking and Advancing LLMs Capability for Bilingual Dentistry Understanding](https://arxiv.org/abs/2508.20416)
*Hengchuan Zhu,Yihuan Xu,Yichen Li,Zijie Meng,Zuozhu Liu*

Main category: cs.CL

> 该研究介绍了DentalBench，一个专门用于牙科领域的双语评估基准，评估了多种LLM在牙科问答任务中的表现，并强调了领域特定模型的重要性。

<details>
  <summary>Details</summary>

**Motivation:** 尽管大型语言模型(Large Language Models, LLMs)及其医疗领域的应用（Med-LLMs）在一般医疗基准测试中表现良好，但它们在特定医疗领域的表现，比如牙科，需要更深层次的专业知识，由于缺乏针对这些领域的评估资源，相关研究尚待探索。因此，本文旨在开发一个专门评估这些模型在牙科领域应用能力的基准。

**Method:** 本文介绍了DentalBench，这是一个用于评估和改进语言模型在牙科领域表现的综合双语基准。DentalBench包含两个主要组件：DentalQA，一个包含36,597个问题的英语-中文问答基准，涵盖了4项任务和16个牙科子领域；以及DentalCorpus，一个有3.37亿个令牌的大规模高质量语料库，用于支持监督微调和检索增强生成。

**Result:** 本研究评估了14种模型，包括专有、开源和专门为医疗设计的模型，并发现了任务类型和语言间存在显著的性能差异。使用Qwen-2.5-3B模型进行的实验表明，领域适应显著提高了模型在知识密集型和术语集中的任务中的性能。

**Conclusion:** 该研究强调了需要有领域特定的基准来开发可靠且有效的特定于医疗应用的LLMs的重要性。

**Abstract:** Recent advances in large language models (LLMs) and medical LLMs (Med-LLMs)
have demonstrated strong performance on general medical benchmarks. However,
their capabilities in specialized medical fields, such as dentistry which
require deeper domain-specific knowledge, remain underexplored due to the lack
of targeted evaluation resources. In this paper, we introduce DentalBench, the
first comprehensive bilingual benchmark designed to evaluate and advance LLMs
in the dental domain. DentalBench consists of two main components: DentalQA, an
English-Chinese question-answering (QA) benchmark with 36,597 questions
spanning 4 tasks and 16 dental subfields; and DentalCorpus, a large-scale,
high-quality corpus with 337.35 million tokens curated for dental domain
adaptation, supporting both supervised fine-tuning (SFT) and
retrieval-augmented generation (RAG). We evaluate 14 LLMs, covering
proprietary, open-source, and medical-specific models, and reveal significant
performance gaps across task types and languages. Further experiments with
Qwen-2.5-3B demonstrate that domain adaptation substantially improves model
performance, particularly on knowledge-intensive and terminology-focused tasks,
and highlight the importance of domain-specific benchmarks for developing
trustworthy and effective LLMs tailored to healthcare applications.

</details>


### [12] [KG-CQR: Leveraging Structured Relation Representations in Knowledge Graphs for Contextual Query Retrieval](https://arxiv.org/abs/2508.20417)
*Chi Minh Bui,Ngoc Mai Thieu,Van Vinh Nguyen,Json J. Jung,Khac-Hoai Nam Bui*

Main category: cs.CL

> The study introduces KG-CQR, a new retrieval framework for RAG systems that integrates knowledge graphs with large language models to enhance query context, yielding significant improvements in retrieval performance on various datasets without additional model training.

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to enhance the retrieval-augmented generation (RAG) systems by integrating knowledge graphs (KGs) with large language models (LLMs). This is done to better address the issue of context loss seen in existing methods and to achieve improvements in retrieval effectiveness, especially for complex and multi-hop tasks.

**Method:** The method involves a novel framework named KG-CQR for Contextual Query Retrieval (CQR). This framework improves the retrieval phase by enriching query context through a structured relation representations approach. It extracts and completes relevant subgraphs from a corpus-centric KG and incorporates these into the query context without requiring additional training for varying sizes of large language models.

**Result:** The results show that KG-CQR provides superior performance, with a 4-6% improvement in mAP and a 2-3% improvement in Recall@25 over strong baseline models. This improvement is demonstrated through experiments on RAGBench and MultiHop-RAG datasets.

**Conclusion:** The conclusion is that by integrating knowledge graphs into the retrieval phase of RAG systems through the KG-CQR framework, significant improvements in retrieval effectiveness can be achieved without the need for additional training, and that this method outperforms existing baselines, especially for multi-hop question answering tasks.

**Abstract:** The integration of knowledge graphs (KGs) with large language models (LLMs)
offers significant potential to improve the retrieval phase of
retrieval-augmented generation (RAG) systems. In this study, we propose KG-CQR,
a novel framework for Contextual Query Retrieval (CQR) that enhances the
retrieval phase by enriching the contextual representation of complex input
queries using a corpus-centric KG. Unlike existing methods that primarily
address corpus-level context loss, KG-CQR focuses on query enrichment through
structured relation representations, extracting and completing relevant KG
subgraphs to generate semantically rich query contexts. Comprising subgraph
extraction, completion, and contextual generation modules, KG-CQR operates as a
model-agnostic pipeline, ensuring scalability across LLMs of varying sizes
without additional training. Experimental results on RAGBench and MultiHop-RAG
datasets demonstrate KG-CQR's superior performance, achieving a 4-6%
improvement in mAP and a 2-3% improvement in Recall@25 over strong baseline
models. Furthermore, evaluations on challenging RAG tasks such as multi-hop
question answering show that, by incorporating KG-CQR, the performance
consistently outperforms the existing baseline in terms of retrieval
effectiveness

</details>


### [13] [CAMB: A comprehensive industrial LLM benchmark on civil aviation maintenance](https://arxiv.org/abs/2508.20420)
*Feng Zhang,Chengjie Pang,Yuehan Zhang,Chenyu Luo*

Main category: cs.CL

> 本文提出了一项针对民用航空维修领域的大规模语言模型（LLMs）的专业评估工具，旨在填补当前评估工具的空白，并提供改进方向，从而推动该领域的智能化解决方案发展。实验表明，该基准测试在评估模型性能方面有效，并且开源以促进进一步的研究和开发。

<details>
  <summary>Details</summary>

**Motivation:** 当前LLMs的评估主要集中在数学和编程推理任务上，缺乏针对民用航空维修这样一个专业领域进行专门评估的工具，该领域需要复杂的推理和专业知识。本文旨在填补这一空白。

**Method:** 本文开发了一个专为民用航空维修领域设计的工业级基准。该基准用于测量LLMs的能力，识别领域知识和复杂推理方面的缺陷，并通过实验探索和分析，评估现有著名矢量嵌入模型和LLMs在民用航空维修情境中的表现。

**Result:** 实验结果展示了该基准在评估民用航空维修领域模型性能方面的有效性，并且开源了该评估基准和代码，以促进进一步的研究和发展。

**Conclusion:** 本文提出的评估基准填补了民用航空维修领域中LLMs评估工具的空白，并为有针对性的改进提供了基础，比如领域特定的微调、RAG优化或专门的提示工程。开源工作也将推动该领域的进一步研究和智能化解决方案的发展。

**Abstract:** Civil aviation maintenance is a domain characterized by stringent industry
standards. Within this field, maintenance procedures and troubleshooting
represent critical, knowledge-intensive tasks that require sophisticated
reasoning. To address the lack of specialized evaluation tools for large
language models (LLMs) in this vertical, we propose and develop an
industrial-grade benchmark specifically designed for civil aviation
maintenance. This benchmark serves a dual purpose: It provides a standardized
tool to measure LLM capabilities within civil aviation maintenance, identifying
specific gaps in domain knowledge and complex reasoning. By pinpointing these
deficiencies, the benchmark establishes a foundation for targeted improvement
efforts (e.g., domain-specific fine-tuning, RAG optimization, or specialized
prompt engineering), ultimately facilitating progress toward more intelligent
solutions within civil aviation maintenance. Our work addresses a significant
gap in the current LLM evaluation, which primarily focuses on mathematical and
coding reasoning tasks. In addition, given that Retrieval-Augmented Generation
(RAG) systems are currently the dominant solutions in practical applications ,
we leverage this benchmark to evaluate existing well-known vector embedding
models and LLMs for civil aviation maintenance scenarios. Through experimental
exploration and analysis, we demonstrate the effectiveness of our benchmark in
assessing model performance within this domain, and we open-source this
evaluation benchmark and code to foster further research and
development:https://github.com/CamBenchmark/cambenchmark

</details>


### [14] [Searching the Title of Practical Work of the Informatics Engineering Bachelor Program with the Case Base Reasoning Method](https://arxiv.org/abs/2508.20442)
*Agung Sukrisna Jaya,Osvari Arsalan,Danny Matthew Saputra*

Main category: cs.CL

> 本文通过CBR技术结合TF-IDF和余弦相似度算法实现实际工作标题的高效搜索，测试表明该系统在准确性和可靠性方面表现出色。

<details>
  <summary>Details</summary>

**Motivation:** 该研究旨在利用CBR技术，通过处理和搜索实际工作标题，为用户提供一个高效且准确的检索系统。

**Method:** 该论文采用案例推理(CBR)技术，结合TF-IDF和余弦相似度算法来为实际工作标题进行搜索。TF-IDF用于处理每个实际工作标题单词的向量化，余弦相似度用于计算相似性值。

**Result:** 测试结果显示，在使用705个实际工作标题进行的测试中，通过两个阶段的测试（第一阶段使用已有的标题搜索，第二阶段随机化第一阶段的标题），系统在第二阶段找到了同样数量的标题并且获得了最高的平均匹配得分。

**Conclusion:** 研究表明，基于CBR、TF-IDF和余弦相似度的系统能够有效地实现实际工作标题的搜索任务，并根据测试结果表明系统具有较高的准确性和可靠性。

**Abstract:** Case Base Reasoning (CBR) is a case solving technique based on experience in
cases that have occurred before with the highest similarity. CBR is used to
search for practical work titles. TF-IDF is applied to process the
vectorization of each practical work title word and Cosine Similarity for the
calculation of similarity values. This system can search either in the form of
titles or keywords. The output of the system is the title of practical work and
the match value of each title. Based on the test results using 705 practical
work titles, testing was carried out with five titles and carried out in two
stages. The first stage searches with existing titles and the second stage
randomizes the title from the first stage. And the results obtained in the
second stage are the same number of titles found and the highest average match
score.

</details>


### [15] [MCP-Bench: Benchmarking Tool-Using LLM Agents with Complex Real-World Tasks via MCP Servers](https://arxiv.org/abs/2508.20453)
*Zhenting Wang,Qi Chang,Hemani Patel,Shashank Biju,Cheng-En Wu,Quan Liu,Aolin Ding,Alireza Rezazadeh,Ankit Shah,Yujia Bao,Eugene Siow*

Main category: cs.CL

> 本文提出了MCP-Bench，一个用于评测LLMs在多步骤任务上的表现的基准，在这些任务中，模型需要使用组合工具进行复杂任务，而不仅是依赖具体的多步操作和以往孤立领域的操作。通过这种方式，可以更真实地评估模型的能力。

<details>
  <summary>Details</summary>

**Motivation:** 与其他基于API的基准测试不同，MCP-Bench让每个MCP服务器提供一组互补工具，这些工具设计用于协同工作，从而能够构建具有丰富输入输出耦合的多步骤真实任务。

**Method:** 我们介绍了MCP-Bench，这是一个用于评估大型语言模型（LLMs）在涉及工具使用、跨工具协调、精确参数控制和规划/推理的多步骤现实任务上的基准。MCP-Bench基于模型上下文协议（MCP），将LLMs与28个具有代表性的MCP服务器连接，这些服务器包含来自金融、旅行、科学计算和学术搜索等领域的250种工具。

**Result:** 实验在20个先进LLMs上进行，显示了在MCP-Bench上存在的持续挑战。

**Conclusion:** 这项研究提出了一种多方面的评估框架，涵盖了工具层面的模式理解和使用、轨迹层面的规划以及任务完成情况。

**Abstract:** We introduce MCP-Bench, a benchmark for evaluating large language models
(LLMs) on realistic, multi-step tasks that demand tool use, cross-tool
coordination, precise parameter control, and planning/reasoning for solving
tasks. Built on the Model Context Protocol (MCP), MCP-Bench connects LLMs to 28
representative live MCP servers spanning 250 tools across domains such as
finance, traveling, scientific computing, and academic search. Unlike prior
API-based benchmarks, each MCP server provides a set of complementary tools
designed to work together, enabling the construction of authentic, multi-step
tasks with rich input-output coupling. Tasks in MCP-Bench test agents' ability
to retrieve relevant tools from fuzzy instructions without explicit tool names,
plan multi-hop execution trajectories for complex objectives, ground responses
in intermediate tool outputs, and orchestrate cross-domain workflows -
capabilities not adequately evaluated by existing benchmarks that rely on
explicit tool specifications, shallow few-step workflows, and isolated domain
operations. We propose a multi-faceted evaluation framework covering tool-level
schema understanding and usage, trajectory-level planning, and task completion.
Experiments on 20 advanced LLMs reveal persistent challenges in MCP-Bench. Code
and data: https://github.com/Accenture/mcp-bench.

</details>


### [16] [Prediction of mortality and resource utilization in critical care: a deep learning approach using multimodal electronic health records with natural language processing techniques](https://arxiv.org/abs/2508.20460)
*Yucheng Ruan,Xiang Lan,Daniel J. Tan,Hairil Rizal Abdullah,Mengling Feng*

Main category: cs.CL

> 研究开发了一种通过深度学习和自然语言处理技术整合多模态电子健康记录来预测重症监护患者死亡率和资源利用的新方法，在真实数据集上的实验表现优于现有方法。

<details>
  <summary>Details</summary>

**Motivation:** 预测死亡率和医疗资源利用率对于提高患者安全和管理成本至关重要，但通过电子健康记录（尤其是自由文本记录）进行预测极具挑战性。现有方法大多集中在结构化的电子健康记录上，忽视了文本中的关键临床洞察。这项研究旨在引入和评估一个利用自然语言处理技术的深度学习框架，整合多模态EHR进行重症监护环境下的死亡率和资源利用预测。

**Method:** 研究采用了一个结合自然语言处理技术的深度学习框架，该框架将多模态电子健康记录（EHR）用于预测重症监护环境中的死亡率和资源利用率。使用两个真实世界的EHR数据集，在三个临床任务上评估了与现有领先方法相比的表现，并对框架中的三个关键组件进行了消融研究。

**Result:** 实验结果显示，研究提出的模型在两个真实数据集上的三种临床任务上分别将死亡率预测的BACC和AUROC提高了1.6%和0.8%，资源使用长度预测（LOS）的RMSE和MAE提高了0.5%和2.2%，手术持续时间估计的RMSE和MAE提高了10.9%和11.0%。模型在不同的数据损坏率下均优于其他基线。

**Conclusion:** 研究提出的框架是一种在重症监护环境中预测死亡率和资源利用率时既有效又准确的深度学习方法。使用提示学习和变压器编码器分析多模态EHR取得成功，值得进一步研究。此外，该模型显示出对结构化数据中高损坏率的强健性。

**Abstract:** Background Predicting mortality and resource utilization from electronic
health records (EHRs) is challenging yet crucial for optimizing patient
outcomes and managing costs in intensive care unit (ICU). Existing approaches
predominantly focus on structured EHRs, often ignoring the valuable clinical
insights in free-text notes. Additionally, the potential of textual information
within structured data is not fully leveraged. This study aimed to introduce
and assess a deep learning framework using natural language processing
techniques that integrates multimodal EHRs to predict mortality and resource
utilization in critical care settings. Methods Utilizing two real-world EHR
datasets, we developed and evaluated our model on three clinical tasks with
leading existing methods. We also performed an ablation study on three key
components in our framework: medical prompts, free-texts, and pre-trained
sentence encoder. Furthermore, we assessed the model's robustness against the
corruption in structured EHRs. Results Our experiments on two real-world
datasets across three clinical tasks showed that our proposed model improved
performance metrics by 1.6\%/0.8\% on BACC/AUROC for mortality prediction,
0.5%/2.2% on RMSE/MAE for LOS prediction, 10.9%/11.0% on RMSE/MAE for surgical
duration estimation compared to the best existing methods. It consistently
demonstrated superior performance compared to other baselines across three
tasks at different corruption rates. Conclusions The proposed framework is an
effective and accurate deep learning approach for predicting mortality and
resource utilization in critical care. The study also highlights the success of
using prompt learning with a transformer encoder in analyzing multimodal EHRs.
Importantly, the model showed strong resilience to data corruption within
structured data, especially at high corruption levels.

</details>


### [17] [ConspirED: A Dataset for Cognitive Traits of Conspiracy Theories and Large Language Model Safety](https://arxiv.org/abs/2508.20468)
*Luke Bates,Max Glockner,Preslav Nakov,Iryna Gurevych*

Main category: cs.CL

> 本研究创建了ConspirED数据集，用于标注和分析阴谋论内容中的认知特征，并通过该数据集开发了识别模型和评估了LLM/LRM对阴谋论输入的反应，发现其易被影响。

<details>
  <summary>Details</summary>

**Motivation:** 阴谋论侵蚀公众对科学和机构的信任，且难以被揭穿，随着AI生成的虚假信息变得更加复杂，理解决议性内容中的修辞模式对于开发针对性事前提醒、评估人工智能脆弱性的干预措施至关重要。

**Method:** 本研究引入了ConspirED评估数据集，它捕捉了在线阴谋论文章中多句摘录（80-120字）的阴谋论思维认知特征，并使用CONSPIR认知框架进行标注。该数据集是首个标注了一般认知特征的阴谋论内容数据集。研究者利用ConspirED开发了识别阴谋论特征和判断文本摘录中主导特征的计算模型，并评估了大型语言/推理模型（LLM/LRM）对阴谋论输入的鲁棒性。

**Result:** 研究结果显示，对于阴谋论内容，计算模型能够识别和判定文本中的阴谋论思维特征，同时大语言/推理模型显示出对阴谋论输入的反应模式与输入相似，即使在能够成功拦截类似的事实核查类不实信息时也会受到误导。

**Conclusion:** 通过ConspirED数据集，研究揭示了阴谋论内容对计算模型识别能力和大语言/推理模型输出的显著影响，这为理解和干预阴谋论传播提供了新的视角。

**Abstract:** Conspiracy theories erode public trust in science and institutions while
resisting debunking by evolving and absorbing counter-evidence. As AI-generated
misinformation becomes increasingly sophisticated, understanding rhetorical
patterns in conspiratorial content is important for developing interventions
such as targeted prebunking and assessing AI vulnerabilities. We introduce
ConspirED (CONSPIR Evaluation Dataset), which captures the cognitive traits of
conspiratorial ideation in multi-sentence excerpts (80--120 words) from online
conspiracy articles, annotated using the CONSPIR cognitive framework
(Lewandowsky and Cook, 2020). ConspirED is the first dataset of conspiratorial
content annotated for general cognitive traits. Using ConspirED, we (i) develop
computational models that identify conspiratorial traits and determine dominant
traits in text excerpts, and (ii) evaluate large language/reasoning model
(LLM/LRM) robustness to conspiratorial inputs. We find that both are misaligned
by conspiratorial content, producing output that mirrors input reasoning
patterns, even when successfully deflecting comparable fact-checked
misinformation.

</details>


### [18] [Languages Still Left Behind: Toward a Better Multilingual Machine Translation Benchmark](https://arxiv.org/abs/2508.20511)
*Chihiro Taguchi,Seng Mai,Keita Kurabe,Yusuke Sakai,Georgina Agyei,Soudabeh Eslami,David Chiang*

Main category: cs.CL

> 本研究揭示了广泛使用的多语言翻译基准FLORES+在评估现代翻译系统的能力方面存在的不足，提出需要改进评估基准，使之更加贴近实际翻译挑战。

<details>
  <summary>Details</summary>

**Motivation:** 本研究的动机在于，现有的FLORES+多语言翻译基准虽然在质量控制上严格，但在实际的多语言评估中存在不足，特别是数据的领域特定性和文化偏向性。

**Method:** 本研究采用了人工评估的方法，选择四种语言（Asante Twi、日语、Jinghpaw 和南阿塞拜疆语）的数据，揭露了FLORES+基准在真正多语言评估中的不足之处。

**Result:** 人工评估发现许多翻译品质低于90%标准，源句对特定领域和文化的偏见严重。简单地复制命名实体等基本策略就能取得不错的BLEU分数，显示出评估协议的脆弱性。

**Conclusion:** 研究表明，训练在高质量、自然数据上的MT模型在FLORES+上的表现不如在领域相关评估集上的表现，因此提倡使用领域通用且文化中立的源文本作为多语言MT基准。

**Abstract:** Multilingual machine translation (MT) benchmarks play a central role in
evaluating the capabilities of modern MT systems. Among them, the FLORES+
benchmark is widely used, offering English-to-many translation data for over
200 languages, curated with strict quality control protocols. However, we study
data in four languages (Asante Twi, Japanese, Jinghpaw, and South Azerbaijani)
and uncover critical shortcomings in the benchmark's suitability for truly
multilingual evaluation. Human assessments reveal that many translations fall
below the claimed 90% quality standard, and the annotators report that source
sentences are often too domain-specific and culturally biased toward the
English-speaking world. We further demonstrate that simple heuristics, such as
copying named entities, can yield non-trivial BLEU scores, suggesting
vulnerabilities in the evaluation protocol. Notably, we show that MT models
trained on high-quality, naturalistic data perform poorly on FLORES+ while
achieving significant gains on our domain-relevant evaluation set. Based on
these findings, we advocate for multilingual MT benchmarks that use
domain-general and culturally neutral source texts rely less on named entities,
in order to better reflect real-world translation challenges.

</details>


### [19] [SciTopic: Enhancing Topic Discovery in Scientific Literature through Advanced LLM](https://arxiv.org/abs/2508.20514)
*Pengjiang Li,Zaitian Wang,Xinhao Zhang,Ran Zhang,Lu Jiang,Pengfei Wang,Yuanchun Zhou*

Main category: cs.CL

> SciTopic, enhanced by large language models, offers improved topic discovery in scientific literature by better capturing nuanced relationships within text and outperforms existing methods.

<details>
  <summary>Details</summary>

**Motivation:** To find a better solution for topic discovery in scientific literature that can understand the complex and high-dimensional relationships within text, surpassing the limitations of word embedding methods.

**Method:** The paper proposes a method named SciTopic which first builds a textual encoder to understand the content, then an optimization module is created using LLMs, which helps in understanding thematic relevance and contextual intricacies. Lastly, it fine-tunes the encoder by optimizing contrastive loss under LLM guidance.

**Result:** The experimental results on three real-world datasets show that SciTopic outperforms existing methods in scientific topic discovery, providing deeper and faster insights.

**Conclusion:** The proposed method, SciTopic, demonstrates superior performance in topic discovery due to its advanced understanding of textual nuances, thus aiding researchers in identifying trends more effectively.

**Abstract:** Topic discovery in scientific literature provides valuable insights for
researchers to identify emerging trends and explore new avenues for
investigation, facilitating easier scientific information retrieval. Many
machine learning methods, particularly deep embedding techniques, have been
applied to discover research topics. However, most existing topic discovery
methods rely on word embedding to capture the semantics and lack a
comprehensive understanding of scientific publications, struggling with
complex, high-dimensional text relationships. Inspired by the exceptional
comprehension of textual information by large language models (LLMs), we
propose an advanced topic discovery method enhanced by LLMs to improve
scientific topic identification, namely SciTopic. Specifically, we first build
a textual encoder to capture the content from scientific publications,
including metadata, title, and abstract. Next, we construct a space
optimization module that integrates entropy-based sampling and triplet tasks
guided by LLMs, enhancing the focus on thematic relevance and contextual
intricacies between ambiguous instances. Then, we propose to fine-tune the
textual encoder based on the guidance from the LLMs by optimizing the
contrastive loss of the triplets, forcing the text encoder to better
discriminate instances of different topics. Finally, extensive experiments
conducted on three real-world datasets of scientific publications demonstrate
that SciTopic outperforms the state-of-the-art (SOTA) scientific topic
discovery methods, enabling researchers to gain deeper and faster insights.

</details>


### [20] [Overview of BioASQ 2024: The twelfth BioASQ challenge on Large-Scale Biomedical Semantic Indexing and Question Answering](https://arxiv.org/abs/2508.20532)
*Anastasios Nentidis,Georgios Katsimpras,Anastasia Krithara,Salvador Lima-López,Eulàlia Farré-Maduell,Martin Krallinger,Natalia Loukachevitch,Vera Davydova,Elena Tutubalina,Georgios Paliouras*

Main category: cs.CL

> BioASQ第十二届挑战赛在CLEF 2024的背景下召开，包括两个既有任务的新版和两个新领域的任务，共有37支队伍参赛，展示了生物医学领域的技术进步。

<details>
  <summary>Details</summary>

**Motivation:** BioASQ挑战赛的动机在于通过国际竞赛促进大规模生物医学语义索引和问答系统的进步。

**Method:** 分析论文摘要的内容，发现其主要是关于BioASQ第十二届挑战赛的概览，该挑战赛旨在推进大规模生物医学语义索引和问答技术的进展。今年的BioASQ包括了两个已有任务b和Synergy的新版本，以及两个新任务：MultiCardioNER和BIONNE。该版BioASQ共有37支参赛队伍，总共提交了超过700份不同的参赛作品，涵盖了四个不同的共享任务。

**Result:** 大多数参赛系统的性能表现具有竞争力，表明领域内的技术水平正在持续进步。

**Conclusion:** 本版BioASQ展示了参赛系统在多个任务上的高水平表现，表明生物医学语义索引和问答系统的开发水平持续提升。

**Abstract:** This is an overview of the twelfth edition of the BioASQ challenge in the
context of the Conference and Labs of the Evaluation Forum (CLEF) 2024. BioASQ
is a series of international challenges promoting advances in large-scale
biomedical semantic indexing and question answering. This year, BioASQ
consisted of new editions of the two established tasks b and Synergy, and two
new tasks: a) MultiCardioNER on the adaptation of clinical entity detection to
the cardiology domain in a multilingual setting, and b) BIONNE on nested NER in
Russian and English. In this edition of BioASQ, 37 competing teams participated
with more than 700 distinct submissions in total for the four different shared
tasks of the challenge. Similarly to previous editions, most of the
participating systems achieved competitive performance, suggesting the
continuous advancement of the state-of-the-art in the field.

</details>


### [21] [Overview of BioASQ 2025: The Thirteenth BioASQ Challenge on Large-Scale Biomedical Semantic Indexing and Question Answering](https://arxiv.org/abs/2508.20554)
*Anastasios Nentidis,Georgios Katsimpras,Anastasia Krithara,Martin Krallinger,Miguel Rodríguez-Ortega,Eduard Rodriguez-López,Natalia Loukachevitch,Andrey Sakhovskiy,Elena Tutubalina,Dimitris Dimitriadis,Grigorios Tsoumakas,George Giannakoulas,Alexandra Bekiaridou,Athanasios Samaras,Giorgio Maria Di Nunzio,Nicola Ferro,Stefano Marchesin,Marco Martinelli,Gianmaria Silvello,Georgios Paliouras*

Main category: cs.CL

> Error

<details>
  <summary>Details</summary>

**Motivation:** Error

**Method:** Error

**Result:** Error

**Conclusion:** Error

**Abstract:** This is an overview of the thirteenth edition of the BioASQ challenge in the
context of the Conference and Labs of the Evaluation Forum (CLEF) 2025. BioASQ
is a series of international challenges promoting advances in large-scale
biomedical semantic indexing and question answering. This year, BioASQ
consisted of new editions of the two established tasks, b and Synergy, and four
new tasks: a) Task MultiClinSum on multilingual clinical summarization. b) Task
BioNNE-L on nested named entity linking in Russian and English. c) Task
ELCardioCC on clinical coding in cardiology. d) Task GutBrainIE on gut-brain
interplay information extraction. In this edition of BioASQ, 83 competing teams
participated with more than 1000 distinct submissions in total for the six
different shared tasks of the challenge. Similar to previous editions, several
participating systems achieved competitive performance, indicating the
continuous advancement of the state-of-the-art in the field.

</details>


### [22] [Adaptive Federated Distillation for Multi-Domain Non-IID Textual Data](https://arxiv.org/abs/2508.20557)
*Jiahao Xiao,Jiangming Liu*

Main category: cs.CL

> 本文提出并研究了一个新的联邦学习框架AdaFD，它能更好地处理多领域的非独立同分布环境，并在实验中证明了其有效性。

<details>
  <summary>Details</summary>

**Motivation:** 先前的工作主要关注输出多样性的非独立同分布情况，忽略了输入的多样性，这对自然语言处理至关重要。我们致力于解决这个问题。

**Method:** 我们提出了一个自适应联邦蒸馏（AdaFD）框架，旨在解决多领域非独立同分布（non-IID）挑战，适用于同质和异质环境。

**Result:** 实验证明，我们的模型能够捕捉客户本地的多样性，并在性能上优于现有的工作。

**Conclusion:** 提出的AdaFD框架能够在处理多领域非独立同分布数据时表现出色，并为联邦学习提供了评估框架和基准。

**Abstract:** The widespread success of pre-trained language models has established a new
training paradigm, where a global PLM is fine-tuned using task-specific data
from local clients. The local data are highly different from each other and can
not capture the global distribution of the whole data in real world. To address
the challenges of non-IID data in real environments, privacy-preserving
federated distillation has been proposed and highly investigated. However,
previous experimental non-IID scenarios are primarily identified with the label
(output) diversity, without considering the diversity of language domains
(input) that is crucial in natural language processing. In this paper, we
introduce a comprehensive set of multi-domain non-IID scenarios and propose a
unified benchmarking framework that includes diverse data. The benchmark can be
used to evaluate the federated learning framework in a real environment. To
this end, we propose an Adaptive Federated Distillation (AdaFD) framework
designed to address multi-domain non-IID challenges in both homogeneous and
heterogeneous settings. Experimental results demonstrate that our models
capture the diversity of local clients and achieve better performance compared
to the existing works. The code for this paper is available at:
https://github.com/jiahaoxiao1228/AdaFD.

</details>


### [23] [Leveraging Generative Models for Real-Time Query-Driven Text Summarization in Large-Scale Web Search](https://arxiv.org/abs/2508.20559)
*Zeyu Xiong,Yixuan Nan,Li Gao,Hengzhu Tang,Shuaiqiang Wang,Junfeng Wang,Dawei Yin*

Main category: cs.CL

> 本研究提出了一改进的框架，通过新颖的方法优化实时的查询驱动文本总结过程。该方法使轻量级模型在大规模网络搜索中表现出色，不仅提高了总结质量，也优化了部署效率。

<details>
  <summary>Details</summary>

**Motivation:** 传统的提取式总结模型由于多阶段管道引入的信息损失和架构瓶颈，以及对用户查询和文档语义理解不足的缺点，往往是工业应用中的主要方法。考虑到这些限制，本研究动机在于提出一种更有效的方法，以改善大规模网络搜索中的即时总结性能。

**Method:** 本研究提出了一种将生成模型应用于实时QDTS（Query-Driven Text Summarization，基于查询的文本总结）的新型框架。该框架整合了大规模模型提炼、监督微调、直接偏好优化以及前瞻解码，将一个仅有0.1B参数的轻量级模型转化为了一个领域专用的QDTS专家。

**Result:** 相比于生产基线，本研究提出的模型在多个工业相关指标上表现更优越，达到了新的先进水平。同时也具有优秀的部署效率，仅需334个NVIDIA L20 GPU即可实现约50,000查询每秒的处理速度。

**Conclusion:** 通过对多个与工业相关的指标进行评估，本研究中的模型超越了生产基线，并达到了一个新的先进水平。此外，模型还表现出优秀的部署效率，仅需334个NVIDIA L20 GPU即可处理约50,000查询每秒，平均每个查询延时在55毫秒以内。

**Abstract:** In the dynamic landscape of large-scale web search, Query-Driven Text
Summarization (QDTS) aims to generate concise and informative summaries from
textual documents based on a given query, which is essential for improving user
engagement and facilitating rapid decision-making. Traditional extractive
summarization models, based primarily on ranking candidate summary segments,
have been the dominant approach in industrial applications. However, these
approaches suffer from two key limitations: 1) The multi-stage pipeline often
introduces cumulative information loss and architectural bottlenecks due to its
weakest component; 2) Traditional models lack sufficient semantic understanding
of both user queries and documents, particularly when dealing with complex
search intents. In this study, we propose a novel framework to pioneer the
application of generative models to address real-time QDTS in industrial web
search. Our approach integrates large model distillation, supervised
fine-tuning, direct preference optimization, and lookahead decoding to
transform a lightweight model with only 0.1B parameters into a
domain-specialized QDTS expert. Evaluated on multiple industry-relevant
metrics, our model outperforms the production baseline and achieves a new state
of the art. Furthermore, it demonstrates excellent deployment efficiency,
requiring only 334 NVIDIA L20 GPUs to handle \textasciitilde50,000 queries per
second under 55~ms average latency per query.

</details>


### [24] [KCS: Diversify Multi-hop Question Generation with Knowledge Composition Sampling](https://arxiv.org/abs/2508.20567)
*Yangfan Wang,Jie Liu,Chen Tang,Lian Yan,Jingchi Jiang*

Main category: cs.CL

> 本研究提出了一种名为KCS的新框架，通过抽样不同的知识组合来提高生成多跳问题的多样性，克服了以往方法忽视整合重要知识的缺点。KCS显著提高了知识组合的选择准确性，并在多个数据集上显示了优于基线方法的表现。

<details>
  <summary>Details</summary>

**Motivation:** 本研究旨在通过解决生成多样化的问题以应对多跳问答中数据稀疏的挑战。先前的研究关注通过内容规划和多样化的表达方式生成多样化的问题，但往往强调生成简单的问题，忽略了整合重要的知识，比如文档中的相关句子。

**Method:** 本研究介绍了一种名为知识组合抽样（Knowledge Composition Sampling, KCS）的新框架，通过在一个给定上下文中抽样各种知识组合来扩大生成的多跳问题的多样性。该框架将知识组合的选择建模为一个句子级别的条件预测任务，并利用概率对比损失来预测下一个最相关的知识。在推理过程中，采用了随机解码策略来有效平衡准确性和多样性。

**Result:** 与竞争的基线方法相比，本研究的KCS将知识组合选择的总体准确性提高了3.9%，并且将其应用于数据增强在HotpotQA和2WikiMultihopQA数据集上有所改进。

**Conclusion:** KCS框架通过抽样不同的知识组合来提升多跳问答数据的多样性，有助于解决数据稀疏问题，并且通过概率对比损失和随机解码策略来平衡生成问题的准确性和多样性表现出了优于基线方法的结果。

**Abstract:** Multi-hop question answering faces substantial challenges due to data
sparsity, which increases the likelihood of language models learning spurious
patterns. To address this issue, prior research has focused on diversifying
question generation through content planning and varied expression. However,
these approaches often emphasize generating simple questions and neglect the
integration of essential knowledge, such as relevant sentences within
documents. This paper introduces the Knowledge Composition Sampling (KCS), an
innovative framework designed to expand the diversity of generated multi-hop
questions by sampling varied knowledge compositions within a given context. KCS
models the knowledge composition selection as a sentence-level conditional
prediction task and utilizes a probabilistic contrastive loss to predict the
next most relevant piece of knowledge. During inference, we employ a stochastic
decoding strategy to effectively balance accuracy and diversity. Compared to
competitive baselines, our KCS improves the overall accuracy of knowledge
composition selection by 3.9%, and its application for data augmentation yields
improvements on HotpotQA and 2WikiMultihopQA datasets. Our code is available
at: https://github.com/yangfanww/kcs.

</details>


### [25] [A Graph Talks, But Who's Listening? Rethinking Evaluations for Graph-Language Models](https://arxiv.org/abs/2508.20583)
*Soham Petkar,Hari Aakash K,Anirudh Vempati,Akshit Sinha,Ponnurangam Kumarauguru,Chirag Agarwal*

Main category: cs.CL

> 提出CLEGR基准测试来改善GLM的评估，发现仅语言模型在多模态任务中表现出色，GLM需要改进其在结构推理任务上的性能。

<details>
  <summary>Details</summary>

**Motivation:** 当前的GLM评估基准无法充分评估多模态推理能力，因此提出了一个新的评估基准CLEGR。

**Method:** 通过引入CLEGR（组合语言-图推理）基准测试来评估多模态推理，并使用合成图生成管道和需要联合推理结构和文本语义的问题进行测试。

**Result:** 发现仅使用提示的LLM基线与包含完整GNN主干的GLM的性能相当，而且在需要结构推理的任务中，GLM的表现显著下降。

**Conclusion:** 当前GLMs在图推理方面存在局限性，该研究为推进融合图结构与语言的多模态推理开辟了道路。

**Abstract:** Developments in Graph-Language Models (GLMs) aim to integrate the structural
reasoning capabilities of Graph Neural Networks (GNNs) with the semantic
understanding of Large Language Models (LLMs). However, we demonstrate that
current evaluation benchmarks for GLMs, which are primarily repurposed
node-level classification datasets, are insufficient to assess multimodal
reasoning. Our analysis reveals that strong performance on these benchmarks is
achievable using unimodal information alone, suggesting that they do not
necessitate graph-language integration. To address this evaluation gap, we
introduce the CLEGR(Compositional Language-Graph Reasoning) benchmark, designed
to evaluate multimodal reasoning at various complexity levels. Our benchmark
employs a synthetic graph generation pipeline paired with questions that
require joint reasoning over structure and textual semantics. We perform a
thorough evaluation of representative GLM architectures and find that
soft-prompted LLM baselines perform on par with GLMs that incorporate a full
GNN backbone. This result calls into question the architectural necessity of
incorporating graph structure into LLMs. We further show that GLMs exhibit
significant performance degradation in tasks that require structural reasoning.
These findings highlight limitations in the graph reasoning capabilities of
current GLMs and provide a foundation for advancing the community toward
explicit multimodal reasoning involving graph structure and language.

</details>


### [26] [Generative Annotation for ASR Named Entity Correction](https://arxiv.org/abs/2508.20700)
*Yuanchang Luo,Daimeng Wei,Shaojun Li,Hengchao Shang,Jiaxin Guo,Zongyao Li,Zhanglin Wu,Xiaoyu Chen,Zhiqiang Rao,Jinlong Yang,Hao Yang*

Main category: cs.CL

> 提出利用语音听感特征检索候选实体和生成方法标注ASR文本中的实体错误的新方法，实验结果表明该方法在单词形式不同的场景下有效。

<details>
  <summary>Details</summary>

**Motivation:** 由于现有的自动语音识别系统在转录特定领域的专有名词时经常失败，导致下游任务出现问题。许多快速轻量级的专有名词校正模型虽然在很大程度上依赖音素级别编辑距离算法展现了令人印象深刻的性能，但对于转录错误单词与真值实体形式显著不同的情况，这些方法常常无法定位错误转录的单词，从而限制了它们的应用。因此，提出了新型的NEC方法。

**Method:** 提出一种新的专有名词校正方法，利用语音听感特征检索候选实体，并设计了一种生成方法来标注ASR转录文本中的实体错误并替换为正确实体。

**Result:** 我们的NEC方法显著提升了实体准确性，通过开源测试集和训练数据进一步促进研究。

**Conclusion:** 该方法在不同单词形式的情况下是有效的，实验结果证明了其在实体识别准确性方面的显著改进。同时，计划将自己构造的测试集和训练数据开源。

**Abstract:** End-to-end automatic speech recognition systems often fail to transcribe
domain-specific named entities, causing catastrophic failures in downstream
tasks. Numerous fast and lightweight named entity correction (NEC) models have
been proposed in recent years. These models, mainly leveraging phonetic-level
edit distance algorithms, have shown impressive performances. However, when the
forms of the wrongly-transcribed words(s) and the ground-truth entity are
significantly different, these methods often fail to locate the wrongly
transcribed words in hypothesis, thus limiting their usage. We propose a novel
NEC method that utilizes speech sound features to retrieve candidate entities.
With speech sound features and candidate entities, we inovatively design a
generative method to annotate entity errors in ASR transcripts and replace the
text with correct entities. This method is effective in scenarios of word form
difference. We test our method using open-source and self-constructed test
sets. The results demonstrate that our NEC method can bring significant
improvement to entity accuracy. We will open source our self-constructed test
set and training data.

</details>


### [27] [Multi-Lingual Implicit Discourse Relation Recognition with Multi-Label Hierarchical Learning](https://arxiv.org/abs/2508.20712)
*Nelson Filipe Costa,Leila Kosseim*

Main category: cs.CL

> 本文开发了一种名为HArch的多语言模型用于隐式话语关系识别，其表现优于其他几种模型，并在多个语料库上取得了SOTA结果。

<details>
  <summary>Details</summary>

**Motivation:** 旨在开发一种有效的多语言模型用于识别隐式话语关系，通过引入分层依赖方式来提高预测的准确性。

**Method:** 提出了一种名为HArch的多语言、多标签分类模型，用于隐式话语关系识别。该模型在新发布的DiscogeM 2.0语料库上进行评估，并利用话语意义之间的分层依赖关系来预测PDTB 3.0框架中三个意义水平的概率分布。

**Result:** 实验显示，在英语情况下，RoBERTa-HArch表现最优；在多语言场景中，XLM-RoBERTa-HArch表现最佳。此外，他们的微调模型在所有语言配置的少样本提示下一致优于GPT-4o和Llama-4-Maverick模型。

**Conclusion:** 研究结果证明了该分层方法的有效性，在DiscogeM 1.0语料库上达到了SOTA的结果，再次证明了针对任务的微调相较于提示在IDRR任务中的优势。

**Abstract:** This paper introduces the first multi-lingual and multi-label classification
model for implicit discourse relation recognition (IDRR). Our model, HArch, is
evaluated on the recently released DiscoGeM 2.0 corpus and leverages
hierarchical dependencies between discourse senses to predict probability
distributions across all three sense levels in the PDTB 3.0 framework. We
compare several pre-trained encoder backbones and find that RoBERTa-HArch
achieves the best performance in English, while XLM-RoBERTa-HArch performs best
in the multi-lingual setting. In addition, we compare our fine-tuned models
against GPT-4o and Llama-4-Maverick using few-shot prompting across all
language configurations. Our results show that our fine-tuned models
consistently outperform these LLMs, highlighting the advantages of
task-specific fine-tuning over prompting in IDRR. Finally, we report SOTA
results on the DiscoGeM 1.0 corpus, further validating the effectiveness of our
hierarchical approach.

</details>


### [28] [Addressing Tokenization Inconsistency in Steganography and Watermarking Based on Large Language Models](https://arxiv.org/abs/2508.20718)
*Ruiyi Yan,Yugo Murawaki*

Main category: cs.CL

> 研究探讨大规模语言模型文本生成中的分词不一致性问题，提出针对性解决方案，并证明其能改善隐写和水印技术的质量与鲁棒性。

<details>
  <summary>Details</summary>

**Motivation:** 大规模语言模型显著提升了文本生成的能力和效率，但是它们也带来了分词不一致性问题，这可能降低隐写和水印技术的鲁棒性。

**Method:** 该研究关注在文本隐写和水印技术中的分词不一致性（TI），提出了两种针对性的解决方案：一种是用于隐写的逐步验证方法，另一种是用于水印的后处理回滚方法。

**Result:** 实验结果显示，直接处理TI相较于传统的隐写歧义解决方法，能提高流畅性、不可感知性和抗隐写分析能力；对于水印技术，处理TI能提高检测性和抗攻击性。

**Conclusion:** 本研究对于理解和解决大规模语言模型应用中的分词不一致性问题具有重要意义。

**Abstract:** Large language models have significantly enhanced the capacities and
efficiency of text generation. On the one hand, they have improved the quality
of text-based steganography. On the other hand, they have also underscored the
importance of watermarking as a safeguard against malicious misuse. In this
study, we focus on tokenization inconsistency (TI) between Alice and Bob in
steganography and watermarking, where TI can undermine robustness. Our
investigation reveals that the problematic tokens responsible for TI exhibit
two key characteristics: infrequency and temporariness. Based on these
findings, we propose two tailored solutions for TI elimination: a stepwise
verification method for steganography and a post-hoc rollback method for
watermarking. Experiments show that (1) compared to traditional disambiguation
methods in steganography, directly addressing TI leads to improvements in
fluency, imperceptibility, and anti-steganalysis capacity; (2) for
watermarking, addressing TI enhances detectability and robustness against
attacks.

</details>


### [29] [rStar2-Agent: Agentic Reasoning Technical Report](https://arxiv.org/abs/2508.20722)
*Ning Shang,Yifei Liu,Yi Zhu,Li Lyna Zhang,Weijiang Xu,Xinyu Guan,Buze Zhang,Bingcheng Dong,Xudong Zhou,Bowen Zhang,Ying Xin,Ziming Miao,Scarlett Li,Fan Yang,Mao Yang*

Main category: cs.CL

> rStar2-Agent是一个通过代理强化学习训练的140亿参数数学推理模型，它在复杂问题解决上展示了先进的认知行为，并在AIME24和AIME25测试中表现出色，超过了一些更大的模型性能。

<details>
  <summary>Details</summary>

**Motivation:** 该研究目的是展示通过代理强化学习训练大型模型进行复杂问题解决的先进能力，特别是数学推理。

**Method:** 本文介绍了rStar2-Agent，一个通过代理强化学习训练的140亿参数数学推理模型。这个模型展示了高级认知行为，例如在使用Python编码工具前仔细思考和利用代码执行的反馈来自主探索、验证和优化复杂问题解决中的中间步骤。这一能力通过三种关键创新得以实现：(i) 一个高效的RL基础设施，提供可靠的Python代码环境，支持高吞吐量执行并降低rollout成本，使模型能在有限的GPU资源上训练；(ii) GRPO-RoC代理RL算法，具有基于正确的Resample-on-Correct rollout策略，减少了代码环境中的噪音影响，提升了模型的推理能力；(iii) 一种高效的代理训练配方，从非推理SFT开始，逐步进入多RL阶段，以较少的计算成本训练出先进认知能力的模型。

**Result:** rStar2-Agent 在预训练的14B模型基础上，在仅需510RL步骤的一个星期内达到最先进水平，AIME24 和 AIME25 的通过率分别达到80.6%和69.8%，比DeepSeek-R1 (671B)更有优势，且响应速度更快。

**Conclusion:** rStar2-Agent模型在数学推理及问题解决方面展示了卓越的能力，并且在特定任务上的表现超过了现有的大型模型，具有广泛的应用前景。

**Abstract:** We introduce rStar2-Agent, a 14B math reasoning model trained with agentic
reinforcement learning to achieve frontier-level performance. Beyond current
long CoT, the model demonstrates advanced cognitive behaviors, such as thinking
carefully before using Python coding tools and reflecting on code execution
feedback to autonomously explore, verify, and refine intermediate steps in
complex problem-solving. This capability is enabled through three key
innovations that makes agentic RL effective at scale: (i) an efficient RL
infrastructure with a reliable Python code environment that supports
high-throughput execution and mitigates the high rollout costs, enabling
training on limited GPU resources (64 MI300X GPUs); (ii) GRPO-RoC, an agentic
RL algorithm with a Resample-on-Correct rollout strategy that addresses the
inherent environment noises from coding tools, allowing the model to reason
more effectively in a code environment; (iii) An efficient agent training
recipe that starts with non-reasoning SFT and progresses through multi-RL
stages, yielding advanced cognitive abilities with minimal compute cost. To
this end, rStar2-Agent boosts a pre-trained 14B model to state of the art in
only 510 RL steps within one week, achieving average pass@1 scores of 80.6% on
AIME24 and 69.8% on AIME25, surpassing DeepSeek-R1 (671B) with significantly
shorter responses. Beyond mathematics, rStar2-Agent-14B also demonstrates
strong generalization to alignment, scientific reasoning, and agentic tool-use
tasks. Code and training recipes are available at
https://github.com/microsoft/rStar.

</details>


### [30] [Leveraging Semantic Triples for Private Document Generation with Local Differential Privacy Guarantees](https://arxiv.org/abs/2508.20736)
*Stephen Meisenbacher,Maulik Chevli,Florian Matthes*

Main category: cs.CL

> This paper presents DP-ST, a method that enhances text privatization under local differential privacy by using semantic triples and LLM post-processing, achieving better balance between privacy and utility at lower ε values.

<details>
  <summary>Details</summary>

**Motivation:** To address the challenge of achieving reasonable text privatization under local differential privacy, especially at lower ε values, where previous methods struggle.

**Method:** DP-ST, which uses semantic triples for neighborhood-aware private document generation under local differential privacy guarantees. It applies the divide-and-conquer paradigm and leverages LLM post-processing to maintain text coherence at lower privacy parameters (ε).

**Result:** The method demonstrates effective text generation under local DP that maintains coherence and balance between privacy and utility at lower ε values compared to previous approaches.

**Conclusion:** The study underscores the significance of semantic coherence in balancing privacy and utility in text privatization, making it viable even under more stringent privacy conditions.

**Abstract:** Many works at the intersection of Differential Privacy (DP) in Natural
Language Processing aim to protect privacy by transforming texts under DP
guarantees. This can be performed in a variety of ways, from word perturbations
to full document rewriting, and most often under local DP. Here, an input text
must be made indistinguishable from any other potential text, within some bound
governed by the privacy parameter $\varepsilon$. Such a guarantee is quite
demanding, and recent works show that privatizing texts under local DP can only
be done reasonably under very high $\varepsilon$ values. Addressing this
challenge, we introduce DP-ST, which leverages semantic triples for
neighborhood-aware private document generation under local DP guarantees.
Through the evaluation of our method, we demonstrate the effectiveness of the
divide-and-conquer paradigm, particularly when limiting the DP notion (and
privacy guarantees) to that of a privatization neighborhood. When combined with
LLM post-processing, our method allows for coherent text generation even at
lower $\varepsilon$ values, while still balancing privacy and utility. These
findings highlight the importance of coherence in achieving balanced
privatization outputs at reasonable $\varepsilon$ levels.

</details>


### [31] [Specializing General-purpose LLM Embeddings for Implicit Hate Speech Detection across Datasets](https://arxiv.org/abs/2508.20750)
*Vassiliy Cheremetiev,Quang Long Ho Ngo,Chau Ying Kot,Alina Elena Baia,Andrea Cavallaro*

Main category: cs.CL

> 研究展示微调大型语言模型在隐性仇恨言论检测中的突出表现，显著提高了检测精度。

<details>
  <summary>Details</summary>

**Motivation:** 隐性仇恨言论因其间接性和隐秘性难以检测，故尝试通过优化模型来改善这一问题。

**Method:** 通过微调基于大型语言模型（LLMs）的通用嵌入模型（如Stella、Jasper、NV-Embed和E5），来提升隐性仇恨言论（IHS）检测的性能。

**Result:** 在多个隐性仇恨言论数据集上的实验表明，F1-macro评分有显著提高，最高认为20.35个百分点。

**Conclusion:** 单靠微调现有通用嵌入模型即可达到最先进的检测效果。

**Abstract:** Implicit hate speech (IHS) is indirect language that conveys prejudice or
hatred through subtle cues, sarcasm or coded terminology. IHS is challenging to
detect as it does not include explicit derogatory or inflammatory words. To
address this challenge, task-specific pipelines can be complemented with
external knowledge or additional information such as context, emotions and
sentiment data. In this paper, we show that, by solely fine-tuning recent
general-purpose embedding models based on large language models (LLMs), such as
Stella, Jasper, NV-Embed and E5, we achieve state-of-the-art performance.
Experiments on multiple IHS datasets show up to 1.10 percentage points
improvements for in-dataset, and up to 20.35 percentage points improvements in
cross-dataset evaluation, in terms of F1-macro score.

</details>


### [32] [GUARD: Glocal Uncertainty-Aware Robust Decoding for Effective and Efficient Open-Ended Text Generation](https://arxiv.org/abs/2508.20757)
*Yuanhao Ding,Esteban Garces Arias,Meimingwei Li,Julian Rodemann,Matthias Aßenmacher,Danlu Chen,Gaojuan Fan,Christian Heumann,Chongsheng Zhang*

Main category: cs.CL

> 本文提出了GUARD方法来解决大语言模型在生成连续文本时面临的连贯性和多样性之间的平衡问题。实验表明，GUARD不仅提高了生成文本的质量，还提升了生成速度。

<details>
  <summary>Details</summary>

**Motivation:** 解决大语言模型生成连续文本时面临的连贯性与多样性之间的平衡问题。现有的对比搜索解码策略虽然试图解决这个问题，但受到超参数依赖和高计算成本的限制。

**Method:** GUARD是一种自我适应的解码方法，通过“全局局部”不确定性驱动框架在连贯性和多样性之间取得平衡。GUARD结合全局熵估计和局部熵偏差来整合长期和短期不确定性信号，并引入基于标记计数的简单有效的惩罚以减少计算开销。

**Result:** 实验结果显示，GUARD在文本多样性和连贯性之间取得了良好的平衡，同时在生成速度上表现出显著的改进。人类和大语言模型评估者在多个文本质量维度的对比研究中验证了其出色性能。

**Conclusion:** 提出GUARD方法，它通过全局熵公式有效缓解了不确定性中的突变，并提供了无偏差和一致性的理论保证，同时减少了计算开销。实验和评估验证了GUARD在文本生成的多样性和连贯性上的优越性。

**Abstract:** Open-ended text generation faces a critical challenge: balancing coherence
with diversity in LLM outputs. While contrastive search-based decoding
strategies have emerged to address this trade-off, their practical utility is
often limited by hyperparameter dependence and high computational costs. We
introduce GUARD, a self-adaptive decoding method that effectively balances
these competing objectives through a novel "Glocal" uncertainty-driven
framework. GUARD combines global entropy estimates with local entropy
deviations to integrate both long-term and short-term uncertainty signals. We
demonstrate that our proposed global entropy formulation effectively mitigates
abrupt variations in uncertainty, such as sudden overconfidence or high entropy
spikes, and provides theoretical guarantees of unbiasedness and consistency. To
reduce computational overhead, we incorporate a simple yet effective
token-count-based penalty into GUARD. Experimental results demonstrate that
GUARD achieves a good balance between text diversity and coherence, while
exhibiting substantial improvements in generation speed. In a more nuanced
comparison study across different dimensions of text quality, both human and
LLM evaluators validated its remarkable performance. Our code is available at
https://github.com/YecanLee/GUARD.

</details>


### [33] [Feel the Difference? A Comparative Analysis of Emotional Arcs in Real and LLM-Generated CBT Sessions](https://arxiv.org/abs/2508.20764)
*Xiaoyi Wang,Jiwei Zhang,Guangtao Zhang,Honglei Guo*

Main category: cs.CL

> 首次进行了真实和大语言模型生成的认知行为治疗对话间的情感弧度比较分析。发现合成对话与真实对话在情感变化、情感蕴含语言及情感反应与调节模式上存在差异。强调了在心理健康应用中情感真实度的重要性。

<details>
  <summary>Details</summary>

**Motivation:** 动机是理解大型语言模型生成的合成治疗对话是否能够捕捉到真实治疗中的情感动态，尤其是在认知行为治疗中的情感弧度。

**Method:** 通过改编的Utterance Emotion Dynamics框架，分析了在真实和LLM生成的认知行为治疗对话之间的情感轨迹，并在包括价值、唤醒和主导度这三个维度上进行了细粒度的情感变化分析。研究涵盖了完整的对话以及个体发言角色（咨询师和客户）之间的分析。使用的数据材料包括从公开视频中转录的真实会话以及来自CACTUS数据集的合成对话。

**Result:** 研究结果表明，真实的治疗对话比合成对话包含了更多的情感变化、更多的情感蕴含语言、更规范的情感反应与调节模式。

**Conclusion:** 发现合成对话尽管流畅且结构一致，但在关键情感属性上与真实对话存在偏差，如情感变化更大、情绪蕴含语言更多和情感反应与调节模式更真实。特别是对于客户角色，真实和合成对话之间的情感弧度相似性较低。这些发现强调了当前LLM生成的治疗数据的局限性，以及在心理健康应用中情感真实度的重要性。

**Abstract:** Synthetic therapy dialogues generated by large language models (LLMs) are
increasingly used in mental health NLP to simulate counseling scenarios, train
models, and supplement limited real-world data. However, it remains unclear
whether these synthetic conversations capture the nuanced emotional dynamics of
real therapy. In this work, we conduct the first comparative analysis of
emotional arcs between real and LLM-generated Cognitive Behavioral Therapy
dialogues. We adapt the Utterance Emotion Dynamics framework to analyze
fine-grained affective trajectories across valence, arousal, and dominance
dimensions. Our analysis spans both full dialogues and individual speaker roles
(counselor and client), using real sessions transcribed from public videos and
synthetic dialogues from the CACTUS dataset. We find that while synthetic
dialogues are fluent and structurally coherent, they diverge from real
conversations in key emotional properties: real sessions exhibit greater
emotional variability,more emotion-laden language, and more authentic patterns
of reactivity and regulation. Moreover, emotional arc similarity between real
and synthetic speakers is low, especially for clients. These findings
underscore the limitations of current LLM-generated therapy data and highlight
the importance of emotional fidelity in mental health applications. We
introduce RealCBT, a curated dataset of real CBT sessions, to support future
research in this space.

</details>


### [34] [Turning the Spell Around: Lightweight Alignment Amplification via Rank-One Safety Injection](https://arxiv.org/abs/2508.20766)
*Harethah Abu Shairah,Hasan Abed Al Kader Hammoud,George Turkiyyah,Bernard Ghanem*

Main category: cs.CL

> This paper introduces ROSI, a method to enhance the safety of LLMs through targeted weight modification, proven effective without compromising model utility.

<details>
  <summary>Details</summary>

**Motivation:** To counteract the risk of safety mechanisms being bypassed by ablating or removing specific representational directions within the model, as shown by recent research.

**Method:** Rank-One Safety Injection (ROSI), a white-box method that amplifies a model's safety alignment by permanently steering its activations toward the refusal-mediating subspace, operating as a simple, fine-tuning-free rank-one weight modification.

**Result:** ROSI consistently increases safety refusal rates while preserving the utility of the model on standard benchmarks and can re-align 'uncensored' models by amplifying their own latent safety directions.

**Conclusion:** The targeted, interpretable weight steering method like ROSI is cheap, potent and effective for improving LLM safety, offering a valuable alternative to resource-intensive fine-tuning procedures.

**Abstract:** Safety alignment in Large Language Models (LLMs) often involves mediating
internal representations to refuse harmful requests. Recent research has
demonstrated that these safety mechanisms can be bypassed by ablating or
removing specific representational directions within the model. In this paper,
we propose the opposite approach: Rank-One Safety Injection (ROSI), a white-box
method that amplifies a model's safety alignment by permanently steering its
activations toward the refusal-mediating subspace. ROSI operates as a simple,
fine-tuning-free rank-one weight modification applied to all residual stream
write matrices. The required safety direction can be computed from a small set
of harmful and harmless instruction pairs. We show that ROSI consistently
increases safety refusal rates - as evaluated by Llama Guard 3 - while
preserving the utility of the model on standard benchmarks such as MMLU,
HellaSwag, and Arc. Furthermore, we show that ROSI can also re-align
'uncensored' models by amplifying their own latent safety directions,
demonstrating its utility as an effective last-mile safety procedure. Our
results suggest that targeted, interpretable weight steering is a cheap and
potent mechanism to improve LLM safety, complementing more resource-intensive
fine-tuning paradigms.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [35] [Mitigating Hallucinations in Multimodal LLMs via Object-aware Preference Optimization](https://arxiv.org/abs/2508.20181)
*Alberto Compagnoni,Davide Caffagni,Nicholas Moratelli,Lorenzo Baraldi,Marcella Cornia,Rita Cucchiara*

Main category: cs.CV

> 研究提出了一种名为CHAIR-DPO的方法，通过CHAIR度量和DPO技术减少多模态大型语言模型的幻觉生成，展示了这种方法在多个幻觉基准上的有效性。

<details>
  <summary>Details</summary>

**Motivation:** 尽管多模态大型语言模型在许多基准测试中表现出色，但生成与视觉输入不符的回答这一问题仍然存在。该研究旨在解决这一幻觉问题，通过将其视为对齐问题并采用更简单有效的微调策略。

**Method:** 该研究利用CHAIR度量区分生成答案中的幻觉和非幻觉样本，并通过直接偏好优化（DPO）微调现成的多模态大型语言模型，实现减少幻觉答案的目标。

**Result:** {immerse}

**Conclusion:** 通过使用CHAIR度量和直接偏好优化（DPO）方法，该研究成功减少了多模态大型语言模型在多个幻觉基准测试中的幻觉答案数量，证明了以CHAIR为基础的奖励进行微调的有效性。代码和训练模型已公开。

**Abstract:** Multimodal Large Language Models (MLLMs) emerge as a unified interface to
address a multitude of tasks, ranging from NLP to computer vision. Despite
showcasing state-of-the-art results in many benchmarks, a long-standing issue
is the tendency of MLLMs to hallucinate, that is to generate answers to the
user's query that are not reflected in the visual input. In this paper, we
address the problem of hallucinations as an alignment problem, seeking to steer
the MLLM so that it prefers generating content without hallucinations. In
contrast to recent approaches that require complicated pipelines to build
synthetic preference data for alignment training, often relying on proprietary
models, we capitalize on the well-known CHAIR metric, originally proposed to
gauge the degree of hallucinations in image captioning. Given a pair of
generated answers, we leverage CHAIR to distinguish winner and loser options
(i.e., non-hallucinated and hallucinated samples) and fine-tune off-the-shelf
MLLMs via Direct Preference Optimization (DPO). The resulting method, which we
refer to as CHAIR-DPO, effectively diminishes the amount of hallucinated
answers on several hallucination benchmarks, demonstrating the effectiveness of
fine-tuning the MLLM with a CHAIR-based reward. Source code and trained models
are publicly available at https://github.com/aimagelab/CHAIR-DPO.

</details>


### [36] [SDiFL: Stable Diffusion-Driven Framework for Image Forgery Localization](https://arxiv.org/abs/2508.20182)
*Yang Su,Shunquan Tan,Jiwu Huang*

Main category: cs.CV

> The paper introduces a novel approach to image forgery localization using Stable DiffusionV3 (SD3), significantly enhancing accuracy and efficiency in detecting forgeries with improvements up to 12% over existing models.

<details>
  <summary>Details</summary>

**Motivation:** The motivation of this paper is to address the challenge of keeping up with the rapid advancements in image manipulation technologies, particularly posed by the new generation of multi-modal large models like Stable Diffusion, while improving on the inefficiencies and high costs associated with traditional forgery localization techniques that require extensive annotated data.

**Method:** The method integrates Stable Diffusion's image generation capabilities and perceptual abilities into an image forensic framework. It leverages SD3's multi-modal processing in the latent space by treating forgery residuals as an explicit modality, which is then fused with the latent space during training to enhance forgery localization.

**Result:** The experimental results demonstrate that the proposed framework achieves up to 12% better performance on benchmark datasets compared to state-of-the-art models. Notably, it shows robust performance even on real-world and unseen data.

**Conclusion:** The paper concludes that by integrating Stable Diffusion's multi-modal architecture into a forensic framework, a significant improvement in the detection of image forgeries can be achieved, providing an efficient alternative to existing methods that are generally data-intensive and costly.

**Abstract:** Driven by the new generation of multi-modal large models, such as Stable
Diffusion (SD), image manipulation technologies have advanced rapidly, posing
significant challenges to image forensics. However, existing image forgery
localization methods, which heavily rely on labor-intensive and costly
annotated data, are struggling to keep pace with these emerging image
manipulation technologies. To address these challenges, we are the first to
integrate both image generation and powerful perceptual capabilities of SD into
an image forensic framework, enabling more efficient and accurate forgery
localization. First, we theoretically show that the multi-modal architecture of
SD can be conditioned on forgery-related information, enabling the model to
inherently output forgery localization results. Then, building on this
foundation, we specifically leverage the multimodal framework of Stable
DiffusionV3 (SD3) to enhance forgery localization performance.We leverage the
multi-modal processing capabilities of SD3 in the latent space by treating
image forgery residuals -- high-frequency signals extracted using specific
highpass filters -- as an explicit modality. This modality is fused into the
latent space during training to enhance forgery localization performance.
Notably, our method fully preserves the latent features extracted by SD3,
thereby retaining the rich semantic information of the input image.
Experimental results show that our framework achieves up to 12% improvements in
performance on widely used benchmarking datasets compared to current
state-of-the-art image forgery localization models. Encouragingly, the model
demonstrates strong performance on forensic tasks involving real-world document
forgery images and natural scene forging images, even when such data were
entirely unseen during training.

</details>


### [37] [Grounding Multimodal Large Language Models with Quantitative Skin Attributes: A Retrieval Study](https://arxiv.org/abs/2508.20188)
*Max Torop,Masih Eskandar,Nicholas Kurtansky,Jinyang Liu,Jochen Weber,Octavia Camps,Veronica Rotemberg,Jennifer Dy,Kivanc Kose*

Main category: cs.CV

> 研究提出了一种提高AI模型诊断皮肤疾病时的可解释性的方法，即通过结合多模态大型语言模型和定量属性使用，并通过实例研究验证了该方法的可行性。

<details>
  <summary>Details</summary>

**Motivation:** 尽管人工智能模型在诊断皮肤疾病方面表现出显著的成功，但模型预测的可解释性需要显著提高才能在实践中应用。

**Method:** 通过结合多模态大型语言模型(MLLMs)和定量属性使用来提高皮肤疾病诊断模型的可解释性。具体来说，对模型进行了微调，使其能够从图像中预测与病变外观相关的定量属性值。

**Result:** 通过使用SLICE-3D数据集进行属性特定的内容检索案例研究，验证了MLLM嵌入空间可以基于这些属性进行标注。

**Conclusion:** 将多模态大型语言模型和定量属性使用相结合的方法可以提高皮肤疾病诊断模型的可解释性。

**Abstract:** Artificial Intelligence models have demonstrated significant success in
diagnosing skin diseases, including cancer, showing the potential to assist
clinicians in their analysis. However, the interpretability of model
predictions must be significantly improved before they can be used in practice.
To this end, we explore the combination of two promising approaches: Multimodal
Large Language Models (MLLMs) and quantitative attribute usage. MLLMs offer a
potential avenue for increased interpretability, providing reasoning for
diagnosis in natural language through an interactive format. Separately, a
number of quantitative attributes that are related to lesion appearance (e.g.,
lesion area) have recently been found predictive of malignancy with high
accuracy. Predictions grounded as a function of such concepts have the
potential for improved interpretability. We provide evidence that MLLM
embedding spaces can be grounded in such attributes, through fine-tuning to
predict their values from images. Concretely, we evaluate this grounding in the
embedding space through an attribute-specific content-based image retrieval
case study using the SLICE-3D dataset.

</details>


### [38] [Enhancing Automatic Modulation Recognition With a Reconstruction-Driven Vision Transformer Under Limited Labels](https://arxiv.org/abs/2508.20193)
*Hossein Ahmadi,Banafsheh Saffari*

Main category: cs.CV

> 提出了一种集成监督、自监督和重建目标的统一Vision Transformer框架，该框架在AMR领域中表现出色，特别是在低标签数量的数据集上。

<details>
  <summary>Details</summary>

**Motivation:** 自动调制识别（AMR）对于认知无线电、频谱监测和安全无线通信至关重要。然而，现有的解决方案通常依赖于大量标注数据集或多阶段训练流程，这限制了其实际应用的扩展性和泛化能力。

**Method:** 我们提出了一种统一的Vision Transformer (ViT)框架，该框架结合了监督、自监督和重建目标。模型包括一个ViT编码器、一个轻量级的卷积解码器和一个线性分类器；重建分支将增强信号转换回原始信号，使编码器锚定在细粒度I/Q结构上。这种策略在预训练过程中促进鲁棒、区分性特征学习，而在微调阶段的部分标签监督使得在有限标签情况下也能有效分类。

**Result:** 在RML2018.01A数据集上，我们的方法在低标签数量的情况下超过了监督CNN和ViT基线模型的表现，仅需15-20%的标注数据就能接近ResNet级别的准确率，并且在不同的信噪比水平下保持了强大的性能。

**Conclusion:** 总体而言，该框架提供了一种简单、通用且标签高效的AMR解决方案。

**Abstract:** Automatic modulation recognition (AMR) is critical for cognitive radio,
spectrum monitoring, and secure wireless communication. However, existing
solutions often rely on large labeled datasets or multi-stage training
pipelines, which limit scalability and generalization in practice. We propose a
unified Vision Transformer (ViT) framework that integrates supervised,
self-supervised, and reconstruction objectives. The model combines a ViT
encoder, a lightweight convolutional decoder, and a linear classifier; the
reconstruction branch maps augmented signals back to their originals, anchoring
the encoder to fine-grained I/Q structure. This strategy promotes robust,
discriminative feature learning during pretraining, while partial label
supervision in fine-tuning enables effective classification with limited
labels. On the RML2018.01A dataset, our approach outperforms supervised CNN and
ViT baselines in low-label regimes, approaches ResNet-level accuracy with only
15-20% labeled data, and maintains strong performance across varying SNR
levels. Overall, the framework provides a simple, generalizable, and
label-efficient solution for AMR.

</details>


### [39] [InfinityHuman: Towards Long-Term Audio-Driven Human](https://arxiv.org/abs/2508.20210)
*Xiaodi Li,Pan Xie,Yi Ren,Qijun Gan,Chen Zhang,Fangyuan Kong,Xiang Yin,Bingyue Peng,Zehuan Yuan*

Main category: cs.CV

> 本文提出了InfinityHuman，一种从粗到精的音频驱动人动画生成框架，解决现有方法中的身份漂移、色彩偏移和手部建模不准确问题，实验结果展示了其优异的视频质量和性能。

<details>
  <summary>Details</summary>

**Motivation:** 解决现有方法在生成高分辨率、长时长视频时出现的身份漂移、色彩偏移、场景不稳定以及手部动作建模不佳的问题。

**Method:** InfinityHuman, 一种从粗到精的框架，首先生成与音频同步的表示，然后利用姿势引导细化器逐步将其细化为高分辨率、长时长的视频。

**Result:** 实验表明，InfinityHuman在视频质量、身份保持、手部准确性以及唇音同步上均达到了最先进的性能。

**Conclusion:** InfinityHuman通过引入姿势引导细化器和手部特有奖励机制，有效解决了音频驱动人类动画生成中存在的问题，验证了每个模块的有效性。

**Abstract:** Audio-driven human animation has attracted wide attention thanks to its
practical applications. However, critical challenges remain in generating
high-resolution, long-duration videos with consistent appearance and natural
hand motions. Existing methods extend videos using overlapping motion frames
but suffer from error accumulation, leading to identity drift, color shifts,
and scene instability. Additionally, hand movements are poorly modeled,
resulting in noticeable distortions and misalignment with the audio. In this
work, we propose InfinityHuman, a coarse-to-fine framework that first generates
audio-synchronized representations, then progressively refines them into
high-resolution, long-duration videos using a pose-guided refiner. Since pose
sequences are decoupled from appearance and resist temporal degradation, our
pose-guided refiner employs stable poses and the initial frame as a visual
anchor to reduce drift and improve lip synchronization. Moreover, to enhance
semantic accuracy and gesture realism, we introduce a hand-specific reward
mechanism trained with high-quality hand motion data. Experiments on the EMTD
and HDTF datasets show that InfinityHuman achieves state-of-the-art performance
in video quality, identity preservation, hand accuracy, and lip-sync. Ablation
studies further confirm the effectiveness of each module. Code will be made
public.

</details>


### [40] [Spherical Vision Transformers for Audio-Visual Saliency Prediction in 360-Degree Videos](https://arxiv.org/abs/2508.20221)
*Mert Cokelek,Halit Ozsoy,Nevrez Imamoglu,Cagri Ozcinar,Inci Ayhan,Erkut Erdem,Aykut Erdem*

Main category: cs.CV

> 研究构建了新的360度视听显著性预测数据集并提出两种模型，显著提升预测准确率。

<details>
  <summary>Details</summary>

**Motivation:** 研究动机源于当前缺少用于预测360度视听显著性的综合数据集。研究团队构建了一个新的数据集YT360-EyeTracking，包含81个ODVs视频，每段视频都处于不同的视听条件。目的在于探索如何利用视听线索有效预测360度视频的视觉显著性。

**Method:** 本研究针对360度视频中的球面几何畸变及空间音频整合的挑战，提出了两种新模型：SalViT360和SalViT360-AV。SalViT360基于视觉变压器框架，并配备球形几何感知的时空注意力层；而SalViT360-AV则在此基础上，进一步加入了受音频输入条件控制的变压器适配器。

**Result:** 在YT360-EyeTracking等多个基准数据集上，SalViT360和SalViT360-AV模型均显著优于当前现有的方法，在预测360度场景中的观看者注意力上表现突出。

**Conclusion:** 研究结果表明，将空间音频线索整合进模型结构对于准确预测360度视频中的显著性是至关重要的。

**Abstract:** Omnidirectional videos (ODVs) are redefining viewer experiences in virtual
reality (VR) by offering an unprecedented full field-of-view (FOV). This study
extends the domain of saliency prediction to 360-degree environments,
addressing the complexities of spherical distortion and the integration of
spatial audio. Contextually, ODVs have transformed user experience by adding a
spatial audio dimension that aligns sound direction with the viewer's
perspective in spherical scenes. Motivated by the lack of comprehensive
datasets for 360-degree audio-visual saliency prediction, our study curates
YT360-EyeTracking, a new dataset of 81 ODVs, each observed under varying
audio-visual conditions. Our goal is to explore how to utilize audio-visual
cues to effectively predict visual saliency in 360-degree videos. Towards this
aim, we propose two novel saliency prediction models: SalViT360, a
vision-transformer-based framework for ODVs equipped with spherical
geometry-aware spatio-temporal attention layers, and SalViT360-AV, which
further incorporates transformer adapters conditioned on audio input. Our
results on a number of benchmark datasets, including our YT360-EyeTracking,
demonstrate that SalViT360 and SalViT360-AV significantly outperform existing
methods in predicting viewer attention in 360-degree scenes. Interpreting these
results, we suggest that integrating spatial audio cues in the model
architecture is crucial for accurate saliency prediction in omnidirectional
videos. Code and dataset will be available at
https://cyberiada.github.io/SalViT360.

</details>


### [41] [A Novel Framework for Automated Explain Vision Model Using Vision-Language Models](https://arxiv.org/abs/2508.20227)
*Phu-Vinh Nguyen,Tan-Hanh Pham,Chris Ngo,Truong Son Hy*

Main category: cs.CV

> 论文提出了一种新的流水线方法，通过视觉-语言模型来提高视觉模型的解释性，既包括对样本的解释也包括对整个数据集的解释，从而改进图像分析并帮助发现模型失败案例。

<details>
  <summary>Details</summary>

**Motivation:** 论文的动机是应对现有解释性AI方法（xAI）在解释视觉模型（尤其是数据集层面的行为）上的不足，以帮助识别模型的趋势和模式，并防止模型做出有偏见的判断。

**Method:** 该论文提出了一种流水线方法，旨在通过视觉-语言模型在样本和数据集层面解释视觉模型。这种方法能够帮助识别模型的失败案例，并深入理解视觉模型。

**Result:** 流水线可以发现视觉模型的失败案例，并提供对视觉模型的理解，这样可以将模型开发与xAI分析相结合，提高图像分析的准确性。

**Conclusion:** 该流水线方法通过视觉-语言模型加强了视觉模型的解释性，实现了在样本和数据集层面的模型理解，从而促进了图像分析的发展。

**Abstract:** The development of many vision models mainly focuses on improving their
performance using metrics such as accuracy, IoU, and mAP, with less attention
to explainability due to the complexity of applying xAI methods to provide a
meaningful explanation of trained models. Although many existing xAI methods
aim to explain vision models sample-by-sample, methods explaining the general
behavior of vision models, which can only be captured after running on a large
dataset, are still underexplored. Furthermore, understanding the behavior of
vision models on general images can be very important to prevent biased
judgments and help identify the model's trends and patterns. With the
application of Vision-Language Models, this paper proposes a pipeline to
explain vision models at both the sample and dataset levels. The proposed
pipeline can be used to discover failure cases and gain insights into vision
models with minimal effort, thereby integrating vision model development with
xAI analysis to advance image analysis.

</details>


### [42] [ATMS-KD: Adaptive Temperature and Mixed Sample Knowledge Distillation for a Lightweight Residual CNN in Agricultural Embedded Systems](https://arxiv.org/abs/2508.20232)
*Mohamed Ohamouddou,Said Ohamouddou,Abdellatif El Afia,Rafik Lasri*

Main category: cs.CV

> ATMS-KD框架结合自适应温度调度和混合样本增强，将知识从教师模型（MobileNetV3 Large）转移到轻量级CNN学生模型中，提升了农业环境下轻量级CNN模型的表现。实验表明，该框架显著优于直接训练和现有知识蒸馏方法。

<details>
  <summary>Details</summary>

**Motivation:** 开发适用于农业资源受限环境的轻量级CNN模型。

**Method:** 提出ATMS-KD框架，结合自适应温度调度和混合样本知识蒸馏技术，将MobileNetV3 Large模型的知识转移到轻量级的残差CNN学生模型中。

**Result:** 实验结果表明，所有学生模型的验证集准确率超过96.7%，优于直接训练方法95-96%的准确率；在玫瑰成熟度分类数据集上的表现超越了11种已建立的知识蒸馏方法。

**Conclusion:** ATMS-KD框架能有效转移知识，即使是容量较小的模型也能达到99%以上的知识保留率，并在准确率和延迟时间上均优于现有方法。

**Abstract:** This study proposes ATMS-KD (Adaptive Temperature and Mixed-Sample Knowledge
Distillation), a novel framework for developing lightweight CNN models suitable
for resource-constrained agricultural environments. The framework combines
adaptive temperature scheduling with mixed-sample augmentation to transfer
knowledge from a MobileNetV3 Large teacher model (5.7\,M parameters) to
lightweight residual CNN students. Three student configurations were evaluated:
Compact (1.3\,M parameters), Standard (2.4\,M parameters), and Enhanced (3.8\,M
parameters). The dataset used in this study consists of images of \textit{Rosa
damascena} (Damask rose) collected from agricultural fields in the Dades Oasis,
southeastern Morocco, providing a realistic benchmark for agricultural computer
vision applications under diverse environmental conditions. Experimental
evaluation on the Damascena rose maturity classification dataset demonstrated
significant improvements over direct training methods. All student models
achieved validation accuracies exceeding 96.7\% with ATMS-KD compared to
95--96\% with direct training. The framework outperformed eleven established
knowledge distillation methods, achieving 97.11\% accuracy with the compact
model -- a 1.60 percentage point improvement over the second-best approach
while maintaining the lowest inference latency of 72.19\,ms. Knowledge
retention rates exceeded 99\% for all configurations, demonstrating effective
knowledge transfer regardless of student model capacity.

</details>


### [43] [Linking heterogeneous microstructure informatics with expert characterization knowledge through customized and hybrid vision-language representations for industrial qualification](https://arxiv.org/abs/2508.20243)
*Mutahar Safdar,Gentry Wood,Max Zimmermann,Guy Lamouche,Priti Wanjara,Yaoyao Fiona Zhao*

Main category: cs.CV

> 本文提出了一种新的框架，该框架结合深度语义分割和预训练的多模态模型（CLIP和FLAVA），将微结构图像数据和专家文本评估编码成共享表示，并通过定制的相似度表示实现复杂结构评估的零样本分类，展现出在加性制造材料领域中巨大应用潜力。

<details>
  <summary>Details</summary>

**Motivation:** 目前，先进的材料快速可靠的质量鉴定仍然是工业制造中的瓶颈，特别是在非传统增材制造技术生产的异质结构中。

**Method:** 研究中采用深度语义分割结合预训练多模态模型（CLIP和FLAVA），整合视觉微结构数据和文本专家评估。通过定制相似度模型，该框架能够实现在没有正样本和负样本的情况下，分类之前未见过的微结构。

**Result:** 验证结果显示该框架可以在一系列的鉴定标准下，有效区分可接受的样本和缺陷样本。实验表明，FLAVA模型具有更高的视觉敏感度，而CLIP模型则在文本标准的一致性上表现更好。

**Conclusion:** 此方法通过启用环中的人类决策制定，提高了资格管线中的可追溯性和解释性，无需特定任务的模型重新训练。这项工作通过推动原始数据和专家知识的语义互操作性，有助于在工程信息学中实现可扩展和领域适应的资格策略。

**Abstract:** Rapid and reliable qualification of advanced materials remains a bottleneck
in industrial manufacturing, particularly for heterogeneous structures produced
via non-conventional additive manufacturing processes. This study introduces a
novel framework that links microstructure informatics with a range of expert
characterization knowledge using customized and hybrid vision-language
representations (VLRs). By integrating deep semantic segmentation with
pre-trained multi-modal models (CLIP and FLAVA), we encode both visual
microstructural data and textual expert assessments into shared
representations. To overcome limitations in general-purpose embeddings, we
develop a customized similarity-based representation that incorporates both
positive and negative references from expert-annotated images and their
associated textual descriptions. This allows zero-shot classification of
previously unseen microstructures through a net similarity scoring approach.
Validation on an additively manufactured metal matrix composite dataset
demonstrates the framework's ability to distinguish between acceptable and
defective samples across a range of characterization criteria. Comparative
analysis reveals that FLAVA model offers higher visual sensitivity, while the
CLIP model provides consistent alignment with the textual criteria. Z-score
normalization adjusts raw unimodal and cross-modal similarity scores based on
their local dataset-driven distributions, enabling more effective alignment and
classification in the hybrid vision-language framework. The proposed method
enhances traceability and interpretability in qualification pipelines by
enabling human-in-the-loop decision-making without task-specific model
retraining. By advancing semantic interoperability between raw data and expert
knowledge, this work contributes toward scalable and domain-adaptable
qualification strategies in engineering informatics.

</details>


### [44] [MedNet-PVS: A MedNeXt-Based Deep Learning Model for Automated Segmentation of Perivascular Spaces](https://arxiv.org/abs/2508.20256)
*Zhen Xuen Brandon Low,Rory Zhang,Hang Min,William Pham,Lucy Vivash,Jasmine Moses,Miranda Lynch,Karina Dorfman,Cassandra Marotta,Shaun Koh,Jacob Bunyamin,Ella Rowsthorn,Alex Jarema,Himashi Peiris,Zhaolin Chen,Sandy R. Shultz,David K. Wright,Dexiao Kong,Sharon L. Naismith,Terence J. O'Brien,Ying Xia,Meng Law,Benjamin Sinclair*

Main category: cs.CV

> 论文探索了使用MedNeXt-L-k5进行PVS自动分割，取得了良好的效果，但未在某些情况下超越基准模型。

<details>
  <summary>Details</summary>

**Motivation:** 传统PVS手动分割费时且具有中等的评分者间可靠性，而现有的自动化深度学习模型表现出中等性能，并且通常无法在不同的临床和研究MRI数据集中进行泛化。这篇论文旨在提供一种自动化的解决方案。

**Method:** 使用了基于Transformer的3D编码-解码卷积网络MedNeXt-L-k5来进行自动PVS分割。构建了两个模型：一个使用来自Human Connectome Project-Aging数据集的200个T2加权MRI扫描进行训练，另一个使用来自七个研究中的六个扫描器的40个T1加权MRI体进行训练。

**Result:** 在HCP-Aging数据集的T2w图像上训练的MedNeXt-L-k5模型达到体素级Dice得分0.88±0.06（WM）。在LOSO交叉验证下，模型达到体素级Dice得分0.38±0.16（WM）和0.35±0.12（BG），团簇级Dice得分0.61±0.19（WM）和0.62±0.21（BG）。

**Conclusion:** MedNeXt-L-k5为跨越不同T1w和T2w MRI数据集的PVS自动分割提供了有效解决方案，但未在PVS分割中超越当前基准模型。说明基于Transformer的全局上下文机制在这种任务中可能不是必要的。

**Abstract:** Enlarged perivascular spaces (PVS) are increasingly recognized as biomarkers
of cerebral small vessel disease, Alzheimer's disease, stroke, and
aging-related neurodegeneration. However, manual segmentation of PVS is
time-consuming and subject to moderate inter-rater reliability, while existing
automated deep learning models have moderate performance and typically fail to
generalize across diverse clinical and research MRI datasets. We adapted
MedNeXt-L-k5, a Transformer-inspired 3D encoder-decoder convolutional network,
for automated PVS segmentation. Two models were trained: one using a
homogeneous dataset of 200 T2-weighted (T2w) MRI scans from the Human
Connectome Project-Aging (HCP-Aging) dataset and another using 40 heterogeneous
T1-weighted (T1w) MRI volumes from seven studies across six scanners. Model
performance was evaluated using internal 5-fold cross validation (5FCV) and
leave-one-site-out cross validation (LOSOCV). MedNeXt-L-k5 models trained on
the T2w images of the HCP-Aging dataset achieved voxel-level Dice scores of
0.88+/-0.06 (white matter, WM), comparable to the reported inter-rater
reliability of that dataset, and the highest yet reported in the literature.
The same models trained on the T1w images of the HCP-Aging dataset achieved a
substantially lower Dice score of 0.58+/-0.09 (WM). Under LOSOCV, the model had
voxel-level Dice scores of 0.38+/-0.16 (WM) and 0.35+/-0.12 (BG), and
cluster-level Dice scores of 0.61+/-0.19 (WM) and 0.62+/-0.21 (BG).
MedNeXt-L-k5 provides an efficient solution for automated PVS segmentation
across diverse T1w and T2w MRI datasets. MedNeXt-L-k5 did not outperform the
nnU-Net, indicating that the attention-based mechanisms present in
transformer-inspired models to provide global context are not required for high
accuracy in PVS segmentation.

</details>


### [45] [Plug-in Feedback Self-adaptive Attention in CLIP for Training-free Open-Vocabulary Segmentation](https://arxiv.org/abs/2508.20265)
*Zhixiang Chi,Yanan Wu,Li Gu,Huan Liu,Ziqiang Wang,Yang Zhang,Yang Wang,Konstantinos N. Plataniotis*

Main category: cs.CV

> This paper introduces a self-adaptive framework that enhances CLIP's capabilities by improving the interaction between output predictions and intermediate attention, leading to consistent performance improvements across multiple benchmarks.

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind this work is to address the limitations of the CLIP model, particularly its struggle with open-vocabulary segmentation due to poor localization and semantic discrepancies in intermediate attention, which do not adequately interact with text representations.

**Method:** In this work, the authors propose a training-free, feedback-driven self-adaptive framework that enhances the semantic consistency between internal model representations and final predictions by leveraging output predictions as stronger spatial coherence priors. Key modules include attention isolation, confidence-based pruning for sparse adaptation, and adaptation ensemble.

**Result:** The proposed method is validated across four state-of-the-art approaches with three different backbones. It is shown to consistently improve performance across eight benchmarks, demonstrating the effectiveness of the framework in enhancing semantic consistency and spatial coherence.

**Conclusion:** The study concludes that the proposed feedback-driven self-adaptive framework can effectively improve the visual-textual alignment and performance of CLIP-like models without requiring additional training, making it a versatile enhancement module for various deep learning architectures.

**Abstract:** CLIP exhibits strong visual-textual alignment but struggle with
open-vocabulary segmentation due to poor localization. Prior methods enhance
spatial coherence by modifying intermediate attention. But, this coherence
isn't consistently propagated to the final output due to subsequent operations
such as projections. Additionally, intermediate attention lacks direct
interaction with text representations, such semantic discrepancy limits the
full potential of CLIP.
  In this work, we propose a training-free, feedback-driven self-adaptive
framework that adapts output-based patch-level correspondences back to the
intermediate attention. The output predictions, being the culmination of the
model's processing, encapsulate the most comprehensive visual and textual
semantics about each patch. Our approach enhances semantic consistency between
internal representations and final predictions by leveraging the model's
outputs as a stronger spatial coherence prior. We design key modules, including
attention isolation, confidence-based pruning for sparse adaptation, and
adaptation ensemble, to effectively feedback the output coherence cues. Our
method functions as a plug-in module, seamlessly integrating into four
state-of-the-art approaches with three backbones (ViT-B, ViT-L, ViT-H). We
further validate our framework across multiple attention types (Q-K, self-self,
and Proxy augmented with MAE, SAM, and DINO). Our approach consistently
improves their performance across eight benchmarks.

</details>


### [46] [How Multimodal LLMs Solve Image Tasks: A Lens on Visual Grounding, Task Reasoning, and Answer Decoding](https://arxiv.org/abs/2508.20279)
*Zhuoran Yu,Yong Jae Lee*

Main category: cs.CV

> 研究提出了一个探针框架，通过分析使用MLLMs的各层次处理视觉和文本输入的效果，发现了一个跨多个模型的统一结构，揭示了模型内部的工作原理。

<details>
  <summary>Details</summary>

**Motivation:** 尽管多模态大规模语言模型已在广泛的视觉-语言任务中展示了强大的性能，但其内部处理机制仍然探索不足。研究希望通过系统性分析来揭开MLLMs中不同层的功能角色。

**Method:** 研究训练了线性分类器，从每一层提取的标记嵌入中预测细粒度的视觉类别（如狗的品种）。通过三种类型的受控提示变化进行评估：（1）词法规则变化；（2）语义否定变化；（3）输出格式变化。

**Result:** 研究通过引入一个探针框架，系统性地分析了多模态大规模语言模型（MLLMs）在各层如何处理视觉和文本输入。研究发现了一个稳定但又随着模型架构变化而有所调整的阶段性结构，早期层进行视觉接地，中期层支持词汇整合与语义推理，后期层准备特定任务输出。该框架在不同的视觉标记化、指令微调数据和预训练语料库下保持稳定。

**Conclusion:** 研究提供了一个统一的观点来看待MLLMs的层组织结构，并提出了一个简洁且模型不可知的方法来分析跨模式表示动力学。

**Abstract:** Multimodal Large Language Models (MLLMs) have demonstrated strong performance
across a wide range of vision-language tasks, yet their internal processing
dynamics remain underexplored. In this work, we introduce a probing framework
to systematically analyze how MLLMs process visual and textual inputs across
layers. We train linear classifiers to predict fine-grained visual categories
(e.g., dog breeds) from token embeddings extracted at each layer, using a
standardized anchor question. To uncover the functional roles of different
layers, we evaluate these probes under three types of controlled prompt
variations: (1) lexical variants that test sensitivity to surface-level
changes, (2) semantic negation variants that flip the expected answer by
modifying the visual concept in the prompt, and (3) output format variants that
preserve reasoning but alter the answer format. Applying our framework to
LLaVA-1.5, LLaVA-Next-LLaMA-3, and Qwen2-VL, we identify a consistent
stage-wise structure in which early layers perform visual grounding, middle
layers support lexical integration and semantic reasoning, and final layers
prepare task-specific outputs. We further show that while the overall
stage-wise structure remains stable across variations in visual tokenization,
instruction tuning data, and pretraining corpus, the specific layer allocation
to each stage shifts notably with changes in the base LLM architecture. Our
findings provide a unified perspective on the layer-wise organization of MLLMs
and offer a lightweight, model-agnostic approach for analyzing multimodal
representation dynamics.

</details>


### [47] [Disentangling Latent Embeddings with Sparse Linear Concept Subspaces (SLiCS)](https://arxiv.org/abs/2508.20322)
*Zhi Li,Hau Phan,Matthew Emigh,Austin J. Brockmeier*

Main category: cs.CV

> 本文提出了一种名为SLiCS的方法，通过监督字典学习估计线性合成模型，实现了从视觉-语言共同嵌入网络中分解得到的概念过滤图像检索的精度提升。

<details>
  <summary>Details</summary>

**Motivation:** 我们假设嵌入空间可以被分解成多个概念特定的子空间中的组件向量，以此分离复杂场景的内容信息。目的是提高概念过滤图像检索的精度。

**Method:** 我们提出了一种监督字典学习方法来估计线性综合模型，该模型由字典中向量组的稀疏、非负组合构成，其组活动与多标签信息相匹配。每个特定概念的组成部分是与标签相关的原子的非负组合。该组结构字典通过一种新的交替优化方法进行优化，该方法具有收敛性保证。利用文本共同嵌入，我们详细说明了如何基于文本嵌入找到语义上有意义的描述，这些文本嵌入最好地近似于概念组的原子，并且无监督字典学习可以利用零样本分类来提供实例级别的多标签。

**Result:** 实验结果表明，使用我们提出的稀疏线性概念子空间（SLiCS）提供的解耦嵌入可以实现更精确的概念过滤图像检索和图像到提示的条件生成。我们也展示了SLiCS应用于高度压缩的自动编码器嵌入（如TiTok）和自我监督DINOv2的潜在嵌入上的应用，结果突出显示了概念过滤图像检索精度的提高。

**Conclusion:** SLiCS方法能够提供更精确的概念过滤图像检索。

**Abstract:** Vision-language co-embedding networks, such as CLIP, provide a latent
embedding space with semantic information that is useful for downstream tasks.
We hypothesize that the embedding space can be disentangled to separate the
information on the content of complex scenes by decomposing the embedding into
multiple concept-specific component vectors that lie in different subspaces. We
propose a supervised dictionary learning approach to estimate a linear
synthesis model consisting of sparse, non-negative combinations of groups of
vectors in the dictionary (atoms), whose group-wise activity matches the
multi-label information. Each concept-specific component is a non-negative
combination of atoms associated to a label. The group-structured dictionary is
optimized through a novel alternating optimization with guaranteed convergence.
Exploiting the text co-embeddings, we detail how semantically meaningful
descriptions can be found based on text embeddings of words best approximated
by a concept's group of atoms, and unsupervised dictionary learning can exploit
zero-shot classification of training set images using the text embeddings of
concept labels to provide instance-wise multi-labels. We show that the
disentangled embeddings provided by our sparse linear concept subspaces (SLiCS)
enable concept-filtered image retrieval (and conditional generation using
image-to-prompt) that is more precise. We also apply SLiCS to highly-compressed
autoencoder embeddings from TiTok and the latent embedding from self-supervised
DINOv2. Quantitative and qualitative results highlight the improved precision
of the concept-filtered image retrieval for all embeddings.

</details>


### [48] [MedFoundationHub: A Lightweight and Secure Toolkit for Deploying Medical Vision Language Foundation Models](https://arxiv.org/abs/2508.20345)
*Xiao Li,Yanfan Zhu,Ruining Deng,Wei-Qi Wei,Yu Wang,Shilin Zhao,Yaohong Wang,Haichun Yang,Yuankai Huo*

Main category: cs.CV

> 本文介绍了MedFoundationHub，一个旨在解决医疗视觉语言模型部署中的安全挑战的工具包。评估结果显示这些模型在临床应用中存在一些问题，如精准度和术语一致性问题。

<details>
  <summary>Details</summary>

**Motivation:** 研究动机在于解决医疗VLM带来的安全问题，包括PHI暴露风险、数据泄露和对网络威胁的脆弱性。通过MedFoundationHub，研究人员希望提供一种安全且易于使用的工具，来促进医疗VLM的实际应用。

**Method:** 分析方法主要集中在通过MedFoundationHub工具包对医疗视觉语言模型(VLMs)进行部署和评估。该工具包提供了一个图形用户界面，使医生能够手动选择和使用不同的模型，同时支持工程师以即插即用的方式高效部署医疗VLM，并确保通过Docker容器化操作系统的无差异部署进行隐私保护推理。

**Result:** 研究结果显示了对五种最先进的VLM的评估结果，由经过认证的病理学家对结肠案例和肾脏案例评估，产生了1015个临床专家-模型评分事件。评估揭示了这些模型的常见局限性，例如答非所问、推理模糊和病理术语不一致。

**Conclusion:** 研究得出的结论是，尽管现代医疗VLM展现了巨大的潜力，但在临床应用中仍面临显著的技术和安全挑战，需要进一步完善模型以提高其临床价值和安全性。

**Abstract:** Recent advances in medical vision-language models (VLMs) open up remarkable
opportunities for clinical applications such as automated report generation,
copilots for physicians, and uncertainty quantification. However, despite their
promise, medical VLMs introduce serious security concerns, most notably risks
of Protected Health Information (PHI) exposure, data leakage, and vulnerability
to cyberthreats - which are especially critical in hospital environments. Even
when adopted for research or non-clinical purposes, healthcare organizations
must exercise caution and implement safeguards. To address these challenges, we
present MedFoundationHub, a graphical user interface (GUI) toolkit that: (1)
enables physicians to manually select and use different models without
programming expertise, (2) supports engineers in efficiently deploying medical
VLMs in a plug-and-play fashion, with seamless integration of Hugging Face
open-source models, and (3) ensures privacy-preserving inference through
Docker-orchestrated, operating system agnostic deployment. MedFoundationHub
requires only an offline local workstation equipped with a single NVIDIA A6000
GPU, making it both secure and accessible within the typical resources of
academic research labs. To evaluate current capabilities, we engaged
board-certified pathologists to deploy and assess five state-of-the-art VLMs
(Google-MedGemma3-4B, Qwen2-VL-7B-Instruct, Qwen2.5-VL-7B-Instruct, and
LLaVA-1.5-7B/13B). Expert evaluation covered colon cases and renal cases,
yielding 1015 clinician-model scoring events. These assessments revealed
recurring limitations, including off-target answers, vague reasoning, and
inconsistent pathology terminology.

</details>


### [49] [Enhancing Mamba Decoder with Bidirectional Interaction in Multi-Task Dense Prediction](https://arxiv.org/abs/2508.20376)
*Mang Cao,Sanping Zhou,Yizhe Li,Ye Deng,Wenli Huang,Le Wang*

Main category: cs.CV

> 本文提出了双向交互曼巴（BIM）模型，通过引入双向交互扫描（BI-Scan）和多尺度扫描（MS-Scan）机制来增强多任务密集预测中的跨任务交互，同时保持计算效率。实验表明BIM优于当前的先进方法。

<details>
  <summary>Details</summary>

**Motivation:** 解决现有方法在多任务密集预测中由于需要充分的跨任务交互而导致的计算复杂度高，难以平衡交互完整性和计算效率的问题。

**Method:** 引入了Bidirectional Interaction Scan (BI-Scan) 机制和Multi-Scale Scan (MS-Scan) 机制。BI-Scan采取任务优先和位置优先的扫描模式，以统一的线性复杂度架构构建任务特定的双向序列，保持关键跨任务信息；而MS-Scan实现了多粒度场景建模，同时提升了跨任务特征交互的细腻度。

**Result:** 在NYUD-V2和PASCAL-Context两个极具挑战性的基准测试中，BIM方法相对于其他先进方法表现更优越。

**Conclusion:** 文章通过引入BIM模型，证实了在多任务密集预测中，通过优化交互机制可以在提升跨任务交互的同时保持计算效率的可行性。

**Abstract:** Sufficient cross-task interaction is crucial for success in multi-task dense
prediction. However, sufficient interaction often results in high computational
complexity, forcing existing methods to face the trade-off between interaction
completeness and computational efficiency. To address this limitation, this
work proposes a Bidirectional Interaction Mamba (BIM), which incorporates novel
scanning mechanisms to adapt the Mamba modeling approach for multi-task dense
prediction. On the one hand, we introduce a novel Bidirectional Interaction
Scan (BI-Scan) mechanism, which constructs task-specific representations as
bidirectional sequences during interaction. By integrating task-first and
position-first scanning modes within a unified linear complexity architecture,
BI-Scan efficiently preserves critical cross-task information. On the other
hand, we employ a Multi-Scale Scan~(MS-Scan) mechanism to achieve
multi-granularity scene modeling. This design not only meets the diverse
granularity requirements of various tasks but also enhances nuanced cross-task
feature interactions. Extensive experiments on two challenging benchmarks,
\emph{i.e.}, NYUD-V2 and PASCAL-Context, show the superiority of our BIM vs its
state-of-the-art competitors.

</details>


### [50] [Audio-Guided Visual Editing with Complex Multi-Modal Prompts](https://arxiv.org/abs/2508.20379)
*Hyeonyu Kim,Seokhoon Jeong,Seonghee Han,Chanhyuk Choi,Taehwan Kim*

Main category: cs.CV

> 本文提出了一种新的音频引导的视觉编辑框架，能够处理复杂的多模态编辑任务，并展示了从音频中获取信息以增强视觉编辑能力的优势。

<details>
  <summary>Details</summary>

**Motivation:** 视觉编辑使用扩散模型已经取得了显著的进步，但在仅靠文本难以描述的复杂场景中往往遇到困难。这揭示了需要额外的非文本编辑提示的需求。

**Method:** 我们提出了一种基于音频引导的视觉编辑框架，它可以处理多个文本和音频提示的复杂编辑任务，并且不需要额外的训练。我们利用了一个具有强零样本能力的预训练多模态编码器，并将多样化的音频整合进视觉编辑任务中，以消除音频编码器空间和扩散模型的提示编码器空间之间的差异。此外，我们提出了一种新的方法来处理复杂的多种模态编辑提示，通过我们独立的噪声分支和自适应补丁选择来实现。

**Result:** 我们在各种编辑任务上的综合实验表明，我们的框架在处理复杂编辑场景时表现优异，能够从音频中融入丰富的信息，而文本单独的方法则无法做到这一点。

**Conclusion:** 我们提出的方法展示了在处理复杂编辑场景时使用音频和文本结合的优越性，而不仅仅依靠文本。这项工作解决了当前音频引导的视觉编辑方法在处理多模态编辑提示时的局限性。

**Abstract:** Visual editing with diffusion models has made significant progress but often
struggles with complex scenarios that textual guidance alone could not
adequately describe, highlighting the need for additional non-text editing
prompts. In this work, we introduce a novel audio-guided visual editing
framework that can handle complex editing tasks with multiple text and audio
prompts without requiring additional training. Existing audio-guided visual
editing methods often necessitate training on specific datasets to align audio
with text, limiting their generalization to real-world situations. We leverage
a pre-trained multi-modal encoder with strong zero-shot capabilities and
integrate diverse audio into visual editing tasks, by alleviating the
discrepancy between the audio encoder space and the diffusion model's prompt
encoder space. Additionally, we propose a novel approach to handle complex
scenarios with multiple and multi-modal editing prompts through our separate
noise branching and adaptive patch selection. Our comprehensive experiments on
diverse editing tasks demonstrate that our framework excels in handling
complicated editing scenarios by incorporating rich information from audio,
where text-only approaches fail.

</details>


### [51] [More Reliable Pseudo-labels, Better Performance: A Generalized Approach to Single Positive Multi-label Learning](https://arxiv.org/abs/2508.20381)
*Luong Tran,Thieu Vo,Anh Nguyen,Sang Dinh,Van Nguyen*

Main category: cs.CV

> 本文提出了一种名为AEVLP的框架，包含GPR Loss和DAMP技术，提高了从部分标注数据中学习的多标签分类任务的性能。

<details>
  <summary>Details</summary>

**Motivation:** 由于大规模数据集的完整注释代价高昂，研究从部分注释数据中学习具有重要意义，特别是在每个图像只有一个正确标注标签的情况下。

**Method:** 我们提出了Generalized Pseudo-Label Robust Loss (GPR Loss) 损失函数和Dynamic Augmented Multi-focus Pseudo-labeling (DAMP) 技术，作为一个自适应和高效的视觉-语言伪标签框架（AEVLP）的一部分。

**Result:** 在四个基准数据集上的广泛实验表明，该框架在多标签分类中显著提高了性能，达到了最先进的结果。

**Conclusion:** AEVLP框架有效地从各种伪标签中学习，同时减少了噪声影响，特别是在Single Positive Multi-Label Learning（SPML）场景中。

**Abstract:** Multi-label learning is a challenging computer vision task that requires
assigning multiple categories to each image. However, fully annotating
large-scale datasets is often impractical due to high costs and effort,
motivating the study of learning from partially annotated data. In the extreme
case of Single Positive Multi-Label Learning (SPML), each image is provided
with only one positive label, while all other labels remain unannotated.
Traditional SPML methods that treat missing labels as unknown or negative tend
to yield inaccuracies and false negatives, and integrating various
pseudo-labeling strategies can introduce additional noise. To address these
challenges, we propose the Generalized Pseudo-Label Robust Loss (GPR Loss), a
novel loss function that effectively learns from diverse pseudo-labels while
mitigating noise. Complementing this, we introduce a simple yet effective
Dynamic Augmented Multi-focus Pseudo-labeling (DAMP) technique. Together, these
contributions form the Adaptive and Efficient Vision-Language Pseudo-Labeling
(AEVLP) framework. Extensive experiments on four benchmark datasets demonstrate
that our framework significantly advances multi-label classification, achieving
state-of-the-art results.

</details>


### [52] [Ultra-Low-Latency Spiking Neural Networks with Temporal-Dependent Integrate-and-Fire Neuron Model for Objects Detection](https://arxiv.org/abs/2508.20392)
*Chengjun Zhang,Yuhao Zhang,Jie Yang,Mohamad Sawan*

Main category: cs.CV

> A new method for Spiking Neural Networks (SNNs) to enhance visual detection performance is proposed, introducing a delay-spike approach and tdIF neuron architecture for better temporal feature representation.

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to improve the performance of SNNs in visual detection tasks by resolving the heterogeneity in spiking patterns, which affects feature representation, leading to suboptimal performance compared to classification tasks.

**Method:** The paper introduces a delay-spike method to address the problem of residual membrane potential and proposes a new temporal-dependent Integrate-and-Fire (tdIF) neuron architecture, which allows IF neurons to dynamically adjust their behaviors based on the sequence of time-steps.

**Result:** The tdIF neuron and delay-spike method show better feature representation with fewer time-steps, enabling high performance and low latency. The method outperforms ANN-SNN conversion approaches in two vision tasks: object detection and lane line detection.

**Conclusion:** The tdIF neuron and delay-spike method enable SNNs to achieve state-of-the-art performance in visual detection tasks with ultra-low latency (within 5 time-steps), making significant advancements in the field of SNN applications.

**Abstract:** Spiking Neural Networks (SNNs), inspired by the brain, are characterized by
minimal power consumption and swift inference capabilities on neuromorphic
hardware, and have been widely applied to various visual perception tasks.
Current ANN-SNN conversion methods have achieved excellent results in
classification tasks with ultra-low time-steps, but their performance in visual
detection tasks remains suboptimal. In this paper, we propose a delay-spike
approach to mitigate the issue of residual membrane potential caused by
heterogeneous spiking patterns. Furthermore, we propose a novel
temporal-dependent Integrate-and-Fire (tdIF) neuron architecture for SNNs. This
enables Integrate-and-fire (IF) neurons to dynamically adjust their
accumulation and firing behaviors based on the temporal order of time-steps.
Our method enables spikes to exhibit distinct temporal properties, rather than
relying solely on frequency-based representations. Moreover, the tdIF neuron
maintains energy consumption on par with traditional IF neuron. We demonstrate
that our method achieves more precise feature representation with lower
time-steps, enabling high performance and ultra-low latency in visual detection
tasks. In this study, we conduct extensive evaluation of the tdIF method across
two critical vision tasks: object detection and lane line detection. The
results demonstrate that the proposed method surpasses current ANN-SNN
conversion approaches, achieving state-of-the-art performance with ultra-low
latency (within 5 time-steps).

</details>


### [53] [Graph-Based Uncertainty Modeling and Multimodal Fusion for Salient Object Detection](https://arxiv.org/abs/2508.20415)
*Yuqi Xiong,Wuzhen Shi,Yang Wen,Ruhan Liu*

Main category: cs.CV

> 本文提出了一种动态不确定性传播和多模态协作推理网络(DUP-MCRNet)，解决了现有显着目标检测方法容易丢失细节、模糊边缘及单一模态信息融合不足的问题。

<details>
  <summary>Details</summary>

**Motivation:** 旨在解决现有显着目标检测方法在复杂场景中易出现细节丢失、边缘模糊以及单一模态信息融合不足的问题，以提高显着目标检测性能。

**Method:** 设计了动态不确定性图卷积模块(DUGC)来通过基于空间语义距离的稀疏图在层间传播不确定性，并结合通道自适应互作用。提出了多模态协作融合策略(MCF)，通过可学习模态门权重加权融合RGB、深度和边缘特征，根据不同场景动态调整各模态的重要性。

**Result:** DUP-MCRNet在大多数常见基准数据集上的显着目标检测性能要优于各种现有方法，尤其是在边缘清晰度和对复杂背景的鲁棒性方面表现突出。

**Conclusion:** 所提出的DUP-MCRNet能够更准确和鲁棒地检测显着目标，特别适合于复杂背景下的边缘和细小结构的检测。

**Abstract:** In view of the problems that existing salient object detection (SOD) methods
are prone to losing details, blurring edges, and insufficient fusion of
single-modal information in complex scenes, this paper proposes a dynamic
uncertainty propagation and multimodal collaborative reasoning network
(DUP-MCRNet). Firstly, a dynamic uncertainty graph convolution module (DUGC) is
designed to propagate uncertainty between layers through a sparse graph
constructed based on spatial semantic distance, and combined with channel
adaptive interaction, it effectively improves the detection accuracy of small
structures and edge regions. Secondly, a multimodal collaborative fusion
strategy (MCF) is proposed, which uses learnable modality gating weights to
weightedly fuse the attention maps of RGB, depth, and edge features. It can
dynamically adjust the importance of each modality according to different
scenes, effectively suppress redundant or interfering information, and
strengthen the semantic complementarity and consistency between
cross-modalities, thereby improving the ability to identify salient regions
under occlusion, weak texture or background interference. Finally, the
detection performance at the pixel level and region level is optimized through
multi-scale BCE and IoU loss, cross-scale consistency constraints, and
uncertainty-guided supervision mechanisms. Extensive experiments show that
DUP-MCRNet outperforms various SOD methods on most common benchmark datasets,
especially in terms of edge clarity and robustness to complex backgrounds. Our
code is publicly available at https://github.com/YukiBear426/DUP-MCRNet.

</details>


### [54] [MSMVD: Exploiting Multi-scale Image Features via Multi-scale BEV Features for Multi-view Pedestrian Detection](https://arxiv.org/abs/2508.20447)
*Taiga Yamane,Satoshi Suzuki,Ryo Masumura,Shota Orihashi,Tomohiro Tanaka,Mana Ihori,Naoki Makishima,Naotaka Kawata*

Main category: cs.CV

> The paper presents MSMVD, a new method to improve pedestrian detection in multi-view images by creating multi-scale bird's eye view features and combining them using a feature pyramid network, which significantly boosts detection accuracy over previous methods on the GMVD dataset.

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind this research is to improve the performance of Multi-View Pedestrian Detection (MVPD) systems. Current deep learning approaches struggle with detecting pedestrians that are consistently small or large in scale across multiple views due to their inability to efficiently use multi-scale image features.

**Method:** The paper proposes a method called Multi-Scale Multi-View Detection (MSMVD). MSMVD generates multi-scale bird’s eye view (BEV) features by projecting multi-scale image features from each view. The combination of multi-scale BEV features through a feature pyramid network enhances pedestrian detection across different scales.

**Result:** Experiments show that MSMVD effectively improves pedestrian detection performance by utilizing multi-scale image features, leading to a 4.5 point increase in MODA compared to the previous best results on the GMVD dataset.

**Conclusion:** The research concludes that leveraging multi-scale BEV features to detect pedestrians across various scales in multi-view images is effective, with MSMVD demonstrating superior performance to existing methods on the GMVD dataset.

**Abstract:** Multi-View Pedestrian Detection (MVPD) aims to detect pedestrians in the form
of a bird's eye view (BEV) from multi-view images. In MVPD, end-to-end
trainable deep learning methods have progressed greatly. However, they often
struggle to detect pedestrians with consistently small or large scales in views
or with vastly different scales between views. This is because they do not
exploit multi-scale image features to generate the BEV feature and detect
pedestrians. To overcome this problem, we propose a novel MVPD method, called
Multi-Scale Multi-View Detection (MSMVD). MSMVD generates multi-scale BEV
features by projecting multi-scale image features extracted from individual
views into the BEV space, scale-by-scale. Each of these BEV features inherits
the properties of its corresponding scale image features from multiple views.
Therefore, these BEV features help the precise detection of pedestrians with
consistently small or large scales in views. Then, MSMVD combines information
at different scales of multiple views by processing the multi-scale BEV
features using a feature pyramid network. This improves the detection of
pedestrians with vastly different scales between views. Extensive experiments
demonstrate that exploiting multi-scale image features via multi-scale BEV
features greatly improves the detection performance, and MSMVD outperforms the
previous highest MODA by $4.5$ points on the GMVD dataset.

</details>


### [55] [A Spatial-Frequency Aware Multi-Scale Fusion Network for Real-Time Deepfake Detection](https://arxiv.org/abs/2508.20449)
*Libo Lv,Tianyi Wang,Mengxiao Huang,Ruixia Liu,Yinglong Wang*

Main category: cs.CV

> Error

<details>
  <summary>Details</summary>

**Motivation:** Error

**Method:** Error

**Result:** Error

**Conclusion:** Error

**Abstract:** With the rapid advancement of real-time deepfake generation techniques,
forged content is becoming increasingly realistic and widespread across
applications like video conferencing and social media. Although
state-of-the-art detectors achieve high accuracy on standard benchmarks, their
heavy computational cost hinders real-time deployment in practical
applications. To address this, we propose the Spatial-Frequency Aware
Multi-Scale Fusion Network (SFMFNet), a lightweight yet effective architecture
for real-time deepfake detection. We design a spatial-frequency hybrid aware
module that jointly leverages spatial textures and frequency artifacts through
a gated mechanism, enhancing sensitivity to subtle manipulations. A
token-selective cross attention mechanism enables efficient multi-level feature
interaction, while a residual-enhanced blur pooling structure helps retain key
semantic cues during downsampling. Experiments on several benchmark datasets
show that SFMFNet achieves a favorable balance between accuracy and efficiency,
with strong generalization and practical value for real-time applications.

</details>


### [56] [Dual-Model Weight Selection and Self-Knowledge Distillation for Medical Image Classification](https://arxiv.org/abs/2508.20461)
*Ayaka Tsutsumi,Guang Li,Ren Togo,Takahiro Ogawa,Satoshi Kondo,Miki Haseyama*

Main category: cs.CV

> 该研究提出了一种新的医学图像分类方法，结合了双模型权重选择和自我知识蒸馏方法，可以有效解决医疗环境中的计算资源限制问题，取得了优越的性能。

<details>
  <summary>Details</summary>

**Motivation:** 在实际医疗环境中，大型模型的部署受到计算资源的限制，因此开发轻量级模型以便达到大型模型的性能同时保持计算效率是必要的。

**Method:** 使用双模型权重选择策略，初始化两个轻量级模型权重来自预训练的大模型，然后应用自我知识蒸馏，最后微调模型以适应分类任务。

**Result:** 在公开数据集包括胸部X光图像，肺部CT扫描和脑部MRI扫描上进行广泛的实验，证明了本方法的优越性能和稳健性优于现有方法。

**Conclusion:** 结合双模型权重选择与自我知识蒸馏，本方法克服了传统方法的局限，即在紧凑模型中难以保留关键信息的问题。

**Abstract:** We propose a novel medical image classification method that integrates
dual-model weight selection with self-knowledge distillation (SKD). In
real-world medical settings, deploying large-scale models is often limited by
computational resource constraints, which pose significant challenges for their
practical implementation. Thus, developing lightweight models that achieve
comparable performance to large-scale models while maintaining computational
efficiency is crucial. To address this, we employ a dual-model weight selection
strategy that initializes two lightweight models with weights derived from a
large pretrained model, enabling effective knowledge transfer. Next, SKD is
applied to these selected models, allowing the use of a broad range of initial
weight configurations without imposing additional excessive computational cost,
followed by fine-tuning for the target classification tasks. By combining
dual-model weight selection with self-knowledge distillation, our method
overcomes the limitations of conventional approaches, which often fail to
retain critical information in compact models. Extensive experiments on
publicly available datasets-chest X-ray images, lung computed tomography scans,
and brain magnetic resonance imaging scans-demonstrate the superior performance
and robustness of our approach compared to existing methods.

</details>


### [57] [Re-Densification Meets Cross-Scale Propagation: Real-Time Compression of LiDAR Point Clouds](https://arxiv.org/abs/2508.20466)
*Pengpeng Yu,Haoran Li,Dingquan Li,Runqing Jiang,Jing Wang,Liang Lin,Yulan Guo*

Main category: cs.CV

> 提出了一种用于LiDAR点云压缩的新框架，该框架包含两个轻量级模块，能够高效地进行特征提取和跨尺度信息共享，实现了低计算成本和高性能，适用于各种LiDAR点云应用。

<details>
  <summary>Details</summary>

**Motivation:** 现有方法将无序点转化为分层八叉树或体素结构以实现密集到稀疏的预测编码，然而几何细节的极端稀疏性阻碍了有效上下文建模，从而限制了其压缩性能与速度。我们提出通过生成紧凑的特征来进行高效的预测编码。

**Method:** 我们的方法包括两个轻量级模块：首先，几何重密化模块重密化编码的稀疏几何结构，在更密集的尺度上提取特征，然后稀疏化这些特征以进行预测编码。此模块避免了在高度稀疏细节上的昂贵计算，同时保持了轻量级的预测头。其次，跨尺度特征传播模块利用多个分辨率级别上的占用提示来指导分层特征传播。这种设计有利于跨尺度的信息共享，从而减少冗余特征提取，并为几何重密化模块提供丰富的特征表示。通过整合这两个模块，我们的方法生成了一个紧凑的特征表示，该表示能够提高上下文建模效率并加速编码过程。

**Result:** 实验表明，我们的方法在KITTI数据集上实现了最领先的压缩比和实时性能，编码和解码速度均为26 FPS。代码可以在https://github.com/pengpeng-yu/FastPCC获得。

**Conclusion:** 实验结果表明，该方法在KITTI数据集上实现了最先进的压缩比和实时性能，实现了12位量化下编码和解码均为26 FPS的性能。

**Abstract:** LiDAR point clouds are fundamental to various applications, yet
high-precision scans incur substantial storage and transmission overhead.
Existing methods typically convert unordered points into hierarchical octree or
voxel structures for dense-to-sparse predictive coding. However, the extreme
sparsity of geometric details hinders efficient context modeling, thereby
limiting their compression performance and speed. To address this challenge, we
propose to generate compact features for efficient predictive coding. Our
framework comprises two lightweight modules. First, the Geometry
Re-Densification Module re-densifies encoded sparse geometry, extracts features
at denser scale, and then re-sparsifies the features for predictive coding.
This module avoids costly computation on highly sparse details while
maintaining a lightweight prediction head. Second, the Cross-scale Feature
Propagation Module leverages occupancy cues from multiple resolution levels to
guide hierarchical feature propagation. This design facilitates information
sharing across scales, thereby reducing redundant feature extraction and
providing enriched features for the Geometry Re-Densification Module. By
integrating these two modules, our method yields a compact feature
representation that provides efficient context modeling and accelerates the
coding process. Experiments on the KITTI dataset demonstrate state-of-the-art
compression ratios and real-time performance, achieving 26 FPS for both
encoding and decoding at 12-bit quantization. Code is available at
https://github.com/pengpeng-yu/FastPCC.

</details>


### [58] [Droplet3D: Commonsense Priors from Videos Facilitate 3D Generation](https://arxiv.org/abs/2508.20470)
*Xiaochuan Li,Guoguang Du,Runze Zhang,Liang Jin,Qi Jia,Lihua Lu,Zhenhua Guo,Yaqian Zhao,Haiyang Liu,Tianqi Wang,Changsheng Li,Xiaoli Gong,Rengang Li,Baoyu Fan*

Main category: cs.CV

> 该论文提出利用带有多视角标注的大规模视频数据集Droplet3D-4M和训练的Droplet3D生成模型，以解决3D领域数据稀缺的问题，方法通过视频中的空间一致性和丰富的语义信息，生成了空间一致且语义合理的内容，该方法有潜力扩展到场景级别的应用，且所有资源已开源。

<details>
  <summary>Details</summary>

**Motivation:** 由于3D数据在网络上的稀缺性限制了该领域的发展，文章希望通过视频中的多视角信息和丰富语义，为3D生成提供一种新的监督信号，以克服数据不足导致的泛化瓶颈。

**Method:** 文章创建了第一个大规模多视角级别的视频数据集Droplet3D-4M，并训练了生成模型Droplet3D，该模型支持图像和密集文本输入。

**Result:** 大量的实验结果验证了这种方法的有效性，表明该方法能够生成空间一致且语义合理的内容。

**Conclusion:** 与现今主流的3D解决方案相比，该方法显示出了向场景级别应用扩展的潜力，从而证明视频中的常识性先验知识对于3D内容的生成非常有帮助。

**Abstract:** Scaling laws have validated the success and promise of large-data-trained
models in creative generation across text, image, and video domains. However,
this paradigm faces data scarcity in the 3D domain, as there is far less of it
available on the internet compared to the aforementioned modalities.
Fortunately, there exist adequate videos that inherently contain commonsense
priors, offering an alternative supervisory signal to mitigate the
generalization bottleneck caused by limited native 3D data. On the one hand,
videos capturing multiple views of an object or scene provide a spatial
consistency prior for 3D generation. On the other hand, the rich semantic
information contained within the videos enables the generated content to be
more faithful to the text prompts and semantically plausible. This paper
explores how to apply the video modality in 3D asset generation, spanning
datasets to models. We introduce Droplet3D-4M, the first large-scale video
dataset with multi-view level annotations, and train Droplet3D, a generative
model supporting both image and dense text input. Extensive experiments
validate the effectiveness of our approach, demonstrating its ability to
produce spatially consistent and semantically plausible content. Moreover, in
contrast to the prevailing 3D solutions, our approach exhibits the potential
for extension to scene-level applications. This indicates that the commonsense
priors from the videos significantly facilitate 3D creation. We have
open-sourced all resources including the dataset, code, technical framework,
and model weights: https://dropletx.github.io/.

</details>


### [59] [Realistic and Controllable 3D Gaussian-Guided Object Editing for Driving Video Generation](https://arxiv.org/abs/2508.20471)
*Jiusi Li,Jackson Jiang,Jinyu Miao,Miao Long,Tuopu Wen,Peijin Jia,Shengxiang Liu,Chunlei Yu,Maolin Liu,Yuzhan Cai,Kun Jiang,Mengmeng Yang,Diange Yang*

Main category: cs.CV

> 本文提出了G^2Editor框架，通过创新的方法对驾驶视频中的对象进行编辑，实现了更好的视觉效果和精确的姿态控制。

<details>
  <summary>Details</summary>

**Motivation:** 自动驾驶系统的训练和验证过程中，角案例数据非常重要，但直接从现实世界获取这类数据成本高昂且危险。因此，编辑采集到的传感器数据来生成多样化的场景成为有效的替代方法，但这种方法通常面临视觉保真度低或姿态控制不精确的问题。本研究旨在通过提出的新框架解决这些问题。

**Method:** 该论文提出了一种名为G^2Editor的框架，用于对驾驶视频中对象的编辑，实现在保真性和精确度上的提升。具体通过3D高斯表示编辑对象作为稠密的先验知识，输入到去噪过程中，以确保准确的姿态控制和空间一致性。同时采用场景级的3D边界框布局重建被遮挡的非目标对象区域，并引入分层细粒度特征来指导编辑对象的外观细节。

**Result:** 实验结果表明，G^2Editor框架能够有效支持对象重新定位、插入和删除，且在Waymo开放数据集上的表现优于现有方法，在姿态控制能力和视觉质量方面都有显著提高，同时也为数据驱动任务提供了帮助。

**Conclusion:** G^2Editor的应用不仅提高了驾驶员视频对象编辑的准确性和视觉保真度，还为基于数据的任务提供了强大的支持。

**Abstract:** Corner cases are crucial for training and validating autonomous driving
systems, yet collecting them from the real world is often costly and hazardous.
Editing objects within captured sensor data offers an effective alternative for
generating diverse scenarios, commonly achieved through 3D Gaussian Splatting
or image generative models. However, these approaches often suffer from limited
visual fidelity or imprecise pose control. To address these issues, we propose
G^2Editor, a framework designed for photorealistic and precise object editing
in driving videos. Our method leverages a 3D Gaussian representation of the
edited object as a dense prior, injected into the denoising process to ensure
accurate pose control and spatial consistency. A scene-level 3D bounding box
layout is employed to reconstruct occluded areas of non-target objects.
Furthermore, to guide the appearance details of the edited object, we
incorporate hierarchical fine-grained features as additional conditions during
generation. Experiments on the Waymo Open Dataset demonstrate that G^2Editor
effectively supports object repositioning, insertion, and deletion within a
unified framework, outperforming existing methods in both pose controllability
and visual quality, while also benefiting downstream data-driven tasks.

</details>


### [60] [Enhancing Corpus Callosum Segmentation in Fetal MRI via Pathology-Informed Domain Randomization](https://arxiv.org/abs/2508.20475)
*Marina Grifell i Plana,Vladyslav Zalevskyi,Léa Schmidt,Yvan Gomez,Thomas Sanchez,Vincent Dunet,Mériam Koob,Vanessa Siffredi,Meritxell Bach Cuadra*

Main category: cs.CV

> The paper introduces a domain randomization strategy informed by the pathology of corpus callosum dysgenesis to improve fetal brain segmentation in rare disease cases.

<details>
  <summary>Details</summary>

**Motivation:** To enhance deep learning model's generalization for fetal brain segmentation in cases of rare diseases like corpus callosum dysgenesis (CCD), where annotated data is scarce.

**Method:** The authors propose a pathology-informed domain randomization strategy that simulates diverse brain changes related to CCD from healthy data alone, allowing the generation of synthetic data without needing pathological annotations.

**Result:** The proposed method improved segmentation performance on CCD cases while maintaining accuracy on healthy cases and those with other pathologies. It also reduced estimation errors for clinically relevant biomarkers like corpus callosum length.

**Conclusion:** Incorporating domain-specific anatomical priors into synthetic data pipelines can mitigate data scarcity issues and enhance analysis in cases of rare but clinically significant malformations.

**Abstract:** Accurate fetal brain segmentation is crucial for extracting biomarkers and
assessing neurodevelopment, especially in conditions such as corpus callosum
dysgenesis (CCD), which can induce drastic anatomical changes. However, the
rarity of CCD severely limits annotated data, hindering the generalization of
deep learning models. To address this, we propose a pathology-informed domain
randomization strategy that embeds prior knowledge of CCD manifestations into a
synthetic data generation pipeline. By simulating diverse brain alterations
from healthy data alone, our approach enables robust segmentation without
requiring pathological annotations.
  We validate our method on a cohort comprising 248 healthy fetuses, 26 with
CCD, and 47 with other brain pathologies, achieving substantial improvements on
CCD cases while maintaining performance on both healthy fetuses and those with
other pathologies. From the predicted segmentations, we derive clinically
relevant biomarkers, such as corpus callosum length (LCC) and volume, and show
their utility in distinguishing CCD subtypes. Our pathology-informed
augmentation reduces the LCC estimation error from 1.89 mm to 0.80 mm in
healthy cases and from 10.9 mm to 0.7 mm in CCD cases. Beyond these
quantitative gains, our approach yields segmentations with improved topological
consistency relative to available ground truth, enabling more reliable
shape-based analyses. Overall, this work demonstrates that incorporating
domain-specific anatomical priors into synthetic data pipelines can effectively
mitigate data scarcity and enhance analysis of rare but clinically significant
malformations.

</details>


### [61] [Towards Inclusive Communication: A Unified LLM-Based Framework for Sign Language, Lip Movements, and Audio Understanding](https://arxiv.org/abs/2508.20476)
*Jeong Hun Yeo,Hyeongseop Rha,Sungjune Park,Junil Won,Yong Man Ro*

Main category: cs.CV

> 本研究提出了一种能够处理手语、唇部动作和语音的统一框架，用于生成口语文本。研究显示，分离式处理唇部动作可以显著提高手语翻译的表现，总体性能优于或与当前最先进的单项任务模型持平。

<details>
  <summary>Details</summary>

**Motivation:** 现有的语音识别技术主要依赖于音频，这对于聋人或听力不佳的人群来说是不友好的。尽管手语和唇读提供了有效的替代方案，但这些模态通常是孤立研究的，它们的整合尚未得到充分探索。因此，本研究旨在开发一个统一的框架，改善无音频通信，并探索不同模态之间的协同效应。

**Method:** 本研究提出了一种统一的框架，能够处理多种模态的输入，包括手语、唇部动作和语音，并生成口语文本。该架构设计为模态无关，能够有效处理异构输入。研究同时探索了不同模态之间的协同效应，特别是唇部动作作为手语理解中的非手动线索的作用。

**Result:** 研究结果显示，该框架在手语翻译、视觉语音识别、自动语音识别和联合语音识别方面，表现出与或优于专门针对单个任务的先进模型的性能。特别地，明确了作为独立模态的唇部动作在提高手语翻译表现上的重要性。

**Conclusion:** 该统一框架成功地整合了多种通信方式，并展现出优秀的跨模态处理能力和比专业化模型更好的性能，特别是在手语翻译任务中。

**Abstract:** Audio is the primary modality for human communication and has driven the
success of Automatic Speech Recognition (ASR) technologies. However, such
systems remain inherently inaccessible to individuals who are deaf or hard of
hearing. Visual alternatives such as sign language and lip reading offer
effective substitutes, and recent advances in Sign Language Translation (SLT)
and Visual Speech Recognition (VSR) have improved audio-less communication.
Yet, these modalities have largely been studied in isolation, and their
integration within a unified framework remains underexplored. In this paper, we
introduce the first unified framework capable of handling diverse combinations
of sign language, lip movements, and audio for spoken-language text generation.
We focus on three main objectives: (i) designing a unified, modality-agnostic
architecture capable of effectively processing heterogeneous inputs; (ii)
exploring the underexamined synergy among modalities, particularly the role of
lip movements as non-manual cues in sign language comprehension; and (iii)
achieving performance on par with or superior to state-of-the-art models
specialized for individual tasks. Building on this framework, we achieve
performance on par with or better than task-specific state-of-the-art models
across SLT, VSR, ASR, and AVSR. Furthermore, our analysis reveals that
explicitly modeling lip movements as a separate modality significantly improves
SLT performance.

</details>


### [62] [Video-MTR: Reinforced Multi-Turn Reasoning for Long Video Understanding](https://arxiv.org/abs/2508.20478)
*Yuan Xie,Tianshui Chen,Zheng Ge,Lionel Ni*

Main category: cs.CV

> This paper introduces Video-MTR, an innovative method for long-form video understanding utilizing multi-turn reasoning and a reinforced reward system to improve performance over existing techniques.

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind Video-MTR is to overcome the challenges posed by long-range temporal dependencies and multiple events in videos, shortcomings of external VLMs, and the need for better end-to-end training methods for improved performance.

**Method:** Unlike traditional methods, Video-MTR introduces an iterative key video segment selection and question comprehension process, using multi-turn reasoning framework. This framework is supported by a novel gated bi-level reward system which evaluates based on both trajectory-level answer correctness and turn-level frame-query relevance.

**Result:** Video-MTR has shown superior performance in accuracy and efficiency compared to existing methods, as shown through extensive experiments on benchmarks like VideoMME, MLVU, and EgoSchema.

**Conclusion:** This research advances the state-of-the-art in the field of long-form video understanding by presenting Video-MTR, a method that not only improves accuracy and efficiency but also eliminates the need for external VLMs.

**Abstract:** Long-form video understanding, characterized by long-range temporal
dependencies and multiple events, remains a challenge. Existing methods often
rely on static reasoning or external visual-language models (VLMs), which face
issues like complexity and sub-optimal performance due to the lack of
end-to-end training. In this paper, we propose Video-MTR, a reinforced
multi-turn reasoning framework designed to enable iterative key video segment
selection and question comprehension. Unlike traditional video reasoning
pipeline, which generate predictions in a single turn, Video-MTR performs
reasoning in multiple turns, selecting video segments progressively based on
the evolving understanding of previously processed segments and the current
question. This iterative process allows for a more refined and contextually
aware analysis of the video. To ensure intermediate reasoning process, we
introduce a novel gated bi-level reward system, combining trajectory-level
rewards based on answer correctness and turn-level rewards emphasizing
frame-query relevance. This system optimizes both video segment selection and
question comprehension, eliminating the need for external VLMs and allowing
end-to-end training. Extensive experiments on benchmarks like VideoMME, MLVU,
and EgoSchema demonstrate that Video-MTR outperforms existing methods in both
accuracy and efficiency, advancing the state-of-the-art in long video
understanding.

</details>


### [63] [Adaptive Dual Uncertainty Optimization: Boosting Monocular 3D Object Detection under Test-Time Shifts](https://arxiv.org/abs/2508.20488)
*Zixuan Hu,Dongxiao Li,Xinzhu Ma,Shixiang Tang,Xiaotong Li,Wenhan Yang,Ling-Yu Duan*

Main category: cs.CV

> 本文提出了Dual Uncertainty Optimization (DUO)框架，以改善单目3D物体检测在面对环境和传感器变动时的性能，实现同时处理语义和几何不确定性的能力，实验表明此方法优越于现有方法。

<details>
  <summary>Details</summary>

**Motivation:** 传统的TTA方法仅关注不确定性与泛化能力之间的正相关，忽视了单目3D目标检测中的双不确定性问题。DUO应对这一挑战，通过开发新的方法使模型能够更精确地适应目标分布，提高在实际操作中的可靠性。

**Method:** 本研究提出了Dual Uncertainty Optimization (DUO)，作为首个针对单目3D物体检测的测试时间自适应(TTA)框架，旨在同时最小化语义不确定性和几何不确定性。DUO通过凸优化的视角，引入了focal loss的凸结构，并开发了无监督版本，实现无标签的不确定性加权和高不确定度对象的平衡学习。同时，设计了一种考虑语义意识的法线场约束，减少由于不稳定的3D表示导致的不确定性。

**Result:** 实验结果表明，在多种数据集和领域转移类型中，DUO优于现有方法，显著提高了单目3D物体检测的可靠性。

**Conclusion:** 研究提出的DUO框架为单目3D目标检测提供了一种新的解决策略，通过双重优化不确定性，实现了在多种环境和传感器变化下的持续准确检测。

**Abstract:** Accurate monocular 3D object detection (M3OD) is pivotal for safety-critical
applications like autonomous driving, yet its reliability deteriorates
significantly under real-world domain shifts caused by environmental or sensor
variations. To address these shifts, Test-Time Adaptation (TTA) methods have
emerged, enabling models to adapt to target distributions during inference.
While prior TTA approaches recognize the positive correlation between low
uncertainty and high generalization ability, they fail to address the dual
uncertainty inherent to M3OD: semantic uncertainty (ambiguous class
predictions) and geometric uncertainty (unstable spatial localization). To
bridge this gap, we propose Dual Uncertainty Optimization (DUO), the first TTA
framework designed to jointly minimize both uncertainties for robust M3OD.
Through a convex optimization lens, we introduce an innovative convex structure
of the focal loss and further derive a novel unsupervised version, enabling
label-agnostic uncertainty weighting and balanced learning for high-uncertainty
objects. In parallel, we design a semantic-aware normal field constraint that
preserves geometric coherence in regions with clear semantic cues, reducing
uncertainty from the unstable 3D representation. This dual-branch mechanism
forms a complementary loop: enhanced spatial perception improves semantic
classification, and robust semantic predictions further refine spatial
understanding. Extensive experiments demonstrate the superiority of DUO over
existing methods across various datasets and domain shift types.

</details>


### [64] [CaddieSet: A Golf Swing Dataset with Human Joint Features and Ball Information](https://arxiv.org/abs/2508.20491)
*Seunghyeon Jung,Seoyoung Hong,Jiwoo Jeong,Seungwon Jeong,Jaerim Choi,Hoki Kim,Woojin Lee*

Main category: cs.CV

> 我们提出了一个新的数据集CaddieSet，并验证了使用关节特征的挥杆反馈与现有的域知识在定量上是一致的，有助于高尔夫挥杆分析。

<details>
  <summary>Details</summary>

**Motivation:** 尽管在深度学习方面取得了进展，但现有研究未能定量地建立挥杆姿势与球飞行轨迹之间的关系，限制了它们为高尔夫球手提供挥杆改进所需的洞察力。

**Method:** 我们提出了一个名为CaddieSet的新数据集，该数据集包含单一击球的关节信息和各种球信息。CaddieSet通过计算机视觉方法将单个挥杆视频分割成八个挥杆阶段来提取关节信息。此外，基于专家高尔夫领域的知识，我们定义了15个关键指标，这些指标影响高尔夫挥杆，能够通过与挥杆相关的特征解释挥杆结果。

**Result:** 通过实验，我们证明了CaddieSet在预测球飞行轨迹方面是可行的，并且通过几种基准方法中的可解释模型验证了使用我们的关节特征的挥杆反馈与现有领域的知识在定量上是一致的。

**Conclusion:** 这项工作有望为高尔夫挥杆分析提供新的视角，不仅对学术界而且对体育产业都有积极影响。

**Abstract:** Recent advances in deep learning have led to more studies to enhance golfers'
shot precision. However, these existing studies have not quantitatively
established the relationship between swing posture and ball trajectory,
limiting their ability to provide golfers with the necessary insights for swing
improvement. In this paper, we propose a new dataset called CaddieSet, which
includes joint information and various ball information from a single shot.
CaddieSet extracts joint information from a single swing video by segmenting it
into eight swing phases using a computer vision-based approach. Furthermore,
based on expert golf domain knowledge, we define 15 key metrics that influence
a golf swing, enabling the interpretation of swing outcomes through
swing-related features. Through experiments, we demonstrated the feasibility of
CaddieSet for predicting ball trajectories using various benchmarks. In
particular, we focus on interpretable models among several benchmarks and
verify that swing feedback using our joint features is quantitatively
consistent with established domain knowledge. This work is expected to offer
new insight into golf swing analysis for both academia and the sports industry.

</details>
