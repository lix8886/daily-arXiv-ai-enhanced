{"id": "2508.20201", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.20201", "abs": "https://arxiv.org/abs/2508.20201", "authors": ["Lance Calvin Lim Gamboa", "Yue Feng", "Mark Lee"], "title": "Social Bias in Multilingual Language Models: A Survey", "comment": "Accepted into EMNLP 2025 Main Conference", "summary": "Pretrained multilingual models exhibit the same social bias as models\nprocessing English texts. This systematic review analyzes emerging research\nthat extends bias evaluation and mitigation approaches into multilingual and\nnon-English contexts. We examine these studies with respect to linguistic\ndiversity, cultural awareness, and their choice of evaluation metrics and\nmitigation techniques. Our survey illuminates gaps in the field's dominant\nmethodological design choices (e.g., preference for certain languages, scarcity\nof multilingual mitigation experiments) while cataloging common issues\nencountered and solutions implemented in adapting bias benchmarks across\nlanguages and cultures. Drawing from the implications of our findings, we chart\ndirections for future research that can reinforce the multilingual bias\nliterature's inclusivity, cross-cultural appropriateness, and alignment with\nstate-of-the-art NLP advancements.", "AI": {"tldr": "本文通过系统回顾发现多语言预训练模型存在社会偏见，提出了未来研究方向以增强文献的包容性、跨文化适宜性和技术先进性。", "motivation": "研究的动机在于分析在多语言和非英语环境下存在的偏见评估及缓解方法，发现研究领域的局限性，并为未来的多元语言偏见研究提供建议。", "method": "本文通过系统的文献回顾方法，分析了将偏见评估和缓解方法扩展到多种语言和非英语环境的研究。", "result": "研究揭示了该领域在一些建设性设计方案上的不足，包括偏好的语言选择、缺乏多语言缓解实验等，同时记录了跨语言和文化适配偏见基准时常见问题和解决方法。", "conclusion": "研究表明，预训练的多语言模型也表现出与处理英语文本的模型相同的社会偏见，并提出了未来研究的方向，以增强多语言偏见文献的包容性、跨文化适宜性和与最先进的NLP进展的一致性。"}}
{"id": "2508.20217", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.20217", "abs": "https://arxiv.org/abs/2508.20217", "authors": ["Mohammad Amini", "Babak Ahmadi", "Xiaomeng Xiong", "Yilin Zhang", "Christopher Qiao"], "title": "Prompting Strategies for Language Model-Based Item Generation in K-12 Education: Bridging the Gap Between Small and Large Language Models", "comment": null, "summary": "This study explores automatic generation (AIG) using language models to\ncreate multiple choice questions (MCQs) for morphological assessment, aiming to\nreduce the cost and inconsistency of manual test development. The study used a\ntwo-fold approach. First, we compared a fine-tuned medium model (Gemma, 2B)\nwith a larger untuned one (GPT-3.5, 175B). Second, we evaluated seven\nstructured prompting strategies, including zero-shot, few-shot,\nchain-of-thought, role-based, sequential, and combinations. Generated items\nwere assessed using automated metrics and expert scoring across five\ndimensions. We also used GPT-4.1, trained on expert-rated samples, to simulate\nhuman scoring at scale. Results show that structured prompting, especially\nstrategies combining chain-of-thought and sequential design, significantly\nimproved Gemma's outputs. Gemma generally produced more construct-aligned and\ninstructionally appropriate items than GPT-3.5's zero-shot responses, with\nprompt design playing a key role in mid-size model performance. This study\ndemonstrates that structured prompting and efficient fine-tuning can enhance\nmidsized models for AIG under limited data conditions. We highlight the value\nof combining automated metrics, expert judgment, and large-model simulation to\nensure alignment with assessment goals. The proposed workflow offers a\npractical and scalable way to develop and validate language assessment items\nfor K-12.", "AI": {"tldr": "研究通过对比模型和评估不同提示策略，旨在使用语言模型自动生成多项选择题，以减少人工开发成本和不一致性。", "motivation": "本研究动机是探索使用语言模型自动生成多项选择题（MCQs）用于形态评估，旨在减少人工开发测试的成本和不一致性。", "method": "本研究采用两阶段方法。首先，比较微调的中型模型（Gemma，2B）和未微调的大模型（GPT-3.5，175B）。其次，评估七种结构性提示策略，包括零样本、少样本、链式思维、角色扮演、顺序提示及其组合。", "result": "研究结果表明，结构性提示尤其是结合链式思维和顺序设计的策略显著提高了Gemma的输出质量。Gemma产生的项目通常比GPT-3.5的零样本响应更符合构建和教学要求，提示设计在中型模型的表现中起关键作用。", "conclusion": "本研究展示了结构性提示和有效的微调可以提高中型模型在数据有限条件下的自动生成性能。强调结合自动化指标、专家判断和大模型模拟的重要性，以确保与评估目标的一致性。提出的流程提供了一种实践且可扩展的方式来开发和验证K-12语言评估项目。"}}
{"id": "2508.20223", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.20223", "abs": "https://arxiv.org/abs/2508.20223", "authors": ["Andrei Mihai Albu", "Giovanni Pollo", "Alessio Burrello", "Daniele Jahier Pagliari", "Cristian Tesconi", "Alessandra Neri", "Dario Soldi", "Fabio Autieri", "Sara Vinco"], "title": "Integrating SystemC TLM into FMI 3.0 Co-Simulations with an Open-Source Approach", "comment": null, "summary": "The growing complexity of cyber-physical systems, particularly in automotive\napplications, has increased the demand for efficient modeling and cross-domain\nco-simulation techniques. While SystemC Transaction-Level Modeling (TLM)\nenables effective hardware/software co-design, its limited interoperability\nwith models from other engineering domains poses integration challenges. This\npaper presents a fully open-source methodology for integrating SystemC TLM\nmodels into Functional Mock-up Interface (FMI)-based co-simulation workflows.\nBy encapsulating SystemC TLM components as FMI 3.0 Co Simulation Functional\nMock-up Units (FMUs), the proposed approach facilitates seamless, standardized\nintegration across heterogeneous simulation environments. We introduce a\nlightweight open-source toolchain, address key technical challenges such as\ntime synchronization and data exchange, and demonstrate the feasibility and\neffectiveness of the integration through representative case studies.", "AI": {"tldr": "本文讨论了将SystemC TLM模型集成到基于FMI的协同仿真中的方法，解决了集成中的技术挑战，并通过实例展示了该方法的有效性。", "motivation": "由于网络物理系统的复杂性不断增加，特别是在汽车应用中，有效建模和跨域协同仿真技术的需求也日益增长。尽管SystemC事务级建模（TLM）能够支持有效的硬件/软件协同设计，但其与其他工程领域模型的有限互操作性带来了集成挑战。", "method": "本文提出了一种完全开源的方法，将SystemC事务级建模（TLM）模型集成到基于功能模拟接口（FMI）的协同仿真工作流中。通过将SystemC TLM组件封装为FMI 3.0协作仿真功能模拟单元（FMU），该方法促进了在异构仿真环境中的无缝、标准化集成。", "result": "该研究介绍了一种轻量级的开源工具链，解决了关键的技术挑战，例如时间同步和数据交换，并通过代表性案例研究证明了该集成方法的可行性和有效性。", "conclusion": "该研究提出了一种全新的方法，不仅证明了SystemC TLM模型与现有基于FMI的协同仿真框架可以无缝集成，还提供了一个解决实际工程挑战的工具链。"}}
{"id": "2508.20324", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.20324", "abs": "https://arxiv.org/abs/2508.20324", "authors": ["Rikuto Kotoge", "Mai Nishimura", "Jiaxin Ma"], "title": "Can Compact Language Models Search Like Agents? Distillation-Guided Policy Optimization for Preserving Agentic RAG Capabilities", "comment": null, "summary": "Reinforcement Learning has emerged as a post-training approach to elicit\nagentic RAG behaviors such as search and planning from language models.\nHowever, compact language models (e.g., 0.5B parameters) struggle due to poor\nreasoning ability, resulting in sparse rewards and unstable training. To\novercome these difficulties, we propose Distillation-Guided Policy Optimization\n(DGPO), which addresses the challenges through cold-start initialization from\nteacher demonstrations and continuous teacher guidance during policy\noptimization. To systematically evaluate our approach, we introduce Agentic RAG\nCapabilities (ARC), a fine-grained metric analyzing reasoning, search\ncoordination, and response synthesis. Comprehensive experiments demonstrate\nthat DGPO enables compact models to achieve sophisticated agentic search\nbehaviors, even outperforming the larger teacher model in some cases. DGPO\nmakes agentic RAG feasible in computing resource-constrained environments.", "AI": {"tldr": "为了克服紧凑型语言模型中推理能力的问题，提出了一种新的策略优化方法（DGPO），实验表明，DGPO使模型在资源受限条件下实现更高级的行为搜索表现。", "motivation": "研究动机是解决紧凑型语言模型（如：0.5B参数）在推理能力上的不足，导致奖励稀疏和训练不稳定的问题。", "method": "研究团队提出了Distillation-Guided Policy Optimization（DGPO）方法，通过冷启动初始化教师演示以及持续的教师指导来解决策略优化中的问题。", "result": "实验结果表明，DGPO使紧凑型模型能够实现复杂的行为搜索，即使在某些情况下，其表现超过了较大的教师模型。这意味着DGPO能够在计算资源受限的环境中实现机构化RAG。", "conclusion": "结论指出，DGPO有效地解决了小型语言模型在训练中的问题，使得即使在计算资源有限的情况下，也能实现高级的机构化RAG行为。"}}
{"id": "2508.20181", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.MM"], "pdf": "https://arxiv.org/pdf/2508.20181", "abs": "https://arxiv.org/abs/2508.20181", "authors": ["Alberto Compagnoni", "Davide Caffagni", "Nicholas Moratelli", "Lorenzo Baraldi", "Marcella Cornia", "Rita Cucchiara"], "title": "Mitigating Hallucinations in Multimodal LLMs via Object-aware Preference Optimization", "comment": "BMVC 2025", "summary": "Multimodal Large Language Models (MLLMs) emerge as a unified interface to\naddress a multitude of tasks, ranging from NLP to computer vision. Despite\nshowcasing state-of-the-art results in many benchmarks, a long-standing issue\nis the tendency of MLLMs to hallucinate, that is to generate answers to the\nuser's query that are not reflected in the visual input. In this paper, we\naddress the problem of hallucinations as an alignment problem, seeking to steer\nthe MLLM so that it prefers generating content without hallucinations. In\ncontrast to recent approaches that require complicated pipelines to build\nsynthetic preference data for alignment training, often relying on proprietary\nmodels, we capitalize on the well-known CHAIR metric, originally proposed to\ngauge the degree of hallucinations in image captioning. Given a pair of\ngenerated answers, we leverage CHAIR to distinguish winner and loser options\n(i.e., non-hallucinated and hallucinated samples) and fine-tune off-the-shelf\nMLLMs via Direct Preference Optimization (DPO). The resulting method, which we\nrefer to as CHAIR-DPO, effectively diminishes the amount of hallucinated\nanswers on several hallucination benchmarks, demonstrating the effectiveness of\nfine-tuning the MLLM with a CHAIR-based reward. Source code and trained models\nare publicly available at https://github.com/aimagelab/CHAIR-DPO.", "AI": {"tldr": "研究提出了一种名为CHAIR-DPO的方法，通过CHAIR度量和DPO技术减少多模态大型语言模型的幻觉生成，展示了这种方法在多个幻觉基准上的有效性。", "motivation": "尽管多模态大型语言模型在许多基准测试中表现出色，但生成与视觉输入不符的回答这一问题仍然存在。该研究旨在解决这一幻觉问题，通过将其视为对齐问题并采用更简单有效的微调策略。", "method": "该研究利用CHAIR度量区分生成答案中的幻觉和非幻觉样本，并通过直接偏好优化（DPO）微调现成的多模态大型语言模型，实现减少幻觉答案的目标。", "result": "{immerse}", "conclusion": "通过使用CHAIR度量和直接偏好优化（DPO）方法，该研究成功减少了多模态大型语言模型在多个幻觉基准测试中的幻觉答案数量，证明了以CHAIR为基础的奖励进行微调的有效性。代码和训练模型已公开。"}}
{"id": "2508.20325", "categories": ["cs.CL", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.20325", "abs": "https://arxiv.org/abs/2508.20325", "authors": ["Haibo Jin", "Ruoxi Chen", "Peiyan Zhang", "Andy Zhou", "Yang Zhang", "Haohan Wang"], "title": "GUARD: Guideline Upholding Test through Adaptive Role-play and Jailbreak Diagnostics for LLMs", "comment": "54 pages", "summary": "As Large Language Models become increasingly integral to various domains,\ntheir potential to generate harmful responses has prompted significant societal\nand regulatory concerns. In response, governments have issued ethics guidelines\nto promote the development of trustworthy AI. However, these guidelines are\ntypically high-level demands for developers and testers, leaving a gap in\ntranslating them into actionable testing questions to verify LLM compliance.\n  To address this challenge, we introduce GUARD (\\textbf{G}uideline\n\\textbf{U}pholding Test through \\textbf{A}daptive \\textbf{R}ole-play and\nJailbreak \\textbf{D}iagnostics), a testing method designed to operationalize\nguidelines into specific guideline-violating questions that assess LLM\nadherence. To implement this, GUARD uses automated generation of\nguideline-violating questions based on government-issued guidelines, thereby\ntesting whether responses comply with these guidelines. When responses directly\nviolate guidelines, GUARD reports inconsistencies. Furthermore, for responses\nthat do not directly violate guidelines, GUARD integrates the concept of\n``jailbreaks'' to diagnostics, named GUARD-JD, which creates scenarios that\nprovoke unethical or guideline-violating responses, effectively identifying\npotential scenarios that could bypass built-in safety mechanisms. Our method\nfinally culminates in a compliance report, delineating the extent of adherence\nand highlighting any violations.\n  We have empirically validated the effectiveness of GUARD on seven LLMs,\nincluding Vicuna-13B, LongChat-7B, Llama2-7B, Llama-3-8B, GPT-3.5, GPT-4,\nGPT-4o, and Claude-3.7, by testing compliance under three government-issued\nguidelines and conducting jailbreak diagnostics. Additionally, GUARD-JD can\ntransfer jailbreak diagnostics to vision-language models, demonstrating its\nusage in promoting reliable LLM-based applications.", "AI": {"tldr": "本文提出了一种名为GUARD的测试方法，旨在通过自动化生成判别问题来评估大语言模型对政府发布伦理准则的遵守情况，验证和提高语言模型的可靠性和安全性。", "motivation": "随着大语言模型在各个领域的广泛应用，其潜在生成有害内容的能力引发了社会和监管的广泛关注。虽然政府发布了伦理准则，但这些准则通常是对开发者和测试者的高层次要求，缺乏可操作的测试问题来验证大语言模型的合规性。", "method": "本文提出了一种名为GUARD的测试方法，用于将政府发布的伦理准则转化为具体的测试问题，以验证大语言模型是否遵循这些准则。GUARD通过自动化生成违反准则的问题来实现这一目标，并对回答进行评估。此外，GUARD引入了“越狱诊断”的概念，通过创建可能引发不道德或违规回答的场景来强化测试，这被命名为GUARD-JD。", "result": "该方法在七个大语言模型上进行了实证验证，测试了它们在三个政府发布的准则下的遵守情况，并进行了越狱诊断。结果显示了各模型的合规性，并指出可能存在的违规情况。", "conclusion": "GUARD方法通过将其转化为具体的测试问题，有效地填补了这一空白，为检验大语言模型的合规性提供了工具，并展示了在视觉语言模型中的潜在应用。"}}
{"id": "2508.20182", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.20182", "abs": "https://arxiv.org/abs/2508.20182", "authors": ["Yang Su", "Shunquan Tan", "Jiwu Huang"], "title": "SDiFL: Stable Diffusion-Driven Framework for Image Forgery Localization", "comment": null, "summary": "Driven by the new generation of multi-modal large models, such as Stable\nDiffusion (SD), image manipulation technologies have advanced rapidly, posing\nsignificant challenges to image forensics. However, existing image forgery\nlocalization methods, which heavily rely on labor-intensive and costly\nannotated data, are struggling to keep pace with these emerging image\nmanipulation technologies. To address these challenges, we are the first to\nintegrate both image generation and powerful perceptual capabilities of SD into\nan image forensic framework, enabling more efficient and accurate forgery\nlocalization. First, we theoretically show that the multi-modal architecture of\nSD can be conditioned on forgery-related information, enabling the model to\ninherently output forgery localization results. Then, building on this\nfoundation, we specifically leverage the multimodal framework of Stable\nDiffusionV3 (SD3) to enhance forgery localization performance.We leverage the\nmulti-modal processing capabilities of SD3 in the latent space by treating\nimage forgery residuals -- high-frequency signals extracted using specific\nhighpass filters -- as an explicit modality. This modality is fused into the\nlatent space during training to enhance forgery localization performance.\nNotably, our method fully preserves the latent features extracted by SD3,\nthereby retaining the rich semantic information of the input image.\nExperimental results show that our framework achieves up to 12% improvements in\nperformance on widely used benchmarking datasets compared to current\nstate-of-the-art image forgery localization models. Encouragingly, the model\ndemonstrates strong performance on forensic tasks involving real-world document\nforgery images and natural scene forging images, even when such data were\nentirely unseen during training.", "AI": {"tldr": "The paper introduces a novel approach to image forgery localization using Stable DiffusionV3 (SD3), significantly enhancing accuracy and efficiency in detecting forgeries with improvements up to 12% over existing models.", "motivation": "The motivation of this paper is to address the challenge of keeping up with the rapid advancements in image manipulation technologies, particularly posed by the new generation of multi-modal large models like Stable Diffusion, while improving on the inefficiencies and high costs associated with traditional forgery localization techniques that require extensive annotated data.", "method": "The method integrates Stable Diffusion's image generation capabilities and perceptual abilities into an image forensic framework. It leverages SD3's multi-modal processing in the latent space by treating forgery residuals as an explicit modality, which is then fused with the latent space during training to enhance forgery localization.", "result": "The experimental results demonstrate that the proposed framework achieves up to 12% better performance on benchmark datasets compared to state-of-the-art models. Notably, it shows robust performance even on real-world and unseen data.", "conclusion": "The paper concludes that by integrating Stable Diffusion's multi-modal architecture into a forensic framework, a significant improvement in the detection of image forgeries can be achieved, providing an efficient alternative to existing methods that are generally data-intensive and costly."}}
{"id": "2508.20351", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.20351", "abs": "https://arxiv.org/abs/2508.20351", "authors": ["Zhirui Chen", "Wei Shen", "Jiashui Huang", "Ling Shao"], "title": "Joint Enhancement of Relational Reasoning for Long-Context LLMs", "comment": "9 pages, 5 pages Accepted by EMNLP 2025 Findings", "summary": "Despite significant progress, large language models (LLMs) still struggle\nwith long contexts due to memory limitations and their inability to tackle\ncomplex and long-context tasks. Additionally, LLMs often suffer from a lack of\ntransparency and are prone to producing hallucinations. To address these\nchallenges, we propose \\textbf{JERR}, a novel framework designed to enhance\nlong-context comprehension via graph-based reasoning in LLMs. JERR integrates\nthree key components: synopsis extraction, graph construction, and relational\nreasoning. First, synopsis is extracted by chunking text strategically,\nallowing the model to summarize and understand information more efficiently.\nSecond, we build a directed acyclic graph (DAG) to resolve redundancy, ensuring\nlogical consistency and clarity. Finally, we incorporate Monte Carlo Tree\nSearch (MCTS) to help the model navigate complex reasoning paths, ensuring more\naccurate and interpretable outputs. This framework provides a novel solution\nthat enables LLMs to handle extended contexts and complex reasoning tasks with\nimproved reliability and transparency. Experimental results show that JERR\nconsistently outperforms all baselines on the ROUGE and F1 metrics, achieving\nthe highest scores on the LLM-Rater evaluation.", "AI": {"tldr": "JERR框架采用策略性文本分块，构建DAG，使用MCTS处理复杂路径，以改进LLMs的长文本理解和推理精度，并提高了模型的透明度，实验中获得优异表现。", "motivation": "面对大语言模型（LLMs）在长文本上下文理解和复杂任务处理方面的局限性，JERR框架旨在通过图推理增强LLMs的长文本处理能力，提高透明度并减少幻觉问题。", "method": "JERR框架整合了三个关键组件：摘要提取，图构建，和关系推理。首先，通过策略性分块文本以总结和理解信息。其次，构建有向无环图(DAG)来解决冗余问题，确保逻辑一致性和清晰度。最后，使用蒙特卡洛树搜索(MCTS)帮助模型处理复杂的推理路径，确保更准确和可解释的结果。", "result": "实验结果表明，JERR在ROUGE和F1评估指标上超越了所有基线，在LLM-Rater评价中得到了最高分。", "conclusion": "JERR框架为解决大语言模型在长文本理解和复杂推理任务上的问题提供了新颖的方法，通过增强LLMs在这些任务中的处理能力，提升了其可靠性和透明度。"}}
{"id": "2508.20188", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.20188", "abs": "https://arxiv.org/abs/2508.20188", "authors": ["Max Torop", "Masih Eskandar", "Nicholas Kurtansky", "Jinyang Liu", "Jochen Weber", "Octavia Camps", "Veronica Rotemberg", "Jennifer Dy", "Kivanc Kose"], "title": "Grounding Multimodal Large Language Models with Quantitative Skin Attributes: A Retrieval Study", "comment": null, "summary": "Artificial Intelligence models have demonstrated significant success in\ndiagnosing skin diseases, including cancer, showing the potential to assist\nclinicians in their analysis. However, the interpretability of model\npredictions must be significantly improved before they can be used in practice.\nTo this end, we explore the combination of two promising approaches: Multimodal\nLarge Language Models (MLLMs) and quantitative attribute usage. MLLMs offer a\npotential avenue for increased interpretability, providing reasoning for\ndiagnosis in natural language through an interactive format. Separately, a\nnumber of quantitative attributes that are related to lesion appearance (e.g.,\nlesion area) have recently been found predictive of malignancy with high\naccuracy. Predictions grounded as a function of such concepts have the\npotential for improved interpretability. We provide evidence that MLLM\nembedding spaces can be grounded in such attributes, through fine-tuning to\npredict their values from images. Concretely, we evaluate this grounding in the\nembedding space through an attribute-specific content-based image retrieval\ncase study using the SLICE-3D dataset.", "AI": {"tldr": "研究提出了一种提高AI模型诊断皮肤疾病时的可解释性的方法，即通过结合多模态大型语言模型和定量属性使用，并通过实例研究验证了该方法的可行性。", "motivation": "尽管人工智能模型在诊断皮肤疾病方面表现出显著的成功，但模型预测的可解释性需要显著提高才能在实践中应用。", "method": "通过结合多模态大型语言模型(MLLMs)和定量属性使用来提高皮肤疾病诊断模型的可解释性。具体来说，对模型进行了微调，使其能够从图像中预测与病变外观相关的定量属性值。", "result": "通过使用SLICE-3D数据集进行属性特定的内容检索案例研究，验证了MLLM嵌入空间可以基于这些属性进行标注。", "conclusion": "将多模态大型语言模型和定量属性使用相结合的方法可以提高皮肤疾病诊断模型的可解释性。"}}
{"id": "2508.20373", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.20373", "abs": "https://arxiv.org/abs/2508.20373", "authors": ["Yuyao Wang", "Bowen Liu", "Jianheng Tang", "Nuo Chen", "Yuhan Li", "Qifan Zhang", "Jia Li"], "title": "Graph-R1: Unleashing LLM Reasoning with NP-Hard Graph Problems", "comment": null, "summary": "Reasoning Large Language Models (RLLMs) have recently achieved remarkable\nprogress on complex reasoning tasks, largely enabled by their long\nchain-of-thought (Long CoT) capabilities. However, developing these Long CoT\nbehaviors relies heavily on post-training with high-quality datasets, which are\ntypically costly and human-curated (e.g., mathematics and code), leaving\nscalable alternatives unexplored. In this work, we introduce NP-hard (NPH)\ngraph problems as a novel synthetic training corpus, as they inherently require\ndeep reasoning, extensive exploration, and reflective strategies, which are\ncore characteristics of Long CoT reasoning. Building on this insight, we\ndevelop a two-stage post-training framework: (i) Long CoT Supervised\nFine-Tuning (SFT) on rejection-sampled NPH graph instances, which substantially\nenhances reasoning depth, and (ii) Reinforcement Learning (RL) with a\nfine-grained reward design, which sharpens reasoning efficiency. Our flagship\nmodel, Graph-R1-7B, demonstrates strong generalization across mathematics,\ncoding, STEM, and logic, and surpasses QwQ-32B on NPH graph problems in both\naccuracy and reasoning efficiency. These results position NPH graph problems as\nan effective and scalable resource for advancing Long CoT reasoning in LLMs,\nopening a new frontier for LLM post-training. Our implementation is available\nat https://github.com/Graph-Reasoner/Graph-R1, with models and datasets hosted\nin our Hugging Face collection HKUST-DSAIL/Graph-R1.", "AI": {"tldr": "该研究提出了一种基于NP-hard图问题的新颖合成训练语料库，用于开发大型语言模型的长链式推理能力，通过两阶段后训练框架显著增强了推理深度与效率。模型Graph-R1-7B在多个领域展示了强大的泛化能力。", "motivation": "解决现有大型语言模型在长链式推理行为上所需的高质量数据集成本高昂且人工编制的问题，探索具有可扩展性的替代方案。", "method": "采用NP-hard图问题作为训练语料库，设计了包括细粒度奖励设计的强化学习在内的两阶段长链式推理监督微调框架。", "result": "旗舰模型Graph-R1-7B在数学、编程、STEM、逻辑推理等多个领域展示了较强的泛化能力，并且在NP-hard图问题上比QwQ-32B模型更具准确性与推理效率。", "conclusion": "NP-hard图问题作为合成训练语料库是提升大型语言模型长链式推理能力的一个有效且可扩展的方法，为大型语言模型训练开创了新的领域。"}}
{"id": "2508.20193", "categories": ["cs.CV", "eess.SP"], "pdf": "https://arxiv.org/pdf/2508.20193", "abs": "https://arxiv.org/abs/2508.20193", "authors": ["Hossein Ahmadi", "Banafsheh Saffari"], "title": "Enhancing Automatic Modulation Recognition With a Reconstruction-Driven Vision Transformer Under Limited Labels", "comment": null, "summary": "Automatic modulation recognition (AMR) is critical for cognitive radio,\nspectrum monitoring, and secure wireless communication. However, existing\nsolutions often rely on large labeled datasets or multi-stage training\npipelines, which limit scalability and generalization in practice. We propose a\nunified Vision Transformer (ViT) framework that integrates supervised,\nself-supervised, and reconstruction objectives. The model combines a ViT\nencoder, a lightweight convolutional decoder, and a linear classifier; the\nreconstruction branch maps augmented signals back to their originals, anchoring\nthe encoder to fine-grained I/Q structure. This strategy promotes robust,\ndiscriminative feature learning during pretraining, while partial label\nsupervision in fine-tuning enables effective classification with limited\nlabels. On the RML2018.01A dataset, our approach outperforms supervised CNN and\nViT baselines in low-label regimes, approaches ResNet-level accuracy with only\n15-20% labeled data, and maintains strong performance across varying SNR\nlevels. Overall, the framework provides a simple, generalizable, and\nlabel-efficient solution for AMR.", "AI": {"tldr": "提出了一种集成监督、自监督和重建目标的统一Vision Transformer框架，该框架在AMR领域中表现出色，特别是在低标签数量的数据集上。", "motivation": "自动调制识别（AMR）对于认知无线电、频谱监测和安全无线通信至关重要。然而，现有的解决方案通常依赖于大量标注数据集或多阶段训练流程，这限制了其实际应用的扩展性和泛化能力。", "method": "我们提出了一种统一的Vision Transformer (ViT)框架，该框架结合了监督、自监督和重建目标。模型包括一个ViT编码器、一个轻量级的卷积解码器和一个线性分类器；重建分支将增强信号转换回原始信号，使编码器锚定在细粒度I/Q结构上。这种策略在预训练过程中促进鲁棒、区分性特征学习，而在微调阶段的部分标签监督使得在有限标签情况下也能有效分类。", "result": "在RML2018.01A数据集上，我们的方法在低标签数量的情况下超过了监督CNN和ViT基线模型的表现，仅需15-20%的标注数据就能接近ResNet级别的准确率，并且在不同的信噪比水平下保持了强大的性能。", "conclusion": "总体而言，该框架提供了一种简单、通用且标签高效的AMR解决方案。"}}
{"id": "2508.20385", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.20385", "abs": "https://arxiv.org/abs/2508.20385", "authors": ["Jivnesh Sandhan", "Fei Cheng", "Tushar Sandhan", "Yugo Murawaki"], "title": "CAPE: Context-Aware Personality Evaluation Framework for Large Language Models", "comment": "Accepted at EMNLP25 (Findings)", "summary": "Psychometric tests, traditionally used to assess humans, are now being\napplied to Large Language Models (LLMs) to evaluate their behavioral traits.\nHowever, existing studies follow a context-free approach, answering each\nquestion in isolation to avoid contextual influence. We term this the Disney\nWorld test, an artificial setting that ignores real-world applications, where\nconversational history shapes responses. To bridge this gap, we propose the\nfirst Context-Aware Personality Evaluation (CAPE) framework for LLMs,\nincorporating prior conversational interactions. To thoroughly analyze the\ninfluence of context, we introduce novel metrics to quantify the consistency of\nLLM responses, a fundamental trait in human behavior.\n  Our exhaustive experiments on 7 LLMs reveal that conversational history\nenhances response consistency via in-context learning but also induces\npersonality shifts, with GPT-3.5-Turbo and GPT-4-Turbo exhibiting extreme\ndeviations. While GPT models are robust to question ordering, Gemini-1.5-Flash\nand Llama-8B display significant sensitivity. Moreover, GPT models response\nstem from their intrinsic personality traits as well as prior interactions,\nwhereas Gemini-1.5-Flash and Llama--8B heavily depend on prior interactions.\nFinally, applying our framework to Role Playing Agents (RPAs) shows\ncontext-dependent personality shifts improve response consistency and better\nalign with human judgments. Our code and datasets are publicly available at:\nhttps://github.com/jivnesh/CAPE", "AI": {"tldr": "本文提出了首个上下文感知性格评估框架（CAPE），用于评估大规模语言模型的一致性。实验表明，在对话历史的基础上，一些模型在响应一致性上表现出显著的提升，但同时也产生了性格的变化。", "motivation": "尽管传统的心理测量测试被应用于评估大规模语言模型的行为特征，现有的研究都采用一种无上下文的方法单独回答每个问题以避免上下文影响。这忽略了现实世界应用中对话历史对响应的影响。本文提出了一种名为CAPE的框架以弥补这一差距。", "method": "我们提出了首个用于大规模语言模型的上下文感知性格评估（CAPE）框架，该框架将之前的对话互动纳入考量。为了全面分析上下文的影响，我们引入了新的度量标准来量化LLM响应的一致性，这是人类行为的基本特征。", "result": "在7个大规模语言模型上的详尽实验显示，对话历史通过上下文学习增强了响应的一致性但也带来了性格转变。GPT-3.5-Turbo和GPT-4-Turbo显示出极端的偏差。GPT模型响应来自其内在性格特征和先前的互动，而Gemini-1.5-Flash和Llama-8B则严重依赖于之前的互动。", "conclusion": "将提出的框架应用于角色扮演代理（RPAs）显示，依赖上下文的性格变化提升了响应的一致性，并且与人类的判断更趋一致。本文研究结果表明，上下文因素在评估LLM性格特征中的重要性。代码和数据集均可在https://github.com/jivnesh/CAPE上公开获取。"}}
{"id": "2508.20210", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.20210", "abs": "https://arxiv.org/abs/2508.20210", "authors": ["Xiaodi Li", "Pan Xie", "Yi Ren", "Qijun Gan", "Chen Zhang", "Fangyuan Kong", "Xiang Yin", "Bingyue Peng", "Zehuan Yuan"], "title": "InfinityHuman: Towards Long-Term Audio-Driven Human", "comment": "Project Page: https://infinityhuman.github.io/", "summary": "Audio-driven human animation has attracted wide attention thanks to its\npractical applications. However, critical challenges remain in generating\nhigh-resolution, long-duration videos with consistent appearance and natural\nhand motions. Existing methods extend videos using overlapping motion frames\nbut suffer from error accumulation, leading to identity drift, color shifts,\nand scene instability. Additionally, hand movements are poorly modeled,\nresulting in noticeable distortions and misalignment with the audio. In this\nwork, we propose InfinityHuman, a coarse-to-fine framework that first generates\naudio-synchronized representations, then progressively refines them into\nhigh-resolution, long-duration videos using a pose-guided refiner. Since pose\nsequences are decoupled from appearance and resist temporal degradation, our\npose-guided refiner employs stable poses and the initial frame as a visual\nanchor to reduce drift and improve lip synchronization. Moreover, to enhance\nsemantic accuracy and gesture realism, we introduce a hand-specific reward\nmechanism trained with high-quality hand motion data. Experiments on the EMTD\nand HDTF datasets show that InfinityHuman achieves state-of-the-art performance\nin video quality, identity preservation, hand accuracy, and lip-sync. Ablation\nstudies further confirm the effectiveness of each module. Code will be made\npublic.", "AI": {"tldr": "本文提出了InfinityHuman，一种从粗到精的音频驱动人动画生成框架，解决现有方法中的身份漂移、色彩偏移和手部建模不准确问题，实验结果展示了其优异的视频质量和性能。", "motivation": "解决现有方法在生成高分辨率、长时长视频时出现的身份漂移、色彩偏移、场景不稳定以及手部动作建模不佳的问题。", "method": "InfinityHuman, 一种从粗到精的框架，首先生成与音频同步的表示，然后利用姿势引导细化器逐步将其细化为高分辨率、长时长的视频。", "result": "实验表明，InfinityHuman在视频质量、身份保持、手部准确性以及唇音同步上均达到了最先进的性能。", "conclusion": "InfinityHuman通过引入姿势引导细化器和手部特有奖励机制，有效解决了音频驱动人类动画生成中存在的问题，验证了每个模块的有效性。"}}
{"id": "2508.20395", "categories": ["cs.CL", "cs.AI", "I.2.7"], "pdf": "https://arxiv.org/pdf/2508.20395", "abs": "https://arxiv.org/abs/2508.20395", "authors": ["Xu Guo"], "title": "Measuring Reasoning Utility in LLMs via Conditional Entropy Reduction", "comment": "11 pages, 4 figures", "summary": "Recent advancements in large language models (LLMs) often rely on generating\nintermediate reasoning steps to enhance accuracy. However, little work has\nexamined how reasoning utility contributes to the final answer's correctness.\nDue to the stochastic nature of autoregressive generation, generating more\ncontext does not guarantee increased confidence in the answer. If we could\npredict, during generation, whether a reasoning step will be useful, we could\nstop early or prune ineffective steps, avoiding distractions in the final\ndecision.\n  We present an oracle study on MATH dataset, using Qwen2.5-32B and GPT-4o to\ngenerate reasoning chains, and then employing a separate model (Qwen3-8B) to\nquantify the utility of these chains for final accuracy. Specifically, we\nmeasure the model's uncertainty on the answer span Y at each reasoning step\nusing conditional entropy (expected negative log-likelihood over the\nvocabulary) with context expanding step by step. Our results show a clear\npattern: conditional entropy that decreases over steps is strongly associated\nwith correct answers, whereas flat or increasing entropy often results in wrong\nanswers. We also corroborate that incorrect reasoning paths tend to be longer\nthan correct ones, suggesting that longer reasoning does not necessarily yield\nbetter outcomes. These findings serve as a foundation to inspire future work on\ndesigning efficient reasoning pipelines that detect and avoid unproductive\nreasoning early.", "AI": {"tldr": "本文针对如何通过预测推理实用性来增强语言模型正确性进行了探究，发现条件熵减少与正确答案相关，而错误答案往往伴随条件熵保持不变或增加，此外还不正确的推理路径通常更长。", "motivation": "本文试图研究推理实用性如何贡献于最终答案的正确性。这是因为自回归生成的随机性质使得生成更多上下文并不一定能增加答案的信心。如果能在生成过程中预测推理步骤是否有用，就能提前停止或剪枝无效步骤，从而避免对最终决策的干扰。", "method": "我们使用MATH数据集进行研究，通过Qwen2.5-32B和GPT-4o生成推理链，并使用单独的模型Qwen3-8B来量化这些推理链对最终准确性的有用性。具体来说，我们在每一步推理过程中使用条件熵（词汇表上期望的负对数似然）测量模型对答案片段Y的不确定性。", "result": "研究结果表明：条件熵在推理步骤中减少与正确答案存在显著相关性；而条件熵保持平缓或增加则往往导致错误答案。同时发现，不正确的推理路径往往比正确的推理路径更长，这表明更长的推理并不一定会产生更好的结果。", "conclusion": "这些发现为未来设计能够检测并避免无效推理的高效推理流水线奠定了基础。"}}
{"id": "2508.20221", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.20221", "abs": "https://arxiv.org/abs/2508.20221", "authors": ["Mert Cokelek", "Halit Ozsoy", "Nevrez Imamoglu", "Cagri Ozcinar", "Inci Ayhan", "Erkut Erdem", "Aykut Erdem"], "title": "Spherical Vision Transformers for Audio-Visual Saliency Prediction in 360-Degree Videos", "comment": "Accepted for publication in IEEE Transaction on Pattern Analysis and\n  Machine Intelligence (IEEE TPAMI)", "summary": "Omnidirectional videos (ODVs) are redefining viewer experiences in virtual\nreality (VR) by offering an unprecedented full field-of-view (FOV). This study\nextends the domain of saliency prediction to 360-degree environments,\naddressing the complexities of spherical distortion and the integration of\nspatial audio. Contextually, ODVs have transformed user experience by adding a\nspatial audio dimension that aligns sound direction with the viewer's\nperspective in spherical scenes. Motivated by the lack of comprehensive\ndatasets for 360-degree audio-visual saliency prediction, our study curates\nYT360-EyeTracking, a new dataset of 81 ODVs, each observed under varying\naudio-visual conditions. Our goal is to explore how to utilize audio-visual\ncues to effectively predict visual saliency in 360-degree videos. Towards this\naim, we propose two novel saliency prediction models: SalViT360, a\nvision-transformer-based framework for ODVs equipped with spherical\ngeometry-aware spatio-temporal attention layers, and SalViT360-AV, which\nfurther incorporates transformer adapters conditioned on audio input. Our\nresults on a number of benchmark datasets, including our YT360-EyeTracking,\ndemonstrate that SalViT360 and SalViT360-AV significantly outperform existing\nmethods in predicting viewer attention in 360-degree scenes. Interpreting these\nresults, we suggest that integrating spatial audio cues in the model\narchitecture is crucial for accurate saliency prediction in omnidirectional\nvideos. Code and dataset will be available at\nhttps://cyberiada.github.io/SalViT360.", "AI": {"tldr": "研究构建了新的360度视听显著性预测数据集并提出两种模型，显著提升预测准确率。", "motivation": "研究动机源于当前缺少用于预测360度视听显著性的综合数据集。研究团队构建了一个新的数据集YT360-EyeTracking，包含81个ODVs视频，每段视频都处于不同的视听条件。目的在于探索如何利用视听线索有效预测360度视频的视觉显著性。", "method": "本研究针对360度视频中的球面几何畸变及空间音频整合的挑战，提出了两种新模型：SalViT360和SalViT360-AV。SalViT360基于视觉变压器框架，并配备球形几何感知的时空注意力层；而SalViT360-AV则在此基础上，进一步加入了受音频输入条件控制的变压器适配器。", "result": "在YT360-EyeTracking等多个基准数据集上，SalViT360和SalViT360-AV模型均显著优于当前现有的方法，在预测360度场景中的观看者注意力上表现突出。", "conclusion": "研究结果表明，将空间音频线索整合进模型结构对于准确预测360度视频中的显著性是至关重要的。"}}
{"id": "2508.20410", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.20410", "abs": "https://arxiv.org/abs/2508.20410", "authors": ["Sam Jung", "Agustin Garcinuno", "Spencer Mateega"], "title": "UI-Bench: A Benchmark for Evaluating Design Capabilities of AI Text-to-App Tools", "comment": null, "summary": "AI text-to-app tools promise high quality applications and websites in\nminutes, yet no public benchmark rigorously verifies those claims. We introduce\nUI-Bench, the first large-scale benchmark that evaluates visual excellence\nacross competing AI text-to-app tools through expert pairwise comparison.\nSpanning 10 tools, 30 prompts, 300 generated sites, and \\textit{4000+} expert\njudgments, UI-Bench ranks systems with a TrueSkill-derived model that yields\ncalibrated confidence intervals. UI-Bench establishes a reproducible standard\nfor advancing AI-driven web design. We release (i) the complete prompt set,\n(ii) an open-source evaluation framework, and (iii) a public leaderboard. The\ngenerated sites rated by participants will be released soon. View the UI-Bench\nleaderboard at https://uibench.ai/leaderboard.", "AI": {"tldr": "开发了UI-Bench作为首个大规模基准测试，用于通过专家配对比较的方式评估十个AI文本转应用工具的视觉表现。", "motivation": "目前没有公开的基准测试来严谨验证AI文本转应用工具的质量声明，因此作者开发了UI-Bench。", "method": "引入UI-Bench，这是一个大规模基准测试，通过专家配对比较评估了十个AI文本转应用工具在视觉卓越性方面的表现。", "result": "UI-Bench 覆盖了十个工具，三十个提示，三百个生成的网站，和4000多个专家判断，它对系统进行了真正的技能模型排名，提供了校准过的置信区间。", "conclusion": "UI-Bench 建立了一个可重现的标准来推动AI驱动的网页设计的进步，并公开了提示集，开源评估框架，和公共排行榜。"}}
{"id": "2508.20227", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.20227", "abs": "https://arxiv.org/abs/2508.20227", "authors": ["Phu-Vinh Nguyen", "Tan-Hanh Pham", "Chris Ngo", "Truong Son Hy"], "title": "A Novel Framework for Automated Explain Vision Model Using Vision-Language Models", "comment": null, "summary": "The development of many vision models mainly focuses on improving their\nperformance using metrics such as accuracy, IoU, and mAP, with less attention\nto explainability due to the complexity of applying xAI methods to provide a\nmeaningful explanation of trained models. Although many existing xAI methods\naim to explain vision models sample-by-sample, methods explaining the general\nbehavior of vision models, which can only be captured after running on a large\ndataset, are still underexplored. Furthermore, understanding the behavior of\nvision models on general images can be very important to prevent biased\njudgments and help identify the model's trends and patterns. With the\napplication of Vision-Language Models, this paper proposes a pipeline to\nexplain vision models at both the sample and dataset levels. The proposed\npipeline can be used to discover failure cases and gain insights into vision\nmodels with minimal effort, thereby integrating vision model development with\nxAI analysis to advance image analysis.", "AI": {"tldr": "论文提出了一种新的流水线方法，通过视觉-语言模型来提高视觉模型的解释性，既包括对样本的解释也包括对整个数据集的解释，从而改进图像分析并帮助发现模型失败案例。", "motivation": "论文的动机是应对现有解释性AI方法（xAI）在解释视觉模型（尤其是数据集层面的行为）上的不足，以帮助识别模型的趋势和模式，并防止模型做出有偏见的判断。", "method": "该论文提出了一种流水线方法，旨在通过视觉-语言模型在样本和数据集层面解释视觉模型。这种方法能够帮助识别模型的失败案例，并深入理解视觉模型。", "result": "流水线可以发现视觉模型的失败案例，并提供对视觉模型的理解，这样可以将模型开发与xAI分析相结合，提高图像分析的准确性。", "conclusion": "该流水线方法通过视觉-语言模型加强了视觉模型的解释性，实现了在样本和数据集层面的模型理解，从而促进了图像分析的发展。"}}
{"id": "2508.20416", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.20416", "abs": "https://arxiv.org/abs/2508.20416", "authors": ["Hengchuan Zhu", "Yihuan Xu", "Yichen Li", "Zijie Meng", "Zuozhu Liu"], "title": "DentalBench: Benchmarking and Advancing LLMs Capability for Bilingual Dentistry Understanding", "comment": null, "summary": "Recent advances in large language models (LLMs) and medical LLMs (Med-LLMs)\nhave demonstrated strong performance on general medical benchmarks. However,\ntheir capabilities in specialized medical fields, such as dentistry which\nrequire deeper domain-specific knowledge, remain underexplored due to the lack\nof targeted evaluation resources. In this paper, we introduce DentalBench, the\nfirst comprehensive bilingual benchmark designed to evaluate and advance LLMs\nin the dental domain. DentalBench consists of two main components: DentalQA, an\nEnglish-Chinese question-answering (QA) benchmark with 36,597 questions\nspanning 4 tasks and 16 dental subfields; and DentalCorpus, a large-scale,\nhigh-quality corpus with 337.35 million tokens curated for dental domain\nadaptation, supporting both supervised fine-tuning (SFT) and\nretrieval-augmented generation (RAG). We evaluate 14 LLMs, covering\nproprietary, open-source, and medical-specific models, and reveal significant\nperformance gaps across task types and languages. Further experiments with\nQwen-2.5-3B demonstrate that domain adaptation substantially improves model\nperformance, particularly on knowledge-intensive and terminology-focused tasks,\nand highlight the importance of domain-specific benchmarks for developing\ntrustworthy and effective LLMs tailored to healthcare applications.", "AI": {"tldr": "该研究介绍了DentalBench，一个专门用于牙科领域的双语评估基准，评估了多种LLM在牙科问答任务中的表现，并强调了领域特定模型的重要性。", "motivation": "尽管大型语言模型(Large Language Models, LLMs)及其医疗领域的应用（Med-LLMs）在一般医疗基准测试中表现良好，但它们在特定医疗领域的表现，比如牙科，需要更深层次的专业知识，由于缺乏针对这些领域的评估资源，相关研究尚待探索。因此，本文旨在开发一个专门评估这些模型在牙科领域应用能力的基准。", "method": "本文介绍了DentalBench，这是一个用于评估和改进语言模型在牙科领域表现的综合双语基准。DentalBench包含两个主要组件：DentalQA，一个包含36,597个问题的英语-中文问答基准，涵盖了4项任务和16个牙科子领域；以及DentalCorpus，一个有3.37亿个令牌的大规模高质量语料库，用于支持监督微调和检索增强生成。", "result": "本研究评估了14种模型，包括专有、开源和专门为医疗设计的模型，并发现了任务类型和语言间存在显著的性能差异。使用Qwen-2.5-3B模型进行的实验表明，领域适应显著提高了模型在知识密集型和术语集中的任务中的性能。", "conclusion": "该研究强调了需要有领域特定的基准来开发可靠且有效的特定于医疗应用的LLMs的重要性。"}}
{"id": "2508.20232", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.20232", "abs": "https://arxiv.org/abs/2508.20232", "authors": ["Mohamed Ohamouddou", "Said Ohamouddou", "Abdellatif El Afia", "Rafik Lasri"], "title": "ATMS-KD: Adaptive Temperature and Mixed Sample Knowledge Distillation for a Lightweight Residual CNN in Agricultural Embedded Systems", "comment": null, "summary": "This study proposes ATMS-KD (Adaptive Temperature and Mixed-Sample Knowledge\nDistillation), a novel framework for developing lightweight CNN models suitable\nfor resource-constrained agricultural environments. The framework combines\nadaptive temperature scheduling with mixed-sample augmentation to transfer\nknowledge from a MobileNetV3 Large teacher model (5.7\\,M parameters) to\nlightweight residual CNN students. Three student configurations were evaluated:\nCompact (1.3\\,M parameters), Standard (2.4\\,M parameters), and Enhanced (3.8\\,M\nparameters). The dataset used in this study consists of images of \\textit{Rosa\ndamascena} (Damask rose) collected from agricultural fields in the Dades Oasis,\nsoutheastern Morocco, providing a realistic benchmark for agricultural computer\nvision applications under diverse environmental conditions. Experimental\nevaluation on the Damascena rose maturity classification dataset demonstrated\nsignificant improvements over direct training methods. All student models\nachieved validation accuracies exceeding 96.7\\% with ATMS-KD compared to\n95--96\\% with direct training. The framework outperformed eleven established\nknowledge distillation methods, achieving 97.11\\% accuracy with the compact\nmodel -- a 1.60 percentage point improvement over the second-best approach\nwhile maintaining the lowest inference latency of 72.19\\,ms. Knowledge\nretention rates exceeded 99\\% for all configurations, demonstrating effective\nknowledge transfer regardless of student model capacity.", "AI": {"tldr": "ATMS-KD框架结合自适应温度调度和混合样本增强，将知识从教师模型（MobileNetV3 Large）转移到轻量级CNN学生模型中，提升了农业环境下轻量级CNN模型的表现。实验表明，该框架显著优于直接训练和现有知识蒸馏方法。", "motivation": "开发适用于农业资源受限环境的轻量级CNN模型。", "method": "提出ATMS-KD框架，结合自适应温度调度和混合样本知识蒸馏技术，将MobileNetV3 Large模型的知识转移到轻量级的残差CNN学生模型中。", "result": "实验结果表明，所有学生模型的验证集准确率超过96.7%，优于直接训练方法95-96%的准确率；在玫瑰成熟度分类数据集上的表现超越了11种已建立的知识蒸馏方法。", "conclusion": "ATMS-KD框架能有效转移知识，即使是容量较小的模型也能达到99%以上的知识保留率，并在准确率和延迟时间上均优于现有方法。"}}
{"id": "2508.20417", "categories": ["cs.CL", "cs.DB"], "pdf": "https://arxiv.org/pdf/2508.20417", "abs": "https://arxiv.org/abs/2508.20417", "authors": ["Chi Minh Bui", "Ngoc Mai Thieu", "Van Vinh Nguyen", "Json J. Jung", "Khac-Hoai Nam Bui"], "title": "KG-CQR: Leveraging Structured Relation Representations in Knowledge Graphs for Contextual Query Retrieval", "comment": "Accepted at Main EMNLP 2025", "summary": "The integration of knowledge graphs (KGs) with large language models (LLMs)\noffers significant potential to improve the retrieval phase of\nretrieval-augmented generation (RAG) systems. In this study, we propose KG-CQR,\na novel framework for Contextual Query Retrieval (CQR) that enhances the\nretrieval phase by enriching the contextual representation of complex input\nqueries using a corpus-centric KG. Unlike existing methods that primarily\naddress corpus-level context loss, KG-CQR focuses on query enrichment through\nstructured relation representations, extracting and completing relevant KG\nsubgraphs to generate semantically rich query contexts. Comprising subgraph\nextraction, completion, and contextual generation modules, KG-CQR operates as a\nmodel-agnostic pipeline, ensuring scalability across LLMs of varying sizes\nwithout additional training. Experimental results on RAGBench and MultiHop-RAG\ndatasets demonstrate KG-CQR's superior performance, achieving a 4-6%\nimprovement in mAP and a 2-3% improvement in Recall@25 over strong baseline\nmodels. Furthermore, evaluations on challenging RAG tasks such as multi-hop\nquestion answering show that, by incorporating KG-CQR, the performance\nconsistently outperforms the existing baseline in terms of retrieval\neffectiveness", "AI": {"tldr": "The study introduces KG-CQR, a new retrieval framework for RAG systems that integrates knowledge graphs with large language models to enhance query context, yielding significant improvements in retrieval performance on various datasets without additional model training.", "motivation": "The motivation is to enhance the retrieval-augmented generation (RAG) systems by integrating knowledge graphs (KGs) with large language models (LLMs). This is done to better address the issue of context loss seen in existing methods and to achieve improvements in retrieval effectiveness, especially for complex and multi-hop tasks.", "method": "The method involves a novel framework named KG-CQR for Contextual Query Retrieval (CQR). This framework improves the retrieval phase by enriching query context through a structured relation representations approach. It extracts and completes relevant subgraphs from a corpus-centric KG and incorporates these into the query context without requiring additional training for varying sizes of large language models.", "result": "The results show that KG-CQR provides superior performance, with a 4-6% improvement in mAP and a 2-3% improvement in Recall@25 over strong baseline models. This improvement is demonstrated through experiments on RAGBench and MultiHop-RAG datasets.", "conclusion": "The conclusion is that by integrating knowledge graphs into the retrieval phase of RAG systems through the KG-CQR framework, significant improvements in retrieval effectiveness can be achieved without the need for additional training, and that this method outperforms existing baselines, especially for multi-hop question answering tasks."}}
{"id": "2508.20243", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.20243", "abs": "https://arxiv.org/abs/2508.20243", "authors": ["Mutahar Safdar", "Gentry Wood", "Max Zimmermann", "Guy Lamouche", "Priti Wanjara", "Yaoyao Fiona Zhao"], "title": "Linking heterogeneous microstructure informatics with expert characterization knowledge through customized and hybrid vision-language representations for industrial qualification", "comment": "46 pages, 33 figures, Submitted to Advanced Engineering Informatics,\n  under revision", "summary": "Rapid and reliable qualification of advanced materials remains a bottleneck\nin industrial manufacturing, particularly for heterogeneous structures produced\nvia non-conventional additive manufacturing processes. This study introduces a\nnovel framework that links microstructure informatics with a range of expert\ncharacterization knowledge using customized and hybrid vision-language\nrepresentations (VLRs). By integrating deep semantic segmentation with\npre-trained multi-modal models (CLIP and FLAVA), we encode both visual\nmicrostructural data and textual expert assessments into shared\nrepresentations. To overcome limitations in general-purpose embeddings, we\ndevelop a customized similarity-based representation that incorporates both\npositive and negative references from expert-annotated images and their\nassociated textual descriptions. This allows zero-shot classification of\npreviously unseen microstructures through a net similarity scoring approach.\nValidation on an additively manufactured metal matrix composite dataset\ndemonstrates the framework's ability to distinguish between acceptable and\ndefective samples across a range of characterization criteria. Comparative\nanalysis reveals that FLAVA model offers higher visual sensitivity, while the\nCLIP model provides consistent alignment with the textual criteria. Z-score\nnormalization adjusts raw unimodal and cross-modal similarity scores based on\ntheir local dataset-driven distributions, enabling more effective alignment and\nclassification in the hybrid vision-language framework. The proposed method\nenhances traceability and interpretability in qualification pipelines by\nenabling human-in-the-loop decision-making without task-specific model\nretraining. By advancing semantic interoperability between raw data and expert\nknowledge, this work contributes toward scalable and domain-adaptable\nqualification strategies in engineering informatics.", "AI": {"tldr": "本文提出了一种新的框架，该框架结合深度语义分割和预训练的多模态模型（CLIP和FLAVA），将微结构图像数据和专家文本评估编码成共享表示，并通过定制的相似度表示实现复杂结构评估的零样本分类，展现出在加性制造材料领域中巨大应用潜力。", "motivation": "目前，先进的材料快速可靠的质量鉴定仍然是工业制造中的瓶颈，特别是在非传统增材制造技术生产的异质结构中。", "method": "研究中采用深度语义分割结合预训练多模态模型（CLIP和FLAVA），整合视觉微结构数据和文本专家评估。通过定制相似度模型，该框架能够实现在没有正样本和负样本的情况下，分类之前未见过的微结构。", "result": "验证结果显示该框架可以在一系列的鉴定标准下，有效区分可接受的样本和缺陷样本。实验表明，FLAVA模型具有更高的视觉敏感度，而CLIP模型则在文本标准的一致性上表现更好。", "conclusion": "此方法通过启用环中的人类决策制定，提高了资格管线中的可追溯性和解释性，无需特定任务的模型重新训练。这项工作通过推动原始数据和专家知识的语义互操作性，有助于在工程信息学中实现可扩展和领域适应的资格策略。"}}
{"id": "2508.20420", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.20420", "abs": "https://arxiv.org/abs/2508.20420", "authors": ["Feng Zhang", "Chengjie Pang", "Yuehan Zhang", "Chenyu Luo"], "title": "CAMB: A comprehensive industrial LLM benchmark on civil aviation maintenance", "comment": null, "summary": "Civil aviation maintenance is a domain characterized by stringent industry\nstandards. Within this field, maintenance procedures and troubleshooting\nrepresent critical, knowledge-intensive tasks that require sophisticated\nreasoning. To address the lack of specialized evaluation tools for large\nlanguage models (LLMs) in this vertical, we propose and develop an\nindustrial-grade benchmark specifically designed for civil aviation\nmaintenance. This benchmark serves a dual purpose: It provides a standardized\ntool to measure LLM capabilities within civil aviation maintenance, identifying\nspecific gaps in domain knowledge and complex reasoning. By pinpointing these\ndeficiencies, the benchmark establishes a foundation for targeted improvement\nefforts (e.g., domain-specific fine-tuning, RAG optimization, or specialized\nprompt engineering), ultimately facilitating progress toward more intelligent\nsolutions within civil aviation maintenance. Our work addresses a significant\ngap in the current LLM evaluation, which primarily focuses on mathematical and\ncoding reasoning tasks. In addition, given that Retrieval-Augmented Generation\n(RAG) systems are currently the dominant solutions in practical applications ,\nwe leverage this benchmark to evaluate existing well-known vector embedding\nmodels and LLMs for civil aviation maintenance scenarios. Through experimental\nexploration and analysis, we demonstrate the effectiveness of our benchmark in\nassessing model performance within this domain, and we open-source this\nevaluation benchmark and code to foster further research and\ndevelopment:https://github.com/CamBenchmark/cambenchmark", "AI": {"tldr": "本文提出了一项针对民用航空维修领域的大规模语言模型（LLMs）的专业评估工具，旨在填补当前评估工具的空白，并提供改进方向，从而推动该领域的智能化解决方案发展。实验表明，该基准测试在评估模型性能方面有效，并且开源以促进进一步的研究和开发。", "motivation": "当前LLMs的评估主要集中在数学和编程推理任务上，缺乏针对民用航空维修这样一个专业领域进行专门评估的工具，该领域需要复杂的推理和专业知识。本文旨在填补这一空白。", "method": "本文开发了一个专为民用航空维修领域设计的工业级基准。该基准用于测量LLMs的能力，识别领域知识和复杂推理方面的缺陷，并通过实验探索和分析，评估现有著名矢量嵌入模型和LLMs在民用航空维修情境中的表现。", "result": "实验结果展示了该基准在评估民用航空维修领域模型性能方面的有效性，并且开源了该评估基准和代码，以促进进一步的研究和发展。", "conclusion": "本文提出的评估基准填补了民用航空维修领域中LLMs评估工具的空白，并为有针对性的改进提供了基础，比如领域特定的微调、RAG优化或专门的提示工程。开源工作也将推动该领域的进一步研究和智能化解决方案的发展。"}}
{"id": "2508.20256", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.20256", "abs": "https://arxiv.org/abs/2508.20256", "authors": ["Zhen Xuen Brandon Low", "Rory Zhang", "Hang Min", "William Pham", "Lucy Vivash", "Jasmine Moses", "Miranda Lynch", "Karina Dorfman", "Cassandra Marotta", "Shaun Koh", "Jacob Bunyamin", "Ella Rowsthorn", "Alex Jarema", "Himashi Peiris", "Zhaolin Chen", "Sandy R. Shultz", "David K. Wright", "Dexiao Kong", "Sharon L. Naismith", "Terence J. O'Brien", "Ying Xia", "Meng Law", "Benjamin Sinclair"], "title": "MedNet-PVS: A MedNeXt-Based Deep Learning Model for Automated Segmentation of Perivascular Spaces", "comment": "59 pages, 9 figures", "summary": "Enlarged perivascular spaces (PVS) are increasingly recognized as biomarkers\nof cerebral small vessel disease, Alzheimer's disease, stroke, and\naging-related neurodegeneration. However, manual segmentation of PVS is\ntime-consuming and subject to moderate inter-rater reliability, while existing\nautomated deep learning models have moderate performance and typically fail to\ngeneralize across diverse clinical and research MRI datasets. We adapted\nMedNeXt-L-k5, a Transformer-inspired 3D encoder-decoder convolutional network,\nfor automated PVS segmentation. Two models were trained: one using a\nhomogeneous dataset of 200 T2-weighted (T2w) MRI scans from the Human\nConnectome Project-Aging (HCP-Aging) dataset and another using 40 heterogeneous\nT1-weighted (T1w) MRI volumes from seven studies across six scanners. Model\nperformance was evaluated using internal 5-fold cross validation (5FCV) and\nleave-one-site-out cross validation (LOSOCV). MedNeXt-L-k5 models trained on\nthe T2w images of the HCP-Aging dataset achieved voxel-level Dice scores of\n0.88+/-0.06 (white matter, WM), comparable to the reported inter-rater\nreliability of that dataset, and the highest yet reported in the literature.\nThe same models trained on the T1w images of the HCP-Aging dataset achieved a\nsubstantially lower Dice score of 0.58+/-0.09 (WM). Under LOSOCV, the model had\nvoxel-level Dice scores of 0.38+/-0.16 (WM) and 0.35+/-0.12 (BG), and\ncluster-level Dice scores of 0.61+/-0.19 (WM) and 0.62+/-0.21 (BG).\nMedNeXt-L-k5 provides an efficient solution for automated PVS segmentation\nacross diverse T1w and T2w MRI datasets. MedNeXt-L-k5 did not outperform the\nnnU-Net, indicating that the attention-based mechanisms present in\ntransformer-inspired models to provide global context are not required for high\naccuracy in PVS segmentation.", "AI": {"tldr": "论文探索了使用MedNeXt-L-k5进行PVS自动分割，取得了良好的效果，但未在某些情况下超越基准模型。", "motivation": "传统PVS手动分割费时且具有中等的评分者间可靠性，而现有的自动化深度学习模型表现出中等性能，并且通常无法在不同的临床和研究MRI数据集中进行泛化。这篇论文旨在提供一种自动化的解决方案。", "method": "使用了基于Transformer的3D编码-解码卷积网络MedNeXt-L-k5来进行自动PVS分割。构建了两个模型：一个使用来自Human Connectome Project-Aging数据集的200个T2加权MRI扫描进行训练，另一个使用来自七个研究中的六个扫描器的40个T1加权MRI体进行训练。", "result": "在HCP-Aging数据集的T2w图像上训练的MedNeXt-L-k5模型达到体素级Dice得分0.88±0.06（WM）。在LOSO交叉验证下，模型达到体素级Dice得分0.38±0.16（WM）和0.35±0.12（BG），团簇级Dice得分0.61±0.19（WM）和0.62±0.21（BG）。", "conclusion": "MedNeXt-L-k5为跨越不同T1w和T2w MRI数据集的PVS自动分割提供了有效解决方案，但未在PVS分割中超越当前基准模型。说明基于Transformer的全局上下文机制在这种任务中可能不是必要的。"}}
{"id": "2508.20442", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.20442", "abs": "https://arxiv.org/abs/2508.20442", "authors": ["Agung Sukrisna Jaya", "Osvari Arsalan", "Danny Matthew Saputra"], "title": "Searching the Title of Practical Work of the Informatics Engineering Bachelor Program with the Case Base Reasoning Method", "comment": null, "summary": "Case Base Reasoning (CBR) is a case solving technique based on experience in\ncases that have occurred before with the highest similarity. CBR is used to\nsearch for practical work titles. TF-IDF is applied to process the\nvectorization of each practical work title word and Cosine Similarity for the\ncalculation of similarity values. This system can search either in the form of\ntitles or keywords. The output of the system is the title of practical work and\nthe match value of each title. Based on the test results using 705 practical\nwork titles, testing was carried out with five titles and carried out in two\nstages. The first stage searches with existing titles and the second stage\nrandomizes the title from the first stage. And the results obtained in the\nsecond stage are the same number of titles found and the highest average match\nscore.", "AI": {"tldr": "本文通过CBR技术结合TF-IDF和余弦相似度算法实现实际工作标题的高效搜索，测试表明该系统在准确性和可靠性方面表现出色。", "motivation": "该研究旨在利用CBR技术，通过处理和搜索实际工作标题，为用户提供一个高效且准确的检索系统。", "method": "该论文采用案例推理(CBR)技术，结合TF-IDF和余弦相似度算法来为实际工作标题进行搜索。TF-IDF用于处理每个实际工作标题单词的向量化，余弦相似度用于计算相似性值。", "result": "测试结果显示，在使用705个实际工作标题进行的测试中，通过两个阶段的测试（第一阶段使用已有的标题搜索，第二阶段随机化第一阶段的标题），系统在第二阶段找到了同样数量的标题并且获得了最高的平均匹配得分。", "conclusion": "研究表明，基于CBR、TF-IDF和余弦相似度的系统能够有效地实现实际工作标题的搜索任务，并根据测试结果表明系统具有较高的准确性和可靠性。"}}
{"id": "2508.20265", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.20265", "abs": "https://arxiv.org/abs/2508.20265", "authors": ["Zhixiang Chi", "Yanan Wu", "Li Gu", "Huan Liu", "Ziqiang Wang", "Yang Zhang", "Yang Wang", "Konstantinos N. Plataniotis"], "title": "Plug-in Feedback Self-adaptive Attention in CLIP for Training-free Open-Vocabulary Segmentation", "comment": "ICCV 2025, code:https://github.com/chi-chi-zx/FSA", "summary": "CLIP exhibits strong visual-textual alignment but struggle with\nopen-vocabulary segmentation due to poor localization. Prior methods enhance\nspatial coherence by modifying intermediate attention. But, this coherence\nisn't consistently propagated to the final output due to subsequent operations\nsuch as projections. Additionally, intermediate attention lacks direct\ninteraction with text representations, such semantic discrepancy limits the\nfull potential of CLIP.\n  In this work, we propose a training-free, feedback-driven self-adaptive\nframework that adapts output-based patch-level correspondences back to the\nintermediate attention. The output predictions, being the culmination of the\nmodel's processing, encapsulate the most comprehensive visual and textual\nsemantics about each patch. Our approach enhances semantic consistency between\ninternal representations and final predictions by leveraging the model's\noutputs as a stronger spatial coherence prior. We design key modules, including\nattention isolation, confidence-based pruning for sparse adaptation, and\nadaptation ensemble, to effectively feedback the output coherence cues. Our\nmethod functions as a plug-in module, seamlessly integrating into four\nstate-of-the-art approaches with three backbones (ViT-B, ViT-L, ViT-H). We\nfurther validate our framework across multiple attention types (Q-K, self-self,\nand Proxy augmented with MAE, SAM, and DINO). Our approach consistently\nimproves their performance across eight benchmarks.", "AI": {"tldr": "This paper introduces a self-adaptive framework that enhances CLIP's capabilities by improving the interaction between output predictions and intermediate attention, leading to consistent performance improvements across multiple benchmarks.", "motivation": "The motivation behind this work is to address the limitations of the CLIP model, particularly its struggle with open-vocabulary segmentation due to poor localization and semantic discrepancies in intermediate attention, which do not adequately interact with text representations.", "method": "In this work, the authors propose a training-free, feedback-driven self-adaptive framework that enhances the semantic consistency between internal model representations and final predictions by leveraging output predictions as stronger spatial coherence priors. Key modules include attention isolation, confidence-based pruning for sparse adaptation, and adaptation ensemble.", "result": "The proposed method is validated across four state-of-the-art approaches with three different backbones. It is shown to consistently improve performance across eight benchmarks, demonstrating the effectiveness of the framework in enhancing semantic consistency and spatial coherence.", "conclusion": "The study concludes that the proposed feedback-driven self-adaptive framework can effectively improve the visual-textual alignment and performance of CLIP-like models without requiring additional training, making it a versatile enhancement module for various deep learning architectures."}}
{"id": "2508.20453", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.20453", "abs": "https://arxiv.org/abs/2508.20453", "authors": ["Zhenting Wang", "Qi Chang", "Hemani Patel", "Shashank Biju", "Cheng-En Wu", "Quan Liu", "Aolin Ding", "Alireza Rezazadeh", "Ankit Shah", "Yujia Bao", "Eugene Siow"], "title": "MCP-Bench: Benchmarking Tool-Using LLM Agents with Complex Real-World Tasks via MCP Servers", "comment": null, "summary": "We introduce MCP-Bench, a benchmark for evaluating large language models\n(LLMs) on realistic, multi-step tasks that demand tool use, cross-tool\ncoordination, precise parameter control, and planning/reasoning for solving\ntasks. Built on the Model Context Protocol (MCP), MCP-Bench connects LLMs to 28\nrepresentative live MCP servers spanning 250 tools across domains such as\nfinance, traveling, scientific computing, and academic search. Unlike prior\nAPI-based benchmarks, each MCP server provides a set of complementary tools\ndesigned to work together, enabling the construction of authentic, multi-step\ntasks with rich input-output coupling. Tasks in MCP-Bench test agents' ability\nto retrieve relevant tools from fuzzy instructions without explicit tool names,\nplan multi-hop execution trajectories for complex objectives, ground responses\nin intermediate tool outputs, and orchestrate cross-domain workflows -\ncapabilities not adequately evaluated by existing benchmarks that rely on\nexplicit tool specifications, shallow few-step workflows, and isolated domain\noperations. We propose a multi-faceted evaluation framework covering tool-level\nschema understanding and usage, trajectory-level planning, and task completion.\nExperiments on 20 advanced LLMs reveal persistent challenges in MCP-Bench. Code\nand data: https://github.com/Accenture/mcp-bench.", "AI": {"tldr": "本文提出了MCP-Bench，一个用于评测LLMs在多步骤任务上的表现的基准，在这些任务中，模型需要使用组合工具进行复杂任务，而不仅是依赖具体的多步操作和以往孤立领域的操作。通过这种方式，可以更真实地评估模型的能力。", "motivation": "与其他基于API的基准测试不同，MCP-Bench让每个MCP服务器提供一组互补工具，这些工具设计用于协同工作，从而能够构建具有丰富输入输出耦合的多步骤真实任务。", "method": "我们介绍了MCP-Bench，这是一个用于评估大型语言模型（LLMs）在涉及工具使用、跨工具协调、精确参数控制和规划/推理的多步骤现实任务上的基准。MCP-Bench基于模型上下文协议（MCP），将LLMs与28个具有代表性的MCP服务器连接，这些服务器包含来自金融、旅行、科学计算和学术搜索等领域的250种工具。", "result": "实验在20个先进LLMs上进行，显示了在MCP-Bench上存在的持续挑战。", "conclusion": "这项研究提出了一种多方面的评估框架，涵盖了工具层面的模式理解和使用、轨迹层面的规划以及任务完成情况。"}}
{"id": "2508.20279", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.20279", "abs": "https://arxiv.org/abs/2508.20279", "authors": ["Zhuoran Yu", "Yong Jae Lee"], "title": "How Multimodal LLMs Solve Image Tasks: A Lens on Visual Grounding, Task Reasoning, and Answer Decoding", "comment": "Accepted by COLM 2025", "summary": "Multimodal Large Language Models (MLLMs) have demonstrated strong performance\nacross a wide range of vision-language tasks, yet their internal processing\ndynamics remain underexplored. In this work, we introduce a probing framework\nto systematically analyze how MLLMs process visual and textual inputs across\nlayers. We train linear classifiers to predict fine-grained visual categories\n(e.g., dog breeds) from token embeddings extracted at each layer, using a\nstandardized anchor question. To uncover the functional roles of different\nlayers, we evaluate these probes under three types of controlled prompt\nvariations: (1) lexical variants that test sensitivity to surface-level\nchanges, (2) semantic negation variants that flip the expected answer by\nmodifying the visual concept in the prompt, and (3) output format variants that\npreserve reasoning but alter the answer format. Applying our framework to\nLLaVA-1.5, LLaVA-Next-LLaMA-3, and Qwen2-VL, we identify a consistent\nstage-wise structure in which early layers perform visual grounding, middle\nlayers support lexical integration and semantic reasoning, and final layers\nprepare task-specific outputs. We further show that while the overall\nstage-wise structure remains stable across variations in visual tokenization,\ninstruction tuning data, and pretraining corpus, the specific layer allocation\nto each stage shifts notably with changes in the base LLM architecture. Our\nfindings provide a unified perspective on the layer-wise organization of MLLMs\nand offer a lightweight, model-agnostic approach for analyzing multimodal\nrepresentation dynamics.", "AI": {"tldr": "研究提出了一个探针框架，通过分析使用MLLMs的各层次处理视觉和文本输入的效果，发现了一个跨多个模型的统一结构，揭示了模型内部的工作原理。", "motivation": "尽管多模态大规模语言模型已在广泛的视觉-语言任务中展示了强大的性能，但其内部处理机制仍然探索不足。研究希望通过系统性分析来揭开MLLMs中不同层的功能角色。", "method": "研究训练了线性分类器，从每一层提取的标记嵌入中预测细粒度的视觉类别（如狗的品种）。通过三种类型的受控提示变化进行评估：（1）词法规则变化；（2）语义否定变化；（3）输出格式变化。", "result": "研究通过引入一个探针框架，系统性地分析了多模态大规模语言模型（MLLMs）在各层如何处理视觉和文本输入。研究发现了一个稳定但又随着模型架构变化而有所调整的阶段性结构，早期层进行视觉接地，中期层支持词汇整合与语义推理，后期层准备特定任务输出。该框架在不同的视觉标记化、指令微调数据和预训练语料库下保持稳定。", "conclusion": "研究提供了一个统一的观点来看待MLLMs的层组织结构，并提出了一个简洁且模型不可知的方法来分析跨模式表示动力学。"}}
{"id": "2508.20460", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.20460", "abs": "https://arxiv.org/abs/2508.20460", "authors": ["Yucheng Ruan", "Xiang Lan", "Daniel J. Tan", "Hairil Rizal Abdullah", "Mengling Feng"], "title": "Prediction of mortality and resource utilization in critical care: a deep learning approach using multimodal electronic health records with natural language processing techniques", "comment": null, "summary": "Background Predicting mortality and resource utilization from electronic\nhealth records (EHRs) is challenging yet crucial for optimizing patient\noutcomes and managing costs in intensive care unit (ICU). Existing approaches\npredominantly focus on structured EHRs, often ignoring the valuable clinical\ninsights in free-text notes. Additionally, the potential of textual information\nwithin structured data is not fully leveraged. This study aimed to introduce\nand assess a deep learning framework using natural language processing\ntechniques that integrates multimodal EHRs to predict mortality and resource\nutilization in critical care settings. Methods Utilizing two real-world EHR\ndatasets, we developed and evaluated our model on three clinical tasks with\nleading existing methods. We also performed an ablation study on three key\ncomponents in our framework: medical prompts, free-texts, and pre-trained\nsentence encoder. Furthermore, we assessed the model's robustness against the\ncorruption in structured EHRs. Results Our experiments on two real-world\ndatasets across three clinical tasks showed that our proposed model improved\nperformance metrics by 1.6\\%/0.8\\% on BACC/AUROC for mortality prediction,\n0.5%/2.2% on RMSE/MAE for LOS prediction, 10.9%/11.0% on RMSE/MAE for surgical\nduration estimation compared to the best existing methods. It consistently\ndemonstrated superior performance compared to other baselines across three\ntasks at different corruption rates. Conclusions The proposed framework is an\neffective and accurate deep learning approach for predicting mortality and\nresource utilization in critical care. The study also highlights the success of\nusing prompt learning with a transformer encoder in analyzing multimodal EHRs.\nImportantly, the model showed strong resilience to data corruption within\nstructured data, especially at high corruption levels.", "AI": {"tldr": "研究开发了一种通过深度学习和自然语言处理技术整合多模态电子健康记录来预测重症监护患者死亡率和资源利用的新方法，在真实数据集上的实验表现优于现有方法。", "motivation": "预测死亡率和医疗资源利用率对于提高患者安全和管理成本至关重要，但通过电子健康记录（尤其是自由文本记录）进行预测极具挑战性。现有方法大多集中在结构化的电子健康记录上，忽视了文本中的关键临床洞察。这项研究旨在引入和评估一个利用自然语言处理技术的深度学习框架，整合多模态EHR进行重症监护环境下的死亡率和资源利用预测。", "method": "研究采用了一个结合自然语言处理技术的深度学习框架，该框架将多模态电子健康记录（EHR）用于预测重症监护环境中的死亡率和资源利用率。使用两个真实世界的EHR数据集，在三个临床任务上评估了与现有领先方法相比的表现，并对框架中的三个关键组件进行了消融研究。", "result": "实验结果显示，研究提出的模型在两个真实数据集上的三种临床任务上分别将死亡率预测的BACC和AUROC提高了1.6%和0.8%，资源使用长度预测（LOS）的RMSE和MAE提高了0.5%和2.2%，手术持续时间估计的RMSE和MAE提高了10.9%和11.0%。模型在不同的数据损坏率下均优于其他基线。", "conclusion": "研究提出的框架是一种在重症监护环境中预测死亡率和资源利用率时既有效又准确的深度学习方法。使用提示学习和变压器编码器分析多模态EHR取得成功，值得进一步研究。此外，该模型显示出对结构化数据中高损坏率的强健性。"}}
{"id": "2508.20322", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.20322", "abs": "https://arxiv.org/abs/2508.20322", "authors": ["Zhi Li", "Hau Phan", "Matthew Emigh", "Austin J. Brockmeier"], "title": "Disentangling Latent Embeddings with Sparse Linear Concept Subspaces (SLiCS)", "comment": null, "summary": "Vision-language co-embedding networks, such as CLIP, provide a latent\nembedding space with semantic information that is useful for downstream tasks.\nWe hypothesize that the embedding space can be disentangled to separate the\ninformation on the content of complex scenes by decomposing the embedding into\nmultiple concept-specific component vectors that lie in different subspaces. We\npropose a supervised dictionary learning approach to estimate a linear\nsynthesis model consisting of sparse, non-negative combinations of groups of\nvectors in the dictionary (atoms), whose group-wise activity matches the\nmulti-label information. Each concept-specific component is a non-negative\ncombination of atoms associated to a label. The group-structured dictionary is\noptimized through a novel alternating optimization with guaranteed convergence.\nExploiting the text co-embeddings, we detail how semantically meaningful\ndescriptions can be found based on text embeddings of words best approximated\nby a concept's group of atoms, and unsupervised dictionary learning can exploit\nzero-shot classification of training set images using the text embeddings of\nconcept labels to provide instance-wise multi-labels. We show that the\ndisentangled embeddings provided by our sparse linear concept subspaces (SLiCS)\nenable concept-filtered image retrieval (and conditional generation using\nimage-to-prompt) that is more precise. We also apply SLiCS to highly-compressed\nautoencoder embeddings from TiTok and the latent embedding from self-supervised\nDINOv2. Quantitative and qualitative results highlight the improved precision\nof the concept-filtered image retrieval for all embeddings.", "AI": {"tldr": "本文提出了一种名为SLiCS的方法，通过监督字典学习估计线性合成模型，实现了从视觉-语言共同嵌入网络中分解得到的概念过滤图像检索的精度提升。", "motivation": "我们假设嵌入空间可以被分解成多个概念特定的子空间中的组件向量，以此分离复杂场景的内容信息。目的是提高概念过滤图像检索的精度。", "method": "我们提出了一种监督字典学习方法来估计线性综合模型，该模型由字典中向量组的稀疏、非负组合构成，其组活动与多标签信息相匹配。每个特定概念的组成部分是与标签相关的原子的非负组合。该组结构字典通过一种新的交替优化方法进行优化，该方法具有收敛性保证。利用文本共同嵌入，我们详细说明了如何基于文本嵌入找到语义上有意义的描述，这些文本嵌入最好地近似于概念组的原子，并且无监督字典学习可以利用零样本分类来提供实例级别的多标签。", "result": "实验结果表明，使用我们提出的稀疏线性概念子空间（SLiCS）提供的解耦嵌入可以实现更精确的概念过滤图像检索和图像到提示的条件生成。我们也展示了SLiCS应用于高度压缩的自动编码器嵌入（如TiTok）和自我监督DINOv2的潜在嵌入上的应用，结果突出显示了概念过滤图像检索精度的提高。", "conclusion": "SLiCS方法能够提供更精确的概念过滤图像检索。"}}
{"id": "2508.20468", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.20468", "abs": "https://arxiv.org/abs/2508.20468", "authors": ["Luke Bates", "Max Glockner", "Preslav Nakov", "Iryna Gurevych"], "title": "ConspirED: A Dataset for Cognitive Traits of Conspiracy Theories and Large Language Model Safety", "comment": null, "summary": "Conspiracy theories erode public trust in science and institutions while\nresisting debunking by evolving and absorbing counter-evidence. As AI-generated\nmisinformation becomes increasingly sophisticated, understanding rhetorical\npatterns in conspiratorial content is important for developing interventions\nsuch as targeted prebunking and assessing AI vulnerabilities. We introduce\nConspirED (CONSPIR Evaluation Dataset), which captures the cognitive traits of\nconspiratorial ideation in multi-sentence excerpts (80--120 words) from online\nconspiracy articles, annotated using the CONSPIR cognitive framework\n(Lewandowsky and Cook, 2020). ConspirED is the first dataset of conspiratorial\ncontent annotated for general cognitive traits. Using ConspirED, we (i) develop\ncomputational models that identify conspiratorial traits and determine dominant\ntraits in text excerpts, and (ii) evaluate large language/reasoning model\n(LLM/LRM) robustness to conspiratorial inputs. We find that both are misaligned\nby conspiratorial content, producing output that mirrors input reasoning\npatterns, even when successfully deflecting comparable fact-checked\nmisinformation.", "AI": {"tldr": "本研究创建了ConspirED数据集，用于标注和分析阴谋论内容中的认知特征，并通过该数据集开发了识别模型和评估了LLM/LRM对阴谋论输入的反应，发现其易被影响。", "motivation": "阴谋论侵蚀公众对科学和机构的信任，且难以被揭穿，随着AI生成的虚假信息变得更加复杂，理解决议性内容中的修辞模式对于开发针对性事前提醒、评估人工智能脆弱性的干预措施至关重要。", "method": "本研究引入了ConspirED评估数据集，它捕捉了在线阴谋论文章中多句摘录（80-120字）的阴谋论思维认知特征，并使用CONSPIR认知框架进行标注。该数据集是首个标注了一般认知特征的阴谋论内容数据集。研究者利用ConspirED开发了识别阴谋论特征和判断文本摘录中主导特征的计算模型，并评估了大型语言/推理模型（LLM/LRM）对阴谋论输入的鲁棒性。", "result": "研究结果显示，对于阴谋论内容，计算模型能够识别和判定文本中的阴谋论思维特征，同时大语言/推理模型显示出对阴谋论输入的反应模式与输入相似，即使在能够成功拦截类似的事实核查类不实信息时也会受到误导。", "conclusion": "通过ConspirED数据集，研究揭示了阴谋论内容对计算模型识别能力和大语言/推理模型输出的显著影响，这为理解和干预阴谋论传播提供了新的视角。"}}
{"id": "2508.20345", "categories": ["cs.CV", "cs.HC"], "pdf": "https://arxiv.org/pdf/2508.20345", "abs": "https://arxiv.org/abs/2508.20345", "authors": ["Xiao Li", "Yanfan Zhu", "Ruining Deng", "Wei-Qi Wei", "Yu Wang", "Shilin Zhao", "Yaohong Wang", "Haichun Yang", "Yuankai Huo"], "title": "MedFoundationHub: A Lightweight and Secure Toolkit for Deploying Medical Vision Language Foundation Models", "comment": null, "summary": "Recent advances in medical vision-language models (VLMs) open up remarkable\nopportunities for clinical applications such as automated report generation,\ncopilots for physicians, and uncertainty quantification. However, despite their\npromise, medical VLMs introduce serious security concerns, most notably risks\nof Protected Health Information (PHI) exposure, data leakage, and vulnerability\nto cyberthreats - which are especially critical in hospital environments. Even\nwhen adopted for research or non-clinical purposes, healthcare organizations\nmust exercise caution and implement safeguards. To address these challenges, we\npresent MedFoundationHub, a graphical user interface (GUI) toolkit that: (1)\nenables physicians to manually select and use different models without\nprogramming expertise, (2) supports engineers in efficiently deploying medical\nVLMs in a plug-and-play fashion, with seamless integration of Hugging Face\nopen-source models, and (3) ensures privacy-preserving inference through\nDocker-orchestrated, operating system agnostic deployment. MedFoundationHub\nrequires only an offline local workstation equipped with a single NVIDIA A6000\nGPU, making it both secure and accessible within the typical resources of\nacademic research labs. To evaluate current capabilities, we engaged\nboard-certified pathologists to deploy and assess five state-of-the-art VLMs\n(Google-MedGemma3-4B, Qwen2-VL-7B-Instruct, Qwen2.5-VL-7B-Instruct, and\nLLaVA-1.5-7B/13B). Expert evaluation covered colon cases and renal cases,\nyielding 1015 clinician-model scoring events. These assessments revealed\nrecurring limitations, including off-target answers, vague reasoning, and\ninconsistent pathology terminology.", "AI": {"tldr": "本文介绍了MedFoundationHub，一个旨在解决医疗视觉语言模型部署中的安全挑战的工具包。评估结果显示这些模型在临床应用中存在一些问题，如精准度和术语一致性问题。", "motivation": "研究动机在于解决医疗VLM带来的安全问题，包括PHI暴露风险、数据泄露和对网络威胁的脆弱性。通过MedFoundationHub，研究人员希望提供一种安全且易于使用的工具，来促进医疗VLM的实际应用。", "method": "分析方法主要集中在通过MedFoundationHub工具包对医疗视觉语言模型(VLMs)进行部署和评估。该工具包提供了一个图形用户界面，使医生能够手动选择和使用不同的模型，同时支持工程师以即插即用的方式高效部署医疗VLM，并确保通过Docker容器化操作系统的无差异部署进行隐私保护推理。", "result": "研究结果显示了对五种最先进的VLM的评估结果，由经过认证的病理学家对结肠案例和肾脏案例评估，产生了1015个临床专家-模型评分事件。评估揭示了这些模型的常见局限性，例如答非所问、推理模糊和病理术语不一致。", "conclusion": "研究得出的结论是，尽管现代医疗VLM展现了巨大的潜力，但在临床应用中仍面临显著的技术和安全挑战，需要进一步完善模型以提高其临床价值和安全性。"}}
{"id": "2508.20511", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.20511", "abs": "https://arxiv.org/abs/2508.20511", "authors": ["Chihiro Taguchi", "Seng Mai", "Keita Kurabe", "Yusuke Sakai", "Georgina Agyei", "Soudabeh Eslami", "David Chiang"], "title": "Languages Still Left Behind: Toward a Better Multilingual Machine Translation Benchmark", "comment": "13 pages, 7 tables, 2 figures. Accepted at EMNLP Main 2025. Code and\n  data released at https://github.com/ctaguchi/LSLB", "summary": "Multilingual machine translation (MT) benchmarks play a central role in\nevaluating the capabilities of modern MT systems. Among them, the FLORES+\nbenchmark is widely used, offering English-to-many translation data for over\n200 languages, curated with strict quality control protocols. However, we study\ndata in four languages (Asante Twi, Japanese, Jinghpaw, and South Azerbaijani)\nand uncover critical shortcomings in the benchmark's suitability for truly\nmultilingual evaluation. Human assessments reveal that many translations fall\nbelow the claimed 90% quality standard, and the annotators report that source\nsentences are often too domain-specific and culturally biased toward the\nEnglish-speaking world. We further demonstrate that simple heuristics, such as\ncopying named entities, can yield non-trivial BLEU scores, suggesting\nvulnerabilities in the evaluation protocol. Notably, we show that MT models\ntrained on high-quality, naturalistic data perform poorly on FLORES+ while\nachieving significant gains on our domain-relevant evaluation set. Based on\nthese findings, we advocate for multilingual MT benchmarks that use\ndomain-general and culturally neutral source texts rely less on named entities,\nin order to better reflect real-world translation challenges.", "AI": {"tldr": "本研究揭示了广泛使用的多语言翻译基准FLORES+在评估现代翻译系统的能力方面存在的不足，提出需要改进评估基准，使之更加贴近实际翻译挑战。", "motivation": "本研究的动机在于，现有的FLORES+多语言翻译基准虽然在质量控制上严格，但在实际的多语言评估中存在不足，特别是数据的领域特定性和文化偏向性。", "method": "本研究采用了人工评估的方法，选择四种语言（Asante Twi、日语、Jinghpaw 和南阿塞拜疆语）的数据，揭露了FLORES+基准在真正多语言评估中的不足之处。", "result": "人工评估发现许多翻译品质低于90%标准，源句对特定领域和文化的偏见严重。简单地复制命名实体等基本策略就能取得不错的BLEU分数，显示出评估协议的脆弱性。", "conclusion": "研究表明，训练在高质量、自然数据上的MT模型在FLORES+上的表现不如在领域相关评估集上的表现，因此提倡使用领域通用且文化中立的源文本作为多语言MT基准。"}}
{"id": "2508.20376", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.20376", "abs": "https://arxiv.org/abs/2508.20376", "authors": ["Mang Cao", "Sanping Zhou", "Yizhe Li", "Ye Deng", "Wenli Huang", "Le Wang"], "title": "Enhancing Mamba Decoder with Bidirectional Interaction in Multi-Task Dense Prediction", "comment": "Codes are available online:\n  \\url{https://github.com/mmm-cc/BIM\\_for\\_MTL}", "summary": "Sufficient cross-task interaction is crucial for success in multi-task dense\nprediction. However, sufficient interaction often results in high computational\ncomplexity, forcing existing methods to face the trade-off between interaction\ncompleteness and computational efficiency. To address this limitation, this\nwork proposes a Bidirectional Interaction Mamba (BIM), which incorporates novel\nscanning mechanisms to adapt the Mamba modeling approach for multi-task dense\nprediction. On the one hand, we introduce a novel Bidirectional Interaction\nScan (BI-Scan) mechanism, which constructs task-specific representations as\nbidirectional sequences during interaction. By integrating task-first and\nposition-first scanning modes within a unified linear complexity architecture,\nBI-Scan efficiently preserves critical cross-task information. On the other\nhand, we employ a Multi-Scale Scan~(MS-Scan) mechanism to achieve\nmulti-granularity scene modeling. This design not only meets the diverse\ngranularity requirements of various tasks but also enhances nuanced cross-task\nfeature interactions. Extensive experiments on two challenging benchmarks,\n\\emph{i.e.}, NYUD-V2 and PASCAL-Context, show the superiority of our BIM vs its\nstate-of-the-art competitors.", "AI": {"tldr": "本文提出了双向交互曼巴（BIM）模型，通过引入双向交互扫描（BI-Scan）和多尺度扫描（MS-Scan）机制来增强多任务密集预测中的跨任务交互，同时保持计算效率。实验表明BIM优于当前的先进方法。", "motivation": "解决现有方法在多任务密集预测中由于需要充分的跨任务交互而导致的计算复杂度高，难以平衡交互完整性和计算效率的问题。", "method": "引入了Bidirectional Interaction Scan (BI-Scan) 机制和Multi-Scale Scan (MS-Scan) 机制。BI-Scan采取任务优先和位置优先的扫描模式，以统一的线性复杂度架构构建任务特定的双向序列，保持关键跨任务信息；而MS-Scan实现了多粒度场景建模，同时提升了跨任务特征交互的细腻度。", "result": "在NYUD-V2和PASCAL-Context两个极具挑战性的基准测试中，BIM方法相对于其他先进方法表现更优越。", "conclusion": "文章通过引入BIM模型，证实了在多任务密集预测中，通过优化交互机制可以在提升跨任务交互的同时保持计算效率的可行性。"}}
{"id": "2508.20514", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.20514", "abs": "https://arxiv.org/abs/2508.20514", "authors": ["Pengjiang Li", "Zaitian Wang", "Xinhao Zhang", "Ran Zhang", "Lu Jiang", "Pengfei Wang", "Yuanchun Zhou"], "title": "SciTopic: Enhancing Topic Discovery in Scientific Literature through Advanced LLM", "comment": null, "summary": "Topic discovery in scientific literature provides valuable insights for\nresearchers to identify emerging trends and explore new avenues for\ninvestigation, facilitating easier scientific information retrieval. Many\nmachine learning methods, particularly deep embedding techniques, have been\napplied to discover research topics. However, most existing topic discovery\nmethods rely on word embedding to capture the semantics and lack a\ncomprehensive understanding of scientific publications, struggling with\ncomplex, high-dimensional text relationships. Inspired by the exceptional\ncomprehension of textual information by large language models (LLMs), we\npropose an advanced topic discovery method enhanced by LLMs to improve\nscientific topic identification, namely SciTopic. Specifically, we first build\na textual encoder to capture the content from scientific publications,\nincluding metadata, title, and abstract. Next, we construct a space\noptimization module that integrates entropy-based sampling and triplet tasks\nguided by LLMs, enhancing the focus on thematic relevance and contextual\nintricacies between ambiguous instances. Then, we propose to fine-tune the\ntextual encoder based on the guidance from the LLMs by optimizing the\ncontrastive loss of the triplets, forcing the text encoder to better\ndiscriminate instances of different topics. Finally, extensive experiments\nconducted on three real-world datasets of scientific publications demonstrate\nthat SciTopic outperforms the state-of-the-art (SOTA) scientific topic\ndiscovery methods, enabling researchers to gain deeper and faster insights.", "AI": {"tldr": "SciTopic, enhanced by large language models, offers improved topic discovery in scientific literature by better capturing nuanced relationships within text and outperforms existing methods.", "motivation": "To find a better solution for topic discovery in scientific literature that can understand the complex and high-dimensional relationships within text, surpassing the limitations of word embedding methods.", "method": "The paper proposes a method named SciTopic which first builds a textual encoder to understand the content, then an optimization module is created using LLMs, which helps in understanding thematic relevance and contextual intricacies. Lastly, it fine-tunes the encoder by optimizing contrastive loss under LLM guidance.", "result": "The experimental results on three real-world datasets show that SciTopic outperforms existing methods in scientific topic discovery, providing deeper and faster insights.", "conclusion": "The proposed method, SciTopic, demonstrates superior performance in topic discovery due to its advanced understanding of textual nuances, thus aiding researchers in identifying trends more effectively."}}
{"id": "2508.20379", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.20379", "abs": "https://arxiv.org/abs/2508.20379", "authors": ["Hyeonyu Kim", "Seokhoon Jeong", "Seonghee Han", "Chanhyuk Choi", "Taehwan Kim"], "title": "Audio-Guided Visual Editing with Complex Multi-Modal Prompts", "comment": "Accepted to BMVC 2025", "summary": "Visual editing with diffusion models has made significant progress but often\nstruggles with complex scenarios that textual guidance alone could not\nadequately describe, highlighting the need for additional non-text editing\nprompts. In this work, we introduce a novel audio-guided visual editing\nframework that can handle complex editing tasks with multiple text and audio\nprompts without requiring additional training. Existing audio-guided visual\nediting methods often necessitate training on specific datasets to align audio\nwith text, limiting their generalization to real-world situations. We leverage\na pre-trained multi-modal encoder with strong zero-shot capabilities and\nintegrate diverse audio into visual editing tasks, by alleviating the\ndiscrepancy between the audio encoder space and the diffusion model's prompt\nencoder space. Additionally, we propose a novel approach to handle complex\nscenarios with multiple and multi-modal editing prompts through our separate\nnoise branching and adaptive patch selection. Our comprehensive experiments on\ndiverse editing tasks demonstrate that our framework excels in handling\ncomplicated editing scenarios by incorporating rich information from audio,\nwhere text-only approaches fail.", "AI": {"tldr": "本文提出了一种新的音频引导的视觉编辑框架，能够处理复杂的多模态编辑任务，并展示了从音频中获取信息以增强视觉编辑能力的优势。", "motivation": "视觉编辑使用扩散模型已经取得了显著的进步，但在仅靠文本难以描述的复杂场景中往往遇到困难。这揭示了需要额外的非文本编辑提示的需求。", "method": "我们提出了一种基于音频引导的视觉编辑框架，它可以处理多个文本和音频提示的复杂编辑任务，并且不需要额外的训练。我们利用了一个具有强零样本能力的预训练多模态编码器，并将多样化的音频整合进视觉编辑任务中，以消除音频编码器空间和扩散模型的提示编码器空间之间的差异。此外，我们提出了一种新的方法来处理复杂的多种模态编辑提示，通过我们独立的噪声分支和自适应补丁选择来实现。", "result": "我们在各种编辑任务上的综合实验表明，我们的框架在处理复杂编辑场景时表现优异，能够从音频中融入丰富的信息，而文本单独的方法则无法做到这一点。", "conclusion": "我们提出的方法展示了在处理复杂编辑场景时使用音频和文本结合的优越性，而不仅仅依靠文本。这项工作解决了当前音频引导的视觉编辑方法在处理多模态编辑提示时的局限性。"}}
{"id": "2508.20532", "categories": ["cs.CL", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2508.20532", "abs": "https://arxiv.org/abs/2508.20532", "authors": ["Anastasios Nentidis", "Georgios Katsimpras", "Anastasia Krithara", "Salvador Lima-López", "Eulàlia Farré-Maduell", "Martin Krallinger", "Natalia Loukachevitch", "Vera Davydova", "Elena Tutubalina", "Georgios Paliouras"], "title": "Overview of BioASQ 2024: The twelfth BioASQ challenge on Large-Scale Biomedical Semantic Indexing and Question Answering", "comment": "25 pages, 16 tables, 1 figure", "summary": "This is an overview of the twelfth edition of the BioASQ challenge in the\ncontext of the Conference and Labs of the Evaluation Forum (CLEF) 2024. BioASQ\nis a series of international challenges promoting advances in large-scale\nbiomedical semantic indexing and question answering. This year, BioASQ\nconsisted of new editions of the two established tasks b and Synergy, and two\nnew tasks: a) MultiCardioNER on the adaptation of clinical entity detection to\nthe cardiology domain in a multilingual setting, and b) BIONNE on nested NER in\nRussian and English. In this edition of BioASQ, 37 competing teams participated\nwith more than 700 distinct submissions in total for the four different shared\ntasks of the challenge. Similarly to previous editions, most of the\nparticipating systems achieved competitive performance, suggesting the\ncontinuous advancement of the state-of-the-art in the field.", "AI": {"tldr": "BioASQ第十二届挑战赛在CLEF 2024的背景下召开，包括两个既有任务的新版和两个新领域的任务，共有37支队伍参赛，展示了生物医学领域的技术进步。", "motivation": "BioASQ挑战赛的动机在于通过国际竞赛促进大规模生物医学语义索引和问答系统的进步。", "method": "分析论文摘要的内容，发现其主要是关于BioASQ第十二届挑战赛的概览，该挑战赛旨在推进大规模生物医学语义索引和问答技术的进展。今年的BioASQ包括了两个已有任务b和Synergy的新版本，以及两个新任务：MultiCardioNER和BIONNE。该版BioASQ共有37支参赛队伍，总共提交了超过700份不同的参赛作品，涵盖了四个不同的共享任务。", "result": "大多数参赛系统的性能表现具有竞争力，表明领域内的技术水平正在持续进步。", "conclusion": "本版BioASQ展示了参赛系统在多个任务上的高水平表现，表明生物医学语义索引和问答系统的开发水平持续提升。"}}
{"id": "2508.20381", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.20381", "abs": "https://arxiv.org/abs/2508.20381", "authors": ["Luong Tran", "Thieu Vo", "Anh Nguyen", "Sang Dinh", "Van Nguyen"], "title": "More Reliable Pseudo-labels, Better Performance: A Generalized Approach to Single Positive Multi-label Learning", "comment": "ICCV 2025", "summary": "Multi-label learning is a challenging computer vision task that requires\nassigning multiple categories to each image. However, fully annotating\nlarge-scale datasets is often impractical due to high costs and effort,\nmotivating the study of learning from partially annotated data. In the extreme\ncase of Single Positive Multi-Label Learning (SPML), each image is provided\nwith only one positive label, while all other labels remain unannotated.\nTraditional SPML methods that treat missing labels as unknown or negative tend\nto yield inaccuracies and false negatives, and integrating various\npseudo-labeling strategies can introduce additional noise. To address these\nchallenges, we propose the Generalized Pseudo-Label Robust Loss (GPR Loss), a\nnovel loss function that effectively learns from diverse pseudo-labels while\nmitigating noise. Complementing this, we introduce a simple yet effective\nDynamic Augmented Multi-focus Pseudo-labeling (DAMP) technique. Together, these\ncontributions form the Adaptive and Efficient Vision-Language Pseudo-Labeling\n(AEVLP) framework. Extensive experiments on four benchmark datasets demonstrate\nthat our framework significantly advances multi-label classification, achieving\nstate-of-the-art results.", "AI": {"tldr": "本文提出了一种名为AEVLP的框架，包含GPR Loss和DAMP技术，提高了从部分标注数据中学习的多标签分类任务的性能。", "motivation": "由于大规模数据集的完整注释代价高昂，研究从部分注释数据中学习具有重要意义，特别是在每个图像只有一个正确标注标签的情况下。", "method": "我们提出了Generalized Pseudo-Label Robust Loss (GPR Loss) 损失函数和Dynamic Augmented Multi-focus Pseudo-labeling (DAMP) 技术，作为一个自适应和高效的视觉-语言伪标签框架（AEVLP）的一部分。", "result": "在四个基准数据集上的广泛实验表明，该框架在多标签分类中显著提高了性能，达到了最先进的结果。", "conclusion": "AEVLP框架有效地从各种伪标签中学习，同时减少了噪声影响，特别是在Single Positive Multi-Label Learning（SPML）场景中。"}}
{"id": "2508.20554", "categories": ["cs.CL", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2508.20554", "abs": "https://arxiv.org/abs/2508.20554", "authors": ["Anastasios Nentidis", "Georgios Katsimpras", "Anastasia Krithara", "Martin Krallinger", "Miguel Rodríguez-Ortega", "Eduard Rodriguez-López", "Natalia Loukachevitch", "Andrey Sakhovskiy", "Elena Tutubalina", "Dimitris Dimitriadis", "Grigorios Tsoumakas", "George Giannakoulas", "Alexandra Bekiaridou", "Athanasios Samaras", "Giorgio Maria Di Nunzio", "Nicola Ferro", "Stefano Marchesin", "Marco Martinelli", "Gianmaria Silvello", "Georgios Paliouras"], "title": "Overview of BioASQ 2025: The Thirteenth BioASQ Challenge on Large-Scale Biomedical Semantic Indexing and Question Answering", "comment": "26 pages, 17 tables, 1 figure", "summary": "This is an overview of the thirteenth edition of the BioASQ challenge in the\ncontext of the Conference and Labs of the Evaluation Forum (CLEF) 2025. BioASQ\nis a series of international challenges promoting advances in large-scale\nbiomedical semantic indexing and question answering. This year, BioASQ\nconsisted of new editions of the two established tasks, b and Synergy, and four\nnew tasks: a) Task MultiClinSum on multilingual clinical summarization. b) Task\nBioNNE-L on nested named entity linking in Russian and English. c) Task\nELCardioCC on clinical coding in cardiology. d) Task GutBrainIE on gut-brain\ninterplay information extraction. In this edition of BioASQ, 83 competing teams\nparticipated with more than 1000 distinct submissions in total for the six\ndifferent shared tasks of the challenge. Similar to previous editions, several\nparticipating systems achieved competitive performance, indicating the\ncontinuous advancement of the state-of-the-art in the field.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2508.20392", "categories": ["cs.CV", "cs.AI", "I.4.0; I.2.6"], "pdf": "https://arxiv.org/pdf/2508.20392", "abs": "https://arxiv.org/abs/2508.20392", "authors": ["Chengjun Zhang", "Yuhao Zhang", "Jie Yang", "Mohamad Sawan"], "title": "Ultra-Low-Latency Spiking Neural Networks with Temporal-Dependent Integrate-and-Fire Neuron Model for Objects Detection", "comment": "12 pages, 8 figures", "summary": "Spiking Neural Networks (SNNs), inspired by the brain, are characterized by\nminimal power consumption and swift inference capabilities on neuromorphic\nhardware, and have been widely applied to various visual perception tasks.\nCurrent ANN-SNN conversion methods have achieved excellent results in\nclassification tasks with ultra-low time-steps, but their performance in visual\ndetection tasks remains suboptimal. In this paper, we propose a delay-spike\napproach to mitigate the issue of residual membrane potential caused by\nheterogeneous spiking patterns. Furthermore, we propose a novel\ntemporal-dependent Integrate-and-Fire (tdIF) neuron architecture for SNNs. This\nenables Integrate-and-fire (IF) neurons to dynamically adjust their\naccumulation and firing behaviors based on the temporal order of time-steps.\nOur method enables spikes to exhibit distinct temporal properties, rather than\nrelying solely on frequency-based representations. Moreover, the tdIF neuron\nmaintains energy consumption on par with traditional IF neuron. We demonstrate\nthat our method achieves more precise feature representation with lower\ntime-steps, enabling high performance and ultra-low latency in visual detection\ntasks. In this study, we conduct extensive evaluation of the tdIF method across\ntwo critical vision tasks: object detection and lane line detection. The\nresults demonstrate that the proposed method surpasses current ANN-SNN\nconversion approaches, achieving state-of-the-art performance with ultra-low\nlatency (within 5 time-steps).", "AI": {"tldr": "A new method for Spiking Neural Networks (SNNs) to enhance visual detection performance is proposed, introducing a delay-spike approach and tdIF neuron architecture for better temporal feature representation.", "motivation": "The motivation is to improve the performance of SNNs in visual detection tasks by resolving the heterogeneity in spiking patterns, which affects feature representation, leading to suboptimal performance compared to classification tasks.", "method": "The paper introduces a delay-spike method to address the problem of residual membrane potential and proposes a new temporal-dependent Integrate-and-Fire (tdIF) neuron architecture, which allows IF neurons to dynamically adjust their behaviors based on the sequence of time-steps.", "result": "The tdIF neuron and delay-spike method show better feature representation with fewer time-steps, enabling high performance and low latency. The method outperforms ANN-SNN conversion approaches in two vision tasks: object detection and lane line detection.", "conclusion": "The tdIF neuron and delay-spike method enable SNNs to achieve state-of-the-art performance in visual detection tasks with ultra-low latency (within 5 time-steps), making significant advancements in the field of SNN applications."}}
{"id": "2508.20557", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.20557", "abs": "https://arxiv.org/abs/2508.20557", "authors": ["Jiahao Xiao", "Jiangming Liu"], "title": "Adaptive Federated Distillation for Multi-Domain Non-IID Textual Data", "comment": null, "summary": "The widespread success of pre-trained language models has established a new\ntraining paradigm, where a global PLM is fine-tuned using task-specific data\nfrom local clients. The local data are highly different from each other and can\nnot capture the global distribution of the whole data in real world. To address\nthe challenges of non-IID data in real environments, privacy-preserving\nfederated distillation has been proposed and highly investigated. However,\nprevious experimental non-IID scenarios are primarily identified with the label\n(output) diversity, without considering the diversity of language domains\n(input) that is crucial in natural language processing. In this paper, we\nintroduce a comprehensive set of multi-domain non-IID scenarios and propose a\nunified benchmarking framework that includes diverse data. The benchmark can be\nused to evaluate the federated learning framework in a real environment. To\nthis end, we propose an Adaptive Federated Distillation (AdaFD) framework\ndesigned to address multi-domain non-IID challenges in both homogeneous and\nheterogeneous settings. Experimental results demonstrate that our models\ncapture the diversity of local clients and achieve better performance compared\nto the existing works. The code for this paper is available at:\nhttps://github.com/jiahaoxiao1228/AdaFD.", "AI": {"tldr": "本文提出并研究了一个新的联邦学习框架AdaFD，它能更好地处理多领域的非独立同分布环境，并在实验中证明了其有效性。", "motivation": "先前的工作主要关注输出多样性的非独立同分布情况，忽略了输入的多样性，这对自然语言处理至关重要。我们致力于解决这个问题。", "method": "我们提出了一个自适应联邦蒸馏（AdaFD）框架，旨在解决多领域非独立同分布（non-IID）挑战，适用于同质和异质环境。", "result": "实验证明，我们的模型能够捕捉客户本地的多样性，并在性能上优于现有的工作。", "conclusion": "提出的AdaFD框架能够在处理多领域非独立同分布数据时表现出色，并为联邦学习提供了评估框架和基准。"}}
{"id": "2508.20415", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.20415", "abs": "https://arxiv.org/abs/2508.20415", "authors": ["Yuqi Xiong", "Wuzhen Shi", "Yang Wen", "Ruhan Liu"], "title": "Graph-Based Uncertainty Modeling and Multimodal Fusion for Salient Object Detection", "comment": "ICONIP 2025", "summary": "In view of the problems that existing salient object detection (SOD) methods\nare prone to losing details, blurring edges, and insufficient fusion of\nsingle-modal information in complex scenes, this paper proposes a dynamic\nuncertainty propagation and multimodal collaborative reasoning network\n(DUP-MCRNet). Firstly, a dynamic uncertainty graph convolution module (DUGC) is\ndesigned to propagate uncertainty between layers through a sparse graph\nconstructed based on spatial semantic distance, and combined with channel\nadaptive interaction, it effectively improves the detection accuracy of small\nstructures and edge regions. Secondly, a multimodal collaborative fusion\nstrategy (MCF) is proposed, which uses learnable modality gating weights to\nweightedly fuse the attention maps of RGB, depth, and edge features. It can\ndynamically adjust the importance of each modality according to different\nscenes, effectively suppress redundant or interfering information, and\nstrengthen the semantic complementarity and consistency between\ncross-modalities, thereby improving the ability to identify salient regions\nunder occlusion, weak texture or background interference. Finally, the\ndetection performance at the pixel level and region level is optimized through\nmulti-scale BCE and IoU loss, cross-scale consistency constraints, and\nuncertainty-guided supervision mechanisms. Extensive experiments show that\nDUP-MCRNet outperforms various SOD methods on most common benchmark datasets,\nespecially in terms of edge clarity and robustness to complex backgrounds. Our\ncode is publicly available at https://github.com/YukiBear426/DUP-MCRNet.", "AI": {"tldr": "本文提出了一种动态不确定性传播和多模态协作推理网络(DUP-MCRNet)，解决了现有显着目标检测方法容易丢失细节、模糊边缘及单一模态信息融合不足的问题。", "motivation": "旨在解决现有显着目标检测方法在复杂场景中易出现细节丢失、边缘模糊以及单一模态信息融合不足的问题，以提高显着目标检测性能。", "method": "设计了动态不确定性图卷积模块(DUGC)来通过基于空间语义距离的稀疏图在层间传播不确定性，并结合通道自适应互作用。提出了多模态协作融合策略(MCF)，通过可学习模态门权重加权融合RGB、深度和边缘特征，根据不同场景动态调整各模态的重要性。", "result": "DUP-MCRNet在大多数常见基准数据集上的显着目标检测性能要优于各种现有方法，尤其是在边缘清晰度和对复杂背景的鲁棒性方面表现突出。", "conclusion": "所提出的DUP-MCRNet能够更准确和鲁棒地检测显着目标，特别适合于复杂背景下的边缘和细小结构的检测。"}}
{"id": "2508.20559", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2508.20559", "abs": "https://arxiv.org/abs/2508.20559", "authors": ["Zeyu Xiong", "Yixuan Nan", "Li Gao", "Hengzhu Tang", "Shuaiqiang Wang", "Junfeng Wang", "Dawei Yin"], "title": "Leveraging Generative Models for Real-Time Query-Driven Text Summarization in Large-Scale Web Search", "comment": "CIKM'25", "summary": "In the dynamic landscape of large-scale web search, Query-Driven Text\nSummarization (QDTS) aims to generate concise and informative summaries from\ntextual documents based on a given query, which is essential for improving user\nengagement and facilitating rapid decision-making. Traditional extractive\nsummarization models, based primarily on ranking candidate summary segments,\nhave been the dominant approach in industrial applications. However, these\napproaches suffer from two key limitations: 1) The multi-stage pipeline often\nintroduces cumulative information loss and architectural bottlenecks due to its\nweakest component; 2) Traditional models lack sufficient semantic understanding\nof both user queries and documents, particularly when dealing with complex\nsearch intents. In this study, we propose a novel framework to pioneer the\napplication of generative models to address real-time QDTS in industrial web\nsearch. Our approach integrates large model distillation, supervised\nfine-tuning, direct preference optimization, and lookahead decoding to\ntransform a lightweight model with only 0.1B parameters into a\ndomain-specialized QDTS expert. Evaluated on multiple industry-relevant\nmetrics, our model outperforms the production baseline and achieves a new state\nof the art. Furthermore, it demonstrates excellent deployment efficiency,\nrequiring only 334 NVIDIA L20 GPUs to handle \\textasciitilde50,000 queries per\nsecond under 55~ms average latency per query.", "AI": {"tldr": "本研究提出了一改进的框架，通过新颖的方法优化实时的查询驱动文本总结过程。该方法使轻量级模型在大规模网络搜索中表现出色，不仅提高了总结质量，也优化了部署效率。", "motivation": "传统的提取式总结模型由于多阶段管道引入的信息损失和架构瓶颈，以及对用户查询和文档语义理解不足的缺点，往往是工业应用中的主要方法。考虑到这些限制，本研究动机在于提出一种更有效的方法，以改善大规模网络搜索中的即时总结性能。", "method": "本研究提出了一种将生成模型应用于实时QDTS（Query-Driven Text Summarization，基于查询的文本总结）的新型框架。该框架整合了大规模模型提炼、监督微调、直接偏好优化以及前瞻解码，将一个仅有0.1B参数的轻量级模型转化为了一个领域专用的QDTS专家。", "result": "相比于生产基线，本研究提出的模型在多个工业相关指标上表现更优越，达到了新的先进水平。同时也具有优秀的部署效率，仅需334个NVIDIA L20 GPU即可实现约50,000查询每秒的处理速度。", "conclusion": "通过对多个与工业相关的指标进行评估，本研究中的模型超越了生产基线，并达到了一个新的先进水平。此外，模型还表现出优秀的部署效率，仅需334个NVIDIA L20 GPU即可处理约50,000查询每秒，平均每个查询延时在55毫秒以内。"}}
{"id": "2508.20447", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.20447", "abs": "https://arxiv.org/abs/2508.20447", "authors": ["Taiga Yamane", "Satoshi Suzuki", "Ryo Masumura", "Shota Orihashi", "Tomohiro Tanaka", "Mana Ihori", "Naoki Makishima", "Naotaka Kawata"], "title": "MSMVD: Exploiting Multi-scale Image Features via Multi-scale BEV Features for Multi-view Pedestrian Detection", "comment": "Accepted by BMVC 2025", "summary": "Multi-View Pedestrian Detection (MVPD) aims to detect pedestrians in the form\nof a bird's eye view (BEV) from multi-view images. In MVPD, end-to-end\ntrainable deep learning methods have progressed greatly. However, they often\nstruggle to detect pedestrians with consistently small or large scales in views\nor with vastly different scales between views. This is because they do not\nexploit multi-scale image features to generate the BEV feature and detect\npedestrians. To overcome this problem, we propose a novel MVPD method, called\nMulti-Scale Multi-View Detection (MSMVD). MSMVD generates multi-scale BEV\nfeatures by projecting multi-scale image features extracted from individual\nviews into the BEV space, scale-by-scale. Each of these BEV features inherits\nthe properties of its corresponding scale image features from multiple views.\nTherefore, these BEV features help the precise detection of pedestrians with\nconsistently small or large scales in views. Then, MSMVD combines information\nat different scales of multiple views by processing the multi-scale BEV\nfeatures using a feature pyramid network. This improves the detection of\npedestrians with vastly different scales between views. Extensive experiments\ndemonstrate that exploiting multi-scale image features via multi-scale BEV\nfeatures greatly improves the detection performance, and MSMVD outperforms the\nprevious highest MODA by $4.5$ points on the GMVD dataset.", "AI": {"tldr": "The paper presents MSMVD, a new method to improve pedestrian detection in multi-view images by creating multi-scale bird's eye view features and combining them using a feature pyramid network, which significantly boosts detection accuracy over previous methods on the GMVD dataset.", "motivation": "The motivation behind this research is to improve the performance of Multi-View Pedestrian Detection (MVPD) systems. Current deep learning approaches struggle with detecting pedestrians that are consistently small or large in scale across multiple views due to their inability to efficiently use multi-scale image features.", "method": "The paper proposes a method called Multi-Scale Multi-View Detection (MSMVD). MSMVD generates multi-scale bird’s eye view (BEV) features by projecting multi-scale image features from each view. The combination of multi-scale BEV features through a feature pyramid network enhances pedestrian detection across different scales.", "result": "Experiments show that MSMVD effectively improves pedestrian detection performance by utilizing multi-scale image features, leading to a 4.5 point increase in MODA compared to the previous best results on the GMVD dataset.", "conclusion": "The research concludes that leveraging multi-scale BEV features to detect pedestrians across various scales in multi-view images is effective, with MSMVD demonstrating superior performance to existing methods on the GMVD dataset."}}
{"id": "2508.20567", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.20567", "abs": "https://arxiv.org/abs/2508.20567", "authors": ["Yangfan Wang", "Jie Liu", "Chen Tang", "Lian Yan", "Jingchi Jiang"], "title": "KCS: Diversify Multi-hop Question Generation with Knowledge Composition Sampling", "comment": null, "summary": "Multi-hop question answering faces substantial challenges due to data\nsparsity, which increases the likelihood of language models learning spurious\npatterns. To address this issue, prior research has focused on diversifying\nquestion generation through content planning and varied expression. However,\nthese approaches often emphasize generating simple questions and neglect the\nintegration of essential knowledge, such as relevant sentences within\ndocuments. This paper introduces the Knowledge Composition Sampling (KCS), an\ninnovative framework designed to expand the diversity of generated multi-hop\nquestions by sampling varied knowledge compositions within a given context. KCS\nmodels the knowledge composition selection as a sentence-level conditional\nprediction task and utilizes a probabilistic contrastive loss to predict the\nnext most relevant piece of knowledge. During inference, we employ a stochastic\ndecoding strategy to effectively balance accuracy and diversity. Compared to\ncompetitive baselines, our KCS improves the overall accuracy of knowledge\ncomposition selection by 3.9%, and its application for data augmentation yields\nimprovements on HotpotQA and 2WikiMultihopQA datasets. Our code is available\nat: https://github.com/yangfanww/kcs.", "AI": {"tldr": "本研究提出了一种名为KCS的新框架，通过抽样不同的知识组合来提高生成多跳问题的多样性，克服了以往方法忽视整合重要知识的缺点。KCS显著提高了知识组合的选择准确性，并在多个数据集上显示了优于基线方法的表现。", "motivation": "本研究旨在通过解决生成多样化的问题以应对多跳问答中数据稀疏的挑战。先前的研究关注通过内容规划和多样化的表达方式生成多样化的问题，但往往强调生成简单的问题，忽略了整合重要的知识，比如文档中的相关句子。", "method": "本研究介绍了一种名为知识组合抽样（Knowledge Composition Sampling, KCS）的新框架，通过在一个给定上下文中抽样各种知识组合来扩大生成的多跳问题的多样性。该框架将知识组合的选择建模为一个句子级别的条件预测任务，并利用概率对比损失来预测下一个最相关的知识。在推理过程中，采用了随机解码策略来有效平衡准确性和多样性。", "result": "与竞争的基线方法相比，本研究的KCS将知识组合选择的总体准确性提高了3.9%，并且将其应用于数据增强在HotpotQA和2WikiMultihopQA数据集上有所改进。", "conclusion": "KCS框架通过抽样不同的知识组合来提升多跳问答数据的多样性，有助于解决数据稀疏问题，并且通过概率对比损失和随机解码策略来平衡生成问题的准确性和多样性表现出了优于基线方法的结果。"}}
{"id": "2508.20449", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.20449", "abs": "https://arxiv.org/abs/2508.20449", "authors": ["Libo Lv", "Tianyi Wang", "Mengxiao Huang", "Ruixia Liu", "Yinglong Wang"], "title": "A Spatial-Frequency Aware Multi-Scale Fusion Network for Real-Time Deepfake Detection", "comment": "Accepted to PRCV 2025", "summary": "With the rapid advancement of real-time deepfake generation techniques,\nforged content is becoming increasingly realistic and widespread across\napplications like video conferencing and social media. Although\nstate-of-the-art detectors achieve high accuracy on standard benchmarks, their\nheavy computational cost hinders real-time deployment in practical\napplications. To address this, we propose the Spatial-Frequency Aware\nMulti-Scale Fusion Network (SFMFNet), a lightweight yet effective architecture\nfor real-time deepfake detection. We design a spatial-frequency hybrid aware\nmodule that jointly leverages spatial textures and frequency artifacts through\na gated mechanism, enhancing sensitivity to subtle manipulations. A\ntoken-selective cross attention mechanism enables efficient multi-level feature\ninteraction, while a residual-enhanced blur pooling structure helps retain key\nsemantic cues during downsampling. Experiments on several benchmark datasets\nshow that SFMFNet achieves a favorable balance between accuracy and efficiency,\nwith strong generalization and practical value for real-time applications.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2508.20583", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.20583", "abs": "https://arxiv.org/abs/2508.20583", "authors": ["Soham Petkar", "Hari Aakash K", "Anirudh Vempati", "Akshit Sinha", "Ponnurangam Kumarauguru", "Chirag Agarwal"], "title": "A Graph Talks, But Who's Listening? Rethinking Evaluations for Graph-Language Models", "comment": null, "summary": "Developments in Graph-Language Models (GLMs) aim to integrate the structural\nreasoning capabilities of Graph Neural Networks (GNNs) with the semantic\nunderstanding of Large Language Models (LLMs). However, we demonstrate that\ncurrent evaluation benchmarks for GLMs, which are primarily repurposed\nnode-level classification datasets, are insufficient to assess multimodal\nreasoning. Our analysis reveals that strong performance on these benchmarks is\nachievable using unimodal information alone, suggesting that they do not\nnecessitate graph-language integration. To address this evaluation gap, we\nintroduce the CLEGR(Compositional Language-Graph Reasoning) benchmark, designed\nto evaluate multimodal reasoning at various complexity levels. Our benchmark\nemploys a synthetic graph generation pipeline paired with questions that\nrequire joint reasoning over structure and textual semantics. We perform a\nthorough evaluation of representative GLM architectures and find that\nsoft-prompted LLM baselines perform on par with GLMs that incorporate a full\nGNN backbone. This result calls into question the architectural necessity of\nincorporating graph structure into LLMs. We further show that GLMs exhibit\nsignificant performance degradation in tasks that require structural reasoning.\nThese findings highlight limitations in the graph reasoning capabilities of\ncurrent GLMs and provide a foundation for advancing the community toward\nexplicit multimodal reasoning involving graph structure and language.", "AI": {"tldr": "提出CLEGR基准测试来改善GLM的评估，发现仅语言模型在多模态任务中表现出色，GLM需要改进其在结构推理任务上的性能。", "motivation": "当前的GLM评估基准无法充分评估多模态推理能力，因此提出了一个新的评估基准CLEGR。", "method": "通过引入CLEGR（组合语言-图推理）基准测试来评估多模态推理，并使用合成图生成管道和需要联合推理结构和文本语义的问题进行测试。", "result": "发现仅使用提示的LLM基线与包含完整GNN主干的GLM的性能相当，而且在需要结构推理的任务中，GLM的表现显著下降。", "conclusion": "当前GLMs在图推理方面存在局限性，该研究为推进融合图结构与语言的多模态推理开辟了道路。"}}
{"id": "2508.20461", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.20461", "abs": "https://arxiv.org/abs/2508.20461", "authors": ["Ayaka Tsutsumi", "Guang Li", "Ren Togo", "Takahiro Ogawa", "Satoshi Kondo", "Miki Haseyama"], "title": "Dual-Model Weight Selection and Self-Knowledge Distillation for Medical Image Classification", "comment": null, "summary": "We propose a novel medical image classification method that integrates\ndual-model weight selection with self-knowledge distillation (SKD). In\nreal-world medical settings, deploying large-scale models is often limited by\ncomputational resource constraints, which pose significant challenges for their\npractical implementation. Thus, developing lightweight models that achieve\ncomparable performance to large-scale models while maintaining computational\nefficiency is crucial. To address this, we employ a dual-model weight selection\nstrategy that initializes two lightweight models with weights derived from a\nlarge pretrained model, enabling effective knowledge transfer. Next, SKD is\napplied to these selected models, allowing the use of a broad range of initial\nweight configurations without imposing additional excessive computational cost,\nfollowed by fine-tuning for the target classification tasks. By combining\ndual-model weight selection with self-knowledge distillation, our method\novercomes the limitations of conventional approaches, which often fail to\nretain critical information in compact models. Extensive experiments on\npublicly available datasets-chest X-ray images, lung computed tomography scans,\nand brain magnetic resonance imaging scans-demonstrate the superior performance\nand robustness of our approach compared to existing methods.", "AI": {"tldr": "该研究提出了一种新的医学图像分类方法，结合了双模型权重选择和自我知识蒸馏方法，可以有效解决医疗环境中的计算资源限制问题，取得了优越的性能。", "motivation": "在实际医疗环境中，大型模型的部署受到计算资源的限制，因此开发轻量级模型以便达到大型模型的性能同时保持计算效率是必要的。", "method": "使用双模型权重选择策略，初始化两个轻量级模型权重来自预训练的大模型，然后应用自我知识蒸馏，最后微调模型以适应分类任务。", "result": "在公开数据集包括胸部X光图像，肺部CT扫描和脑部MRI扫描上进行广泛的实验，证明了本方法的优越性能和稳健性优于现有方法。", "conclusion": "结合双模型权重选择与自我知识蒸馏，本方法克服了传统方法的局限，即在紧凑模型中难以保留关键信息的问题。"}}
{"id": "2508.20700", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.20700", "abs": "https://arxiv.org/abs/2508.20700", "authors": ["Yuanchang Luo", "Daimeng Wei", "Shaojun Li", "Hengchao Shang", "Jiaxin Guo", "Zongyao Li", "Zhanglin Wu", "Xiaoyu Chen", "Zhiqiang Rao", "Jinlong Yang", "Hao Yang"], "title": "Generative Annotation for ASR Named Entity Correction", "comment": "12 pages, 7 figures, 7 tables, EMNLP 2025", "summary": "End-to-end automatic speech recognition systems often fail to transcribe\ndomain-specific named entities, causing catastrophic failures in downstream\ntasks. Numerous fast and lightweight named entity correction (NEC) models have\nbeen proposed in recent years. These models, mainly leveraging phonetic-level\nedit distance algorithms, have shown impressive performances. However, when the\nforms of the wrongly-transcribed words(s) and the ground-truth entity are\nsignificantly different, these methods often fail to locate the wrongly\ntranscribed words in hypothesis, thus limiting their usage. We propose a novel\nNEC method that utilizes speech sound features to retrieve candidate entities.\nWith speech sound features and candidate entities, we inovatively design a\ngenerative method to annotate entity errors in ASR transcripts and replace the\ntext with correct entities. This method is effective in scenarios of word form\ndifference. We test our method using open-source and self-constructed test\nsets. The results demonstrate that our NEC method can bring significant\nimprovement to entity accuracy. We will open source our self-constructed test\nset and training data.", "AI": {"tldr": "提出利用语音听感特征检索候选实体和生成方法标注ASR文本中的实体错误的新方法，实验结果表明该方法在单词形式不同的场景下有效。", "motivation": "由于现有的自动语音识别系统在转录特定领域的专有名词时经常失败，导致下游任务出现问题。许多快速轻量级的专有名词校正模型虽然在很大程度上依赖音素级别编辑距离算法展现了令人印象深刻的性能，但对于转录错误单词与真值实体形式显著不同的情况，这些方法常常无法定位错误转录的单词，从而限制了它们的应用。因此，提出了新型的NEC方法。", "method": "提出一种新的专有名词校正方法，利用语音听感特征检索候选实体，并设计了一种生成方法来标注ASR转录文本中的实体错误并替换为正确实体。", "result": "我们的NEC方法显著提升了实体准确性，通过开源测试集和训练数据进一步促进研究。", "conclusion": "该方法在不同单词形式的情况下是有效的，实验结果证明了其在实体识别准确性方面的显著改进。同时，计划将自己构造的测试集和训练数据开源。"}}
{"id": "2508.20466", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.20466", "abs": "https://arxiv.org/abs/2508.20466", "authors": ["Pengpeng Yu", "Haoran Li", "Dingquan Li", "Runqing Jiang", "Jing Wang", "Liang Lin", "Yulan Guo"], "title": "Re-Densification Meets Cross-Scale Propagation: Real-Time Compression of LiDAR Point Clouds", "comment": null, "summary": "LiDAR point clouds are fundamental to various applications, yet\nhigh-precision scans incur substantial storage and transmission overhead.\nExisting methods typically convert unordered points into hierarchical octree or\nvoxel structures for dense-to-sparse predictive coding. However, the extreme\nsparsity of geometric details hinders efficient context modeling, thereby\nlimiting their compression performance and speed. To address this challenge, we\npropose to generate compact features for efficient predictive coding. Our\nframework comprises two lightweight modules. First, the Geometry\nRe-Densification Module re-densifies encoded sparse geometry, extracts features\nat denser scale, and then re-sparsifies the features for predictive coding.\nThis module avoids costly computation on highly sparse details while\nmaintaining a lightweight prediction head. Second, the Cross-scale Feature\nPropagation Module leverages occupancy cues from multiple resolution levels to\nguide hierarchical feature propagation. This design facilitates information\nsharing across scales, thereby reducing redundant feature extraction and\nproviding enriched features for the Geometry Re-Densification Module. By\nintegrating these two modules, our method yields a compact feature\nrepresentation that provides efficient context modeling and accelerates the\ncoding process. Experiments on the KITTI dataset demonstrate state-of-the-art\ncompression ratios and real-time performance, achieving 26 FPS for both\nencoding and decoding at 12-bit quantization. Code is available at\nhttps://github.com/pengpeng-yu/FastPCC.", "AI": {"tldr": "提出了一种用于LiDAR点云压缩的新框架，该框架包含两个轻量级模块，能够高效地进行特征提取和跨尺度信息共享，实现了低计算成本和高性能，适用于各种LiDAR点云应用。", "motivation": "现有方法将无序点转化为分层八叉树或体素结构以实现密集到稀疏的预测编码，然而几何细节的极端稀疏性阻碍了有效上下文建模，从而限制了其压缩性能与速度。我们提出通过生成紧凑的特征来进行高效的预测编码。", "method": "我们的方法包括两个轻量级模块：首先，几何重密化模块重密化编码的稀疏几何结构，在更密集的尺度上提取特征，然后稀疏化这些特征以进行预测编码。此模块避免了在高度稀疏细节上的昂贵计算，同时保持了轻量级的预测头。其次，跨尺度特征传播模块利用多个分辨率级别上的占用提示来指导分层特征传播。这种设计有利于跨尺度的信息共享，从而减少冗余特征提取，并为几何重密化模块提供丰富的特征表示。通过整合这两个模块，我们的方法生成了一个紧凑的特征表示，该表示能够提高上下文建模效率并加速编码过程。", "result": "实验表明，我们的方法在KITTI数据集上实现了最领先的压缩比和实时性能，编码和解码速度均为26 FPS。代码可以在https://github.com/pengpeng-yu/FastPCC获得。", "conclusion": "实验结果表明，该方法在KITTI数据集上实现了最先进的压缩比和实时性能，实现了12位量化下编码和解码均为26 FPS的性能。"}}
{"id": "2508.20712", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.20712", "abs": "https://arxiv.org/abs/2508.20712", "authors": ["Nelson Filipe Costa", "Leila Kosseim"], "title": "Multi-Lingual Implicit Discourse Relation Recognition with Multi-Label Hierarchical Learning", "comment": "Published at SIGDIAL 2025. Best paper award", "summary": "This paper introduces the first multi-lingual and multi-label classification\nmodel for implicit discourse relation recognition (IDRR). Our model, HArch, is\nevaluated on the recently released DiscoGeM 2.0 corpus and leverages\nhierarchical dependencies between discourse senses to predict probability\ndistributions across all three sense levels in the PDTB 3.0 framework. We\ncompare several pre-trained encoder backbones and find that RoBERTa-HArch\nachieves the best performance in English, while XLM-RoBERTa-HArch performs best\nin the multi-lingual setting. In addition, we compare our fine-tuned models\nagainst GPT-4o and Llama-4-Maverick using few-shot prompting across all\nlanguage configurations. Our results show that our fine-tuned models\nconsistently outperform these LLMs, highlighting the advantages of\ntask-specific fine-tuning over prompting in IDRR. Finally, we report SOTA\nresults on the DiscoGeM 1.0 corpus, further validating the effectiveness of our\nhierarchical approach.", "AI": {"tldr": "本文开发了一种名为HArch的多语言模型用于隐式话语关系识别，其表现优于其他几种模型，并在多个语料库上取得了SOTA结果。", "motivation": "旨在开发一种有效的多语言模型用于识别隐式话语关系，通过引入分层依赖方式来提高预测的准确性。", "method": "提出了一种名为HArch的多语言、多标签分类模型，用于隐式话语关系识别。该模型在新发布的DiscogeM 2.0语料库上进行评估，并利用话语意义之间的分层依赖关系来预测PDTB 3.0框架中三个意义水平的概率分布。", "result": "实验显示，在英语情况下，RoBERTa-HArch表现最优；在多语言场景中，XLM-RoBERTa-HArch表现最佳。此外，他们的微调模型在所有语言配置的少样本提示下一致优于GPT-4o和Llama-4-Maverick模型。", "conclusion": "研究结果证明了该分层方法的有效性，在DiscogeM 1.0语料库上达到了SOTA的结果，再次证明了针对任务的微调相较于提示在IDRR任务中的优势。"}}
{"id": "2508.20470", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.20470", "abs": "https://arxiv.org/abs/2508.20470", "authors": ["Xiaochuan Li", "Guoguang Du", "Runze Zhang", "Liang Jin", "Qi Jia", "Lihua Lu", "Zhenhua Guo", "Yaqian Zhao", "Haiyang Liu", "Tianqi Wang", "Changsheng Li", "Xiaoli Gong", "Rengang Li", "Baoyu Fan"], "title": "Droplet3D: Commonsense Priors from Videos Facilitate 3D Generation", "comment": null, "summary": "Scaling laws have validated the success and promise of large-data-trained\nmodels in creative generation across text, image, and video domains. However,\nthis paradigm faces data scarcity in the 3D domain, as there is far less of it\navailable on the internet compared to the aforementioned modalities.\nFortunately, there exist adequate videos that inherently contain commonsense\npriors, offering an alternative supervisory signal to mitigate the\ngeneralization bottleneck caused by limited native 3D data. On the one hand,\nvideos capturing multiple views of an object or scene provide a spatial\nconsistency prior for 3D generation. On the other hand, the rich semantic\ninformation contained within the videos enables the generated content to be\nmore faithful to the text prompts and semantically plausible. This paper\nexplores how to apply the video modality in 3D asset generation, spanning\ndatasets to models. We introduce Droplet3D-4M, the first large-scale video\ndataset with multi-view level annotations, and train Droplet3D, a generative\nmodel supporting both image and dense text input. Extensive experiments\nvalidate the effectiveness of our approach, demonstrating its ability to\nproduce spatially consistent and semantically plausible content. Moreover, in\ncontrast to the prevailing 3D solutions, our approach exhibits the potential\nfor extension to scene-level applications. This indicates that the commonsense\npriors from the videos significantly facilitate 3D creation. We have\nopen-sourced all resources including the dataset, code, technical framework,\nand model weights: https://dropletx.github.io/.", "AI": {"tldr": "该论文提出利用带有多视角标注的大规模视频数据集Droplet3D-4M和训练的Droplet3D生成模型，以解决3D领域数据稀缺的问题，方法通过视频中的空间一致性和丰富的语义信息，生成了空间一致且语义合理的内容，该方法有潜力扩展到场景级别的应用，且所有资源已开源。", "motivation": "由于3D数据在网络上的稀缺性限制了该领域的发展，文章希望通过视频中的多视角信息和丰富语义，为3D生成提供一种新的监督信号，以克服数据不足导致的泛化瓶颈。", "method": "文章创建了第一个大规模多视角级别的视频数据集Droplet3D-4M，并训练了生成模型Droplet3D，该模型支持图像和密集文本输入。", "result": "大量的实验结果验证了这种方法的有效性，表明该方法能够生成空间一致且语义合理的内容。", "conclusion": "与现今主流的3D解决方案相比，该方法显示出了向场景级别应用扩展的潜力，从而证明视频中的常识性先验知识对于3D内容的生成非常有帮助。"}}
{"id": "2508.20718", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.20718", "abs": "https://arxiv.org/abs/2508.20718", "authors": ["Ruiyi Yan", "Yugo Murawaki"], "title": "Addressing Tokenization Inconsistency in Steganography and Watermarking Based on Large Language Models", "comment": null, "summary": "Large language models have significantly enhanced the capacities and\nefficiency of text generation. On the one hand, they have improved the quality\nof text-based steganography. On the other hand, they have also underscored the\nimportance of watermarking as a safeguard against malicious misuse. In this\nstudy, we focus on tokenization inconsistency (TI) between Alice and Bob in\nsteganography and watermarking, where TI can undermine robustness. Our\ninvestigation reveals that the problematic tokens responsible for TI exhibit\ntwo key characteristics: infrequency and temporariness. Based on these\nfindings, we propose two tailored solutions for TI elimination: a stepwise\nverification method for steganography and a post-hoc rollback method for\nwatermarking. Experiments show that (1) compared to traditional disambiguation\nmethods in steganography, directly addressing TI leads to improvements in\nfluency, imperceptibility, and anti-steganalysis capacity; (2) for\nwatermarking, addressing TI enhances detectability and robustness against\nattacks.", "AI": {"tldr": "研究探讨大规模语言模型文本生成中的分词不一致性问题，提出针对性解决方案，并证明其能改善隐写和水印技术的质量与鲁棒性。", "motivation": "大规模语言模型显著提升了文本生成的能力和效率，但是它们也带来了分词不一致性问题，这可能降低隐写和水印技术的鲁棒性。", "method": "该研究关注在文本隐写和水印技术中的分词不一致性（TI），提出了两种针对性的解决方案：一种是用于隐写的逐步验证方法，另一种是用于水印的后处理回滚方法。", "result": "实验结果显示，直接处理TI相较于传统的隐写歧义解决方法，能提高流畅性、不可感知性和抗隐写分析能力；对于水印技术，处理TI能提高检测性和抗攻击性。", "conclusion": "本研究对于理解和解决大规模语言模型应用中的分词不一致性问题具有重要意义。"}}
{"id": "2508.20471", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.20471", "abs": "https://arxiv.org/abs/2508.20471", "authors": ["Jiusi Li", "Jackson Jiang", "Jinyu Miao", "Miao Long", "Tuopu Wen", "Peijin Jia", "Shengxiang Liu", "Chunlei Yu", "Maolin Liu", "Yuzhan Cai", "Kun Jiang", "Mengmeng Yang", "Diange Yang"], "title": "Realistic and Controllable 3D Gaussian-Guided Object Editing for Driving Video Generation", "comment": null, "summary": "Corner cases are crucial for training and validating autonomous driving\nsystems, yet collecting them from the real world is often costly and hazardous.\nEditing objects within captured sensor data offers an effective alternative for\ngenerating diverse scenarios, commonly achieved through 3D Gaussian Splatting\nor image generative models. However, these approaches often suffer from limited\nvisual fidelity or imprecise pose control. To address these issues, we propose\nG^2Editor, a framework designed for photorealistic and precise object editing\nin driving videos. Our method leverages a 3D Gaussian representation of the\nedited object as a dense prior, injected into the denoising process to ensure\naccurate pose control and spatial consistency. A scene-level 3D bounding box\nlayout is employed to reconstruct occluded areas of non-target objects.\nFurthermore, to guide the appearance details of the edited object, we\nincorporate hierarchical fine-grained features as additional conditions during\ngeneration. Experiments on the Waymo Open Dataset demonstrate that G^2Editor\neffectively supports object repositioning, insertion, and deletion within a\nunified framework, outperforming existing methods in both pose controllability\nand visual quality, while also benefiting downstream data-driven tasks.", "AI": {"tldr": "本文提出了G^2Editor框架，通过创新的方法对驾驶视频中的对象进行编辑，实现了更好的视觉效果和精确的姿态控制。", "motivation": "自动驾驶系统的训练和验证过程中，角案例数据非常重要，但直接从现实世界获取这类数据成本高昂且危险。因此，编辑采集到的传感器数据来生成多样化的场景成为有效的替代方法，但这种方法通常面临视觉保真度低或姿态控制不精确的问题。本研究旨在通过提出的新框架解决这些问题。", "method": "该论文提出了一种名为G^2Editor的框架，用于对驾驶视频中对象的编辑，实现在保真性和精确度上的提升。具体通过3D高斯表示编辑对象作为稠密的先验知识，输入到去噪过程中，以确保准确的姿态控制和空间一致性。同时采用场景级的3D边界框布局重建被遮挡的非目标对象区域，并引入分层细粒度特征来指导编辑对象的外观细节。", "result": "实验结果表明，G^2Editor框架能够有效支持对象重新定位、插入和删除，且在Waymo开放数据集上的表现优于现有方法，在姿态控制能力和视觉质量方面都有显著提高，同时也为数据驱动任务提供了帮助。", "conclusion": "G^2Editor的应用不仅提高了驾驶员视频对象编辑的准确性和视觉保真度，还为基于数据的任务提供了强大的支持。"}}
{"id": "2508.20722", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.20722", "abs": "https://arxiv.org/abs/2508.20722", "authors": ["Ning Shang", "Yifei Liu", "Yi Zhu", "Li Lyna Zhang", "Weijiang Xu", "Xinyu Guan", "Buze Zhang", "Bingcheng Dong", "Xudong Zhou", "Bowen Zhang", "Ying Xin", "Ziming Miao", "Scarlett Li", "Fan Yang", "Mao Yang"], "title": "rStar2-Agent: Agentic Reasoning Technical Report", "comment": null, "summary": "We introduce rStar2-Agent, a 14B math reasoning model trained with agentic\nreinforcement learning to achieve frontier-level performance. Beyond current\nlong CoT, the model demonstrates advanced cognitive behaviors, such as thinking\ncarefully before using Python coding tools and reflecting on code execution\nfeedback to autonomously explore, verify, and refine intermediate steps in\ncomplex problem-solving. This capability is enabled through three key\ninnovations that makes agentic RL effective at scale: (i) an efficient RL\ninfrastructure with a reliable Python code environment that supports\nhigh-throughput execution and mitigates the high rollout costs, enabling\ntraining on limited GPU resources (64 MI300X GPUs); (ii) GRPO-RoC, an agentic\nRL algorithm with a Resample-on-Correct rollout strategy that addresses the\ninherent environment noises from coding tools, allowing the model to reason\nmore effectively in a code environment; (iii) An efficient agent training\nrecipe that starts with non-reasoning SFT and progresses through multi-RL\nstages, yielding advanced cognitive abilities with minimal compute cost. To\nthis end, rStar2-Agent boosts a pre-trained 14B model to state of the art in\nonly 510 RL steps within one week, achieving average pass@1 scores of 80.6% on\nAIME24 and 69.8% on AIME25, surpassing DeepSeek-R1 (671B) with significantly\nshorter responses. Beyond mathematics, rStar2-Agent-14B also demonstrates\nstrong generalization to alignment, scientific reasoning, and agentic tool-use\ntasks. Code and training recipes are available at\nhttps://github.com/microsoft/rStar.", "AI": {"tldr": "rStar2-Agent是一个通过代理强化学习训练的140亿参数数学推理模型，它在复杂问题解决上展示了先进的认知行为，并在AIME24和AIME25测试中表现出色，超过了一些更大的模型性能。", "motivation": "该研究目的是展示通过代理强化学习训练大型模型进行复杂问题解决的先进能力，特别是数学推理。", "method": "本文介绍了rStar2-Agent，一个通过代理强化学习训练的140亿参数数学推理模型。这个模型展示了高级认知行为，例如在使用Python编码工具前仔细思考和利用代码执行的反馈来自主探索、验证和优化复杂问题解决中的中间步骤。这一能力通过三种关键创新得以实现：(i) 一个高效的RL基础设施，提供可靠的Python代码环境，支持高吞吐量执行并降低rollout成本，使模型能在有限的GPU资源上训练；(ii) GRPO-RoC代理RL算法，具有基于正确的Resample-on-Correct rollout策略，减少了代码环境中的噪音影响，提升了模型的推理能力；(iii) 一种高效的代理训练配方，从非推理SFT开始，逐步进入多RL阶段，以较少的计算成本训练出先进认知能力的模型。", "result": "rStar2-Agent 在预训练的14B模型基础上，在仅需510RL步骤的一个星期内达到最先进水平，AIME24 和 AIME25 的通过率分别达到80.6%和69.8%，比DeepSeek-R1 (671B)更有优势，且响应速度更快。", "conclusion": "rStar2-Agent模型在数学推理及问题解决方面展示了卓越的能力，并且在特定任务上的表现超过了现有的大型模型，具有广泛的应用前景。"}}
{"id": "2508.20475", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.20475", "abs": "https://arxiv.org/abs/2508.20475", "authors": ["Marina Grifell i Plana", "Vladyslav Zalevskyi", "Léa Schmidt", "Yvan Gomez", "Thomas Sanchez", "Vincent Dunet", "Mériam Koob", "Vanessa Siffredi", "Meritxell Bach Cuadra"], "title": "Enhancing Corpus Callosum Segmentation in Fetal MRI via Pathology-Informed Domain Randomization", "comment": "Accepted at the PIPPI Workshop of MICCAI 2025", "summary": "Accurate fetal brain segmentation is crucial for extracting biomarkers and\nassessing neurodevelopment, especially in conditions such as corpus callosum\ndysgenesis (CCD), which can induce drastic anatomical changes. However, the\nrarity of CCD severely limits annotated data, hindering the generalization of\ndeep learning models. To address this, we propose a pathology-informed domain\nrandomization strategy that embeds prior knowledge of CCD manifestations into a\nsynthetic data generation pipeline. By simulating diverse brain alterations\nfrom healthy data alone, our approach enables robust segmentation without\nrequiring pathological annotations.\n  We validate our method on a cohort comprising 248 healthy fetuses, 26 with\nCCD, and 47 with other brain pathologies, achieving substantial improvements on\nCCD cases while maintaining performance on both healthy fetuses and those with\nother pathologies. From the predicted segmentations, we derive clinically\nrelevant biomarkers, such as corpus callosum length (LCC) and volume, and show\ntheir utility in distinguishing CCD subtypes. Our pathology-informed\naugmentation reduces the LCC estimation error from 1.89 mm to 0.80 mm in\nhealthy cases and from 10.9 mm to 0.7 mm in CCD cases. Beyond these\nquantitative gains, our approach yields segmentations with improved topological\nconsistency relative to available ground truth, enabling more reliable\nshape-based analyses. Overall, this work demonstrates that incorporating\ndomain-specific anatomical priors into synthetic data pipelines can effectively\nmitigate data scarcity and enhance analysis of rare but clinically significant\nmalformations.", "AI": {"tldr": "The paper introduces a domain randomization strategy informed by the pathology of corpus callosum dysgenesis to improve fetal brain segmentation in rare disease cases.", "motivation": "To enhance deep learning model's generalization for fetal brain segmentation in cases of rare diseases like corpus callosum dysgenesis (CCD), where annotated data is scarce.", "method": "The authors propose a pathology-informed domain randomization strategy that simulates diverse brain changes related to CCD from healthy data alone, allowing the generation of synthetic data without needing pathological annotations.", "result": "The proposed method improved segmentation performance on CCD cases while maintaining accuracy on healthy cases and those with other pathologies. It also reduced estimation errors for clinically relevant biomarkers like corpus callosum length.", "conclusion": "Incorporating domain-specific anatomical priors into synthetic data pipelines can mitigate data scarcity issues and enhance analysis in cases of rare but clinically significant malformations."}}
{"id": "2508.20736", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.20736", "abs": "https://arxiv.org/abs/2508.20736", "authors": ["Stephen Meisenbacher", "Maulik Chevli", "Florian Matthes"], "title": "Leveraging Semantic Triples for Private Document Generation with Local Differential Privacy Guarantees", "comment": "17 pages, 2 figures, 11 tables. Accepted to EMNLP 2025 (Main)", "summary": "Many works at the intersection of Differential Privacy (DP) in Natural\nLanguage Processing aim to protect privacy by transforming texts under DP\nguarantees. This can be performed in a variety of ways, from word perturbations\nto full document rewriting, and most often under local DP. Here, an input text\nmust be made indistinguishable from any other potential text, within some bound\ngoverned by the privacy parameter $\\varepsilon$. Such a guarantee is quite\ndemanding, and recent works show that privatizing texts under local DP can only\nbe done reasonably under very high $\\varepsilon$ values. Addressing this\nchallenge, we introduce DP-ST, which leverages semantic triples for\nneighborhood-aware private document generation under local DP guarantees.\nThrough the evaluation of our method, we demonstrate the effectiveness of the\ndivide-and-conquer paradigm, particularly when limiting the DP notion (and\nprivacy guarantees) to that of a privatization neighborhood. When combined with\nLLM post-processing, our method allows for coherent text generation even at\nlower $\\varepsilon$ values, while still balancing privacy and utility. These\nfindings highlight the importance of coherence in achieving balanced\nprivatization outputs at reasonable $\\varepsilon$ levels.", "AI": {"tldr": "This paper presents DP-ST, a method that enhances text privatization under local differential privacy by using semantic triples and LLM post-processing, achieving better balance between privacy and utility at lower ε values.", "motivation": "To address the challenge of achieving reasonable text privatization under local differential privacy, especially at lower ε values, where previous methods struggle.", "method": "DP-ST, which uses semantic triples for neighborhood-aware private document generation under local differential privacy guarantees. It applies the divide-and-conquer paradigm and leverages LLM post-processing to maintain text coherence at lower privacy parameters (ε).", "result": "The method demonstrates effective text generation under local DP that maintains coherence and balance between privacy and utility at lower ε values compared to previous approaches.", "conclusion": "The study underscores the significance of semantic coherence in balancing privacy and utility in text privatization, making it viable even under more stringent privacy conditions."}}
{"id": "2508.20476", "categories": ["cs.CV", "cs.MM", "eess.AS", "eess.IV"], "pdf": "https://arxiv.org/pdf/2508.20476", "abs": "https://arxiv.org/abs/2508.20476", "authors": ["Jeong Hun Yeo", "Hyeongseop Rha", "Sungjune Park", "Junil Won", "Yong Man Ro"], "title": "Towards Inclusive Communication: A Unified LLM-Based Framework for Sign Language, Lip Movements, and Audio Understanding", "comment": "Code available at: https://github.com/JeongHun0716/UniSLA", "summary": "Audio is the primary modality for human communication and has driven the\nsuccess of Automatic Speech Recognition (ASR) technologies. However, such\nsystems remain inherently inaccessible to individuals who are deaf or hard of\nhearing. Visual alternatives such as sign language and lip reading offer\neffective substitutes, and recent advances in Sign Language Translation (SLT)\nand Visual Speech Recognition (VSR) have improved audio-less communication.\nYet, these modalities have largely been studied in isolation, and their\nintegration within a unified framework remains underexplored. In this paper, we\nintroduce the first unified framework capable of handling diverse combinations\nof sign language, lip movements, and audio for spoken-language text generation.\nWe focus on three main objectives: (i) designing a unified, modality-agnostic\narchitecture capable of effectively processing heterogeneous inputs; (ii)\nexploring the underexamined synergy among modalities, particularly the role of\nlip movements as non-manual cues in sign language comprehension; and (iii)\nachieving performance on par with or superior to state-of-the-art models\nspecialized for individual tasks. Building on this framework, we achieve\nperformance on par with or better than task-specific state-of-the-art models\nacross SLT, VSR, ASR, and AVSR. Furthermore, our analysis reveals that\nexplicitly modeling lip movements as a separate modality significantly improves\nSLT performance.", "AI": {"tldr": "本研究提出了一种能够处理手语、唇部动作和语音的统一框架，用于生成口语文本。研究显示，分离式处理唇部动作可以显著提高手语翻译的表现，总体性能优于或与当前最先进的单项任务模型持平。", "motivation": "现有的语音识别技术主要依赖于音频，这对于聋人或听力不佳的人群来说是不友好的。尽管手语和唇读提供了有效的替代方案，但这些模态通常是孤立研究的，它们的整合尚未得到充分探索。因此，本研究旨在开发一个统一的框架，改善无音频通信，并探索不同模态之间的协同效应。", "method": "本研究提出了一种统一的框架，能够处理多种模态的输入，包括手语、唇部动作和语音，并生成口语文本。该架构设计为模态无关，能够有效处理异构输入。研究同时探索了不同模态之间的协同效应，特别是唇部动作作为手语理解中的非手动线索的作用。", "result": "研究结果显示，该框架在手语翻译、视觉语音识别、自动语音识别和联合语音识别方面，表现出与或优于专门针对单个任务的先进模型的性能。特别地，明确了作为独立模态的唇部动作在提高手语翻译表现上的重要性。", "conclusion": "该统一框架成功地整合了多种通信方式，并展现出优秀的跨模态处理能力和比专业化模型更好的性能，特别是在手语翻译任务中。"}}
{"id": "2508.20750", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.20750", "abs": "https://arxiv.org/abs/2508.20750", "authors": ["Vassiliy Cheremetiev", "Quang Long Ho Ngo", "Chau Ying Kot", "Alina Elena Baia", "Andrea Cavallaro"], "title": "Specializing General-purpose LLM Embeddings for Implicit Hate Speech Detection across Datasets", "comment": "Paper accepted at the DHOW Workshop at ACM Multimedia 2025. Code\n  available at https://github.com/idiap/implicit-hsd", "summary": "Implicit hate speech (IHS) is indirect language that conveys prejudice or\nhatred through subtle cues, sarcasm or coded terminology. IHS is challenging to\ndetect as it does not include explicit derogatory or inflammatory words. To\naddress this challenge, task-specific pipelines can be complemented with\nexternal knowledge or additional information such as context, emotions and\nsentiment data. In this paper, we show that, by solely fine-tuning recent\ngeneral-purpose embedding models based on large language models (LLMs), such as\nStella, Jasper, NV-Embed and E5, we achieve state-of-the-art performance.\nExperiments on multiple IHS datasets show up to 1.10 percentage points\nimprovements for in-dataset, and up to 20.35 percentage points improvements in\ncross-dataset evaluation, in terms of F1-macro score.", "AI": {"tldr": "研究展示微调大型语言模型在隐性仇恨言论检测中的突出表现，显著提高了检测精度。", "motivation": "隐性仇恨言论因其间接性和隐秘性难以检测，故尝试通过优化模型来改善这一问题。", "method": "通过微调基于大型语言模型（LLMs）的通用嵌入模型（如Stella、Jasper、NV-Embed和E5），来提升隐性仇恨言论（IHS）检测的性能。", "result": "在多个隐性仇恨言论数据集上的实验表明，F1-macro评分有显著提高，最高认为20.35个百分点。", "conclusion": "单靠微调现有通用嵌入模型即可达到最先进的检测效果。"}}
{"id": "2508.20478", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.20478", "abs": "https://arxiv.org/abs/2508.20478", "authors": ["Yuan Xie", "Tianshui Chen", "Zheng Ge", "Lionel Ni"], "title": "Video-MTR: Reinforced Multi-Turn Reasoning for Long Video Understanding", "comment": "15 pages, 9 figures", "summary": "Long-form video understanding, characterized by long-range temporal\ndependencies and multiple events, remains a challenge. Existing methods often\nrely on static reasoning or external visual-language models (VLMs), which face\nissues like complexity and sub-optimal performance due to the lack of\nend-to-end training. In this paper, we propose Video-MTR, a reinforced\nmulti-turn reasoning framework designed to enable iterative key video segment\nselection and question comprehension. Unlike traditional video reasoning\npipeline, which generate predictions in a single turn, Video-MTR performs\nreasoning in multiple turns, selecting video segments progressively based on\nthe evolving understanding of previously processed segments and the current\nquestion. This iterative process allows for a more refined and contextually\naware analysis of the video. To ensure intermediate reasoning process, we\nintroduce a novel gated bi-level reward system, combining trajectory-level\nrewards based on answer correctness and turn-level rewards emphasizing\nframe-query relevance. This system optimizes both video segment selection and\nquestion comprehension, eliminating the need for external VLMs and allowing\nend-to-end training. Extensive experiments on benchmarks like VideoMME, MLVU,\nand EgoSchema demonstrate that Video-MTR outperforms existing methods in both\naccuracy and efficiency, advancing the state-of-the-art in long video\nunderstanding.", "AI": {"tldr": "This paper introduces Video-MTR, an innovative method for long-form video understanding utilizing multi-turn reasoning and a reinforced reward system to improve performance over existing techniques.", "motivation": "The motivation behind Video-MTR is to overcome the challenges posed by long-range temporal dependencies and multiple events in videos, shortcomings of external VLMs, and the need for better end-to-end training methods for improved performance.", "method": "Unlike traditional methods, Video-MTR introduces an iterative key video segment selection and question comprehension process, using multi-turn reasoning framework. This framework is supported by a novel gated bi-level reward system which evaluates based on both trajectory-level answer correctness and turn-level frame-query relevance.", "result": "Video-MTR has shown superior performance in accuracy and efficiency compared to existing methods, as shown through extensive experiments on benchmarks like VideoMME, MLVU, and EgoSchema.", "conclusion": "This research advances the state-of-the-art in the field of long-form video understanding by presenting Video-MTR, a method that not only improves accuracy and efficiency but also eliminates the need for external VLMs."}}
{"id": "2508.20757", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.20757", "abs": "https://arxiv.org/abs/2508.20757", "authors": ["Yuanhao Ding", "Esteban Garces Arias", "Meimingwei Li", "Julian Rodemann", "Matthias Aßenmacher", "Danlu Chen", "Gaojuan Fan", "Christian Heumann", "Chongsheng Zhang"], "title": "GUARD: Glocal Uncertainty-Aware Robust Decoding for Effective and Efficient Open-Ended Text Generation", "comment": "Accepted at Findings of the Association for Computational\n  Linguistics: EMNLP (Findings) 2025", "summary": "Open-ended text generation faces a critical challenge: balancing coherence\nwith diversity in LLM outputs. While contrastive search-based decoding\nstrategies have emerged to address this trade-off, their practical utility is\noften limited by hyperparameter dependence and high computational costs. We\nintroduce GUARD, a self-adaptive decoding method that effectively balances\nthese competing objectives through a novel \"Glocal\" uncertainty-driven\nframework. GUARD combines global entropy estimates with local entropy\ndeviations to integrate both long-term and short-term uncertainty signals. We\ndemonstrate that our proposed global entropy formulation effectively mitigates\nabrupt variations in uncertainty, such as sudden overconfidence or high entropy\nspikes, and provides theoretical guarantees of unbiasedness and consistency. To\nreduce computational overhead, we incorporate a simple yet effective\ntoken-count-based penalty into GUARD. Experimental results demonstrate that\nGUARD achieves a good balance between text diversity and coherence, while\nexhibiting substantial improvements in generation speed. In a more nuanced\ncomparison study across different dimensions of text quality, both human and\nLLM evaluators validated its remarkable performance. Our code is available at\nhttps://github.com/YecanLee/GUARD.", "AI": {"tldr": "本文提出了GUARD方法来解决大语言模型在生成连续文本时面临的连贯性和多样性之间的平衡问题。实验表明，GUARD不仅提高了生成文本的质量，还提升了生成速度。", "motivation": "解决大语言模型生成连续文本时面临的连贯性与多样性之间的平衡问题。现有的对比搜索解码策略虽然试图解决这个问题，但受到超参数依赖和高计算成本的限制。", "method": "GUARD是一种自我适应的解码方法，通过“全局局部”不确定性驱动框架在连贯性和多样性之间取得平衡。GUARD结合全局熵估计和局部熵偏差来整合长期和短期不确定性信号，并引入基于标记计数的简单有效的惩罚以减少计算开销。", "result": "实验结果显示，GUARD在文本多样性和连贯性之间取得了良好的平衡，同时在生成速度上表现出显著的改进。人类和大语言模型评估者在多个文本质量维度的对比研究中验证了其出色性能。", "conclusion": "提出GUARD方法，它通过全局熵公式有效缓解了不确定性中的突变，并提供了无偏差和一致性的理论保证，同时减少了计算开销。实验和评估验证了GUARD在文本生成的多样性和连贯性上的优越性。"}}
{"id": "2508.20488", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.20488", "abs": "https://arxiv.org/abs/2508.20488", "authors": ["Zixuan Hu", "Dongxiao Li", "Xinzhu Ma", "Shixiang Tang", "Xiaotong Li", "Wenhan Yang", "Ling-Yu Duan"], "title": "Adaptive Dual Uncertainty Optimization: Boosting Monocular 3D Object Detection under Test-Time Shifts", "comment": "Accepted by ICCV 2025 (Highlight)", "summary": "Accurate monocular 3D object detection (M3OD) is pivotal for safety-critical\napplications like autonomous driving, yet its reliability deteriorates\nsignificantly under real-world domain shifts caused by environmental or sensor\nvariations. To address these shifts, Test-Time Adaptation (TTA) methods have\nemerged, enabling models to adapt to target distributions during inference.\nWhile prior TTA approaches recognize the positive correlation between low\nuncertainty and high generalization ability, they fail to address the dual\nuncertainty inherent to M3OD: semantic uncertainty (ambiguous class\npredictions) and geometric uncertainty (unstable spatial localization). To\nbridge this gap, we propose Dual Uncertainty Optimization (DUO), the first TTA\nframework designed to jointly minimize both uncertainties for robust M3OD.\nThrough a convex optimization lens, we introduce an innovative convex structure\nof the focal loss and further derive a novel unsupervised version, enabling\nlabel-agnostic uncertainty weighting and balanced learning for high-uncertainty\nobjects. In parallel, we design a semantic-aware normal field constraint that\npreserves geometric coherence in regions with clear semantic cues, reducing\nuncertainty from the unstable 3D representation. This dual-branch mechanism\nforms a complementary loop: enhanced spatial perception improves semantic\nclassification, and robust semantic predictions further refine spatial\nunderstanding. Extensive experiments demonstrate the superiority of DUO over\nexisting methods across various datasets and domain shift types.", "AI": {"tldr": "本文提出了Dual Uncertainty Optimization (DUO)框架，以改善单目3D物体检测在面对环境和传感器变动时的性能，实现同时处理语义和几何不确定性的能力，实验表明此方法优越于现有方法。", "motivation": "传统的TTA方法仅关注不确定性与泛化能力之间的正相关，忽视了单目3D目标检测中的双不确定性问题。DUO应对这一挑战，通过开发新的方法使模型能够更精确地适应目标分布，提高在实际操作中的可靠性。", "method": "本研究提出了Dual Uncertainty Optimization (DUO)，作为首个针对单目3D物体检测的测试时间自适应(TTA)框架，旨在同时最小化语义不确定性和几何不确定性。DUO通过凸优化的视角，引入了focal loss的凸结构，并开发了无监督版本，实现无标签的不确定性加权和高不确定度对象的平衡学习。同时，设计了一种考虑语义意识的法线场约束，减少由于不稳定的3D表示导致的不确定性。", "result": "实验结果表明，在多种数据集和领域转移类型中，DUO优于现有方法，显著提高了单目3D物体检测的可靠性。", "conclusion": "研究提出的DUO框架为单目3D目标检测提供了一种新的解决策略，通过双重优化不确定性，实现了在多种环境和传感器变化下的持续准确检测。"}}
{"id": "2508.20764", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.20764", "abs": "https://arxiv.org/abs/2508.20764", "authors": ["Xiaoyi Wang", "Jiwei Zhang", "Guangtao Zhang", "Honglei Guo"], "title": "Feel the Difference? A Comparative Analysis of Emotional Arcs in Real and LLM-Generated CBT Sessions", "comment": "Accepted at EMNLP 2025,14 page,3 figures", "summary": "Synthetic therapy dialogues generated by large language models (LLMs) are\nincreasingly used in mental health NLP to simulate counseling scenarios, train\nmodels, and supplement limited real-world data. However, it remains unclear\nwhether these synthetic conversations capture the nuanced emotional dynamics of\nreal therapy. In this work, we conduct the first comparative analysis of\nemotional arcs between real and LLM-generated Cognitive Behavioral Therapy\ndialogues. We adapt the Utterance Emotion Dynamics framework to analyze\nfine-grained affective trajectories across valence, arousal, and dominance\ndimensions. Our analysis spans both full dialogues and individual speaker roles\n(counselor and client), using real sessions transcribed from public videos and\nsynthetic dialogues from the CACTUS dataset. We find that while synthetic\ndialogues are fluent and structurally coherent, they diverge from real\nconversations in key emotional properties: real sessions exhibit greater\nemotional variability,more emotion-laden language, and more authentic patterns\nof reactivity and regulation. Moreover, emotional arc similarity between real\nand synthetic speakers is low, especially for clients. These findings\nunderscore the limitations of current LLM-generated therapy data and highlight\nthe importance of emotional fidelity in mental health applications. We\nintroduce RealCBT, a curated dataset of real CBT sessions, to support future\nresearch in this space.", "AI": {"tldr": "首次进行了真实和大语言模型生成的认知行为治疗对话间的情感弧度比较分析。发现合成对话与真实对话在情感变化、情感蕴含语言及情感反应与调节模式上存在差异。强调了在心理健康应用中情感真实度的重要性。", "motivation": "动机是理解大型语言模型生成的合成治疗对话是否能够捕捉到真实治疗中的情感动态，尤其是在认知行为治疗中的情感弧度。", "method": "通过改编的Utterance Emotion Dynamics框架，分析了在真实和LLM生成的认知行为治疗对话之间的情感轨迹，并在包括价值、唤醒和主导度这三个维度上进行了细粒度的情感变化分析。研究涵盖了完整的对话以及个体发言角色（咨询师和客户）之间的分析。使用的数据材料包括从公开视频中转录的真实会话以及来自CACTUS数据集的合成对话。", "result": "研究结果表明，真实的治疗对话比合成对话包含了更多的情感变化、更多的情感蕴含语言、更规范的情感反应与调节模式。", "conclusion": "发现合成对话尽管流畅且结构一致，但在关键情感属性上与真实对话存在偏差，如情感变化更大、情绪蕴含语言更多和情感反应与调节模式更真实。特别是对于客户角色，真实和合成对话之间的情感弧度相似性较低。这些发现强调了当前LLM生成的治疗数据的局限性，以及在心理健康应用中情感真实度的重要性。"}}
{"id": "2508.20491", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.20491", "abs": "https://arxiv.org/abs/2508.20491", "authors": ["Seunghyeon Jung", "Seoyoung Hong", "Jiwoo Jeong", "Seungwon Jeong", "Jaerim Choi", "Hoki Kim", "Woojin Lee"], "title": "CaddieSet: A Golf Swing Dataset with Human Joint Features and Ball Information", "comment": "12 pages with supplementary material", "summary": "Recent advances in deep learning have led to more studies to enhance golfers'\nshot precision. However, these existing studies have not quantitatively\nestablished the relationship between swing posture and ball trajectory,\nlimiting their ability to provide golfers with the necessary insights for swing\nimprovement. In this paper, we propose a new dataset called CaddieSet, which\nincludes joint information and various ball information from a single shot.\nCaddieSet extracts joint information from a single swing video by segmenting it\ninto eight swing phases using a computer vision-based approach. Furthermore,\nbased on expert golf domain knowledge, we define 15 key metrics that influence\na golf swing, enabling the interpretation of swing outcomes through\nswing-related features. Through experiments, we demonstrated the feasibility of\nCaddieSet for predicting ball trajectories using various benchmarks. In\nparticular, we focus on interpretable models among several benchmarks and\nverify that swing feedback using our joint features is quantitatively\nconsistent with established domain knowledge. This work is expected to offer\nnew insight into golf swing analysis for both academia and the sports industry.", "AI": {"tldr": "我们提出了一个新的数据集CaddieSet，并验证了使用关节特征的挥杆反馈与现有的域知识在定量上是一致的，有助于高尔夫挥杆分析。", "motivation": "尽管在深度学习方面取得了进展，但现有研究未能定量地建立挥杆姿势与球飞行轨迹之间的关系，限制了它们为高尔夫球手提供挥杆改进所需的洞察力。", "method": "我们提出了一个名为CaddieSet的新数据集，该数据集包含单一击球的关节信息和各种球信息。CaddieSet通过计算机视觉方法将单个挥杆视频分割成八个挥杆阶段来提取关节信息。此外，基于专家高尔夫领域的知识，我们定义了15个关键指标，这些指标影响高尔夫挥杆，能够通过与挥杆相关的特征解释挥杆结果。", "result": "通过实验，我们证明了CaddieSet在预测球飞行轨迹方面是可行的，并且通过几种基准方法中的可解释模型验证了使用我们的关节特征的挥杆反馈与现有领域的知识在定量上是一致的。", "conclusion": "这项工作有望为高尔夫挥杆分析提供新的视角，不仅对学术界而且对体育产业都有积极影响。"}}
{"id": "2508.20766", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.20766", "abs": "https://arxiv.org/abs/2508.20766", "authors": ["Harethah Abu Shairah", "Hasan Abed Al Kader Hammoud", "George Turkiyyah", "Bernard Ghanem"], "title": "Turning the Spell Around: Lightweight Alignment Amplification via Rank-One Safety Injection", "comment": "Under Review", "summary": "Safety alignment in Large Language Models (LLMs) often involves mediating\ninternal representations to refuse harmful requests. Recent research has\ndemonstrated that these safety mechanisms can be bypassed by ablating or\nremoving specific representational directions within the model. In this paper,\nwe propose the opposite approach: Rank-One Safety Injection (ROSI), a white-box\nmethod that amplifies a model's safety alignment by permanently steering its\nactivations toward the refusal-mediating subspace. ROSI operates as a simple,\nfine-tuning-free rank-one weight modification applied to all residual stream\nwrite matrices. The required safety direction can be computed from a small set\nof harmful and harmless instruction pairs. We show that ROSI consistently\nincreases safety refusal rates - as evaluated by Llama Guard 3 - while\npreserving the utility of the model on standard benchmarks such as MMLU,\nHellaSwag, and Arc. Furthermore, we show that ROSI can also re-align\n'uncensored' models by amplifying their own latent safety directions,\ndemonstrating its utility as an effective last-mile safety procedure. Our\nresults suggest that targeted, interpretable weight steering is a cheap and\npotent mechanism to improve LLM safety, complementing more resource-intensive\nfine-tuning paradigms.", "AI": {"tldr": "This paper introduces ROSI, a method to enhance the safety of LLMs through targeted weight modification, proven effective without compromising model utility.", "motivation": "To counteract the risk of safety mechanisms being bypassed by ablating or removing specific representational directions within the model, as shown by recent research.", "method": "Rank-One Safety Injection (ROSI), a white-box method that amplifies a model's safety alignment by permanently steering its activations toward the refusal-mediating subspace, operating as a simple, fine-tuning-free rank-one weight modification.", "result": "ROSI consistently increases safety refusal rates while preserving the utility of the model on standard benchmarks and can re-align 'uncensored' models by amplifying their own latent safety directions.", "conclusion": "The targeted, interpretable weight steering method like ROSI is cheap, potent and effective for improving LLM safety, offering a valuable alternative to resource-intensive fine-tuning procedures."}}
