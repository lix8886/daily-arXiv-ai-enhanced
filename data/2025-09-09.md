<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 20]
- [cs.CV](#cs.CV) [Total: 17]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [An Empirical Analysis of Discrete Unit Representations in Speech Language Modeling Pre-training](https://arxiv.org/abs/2509.05359)
*Yanis Labrak,Richard Dufour,Mickaël Rouvier*

Main category: cs.CL

> 本文研究了语音语言模型中离散单元表示的优化，特别是在动态预训练期间，探讨了这些因素如何影响模型性能，并展示了如何随着模型规模变化来改进离散化策略。

<details>
  <summary>Details</summary>

**Motivation:** 研究离散单元表示的目的是为了提高语音语言模型中的语音建模能力，尤其是在动态预训练阶段，通过优化这些单元表示可以提升整体模型的性能和泛化能力。

**Method:** 本文通过系统性地考察模型架构、数据表示和训练稳健性在预训练阶段对语音模态适应的影响，研究了语音语言模型中离散单元表示的优化。特别关注了语音编码器在不同模型规模下的作用以及聚类粒度的影响，探讨了随着模型容量的变化，最优离散化策略的变化。此外，文章还研究了离散化训练与目标应用之间的域匹配对于模型稳健性的重要性。

**Result:** 实验表明，在不同的模型规模下，语音编码器和聚类粒度对预训练阶段有重要影响。通过对聚类分布和音素对齐的分析，揭示了有声词汇的有效使用，发现了语言学和副语言学模式。

**Conclusion:** 研究表明，优化语音语言模型中的离散单元表示需要考虑模型架构、数据表示和训练稳健性等多方面因素，并且最优的离散化策略会随着模型容量的变化而变化。

**Abstract:** This paper investigates discrete unit representations in Speech Language
Models (SLMs), focusing on optimizing speech modeling during continual
pre-training. In this paper, we systematically examine how model architecture,
data representation, and training robustness influence the pre-training stage
in which we adapt existing pre-trained language models to the speech modality.
Our experiments highlight the role of speech encoders and clustering
granularity across different model scales, showing how optimal discretization
strategies vary with model capacity. By examining cluster distribution and
phonemic alignments, we investigate the effective use of discrete vocabulary,
uncovering both linguistic and paralinguistic patterns. Additionally, we
explore the impact of clustering data selection on model robustness,
highlighting the importance of domain matching between discretization training
and target applications.

</details>


### [2] [Beyond ROUGE: N-Gram Subspace Features for LLM Hallucination Detection](https://arxiv.org/abs/2509.05360)
*Jerry Li,Evangelos Papalexakis*

Main category: cs.CL

> Error

<details>
  <summary>Details</summary>

**Motivation:** Error

**Method:** Error

**Result:** Error

**Conclusion:** Error

**Abstract:** Large Language Models (LLMs) have demonstrated effectiveness across a wide
variety of tasks involving natural language, however, a fundamental problem of
hallucinations still plagues these models, limiting their trustworthiness in
generating consistent, truthful information. Detecting hallucinations has
quickly become an important topic, with various methods such as uncertainty
estimation, LLM Judges, retrieval augmented generation (RAG), and consistency
checks showing promise. Many of these methods build upon foundational metrics,
such as ROUGE, BERTScore, or Perplexity, which often lack the semantic depth
necessary to detect hallucinations effectively. In this work, we propose a
novel approach inspired by ROUGE that constructs an N-Gram frequency tensor
from LLM-generated text. This tensor captures richer semantic structure by
encoding co-occurrence patterns, enabling better differentiation between
factual and hallucinated content. We demonstrate this by applying tensor
decomposition methods to extract singular values from each mode and use these
as input features to train a multi-layer perceptron (MLP) binary classifier for
hallucinations. Our method is evaluated on the HaluEval dataset and
demonstrates significant improvements over traditional baselines, as well as
competitive performance against state-of-the-art LLM judges.

</details>


### [3] [A Lightweight Framework for Trigger-Guided LoRA-Based Self-Adaptation in LLMs](https://arxiv.org/abs/2509.05385)
*Jiacheng Wei,Faguo Wu,Xiao Zhang*

Main category: cs.CL

> SAGE框架通过分解复杂任务和动态调整参数，解决了大语言模型在推理阶段适应新数据的问题。

<details>
  <summary>Details</summary>

**Motivation:** 旨在解决大语言模型不能在推理阶段持续适应和学习新数据的问题，以改善复杂推理任务的处理。

**Method:** 内容提出了SAGE框架，以解决大语言模型无法在推理过程中持续适应新数据的问题。SAGE框架包含三个关键部分：(1) 触发器模块，用于通过多个评估指标实时检测推理失败；(2) 触发器缓冲模块，使用HDBSCAN进行流聚类并进行稳定性检查和基于相似性的合并；(3) Lora存储模块，通过适应器池实现动态参数更新以保持知识。

**Result:** 评估结果显示，SAGE框架在原子推理子任务上具有出色的准确性、鲁棒性和稳定性，特别是在测试期间动态更新知识时。

**Conclusion:** SAGE框架通过动态知识更新来提高复杂推理任务的处理能力，展示了在原子推理任务上的优越性能。

**Abstract:** Large language models are unable to continuously adapt and learn from new
data during reasoning at inference time. To address this limitation, we propose
that complex reasoning tasks be decomposed into atomic subtasks and introduce
SAGE, a trigger-guided dynamic fine-tuning framework that enables adaptive
updates during reasoning at inference time. SAGE consists of three key
components: (1) a Trigger module that detects reasoning failures through
multiple evaluation metrics in real time; (2) a Trigger Buffer module that
clusters anomaly samples using a streaming clustering process with HDBSCAN,
followed by stability checks and similarity-based merging; and (3) a Lora Store
module that dynamically optimizes parameter updates with an adapter pool for
knowledge retention. Evaluation results show that SAGE demonstrates excellent
accuracy, robustness, and stability on the atomic reasoning subtask through
dynamic knowledge updating during test time.

</details>


### [4] [Talk Isn't Always Cheap: Understanding Failure Modes in Multi-Agent Debate](https://arxiv.org/abs/2509.05396)
*Andrea Wynn,Harsh Satija,Gillian Hadfield*

Main category: cs.CL

> 研究发现，多智能体辩论可能会导致准确性随着时间的推移而下降，即使更强大的模型占多数，因为模型倾向于与错误推理达成一致。

<details>
  <summary>Details</summary>

**Motivation:** 尽管多智能体辩论被认为是一种提高AI推理能力的有前途的策略，但现有工作仅关注同质化代理组内的辩论。本文动机在于探索不同能力模型如何影响多智能体交互。

**Method:** 通过一系列实验，探讨了模型能力的多样性如何影响多智能体交互的动态和结果。

**Result:** 实验表明，辩论可能导致正确答案转向错误答案，即使能力更强的模型占多数，这揭示了在多智能体辩论中，模型往往偏向于达成一致而不是挑战错误的推理。

**Conclusion:** 这些结果揭示了多智能体辩论中交换理由时的重要失败模式，表明在代理没有被激励或充分装备以抵抗有说服力但错误的推理时，辩论可能导致性能下降。

**Abstract:** While multi-agent debate has been proposed as a promising strategy for
improving AI reasoning ability, we find that debate can sometimes be harmful
rather than helpful. The prior work has exclusively focused on debates within
homogeneous groups of agents, whereas we explore how diversity in model
capabilities influences the dynamics and outcomes of multi-agent interactions.
Through a series of experiments, we demonstrate that debate can lead to a
decrease in accuracy over time -- even in settings where stronger (i.e., more
capable) models outnumber their weaker counterparts. Our analysis reveals that
models frequently shift from correct to incorrect answers in response to peer
reasoning, favoring agreement over challenging flawed reasoning. These results
highlight important failure modes in the exchange of reasons during multi-agent
debate, suggesting that naive applications of debate may cause performance
degradation when agents are neither incentivized nor adequately equipped to
resist persuasive but incorrect reasoning.

</details>


### [5] [No Translation Needed: Forecasting Quality from Fertility and Metadata](https://arxiv.org/abs/2509.05425)
*Jessica M. Lundin,Ada Zhang,David Adelani,Cody Carroll*

Main category: cs.CL

> 通过少量特征实现了不运行翻译系统而预测翻译质量的准确方法，梯度提升模型展示了良好的预测效果。

<details>
  <summary>Details</summary>

**Motivation:** 研究目标在于探索是否能在不实际运行翻译模型的情况下，仅通过一些简单特征对翻译质量进行预测。

**Method:** 该论文使用令牌生育率比率、令牌计数和基本语言元数据（如同语言族系、书写系统、地区）构建预测模型，并使用梯度提升算法预测翻译质量，该模型在FLORES-200基准上评估GPT-4o翻译在203种语言的效果。

**Result:** 我们展示了可以在不运行翻译系统的情况下，以惊人的准确性预测翻译质量。使用少量特征，如令牌生育率比率、令牌计数和基本语言元数据（语言族系、书写系统、地区），我们能够预测FLORES-200基准上GPT-4o翻译的ChrF分数，跨越203种语言。梯度提升模型展现了良好的性能（XX$ightarrow$英语的$R^{2}=0.66$和英语$ightarrow$XX的$R^{2}=0.72$）。特征求重要性分析表明，对英语的预测主要由类型学因素决定，而对于多样化的目标语言翻译，生育率则扮演更重要的角色。这些发现表明翻译质量受令牌层次的生育率和更广泛的语言类型学影响，为多语言评估和质量估计提供了新见解。

**Conclusion:** 研究表明翻译质量不仅受令牌生育率影响，还受语言类型学因素影响，这为未来的多语言评估和质量估计提供了新的研究方向。

**Abstract:** We show that translation quality can be predicted with surprising accuracy
\textit{without ever running the translation system itself}. Using only a
handful of features, token fertility ratios, token counts, and basic linguistic
metadata (language family, script, and region), we can forecast ChrF scores for
GPT-4o translations across 203 languages in the FLORES-200 benchmark. Gradient
boosting models achieve favorable performance ($R^{2}=0.66$ for
XX$\rightarrow$English and $R^{2}=0.72$ for English$\rightarrow$XX). Feature
importance analyses reveal that typological factors dominate predictions into
English, while fertility plays a larger role for translations into diverse
target languages. These findings suggest that translation quality is shaped by
both token-level fertility and broader linguistic typology, offering new
insights for multilingual evaluation and quality estimation.

</details>


### [6] [Direct-Scoring NLG Evaluators Can Use Pairwise Comparisons Too](https://arxiv.org/abs/2509.05440)
*Logan Lawrence,Ashton Williamson,Alexander Shelton*

Main category: cs.CL

> 研究提出了一种直接评分方法，利用合成摘要作为测试时的机器排名，展示了在多个评测基准上与当前最先进方法相当或略优的性能。

<details>
  <summary>Details</summary>

**Motivation:** 由于现有的基于大语言模型的自动评分方法难以对个体生成的内容进行绝对评分，本研究旨在提供一种可替代的直接评分方法。

**Method:** 本研究提出了一种直接评分方法，该方法利用合成摘要，在测试时作为机器成对排名，解决了以往方法难以对个体摘要进行绝对评分的问题。

**Result:** 实验结果显示，所提出的方法在SummEval，TopicalChat和HANNA等评测基准上的样本级相关性与最先进的成对评估器相当，且略胜一筹。

**Conclusion:** 本研究释放了合成上下文摘要作为数据，以促进未来研究，并证明了直接评分方法的有效性。

**Abstract:** As large-language models have been increasingly used as automatic raters for
evaluating free-form content, including document summarization, dialog, and
story generation, work has been dedicated to evaluating such models by
measuring their correlations with human judgment. For \textit{sample-level}
performance, methods which operate by using pairwise comparisons between
machine-generated text perform well but often lack the ability to assign
absolute scores to individual summaries, an ability crucial for use cases that
require thresholding. In this work, we propose a direct-scoring method which
uses synthetic summaries to act as pairwise machine rankings at test time. We
show that our method performs comparably to state-of-the-art pairwise
evaluators in terms of axis-averaged sample-level correlations on the SummEval
(\textbf{+0.03}), TopicalChat (\textbf{-0.03}), and HANNA (\textbf{+0.05})
meta-evaluation benchmarks, and release the synthetic in-context summaries as
data to facilitate future work.

</details>


### [7] [From Staff Messages to Actionable Insights: A Multi-Stage LLM Classification Framework for Healthcare Analytics](https://arxiv.org/abs/2509.05484)
*Hajar Sakai,Yi-En Tseng,Mohammadsadegh Mikaeili,Joshua Bosire,Franziska Jovin*

Main category: cs.CL

> 本文提出了一种基于大语言模型的多阶段框架，用于自动化分类医院呼叫中心的员工消息，结果表明该方法显著提升了效率，并且能在满足HIPAA合规要求的同时提供决策支持。

<details>
  <summary>Details</summary>

**Motivation:** 传统的监督学习方法需要标注数据、大量的训练和模型调优，而大语言模型（LLM）提供了一种更计算效率的方法来进行医疗数据分析。

**Method:** 本文提出了一种多阶段基于大语言模型（LLM）的框架，用于识别员工信息主题并按多类别分类。该方法评估了多种LLM类型，包括推理型、通用型和轻量级模型。

**Result:** 表现最佳的模型是o3，其实现了78.4%的加权F1分数和79.2%的准确率，紧随其后的是gpt-5，达到了75.3%的加权F1分数和76.2%的准确率。

**Conclusion:** 该方法结合了数据安全措施和必要的HIPAA合规要求，提供的可视化决策支持工具将员工消息转化为可操作的见解，从而更有效地利用收集的员工消息数据，识别导航员培训机会，并支持改善患者体验和护理质量。

**Abstract:** Hospital call centers serve as the primary contact point for patients within
a hospital system. They also generate substantial volumes of staff messages as
navigators process patient requests and communicate with the hospital offices
following the established protocol restrictions and guidelines. This
continuously accumulated large amount of text data can be mined and processed
to retrieve insights; however, traditional supervised learning approaches
require annotated data, extensive training, and model tuning. Large Language
Models (LLMs) offer a paradigm shift toward more computationally efficient
methodologies for healthcare analytics. This paper presents a multi-stage
LLM-based framework that identifies staff message topics and classifies
messages by their reasons in a multi-class fashion. In the process, multiple
LLM types, including reasoning, general-purpose, and lightweight models, were
evaluated. The best-performing model was o3, achieving 78.4% weighted F1-score
and 79.2% accuracy, followed closely by gpt-5 (75.3% Weighted F1-score and
76.2% accuracy). The proposed methodology incorporates data security measures
and HIPAA compliance requirements essential for healthcare environments. The
processed LLM outputs are integrated into a visualization decision support tool
that transforms the staff messages into actionable insights accessible to
healthcare professionals. This approach enables more efficient utilization of
the collected staff messaging data, identifies navigator training
opportunities, and supports improved patient experience and care quality.

</details>


### [8] [The Token Tax: Systematic Bias in Multilingual Tokenization](https://arxiv.org/abs/2509.05486)
*Jessica M. Lundin,Ada Zhang,Nihal Karim,Hamza Louzan,Victor Wei,David Adelani,Cody Carroll*

Main category: cs.CL

> 该研究发现，形态复杂且资源较少的语言在大型语言模型中的标记化效率低下，导致计算资源增加和准确率降低。研究通过AfriMMLU数据集评估10个大型语言模型，发现生育率（词/标记数）与准确性高度相关。此外，推理模型在这些语言中表现更好，缩小了之前代次中的准确性差距。研究还指出，标记数量的增加会导致训练成本和时间的显著增加，强调了公平定价和多语言基准的重要性。

<details>
  <summary>Details</summary>

**Motivation:** 研究的动机在于揭示形态复杂且资源较少的语言在大型语言模型中面临的固有劣势，并探讨标记化效率低下对计算资源和准确率的影响。

**Method:** 研究方法包括对10个大型语言模型在包含9,000个多项选择题的AfriMMLU数据集上进行评估，并分析生育率如何影响这些模型的准确性。同时，研究比较了推理模型与非推理模型的性能差异。

**Result:** 研究结果表明，生育率是预测语言模型准确性的一个重要指标，推理模型在多种非洲语言数据集中表现更好。标记数量翻倍会导致训练时间和成本增加四倍。

**Conclusion:** 该研究的结论强调了形态化感知标记化方法、公平定价策略以及多语言基准测试对实现更公平的自然语言处理的重要性。

**Abstract:** Tokenization inefficiency imposes structural disadvantages on morphologically
complex, low-resource languages, inflating compute resources and depressing
accuracy. We evaluate 10 large language models (LLMs) on AfriMMLU (9,000 MCQA
items; 5 subjects; 16 African languages) and show that fertility (tokens/word)
reliably predicts accuracy. Higher fertility consistently predicts lower
accuracy across all models and subjects. We further find that reasoning models
(DeepSeek, o1) consistently outperform non-reasoning peers across high and low
resource languages in the AfriMMLU dataset, narrowing accuracy gaps observed in
prior generations. Finally, translating token inflation to economics, a
doubling in tokens results in quadrupled training cost and time, underscoring
the token tax faced by many languages. These results motivate morphologically
aware tokenization, fair pricing, and multilingual benchmarks for equitable
natural language processing (NLP).

</details>


### [9] [Biomedical Literature Q&A System Using Retrieval-Augmented Generation (RAG)](https://arxiv.org/abs/2509.05505)
*Mansi Garg,Lee-Chi Wang,Bhavesh Ghanchi,Sanjana Dumpala,Shreyash Kakde,Yen Chih Chen*

Main category: cs.CL

> This work presents a Q&A system for biomedical literature using RAG architecture that demonstrates substantial improvements in accessibility and accuracy of medical information.

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to improve access to accurate, evidence-based medical information, addressing the limitations of traditional health search engines and the delay in public access to biomedical research.

**Method:** This paper uses a Retrieval-Augmented Generation (RAG) architecture, integrating diverse biomedical sources such as PubMed articles, curated Q&A datasets, and medical encyclopedias. The retrieval pipeline utilizes MiniLM-based semantic embeddings and FAISS vector search, while answer generation is handled by a fine-tuned Mistral-7B-v0.3 language model optimized with QLoRA for training efficiency.

**Result:** The system shows significant improvements in factual consistency and semantic relevance, as measured by BERTScore (F1), compared to baseline models, especially with a focus on breast cancer literature.

**Conclusion:** The findings suggest that RAG-enhanced language models have the potential to enhance the accessibility of complex biomedical literature in the public domain, with future plans for multilingual adaptation and personalized AI systems in medicine.

**Abstract:** This work presents a Biomedical Literature Question Answering (Q&A) system
based on a Retrieval-Augmented Generation (RAG) architecture, designed to
improve access to accurate, evidence-based medical information. Addressing the
shortcomings of conventional health search engines and the lag in public access
to biomedical research, the system integrates diverse sources, including PubMed
articles, curated Q&A datasets, and medical encyclopedias ,to retrieve relevant
information and generate concise, context-aware responses. The retrieval
pipeline uses MiniLM-based semantic embeddings and FAISS vector search, while
answer generation is performed by a fine-tuned Mistral-7B-v0.3 language model
optimized using QLoRA for efficient, low-resource training. The system supports
both general medical queries and domain-specific tasks, with a focused
evaluation on breast cancer literature demonstrating the value of
domain-aligned retrieval. Empirical results, measured using BERTScore (F1),
show substantial improvements in factual consistency and semantic relevance
compared to baseline models. The findings underscore the potential of
RAG-enhanced language models to bridge the gap between complex biomedical
literature and accessible public health knowledge, paving the way for future
work on multilingual adaptation, privacy-preserving inference, and personalized
medical AI systems.

</details>


### [10] [Using Contrastive Learning to Improve Two-Way Reasoning in Large Language Models: The Obfuscation Task as a Case Study](https://arxiv.org/abs/2509.05553)
*Serge Lionel Nikiema,Jordan Samhi,Micheline Bénédicte Moumoula,Albérick Euraste Djiré,Abdoul Kader Kaboré,Jacques Klein,Tegawendé F. Bissyandé*

Main category: cs.CL

> 研究探讨了大型语言模型是否真正理解概念或仅识别模式，并提出双向推理作为测试真实理解的手段。通过对比细调方法，成功提升了模型的双向推理能力，而不仅仅是在表面模式识别上优化。

<details>
  <summary>Details</summary>

**Motivation:** 探讨AI模型是否具备真正理解能力，而非仅仅识别模式。

**Method:** 提出双向推理的概念并开发了对比细调（CFT）方法，使用三类示例进行训练：保持语义一致的正例、具有不同语义的反例以及前向混淆示例。

**Result:** 实验表明CFT方法能够实现双向推理，增强了反向能力同时保持前向任务性能。

**Conclusion:** 双向推理不仅作为一个评估真正理解的理论框架，同时作为提升AI系统能力的实用训练方法。

**Abstract:** This research addresses a fundamental question in AI: whether large language
models truly understand concepts or simply recognize patterns. The authors
propose bidirectional reasoning,the ability to apply transformations in both
directions without being explicitly trained on the reverse direction, as a test
for genuine understanding. They argue that true comprehension should naturally
allow reversibility. For example, a model that can change a variable name like
userIndex to i should also be able to infer that i represents a user index
without reverse training. The researchers tested current language models and
discovered what they term cognitive specialization: when models are fine-tuned
on forward tasks, their performance on those tasks improves, but their ability
to reason bidirectionally becomes significantly worse. To address this issue,
they developed Contrastive Fine-Tuning (CFT), which trains models using three
types of examples: positive examples that maintain semantic meaning, negative
examples with different semantics, and forward-direction obfuscation examples.
This approach aims to develop deeper understanding rather than surface-level
pattern recognition and allows reverse capabilities to develop naturally
without explicit reverse training. Their experiments demonstrated that CFT
successfully achieved bidirectional reasoning, enabling strong reverse
performance while maintaining forward task capabilities. The authors conclude
that bidirectional reasoning serves both as a theoretical framework for
assessing genuine understanding and as a practical training approach for
developing more capable AI systems.

</details>


### [11] [Ad hoc conventions generalize to new referents](https://arxiv.org/abs/2509.05566)
*Anya Ji,Claire Augusta Bergey,Ron Eliav,Yoav Artzi,Robert D. Hawkins*

Main category: cs.CL

> 研究通过实验表明，当人们讨论前所未闻的概念时，他们的描述方式不仅限于特定的具体实例，而是反映出更多的概念性对齐。

<details>
  <summary>Details</summary>

**Motivation:** 研究旨在探讨人们如何讨论之前从未提及过的事物。针对这一问题，本文提出了两种不同的观点，并采用实验来检验这些理论的有效性。

**Method:** 本研究通过二元交流实验（N=302）利用最近发布的KiloGram数据集来测试这些竞争性的观点，该数据集包含超过1,000个抽象的七巧板图像。参与者成对进行，并就一套图像的指示惯例进行了反复交流，然后测量他们对未讨论过的图像描述的一致性。

**Result:** 研究发现强有力的证据表明存在概念泛化：参与双方在未讨论过的图像描述上展现了一致性。这种泛化效果非线性地受到视觉相似性的影响（符合谢夫德定律），并且在不同形象命名水平上保持稳健。

**Conclusion:** 研究结果表明，临时的惯例并不是任意标签，而是反映真实的认知协调，这对于参考理论和更具适应性的语言代理设计具有重要意义。

**Abstract:** How do people talk about things they've never talked about before? One view
suggests that a new shared naming system establishes an arbitrary link to a
specific target, like proper names that cannot extend beyond their bearers. An
alternative view proposes that forming a shared way of describing objects
involves broader conceptual alignment, reshaping each individual's semantic
space in ways that should generalize to new referents. We test these competing
accounts in a dyadic communication study (N=302) leveraging the
recently-released KiloGram dataset containing over 1,000 abstract tangram
images. After pairs of participants coordinated on referential conventions for
one set of images through repeated communication, we measured the extent to
which their descriptions aligned for undiscussed images. We found strong
evidence for generalization: partners showed increased alignment relative to
their pre-test labels. Generalization also decayed nonlinearly with visual
similarity (consistent with Shepard's law) and was robust across levels of the
images' nameability. These findings suggest that ad hoc conventions are not
arbitrary labels but reflect genuine conceptual coordination, with implications
for theories of reference and the design of more adaptive language agents.

</details>


### [12] [Mitigating Spurious Correlations Between Question and Answer via Chain-of-Thought Correctness Perception Distillation](https://arxiv.org/abs/2509.05602)
*Hongyan Xie,Yitong Yao,Yikun Ban,Zixuan Huang,Deqing Wang,Zhenhe Wu,Haoxiang Su,Chao Wang,Shuangyong Song,Xuelong Li*

Main category: cs.CL

> Proposes CoPeD to improve reasoning of small language models by addressing noisy rationales in CoT data.

<details>
  <summary>Details</summary>

**Motivation:** To address the noise in CoT data that causes small language models to develop spurious correlations, thereby compromising their reasoning quality.

**Method:** Chain-of-Thought Correctness Perception Distillation (CoPeD), including a correctness-aware task setting and a Correctness-Aware Weighted loss to improve SLMs reasoning quality.

**Result:** The proposed method, CoPeD, shows effectiveness on both in-distribution and out-of-distribution reasoning datasets for small language models.

**Conclusion:** CoPeD enhances the reasoning capability of small language models, making fine-tuning more effective and robust in various reasoning tasks.

**Abstract:** Large language models (LLMs) excel at reasoning tasks but are expensive to
deploy. Thus small language models (SLMs) are fine-tuned on CoT data generated
by LLMs to copy LLMs' abilities. However, these CoT data may include noisy
rationales that either fail to substantiate the answers or contribute no
additional information to support answer prediction, which leads SLMs to
capture spurious correlations between questions and answers and compromise the
quality of reasoning. In this work, we propose Chain-of-Thought Correctness
Perception Distillation (CoPeD), which aims to improve the reasoning quality of
the student model from the perspectives of task setting and data utilization.
Firstly, we introduce a correctness-aware task setting that encourages the
student model to predict answers based on correct rationales and revise them
when they are incorrect. This setting improves the faithfulness of reasoning
and allows the model to learn from its mistakes. Then, we propose a
Correctness-Aware Weighted loss, which dynamically adjusts the contribution of
each training instance based on the combined loss of the rationale and the
answer. This strategy encourages the model to focus more on samples where the
rationale offers stronger support for the correct answer. Experiments have
shown that CoPeD is effective on both in-distribution (IND) and
out-of-distribution (OOD) benchmark reasoning datasets.

</details>


### [13] [Icon$^{2}$: Aligning Large Language Models Using Self-Synthetic Preference Data via Inherent Regulation](https://arxiv.org/abs/2509.05605)
*Qiyuan Chen,Hongsen Huang,Qian Shao,Jiahe Chen,Jintai Chen,Hongxia Xu,Renjie Hua,Ren Chuan,Jian Wu*

Main category: cs.CL

> 研究提出了一种名为Icon$^{2}$的新方法，通过利用大语言模型表示空间的内在调节，提高了偏好数据集构建的效率和目标模型的一致性，同时减少了计算成本。

<details>
  <summary>Details</summary>

**Motivation:** 传统的偏好数据集构建方法存在与目标模型分布不匹配及计算成本高的问题，该研究旨在改进这些问题。

**Method:** Structure

**Result:** {
  "tldr": "研究提出了一种名为Icon\u00b2的新方法，通过利用大语言模型表示空间的内在调节，提高了偏好数据集构建的效率和目标模型的一致性，同时减少了计算成本。", 
  "motivation": "传统的偏好数据集构建方法存在与目标模型分布不匹配及计算成本高的问题，该研究旨在改进这些问题。", 
  "method": "Icon\u00b2 方法首先提取层级别的方向向量以编码复杂的人类偏好，再利用这些向量筛选出自合成指令，并在解码过程中调整标记表示，生成具有明确对齐差异的响应对。", 
  "result": "实验结果显示，Llama3-8B和Qwen2-7B在AlpacaEval 2.0和Arena-Hard上的胜率分别提高了13.89%和13.45%，同时计算成本降低了48.1%。", 
  "conclusion": "Icon\u00b2 方法能够有效提升大语言模型与人类偏好的对齐度，同时显著减少计算资源的消耗。"]

**Conclusion:** Icon$^{2}$ 方法能够有效提升大语言模型与人类偏好的对齐度，同时显著减少计算资源的消耗。

**Abstract:** Large Language Models (LLMs) require high quality preference datasets to
align with human preferences. However, conventional methods for constructing
such datasets face significant challenges: reliance on pre-collected
instructions often leads to distribution mismatches with target models, while
the need for sampling multiple stochastic responses introduces substantial
computational overhead. In this work, we explore a paradigm shift by leveraging
inherent regulation of LLMs' representation space for efficient and tailored
preference dataset construction, named Icon$^{2}$. Specifically, it first
extracts layer-wise direction vectors to encode sophisticated human preferences
and then uses these vectors to filter self-synthesized instructions based on
their inherent consistency. During decoding, bidirectional inherent control is
applied to steer token representations, enabling the precise generation of
response pairs with clear alignment distinctions. Experimental results
demonstrate significant improvements in both alignment and efficiency.
Llama3-8B and Qwen2-7B achieve an average win rate improvement of 13.89% on
AlpacaEval 2.0 and 13.45% on Arena-Hard, while reducing computational costs by
up to 48.1%.

</details>


### [14] [Beyond Keywords: Driving Generative Search Engine Optimization with Content-Centric Agents](https://arxiv.org/abs/2509.05607)
*Qiyuan Chen,Jiahe Chen,Hongsen Huang,Qian Shao,Jintai Chen,Renjie Hua,Hongxia Xu,Ruijia Wu,Ren Chuan,Jian Wu*

Main category: cs.CL

> 本文介绍了一个新的GSEO框架，包括构建基准、评价系统和多智能体系统，揭示内容影响力的动态，并提供了未来研究的理论基础。

<details>
  <summary>Details</summary>

**Motivation:** 传统排名搜索引擎向生成式搜索引擎的转变使旧的SEO指标过时，解决如何理解和优化内容对合成答案影响的需求。

**Method:** 提出了一种端到端的生成式搜索引擎优化（GSEO）框架，包括构建一个大规模的内容为中心的基准（CC-GSEO-Bench）和一个多维度的评价体系，以及设计一个多智能体系统来自动优化内容策略。

**Result:** 通过这个框架的实证分析揭示了内容影响力的新见解，并为内容创作者提供了可行的策略，为未来的GSEO研究建立了理论基础。

**Conclusion:** 生成式搜索引擎优化的框架提供了理解和衡量内容对合成答案影响力的系统方法，为内容创作者提供了操作策略，并为未来的GSEO研究奠定了基础。

**Abstract:** The paradigm shift from traditional ranked-based search to Generative Search
Engines has rendered conventional SEO metrics obsolete, creating an urgent need
to understand, measure, and optimize for content influence on synthesized
answers. This paper introduces a comprehensive, end-to-end framework for
Generative Search Engine Optimization (GSEO) to address this challenge. We make
two primary contributions. First, we construct CC-GSEO-Bench, a large-scale,
content-centric benchmark, and propose a multi-dimensional evaluation framework
that systematically quantifies influence, moving beyond surface-level
attribution to assess substantive semantic impact. Second, we design a novel
multi-agent system that operationalizes this framework, automating the
strategic refinement of content through a collaborative analyze-revise-evaluate
workflow. Our empirical analysis using this framework reveals novel insights
into the dynamics of content influence, offering actionable strategies for
creators and establishing a principled foundation for future GSEO research.

</details>


### [15] [New Insights into Optimal Alignment of Acoustic and Linguistic Representations for Knowledge Transfer in ASR](https://arxiv.org/abs/2509.05609)
*Xugang Lu,Peng Shen,Yu Tsao,Hisashi Kawai*

Main category: cs.CL

> 本文将对齐和匹配视为检测问题，引入了基于不平衡最优传输的对齐模型来处理ASR中的声学和语言表示对齐问题，并展示了该方法在实验中的有效性。

<details>
  <summary>Details</summary>

**Motivation:** 对齐声学和语言表示是自动语音识别（ASR）中预训练模型知识转移的关键挑战。而这种对齐具有固有的结构化和非对称特性。

**Method:** 我们提出了一种基于不平衡最优传输的对齐模型，该模型能够显式处理声学和语言模态之间的分布不匹配和结构不对称问题，通过软且部分的匹配实现灵活的知识转移。

**Result:** 实验结果展示了我们方法的有效性，能够灵活控制匹配程度，从而提升ASR性能。

**Conclusion:** 我们的方法确保了每个语言标记都被至少一个声学观测所支撑，同时允许声学单位到语言单位的灵活概率化映射，从而改善了ASR性能。

**Abstract:** Aligning acoustic and linguistic representations is a central challenge to
bridge the pre-trained models in knowledge transfer for automatic speech
recognition (ASR). This alignment is inherently structured and asymmetric:
while multiple consecutive acoustic frames typically correspond to a single
linguistic token (many-to-one), certain acoustic transition regions may relate
to multiple adjacent tokens (one-to-many). Moreover, acoustic sequences often
include frames with no linguistic counterpart, such as background noise or
silence may lead to imbalanced matching conditions. In this work, we take a new
insight to regard alignment and matching as a detection problem, where the goal
is to identify meaningful correspondences with high precision and recall
ensuring full coverage of linguistic tokens while flexibly handling redundant
or noisy acoustic frames in transferring linguistic knowledge for ASR. Based on
this new insight, we propose an unbalanced optimal transport-based alignment
model that explicitly handles distributional mismatch and structural
asymmetries with soft and partial matching between acoustic and linguistic
modalities. Our method ensures that every linguistic token is grounded in at
least one acoustic observation, while allowing for flexible, probabilistic
mappings from acoustic to linguistic units. We evaluate our proposed model with
experiments on an CTC-based ASR system with a pre-trained language model for
knowledge transfer. Experimental results demonstrate the effectiveness of our
approach in flexibly controlling degree of matching and hence to improve ASR
performance.

</details>


### [16] [From Joy to Fear: A Benchmark of Emotion Estimation in Pop Song Lyrics](https://arxiv.org/abs/2509.05617)
*Shay Dahary,Avi Edana,Alexander Apartsin,Yehudit Aperstein*

Main category: cs.CL

> 文章研究了歌曲歌词多标签情感标注任务，评估了多个语言模型在零样本场景下的表现，并对BERT模型进行了微调。结果表明，大规模语言模型在识别歌曲歌词中的情感方面具有潜力。

<details>
  <summary>Details</summary>

**Motivation:** 歌曲歌词的情感内容对听众体验和音乐喜好有着重要的影响。本文旨在探索歌曲歌词的多标签情感标注任务，旨在为音乐信息检索应用提供基于情感的决策支持。

**Method:** 本研究通过多标签情感标注任务来预测歌曲歌词的六种基础情感强度。创建了一个基于平均意见分数（MOS）的手动标注数据集，并对几个公开的大型语言模型（LLMs）在零样本场景下的表现进行了全面评估。此外，还对一个基于BERT的模型进行了微调，专门用于预测多标签情感得分。

**Result:** 实验结果显示零样本模型和微调模型在捕捉歌词情感细微差别方面的相对优劣。研究结果表明LLMs在创意文本情感识别方面的潜力，并为基于情感的音乐信息检索提供了模型选择策略的见解。

**Conclusion:** 本研究强调了语言模型在歌曲歌词情感分析中的潜力和应用前景，有助于优化基于情感的音乐推荐系统。

**Abstract:** The emotional content of song lyrics plays a pivotal role in shaping listener
experiences and influencing musical preferences. This paper investigates the
task of multi-label emotional attribution of song lyrics by predicting six
emotional intensity scores corresponding to six fundamental emotions. A
manually labeled dataset is constructed using a mean opinion score (MOS)
approach, which aggregates annotations from multiple human raters to ensure
reliable ground-truth labels. Leveraging this dataset, we conduct a
comprehensive evaluation of several publicly available large language models
(LLMs) under zero-shot scenarios. Additionally, we fine-tune a BERT-based model
specifically for predicting multi-label emotion scores. Experimental results
reveal the relative strengths and limitations of zero-shot and fine-tuned
models in capturing the nuanced emotional content of lyrics. Our findings
highlight the potential of LLMs for emotion recognition in creative texts,
providing insights into model selection strategies for emotion-based music
information retrieval applications. The labeled dataset is available at
https://github.com/LLM-HITCS25S/LyricsEmotionAttribution.

</details>


### [17] [Few-Shot Query Intent Detection via Relation-Aware Prompt Learning](https://arxiv.org/abs/2509.05635)
*Liang Zhang,Yuan Li,Shijie Zhang,Zheng Zhang,Xitong Li*

Main category: cs.CL

> 研究了一种新的对话系统意图检测框架SAID，整合文本和结构信息，并引入QueryAdapt机制，实现更精细的知识迁移和意图检测。

<details>
  <summary>Details</summary>

**Motivation:** 现有的方法主要集中在文本数据上，忽略了对话系统中关键的结构信息，如查询-查询关系和查询-回答关系。通过融合结构信息，可以更有效地进行意图检测。

**Method:** SAID框架结合了文本和关系结构信息进行模型预训练，并提出QueryAdapt机制，在关系标记级别生成特定意图的关系标记，提升了知识的细粒度转移。

**Result:** 在两个真实世界的数据集上，SAID显著优于现有最先进方法。

**Conclusion:** 通过集成SAID框架和QueryAdapt机制，能更有效地利用结构信息进行少样本环境下意图检测任务的预训练。

**Abstract:** Intent detection is a crucial component of modern conversational systems,
since accurately identifying user intent at the beginning of a conversation is
essential for generating effective responses. Recent efforts have focused on
studying this problem under a challenging few-shot scenario. These approaches
primarily leverage large-scale unlabeled dialogue text corpora to pretrain
language models through various pretext tasks, followed by fine-tuning for
intent detection with very limited annotations. Despite the improvements
achieved, existing methods have predominantly focused on textual data,
neglecting to effectively capture the crucial structural information inherent
in conversational systems, such as the query-query relation and query-answer
relation. To address this gap, we propose SAID, a novel framework that
integrates both textual and relational structure information in a unified
manner for model pretraining for the first time. Building on this framework, we
further propose a novel mechanism, the query-adaptive attention network
(QueryAdapt), which operates at the relation token level by generating
intent-specific relation tokens from well-learned query-query and query-answer
relations explicitly, enabling more fine-grained knowledge transfer. Extensive
experimental results on two real-world datasets demonstrate that SAID
significantly outperforms state-of-the-art methods.

</details>


### [18] [LM-Searcher: Cross-domain Neural Architecture Search with LLMs via Unified Numerical Encoding](https://arxiv.org/abs/2509.05657)
*Yuxuan Hu,Jihao Liu,Ke Wang,Jinliang Zhen,Weikang Shi,Manyuan Zhang,Qi Dou,Rui Liu,Aojun Zhou,Hongsheng Li*

Main category: cs.CL

> The paper introduces LM-Searcher, a novel framework for cross-domain neural architecture optimization utilizing LLMs without extensive domain-specific adaptation, focusing on a numerical string representation and ranking task reformulation.

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations posed by existing LLM-driven neural architecture search methods which are heavily dependent on prompt engineering and domain-specific tuning, and to enhance practicality and scalability across diverse tasks.

**Method:** The framework uses a universal numerical string representation for neural architectures named NCode, and transforms the NAS problem into a ranking task to select high-performing models from candidate pools using instruction-tuning samples derived from a novel subspace sampling strategy.

**Result:** Comprehensive experiments show that LM-Searcher achieves competitive results in both in-domain and out-of-domain tasks, indicating its robustness and adaptability.

**Conclusion:** LM-Searcher introduces a new approach for generalizable LLM-based architecture search by leveraging a cross-domain numerical representation and a redefined NAS as a ranking task, overcoming the existing limitations of LLM-driven NAS methods.

**Abstract:** Recent progress in Large Language Models (LLMs) has opened new avenues for
solving complex optimization problems, including Neural Architecture Search
(NAS). However, existing LLM-driven NAS approaches rely heavily on prompt
engineering and domain-specific tuning, limiting their practicality and
scalability across diverse tasks. In this work, we propose LM-Searcher, a novel
framework that leverages LLMs for cross-domain neural architecture optimization
without the need for extensive domain-specific adaptation. Central to our
approach is NCode, a universal numerical string representation for neural
architectures, which enables cross-domain architecture encoding and search. We
also reformulate the NAS problem as a ranking task, training LLMs to select
high-performing architectures from candidate pools using instruction-tuning
samples derived from a novel pruning-based subspace sampling strategy. Our
curated dataset, encompassing a wide range of architecture-performance pairs,
encourages robust and transferable learning. Comprehensive experiments
demonstrate that LM-Searcher achieves competitive performance in both in-domain
(e.g., CNNs for image classification) and out-of-domain (e.g., LoRA
configurations for segmentation and generation) tasks, establishing a new
paradigm for flexible and generalizable LLM-based architecture search. The
datasets and models will be released at https://github.com/Ashone3/LM-Searcher.

</details>


### [19] [Cross-Question Method Reuse in Large Language Models: From Word-Level Prediction to Rational Logical-Layer Reasoning](https://arxiv.org/abs/2509.05660)
*Hong Su*

Main category: cs.CL

> 本文提出了一种新的方法复用策略，扩展了问题之间的相似性定义，能够在低相似性或隐性相似性问题中复用解决方案，实验表明这种方法能够提升跨问题方法复用的有效性。

<details>
  <summary>Details</summary>

**Motivation:** 该研究的动机在于解决现有方法要求问题高度相似才能复用已有解决方案的问题，提出了一种新的方法来拓展低相似性问题或隐性相似性问题的方法复用范围。

**Method:** 该研究提出了一种扩展方法复用范围的新方法。对于一般-特定意义的相似问题，即广义上或狭义上的相关问题，提出先将问题和解决方案分离，指导大型语言模型（LLM）适应新但相关的问题，从而专注于解决方案的转移，而不是问题识别。此外，该方法还扩展到问题部分特征或隐藏特征相似的情况。

**Result:** 实验结果显示，该方法能够增加过滤出可复用解决方案的概率，提高了跨问题方法复用的效果。

**Conclusion:** 实验验证表明，通过扩展方法复用范围，可以提高过滤可复用解决方案的概率，从而提高跨问题方法复用的有效性。

**Abstract:** Large language models (LLMs) have been widely applied to assist in finding
solutions for diverse questions. Prior work has proposed representing a method
as a pair of a question and its corresponding solution, enabling method reuse.
However, existing approaches typically require the questions to be highly
similar. In this paper, we extend the scope of method reuse to address
questions with low similarity or with hidden similarities that are not
explicitly observable. For questions that are similar in a general-specific
sense (i.e., broader or narrower in scope), we propose to first separate the
question and solution, rather than directly feeding the pair to the LLM. The
LLM is then guided to adapt the solution to new but related questions, allowing
it to focus on solution transfer rather than question recognition. Furthermore,
we extend this approach to cases where questions only share partial features or
hidden characteristics. This enables cross-question method reuse beyond
conventional similarity constraints. Experimental verification shows that our
scope-extension approach increases the probability of filtering out reusable
solutions, thereby improving the effectiveness of cross-question method reuse.

</details>


### [20] [Llama-GENBA-10B: A Trilingual Large Language Model for German, English and Bavarian](https://arxiv.org/abs/2509.05668)
*Michael Hoffmann,Jophin John,Stefan Schweter,Gokul Ramakrishnan,Hoi-Fong Mak,Alice Zhang,Dmitry Gaynullin,Nicolay J. Hammer*

Main category: cs.CL

> Llama-GENBA-10B是一个平衡了英语、德语和巴伐利亚语训练数据的多语种基础模型，尤其在巴伐利亚语中表现优秀，为包含低资源语言的模型开发提供了新视角。

<details>
  <summary>Details</summary>

**Motivation:** 该项目的目标是为了服务于德语NLP社区，并推广作为低资源语言的巴伐利亚语。

**Method:** 该研究开发了一个名为Llama-GENBA-10B的多语种基础模型，旨在解决大型语言模型中的英语中心偏见问题。该模型基于Llama 3.1-8B，扩展到10B参数。通过对164B个令牌进行持续预训练（其中82B个英语令牌、82B个德语令牌和80M个巴伐利亚语令牌），平衡资源分配以防止英语占据主导地位。开发过程中旨在解决的挑战包括多语种语料库的创建、多语种统一分词器的创建、架构和语言比例超参数的优化，以及建立第一个标准的三语种评估套件。

**Result:** 评估结果显示，Llama-GENBA-10B在跨语言性能上表现出色，微调后的变体在巴伐利亚语中超过了Apertus-8B-2509和gemma-2-9b，并且在英语方面优于EuroLLM，在德语方面与EuroLLM相当。

**Conclusion:** 研究证明了Cerebras CS-2上的大规模多语种预训练效果良好，并记录了能效，为整合低资源语言的包容性基础模型发展提供了蓝图。

**Abstract:** We present Llama-GENBA-10B, a trilingual foundation model addressing
English-centric bias in large language models. Built on Llama 3.1-8B and scaled
to 10B parameters, Llama-GENBA-10B is continuously pretrained on 164B tokens
(82B English, 82B German, and 80M Bavarian), balancing resources while
preventing English dominance. Targeted at the German NLP community, the model
also promotes Bavarian as a low-resource language. Development tackled four
challenges: (1) curating a multilingual corpus despite Bavarian scarcity, (2)
creating a unified tokenizer for English, German, and Bavarian, (3) optimizing
architecture and language-ratio hyperparameters for cross-lingual transfer, and
(4) establishing the first standardized trilingual evaluation suite by
translating German benchmarks into Bavarian. Evaluations show that
Llama-GENBA-10B achieves strong cross-lingual performance, with the fine-tuned
variant surpassing Apertus-8B-2509 and gemma-2-9b in Bavarian and establishing
itself as the best model in its class for this language, while also
outperforming EuroLLM in English and matching its results in German. Training
on the Cerebras CS-2 demonstrated efficient large-scale multilingual
pretraining with documented energy use, offering a blueprint for inclusive
foundation models that integrate low-resource languages.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [21] [Label Smoothing++: Enhanced Label Regularization for Training Neural Networks](https://arxiv.org/abs/2509.05307)
*Sachin Chhabra,Hemanth Venkateswara,Baoxin Li*

Main category: cs.CV

> 本文提出了一种新的标签正则化训练策略Label Smoothing++，它解决了传统标签平滑忽略类别间关系的问题，提升了模型的泛化能力，并减少了过于自信的预测

<details>
  <summary>Details</summary>

**Motivation:** 训练神经网络时，使用one-hot目标标签会导致过拟合和过度自信。虽然标签平滑通过为一个hot目标标签添加均匀概率向量来解决这个问题，但这种方法赋予了所有的非目标类别相同的重视程度，隐去了类别之间的关系

**Method:** 本文提出了一种名为Label Smoothing++的新标签正则化训练策略。该方法为非目标类别指定了非零概率，并考虑了类别之间的关系。与传统的标签平滑方法不同，本文的策略为每个目标类别使用固定标签，同时允许网络学习非目标类别的标签

**Result:** 通过在多个数据集上的广泛实验，作者展示了Label Smoothing++如何减少过度自信的预测，促进跨类关系，提高泛化能力

**Conclusion:** Label Smoothing++为非目标类赋予了非零概率，同时考虑了类别间的相互关系。这种新方法能够有效地提升模型的泛化能力和减少过度自信的预测

**Abstract:** Training neural networks with one-hot target labels often results in
overconfidence and overfitting. Label smoothing addresses this issue by
perturbing the one-hot target labels by adding a uniform probability vector to
create a regularized label. Although label smoothing improves the network's
generalization ability, it assigns equal importance to all the non-target
classes, which destroys the inter-class relationships. In this paper, we
propose a novel label regularization training strategy called Label
Smoothing++, which assigns non-zero probabilities to non-target classes and
accounts for their inter-class relationships. Our approach uses a fixed label
for the target class while enabling the network to learn the labels associated
with non-target classes. Through extensive experiments on multiple datasets, we
demonstrate how Label Smoothing++ mitigates overconfident predictions while
promoting inter-class relationships and generalization capabilities.

</details>


### [22] [VILOD: A Visual Interactive Labeling Tool for Object Detection](https://arxiv.org/abs/2509.05317)
*Isac Holm*

Main category: cs.CV

> The paper presents VILOD, a Human-in-the-Loop labeling tool for Object Detection that incorporates Visual Analytics to improve upon traditional Active Learning methods by enhancing transparency and human control.

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to address the challenges faced in Object Detection related to the acquisition of large and accurately labeled datasets. It aims to improve upon traditional Active Learning methods by integrating human expertise and providing transparency through interactive visualizations.

**Method:** The paper introduces VILOD, a visual interactive labeling tool for Object Detection that combines Active Learning (AL), Human-in-the-Loop (HITL), and Visual Analytics (VA). It uses t-SNE projections, uncertainty heatmaps, and model state views to empower users to implement diverse sample selection strategies.

**Result:** Empirical investigation using comparative use cases showed that distinct labeling strategies facilitated by VILOD's interactive visualizations led to competitive Object Detection performance trajectories when compared to a standard automated uncertainty sampling AL method.

**Conclusion:** VILOD is proposed as a novel tool and empirical insight into enhancing the transparency, manageability, and potential effectiveness of the HITL-AL workflow for Object Detection dataset annotation.

**Abstract:** The advancement of Object Detection (OD) using Deep Learning (DL) is often
hindered by the significant challenge of acquiring large, accurately labeled
datasets, a process that is time-consuming and expensive. While techniques like
Active Learning (AL) can reduce annotation effort by intelligently querying
informative samples, they often lack transparency, limit the strategic insight
of human experts, and may overlook informative samples not aligned with an
employed query strategy. To mitigate these issues, Human-in-the-Loop (HITL)
approaches integrating human intelligence and intuition throughout the machine
learning life-cycle have gained traction. Leveraging Visual Analytics (VA),
effective interfaces can be created to facilitate this human-AI collaboration.
This thesis explores the intersection of these fields by developing and
investigating "VILOD: A Visual Interactive Labeling tool for Object Detection".
VILOD utilizes components such as a t-SNE projection of image features,
together with uncertainty heatmaps and model state views. Enabling users to
explore data, interpret model states, AL suggestions, and implement diverse
sample selection strategies within an iterative HITL workflow for OD. An
empirical investigation using comparative use cases demonstrated how VILOD,
through its interactive visualizations, facilitates the implementation of
distinct labeling strategies by making the model's state and dataset
characteristics more interpretable (RQ1). The study showed that different
visually-guided labeling strategies employed within VILOD result in competitive
OD performance trajectories compared to an automated uncertainty sampling AL
baseline (RQ2). This work contributes a novel tool and empirical insight into
making the HITL-AL workflow for OD annotation more transparent, manageable, and
potentially more effective.

</details>


### [23] [Context-Aware Knowledge Distillation with Adaptive Weighting for Image Classification](https://arxiv.org/abs/2509.05319)
*Zhengda Li*

Main category: cs.CV

> 本文提出了自适应知识蒸馏方法，通过自学习的方法调整平衡因子alpha，并采用上下文感知模块来优化模型性能，实验表明其优于传统方法。

<details>
  <summary>Details</summary>

**Motivation:** 传统的知识蒸馏方法使用固定的平衡因子alpha作为超参数来组合硬标签和软标签的损失函数。然而，静态的alpha是次优的，因为在训练过程中硬监督和软监督之间的最优平衡可能会有所不同。

**Method:** 提出了一种自适应知识蒸馏（AKD）框架，首先将平衡因子alpha作为可学习参数来自动优化。然后引入一个公式，利用学生模型和教师模型之间的差距动态计算alpha，并且引入了一个上下文感知模块（CAM），使用MLP+Attention来自适应地重加权各个类别的教师模型输出。

**Result:** 在CIFAR-10数据集上，使用ResNet-50作为教师模型和ResNet-18作为学生模型的实验表明，该方法相较于固定权重的知识蒸馏基线方法，在准确性和收敛稳定性方面取得了更优的表现。

**Conclusion:** 自适应知识蒸馏（AKD）框架通过动态调整平衡因子alpha及引入上下文感知模块进行自适应加权，能有效提高学生模型的性能和训练的稳定性。

**Abstract:** Knowledge distillation (KD) is a widely used technique to transfer knowledge
from a large teacher network to a smaller student model. Traditional KD uses a
fixed balancing factor alpha as a hyperparameter to combine the hard-label
cross-entropy loss with the soft-label distillation loss. However, a static
alpha is suboptimal because the optimal trade-off between hard and soft
supervision can vary during training.
  In this work, we propose an Adaptive Knowledge Distillation (AKD) framework.
First we try to make alpha as learnable parameter that can be automatically
learned and optimized during training. Then we introduce a formula to reflect
the gap between the student and the teacher to compute alpha dynamically,
guided by student-teacher discrepancies, and further introduce a Context-Aware
Module (CAM) using MLP + Attention to adaptively reweight class-wise teacher
outputs. Experiments on CIFAR-10 with ResNet-50 as teacher and ResNet-18 as
student demonstrate that our approach achieves superior accuracy compared to
fixed-weight KD baselines, and yields more stable convergence.

</details>


### [24] [A Dataset Generation Scheme Based on Video2EEG-SPGN-Diffusion for SEED-VD](https://arxiv.org/abs/2509.05321)
*Yunfei Guo,Tao Zhang,Wu Huang,Yao Song*

Main category: cs.CV

> 本文介绍了一个新的多模态框架和方法，用于脑电波信号和视频刺激的对齐，开发此框架和相关数据集旨在促进情感分析、数据增强和脑-计算机接口领域的研究和应用。

<details>
  <summary>Details</summary>

**Motivation:** 本文的动机是开发先进工具以支持情感分析、数据增强及脑-计算机接口的应用，并且具有重要的研究和工程意义。

**Method:** 本文提出了一种开源框架Video2EEG-SPGN-Diffusion，该框架利用SEED-VD数据集生成以视频刺激为条件的脑电波信号的多模态数据集。同时，披露了一种用于对齐视频和脑电波数据对的工程管道，促进了具备脑电波对齐能力的多模态大型模型的训练。

**Result:** 作为主要贡献，本文发布了一个新的数据集，包含超过1000个SEED-VD视频刺激样本，配以62通道脑电波信号（200Hz）及情感标签，实现了视频-脑电波对齐，推动了多模态研究的进展。

**Conclusion:** 该框架为情感分析、数据增强和脑-计算机接口应用提供了新的工具，具有重要的研究和工程意义。

**Abstract:** This paper introduces an open-source framework, Video2EEG-SPGN-Diffusion,
that leverages the SEED-VD dataset to generate a multimodal dataset of EEG
signals conditioned on video stimuli. Additionally, we disclose an engineering
pipeline for aligning video and EEG data pairs, facilitating the training of
multimodal large models with EEG alignment capabilities. Personalized EEG
signals are generated using a self-play graph network (SPGN) integrated with a
diffusion model. As a major contribution, we release a new dataset comprising
over 1000 samples of SEED-VD video stimuli paired with generated 62-channel EEG
signals at 200 Hz and emotion labels, enabling video-EEG alignment and
advancing multimodal research. This framework offers novel tools for emotion
analysis, data augmentation, and brain-computer interface applications, with
substantial research and engineering significance.

</details>


### [25] [Application of discrete Ricci curvature in pruning randomly wired neural networks: A case study with chest x-ray classification of COVID-19](https://arxiv.org/abs/2509.05322)
*Pavithra Elumalai,Sudharsan Vijayaraghavan,Madhumita Mondal,Areejit Samal*

Main category: cs.CV

> This study explores the use of three edge-centric network measures (Forman-Ricci curvature, Ollivier-Ricci curvature, and edge betweenness centrality) to compress randomly wired neural networks (RWNNs) for efficient and performant COVID-19 chest x-ray image classification.

<details>
  <summary>Details</summary>

**Motivation:** To investigate how different edge-centric measures impact the efficiency and performance of pruned randomly wired neural networks, with a focus on identifying computationally efficient methods that maintain model accuracy.

**Method:** RWNNs are compressed using three edge-centric measures across three different network generators (ER, WS, and BA models).

**Result:** Results indicate that FRC-based pruning can effectively simplify RWNNs with significant computational advantages while maintaining performance comparable to ORC-based pruning.

**Conclusion:** FRC is suggested as a computationally efficient method for RWNN pruning that can maintain model performance, providing insights into the structural properties and efficiency of pruned networks.

**Abstract:** Randomly Wired Neural Networks (RWNNs) serve as a valuable testbed for
investigating the impact of network topology in deep learning by capturing how
different connectivity patterns impact both learning efficiency and model
performance. At the same time, they provide a natural framework for exploring
edge-centric network measures as tools for pruning and optimization. In this
study, we investigate three edge-centric network measures: Forman-Ricci
curvature (FRC), Ollivier-Ricci curvature (ORC), and edge betweenness
centrality (EBC), to compress RWNNs by selectively retaining important synapses
(or edges) while pruning the rest. As a baseline, RWNNs are trained for
COVID-19 chest x-ray image classification, aiming to reduce network complexity
while preserving performance in terms of accuracy, specificity, and
sensitivity. We extend prior work on pruning RWNN using ORC by incorporating
two additional edge-centric measures, FRC and EBC, across three network
generators: Erd\"{o}s-R\'{e}nyi (ER) model, Watts-Strogatz (WS) model, and
Barab\'{a}si-Albert (BA) model. We provide a comparative analysis of the
pruning performance of the three measures in terms of compression ratio and
theoretical speedup. A central focus of our study is to evaluate whether FRC,
which is computationally more efficient than ORC, can achieve comparable
pruning effectiveness. Along with performance evaluation, we further
investigate the structural properties of the pruned networks through modularity
and global efficiency, offering insights into the trade-off between modular
segregation and network efficiency in compressed RWNNs. Our results provide
initial evidence that FRC-based pruning can effectively simplify RWNNs,
offering significant computational advantages while maintaining performance
comparable to ORC.

</details>


### [26] [Optical Music Recognition of Jazz Lead Sheets](https://arxiv.org/abs/2509.05329)
*Juan Carlos Martinez-Sevilla,Francesco Foscarin,Patricia Garcia-Iasci,David Rizo,Jorge Calvo-Zaragoza,Gerhard Widmer*

Main category: cs.CV

> 本文为手写爵士乐领谱提供了一个新的数据集和专用的OMR模型，特别是解决了和弦识别问题。

<details>
  <summary>Details</summary>

**Motivation:** 本文旨在解决现有光学音乐识别系统未处理的和弦，并且处理手写图像存在高变异性及质量问题的挑战。

**Method:** 本文的方法包括两部分：首先，构建了一个包含293份手写爵士乐领谱的新数据集，这些领谱来自163个独特的曲目，共计2021个五线谱，并与Humdrum **kern和MusicXML的基准评分对齐。同时提供基于基准评分生成的合成评分图像。其次，开发了一种专门用于爵士乐领谱的光学音乐识别（OMR）模型，讨论了数据特定的分词选择，以及使用合成评分和预训练模型的优势。

**Result:** 尚未具体提供实验结果，但公开发布了所有代码、数据和模型，可供进一步研究和验证。

**Conclusion:** 本文为手写爵士乐领谱的光学音乐识别提出了一个新的数据集和一个专门的OMR模型，从而为研究这一领域的其他学者提供了一个有价值的研究资源。

**Abstract:** In this paper, we address the challenge of Optical Music Recognition (OMR)
for handwritten jazz lead sheets, a widely used musical score type that encodes
melody and chords. The task is challenging due to the presence of chords, a
score component not handled by existing OMR systems, and the high variability
and quality issues associated with handwritten images. Our contribution is
two-fold. We present a novel dataset consisting of 293 handwritten jazz lead
sheets of 163 unique pieces, amounting to 2021 total staves aligned with
Humdrum **kern and MusicXML ground truth scores. We also supply synthetic score
images generated from the ground truth. The second contribution is the
development of an OMR model for jazz lead sheets. We discuss specific
tokenisation choices related to our kind of data, and the advantages of using
synthetic scores and pretrained models. We publicly release all code, data, and
models.

</details>


### [27] [RT-VLM: Re-Thinking Vision Language Model with 4-Clues for Real-World Object Recognition Robustness](https://arxiv.org/abs/2509.05333)
*Junghyun Park,Tuan Anh Nguyen,Dugki Min*

Main category: cs.CV

> 介绍了RT-VLM框架，用于缓解因域转移而导致的现代对象识别模型准确性下降的问题。通过合成数据和参数高效的微调，以及在推理过程中采用两阶段重新思考方案，该方法在鲁棒性基准测试中表现优异。

<details>
  <summary>Details</summary>

**Motivation:** 为了解决现实世界中对象识别模型部署时由于域转移而导致的准确性严重下降问题，这些域转移涵盖了低级图像统计变化、对象姿态和视角变化、部分遮挡和类间视觉混淆。

**Method:** 通过独特的合成数据集生成管道产生标注有'4种线索'（精确边界框、类别名称、详细的对象级描述和全面的场景上下文描述）的图像，然后在该数据资源上对Llama 3.2 11B视觉指令进行参数高效的有监督微调。在推理时，采用两阶段的重新思考方案：模型首先发出自己的'四种线索'，然后作为证据重新检查这些答复并进行迭代修正。

**Result:** 在隔离单个域转移的鲁棒性基准测试中，RT-VLM 框架持续优于强大的基线。

**Conclusion:** 这些发现表明，结构化的多模态证据与明确的自我批判循环相结合，是实现可靠和可转移的视觉理解的一种有前景的方法。

**Abstract:** Real world deployments often expose modern object recognition models to
domain shifts that precipitate a severe drop in accuracy. Such shifts encompass
(i) variations in low level image statistics, (ii) changes in object pose and
viewpoint, (iii) partial occlusion, and (iv) visual confusion across adjacent
classes. To mitigate this degradation, we introduce the Re-Thinking Vision
Language Model (RT-VLM) framework. The foundation of this framework is a unique
synthetic dataset generation pipeline that produces images annotated with
"4-Clues": precise bounding boxes, class names, detailed object-level captions,
and a comprehensive context-level caption for the entire scene. We then perform
parameter efficient supervised tuning of Llama 3.2 11B Vision Instruct on this
resource. At inference time, a two stage Re-Thinking scheme is executed: the
model first emits its own four clues, then re examines these responses as
evidence and iteratively corrects them. Across robustness benchmarks that
isolate individual domain shifts, RT-VLM consistently surpasses strong
baselines. These findings indicate that the integration of structured
multimodal evidence with an explicit self critique loop constitutes a promising
route toward reliable and transferable visual understanding.

</details>


### [28] [A Real-Time, Vision-Based System for Badminton Smash Speed Estimation on Mobile Devices](https://arxiv.org/abs/2509.05334)
*Diwen Huang*

Main category: cs.CV

> This paper presents a low-cost, accessible system for measuring badminton smash speeds using a custom smartphone app that integrates computer vision technology and kinematic analysis, broadening access to advanced performance data.

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind this study is to address the gap in accessible technology for capturing performance metrics like shot speed and angle, which are crucial for athlete development but have historically been unaffordable and complex for amateur and recreational players.

**Method:** The paper describes a novel, cost-effective system for measuring smash speed in badminton using a custom-trained YOLOv5 model for shuttlecock detection and a Kalman filter for trajectory tracking. It also implements a video-based kinematic speed estimation method with spatiotemporal scaling to automatically calculate the shuttlecock's velocity from a standard video recording.

**Result:** The system is implemented into an intuitive mobile application, making high-level performance analytics accessible to a broader audience and enabling players at all levels to analyze and enhance their game.

**Conclusion:** The developed mobile application, which uses advanced computer vision and kinematic techniques, democratizes the access to badminton performance analytics, promoting player improvement and the sport's development.

**Abstract:** Performance metrics in sports, such as shot speed and angle, provide crucial
feedback for athlete development. However, the technology to capture these
metrics has historically been expensive, complex, and largely inaccessible to
amateur and recreational players. This paper addresses this gap in the context
of badminton, one of the world's most popular sports, by introducing a novel,
cost-effective, and user-friendly system for measuring smash speed using
ubiquitous smartphone technology. Our approach leverages a custom-trained
YOLOv5 model for shuttlecock detection, combined with a Kalman filter for
robust trajectory tracking. By implementing a video-based kinematic speed
estimation method with spatiotemporal scaling, the system automatically
calculates the shuttlecock's velocity from a standard video recording. The
entire process is packaged into an intuitive mobile application, democratizing
access to high-level performance analytics and empowering players at all levels
to analyze and improve their game.

</details>


### [29] [A Stroke-Level Large-Scale Database of Chinese Character Handwriting and the OpenHandWrite_Toolbox for Handwriting Research](https://arxiv.org/abs/2509.05335)
*Zebo Xu,Shaoyun Yu,Mark Torrance,Guido Nottbusch,Nan Zhao,Zhenguang Cai*

Main category: cs.CV

> 研究构建了一个大规模的汉字手写数据库，并改进了手写工具箱，揭示了正字法预测因子对汉字书写的准备和执行有影响，音韵学因素也在所有层级有影响，这表明汉字书写的准备和执行与语言组成部分紧密相关。

<details>
  <summary>Details</summary>

**Motivation:** 研究探讨了哪些语言组成部分（如音韵学、语义学和正字法系统）在汉字、部首和笔画级别上调节汉字书写，这是一个重要但研究不足的话题。此外，缺乏捕捉并批量处理精细手写数据的全面工具。研究旨在解决这些问题。

**Method:** 我们构建了一个大规模手写数据库，其中42名中文使用者在手写-听写任务中各手写1200个汉字。我们改进了现有的手写工具包，并提供了全面的文档，以支持实验设计的修改，捕捉笔画级手写轨迹并且批量处理手写测量数据（如延迟时间、持续时间和笔压）。

**Result:** 多重回归分析显示，正字法预测因子在汉字、部首和笔画级别上的书写准备和执行上产生影响，音韵学因素也对所有三个层级的执行有影响，这些影响在字符级别最为明显，随后是部首级别，在笔画级别最弱。

**Conclusion:** 本研究发现，部首和笔画级别的书写准备和执行与语言组成部分紧密相关。这个数据库和工具箱将为未来汉语及各种语言的心理语言学和神经语言学研究提供有价值的资源。

**Abstract:** Understanding what linguistic components (e.g., phonological, semantic, and
orthographic systems) modulate Chinese handwriting at the character, radical,
and stroke levels remains an important yet understudied topic. Additionally,
there is a lack of comprehensive tools for capturing and batch-processing
fine-grained handwriting data. To address these issues, we constructed a
large-scale handwriting database in which 42 Chinese speakers for each
handwriting 1200 characters in a handwriting-to-dictation task. Additionally,
we enhanced the existing handwriting package and provided comprehensive
documentation for the upgraded OpenHandWrite_Toolbox, which can easily modify
the experimental design, capture the stroke-level handwriting trajectory, and
batch-process handwriting measurements (e.g., latency, duration, and
pen-pressure). In analysing our large-scale database, multiple regression
results show that orthographic predictors impact handwriting preparation and
execution across character, radical, and stroke levels. Phonological factors
also influence execution at all three levels. Importantly, these lexical
effects demonstrate hierarchical attenuation - they were most pronounced at the
character level, followed by the radical, and were weakest at the stroke
levels. These findings demonstrate that handwriting preparation and execution
at the radical and stroke levels are closely intertwined with linguistic
components. This database and toolbox offer valuable resources for future
psycholinguistic and neurolinguistic research on the handwriting of characters
and sub-characters across different languages.

</details>


### [30] [Anticipatory Fall Detection in Humans with Hybrid Directed Graph Neural Networks and Long Short-Term Memory](https://arxiv.org/abs/2509.05337)
*Younggeol Cho,Gokhan Solak,Olivia Nocentini,Marta Lorenzini,Andrea Fortuna,Arash Ajoudani*

Main category: cs.CV

> 本文提出了一种预见性跌倒检测方法，使用结合DGNN和LSTM的混合模型，该模型展示了在跌倒预测误差和识别准确度方面优于文献中现有模型的表现。

<details>
  <summary>Details</summary>

**Motivation:** 尽管在跌倒检测方面已经取得了一定的进展，预测跌倒之前发生以及分析在稳定性和即将跌倒之间的瞬态状态仍然是未被充分探索的领域。本研究旨在提高跌倒预测的准确度和分析瞬态状态，以优化辅助性机器人的功能。

**Method:** 本研究提出了一种基于动态图神经网络（DGNN）和长短期记忆（LSTM）网络的混合模型的预见性跌倒检测方法。该模型将运动预测和步态分类任务分离，以实现跌倒的高准确度预测。实现实时从视频序列中提取骨骼特征作为输入。DGNN作为分类器区分三种步态状态：稳定、过渡和跌倒。LSTM网络则用于预测后续时间步骤中的人体运动，实现跌倒的早期检测。

**Result:** 该模型使用OUMVLP-Pose和URFD数据集进行训练和验证，展示出比仅依赖DGNN的模型和文献中的模型在预测误差和识别准确度方面更优的表现。

**Conclusion:** 研究结果表明，分离预测和分类任务比使用仅依赖DGNN解决统一问题具有更好的性能。此外，该方法允许监测瞬态状态，可以为增强先进辅助系统的功能提供有价值的信息。

**Abstract:** Detecting and preventing falls in humans is a critical component of assistive
robotic systems. While significant progress has been made in detecting falls,
the prediction of falls before they happen, and analysis of the transient state
between stability and an impending fall remain unexplored. In this paper, we
propose a anticipatory fall detection method that utilizes a hybrid model
combining Dynamic Graph Neural Networks (DGNN) with Long Short-Term Memory
(LSTM) networks that decoupled the motion prediction and gait classification
tasks to anticipate falls with high accuracy. Our approach employs real-time
skeletal features extracted from video sequences as input for the proposed
model. The DGNN acts as a classifier, distinguishing between three gait states:
stable, transient, and fall. The LSTM-based network then predicts human
movement in subsequent time steps, enabling early detection of falls. The
proposed model was trained and validated using the OUMVLP-Pose and URFD
datasets, demonstrating superior performance in terms of prediction error and
recognition accuracy compared to models relying solely on DGNN and models from
literature. The results indicate that decoupling prediction and classification
improves performance compared to addressing the unified problem using only the
DGNN. Furthermore, our method allows for the monitoring of the transient state,
offering valuable insights that could enhance the functionality of advanced
assistance systems.

</details>


### [31] [Comparative Evaluation of Hard and Soft Clustering for Precise Brain Tumor Segmentation in MR Imaging](https://arxiv.org/abs/2509.05340)
*Dibya Jyoti Bora,Mrinal Kanti Mishra*

Main category: cs.CV

> The analysis compared K-Means and FCM for brain MRI tumor segmentation, finding that FCM improves accuracy while K-Means increases speed.

<details>
  <summary>Details</summary>

**Motivation:** Accurate segmentation of brain tumors from MRI is crucial for clinical decision-making, treatment planning, and disease monitoring. The heterogeneity of tumor morphology and intensity complicates this task.

**Method:** The study compares hard clustering (K-Means) and soft clustering (Fuzzy C-Means, FCM) in brain MRI tumor segmentation using the BraTS2020 dataset. The analysis incorporates image pre-processing techniques like Gaussian filtering and CLAHE.

**Result:** K-Means showed faster processing with an average runtime of 0.3s per image, and FCM yielded higher segmentation accuracy with an average DSC of 0.67 versus 0.43 for K-Means, at the cost of longer processing time (1.3s per image).

**Conclusion:** The trade-off between computational efficiency and segmentation accuracy in MRI tumor segmentation is evident, with K-Means and FCM each prioritizing different aspects of performance.

**Abstract:** Segmentation of brain tumors from Magnetic Resonance Imaging (MRI) remains a
pivotal challenge in medical image analysis due to the heterogeneous nature of
tumor morphology and intensity distributions. Accurate delineation of tumor
boundaries is critical for clinical decision-making, radiotherapy planning, and
longitudinal disease monitoring. In this study, we perform a comprehensive
comparative analysis of two major clustering paradigms applied in MRI tumor
segmentation: hard clustering, exemplified by the K-Means algorithm, and soft
clustering, represented by Fuzzy C-Means (FCM). While K-Means assigns each
pixel strictly to a single cluster, FCM introduces partial memberships, meaning
each pixel can belong to multiple clusters with varying degrees of association.
Experimental validation was performed using the BraTS2020 dataset,
incorporating pre-processing through Gaussian filtering and Contrast Limited
Adaptive Histogram Equalization (CLAHE). Evaluation metrics included the Dice
Similarity Coefficient (DSC) and processing time, which collectively
demonstrated that K-Means achieved superior speed with an average runtime of
0.3s per image, whereas FCM attained higher segmentation accuracy with an
average DSC of 0.67 compared to 0.43 for K-Means, albeit at a higher
computational cost (1.3s per image). These results highlight the inherent
trade-off between computational efficiency and boundary precision.

</details>


### [32] [Handling imbalance and few-sample size in ML based Onion disease classification](https://arxiv.org/abs/2509.05341)
*Abhijeet Manoj Pal,Rajbabu Velmurugan*

Main category: cs.CV

> 为了提高精准农业中病虫害分类的准确性，我们提出了一种基于深度学习的多类分类模型，该模型优于现有的二分类方法，并在实际数据集中取得了高准确率。

<details>
  <summary>Details</summary>

**Motivation:** 现有的病虫害分类方法主要集中在二分类上，这限制了它们在实际中的应用，特别是在需要准确识别具体病虫害种类的情况下。准确的病虫害分类对于精确农业中有效的识别、针对性干预以及防止其进一步传播至关重要。

**Method:** 提出了一种基于深度学习的多类分类模型，用于洋葱病虫害的分类。该模型通过对预训练的卷积神经网络(CNN)进行改进，集成注意力模块，并使用全面的数据增强管道来缓解类别不平衡的问题。

**Result:** 该模型在真实田间图像数据集上的整体准确率为96.90%，F1得分为0.96，优于使用相同数据集的其他方法。

**Conclusion:** 提出的模型在真实田间图像数据集中达到了很高的准确性和F1得分，证明了它在实际应用中的有效性。

**Abstract:** Accurate classification of pests and diseases plays a vital role in precision
agriculture, enabling efficient identification, targeted interventions, and
preventing their further spread. However, current methods primarily focus on
binary classification, which limits their practical applications, especially in
scenarios where accurately identifying the specific type of disease or pest is
essential. We propose a robust deep learning based model for multi-class
classification of onion crop diseases and pests. We enhance a pre-trained
Convolutional Neural Network (CNN) model by integrating attention based modules
and employing comprehensive data augmentation pipeline to mitigate class
imbalance. We propose a model which gives 96.90% overall accuracy and 0.96 F1
score on real-world field image dataset. This model gives better results than
other approaches using the same datasets.

</details>


### [33] [Delta Velocity Rectified Flow for Text-to-Image Editing](https://arxiv.org/abs/2509.05342)
*Gaspard Beaudouin,Minghan Li,Jaeyeon Kim,Sunghoon Yoon,Mengyu Wang*

Main category: cs.CV

> 提出了一种名为Delta Velocity Rectified Flow (DVRF) 的新颖无逆编辑框架，旨在改善text-to-image编辑的质量。

<details>
  <summary>Details</summary>

**Motivation:** 解决先前方法中存在的过平滑问题，以提高文本到图像编辑任务的质量和控制精度。

**Method:** 通过建模源和目标速度场之间的差距以减轻过平滑问题，并引入时间依赖的偏移项来提高对目标分布的对齐精度。

**Result:** 实验表明，DVRF能够在无需架构修改的情况下，提供更强的编辑品质、保真度和可控性。

**Conclusion:** DVRF方法在text-to-image编辑任务中展示出了优越性，既高效又广泛应用。

**Abstract:** We propose Delta Velocity Rectified Flow (DVRF), a novel inversion-free,
path-aware editing framework within rectified flow models for text-to-image
editing. DVRF is a distillation-based method that explicitly models the
discrepancy between the source and target velocity fields in order to mitigate
over-smoothing artifacts rampant in prior distillation sampling approaches. We
further introduce a time-dependent shift term to push noisy latents closer to
the target trajectory, enhancing the alignment with the target distribution. We
theoretically demonstrate that when this shift is disabled, DVRF reduces to
Delta Denoising Score, thereby bridging score-based diffusion optimization and
velocity-based rectified-flow optimization. Moreover, when the shift term
follows a linear schedule under rectified-flow dynamics, DVRF generalizes the
Inversion-free method FlowEdit and provides a principled theoretical
interpretation for it. Experimental results indicate that DVRF achieves
superior editing quality, fidelity, and controllability while requiring no
architectural modifications, making it efficient and broadly applicable to
text-to-image editing tasks. Code is available at
https://github.com/gaspardbd/DeltaVelocityRectifiedFlow.

</details>


### [34] [Systematic Integration of Attention Modules into CNNs for Accurate and Generalizable Medical Image Diagnosis](https://arxiv.org/abs/2509.05343)
*Zahid Ullah,Minki Hong,Tahir Mahmood,Jihie Kim*

Main category: cs.CV

> 本研究将在五种流行卷积神经网络架构中集成注意力机制，以提高对医学影像中关键区域的关注度和判别表现，并通过两个不同的医学影像数据集验证其有效性。研究表明，注意力增强的CNN显著提升了分类准确度和特征定位能力。

<details>
  <summary>Details</summary>

**Motivation:** 传统的CNN在医学影像分析中难以捕捉细微和复杂的特征，因此研究中引入注意力机制以解决这一问题，提高模型对关键区域的关注及诊断准确性。

**Method:** 研究系统性地将挤压激励模块或混合卷积块注意力模块集成到VGG16、ResNet18、InceptionV3、DenseNet121和EfficientNetB5等五种网络架构中，通过自适应调整通道和空间特征表示来增强模型性能。

**Result:** 实验结果在两个医学影像数据集上验证了注意力增强的CNN普遍优于原始架构，特别是EfficientNetB5结合混合注意力模块在两个数据集上都表现出色。

**Conclusion:** 研究搭建了一个系统的比较框架，评估了在多种CNN架构中嵌入注意力模块的影响，为开发健壮、可解释、临床上实用的基于深度学习的决策支持系统提供了实用见解。

**Abstract:** Deep learning has become a powerful tool for medical image analysis; however,
conventional Convolutional Neural Networks (CNNs) often fail to capture the
fine-grained and complex features critical for accurate diagnosis. To address
this limitation, we systematically integrate attention mechanisms into five
widely adopted CNN architectures, namely, VGG16, ResNet18, InceptionV3,
DenseNet121, and EfficientNetB5, to enhance their ability to focus on salient
regions and improve discriminative performance. Specifically, each baseline
model is augmented with either a Squeeze and Excitation block or a hybrid
Convolutional Block Attention Module, allowing adaptive recalibration of
channel and spatial feature representations. The proposed models are evaluated
on two distinct medical imaging datasets, a brain tumor MRI dataset comprising
multiple tumor subtypes, and a Products of Conception histopathological dataset
containing four tissue categories. Experimental results demonstrate that
attention augmented CNNs consistently outperform baseline architectures across
all metrics. In particular, EfficientNetB5 with hybrid attention achieves the
highest overall performance, delivering substantial gains on both datasets.
Beyond improved classification accuracy, attention mechanisms enhance feature
localization, leading to better generalization across heterogeneous imaging
modalities. This work contributes a systematic comparative framework for
embedding attention modules in diverse CNN architectures and rigorously
assesses their impact across multiple medical imaging tasks. The findings
provide practical insights for the development of robust, interpretable, and
clinically applicable deep learning based decision support systems.

</details>


### [35] [Vision-Based Object Detection for UAV Solar Panel Inspection Using an Enhanced Defects Dataset](https://arxiv.org/abs/2509.05348)
*Ashen Rodrigo,Isuru Munasinghe,Asanka Perera*

Main category: cs.CV

> 本研究评估了五种目标检测模型，在一个专为太阳能板缺陷和污染检测设计的数据集上进行训练和评估，展示了各模型在检测精度和计算效率上的权衡。

<details>
  <summary>Details</summary>

**Motivation:** 及时且准确地检测太阳能板上的缺陷和污染物，对维护光伏系统的效率和可靠性至关重要。

**Method:** 本研究对五种最先进的目标检测模型（YOLOv3、Faster R-CNN、RetinaNet、EfficientDet、Swin Transformer）进行了全面评估，用于识别太阳能板上的物理和电气缺陷及表面污染物（如尘土、污垢和鸟粪）。为此开发了一个以COCO格式标注的自定义数据集，以及用于训练和评估模型的用户界面。

**Result:** 研究结果展示了检测精度与计算效率之间的权衡，突出了每个模型的相对优势与限制。

**Conclusion:** 这些结果为实际的太阳能板监控和维护场景中选择合适的检测方法提供了有价值的指导。

**Abstract:** Timely and accurate detection of defects and contaminants in solar panels is
critical for maintaining the efficiency and reliability of photovoltaic
systems. This study presents a comprehensive evaluation of five
state-of-the-art object detection models: YOLOv3, Faster R-CNN, RetinaNet,
EfficientDet, and Swin Transformer, for identifying physical and electrical
defects as well as surface contaminants such as dust, dirt, and bird droppings
on solar panels. A custom dataset, annotated in the COCO format and
specifically designed for solar panel defect and contamination detection, was
developed alongside a user interface to train and evaluate the models. The
performance of each model is assessed and compared based on mean Average
Precision (mAP), precision, recall, and inference speed. The results
demonstrate the trade-offs between detection accuracy and computational
efficiency, highlighting the relative strengths and limitations of each model.
These findings provide valuable guidance for selecting appropriate detection
approaches in practical solar panel monitoring and maintenance scenarios.
  The dataset will be publicly available at
https://github.com/IsuruMunasinghe98/solar-panel-inspection-dataset.

</details>


### [36] [Unsupervised Instance Segmentation with Superpixels](https://arxiv.org/abs/2509.05352)
*Cuong Manh Hoang*

Main category: cs.CV

> 本文介绍了一种新型框架，它通过自监督的方法替代手工标注实现了高效的实例分割。实验表明，新方法在多个数据集上比现有的最先进方法表现更优。

<details>
  <summary>Details</summary>

**Motivation:** 由于现有的实例分割模型需要大量的手工标注数据，这些数据成本高昂。本文旨在提供一个不需要手工标注就可以有效地进行实例分割的新方案。

**Method:** 本文提出了一种新的框架，首先使用MultiCut算法对自监督特征进行粗糙的掩码分割，然后通过掩码过滤器获得高质量的粗糙掩码。为了训练分割网络，提出了一个新颖的超像素引导的掩码损失，包括硬损失和软损失。最后，引入了一个带有自适应损失的自训练过程来提高预测掩码的质量。

**Result:** 实验结果显示，本文提出的方法在公共数据集上取得了比现有最先进方法更好的效果。

**Conclusion:** 本文提出的新框架展示了在实例分割和目标检测上无需大量的手工标注就能达到有效的分割效果。

**Abstract:** Instance segmentation is essential for numerous computer vision applications,
including robotics, human-computer interaction, and autonomous driving.
Currently, popular models bring impressive performance in instance segmentation
by training with a large number of human annotations, which are costly to
collect. For this reason, we present a new framework that efficiently and
effectively segments objects without the need for human annotations. Firstly, a
MultiCut algorithm is applied to self-supervised features for coarse mask
segmentation. Then, a mask filter is employed to obtain high-quality coarse
masks. To train the segmentation network, we compute a novel superpixel-guided
mask loss, comprising hard loss and soft loss, with high-quality coarse masks
and superpixels segmented from low-level image features. Lastly, a
self-training process with a new adaptive loss is proposed to improve the
quality of predicted masks. We conduct experiments on public datasets in
instance segmentation and object detection to demonstrate the effectiveness of
the proposed framework. The results show that the proposed framework
outperforms previous state-of-the-art methods.

</details>


### [37] [Augmented Structure Preserving Neural Networks for cell biomechanics](https://arxiv.org/abs/2509.05388)
*Juan Olalla-Pombo,Alberto Badías,Miguel Ángel Sanz-Gómez,José María Benítez,Francisco Javier Montáns*

Main category: cs.CV

> 研究提出一种新方法，结合结构保留神经网络和其他机器学习工具，用于准确预测细胞轨迹和有丝分裂事件。

<details>
  <summary>Details</summary>

**Motivation:** 鉴于细胞生物力学现象的重要性，尽管有许多研究致力于理解这些现象，但它们之间的许多互动及对细胞集体网络或集群决策的影响仍旧模糊不清，因此提出了这一新方法。

**Method:** 结合结构保留神经网络研究细胞运动作为一种纯粹的机械系统，并使用计算机视觉技术从实验中直接推断出环境因素，与其他机器学习工具（人工神经网络）结合。

**Result:** 在模拟和真实细胞迁移案例中，该新模型能够以高精度预测完整的细胞轨迹，并包含了一个基于神经网络架构的有丝分裂事件预测模型。

**Conclusion:** 该研究为理解和预测细胞行为提供了一种创新的方法，可能在生物医学领域有广泛的应用。

**Abstract:** Cell biomechanics involve a great number of complex phenomena that are
fundamental to the evolution of life itself and other associated processes,
ranging from the very early stages of embryo-genesis to the maintenance of
damaged structures or the growth of tumors. Given the importance of such
phenomena, increasing research has been dedicated to their understanding, but
the many interactions between them and their influence on the decisions of
cells as a collective network or cluster remain unclear. We present a new
approach that combines Structure Preserving Neural Networks, which study cell
movements as a purely mechanical system, with other Machine Learning tools
(Artificial Neural Networks), which allow taking into consideration
environmental factors that can be directly deduced from an experiment with
Computer Vision techniques. This new model, tested on simulated and real cell
migration cases, predicts complete cell trajectories following a roll-out
policy with a high level of accuracy. This work also includes a mitosis event
prediction model based on Neural Networks architectures which makes use of the
same observed features.

</details>
