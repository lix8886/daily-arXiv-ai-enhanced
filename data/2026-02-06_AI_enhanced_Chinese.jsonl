{"id": "2602.04982", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.04982", "abs": "https://arxiv.org/abs/2602.04982", "authors": ["Deepak Gupta", "Davis Bartels", "Dina Demner-Fuhsman"], "title": "BioACE: An Automated Framework for Biomedical Answer and Citation Evaluations", "comment": "Work in progress", "summary": "With the increasing use of large language models (LLMs) for generating answers to biomedical questions, it is crucial to evaluate the quality of the generated answers and the references provided to support the facts in the generated answers. Evaluation of text generated by LLMs remains a challenge for question answering, retrieval-augmented generation (RAG), summarization, and many other natural language processing tasks in the biomedical domain, due to the requirements of expert assessment to verify consistency with the scientific literature and complex medical terminology. In this work, we propose BioACE, an automated framework for evaluating biomedical answers and citations against the facts stated in the answers. The proposed BioACE framework considers multiple aspects, including completeness, correctness, precision, and recall, in relation to the ground-truth nuggets for answer evaluation. We developed automated approaches to evaluate each of the aforementioned aspects and performed extensive experiments to assess and analyze their correlation with human evaluations. In addition, we considered multiple existing approaches, such as natural language inference (NLI) and pre-trained language models and LLMs, to evaluate the quality of evidence provided to support the generated answers in the form of citations into biomedical literature. With the detailed experiments and analysis, we provide the best approaches for biomedical answer and citation evaluation as a part of BioACE (https://github.com/deepaknlp/BioACE) evaluation package.", "AI": {"tldr": "本研究介绍了一种名为BioACE的自动化评价框架，用于评估LLMs在生物医学问答生成的答案和引用的质量。通过多种实验和分析，提供了用于生物医学答案和引用评估的最佳方法。", "motivation": "由于大型语言模型（LLMs）在生成生物医学问题答案中的使用日益增多，评估这些生成答案的质量及其引用变得越来越重要。这因需要专家来验证文本的一致性以及医学术语的复杂性而变得具有挑战性。", "method": "本研究提出了一个名为BioACE的自动化框架，用于评估生物医学领域生成的答案和引用的准确性。该框架考虑了多个方面，包括答案的完整性、正确性、精确性和召回率，并开发了自动化方法来评估这些方面。此外，还考虑了现有的方法，如自然语言推理（NLI）和预训练语言模型，来评估支持答案的引用证据的质量。", "result": "通过详细的实验和分析，研究提供了在BioACE框架中评估生物医学答案和引用的最佳方法。", "conclusion": "该研究为评估生物医学领域生成的答案和引用的质量提供了一个自动化框架，该框架考虑了多个方面并提供了最佳实践方法。"}}
{"id": "2602.05004", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.05004", "abs": "https://arxiv.org/abs/2602.05004", "authors": ["Zexin Lin", "Jiachen Yu", "Haoyang Zhang", "Yuzhao Li", "Zhonghang Li", "Yujiu Yang", "Junjie Wang", "Xiaoqiang Ji"], "title": "CoWork-X: Experience-Optimized Co-Evolution for Multi-Agent Collaboration System", "comment": null, "summary": "Large language models are enabling language-conditioned agents in interactive environments, but highly cooperative tasks often impose two simultaneous constraints: sub-second real-time coordination and sustained multi-episode adaptation under a strict online token budget. Existing approaches either rely on frequent in-episode reasoning that induces latency and timing jitter, or deliver post-episode improvements through unstructured text that is difficult to compile into reliable low-cost execution. We propose CoWork-X, an active co-evolution framework that casts peer collaboration as a closed-loop optimization problem across episodes, inspired by fast--slow memory separation. CoWork-X instantiates a Skill-Agent that executes via HTN (hierarchical task network)-based skill retrieval from a structured, interpretable, and compositional skill library, and a post-episode Co-Optimizer that performs patch-style skill consolidation with explicit budget constraints and drift regularization. Experiments in challenging Overcooked-AI-like realtime collaboration benchmarks demonstrate that CoWork-X achieves stable, cumulative performance gains while steadily reducing online latency and token usage.", "AI": {"tldr": "提出了CoWork-X框架，解决语言模型在实时协作任务中的在线推理延迟和适应性问题，实现了稳定性能提升和资源节约。", "motivation": "现有的方法要么依赖频繁的任务中推理导致延迟和时间抖动，要么通过非结构化文本提供任务后改进，难以转化为可靠的低成本执行。我们旨在解决实时协调与多任务适应之间的冲突。", "method": "我们提出了一种名为CoWork-X的主动共进化框架，该框架将同伴协作视为跨时间跨度的闭环优化问题，灵感源自快速-慢速记忆分离。CoWork-X实现了一个通过从结构化、可解释且组合技能库中检索技能的HTN（层次任务网络）技能代理，并在每个任务后进行显式预算约束和漂移正则化的技能巩固的共同优化器。", "result": "实验显示，CoWork-X在具有挑战性的实时协作基准测试中实现了稳定、累积的性能提升，同时逐步减少在线延迟和令牌使用量。", "conclusion": "实验结果表明，CoWork-X框架有效解决了实时协作任务中存在的时间延迟问题，并且提高了任务性能。这种方法对构建高效协作代理具有潜在的应用价值。"}}
{"id": "2602.05035", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.05035", "abs": "https://arxiv.org/abs/2602.05035", "authors": ["Sean Trott", "Pamela D. Rivière"], "title": "Capacity Constraints and the Multilingual Penalty for Lexical Disambiguation", "comment": "9 pages, 5 figures, conference", "summary": "Multilingual language models (LMs) sometimes under-perform their monolingual counterparts, possibly due to capacity limitations. We quantify this ``multilingual penalty'' for lexical disambiguation--a task requiring precise semantic representations and contextualization mechanisms--using controlled datasets of human relatedness judgments for ambiguous words in both English and Spanish. Comparing monolingual and multilingual LMs from the same families, we find consistently reduced performance in multilingual LMs. We then explore three potential capacity constraints: representational (reduced embedding isotropy), attentional (reduced attention to disambiguating cues), and vocabulary-related (increased multi-token segmentation). Multilingual LMs show some evidence of all three limitations; moreover, these factors statistically account for the variance formerly attributed to a model's multilingual status. These findings suggest both that multilingual LMs do suffer from multiple capacity constraints, and that these constraints correlate with reduced disambiguation performance.", "AI": {"tldr": "研究发现多语言语言模型在词汇消歧任务中性能较低，可能是因为其容量限制，具体表现为表示、注意和词汇相关的约束，并统计分析了这些因素对性能的影响。", "motivation": "研究旨在探索多语言模型在像词汇消歧这类需要精确语义表征和上下文化机制的任务上的表现是否低于单语模型，以及其背后导致该现象的具体原因。", "method": "通过对比同系列的单语模型和多语模型在英语和西班牙语的人类语义相似度判断数据集上的表现，来量化所谓的“多语损失”，并通过分析表示、注意力和词汇三个方面的约束来探索可能的原因。", "result": "多语模型在词汇消歧表现上低于单语模型，且表现出一定的表示性（嵌入等轴性减弱）、注意性（对消歧提示词注意减少）和词汇性（增大多令牌分割）约束。这些因素可以解释模型多语状态导致的差异显著性。", "conclusion": "研究结果说明了多语模型在词汇消歧上性能下降的问题，主要原因在于多语言模型面临多个容量约束，并且这些约束与词汇消歧性能表现不良之间存在相关性。"}}
{"id": "2602.05085", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.05085", "abs": "https://arxiv.org/abs/2602.05085", "authors": ["Sidi Lu", "Zhenwen Liang", "Dongyang Ma", "Yan Wang", "Haitao Mi", "Dong Yu"], "title": "Locas: Your Models are Principled Initializers of Locally-Supported Parametric Memories", "comment": "Tencent AI Lab Technical Report", "summary": "In this paper, we aim to bridge test-time-training with a new type of parametric memory that can be flexibly offloaded from or merged into model parameters. We present Locas, a Locally-Supported parametric memory that shares the design of FFN blocks in modern transformers, allowing it to be flexibly permanentized into the model parameters while supporting efficient continual learning. We discuss two major variants of Locas: one with a conventional two-layer MLP design that has a clearer theoretical guarantee; the other one shares the same GLU-FFN structure with SOTA LLMs, and can be easily attached to existing models for both parameter-efficient and computation-efficient continual learning. Crucially, we show that proper initialization of such low-rank sideway-FFN-style memories -- performed in a principled way by reusing model parameters, activations and/or gradients -- is essential for fast convergence, improved generalization, and catastrophic forgetting prevention. We validate the proposed memory mechanism on the PG-19 whole-book language modeling and LoCoMo long-context dialogue question answering tasks. With only 0.02\\% additional parameters in the lowest case, Locas-GLU is capable of storing the information from past context while maintaining a much smaller context window. In addition, we also test the model's general capability loss after memorizing the whole book with Locas, through comparative MMLU evaluation. Results show the promising ability of Locas to permanentize past context into parametric knowledge with minimized catastrophic forgetting of the model's existing internal knowledge.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2602.04994", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.04994", "abs": "https://arxiv.org/abs/2602.04994", "authors": ["Zhuosen Bao", "Xia Du", "Zheng Lin", "Jizhe Zhou", "Zihan Fang", "Jiening Wu", "Yuxin Zhang", "Zhe Chen", "Chi-man Pun", "Wei Ni", "Jun Luo"], "title": "SIDeR: Semantic Identity Decoupling for Unrestricted Face Privacy", "comment": "14 pages, 8 figures", "summary": "With the deep integration of facial recognition into online banking, identity verification, and other networked services, achieving effective decoupling of identity information from visual representations during image storage and transmission has become a critical challenge for privacy protection. To address this issue, we propose SIDeR, a Semantic decoupling-driven framework for unrestricted face privacy protection. SIDeR decomposes a facial image into a machine-recognizable identity feature vector and a visually perceptible semantic appearance component. By leveraging semantic-guided recomposition in the latent space of a diffusion model, it generates visually anonymous adversarial faces while maintaining machine-level identity consistency. The framework incorporates momentum-driven unrestricted perturbation optimization and a semantic-visual balancing factor to synthesize multiple visually diverse, highly natural adversarial samples. Furthermore, for authorized access, the protected image can be restored to its original form when the correct password is provided. Extensive experiments on the CelebA-HQ and FFHQ datasets demonstrate that SIDeR achieves a 99% attack success rate in black-box scenarios and outperforms baseline methods by 41.28% in PSNR-based restoration quality.", "AI": {"tldr": "SIDeR提出了一种面部隐私保护框架，该框架能够在图像存储和传输过程中解耦身份信息和视觉表示，同时保持身份特征的机器识别一致性，并在提供正确密码时恢复图像的原貌。", "motivation": "针对面部识别技术在在线银行、身份验证和其他网络服务中深入整合所带来的身份信息与视觉表示有效解耦的隐私保护挑战，提出SIDeR框架，以实现无限制的面部隐私保护。", "method": "SIDeR框架通过将面部图像分解为机器可识别的身份特征向量和可视感知的语义外观组件，利用语义引导的重组成分在扩散模型的潜在空间中生成视觉匿名的对抗性面孔，同时保持机器级别的身份一致性。框架中引入了动量驱动的无约束扰动优化和语义-视觉平衡因子，以合成多样的、高度自然的对抗样本。此外，当提供正确的密码时，保护后的图像可以恢复到其原始形式。", "result": "在CelebA-HQ和FFHQ数据集上的广泛实验表明，SIDeR在黑盒场景中实现了99%的攻击成功率，并且在基于PSNR的图像恢复质量上相对于基线方法提高了41.28%。", "conclusion": "SIDeR通过创新的方法提供了一种有效的无限制面部隐私保护策略，能够生成高自然度的对抗样本，并在必要时恢复图像的原始面貌，为隐私保护提供了新的解决方案。"}}
{"id": "2602.05106", "categories": ["cs.CL", "cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.05106", "abs": "https://arxiv.org/abs/2602.05106", "authors": ["Michael Browder", "Kevin Duh", "J. David Harris", "Vince Lyzinski", "Paul McNamee", "Youngser Park", "Carey E. Priebe", "Peter Viechnicki"], "title": "Data Kernel Perspective Space Performance Guarantees for Synthetic Data from Transformer Models", "comment": null, "summary": "Scarcity of labeled training data remains the long pole in the tent for building performant language technology and generative AI models. Transformer models -- particularly LLMs -- are increasingly being used to mitigate the data scarcity problem via synthetic data generation. However, because the models are black boxes, the properties of the synthetic data are difficult to predict. In practice it is common for language technology engineers to 'fiddle' with the LLM temperature setting and hope that what comes out the other end improves the downstream model. Faced with this uncertainty, here we propose Data Kernel Perspective Space (DKPS) to provide the foundation for mathematical analysis yielding concrete statistical guarantees for the quality of the outputs of transformer models. We first show the mathematical derivation of DKPS and how it provides performance guarantees. Next we show how DKPS performance guarantees can elucidate performance of a downstream task, such as neural machine translation models or LLMs trained using Contrastive Preference Optimization (CPO). Limitations of the current work and future research are also discussed.", "AI": {"tldr": "本研究提出了DKPS用以提供变换器模型输出质量的数学分析和统计保证，从而解决因标注数据缺乏而导致的模型性能瓶颈。", "motivation": "由于标注训练数据的匮乏仍然是构建高性能语言技术和生成式AI模型的主要障碍，而变换器模型特别是大型语言模型（LLMs）正被越来越多地用于生成合成数据以缓解这一问题。然而，因为模型的黑箱性质，合成数据的性质难以预测。因此，本研究旨在通过DKPS提供一种解决方案，以数学分析的方式提供对输出质量的统计保证，从而解决这一问题。", "method": "本研究提出了数据内核透视空间（DKPS），用以提供对变换器模型输出质量进行数学分析和统计保证的基础。研究首先展示了DKPS的数学推导及其如何提供性能保证，并进一步探讨了DKPS性能保证如何阐明下游任务的性能，比如神经机器翻译模型或是使用对比偏好优化（CPO）训练的大规模语言模型（LLM）的性能。", "result": "研究展示了DKPS的数学推导，及其性能保证如何应用于阐明神经机器翻译或使用对比偏好优化训练的语言模型的性能提升。", "conclusion": "尽管研究指出了DKPS提供数学分析和统计保证的能力，但也讨论了当前工作的一些局限性及未来的研究方向。"}}
{"id": "2602.05037", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.05037", "abs": "https://arxiv.org/abs/2602.05037", "authors": ["Bishoy Galoaa", "Xiangyu Bai", "Utsav Nandi", "Sai Siddhartha Vivek Dhir Rangoju", "Somaieh Amraee", "Sarah Ostadabbas"], "title": "UniTrack: Differentiable Graph Representation Learning for Multi-Object Tracking", "comment": null, "summary": "We present UniTrack, a plug-and-play graph-theoretic loss function designed to significantly enhance multi-object tracking (MOT) performance by directly optimizing tracking-specific objectives through unified differentiable learning. Unlike prior graph-based MOT methods that redesign tracking architectures, UniTrack provides a universal training objective that integrates detection accuracy, identity preservation, and spatiotemporal consistency into a single end-to-end trainable loss function, enabling seamless integration with existing MOT systems without architectural modifications. Through differentiable graph representation learning, UniTrack enables networks to learn holistic representations of motion continuity and identity relationships across frames. We validate UniTrack across diverse tracking models and multiple challenging benchmarks, demonstrating consistent improvements across all tested architectures and datasets including Trackformer, MOTR, FairMOT, ByteTrack, GTR, and MOTE. Extensive evaluations show up to 53\\% reduction in identity switches and 12\\% IDF1 improvements across challenging benchmarks, with GTR achieving peak performance gains of 9.7\\% MOTA on SportsMOT.", "AI": {"tldr": "UniTrack，一种图论损失函数，提升了多对象跟踪的性能，证明了其在多种模型和基准测试上的有效性。", "motivation": "不同于以前需要重新设计跟踪架构的基于图的MOT方法，UniTrack 提供一个通用的训练目标，可以无缝集成到现有的MOT系统中。", "method": "UniTrack 是一个即插即用的图论损失函数，它通过统一的可微学习直接优化多对象跟踪（MOT）中的跟踪特定目标，提升性能。", "result": "在多种跟踪模型和具有挑战性的基准测试中验证了UniTrack，显示了在所有测试架构和数据集上的持续改进。", "conclusion": "UniTrack 在减少身份切换和提高IDF1等方面显示出显著改进，尤其是GTR在SportsMOT上达到了9.7%的MOTA性能提升。"}}
{"id": "2602.05107", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.05107", "abs": "https://arxiv.org/abs/2602.05107", "authors": ["Ahmed Ruby", "Christian Hardmeier", "Sara Stymne"], "title": "Multilingual Extraction and Recognition of Implicit Discourse Relations in Speech and Text", "comment": null, "summary": "Implicit discourse relation classification is a challenging task, as it requires inferring meaning from context. While contextual cues can be distributed across modalities and vary across languages, they are not always captured by text alone. To address this, we introduce an automatic method for distantly related and unrelated language pairs to construct a multilingual and multimodal dataset for implicit discourse relations in English, French, and Spanish. For classification, we propose a multimodal approach that integrates textual and acoustic information through Qwen2-Audio, allowing joint modeling of text and audio for implicit discourse relation classification across languages. We find that while text-based models outperform audio-based models, integrating both modalities can enhance performance, and cross-lingual transfer can provide substantial improvements for low-resource languages.", "AI": {"tldr": "本文提出了一种基于Qwen2-Audio的多模态方法，用于隐式话语关系的跨语言分类，验证了结合文本和音频数据以及跨语言转移的好处。", "motivation": "隐式话语关系分类是一个具有挑战性的任务，因为它需要从上下文中推断意义。文本本身并不总是能够捕捉到上下文线索，这些线索可能分布在多模态和语言之间。为此，我们开发了一种新的自动方法来构建英语、法语和西班牙语的多语言、多模态数据集。", "method": "我们提出了一种多模态方法，通过Qwen2-Audio结合文本和声学信息，实现跨语言的隐式话语关系分类。", "result": "研究发现文本模型的性能超过了音频模型，但结合两种模态可以提升性能，并且跨语言转移对于低资源语言有很大的提升作用。", "conclusion": "通过整合文本和音频信息，我们的方法在跨语言的隐式话语关系分类任务上取得了显著的效果提升，特别是对于低资源语言，通过跨语言转移进一步提高了性能。"}}
{"id": "2602.05049", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.05049", "abs": "https://arxiv.org/abs/2602.05049", "authors": ["Yiye Chen", "Yanan Jian", "Xiaoyi Dong", "Shuxin Cao", "Jing Wu", "Patricio Vela", "Benjamin E. Lundell", "Dongdong Chen"], "title": "VISTA: Enhancing Visual Conditioning via Track-Following Preference Optimization in Vision-Language-Action Models", "comment": "In submission. Project website: https://vista-vla.github.io/", "summary": "Vision-Language-Action (VLA) models have demonstrated strong performance across a wide range of robotic manipulation tasks. Despite the success, extending large pretrained Vision-Language Models (VLMs) to the action space can induce vision-action misalignment, where action predictions exhibit weak dependence on the current visual state, leading to unreliable action outputs. In this work, we study VLA models through the lens of visual conditioning and empirically show that successful rollouts consistently exhibit stronger visual dependence than failed ones. Motivated by this observation, we propose a training framework that explicitly strengthens visual conditioning in VLA models. Our approach first aligns action prediction with visual input via preference optimization on a track-following surrogate task, and then transfers the enhanced alignment to instruction-following task through latent-space distillation during supervised finetuning. Without introducing architectural modifications or additional data collection, our method improves both visual conditioning and task performance for discrete OpenVLA, and further yields consistent gains when extended to the continuous OpenVLA-OFT setting. Project website: https://vista-vla.github.io/ .", "AI": {"tldr": "研究提出了一种改进视觉语言动作模型在执行任务时表现的方法，该方法在离散和连续任务上均取得了一致性性能提升。", "motivation": "针对大规模预训练视觉语言模型扩展至动作空间时出现的视觉-动作不对齐问题，提出一种明确加强视觉条件的训练框架。", "method": "通过视觉条件优化在跟随路径的代理任务上对动作预测与视觉输入进行对齐，并在监督微调过程中通过潜在空间蒸馏将增强的对齐转移到指令跟随任务中。", "result": "在不引入架构修改或额外数据收集的情况下，该方法提高了离散OpenVLA的视觉条件和任务性能，并在扩展至连续OpenVLA-OFT设定时表现一致增益。", "conclusion": "加强视觉条件训练框架对提升视觉语言动作模型在执行任务时的性能有效。"}}
{"id": "2602.05150", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.05150", "abs": "https://arxiv.org/abs/2602.05150", "authors": ["Yang Zhang", "Mersin Konomi", "Christos Xypolopoulos", "Konstantinos Divriotis", "Konstantinos Skianis", "Giannis Nikolentzos", "Giorgos Stamou", "Guokan Shang", "Michalis Vazirgiannis"], "title": "GreekMMLU: A Native-Sourced Multitask Benchmark for Evaluating Language Models in Greek", "comment": null, "summary": "Large Language Models (LLMs) are commonly trained on multilingual corpora that include Greek, yet reliable evaluation benchmarks for Greek-particularly those based on authentic, native-sourced content-remain limited. Existing datasets are often machine-translated from English, failing to capture Greek linguistic and cultural characteristics. We introduce GreekMMLU, a native-sourced benchmark for massive multitask language understanding in Greek, comprising 21,805 multiple-choice questions across 45 subject areas, organized under a newly defined subject taxonomy and annotated with educational difficulty levels spanning primary to professional examinations. All questions are sourced or authored in Greek from academic, professional, and governmental exams. We publicly release 16,857 samples and reserve 4,948 samples for a private leaderboard to enable robust and contamination-resistant evaluation. Evaluations of over 80 open- and closed-source LLMs reveal substantial performance gaps between frontier and open-weight models, as well as between Greek-adapted models and general multilingual ones. Finally, we provide a systematic analysis of factors influencing performance-including model scale, adaptation, and prompting-and derive insights for improving LLM capabilities in Greek.", "AI": {"tldr": "我们提出了GreekMMLU，这是一个用于评估大语言模型在希腊语领域的理解能力的基准，包含21,805个多选题，涵盖45个科目领域，揭示了不同模型的性能差距。", "motivation": "现有的希腊语数据集往往是从英语机器翻译而来，无法捕捉希腊语的语言和文化特性，因此需要一个可靠的评估标准。", "method": "我们介绍了GreekMMLU，这是一个基于希腊语本土内容的大规模多任务理解基准测试，包含来自45个科目领域的21,805个多选题，并且所有问题都直接用希腊语编写或创作。", "result": "对超过80个开源和闭源的大语言模型进行评估，发现前沿模型与开源模型之间、希腊适应性模型与通用多语言模型之间的性能存在显著差距。", "conclusion": "通过系统分析影响性能的因素，包括模型规模、适应性和提示设计，提供了提升大语言模型在希腊语表现的见解。"}}
{"id": "2602.05078", "categories": ["cs.CV", "cs.AI", "cs.MM", "eess.IV"], "pdf": "https://arxiv.org/pdf/2602.05078", "abs": "https://arxiv.org/abs/2602.05078", "authors": ["Gautham Vinod", "Fengqing Zhu"], "title": "Food Portion Estimation: From Pixels to Calories", "comment": null, "summary": "Reliance on images for dietary assessment is an important strategy to accurately and conveniently monitor an individual's health, making it a vital mechanism in the prevention and care of chronic diseases and obesity. However, image-based dietary assessment suffers from estimating the three dimensional size of food from 2D image inputs. Many strategies have been devised to overcome this critical limitation such as the use of auxiliary inputs like depth maps, multi-view inputs, or model-based approaches such as template matching. Deep learning also helps bridge the gap by either using monocular images or combinations of the image and the auxillary inputs to precisely predict the output portion from the image input. In this paper, we explore the different strategies employed for accurate portion estimation.", "AI": {"tldr": "本文探讨了用于准确估计食物份量的不同策略，解决了从2D图像中估计食物三维尺寸的难题。", "motivation": "依赖图像进行膳食评估是准确和方便监测个体健康状况的重要策略，对预防和治疗慢性疾病及肥胖具有重要意义。然而，从2D图像输入中估计食物的三维尺寸是一个重要挑战。", "method": "本研究探讨了用于准确估计食物份量的不同策略，包括使用深度图、多视角输入或基于模型的方法如模板匹配，以及通过深度学习利用单眼图像或图像与辅助输入的组合进行精确预测。", "result": "通过分析，确认利用深度图、多视角输入或基于模型的方法，以及深度学习技术，可以提高从2D图像中估计食物三维尺寸的准确度，从而改善饮食评估的精度。", "conclusion": "本研究总结了当前在食物份量准确估算上的多种解决方案，并讨论了深度学习技术在其中的应用及其对饮食评估的重要性。"}}
{"id": "2602.05176", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.05176", "abs": "https://arxiv.org/abs/2602.05176", "authors": ["Ziyuan Yang", "Wenxuan Ding", "Shangbin Feng", "Yulia Tsvetkov"], "title": "Among Us: Measuring and Mitigating Malicious Contributions in Model Collaboration Systems", "comment": "19 pages, 15 tables, 4 figures", "summary": "Language models (LMs) are increasingly used in collaboration: multiple LMs trained by different parties collaborate through routing systems, multi-agent debate, model merging, and more. Critical safety risks remain in this decentralized paradigm: what if some of the models in multi-LLM systems are compromised or malicious? We first quantify the impact of malicious models by engineering four categories of malicious LMs, plug them into four types of popular model collaboration systems, and evaluate the compromised system across 10 datasets. We find that malicious models have a severe impact on the multi-LLM systems, especially for reasoning and safety domains where performance is lowered by 7.12% and 7.94% on average. We then propose mitigation strategies to alleviate the impact of malicious components, by employing external supervisors that oversee model collaboration to disable/mask them out to reduce their influence. On average, these strategies recover 95.31% of the initial performance, while making model collaboration systems fully resistant to malicious models remains an open research question.", "AI": {"tldr": "本研究评估了恶意模型对多语言模型系统的影响，并提出了解决方案以减轻这些影响。", "motivation": "由于多语言模型系统在被不同方开发的语言模型之间协作方面变得越来越普遍，该研究旨在量化系统中被黑或恶意模型的影响，并提出减轻这些影响的策略。", "method": "研究中构建了四类恶意语言模型，并将其嵌入到四种流行的模型协作系统中，以评估这些受污染系统在10个数据集上的表现。", "result": "发现恶意模型对多语言模型系统产生了严重影响，特别是在推理和安全领域，性能平均下降了7.12%和7.94%。同时，通过使用外部监管者监控模型协作并禁用/屏蔽恶意组件，可以恢复大约95.31%的初始性能。", "conclusion": "尽管提出的策略能够有效减轻恶意模型的影响，但实现模型协作系统对恶意模型的完全防御仍然是一个公开的研究问题。"}}
{"id": "2602.05096", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.05096", "abs": "https://arxiv.org/abs/2602.05096", "authors": ["Joseph D. Janizek", "Sonnet Xu", "Junayd Lateef", "Roxana Daneshjou"], "title": "Visual concept ranking uncovers medical shortcuts used by large multimodal models", "comment": null, "summary": "Ensuring the reliability of machine learning models in safety-critical domains such as healthcare requires auditing methods that can uncover model shortcomings. We introduce a method for identifying important visual concepts within large multimodal models (LMMs) and use it to investigate the behaviors these models exhibit when prompted with medical tasks. We primarily focus on the task of classifying malignant skin lesions from clinical dermatology images, with supplemental experiments including both chest radiographs and natural images. After showing how LMMs display unexpected gaps in performance between different demographic subgroups when prompted with demonstrating examples, we apply our method, Visual Concept Ranking (VCR), to these models and prompts. VCR generates hypotheses related to different visual feature dependencies, which we are then able to validate with manual interventions.", "AI": {"tldr": "The paper introduces a method, VCR, for uncovering model shortcomings in large multimodal models on medical tasks and identifies unexpected performance gaps.", "motivation": "The motivation is to ensure the reliability of machine learning models in safety-critical domains, such as healthcare, by identifying model shortcomings.", "method": "Visual Concept Ranking (VCR) method is used to investigate behaviors of large multimodal models (LMMs) on medical tasks, such as classifying malignant skin lesions.", "result": "The LMMs display unexpected performance gaps between different demographic subgroups when prompted with demonstrating examples.", "conclusion": "VCR can generate hypotheses related to different visual feature dependencies, which can be validated with manual interventions, helping to improve the reliability of machine learning models in healthcare."}}
{"id": "2602.05182", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.05182", "abs": "https://arxiv.org/abs/2602.05182", "authors": ["Shangbin Feng", "Kishan Panaganti", "Yulia Tsvetkov", "Wenhao Yu"], "title": "The Single-Multi Evolution Loop for Self-Improving Model Collaboration Systems", "comment": "Code at https://github.com/BunsenFeng/moco_distill", "summary": "Model collaboration -- systems where multiple language models (LMs) collaborate -- combines the strengths of diverse models with cost in loading multiple LMs. We improve efficiency while preserving the strengths of collaboration by distilling collaborative patterns into a single model, where the model is trained on the outputs of the model collaboration system. At inference time, only the distilled model is employed: it imitates the collaboration while only incurring the cost of a single model. Furthermore, we propose the single-multi evolution loop: multiple LMs collaborate, each distills from the collaborative outputs, and these post-distillation improved LMs collaborate again, forming a collective evolution ecosystem where models evolve and self-improve by interacting with an environment of other models. Extensive experiments with 7 collaboration strategies and 15 tasks (QA, reasoning, factuality, etc.) demonstrate that: 1) individual models improve by 8.0% on average, absorbing the strengths of collaboration while reducing the cost to a single model; 2) the collaboration also benefits from the stronger and more synergistic LMs after distillation, improving over initial systems without evolution by 14.9% on average. Analysis reveals that the single-multi evolution loop outperforms various existing evolutionary AI methods, is compatible with diverse model/collaboration/distillation settings, and helps solve problems where the initial model/system struggles to.", "AI": {"tldr": "A study proposes an evolutionary distillation process called the single-multi evolution loop, which combines the strengths of multiple language models into a single, more efficient model without sacrificing the benefits of collaboration.", "motivation": "The motivation behind the research is to address the challenge of balancing the strengths of model collaboration and the associated costs, by employing a more efficient evolutionary distillation process that improves model collaboration without incurring the overhead of multiple models.", "method": "The paper proposes a method for improving efficiency in model collaboration among multiple language models by distilling the collaborative process into a single model. This distilled model is trained on outputs from the collaboration system, imitating collaboration while reducing costs to a single model. The single-multi evolution loop is introduced, a cycle where multiple models collaborate, distill improved versions of themselves, and repeat the cycle for iterative improvement.", "result": "Experiments with 7 collaboration strategies and 15 tasks demonstrate that individual models improve by 8.0% on average, and the collaborative system improves by 14.9% on average after evolution. The method outperforms existing evolutionary AI methods, is adaptable to various settings, and aids in solving complex problems.", "conclusion": "The single-multi evolution loop offers a solution for enhancing efficiency in model collaboration through evolutionary distillation, resulting in improved performance and adaptability."}}
{"id": "2602.05126", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.05126", "abs": "https://arxiv.org/abs/2602.05126", "authors": ["Weiyi Qin", "Yingci Liu-Swetz", "Shiwei Tan", "Hao Wang"], "title": "CLEAR-HPV: Interpretable Concept Discovery for HPV-Associated Morphology in Whole-Slide Histology", "comment": null, "summary": "Human papillomavirus (HPV) status is a critical determinant of prognosis and treatment response in head and neck and cervical cancers. Although attention-based multiple instance learning (MIL) achieves strong slide-level prediction for HPV-related whole-slide histopathology, it provides limited morphologic interpretability. To address this limitation, we introduce Concept-Level Explainable Attention-guided Representation for HPV (CLEAR-HPV), a framework that restructures the MIL latent space using attention to enable concept discovery without requiring concept labels during training. Operating in an attention-weighted latent space, CLEAR-HPV automatically discovers keratinizing, basaloid, and stromal morphologic concepts, generates spatial concept maps, and represents each slide using a compact concept-fraction vector. CLEAR-HPV's concept-fraction vectors preserve the predictive information of the original MIL embeddings while reducing the high-dimensional feature space (e.g., 1536 dimensions) to only 10 interpretable concepts. CLEAR-HPV generalizes consistently across TCGA-HNSCC, TCGA-CESC, and CPTAC-HNSCC, providing compact, concept-level interpretability through a general, backbone-agnostic framework for attention-based MIL models of whole-slide histopathology.", "AI": {"tldr": "The paper introduces CLEAR-HPV, a method using attention to discover interpretable morphologic concepts in HPV-related whole-slide histopathology.", "motivation": "To enhance interpretability of attention-based multiple instance learning (MIL) for HPV status prediction in histopathology, providing insights missing from standard MIL methods.", "method": "CLEAR-HPV restructures MIL latent space using attention to discover keratinizing, basaloid, and stromal concepts without needing labeled concepts, creating concept maps and compact concept-fraction vectors representative of the slide.", "result": "The method preserves predictive accuracy while reducing feature dimensions from 1536 to 10 interpretable concepts, showing consistent generalization across multiple datasets (TCGA-HNSCC, TCGA-CESC, CPTAC-HNSCC).", "conclusion": "CLEAR-HPV provides a generic way to enhance concept-level interpretability of attention-based MIL models in HPV-related cancer histopathology prediction."}}
{"id": "2602.05189", "categories": ["cs.CL", "cs.HC", "cs.LG", "cs.SI"], "pdf": "https://arxiv.org/pdf/2602.05189", "abs": "https://arxiv.org/abs/2602.05189", "authors": ["Hsuan-Yu Chou", "Wajiha Naveed", "Shuyan Zhou", "Xiaowei Yang"], "title": "Are Open-Weight LLMs Ready for Social Media Moderation? A Comparative Study on Bluesky", "comment": null, "summary": "As internet access expands, so does exposure to harmful content, increasing the need for effective moderation. Research has demonstrated that large language models (LLMs) can be effectively utilized for social media moderation tasks, including harmful content detection. While proprietary LLMs have been shown to zero-shot outperform traditional machine learning models, the out-of-the-box capability of open-weight LLMs remains an open question.\n  Motivated by recent developments of reasoning LLMs, we evaluate seven state-of-the-art models: four proprietary and three open-weight. Testing with real-world posts on Bluesky, moderation decisions by Bluesky Moderation Service, and annotations by two authors, we find a considerable degree of overlap between the sensitivity (81%--97%) and specificity (91%--100%) of the open-weight LLMs and those (72%--98%, and 93%--99%) of the proprietary ones. Additionally, our analysis reveals that specificity exceeds sensitivity for rudeness detection, but the opposite holds for intolerance and threats. Lastly, we identify inter-rater agreement across human moderators and the LLMs, highlighting considerations for deploying LLMs in both platform-scale and personalized moderation contexts. These findings show open-weight LLMs can support privacy-preserving moderation on consumer-grade hardware and suggest new directions for designing moderation systems that balance community values with individual user preferences.", "AI": {"tldr": "研究表明开源LLM在检测有害内容方面可以达到与专有LLM类似的表现，支持在保护隐私的情况下运用开源LLM进行社区内容审核，同时指出可以设计出既是平台级又可个性化的内容审核系统。", "motivation": "鉴于互联网上有害内容暴露增多以及现有专有LLM表现出的优势，研究动机在于评估开源LLM在有害内容检测方面的即用性能。", "method": "通过使用Bluesky上的真实帖子、BlueskyModerationService的审核决定以及两位作者的标注，对七个最先进的LLM模型（四个专有模型，三个开源模型）进行了评测。评测结果发现开源LLM在敏感度和特异度方面与专有模型相似。此外，发现对于不礼貌内容审核的特异度高于敏感度，而对不宽容和威胁检测则反之。在人类审核员与LLM之间也观察到评委的一致性。", "result": "{serialized_structure}", "conclusion": "开源LLM可以支持在保护隐私的前提下，在消费者级别硬件上进行内容审核，并指导新型审核系统的开发，这些系统能够平衡社区价值和个人用户偏好。"}}
{"id": "2602.05132", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.05132", "abs": "https://arxiv.org/abs/2602.05132", "authors": ["Jia Li", "Wenjie Zhao", "Shijian Deng", "Bolin Lai", "Yuheng Wu", "RUijia Chen", "Jon E. Froehlich", "Yuhang Zhao", "Yapeng Tian"], "title": "ARGaze: Autoregressive Transformers for Online Egocentric Gaze Estimation", "comment": null, "summary": "Online egocentric gaze estimation predicts where a camera wearer is looking from first-person video using only past and current frames, a task essential for augmented reality and assistive technologies. Unlike third-person gaze estimation, this setting lacks explicit head or eye signals, requiring models to infer current visual attention from sparse, indirect cues such as hand-object interactions and salient scene content. We observe that gaze exhibits strong temporal continuity during goal-directed activities: knowing where a person looked recently provides a powerful prior for predicting where they look next. Inspired by vision-conditioned autoregressive decoding in vision-language models, we propose ARGaze, which reformulates gaze estimation as sequential prediction: at each timestep, a transformer decoder predicts current gaze by conditioning on (i) current visual features and (ii) a fixed-length Gaze Context Window of recent gaze target estimates. This design enforces causality and enables bounded-resource streaming inference. We achieve state-of-the-art performance across multiple egocentric benchmarks under online evaluation, with extensive ablations validating that autoregressive modeling with bounded gaze history is critical for robust prediction. We will release our source code and pre-trained models.", "AI": {"tldr": "ARGaze方法通过序列预测，利用变压器解码器结合当前视觉特征和最近凝视目标的固定长度窗口预测第一人称视频中的视线位置，这种方法显著提升了在线自我中心凝视估计的性能。", "motivation": "自我中心凝视估计是一项从第一人称视角的视频中预测用户视线的技术，对于增强现实和辅助技术至关重要。与第三人称凝视估计相比，第一人称视角凝视估计缺乏明确的头部或眼部信号，需要从稀疏、间接的线索（如手-物体交互和显著场景内容）推断当前视觉注意力，ARGaze方法就是为了解决这个问题。", "method": "ARGaze方法通过将凝视估计重新定义为序列预测来解决第一人称视频中的在线自我中心凝视估计问题。在每个时间戳，变压器解码器根据当前视觉特征和最近凝视目标估计的固定长度窗口（Gaze Context Window）预测当前凝视。这种方法保证了因果性和有界资源流式推理。", "result": "ARGaze方法在多个自我中心基准测试下取得了在线评估的最佳性能，并通过广泛的消融分析验证了有界凝视历史的自回归建模对鲁棒预测至关重要。", "conclusion": "研究提出了ARGaze方法，这是一种利用自回归建模和有界凝视历史来改进第一人称视频流中的凝视估计的技术，该技术在多个基准测试中表现出色。我们将发布源代码和预训练模型。"}}
{"id": "2602.05205", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.05205", "abs": "https://arxiv.org/abs/2602.05205", "authors": ["Kenichiro Ando", "Tatsuya Harada"], "title": "Aligning Large Language Model Behavior with Human Citation Preferences", "comment": "Work In Progress", "summary": "Most services built on powerful large-scale language models (LLMs) add citations to their output to enhance credibility. Recent research has paid increasing attention to the question of what reference documents to link to outputs. However, how LLMs recognize cite-worthiness and how this process should be controlled remains underexplored. In this study, we focus on what kinds of content LLMs currently tend to cite and how well that behavior aligns with human preferences. We construct a dataset to characterize the relationship between human citation preferences and LLM behavior. Web-derived texts are categorized into eight citation-motivation types, and pairwise citation preferences are exhaustively evaluated across all type combinations to capture fine-grained contrasts. Our results show that humans most frequently seek citations for medical text, and stronger models display a similar tendency. We also find that current models are as much as $27\\%$ more likely than humans to add citations to text that is explicitly marked as needing citations on sources such as Wikipedia, and this overemphasis reduces alignment accuracy. Conversely, models systematically underselect numeric sentences (by $-22.6\\%$ relative to humans) and sentences containing personal names (by $-20.1\\%$), categories for which humans typically demand citations. Furthermore, experiments with Direct Preference Optimization demonstrate that model behavior can be calibrated to better match human citation preferences. We expect this study to provide a foundation for more fine-grained investigations into LLM citation preferences.", "AI": {"tldr": "研究探讨了大型语言模型（LLMs）引用行为及其与人类偏好的一致性，发现模型在某些类型的文本引用上存在偏差，并证明通过直接偏好优化可以调整模型行为以更好地匹配人类偏好。", "motivation": "近期研究关注大型语言模型（LLMs）引用文献的行为，但对模型识别引用的适宜性及其控制机制研究较少。研究旨在分析LLMs引用内容类型与人类偏好之间的关系，构建数据集进行对比研究。", "method": "研究构建了一个数据集，将网络来源的文本分为八类，并对所有类别的组合进行两两比较评估偏好。具体数值如：模型比人类多引用标注需要引用的文本27%，而少引用数字和人名的句子。", "result": "研究结果表明人类最常寻求引用的是医学文本，且更强的模型表现相似。当前模型过度引用已标注需引用的文本，而较少引用包含数字和人名的句子。", "conclusion": "通过实验，直接偏好优化能校准模型行为以更接近人类引用偏好。此研究为更深入探究LLMs引用偏好提供了基础。"}}
{"id": "2602.05159", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.05159", "abs": "https://arxiv.org/abs/2602.05159", "authors": ["Wenhui Cui", "Ziyi Kou", "Chuan Qin", "Ergys Ristani", "Li Guan"], "title": "AirGlove: Exploring Egocentric 3D Hand Tracking and Appearance Generalization for Sensing Gloves", "comment": "Accepted by ICASSP 2026", "summary": "Sensing gloves have become important tools for teleoperation and robotic policy learning as they are able to provide rich signals like speed, acceleration and tactile feedback. A common approach to track gloved hands is to directly use the sensor signals (e.g., angular velocity, gravity orientation) to estimate 3D hand poses. However, sensor-based tracking can be restrictive in practice as the accuracy is often impacted by sensor signal and calibration quality. Recent advances in vision-based approaches have achieved strong performance on human hands via large-scale pre-training, but their performance on gloved hands with distinct visual appearances remains underexplored. In this work, we present the first systematic evaluation of vision-based hand tracking models on gloved hands under both zero-shot and fine-tuning setups. Our analysis shows that existing bare-hand models suffer from substantial performance degradation on sensing gloves due to large appearance gap between bare-hand and glove designs. We therefore propose AirGlove, which leverages existing gloves to generalize the learned glove representations towards new gloves with limited data. Experiments with multiple sensing gloves show that AirGlove effectively generalizes the hand pose models to new glove designs and achieves a significant performance boost over the compared schemes.", "AI": {"tldr": "本研究介绍了AirGlove系统，该系统基于视觉的手势追踪模型，旨在提升穿戴传感手套时的性能，并展示在不同手套设计下有效推广的潜力。", "motivation": "鉴于传感器手套在遥操作和机器人政策学习中的重要性，目前基于传感器的手套追踪方法因其精确度受传感器信号质量影响而遭限制，本研究旨在探讨基于视觉的手掌追踪模型在佩戴手套情况下的性能表现。", "method": "Structure", "result": "{\n  \"tldr\": \"本研究介绍了AirGlove系统，该系统基于视觉的手势追踪模型，旨在提升穿戴传感手套时的性能，并展示在不同手套设计下有效推广的潜力。\", \n  \"motivation\": \"鉴于传感器手套在遥操作和机器人政策学习中的重要性，目前基于传感器的手套追踪方法因其精确度受传感器信号质量影响而遭限制，本研究旨在探讨基于视觉的手掌追踪模型在佩戴手套情况下的性能表现。\", \n  \"method\": \"研究通过零样本设定和微调设定下对现有裸手模型的系统性评估，提出AirGlove以缩小裸手与手套设计之间的外貌差距，并试验了AirGlove在多样化传感手套上的性能表现。\", \n  \"result\": \"实验表明，AirGlove可以在新设计的传感手套上实现显著的性能提升，证明了其推广性的有效性。\", \n  \"conclusion\": \"研究确认现有裸手模型在传感手套上性能显著下降的原因，并验证了AirGlove在少量数据的情况下推广到新设计手套的能力，表明其在手套追踪上的潜力。\")", "conclusion": "研究确认现有裸手模型在传感手套上性能显著下降的原因，并验证了AirGlove在少量数据的情况下推广到新设计手套的能力，表明其在手套追踪上的潜力。"}}
{"id": "2602.05211", "categories": ["cs.CL", "cs.DL"], "pdf": "https://arxiv.org/pdf/2602.05211", "abs": "https://arxiv.org/abs/2602.05211", "authors": ["Hongye Zhao", "Yi Zhao", "Chengzhi Zhang"], "title": "Quantifying the Knowledge Proximity Between Academic and Industry Research: An Entity and Semantic Perspective", "comment": null, "summary": "The academia and industry are characterized by a reciprocal shaping and dynamic feedback mechanism. Despite distinct institutional logics, they have adapted closely in collaborative publishing and talent mobility, demonstrating tension between institutional divergence and intensive collaboration. Existing studies on their knowledge proximity mainly rely on macro indicators such as the number of collaborative papers or patents, lacking an analysis of knowledge units in the literature. This has led to an insufficient grasp of fine-grained knowledge proximity between industry and academia, potentially undermining collaboration frameworks and resource allocation efficiency. To remedy the limitation, this study quantifies the trajectory of academia-industry co-evolution through fine-grained entities and semantic space. In the entity measurement part, we extract fine-grained knowledge entities via pre-trained models, measure sequence overlaps using cosine similarity, and analyze topological features through complex network analysis. At the semantic level, we employ unsupervised contrastive learning to quantify convergence in semantic spaces by measuring cross-institutional textual similarities. Finally, we use citation distribution patterns to examine correlations between bidirectional knowledge flows and similarity. Analysis reveals that knowledge proximity between academia and industry rises, particularly following technological change. This provides textual evidence of bidirectional adaptation in co-evolution. Additionally, academia's knowledge dominance weakens during technological paradigm shifts. The dataset and code for this paper can be accessed at https://github.com/tinierZhao/Academic-Industrial-associations.", "AI": {"tldr": "研究通过细粒度实体和语义空间量化学术界和产业界的共进化轨迹，揭示出随着技术变化，两者之间的知识接近性提升，并且在技术范式转变期间，学术界的主导地位减弱。", "motivation": "现有的研究主要依赖于宏观指标，如合作论文或专利的数量，缺乏对文献中知识单元的分析，这导致了对学术界和产业界之间细粒度知识接近性的认识不足，可能会削弱合作框架和资源配置效率。", "method": "通过预训练模型提取细粒度知识实体，使用余弦相似度测量序列重叠，并通过复杂网络分析来分析拓扑特征。在语义层面，通过非监督对比学习来量化机构间文本相似性的语义空间中的趋同。最后，利用引用分布模式来检查双向知识流与相似性的相关性。", "result": "分析表明，随着技术的变化，学术界和产业界之间的知识接近性在增加，这提供了文本证据，证明在共进化中存在双方适应的现象。另外，在技术范式转变期间，学术界的知识主导地位会减弱。", "conclusion": "该研究展示了学术界和产业界共进化过程中的知识接近性的提升，特别是在技术变化后，同时也揭示了学术界主导地位在技术范式转变期间的减弱趋势。"}}
{"id": "2602.05162", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.05162", "abs": "https://arxiv.org/abs/2602.05162", "authors": ["Anay Majee", "Rishabh Iyer"], "title": "SHaSaM: Submodular Hard Sample Mining for Fair Facial Attribute Recognition", "comment": "21 pages, 7 tables, 10 figures", "summary": "Deep neural networks often inherit social and demographic biases from annotated data during model training, leading to unfair predictions, especially in the presence of sensitive attributes like race, age, gender etc. Existing methods fall prey to the inherent data imbalance between attribute groups and inadvertently emphasize on sensitive attributes, worsening unfairness and performance. To surmount these challenges, we propose SHaSaM (Submodular Hard Sample Mining), a novel combinatorial approach that models fairness-driven representation learning as a submodular hard-sample mining problem. Our two-stage approach comprises of SHaSaM-MINE, which introduces a submodular subset selection strategy to mine hard positives and negatives - effectively mitigating data imbalance, and SHaSaM-LEARN, which introduces a family of combinatorial loss functions based on Submodular Conditional Mutual Information to maximize the decision boundary between target classes while minimizing the influence of sensitive attributes. This unified formulation restricts the model from learning features tied to sensitive attributes, significantly enhancing fairness without sacrificing performance. Experiments on CelebA and UTKFace demonstrate that SHaSaM achieves state-of-the-art results, with up to 2.7 points improvement in model fairness (Equalized Odds) and a 3.5% gain in Accuracy, within fewer epochs as compared to existing methods.", "AI": {"tldr": "SHaSaM is a method that enhances model fairness without sacrificing performance by addressing data imbalance and mitigating influence of sensitive attributes in deep neural networks.", "motivation": "To improve fairness in machine learning models trained with imbalanced datasets containing sensitive attributes.", "method": "SHaSaM (Submodular Hard Sample Mining), a two-stage approach that includes submodular subset selection to mine hard positives and negatives, and combinatorial loss functions to maximize decision boundary while minimizing sensitive attribute influence.", "result": "SHaSaM achieved state-of-the-art results on CelebA and UTKFace with significant improvements in Equalized Odds and Accuracy.", "conclusion": "The proposed SHaSaM method successfully improves fairness in machine learning models while maintaining or even enhancing performance."}}
{"id": "2602.05220", "categories": ["cs.CL", "cs.SD"], "pdf": "https://arxiv.org/pdf/2602.05220", "abs": "https://arxiv.org/abs/2602.05220", "authors": ["Jinchuan Tian", "Haoran Wang", "Bo-Hao Su", "Chien-yu Huang", "Qingzheng Wang", "Jiatong Shi", "William Chen", "Xun Gong", "Siddhant Arora", "Chin-Jou Li", "Masao Someki", "Takashi Maekaku", "Yusuke Shinohara", "Jin Sakuma", "Chao-Han Huck Yang", "Shinji Watanabe"], "title": "Bagpiper: Solving Open-Ended Audio Tasks via Rich Captions", "comment": null, "summary": "Current audio foundation models typically rely on rigid, task-specific supervision, addressing isolated factors of audio rather than the whole. In contrast, human intelligence processes audio holistically, seamlessly bridging physical signals with abstract cognitive concepts to execute complex tasks. Grounded in this philosophy, we introduce Bagpiper, an 8B audio foundation model that interprets physical audio via rich captions, i.e., comprehensive natural language descriptions that encapsulate the critical cognitive concepts inherent in the signal (e.g., transcription, audio events). By pre-training on a massive corpus of 600B tokens, the model establishes a robust bidirectional mapping between raw audio and this high-level conceptual space. During fine-tuning, Bagpiper adopts a caption-then-process workflow, simulating an intermediate cognitive reasoning step to solve diverse tasks without task-specific priors. Experimentally, Bagpiper outperforms Qwen-2.5-Omni on MMAU and AIRBench for audio understanding and surpasses CosyVoice3 and TangoFlux in generation quality, capable of synthesizing arbitrary compositions of speech, music, and sound effects. To the best of our knowledge, Bagpiper is among the first works that achieve unified understanding generation for general audio. Model, data, and code are available at Bagpiper Home Page.", "AI": {"tldr": "Bagpiper是一个8B的音频基础模型，它通过自然语言描述来解释音频信号，实现了音频的理解和生成，且在多个音频理解和生成任务中表现突出。", "motivation": "当前的音频基础模型通常依赖于刚性且任务特定的监督，只解决了音频的孤立因素，而不是整体。人类的智能则可以全面处理音频，无缝地连接物理信号和抽象认知概念来执行复杂的任务。Bagpiper正是基于这一理念被提出。", "method": "Bagpiper采用了一种通过丰富的标题（全面的自然语言描述）来解释物理音频的方法，这些标题涵盖了信号中固有的关键认知概念。通过在600B token的大型语料库上进行预训练，模型建立了原始音频与高层次概念空间之间的稳健双向映射。在微调过程中，Bagpiper采用了一个“先标注后处理”的工作流程，模拟了一个中间的认知推理步骤来解决多样化的任务，而不需要特定任务的先验知识。", "result": "实验结果显示，Bagpiper在MMAU和AIRBench上超过了Qwen-2.5-Omni，在音频理解方面表现出色；在生成质量方面，它也超过了CosyVoice3和TangoFlux，能够合成任意组合的语音、音乐和声音效果。", "conclusion": "Bagpiper是一个综合理解生成通用音频的第一批工作中之一，它可以不需要特定任务的先验知识就能解决多样化的任务，并且已经在模型、数据和代码上公开。"}}
