{"id": "2510.03315", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.03315", "abs": "https://arxiv.org/abs/2510.03315", "authors": ["Alex Gibson"], "title": "Decomposing Attention To Find Context-Sensitive Neurons", "comment": "10 pages, 7 figures. Submitted to the Mechanistic Interpretability\n  Workshop at NeurIPS 2025", "summary": "We study transformer language models, analyzing attention heads whose\nattention patterns are spread out, and whose attention scores depend weakly on\ncontent. We argue that the softmax denominators of these heads are stable when\nthe underlying token distribution is fixed. By sampling softmax denominators\nfrom a \"calibration text\", we can combine together the outputs of multiple such\nstable heads in the first layer of GPT2-Small, approximating their combined\noutput by a linear summary of the surrounding text. This approximation enables\na procedure where from the weights alone - and a single calibration text - we\ncan uncover hundreds of first layer neurons that respond to high-level\ncontextual properties of the surrounding text, including neurons that didn't\nactivate on the calibration text.", "AI": {"tldr": "通过分析Transformer模型中注意力分散且对内容依赖性弱的注意力头，研究提出了一种方法，在给定权重和少量校准文本的情况下，可以找到数百个对周围文本的高层次上下文属性敏感的第一层神经元。", "motivation": "理解注意力分散且对内容依赖性弱的注意力头的性质，探索在不依赖大量训练数据的情况下发现对文本高层次属性敏感的神经元。", "method": "从校准文本中采样softmax分母，结合多个稳定注意力头的输出，通过一个线性总结周围文本的过程来近似这些头的组合输出。", "result": "发现了许多第一层神经元，这些神经元对周围文本的高层次上下文属性敏感，即使它们在训练中未被激活。", "conclusion": "该方法可以通过少量信息（权重和一个校准文本）有效地发现模型中对高层次属性敏感的神经元，为理解 Transformer 模型提供了一种新方式。"}}
{"id": "2510.03323", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.03323", "abs": "https://arxiv.org/abs/2510.03323", "authors": ["Ge Chang", "Jinbo Su", "Jiacheng Liu", "Pengfei Yang", "Yuhao Shang", "Huiwen Zheng", "Hongli Ma", "Yan Liang", "Yuanchun Li", "Yunxin Liu"], "title": "Graph-S3: Enhancing Agentic textual Graph Retrieval with Synthetic Stepwise Supervision", "comment": null, "summary": "A significant portion of real-world data is inherently represented as textual\ngraphs, and integrating these graphs into large language models (LLMs) is\npromising to enable complex graph-based question answering. However, a key\nchallenge in LLM-based textual graph QA systems lies in graph retrieval, i.e.,\nhow to retrieve relevant content from large graphs that is sufficiently\ninformative while remaining compact for the LLM context. Existing retrievers\nsuffer from poor performance since they either rely on shallow embedding\nsimilarity or employ interactive retrieving policies that demand excessive data\nlabeling and training cost. To address these issues, we present Graph-$S^3$, an\nagentic textual graph reasoning framework that employs an LLM-based retriever\ntrained with synthetic stepwise supervision. Instead of rewarding the agent\nbased on the final answers, which may lead to sparse and unstable training\nsignals, we propose to closely evaluate each step of the retriever based on\noffline-extracted golden subgraphs. Our main techniques include a data\nsynthesis pipeline to extract the golden subgraphs for reward generation and a\ntwo-stage training scheme to learn the interactive graph exploration policy\nbased on the synthesized rewards. Based on extensive experiments on three\ncommon datasets in comparison with seven strong baselines, our approach\nachieves an average improvement of 8.1\\% in accuracy and 9.7\\% in F$_1$ score.\nThe advantage is even higher in more complicated multi-hop reasoning tasks. Our\ncode will be open-sourced.", "AI": {"tldr": "Introduces Graph-$S^3$ for improving LLM-based textual graph question answering by enhancing retriever performance.", "motivation": "To address the challenge of effective graph retrieval for LLM-based systems, moving beyond existing methods that use shallow embeddings or require excessive labeling and training.", "method": "Graph-$S^3$, an agentic textual graph reasoning framework that uses an LLM-based retriever trained with synthetic stepwise supervision, evaluates each step based on offline-extracted golden subgraphs, and employs a two-stage training scheme to learn graph exploration policy.", "result": "Achieves an average improvement of 8.1% in accuracy and 9.7% in F1 score over seven strong baselines on three common datasets, with a more significant advantage in complex multi-hop reasoning tasks.", "conclusion": "Graph-$S^3$ demonstrates superior performance in extracting relevant graph content for LLMs, enhancing accuracy and F1 scores especially in complex multi-hop questions."}}
{"id": "2510.03384", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03384", "abs": "https://arxiv.org/abs/2510.03384", "authors": ["Arjun Arunasalam", "Madison Pickering", "Z. Berkay Celik", "Blase Ur"], "title": "Implicit Values Embedded in How Humans and LLMs Complete Subjective Everyday Tasks", "comment": null, "summary": "Large language models (LLMs) can underpin AI assistants that help users with\neveryday tasks, such as by making recommendations or performing basic\ncomputation. Despite AI assistants' promise, little is known about the implicit\nvalues these assistants display while completing subjective everyday tasks.\nHumans may consider values like environmentalism, charity, and diversity. To\nwhat extent do LLMs exhibit these values in completing everyday tasks? How do\nthey compare with humans? We answer these questions by auditing how six popular\nLLMs complete 30 everyday tasks, comparing LLMs to each other and to 100 human\ncrowdworkers from the US. We find LLMs often do not align with humans, nor with\nother LLMs, in the implicit values exhibited.", "AI": {"tldr": "本研究通过比较六种流行的LLMs与100名人类在完成日常任务时的表现，发现LLMs的隐含价值观与其人类同行和其他LLMs均存在差异。", "motivation": "尽管AI助手有着巨大的潜力，但人们对于这些助手在完成主观日常任务时所展现的隐含价值观知之甚少。本研究旨在探讨大规模语言模型（LLMs）在完成日常任务时如何展现环境主义、慈善和多样性等价值观。", "method": "研究通过审计六种流行的LLMs在完成30项日常任务时的表现来回答这些问题，并将LLMs与来自美国的100名人类众包工作者进行比较。", "result": "研究发现LLMs在展示隐含价值观方面往往与人类不同，与其他LLMs之间也存在差异。", "conclusion": "LLMs在完成日常任务时所展示的隐含价值观并不总与人类一致，并且不同的LLMs之间也有差异。"}}
{"id": "2510.03439", "categories": ["cs.CL", "I.2.7; I.6.m"], "pdf": "https://arxiv.org/pdf/2510.03439", "abs": "https://arxiv.org/abs/2510.03439", "authors": ["Brendon Boldt", "David Mortensen"], "title": "Morpheme Induction for Emergent Language", "comment": "Accepted for publication at the 2025 Conference on Empirical Methods\n  in Natural Language Processing; 16 pages, 4 figures", "summary": "We introduce CSAR, an algorithm for inducing morphemes from emergent language\ncorpora of parallel utterances and meanings. It is a greedy algorithm that (1)\nweights morphemes based on mutual information between forms and meanings, (2)\nselects the highest-weighted pair, (3) removes it from the corpus, and (4)\nrepeats the process to induce further morphemes (i.e., Count, Select, Ablate,\nRepeat). The effectiveness of CSAR is first validated on procedurally generated\ndatasets and compared against baselines for related tasks. Second, we validate\nCSAR's performance on human language data to show that the algorithm makes\nreasonable predictions in adjacent domains. Finally, we analyze a handful of\nemergent languages, quantifying linguistic characteristics like degree of\nsynonymy and polysemy.", "AI": {"tldr": "CSAR算法通过结合形态和意义的互信息，从平行话语和意义的新兴语言语料库中有效地诱导出形态。", "motivation": "提出CSAR算法来从平行话语和意义的新兴语言语料库中诱导构成语素。", "method": "CSAR算法通过计算形式和意义之间的互信息来加权构成语素，选择权重最高的配对，然后从语料库中移除，并重复此过程以诱导进一步的语素。", "result": "该算法首先在程序生成的数据集上验证其有效性，并与相关任务的基线进行比较。其次，在人类语言数据上验证其性能，表明该算法在相邻领域中做出了合理的预测。最后，通过分析几个新兴语言，量化了诸如同义程度和多义程度等语言特征。", "conclusion": "CSAR算法在程序生成数据集和人类语言数据上都表现出了有效地诱导语素的能力，并且能够分析新兴语言，从而量化其语言特性，如同义和多义的程度。"}}
{"id": "2510.03287", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.03287", "abs": "https://arxiv.org/abs/2510.03287", "authors": ["Moinak Bhattacharya", "Gagandeep Singh", "Prateek Prasanna"], "title": "SoC-DT: Standard-of-Care Aligned Digital Twins for Patient-Specific Tumor Dynamics", "comment": null, "summary": "Accurate prediction of tumor trajectories under standard-of-care (SoC)\ntherapies remains a major unmet need in oncology. This capability is essential\nfor optimizing treatment planning and anticipating disease progression.\nConventional reaction-diffusion models are limited in scope, as they fail to\ncapture tumor dynamics under heterogeneous therapeutic paradigms. There is\nhence a critical need for computational frameworks that can realistically\nsimulate SoC interventions while accounting for inter-patient variability in\ngenomics, demographics, and treatment regimens. We introduce Standard-of-Care\nDigital Twin (SoC-DT), a differentiable framework that unifies\nreaction-diffusion tumor growth models, discrete SoC interventions (surgery,\nchemotherapy, radiotherapy) along with genomic and demographic personalization\nto predict post-treatment tumor structure on imaging. An implicit-explicit\nexponential time-differencing solver, IMEX-SoC, is also proposed, which ensures\nstability, positivity, and scalability in SoC treatment situations. Evaluated\non both synthetic data and real world glioma data, SoC-DT consistently\noutperforms classical PDE baselines and purely data-driven neural models in\npredicting tumor dynamics. By bridging mechanistic interpretability with modern\ndifferentiable solvers, SoC-DT establishes a principled foundation for\npatient-specific digital twins in oncology, enabling biologically consistent\ntumor dynamics estimation. Code will be made available upon acceptance.", "AI": {"tldr": "研究提出了SoC-DT框架结合反应扩散模型和标准治疗方法，用于预测治疗后的肿瘤结构，该框架优于传统的PDE模型和纯数据驱动的方法。", "motivation": "现有的肿瘤反应-扩散模型因无法捕捉肿瘤在异质治疗下的动态变化而存在局限性，本研究旨在开发一个可以模拟真实世界中标准治疗干预，并考虑患者间基因组学、人口统计学和治疗方案差异的计算框架。", "method": "SoC-DT框架结合反应扩散肿瘤生长模型、离散标准治疗干预、基因组和人口统计学的个性化信息，使用IMEX-SoC隐式-显式指数时间差分求解器来确保稳定性、正性和可扩展性。", "result": "该研究提出了一个名为标准治疗数字孪生（SoC-DT）的可微框架，结合了反应扩散肿瘤生长模型、离散标准治疗方法（如手术、化疗、放疗）以及基因组和人口统计学个性化元素，来预测治疗后的肿瘤结构。该框架使用隐式-显式指数时间差分求解器IMEX-SoC，确保了稳定性和扩展性。实验结果表明，SoC-DT在预测肿瘤动力学方面优于传统的偏微分方程基准和纯数据驱动的神经模型。这一研究建立了肿瘤数字孪生技术的原理性基础，有助于实现个性化肿瘤治疗方案的制定和疾病进展的预测。", "conclusion": "SoC-DT以机制可解释性和现代可微分求解器相结合，为肿瘤学创建了一个个性化数字孪生的原理性基础，支持生物一致的肿瘤动态估计。"}}
{"id": "2510.03458", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2510.03458", "abs": "https://arxiv.org/abs/2510.03458", "authors": ["Mengyao Xu", "Wenfei Zhou", "Yauhen Babakhin", "Gabriel Moreira", "Ronay Ak", "Radek Osmulski", "Bo Liu", "Even Oldridge", "Benedikt Schifferer"], "title": "Omni-Embed-Nemotron: A Unified Multimodal Retrieval Model for Text, Image, Audio, and Video", "comment": null, "summary": "We present Omni-Embed-Nemotron, a unified multimodal retrieval embedding\nmodel developed to handle the increasing complexity of real-world information\nneeds. While Retrieval-Augmented Generation (RAG) has significantly advanced\nlanguage models by incorporating external knowledge, existing text-based\nretrievers rely on clean, structured input and struggle with the visually and\nsemantically rich content found in real-world documents such as PDFs, slides,\nor videos. Recent work such as ColPali has shown that preserving document\nlayout using image-based representations can improve retrieval quality.\nBuilding on this, and inspired by the capabilities of recent multimodal models\nsuch as Qwen2.5-Omni, we extend retrieval beyond text and images to also\nsupport audio and video modalities. Omni-Embed-Nemotron enables both\ncross-modal (e.g., text - video) and joint-modal (e.g., text - video+audio)\nretrieval using a single model. We describe the architecture, training setup,\nand evaluation results of Omni-Embed-Nemotron, and demonstrate its\neffectiveness in text, image, and video retrieval.", "AI": {"tldr": "Omni-Embed-Nemotron is a unified multimodal retrieval embedding model that extends existing text-based retrieval systems to incorporate multimodal data types, including audio and video, demonstrating improved retrieval quality.", "motivation": "The motivation behind this paper is to address the limitations of existing retrieval systems which are primarily text-based and struggle with complex multimedia content. The goal is to improve the retrieval quality and capability to process unstructured and semantically rich data found in real-world documents such as PDFs, slides, and videos.", "method": "The paper introduces Omni-Embed-Nemotron, a multimodal retrieval embedding model capable of handling various types of real-world information, including text, images, audio, and video. The model extends the capabilities of Retrieval-Augmented Generation (RAG) systems, which typically handle only textual data, by incorporating multimodal data using a unified approach.", "result": "Omni-Embed-Nemotron is shown to be effective in text, image, and video retrieval. The results suggest an improvement in retrieval quality by incorporating multimodal data.", "conclusion": "The paper concludes that Omni-Embed-Nemotron can effectively handle the increasing complexity of real-world information needs, providing a versatile solution for multimedia retrieval tasks."}}
{"id": "2510.03292", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.03292", "abs": "https://arxiv.org/abs/2510.03292", "authors": ["Doğanay Demir", "İlknur Durgar Elkahlout"], "title": "Visualizing Celebrity Dynamics in Video Content: A Proposed Approach Using Face Recognition Timestamp Data", "comment": null, "summary": "In an era dominated by video content, understanding its structure and\ndynamics has become increasingly important. This paper presents a hybrid\nframework that combines a distributed multi-GPU inference system with an\ninteractive visualization platform for analyzing celebrity dynamics in video\nepisodes. The inference framework efficiently processes large volumes of video\ndata by leveraging optimized ONNX models, heterogeneous batch inference, and\nhigh-throughput parallelism, ensuring scalable generation of timestamped\nappearance records. These records are then transformed into a comprehensive\nsuite of visualizations, including appearance frequency charts, duration\nanalyses, pie charts, co-appearance matrices, network graphs, stacked area\ncharts, seasonal comparisons, and heatmaps. Together, these visualizations\nprovide multi-dimensional insights into video content, revealing patterns in\ncelebrity prominence, screen-time distribution, temporal dynamics,\nco-appearance relationships, and intensity across episodes and seasons. The\ninteractive nature of the system allows users to dynamically explore data,\nidentify key moments, and uncover evolving relationships between individuals.\nBy bridging distributed recognition with structured, visually-driven analytics,\nthis work enables new possibilities for entertainment analytics, content\ncreation strategies, and audience engagement studies.", "AI": {"tldr": "本文介绍一种新框架，结合多GPU分布式推理系统和互动可视化工具，用于分析视频中名人动态。", "motivation": "在以视频内容为主导的时代，了解视频内容的结构和动态变得越来越重要，因此提出此框架。", "method": "该论文提出了一种结合分布式多GPU推理系统和交互式可视化平台的混合框架，用于分析视频片段中名人动态。推理框架通过利用优化的ONNX模型、异构批处理推理和高吞吐量并行化，在视频数据处理中提高了效率，确保了带时间戳的出场记录的可扩展生成。然后，这些记录被转化为一系列详细的可视化，包括出场频率图表、持续时间分析、饼状图、共同出场矩阵、网络图形、堆叠面积图表、季节性比较以及热力图。", "result": "这些可视化提供了对视频内容的多维度洞察，可以揭示名人地位、屏幕时间分布、时间动态、共同出场关系以及跨剧集和赛季的活跃程度。系统的交互性质允许用户动态探索数据、识别关键时刻并揭示个人之间的关系演变。", "conclusion": "通过结合分布式识别与结构化、以视觉驱动的分析，此框架为娱乐分析、内容创作策略及观众互动研究开启了新的可能性。"}}
{"id": "2510.03467", "categories": ["cs.CL", "I.2.7; I.6.m"], "pdf": "https://arxiv.org/pdf/2510.03467", "abs": "https://arxiv.org/abs/2510.03467", "authors": ["Brendon Boldt", "David Mortensen"], "title": "Searching for the Most Human-like Emergent Language", "comment": "Accepted for publication at the 2025 Conference on Empirical Methods\n  in Natural Language Processing; 19 pages, 12 figures", "summary": "In this paper, we design a signalling game-based emergent communication\nenvironment to generate state-of-the-art emergent languages in terms of\nsimilarity to human language. This is done with hyperparameter optimization,\nusing XferBench as the objective function. XferBench quantifies the statistical\nsimilarity of emergent language to human language by measuring its suitability\nfor deep transfer learning to human language. Additionally, we demonstrate the\npredictive power of entropy on the transfer learning performance of emergent\nlanguage as well as corroborate previous results on the entropy-minimization\nproperties of emergent communication systems. Finally, we report\ngeneralizations regarding what hyperparameters produce more realistic emergent\nlanguages, that is, ones which transfer better to human language.", "AI": {"tldr": "研究设计了一种新兴语言生成方法，该方法能生成与人类语言相似的新兴语言。通过超参数优化和XferBench评估语言相似性，发现熵最小化有助于提高新兴语言的迁移学习性能，同时揭示了产生更真实新兴语言的超参数特性。", "motivation": "目的是生成类似于人类语言状态的新兴语言，并探索什么超参数能够产生更真实的新兴语言。", "method": "设计了一个基于信号博弈的新兴交流环境，利用超参数优化和XferBench作为目标函数来生成状态最先进的人类语言相似的新兴语言。XferBench通过测量新兴语言进行深度迁移学习到人类语言的适用性来量化其与人类语言的统计相似度。", "result": "验证了熵对新兴语言迁移学习性能的预测能力，并证实了关于新兴交流系统熵最小化的先前结果，报告了一般化发现，即哪些超参数能够生成更真实的新兴语言，这些语言能够更好地迁移到人类语言。", "conclusion": "研究表明，通过超参数优化可以生成与人类语言相似的新兴语言，并且熵的最小化有助于提高新兴语言的迁移学习性能。"}}
{"id": "2510.03294", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2510.03294", "abs": "https://arxiv.org/abs/2510.03294", "authors": ["Saanvi Kataria"], "title": "Domain-Robust Marine Plastic Detection Using Vision Models", "comment": "16 pages, 5 figures, 1 table", "summary": "Marine plastic pollution is a pressing environmental threat, making reliable\nautomation for underwater debris detection essential. However, vision systems\ntrained on one dataset often degrade on new imagery due to domain shift. This\nstudy benchmarks models for cross-domain robustness, training convolutional\nneural networks - CNNs (MobileNetV2, ResNet-18, EfficientNet-B0) and vision\ntransformers (DeiT-Tiny, ViT-B16) on a labeled underwater dataset and then\nevaluates them on a balanced cross-domain test set built from plastic-positive\nimages drawn from a different source and negatives from the training domain.\nTwo zero-shot models were assessed, CLIP ViT-L14 and Google's Gemini 2.0 Flash,\nthat leverage pretraining to classify images without fine-tuning. Results show\nthe lightweight MobileNetV2 delivers the strongest cross-domain performance (F1\n0.97), surpassing larger models. All fine-tuned models achieved high Precision\n(around 99%), but differ in Recall, indicating varying sensitivity to plastic\ninstances. Zero-shot CLIP is comparatively sensitive (Recall around 80%) yet\nprone to false positives (Precision around 56%), whereas Gemini exhibits the\ninverse profile (Precision around 99%, Recall around 81%). Error analysis\nhighlights recurring confusions with coral textures, suspended particulates,\nand specular glare. Overall, compact CNNs with supervised training can\ngeneralize effectively for cross-domain underwater detection, while large\npretrained vision-language models provide complementary strengths.", "AI": {"tldr": "研究评估了不同模型在跨域检测水下塑料污染的性能，发现轻量级的MobileNetV2表现最佳，同时指出预训练零样本模型有其独特的优缺点。", "motivation": "水下视觉系统的训练数据集转移导致性能下降，本研究旨在评估模型在跨域检测水下塑料污染的鲁棒性。", "method": "使用卷积神经网络（如MobileNetV2, ResNet-18, EfficientNet-B0）和视觉变换器（如DeiT-Tiny, ViT-B16）在标记的水下数据集上训练模型，并在一个来自不同数据源的均衡跨域测试集上进行评估，该测试集包括阳性塑料图像和来自训练域的阴性图像。同时评估了两个零样本模型CLIP ViT-L14和Google的Gemini 2.0 Flash。", "result": "轻量级的MobileNetV2在跨域性能（F1为0.97）方面表现最佳，优于较大模型。所有微调模型在精度方面表现高（约99%），但在召回率上有所差异。零样本模型CLIP具有较高的敏感性（召回率约80%），但容易产生假阳性（精度约56%）。Gemini则表现出相反的性能特点（精度约99%，召回率约81%）。错误分析表明，珊瑚纹理、悬浮颗粒和镜面眩光是常见混淆来源。", "conclusion": "紧凑的监督训练卷积神经网络能有效地跨域检测水下塑料污染，而大型预训练视觉-语言模型提供互补优势。"}}
{"id": "2510.03490", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2510.03490", "abs": "https://arxiv.org/abs/2510.03490", "authors": ["Aneesha Sampath", "Oya Aran", "Emily Mower Provost"], "title": "SEER: The Span-based Emotion Evidence Retrieval Benchmark", "comment": null, "summary": "We introduce the SEER (Span-based Emotion Evidence Retrieval) Benchmark to\ntest Large Language Models' (LLMs) ability to identify the specific spans of\ntext that express emotion. Unlike traditional emotion recognition tasks that\nassign a single label to an entire sentence, SEER targets the underexplored\ntask of emotion evidence detection: pinpointing which exact phrases convey\nemotion. This span-level approach is crucial for applications like empathetic\ndialogue and clinical support, which need to know how emotion is expressed, not\njust what the emotion is. SEER includes two tasks: identifying emotion evidence\nwithin a single sentence, and identifying evidence across a short passage of\nfive consecutive sentences. It contains new annotations for both emotion and\nemotion evidence on 1200 real-world sentences. We evaluate 14 open-source LLMs\nand find that, while some models approach average human performance on\nsingle-sentence inputs, their accuracy degrades in longer passages. Our error\nanalysis reveals key failure modes, including overreliance on emotion keywords\nand false positives in neutral text.", "AI": {"tldr": "介绍了SEER基准测试，用于评估语言模型识别表达情感的具体文本片段的能力，涵盖了单句及短文本段落两种任务，并通过误差分析揭示了模型的优势和不足。", "motivation": "针对传统情绪识别任务仅给整个句子打标签的局限，强调了准确识别情绪表达的具体词汇片段的重要性，对于共情对话和临床支持等应用场景尤为关键。", "method": "Structure", "result": "{\n  \"tldr\": \"介绍了SEER基准测试，用于评估语言模型识别表达情感的具体文本片段的能力，涵盖了单句及短文本段落两种任务，并通过误差分析揭示了模型的优势和不足。\",\n  \"motivation\": \"针对传统情绪识别任务仅给整个句子打标签的局限，强调了准确识别情绪表达的具体词汇片段的重要性，对于共情对话和临床支持等应用场景尤为关键。\",\n  \"method\": \"SEER包含两个任务：单句情绪证据识别和短段落情绪证据识别。该基准涵盖了1200条真实句子的新注释，对情绪和情绪证据进行了标注，并评估了14个开源语言模型的表现。\",\n  \"result\": \"部分模型在单句输入上接近人类平均水平，但在更长的文本片段中准确性降低。错误分析揭示了模型过分依赖情绪相关词汇及在中性文本中的误报等问题。\",\n  \"conclusion\": \"SEER为语言模型在情绪证据检测上的能力提供了新的评估视角，展示了模型的挑战和改进方向。\")}\n}\n", "conclusion": "SEER为语言模型在情绪证据检测上的能力提供了新的评估视角，展示了模型的挑战和改进方向。"}}
{"id": "2510.03295", "categories": ["cs.CV", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.03295", "abs": "https://arxiv.org/abs/2510.03295", "authors": ["Passant Elchafei", "Amany Fashwan"], "title": "Multimodal Arabic Captioning with Interpretable Visual Concept Integration", "comment": null, "summary": "We present VLCAP, an Arabic image captioning framework that integrates\nCLIP-based visual label retrieval with multimodal text generation. Rather than\nrelying solely on end-to-end captioning, VLCAP grounds generation in\ninterpretable Arabic visual concepts extracted with three multilingual\nencoders, mCLIP, AraCLIP, and Jina V4, each evaluated separately for label\nretrieval. A hybrid vocabulary is built from training captions and enriched\nwith about 21K general domain labels translated from the Visual Genome dataset,\ncovering objects, attributes, and scenes. The top-k retrieved labels are\ntransformed into fluent Arabic prompts and passed along with the original image\nto vision-language models. In the second stage, we tested Qwen-VL and Gemini\nPro Vision for caption generation, resulting in six encoder-decoder\nconfigurations. The results show that mCLIP + Gemini Pro Vision achieved the\nbest BLEU-1 (5.34%) and cosine similarity (60.01%), while AraCLIP + Qwen-VL\nobtained the highest LLM-judge score (36.33%). This interpretable pipeline\nenables culturally coherent and contextually accurate Arabic captions.", "AI": {"tldr": "论文展示了VLCAP框架，使用多语言编码器提取视觉概念并通过多模态文本生成技术生成阿拉伯语图像字幕，在多种配置下实现了较高的BLEU和相似性评分。", "motivation": "研究旨在通过结合CLIP基础的视觉标签检索和多模态文本生成技术，开发一种更准确、文化和情境相关的阿拉伯语图像字幕生成框架。", "method": "本篇论文提出了VLCAP框架，结合CLIP基础的视觉标签检索和多模态文本生成技术，用来生成阿拉伯语图像字幕。该框架使用了三种多语言编码器：mCLIP、AraCLIP、Jina V4，分别用于标签检索。同时，构建了一个混合词汇表，并使用Visual Genome数据集中的标签进行扩充。在第二阶段，使用了Qwen-VL和Gemini Pro Vision两种模型进行字幕生成，共得到六种编码-解码配置。", "result": "实验结果表明，mCLIP + Gemini Pro Vision在BLEU-1（5.34%）和余弦相似性（60.01%）上取得了最佳性能，而AraCLIP + Qwen-VL在LLM-judge评分上取得了最高值（36.33%）。", "conclusion": "该研究的可解释性框架使得生成的文化一致且情境准确的阿拉伯语图像字幕成为可能。"}}
{"id": "2510.03502", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2510.03502", "abs": "https://arxiv.org/abs/2510.03502", "authors": ["Ali Khairallah", "Arkaitz Zubiaga"], "title": "ALHD: A Large-Scale and Multigenre Benchmark Dataset for Arabic LLM-Generated Text Detection", "comment": "47 pages, 15 figures. Dataset available at Zenodo:\n  https://doi.org/10.5281/zenodo.17249602 Codebase available at GitHub:\n  https://github.com/alikhairallah/ALHD-Benchmarking", "summary": "We introduce ALHD, the first large-scale comprehensive Arabic dataset\nexplicitly designed to distinguish between human- and LLM-generated texts. ALHD\nspans three genres (news, social media, reviews), covering both MSA and\ndialectal Arabic, and contains over 400K balanced samples generated by three\nleading LLMs and originated from multiple human sources, which enables studying\ngeneralizability in Arabic LLM-genearted text detection. We provide rigorous\npreprocessing, rich annotations, and standardized balanced splits to support\nreproducibility. In addition, we present, analyze and discuss benchmark\nexperiments using our new dataset, in turn identifying gaps and proposing\nfuture research directions. Benchmarking across traditional classifiers,\nBERT-based models, and LLMs (zero-shot and few-shot) demonstrates that\nfine-tuned BERT models achieve competitive performance, outperforming LLM-based\nmodels. Results are however not always consistent, as we observe challenges\nwhen generalizing across genres; indeed, models struggle to generalize when\nthey need to deal with unseen patterns in cross-genre settings, and these\nchallenges are particularly prominent when dealing with news articles, where\nLLM-generated texts resemble human texts in style, which opens up avenues for\nfuture research. ALHD establishes a foundation for research related to Arabic\nLLM-detection and mitigating risks of misinformation, academic dishonesty, and\ncyber threats.", "AI": {"tldr": "ALHD数据集用于区分阿拉伯语的机器生成和人类生成的文本，研究发现微调的BERT模型性能突出，但在不同体裁（尤其是新闻）之间泛化能力存在挑战。", "motivation": "研究旨在建立一个能够有效区分人类和语言模型生成的文本的数据集，进而研究一般化的阿拉伯语言模型生成文本检测问题，并探索相关风险管控。", "method": "通过引入ALHD数据集，研究采用了传统的分类器、基于BERT的模型以及语言模型（零样本和少量样本）来进行基准实验，评估不同方法在检测阿拉伯语文本中生成人类文本和模型生成文本的能力。", "result": "ALHD是一个专为区分人类和大型语言模型生成的阿拉伯文本而设计的首个大规模综合性数据集。它涵盖了三种体裁（新闻、社交媒体、评论），包括现代标准阿拉伯语和方言，并含有超过40万份由三个领先的语言模型生成的均衡样本，以及多个人类来源的文本。该数据集支持再现性研究，并对传统分类器、基于BERT的模型和零样本/少量样本的LLM进行了基准测试实验，得出基于BERT的微调模型能够达到较高的性能。实验表明在跨体裁泛化上仍存在挑战，尤其是在处理新闻文章时，因为语言模型生成的文本风格类似于人类文本。ALHD为阿拉伯语语言模型检测及其风险管控建立了研究基础。", "conclusion": "通过ALHD数据集的实验分析表明，基于BERT的微调模型在检测阿拉伯语文本中的人类和模型生成的文本时表现出色。但同时也指出模型泛化能力在跨体裁的文本分析中存在挑战，特别是在处理新闻类文本时。这些发现揭示了未来研究的方向。"}}
