<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 19]
- [cs.CV](#cs.CV) [Total: 39]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Short-Context Dominance: How Much Local Context Natural Language Actually Needs?](https://arxiv.org/abs/2512.08082)
*Vala Vakilian,Zimeng Wang,Ankit Singh Rawat,Christos Thrampoulidis*

Main category: cs.CL

> 研究结果显示大多数序列只需要96个或更少的最近token就能准确预测下一token。提出DaMCL来检测长上下文序列，开发解码算法以抵消短上下文优势对模型预测性能的影响。

<details>
  <summary>Details</summary>

**Motivation:** 探讨短上下文优势假设，并提出一种实用的方法来检测那些需要较长上下文才能准确预测下一token的序列。

**Method:** 使用大型语言模型作为统计预言者，测量在不同长度序列数据集中，准确预测下一token所需的最小上下文长度（MCL）。提出了一种无需实际下一token知识的MCL实用代理--分布式感知MCL（DaMCL），并验证了其在检测长与短上下文序列方面的性能。开发了一种利用检测器识别并增强长距离相关的token的解码算法，以抵消短上下文优势带来的偏差。

**Result:** 实验验证了DaMCL在检测长与短上下文序列方面的高性能，开发的解码算法证实了消除偏差可以提高模型性能。

**Conclusion:** 通过提出和验证DaMCL及解码算法，研究表明短上下文优势可以通过特定方法进行检测和缓解。改善了基于LLM的任务性能。

**Abstract:** We investigate the short-context dominance hypothesis: that for most sequences, a small local prefix suffices to predict their next tokens. Using large language models as statistical oracles, we measure the minimum context length (MCL) needed to reproduce accurate full-context predictions across datasets with sequences of varying lengths. For sequences with 1-7k tokens from long-context documents, we consistently find that 75-80% require only the last 96 tokens at most. Given the dominance of short-context tokens, we then ask whether it is possible to detect challenging long-context sequences for which a short local prefix does not suffice for prediction. We introduce a practical proxy to MCL, called Distributionally Aware MCL (DaMCL), that does not require knowledge of the actual next-token and is compatible with sampling strategies beyond greedy decoding. Our experiments validate that simple thresholding of the metric defining DaMCL achieves high performance in detecting long vs. short context sequences. Finally, to counter the bias that short-context dominance induces in LLM output distributions, we develop an intuitive decoding algorithm that leverages our detector to identify and boost tokens that are long-range-relevant. Across Q&A tasks and model architectures, we confirm that mitigating the bias improves performance.

</details>


### [2] [Adaptation of Embedding Models to Financial Filings via LLM Distillation](https://arxiv.org/abs/2512.08088)
*Eliot Brenner,Dominic Seyler,Manjunath Hegde,Andrei Simion,Koustuv Dasgupta,Bing Xiang*

Main category: cs.CL

> 本研究提出了一种可扩展的管道，用于从未经标注的语料库中训练专业模型，使用通用检索嵌入模型作为基础。实验结果显示，该方法在金融文件类型的信息检索中，相较于现有嵌入模型有显著提升。方法采用了师徒模型的互动，逐步提升学生模型在未标注语料库中检索硬正负样本的能力。

<details>
  <summary>Details</summary>

**Motivation:** 尽管生成型大语言模型取得了进展，但专业对话AI代理的实际应用仍受限于计算成本、延迟要求和专业特定的相关性度量。现有的嵌入模型虽解决了部分问题，但在特定领域如金融的信息检索中表现不佳。本研究旨在弥补通用模型和特定领域之间的差距，提供一种成本效益高的解决方案。

**Method:** Structure

**Result:** <tool_call>
data = {{"tldr": "本研究提出了一种可扩展的管道，用于从未经标注的语料库中训练专业模型，使用通用检索嵌入模型作为基础。实验结果显示，该方法在金融文件类型的信息检索中，相较于现有嵌入模型有显著提升。方法采用了师徒模型的互动，逐步提升学生模型在未标注语料库中检索硬正负样本的能力。", "motivation": "尽管生成型大语言模型取得了进展，但专业对话AI代理的实际应用仍受限于计算成本、延迟要求和专业特定的相关性度量。现有的嵌入模型虽解决了部分问题，但在特定领域如金融的信息检索中表现不佳。本研究旨在弥补通用模型和特定领域之间的差距，提供一种成本效益高的解决方案。", "method": "采用了通用检索嵌入模型作为基础，通过一种师徒模型互动的管道，迭代地改进学生模型的权重。该方法无需大量的人工标注，而是通过检索库存找硬性正负例子进行正则训练。", "result": "实验结果显示在金融文件检索中，平均MRR@5提高了27.7%，均值DCG@5提高了44.6%，并在FinanceBench中的3个文档类别中改进了NDCG。", "conclusion": "本研究提供了一种成本效益高的方法来改进特定领域中的信息检索，相对于直接对检索模型进行微调的方法，我们的管道通过师徒模型的互动和迭代式训练，提高了模型在特定领域的性能。"}}


**Conclusion:** 本研究提供了一种成本效益高的方法来改进特定领域中的信息检索，相对于直接对检索模型进行微调的方法，我们的管道通过师徒模型的互动和迭代式训练，提高了模型在特定领域的性能。

**Abstract:** Despite advances in generative large language models (LLMs), practical application of specialized conversational AI agents remains constrained by computation costs, latency requirements, and the need for precise domain-specific relevance measures. While existing embedding models address the first two constraints, they underperform on information retrieval in specialized domains like finance. This paper introduces a scalable pipeline that trains specialized models from an unlabeled corpus using a general purpose retrieval embedding model as foundation. Our method yields an average of 27.7% improvement in MRR$\texttt{@}$5, 44.6% improvement in mean DCG$\texttt{@}$5 across 14 financial filing types measured over 21,800 query-document pairs, and improved NDCG on 3 of 4 document classes in FinanceBench. We adapt retrieval embeddings (bi-encoder) for RAG, not LLM generators, using LLM-judged relevance to distill domain knowledge into a compact retriever. There are prior works which pair synthetically generated queries with real passages to directly fine-tune the retrieval model. Our pipeline differs from these by introducing interaction between student and teacher models that interleaves retrieval-based mining of hard positive/negative examples from the unlabeled corpus with iterative retraining of the student model's weights using these examples. Each retrieval iteration uses the refined student model to mine the corpus for progressively harder training examples for the subsequent training iteration. The methodology provides a cost-effective solution to bridging the gap between general-purpose models and specialized domains without requiring labor-intensive human annotation.

</details>


### [3] [Segment, Embed, and Align: A Universal Recipe for Aligning Subtitles to Signing](https://arxiv.org/abs/2512.08094)
*Zifan Jiang,Youngjoon Jang,Liliane Momeni,Gül Varol,Sarah Ebling,Andrew Zisserman*

Main category: cs.CL

> A universal method (SEA) is developed to align subtitles to sign language videos, showing state-of-the-art performance across multiple datasets.

<details>
  <summary>Details</summary>

**Motivation:** To develop a universal approach that works across multiple languages and domains, unlike previous methods that are limited to specific languages or datasets.

**Method:** Segment, Embed, and Align (SEA) is proposed to align subtitles to continuous sign language videos. SEA uses two pretrained models: one to segment the video frames into signs and another to embed the video clips into a shared latent space with text. A lightweight dynamic programming procedure is then used for alignment.

**Result:** State-of-the-art alignment performance was demonstrated on four sign language datasets.

**Conclusion:** SEA shows potential to generate high-quality parallel data for improving sign language processing.

**Abstract:** The goal of this work is to develop a universal approach for aligning subtitles (i.e., spoken language text with corresponding timestamps) to continuous sign language videos. Prior approaches typically rely on end-to-end training tied to a specific language or dataset, which limits their generality. In contrast, our method Segment, Embed, and Align (SEA) provides a single framework that works across multiple languages and domains. SEA leverages two pretrained models: the first to segment a video frame sequence into individual signs and the second to embed the video clip of each sign into a shared latent space with text. Alignment is subsequently performed with a lightweight dynamic programming procedure that runs efficiently on CPUs within a minute, even for hour-long episodes. SEA is flexible and can adapt to a wide range of scenarios, utilizing resources from small lexicons to large continuous corpora. Experiments on four sign language datasets demonstrate state-of-the-art alignment performance, highlighting the potential of SEA to generate high-quality parallel data for advancing sign language processing. SEA's code and models are openly available.

</details>


### [4] [Universal Adversarial Suffixes Using Calibrated Gumbel-Softmax Relaxation](https://arxiv.org/abs/2512.08123)
*Sampriti Soor,Suklav Ghosh,Arijit Sur*

Main category: cs.CL

> 研究了一种通用对抗后缀的方法，该方法可以广泛降低多种任务和模型的准确性，并能够有效迁移到其他模型上。实验表明了其攻击效果以及跨任务跨模型家庭的迁移能力。

<details>
  <summary>Details</summary>

**Motivation:** 现有的工作通常优化任务或模型特定的触发机制，难以相互比较，且限制了迁移能力。研究目标是寻找一种通用的对抗后缀，使其能够广泛应用于多种任务和模型中。

**Method:** 研究了通用的对抗后缀，这是一种短的令牌序列（4-10个令牌），当附加到任何输入时，可以广泛降低多种任务和模型的准确性。研究方法采用可微分的“软”形式学习后缀，使用Gumbel-Softmax放松技术，并在推理时离散化。训练过程中，通过最大化校准后的交叉熵，同时屏蔽黄金令牌以防止信息泄露，并加入熵正则化以避免模型崩溃。

**Result:** 训练出的单个后缀可以在多个模型之间有效转移，一致降低准确性和校准信心。实验展示了该方法在情感分析、自然语言推理、语句相似度检测、常识问题回答以及使用Qwen2-1.5B、Phi-1.5和TinyLlama-1.1B模型的物理推理任务中的攻击效果和跨任务跨模型家庭的转移能力。

**Conclusion:** 研究表明，学习到的通用对抗后缀可以在不同任务和模型中有效降低准确性和校准信心，验证了其作为模型脆弱性攻击的有效性和通用性。

**Abstract:** Language models (LMs) are often used as zero-shot or few-shot classifiers by scoring label words, but they remain fragile to adversarial prompts. Prior work typically optimizes task- or model-specific triggers, making results difficult to compare and limiting transferability. We study universal adversarial suffixes: short token sequences (4-10 tokens) that, when appended to any input, broadly reduce accuracy across tasks and models. Our approach learns the suffix in a differentiable "soft" form using Gumbel-Softmax relaxation and then discretizes it for inference. Training maximizes calibrated cross-entropy on the label region while masking gold tokens to prevent trivial leakage, with entropy regularization to avoid collapse. A single suffix trained on one model transfers effectively to others, consistently lowering both accuracy and calibrated confidence. Experiments on sentiment analysis, natural language inference, paraphrase detection, commonsense QA, and physical reasoning with Qwen2-1.5B, Phi-1.5, and TinyLlama-1.1B demonstrate consistent attack effectiveness and transfer across tasks and model families.

</details>


### [5] [Universal Adversarial Suffixes for Language Models Using Reinforcement Learning with Calibrated Reward](https://arxiv.org/abs/2512.08131)
*Sampriti Soor,Suklav Ghosh,Arijit Sur*

Main category: cs.CL

> 本研究利用强化学习生成对抗后缀，击败了多种语言模型，并展示了对抗后缀的泛化能力。

<details>
  <summary>Details</summary>

**Motivation:** 以前的方法通常使用梯度搜索或基于规则的方法来找到可以可靠改变预测的短对抗后缀，但这些方法在单一任务或模型上是脆弱的且效果有限。本研究旨在提出一种更稳健且可跨任务、跨模型传递的方法。

**Method:** 使用强化学习框架，将后缀视为策略，使用近端策略优化（PPO）对抗冻结模型作为奖励预言机进行训练。通过校准交叉熵调整奖励，消除标签偏差并聚合表面形式以提高传递性。

**Result:** 在五个不同类型的NLP基准数据集上进行了评估，涵盖了情感分析、自然语言推理、同义句识别和常识推理等多个任务，使用了Qwen2-1.5B Instruct、TinyLlama-1.1B Chat和Phi-1.5这三种不同的语言模型，实验证明了RL训练的后缀能一致地降低准确性，并且比之前类似类型的对抗触发器更有效地跨任务和模型传递。

**Conclusion:** 研究表明，使用强化学习生成的对抗后缀可以更稳健地降低语言模型的预测准确性，并且能够跨任务和模型进行有效传递。

**Abstract:** Language models are vulnerable to short adversarial suffixes that can reliably alter predictions. Previous works usually find such suffixes with gradient search or rule-based methods, but these are brittle and often tied to a single task or model. In this paper, a reinforcement learning framework is used where the suffix is treated as a policy and trained with Proximal Policy Optimization against a frozen model as a reward oracle. Rewards are shaped using calibrated cross-entropy, removing label bias and aggregating across surface forms to improve transferability. The proposed method is evaluated on five diverse NLP benchmark datasets, covering sentiment, natural language inference, paraphrase, and commonsense reasoning, using three distinct language models: Qwen2-1.5B Instruct, TinyLlama-1.1B Chat, and Phi-1.5. Results show that RL-trained suffixes consistently degrade accuracy and transfer more effectively across tasks and models than previous adversarial triggers of similar genres.

</details>


### [6] [ClinicalTrialsHub: Bridging Registries and Literature for Comprehensive Clinical Trial Access](https://arxiv.org/abs/2512.08193)
*Jiwoo Park,Ruoqi Liu,Avani Jagdale,Andrew Srisuwananukorn,Jing Zhao,Lang Li,Ping Zhang,Sachin Kumar*

Main category: cs.CL

> ClinicalTrialsHub 是一个互动搜索平台，通过整合 ClinicalTrials.gov 数据并从 PubMed 论文中提取相关信息，有效提高了结构化临床试验数据的可访问性。

<details>
  <summary>Details</summary>

**Motivation:** 目的是扩大临床试验数据的访问范围，使患者、医生、研究人员和政策制定者更容易获取数据，推动循证医学发展。

**Method:** 通过使用大型语言模型如 GPT-5.1 和 Gemini-3-Pro，平台能自动解析全文研究文章，提取结构化试验信息，并将用户查询转化为结构化的数据库搜索。同时，提供证据支撑的回答系统。

**Result:** 通过用户研究和系统自动评估验证了其信息提取能力和问题回答能力的有效性。

**Conclusion:** ClinicalTrialsHub 通过整合和增强临床试验数据的可访问性，为推动循证医学实践做出了重要贡献。

**Abstract:** We present ClinicalTrialsHub, an interactive search-focused platform that consolidates all data from ClinicalTrials.gov and augments it by automatically extracting and structuring trial-relevant information from PubMed research articles. Our system effectively increases access to structured clinical trial data by 83.8% compared to relying on ClinicalTrials.gov alone, with potential to make access easier for patients, clinicians, researchers, and policymakers, advancing evidence-based medicine. ClinicalTrialsHub uses large language models such as GPT-5.1 and Gemini-3-Pro to enhance accessibility. The platform automatically parses full-text research articles to extract structured trial information, translates user queries into structured database searches, and provides an attributed question-answering system that generates evidence-grounded answers linked to specific source sentences. We demonstrate its utility through a user study involving clinicians, clinical researchers, and PhD students of pharmaceutical sciences and nursing, and a systematic automatic evaluation of its information extraction and question answering capabilities.

</details>


### [7] [Are generative AI text annotations systematically biased?](https://arxiv.org/abs/2512.08404)
*Sjoerd B. Stolwijk,Mark Boukes,Damian Trilling*

Main category: cs.CL

> 该研究通过对Boukes (2024)的手动注释进行概念性复制来调查GLLM注释中的偏差。使用了五种概念（政治内容、互动性、合理性、不文明和意识形态）和五种不同的提示词，对四种GLLMs（Llama3.1:8b、Llama3.3:70b、GPT4o、Qwen2.5:72b）进行了评估。发现尽管GLLMs在F1分数方面表现良好，但它们在普遍性上与手动注释不同，产生实质性的不同下游结果，并显示出系统性偏差，即它们彼此之间的一致性高于与手动注释的一致性。F1分数的差异无法解释偏差程度。

<details>
  <summary>Details</summary>

**Motivation:** 了解GLLM在标注任务中的系统性偏差，以便改进标注质量和减少偏差。

**Method:** 概念性复制了Boukes (2024)的手动标注过程，使用五种概念和不同GLLMs及提示词进行评估。

**Result:** GLLMs在F1分数上表现出良好的性能，但存在显著的信息偏见，导致与手动标注结果有系统性差异。

**Conclusion:** 尽管GLLMs在F1分数方面表现出良好的性能，但其与手动标注的一致性和偏差水平存在显著的不一致。

**Abstract:** This paper investigates bias in GLLM annotations by conceptually replicating manual annotations of Boukes (2024). Using various GLLMs (Llama3.1:8b, Llama3.3:70b, GPT4o, Qwen2.5:72b) in combination with five different prompts for five concepts (political content, interactivity, rationality, incivility, and ideology). We find GLLMs perform adequate in terms of F1 scores, but differ from manual annotations in terms of prevalence, yield substantively different downstream results, and display systematic bias in that they overlap more with each other than with manual annotations. Differences in F1 scores fail to account for the degree of bias.

</details>


### [8] [What Triggers my Model? Contrastive Explanations Inform Gender Choices by Translation Models](https://arxiv.org/abs/2512.08440)
*Janiça Hackenbuchner,Arda Tezcan,Joke Daems*

Main category: cs.CL

> 研究通过对比解释和特征归属的方法，分析性别中立的自然源数据，探索输入标记如何影响翻译中性别词形的选择，展示了模型与人类性别感知的重叠，并强调了利用这些信息减少性别偏差的相关性。

<details>
  <summary>Details</summary>

**Motivation:** 旨在超越仅仅测量偏差，探索大型语言模型和机器翻译中性别偏差的根源，并深入理解这些模型的决策过程，特别是与性别相关的决策。

**Method:** 采用对比解释和计算特征归属的方法，来分析源句子中的哪些上下文（输入标记）影响或触发了翻译模型对目标语言中特定性别词形的选择。

**Result:** 研究表明，模型选择的性别词形与人类对性别的感知之间存在显著的重叠，并提供了对显著源词的语义分析。

**Conclusion:** 研究展示了理解性别翻译决策的模型在性别方面的相关性，对比了模型与人类的决定，并强调了使用此信息来减少性别偏见的重要性。

**Abstract:** Interpretability can be implemented as a means to understand decisions taken by (black box) models, such as machine translation (MT) or large language models (LLMs). Yet, research in this area has been limited in relation to a manifested problem in these models: gender bias. With this research, we aim to move away from simply measuring bias to exploring its origins. Working with gender-ambiguous natural source data, this study examines which context, in the form of input tokens in the source sentence, influences (or triggers) the translation model choice of a certain gender inflection in the target language. To analyse this, we use contrastive explanations and compute saliency attribution. We first address the challenge of a lacking scoring threshold and specifically examine different attribution levels of source words on the model gender decisions in the translation. We compare salient source words with human perceptions of gender and demonstrate a noticeable overlap between human perceptions and model attribution. Additionally, we provide a linguistic analysis of salient words. Our work showcases the relevance of understanding model translation decisions in terms of gender, how this compares to human decisions and that this information should be leveraged to mitigate gender bias.

</details>


### [9] [Soft Inductive Bias Approach via Explicit Reasoning Perspectives in Inappropriate Utterance Detection Using Large Language Models](https://arxiv.org/abs/2512.08480)
*Ju-Young Kim,Ji-Hong Park,Se-Yeon Lee,Sujin Park,Gun-Woo Kim*

Main category: cs.CL

> 研究提出了一种软归纳偏差方法来指导韩语大模型推理，应用于检测不当言论，显示了比传统监督学习方法更优的精准性和一致性。

<details>
  <summary>Details</summary>

**Motivation:** 由于在线游戏中匿名环境下不当言论的频繁升级，研究能够检测对话文本中不当言论的技术越来越迫切，以帮助构建更安全的交流环境。

**Method:** 本研究提出了一种软归纳偏差方法，明确界定了推理视角以引导推理过程，促进理性决策，防止推理过程中的错误。我们使用这种方法对韩语大型语言模型进行了微调，并进行了定量和定性的评估。

**Result:** 实验结果显示，Kanana-1.5 模型的平均准确率为 87.0046%，比标准监督学习提高了大约 3.89 个百分点。

**Conclusion:** 研究发现表明，所提出的方法超越了大型语言模型简单的知识模仿，通过限制推理视角实现了更准确和一致的判断，证明了其对于检测不当言论的有效性。

**Abstract:** Recent incidents in certain online games and communities, where anonymity is guaranteed, show that unchecked inappropriate remarks frequently escalate into verbal abuse and even criminal behavior, raising significant social concerns. Consequently, there is a growing need for research on techniques that can detect inappropriate utterances within conversational texts to help build a safer communication environment. Although large-scale language models trained on Korean corpora and chain-of-thought reasoning have recently gained attention, research applying these approaches to inappropriate utterance detection remains limited. In this study, we propose a soft inductive bias approach that explicitly defines reasoning perspectives to guide the inference process, thereby promoting rational decision-making and preventing errors that may arise during reasoning. We fine-tune a Korean large language model using the proposed method and conduct both quantitative performance comparisons and qualitative evaluations across different training strategies. Experimental results show that the Kanana-1.5 model achieves an average accuracy of 87.0046, improving by approximately 3.89 percent over standard supervised learning. These findings indicate that the proposed method goes beyond simple knowledge imitation by large language models and enables more precise and consistent judgments through constrained reasoning perspectives, demonstrating its effectiveness for inappropriate utterance detection.

</details>


### [10] [Curriculum Guided Massive Multi Agent System Solving For Robust Long Horizon Tasks](https://arxiv.org/abs/2512.08545)
*Indrajit Kar,Kalathur Chenchu Kishore Kumar*

Main category: cs.CL

> 研究提出了一种基于分布的轻量级智能体的分层多智能体架构，旨在解决长时间推理任务和减少计算成本的问题。

<details>
  <summary>Details</summary>

**Motivation:** 大型语言模型和多智能体系统在分解复杂任务方面显示出了潜力，但在处理长时间推理任务和不断增加的计算成本方面存在问题。

**Method:** 本研究提出了一种分层多智能体架构，该架构基于64*64网格分布轻量级智能体，并通过选择性预言机提供支持。一种空间课程教学方法逐渐扩展网格的操作区域，确保智能体先掌握更容易的中心任务，再处理更难的周边任务。为了提高可靠性，该系统将负对数似然（NLL）作为衡量信心的指标，使得课程教学能优先选择智能体既准确又有良好校准的区域。课程教学管理器使用汤普森采样在基于能力和NLL奖励信号的基础上自适应地选择训练区域。

**Result:** 研究方法在空间定向的汉诺塔基准测试中的评估展示了提高的稳定性，减少了预言机的使用，以及从分布式智能体合作中获得更强大的长距离推理能力。

**Conclusion:** 该分层多智能体架构通过集成自适应课程教学和信任度量，如NLL，有效地提升了长时间推理任务中的智能体表现及减少了计算资源的消耗。

**Abstract:** Large Language Models and multi-agent systems have shown promise in decomposing complex tasks, yet they struggle with long-horizon reasoning tasks and escalating computation cost. This work introduces a hierarchical multi-agent architecture that distributes reasoning across a 64*64 grid of lightweight agents, supported by a selective oracle. A spatial curriculum progressively expands the operational region of the grid, ensuring that agents master easier central tasks before tackling harder peripheral ones. To improve reliability, the system integrates Negative Log-Likelihood as a measure of confidence, allowing the curriculum to prioritize regions where agents are both accurate and well calibrated. A Thompson Sampling curriculum manager adaptively chooses training zones based on competence and NLL-driven reward signals. We evaluate the approach on a spatially grounded Tower of Hanoi benchmark, which mirrors the long-horizon structure of many robotic manipulation and planning tasks. Results demonstrate improved stability, reduced oracle usage, and stronger long-range reasoning from distributed agent cooperation.

</details>


### [11] [HealthcareNLP: where are we and what is next?](https://arxiv.org/abs/2512.08617)
*Lifeng Han,Paul Rayson,Suzan Verberne,Andrew Moore,Goran Nenadic*

Main category: cs.CL

> 该教程面向医疗NLP领域，概述了应用现状与未来方向，包含合成数据生成、可解释NLP以及患者-资源导向等层次的介绍，并提供实践环节。

<details>
  <summary>Details</summary>

**Motivation:** 现有的医疗NLP领域综述要么忽视了一些关键任务，例如为解决隐私问题而生成合成数据，或者是可解释的临床NLP，要么遗漏了重要的方法论如检索增强生成和神经符号LLMs与知识图谱的融合。因此，本教程的目的是展望具有层级结构的医疗NLP的最重要子领域。

**Method:** 本教程专注于医疗领域的自然语言处理（NLP）应用，涵盖了已取得的成果以及未来面临的挑战。它分为三层结构：数据/资源层：标注准则、伦理批准、治理和合成数据；NLP评估层：命名实体识别（NER）、关系抽取（RE）、情感分析等任务及可解释方法；患者层面：患者公共参与及互动（PPIE）、健康素养、简化转译及摘要化等NLP任务，以及共享决策支持。

**Result:** 本教程以介绍性的方式概述了医疗NLP的重要部分，并没有具体的结果部分，但提供了一个实用性的实践环节，以便参与者能够使用医疗NLP应用程序。

**Conclusion:** 本教程适合NLP医疗应用领域的从业人员、对医疗应用领域感兴趣的NLP研究人员、医疗研究人员和NLP领域的学生。此外，本教程没有先修知识要求。

**Abstract:** This proposed tutorial focuses on Healthcare Domain Applications of NLP, what we have achieved around HealthcareNLP, and the challenges that lie ahead for the future. Existing reviews in this domain either overlook some important tasks, such as synthetic data generation for addressing privacy concerns, or explainable clinical NLP for improved integration and implementation, or fail to mention important methodologies, including retrieval augmented generation and the neural symbolic integration of LLMs and KGs. In light of this, the goal of this tutorial is to provide an introductory overview of the most important sub-areas of a patient- and resource-oriented HealthcareNLP, with three layers of hierarchy: data/resource layer: annotation guidelines, ethical approvals, governance, synthetic data; NLP-Eval layer: NLP tasks such as NER, RE, sentiment analysis, and linking/coding with categorised methods, leading to explainable HealthAI; patients layer: Patient Public Involvement and Engagement (PPIE), health literacy, translation, simplification, and summarisation (also NLP tasks), and shared decision-making support. A hands-on session will be included in the tutorial for the audience to use HealthcareNLP applications. The target audience includes NLP practitioners in the healthcare application domain, NLP researchers who are interested in domain applications, healthcare researchers, and students from NLP fields. The type of tutorial is "Introductory to CL/NLP topics (HealthcareNLP)" and the audience does not need prior knowledge to attend this. Tutorial materials: https://github.com/4dpicture/HealthNLP

</details>


### [12] [QSTN: A Modular Framework for Robust Questionnaire Inference with Large Language Models](https://arxiv.org/abs/2512.08646)
*Maximilian Kreutner,Jens Rupprecht,Georg Ahnert,Ahmed Salem,Markus Strohmaier*

Main category: cs.CL

> QSTN 是一个开源的 Python 框架，用于生成问卷调查的响应，支持使用大型语言模型进行模拟调查和注释任务，展示了问题结构和响应生成方法对生成的调查响应和人类答案的对齐有显著影响，并提供了无需编码知识即可设置健实验的非代码用户界面。

<details>
  <summary>Details</summary>

**Motivation:** 作者旨在提供一种方法，使用大型语言模型以高效率和低成本的方式生成问卷调查的响应，同时促进该领域研究的可重复性和可靠性。

**Method:** QSTN 框架通过结构化的问卷问题生成响应，并允许对问题结构和响应生成方法进行广泛的评估。

**Result:** 实验结果表明，问题结构和响应生成方法显著影响了生成的调查响应与人类答案的对齐，并且这些响应可以通过较少的计算成本获得。

**Conclusion:** QSTN 为研究者提供了一个无需编码知识就能进行大型语言模型实验的工具，对未来基于大型语言模型的研究的可重复性和可靠性提供了支持。

**Abstract:** We introduce QSTN, an open-source Python framework for systematically generating responses from questionnaire-style prompts to support in-silico surveys and annotation tasks with large language models (LLMs). QSTN enables robust evaluation of questionnaire presentation, prompt perturbations, and response generation methods. Our extensive evaluation ($>40 $ million survey responses) shows that question structure and response generation methods have a significant impact on the alignment of generated survey responses with human answers, and can be obtained for a fraction of the compute cost. In addition, we offer a no-code user interface that allows researchers to set up robust experiments with LLMs without coding knowledge. We hope that QSTN will support the reproducibility and reliability of LLM-based research in the future.

</details>


### [13] [An Agentic AI System for Multi-Framework Communication Coding](https://arxiv.org/abs/2512.08659)
*Bohao Yang,Rui Yang,Joshua M. Biro,Haoyuan Wang,Jessica L. Handley,Brianna Richardson,Sophia Bessias,Nicoleta Economou-Zavlanos,Armando D. Bedoya,Monica Agrawal,Michael M. Zavlanos,Anand Chowdhury,Raj M. Ratwani,Kai Sun,Kathryn I. Pollak,Michael J. Pencina,Chuan Hong*

Main category: cs.CL

> 研究开发了一种名为MOSAIC的多框架结构代理AI系统，用于临床沟通，该系统在测试集中实现了总体F1得分为0.928，表现出在风湿病领域最佳性能（F1 = 0.962）。

<details>
  <summary>Details</summary>

**Motivation:** 为了克服临床沟通中人工标注耗费劳动力、不一致且难以扩展的问题，以及现有方法基于大语言模型时缺乏适应性、解释性和可靠性的问题，尤其是在应用于各种沟通框架和临床领域时。

**Method:** 采用了基于LangGraph架构的多框架结构代理AI系统MOSAIC，该系统由四个核心代理组成，包括计划代理、更新代理、注释代理和验证代理。注释代理使用代码本引导的检索增强生成(RAG)和动态少样本提示方法。验证代理提供一致性和反馈检查。

**Result:** MOSAIC系统在测试集中实现了总体F1得分为0.928。系统在风湿病领域表现最佳（F1 = 0.962），尤其在患者行为方面表现优异。

**Conclusion:** 通过与人工标注比较，MOSAIC系统展示了在临床沟通领域中强大的性能和优越性。系统通过其先进的结构和代理组合，能够有效处理临床沟通中的各种挑战。通过消融实验，MOSAIC证明其优于基线模型。

**Abstract:** Clinical communication is central to patient outcomes, yet large-scale human annotation of patient-provider conversation remains labor-intensive, inconsistent, and difficult to scale. Existing approaches based on large language models typically rely on single-task models that lack adaptability, interpretability, and reliability, especially when applied across various communication frameworks and clinical domains. In this study, we developed a Multi-framework Structured Agentic AI system for Clinical Communication (MOSAIC), built on a LangGraph-based architecture that orchestrates four core agents, including a Plan Agent for codebook selection and workflow planning, an Update Agent for maintaining up-to-date retrieval databases, a set of Annotation Agents that applies codebook-guided retrieval-augmented generation (RAG) with dynamic few-shot prompting, and a Verification Agent that provides consistency checks and feedback. To evaluate performance, we compared MOSAIC outputs against gold-standard annotations created by trained human coders. We developed and evaluated MOSAIC using 26 gold standard annotated transcripts for training and 50 transcripts for testing, spanning rheumatology and OB/GYN domains. On the test set, MOSAIC achieved an overall F1 score of 0.928. Performance was highest in the Rheumatology subset (F1 = 0.962) and strongest for Patient Behavior (e.g., patients asking questions, expressing preferences, or showing assertiveness). Ablations revealed that MOSAIC outperforms baseline benchmarking.

</details>


### [14] [Automatic Essay Scoring and Feedback Generation in Basque Language Learning](https://arxiv.org/abs/2512.08713)
*Ekhi Azurmendi,Xabier Arregi,Oier Lopez de Lacalle*

Main category: cs.CL

> The paper presents a novel dataset and fine-tuned models for AES and feedback generation in Basque, showcasing superior performance in scoring consistency and feedback quality compared to closed-source models.

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind this paper is to provide the first publicly available dataset for AES with feedback in Basque, aimed at the CEFR C1 level. This fills a gap in research for low-resource languages, promoting transparency, reproducibility, and educationally grounded Natural Language Processing (NLP) research.

**Method:** This paper introduces a new dataset for Automatic Essay Scoring (AES) and feedback generation in Basque. The dataset consists of 3,200 essays annotated by experts. The models used are RoBERTa-EusCrawl and Latxa 8B/70B, fine-tuned for AES and feedback generation. A novel evaluation methodology is also proposed to assess feedback quality.

**Result:** Experiments show that encoder models are reliable for AES, with Latxa's supervised fine-tuning outperforming proprietary models like GPT-5 and Claude Sonnet 4.5 in terms of scoring consistency and feedback quality. The fine-tuned Latxa model can produce pedagogically meaningful feedback, identifying a broader range of error types.

**Conclusion:** This resource and benchmark set a foundation for further transparent and reproducible NLP research in low-resource languages such as Basque, emphasizing the importance of educational grounding and the potential of open-source models in enhancing AES systems.

**Abstract:** This paper introduces the first publicly available dataset for Automatic Essay Scoring (AES) and feedback generation in Basque, targeting the CEFR C1 proficiency level. The dataset comprises 3,200 essays from HABE, each annotated by expert evaluators with criterion specific scores covering correctness, richness, coherence, cohesion, and task alignment enriched with detailed feedback and error examples. We fine-tune open-source models, including RoBERTa-EusCrawl and Latxa 8B/70B, for both scoring and explanation generation. Our experiments show that encoder models remain highly reliable for AES, while supervised fine-tuning (SFT) of Latxa significantly enhances performance, surpassing state-of-the-art (SoTA) closed-source systems such as GPT-5 and Claude Sonnet 4.5 in scoring consistency and feedback quality. We also propose a novel evaluation methodology for assessing feedback generation, combining automatic consistency metrics with expert-based validation of extracted learner errors. Results demonstrate that the fine-tuned Latxa model produces criterion-aligned, pedagogically meaningful feedback and identifies a wider range of error types than proprietary models. This resource and benchmark establish a foundation for transparent, reproducible, and educationally grounded NLP research in low-resource languages such as Basque.

</details>


### [15] [Fluent Alignment with Disfluent Judges: Post-training for Lower-resource Languages](https://arxiv.org/abs/2512.08777)
*David Samuel,Lilja Øvrelid,Erik Velldal,Andrey Kutuzov*

Main category: cs.CL

> 本研究提出了一种针对低资源语言的后训练方法，可以在使用不流畅奖励模型对齐的情况下依然保持语言模型的流畅性。研究使用了基于策略的训练方法，并通过挪威语布克莫尔语的案例研究表明这种方法在没有依赖难以获取的数据的情况下超越了其它常见方法。

<details>
  <summary>Details</summary>

**Motivation:** 虽然偏好优化研究已经很成熟，但大多数研究集中于英语和中文。低资源语言缺乏由母语者编写的语料库以及能够生成流畅合成数据的语言模型。因此，本研究侧重于不依赖任何目标语言的指令调优数据，开发流畅通偏好对齐的语言模型。

**Method:** 研究采用基于策略的训练方法来进行语言模型的优化，并将该方法与常用方法（监督微调和多语言微调）进行了对比。

**Result:** 通过挪威语布克莫尔语的案例研究，结果表明基于策略的训练方法对于提升语言流畅性至关重要，并且该方法的表现优于其它对比方法。

**Conclusion:** 没有依赖任何难以获取的数据，研究表明基于策略训练方法可以有效提升低资源语言模型的流畅性。

**Abstract:** We propose a post-training method for lower-resource languages that preserves fluency of language models even when aligned by disfluent reward models. Preference-optimization is now a well-researched topic, but previous work has mostly addressed models for English and Chinese. Lower-resource languages lack both datasets written by native speakers and language models capable of generating fluent synthetic data. Thus, in this work, we focus on developing a fluent preference-aligned language model without any instruction-tuning data in the target language. Our approach uses an on-policy training method, which we compare with two common approaches: supervised finetuning on machine-translated data and multilingual finetuning. We conduct a case study on Norwegian Bokmål and evaluate fluency through native-speaker assessments. The results show that the on-policy aspect is crucial and outperforms the alternatives without relying on any hard-to-obtain data.

</details>


### [16] [A Systematic Evaluation of Preference Aggregation in Federated RLHF for Pluralistic Alignment of LLMs](https://arxiv.org/abs/2512.08786)
*Mahmoud Srewa,Tianyu Zhao,Salma Elmalaki*

Main category: cs.CL

> 该论文提出了一种新的评估框架，用于在联邦学习环境中评估大型语言模型（LLMs）与人类偏好的对齐度及公平性，并提出了一种适应性方案来改进对齐质量。

<details>
  <summary>Details</summary>

**Motivation:** 标准方法在联邦学习环境中难以充分表示多样性观点，因此需要一个系统来评估不同的聚合策略在对齐质量和公平性之间的权衡。

**Method:** 提出了一种新的适应性聚合策略，它基于历史对齐性能动态调整偏好权重，测试了包括最小值、最大值和平均值在内的标准奖赏聚合技术。

**Result:** 实验显示，适应性方法在保持竞争力对齐得分的同时，一致性地实现了更好的公平性。

**Conclusion:** 这项工作为评估LLMs行为提供了一个强大的方法，并提出了一种实用的解决方案，即开发真正多样化和公平对齐的模型。

**Abstract:** This paper addresses the challenge of aligning large language models (LLMs) with diverse human preferences within federated learning (FL) environments, where standard methods often fail to adequately represent diverse viewpoints. We introduce a comprehensive evaluation framework that systematically assesses the trade-off between alignment quality and fairness when using different aggregation strategies for human preferences. In our federated setting, each group locally evaluates rollouts and produces reward signals, and the server aggregates these group-level rewards without accessing any raw data. Specifically, we evaluate standard reward aggregation techniques (min, max, and average) and introduce a novel adaptive scheme that dynamically adjusts preference weights based on a group's historical alignment performance. Our experiments on question-answering (Q/A) tasks using a PPO-based RLHF pipeline demonstrate that our adaptive approach consistently achieves superior fairness while maintaining competitive alignment scores. This work offers a robust methodology for evaluating LLM behavior across diverse populations and provides a practical solution for developing truly pluralistic and fairly aligned models.

</details>


### [17] [Ask, Answer, and Detect: Role-Playing LLMs for Personality Detection with Question-Conditioned Mixture-of-Experts](https://arxiv.org/abs/2512.08814)
*Yifan Lyu,Liang Zhang*

Main category: cs.CL

> ROME framework enhances personality detection by leveraging large language models to simulate responses to psychometric questionnaires, generating rich intermediate supervision and improving text-to-personality mapping.

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind the ROME framework is to enhance the accuracy and interpretability of personality detection by addressing label scarcity and under-specified semantic mappings in existing models.

**Method:** ROME simulates user responses to validated psychometric questionnaires using large language models' role-play capability and uses a question-conditioned Mixture-of-Experts module to route predictions.

**Result:** Experimental results on real-world datasets show consistent improvement over state-of-the-art baselines, with a significant 15.41% improvement on the Kaggle dataset.

**Conclusion:** The introduction of psychological knowledge in personality detection through ROME demonstrates superior performance and offers a new approach for enhancing model interpretability and supervision.

**Abstract:** Understanding human personality is crucial for web applications such as personalized recommendation and mental health assessment. Existing studies on personality detection predominantly adopt a "posts -> user vector -> labels" modeling paradigm, which encodes social media posts into user representations for predicting personality labels (e.g., MBTI labels). While recent advances in large language models (LLMs) have improved text encoding capacities, these approaches remain constrained by limited supervision signals due to label scarcity, and under-specified semantic mappings between user language and abstract psychological constructs. We address these challenges by proposing ROME, a novel framework that explicitly injects psychological knowledge into personality detection. Inspired by standardized self-assessment tests, ROME leverages LLMs' role-play capability to simulate user responses to validated psychometric questionnaires. These generated question-level answers transform free-form user posts into interpretable, questionnaire-grounded evidence linking linguistic cues to personality labels, thereby providing rich intermediate supervision to mitigate label scarcity while offering a semantic reasoning chain that guides and simplifies the text-to-personality mapping learning. A question-conditioned Mixture-of-Experts module then jointly routes over post and question representations, learning to answer questionnaire items under explicit supervision. The predicted answers are summarized into an interpretable answer vector and fused with the user representation for final prediction within a multi-task learning framework, where question answering serves as a powerful auxiliary task for personality detection. Extensive experiments on two real-world datasets demonstrate that ROME consistently outperforms state-of-the-art baselines, achieving improvements (15.41% on Kaggle dataset).

</details>


### [18] [Do Depth-Grown Models Overcome the Curse of Depth? An In-Depth Analysis](https://arxiv.org/abs/2512.08819)
*Ferdinand Kapl,Emmanouil Angelis,Tobias Höppe,Kaitlin Maile,Johannes von Oswald,Nino Scherrer,Stefan Bauer*

Main category: cs.CL

> 研究揭示了逐渐增加Transformer模型深度在训练中的优势，不仅降低了训练成本，还提高了推理性能。通过深度分析，证明了逐渐中间堆叠增长提高了模型深度的有效利用，调整了残差流结构，并促进了可互换计算块的形成。提出了对MIDAS的轻量级改进，在下游推理基准测试中取得了进一步的改进。

<details>
  <summary>Details</summary>

**Motivation:** 缺乏对MIDAS方法提高推理性能的机制理解，研究旨在建立与深度利用低效问题的联系，并揭示逐渐增加模型深度的优势。

**Method:** 采用深度分析，通过研究逐层分析，探讨逐渐中间堆叠增长如何影响模型深度的有效利用、残差流结构以及促进计算块的形成。

**Result:** 证明了逐渐增加模型深度可以更有效地利用模型深度，改变残差流结构，促进形成可互换计算块，并提出了轻量级改进以提高推理基准测试的结果。

**Conclusion:** 指出逐渐增加模型深度可形成不同的计算电路，解决标准固定深度模型的深度利用有限的问题。

**Abstract:** Gradually growing the depth of Transformers during training can not only reduce training cost but also lead to improved reasoning performance, as shown by MIDAS (Saunshi et al., 2024). Thus far, however, a mechanistic understanding of these gains has been missing. In this work, we establish a connection to recent work showing that layers in the second half of non-grown, pre-layernorm Transformers contribute much less to the final output distribution than those in the first half - also known as the Curse of Depth (Sun et al., 2025, Csordás et al., 2025). Using depth-wise analyses, we demonstrate that growth via gradual middle stacking yields more effective utilization of model depth, alters the residual stream structure, and facilitates the formation of permutable computational blocks. In addition, we propose a lightweight modification of MIDAS that yields further improvements in downstream reasoning benchmarks. Overall, this work highlights how the gradual growth of model depth can lead to the formation of distinct computational circuits and overcome the limited depth utilization seen in standard non-grown models.

</details>


### [19] [Toward Faithful Retrieval-Augmented Generation with Sparse Autoencoders](https://arxiv.org/abs/2512.08892)
*Guangzhi Xiong,Zhenghao He,Bohan Liu,Sanchit Sinha,Aidong Zhang*

Main category: cs.CL

> 通过使用稀疏自编码器，研究提出了RAGLens来解决现有的用于RAG的幻觉检测方法存在的问题，可以准确检测和解释不忠实的RAG输出。

<details>
  <summary>Details</summary>

**Motivation:** 受最近机制可解释性的进展启发，动机在于解决现有的用于RAG的幻觉检测方法依赖大规模检测器训练或查询外部LLM评判者存在的问题。

**Method:** 通过使用稀疏自编码器（SAEs）来解缠内部激活，以识别出在RAG幻觉期间特别触发的特征，从而引入了RAGLens，这是一种使用LLM内部表示来准确标记不忠实的RAG输出的轻量级幻觉检测器。

**Result:** RAGLens不仅在检测性能上优于现有方法，而且还提供了对其决策的解释性原因，从而能够有效缓解不忠实的RAG。

**Conclusion:** 本研究展示了利用内部表示对RAG幻觉进行准确检测和提供解释性原因的可能性，并有可能在提高LLM输出忠实度方面产生新的见解。代码可在https://github.com/Teddy-XiongGZ/RAGLens获得。

**Abstract:** Retrieval-Augmented Generation (RAG) improves the factuality of large language models (LLMs) by grounding outputs in retrieved evidence, but faithfulness failures, where generations contradict or extend beyond the provided sources, remain a critical challenge. Existing hallucination detection methods for RAG often rely either on large-scale detector training, which requires substantial annotated data, or on querying external LLM judges, which leads to high inference costs. Although some approaches attempt to leverage internal representations of LLMs for hallucination detection, their accuracy remains limited. Motivated by recent advances in mechanistic interpretability, we employ sparse autoencoders (SAEs) to disentangle internal activations, successfully identifying features that are specifically triggered during RAG hallucinations. Building on a systematic pipeline of information-based feature selection and additive feature modeling, we introduce RAGLens, a lightweight hallucination detector that accurately flags unfaithful RAG outputs using LLM internal representations. RAGLens not only achieves superior detection performance compared to existing methods, but also provides interpretable rationales for its decisions, enabling effective post-hoc mitigation of unfaithful RAG. Finally, we justify our design choices and reveal new insights into the distribution of hallucination-related signals within LLMs. The code is available at https://github.com/Teddy-XiongGZ/RAGLens.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [20] [Detection of Cyberbullying in GIF using AI](https://arxiv.org/abs/2512.07838)
*Pal Dave,Xiaohong Yuan,Madhuri Siddula,Kaushik Roy*

Main category: cs.CV

> 研究者收集了一个包含4100多个涉及网络欺凌和非网络欺凌的GIF数据集，并使用VGG16深度学习模型检测到了97%的准确性。

<details>
  <summary>Details</summary>

**Motivation:** 虽然有许多研究集中在文本数据上的网络欺凌检测，但很少有研究关注如何在GIF/贴纸中检测网络欺凌。这项研究旨在填补这一空白。

**Method:** 通过从Twitter中提取与网络欺凌相关的标签，并使用GIPHY公开API下载GIF文件，收集了一个包含4100多个GIF的网络欺凌数据集。使用预训练的VGG16深度学习模型进行检测。

**Result:** 研究中使用的深度学习模型达到了97%的准确率。

**Conclusion:** 这项工作不仅提供了一个有价值的GIF数据集供相关领域的研究人员使用，而且证明了在GIF中检测网络欺凌问题是可行的。

**Abstract:** Cyberbullying is a well-known social issue, and it is escalating day by day. Due to the vigorous development of the internet, social media provide many different ways for the user to express their opinions and exchange information. Cyberbullying occurs on social media using text messages, comments, sharing images and GIFs or stickers, and audio and video. Much research has been done to detect cyberbullying on textual data; some are available for images. Very few studies are available to detect cyberbullying on GIFs/stickers. We collect a GIF dataset from Twitter and Applied a deep learning model to detect cyberbullying from the dataset. Firstly, we extracted hashtags related to cyberbullying using Twitter. We used these hashtags to download GIF file using publicly available API GIPHY. We collected over 4100 GIFs including cyberbullying and non cyberbullying. we applied deep learning pre-trained model VGG16 for the detection of the cyberbullying. The deep learning model achieved the accuracy of 97%. Our work provides the GIF dataset for researchers working in this area.

</details>


### [21] [Near-real time fires detection using satellite imagery in Sudan conflict](https://arxiv.org/abs/2512.07925)
*Kuldip Singh Atwal,Dieter Pfoser,Daniel Rothbart*

Main category: cs.CV

> 使用Planet Labs的4波段影像和深度学习模型，对苏丹武装冲突中的火灾损害进行近乎实时的监测，展示了相比基线，自动化方法能更准确地捕捉活跃火灾和被烧毁区域。结果表明，使用8波段或时间序列影像仅带来微小改进。

<details>
  <summary>Details</summary>

**Motivation:** 苏丹持续战争带来的挑战凸显了需要快速监测和分析此类冲突的需求。

**Method:** 利用Planet Labs的4波段卫星影像和深度学习模型。

**Result:** 在苏丹的五个案例研究中，展示了自动化方法能更准确地捕捉活跃火灾和被烧毁区域。然而，使用8波段或时间序列影像仅带来微小改进。

**Conclusion:** 深学习结合卫星影像提供了实时监测冲突火灾损害的有效手段，但更高分辨率影像的提升效果有限。

**Abstract:** The challenges of ongoing war in Sudan highlight the need for rapid monitoring and analysis of such conflicts. Advances in deep learning and readily available satellite remote sensing imagery allow for near real-time monitoring. This paper uses 4-band imagery from Planet Labs with a deep learning model to show that fire damage in armed conflicts can be monitored with minimal delay. We demonstrate the effectiveness of our approach using five case studies in Sudan. We show that, compared to a baseline, the automated method captures the active fires and charred areas more accurately. Our results indicate that using 8-band imagery or time series of such imagery only result in marginal gains.

</details>


### [22] [Preserving Source Video Realism: High-Fidelity Face Swapping for Cinematic Quality](https://arxiv.org/abs/2512.07951)
*Zekai Luo,Zongze Du,Zhouhang Zhu,Hao Zhong,Muzhi Zhu,Wen Wang,Yuling Xi,Chenchen Jing,Hao Chen,Chunhua Shen*

Main category: cs.CV

> 本文提出了LivingSwap，一个使用关键帧引导的视频脸部交换模型，能够实现高质量的脸部交换效果，并在长时间视频中保持身份的一致性。

<details>
  <summary>Details</summary>

**Motivation:** 文章旨在解决在长时间和复杂的视频序列中实现面部替换的高保真度和时间一致性这一重大挑战。

**Method:** 通过使用关键帧作为条件信号来注入目标身份，并结合视频参考引导来进行时间拼接，LivingSwap模型能够在长时间视频序列中稳定地保持身份并实现高质量的重建。为了应对用于参考引导训练的数据稀缺问题，该研究还构建了一个配对的脸部交换数据集Face2Face，并通过反转数据对来确保可靠的地面真实监督。

**Result:** 实验结果表明该方法能够以较低的人工干预实现目标身份与源视频的表情、光照和动作的无缝整合，并达到最先进的水平。

**Conclusion:** LivingSwap模型成功地利用关键帧和视频参考引导技术提升了视频脸部替换的保真度和时间一致性，显著减少了生产工作流程中的手动工作量。

**Abstract:** Video face swapping is crucial in film and entertainment production, where achieving high fidelity and temporal consistency over long and complex video sequences remains a significant challenge. Inspired by recent advances in reference-guided image editing, we explore whether rich visual attributes from source videos can be similarly leveraged to enhance both fidelity and temporal coherence in video face swapping. Building on this insight, this work presents LivingSwap, the first video reference guided face swapping model. Our approach employs keyframes as conditioning signals to inject the target identity, enabling flexible and controllable editing. By combining keyframe conditioning with video reference guidance, the model performs temporal stitching to ensure stable identity preservation and high-fidelity reconstruction across long video sequences. To address the scarcity of data for reference-guided training, we construct a paired face-swapping dataset, Face2Face, and further reverse the data pairs to ensure reliable ground-truth supervision. Extensive experiments demonstrate that our method achieves state-of-the-art results, seamlessly integrating the target identity with the source video's expressions, lighting, and motion, while significantly reducing manual effort in production workflows. Project webpage: https://aim-uofa.github.io/LivingSwap

</details>


### [23] [Restrictive Hierarchical Semantic Segmentation for Stratified Tooth Layer Detection](https://arxiv.org/abs/2512.07984)
*Ryan Banks,Camila Lindoni Azevedo,Hongying Tang,Yunpeng Li*

Main category: cs.CV

> Developed an explicit hierarchical framework for better semantic segmentation of dental anatomical structures, validated on a new dataset TL-pano, demonstrating improved performance especially with less data.

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of existing hierarchy-aware segmentation methods that rely on loss functions for encoding anatomical structures, providing weak and indirect supervision.

**Method:** A general framework that embeds an explicit anatomical hierarchy into semantic segmentation by coupling a recurrent, level-wise prediction scheme with restrictive output heads and top-down feature conditioning. The framework re-runs the backbone on the original image concatenated with logits from the previous level. Child class features are conditioned using Feature-wise Linear Modulation of their parent class probabilities, while a probabilistic composition rule enforces parent-child consistency.

**Result:** Validated on the TL-pano dataset with 194 panoramic radiographs, hierarchical variants consistently increase IoU, Dice, and recall for fine-grained anatomies, leading to more anatomically coherent masks, though with increased false positives.

**Conclusion:** The explicit hierarchical structuring improves both performance metrics and clinical plausibility, especially in low data dental imaging scenarios.

**Abstract:** Accurate understanding of anatomical structures is essential for reliably staging certain dental diseases. A way of introducing this within semantic segmentation models is by utilising hierarchy-aware methodologies. However, existing hierarchy-aware segmentation methods largely encode anatomical structure through the loss functions, providing weak and indirect supervision. We introduce a general framework that embeds an explicit anatomical hierarchy into semantic segmentation by coupling a recurrent, level-wise prediction scheme with restrictive output heads and top-down feature conditioning. At each depth of the class tree, the backbone is re-run on the original image concatenated with logits from the previous level. Child class features are conditioned using Feature-wise Linear Modulation of their parent class probabilities, to modulate child feature spaces for fine grained detection. A probabilistic composition rule enforces consistency between parent and descendant classes. Hierarchical loss combines per-level class weighted Dice and cross entropy loss and a consistency term loss, ensuring parent predictions are the sum of their children. We validate our approach on our proposed dataset, TL-pano, containing 194 panoramic radiographs with dense instance and semantic segmentation annotations, of tooth layers and alveolar bone. Utilising UNet and HRNet as donor models across a 5-fold cross validation scheme, the hierarchical variants consistently increase IoU, Dice, and recall, particularly for fine-grained anatomies, and produce more anatomically coherent masks. However, hierarchical variants also demonstrated increased recall over precision, implying increased false positives. The results demonstrate that explicit hierarchical structuring improves both performance and clinical plausibility, especially in low data dental imaging regimes.

</details>


### [24] [FRIEDA: Benchmarking Multi-Step Cartographic Reasoning in Vision-Language Models](https://arxiv.org/abs/2512.08016)
*Jiyoon Pyo,Yuankun Jiao,Dongwon Jung,Zekun Li,Leeje Jang,Sofia Kirsanova,Jina Kim,Yijun Lin,Qin Liu,Junyi Xie,Hadi Askari,Nan Xu,Muhao Chen,Yao-Yi Chiang*

Main category: cs.CV

> 介绍了一个新的基准测试FRIEDA，用于评估大型视觉语言模型在复杂地图视觉问题回答中的表现，揭示了这些模型在多步骤地图推理上的不足。

<details>
  <summary>Details</summary>

**Motivation:** 现有地图VQA研究往往将地图视作图表的一种特殊形式，忽略了地图符号和空间关系的理解需求，因此需要一个专门针对地图的评估基准。

**Method:** FRIEDA 使用来自不同领域和地区的实际地图图像，涵盖了GIS文献中的所有三种空间关系：拓扑、度量和方向关系，涉及多项推理步骤。

**Result:** 在直接设置和上下文设置下测试了11个最先进的大型视觉语言模型，最强的两个模型 Gemini-2.5-Pro 和 GPT-5-Think 的准确率分别为38.20%和37.20%，远低于人类的准确率84.87%。

**Conclusion:** FRIEDA揭示了模型在多步骤地图推理上的持续差距，作为一个严格的基准，可以帮助推进在视觉语言模型中的空间智能研究。

**Abstract:** Cartographic reasoning is the skill of interpreting geographic relationships by aligning legends, map scales, compass directions, map texts, and geometries across one or more map images. Although essential as a concrete cognitive capability and for critical tasks such as disaster response and urban planning, it remains largely unevaluated. Building on progress in chart and infographic understanding, recent large vision language model studies on map visual question-answering often treat maps as a special case of charts. In contrast, map VQA demands comprehension of layered symbology (e.g., symbols, geometries, and text labels) as well as spatial relations tied to orientation and distance that often span multiple maps and are not captured by chart-style evaluations. To address this gap, we introduce FRIEDA, a benchmark for testing complex open-ended cartographic reasoning in LVLMs. FRIEDA sources real map images from documents and reports in various domains and geographical areas. Following classifications in Geographic Information System (GIS) literature, FRIEDA targets all three categories of spatial relations: topological (border, equal, intersect, within), metric (distance), and directional (orientation). All questions require multi-step inference, and many require cross-map grounding and reasoning. We evaluate eleven state-of-the-art LVLMs under two settings: (1) the direct setting, where we provide the maps relevant to the question, and (2) the contextual setting, where the model may have to identify the maps relevant to the question before reasoning. Even the strongest models, Gemini-2.5-Pro and GPT-5-Think, achieve only 38.20% and 37.20% accuracy, respectively, far below human performance of 84.87%. These results reveal a persistent gap in multi-step cartographic reasoning, positioning FRIEDA as a rigorous benchmark to drive progress on spatial intelligence in LVLMs.

</details>


### [25] [SSplain: Sparse and Smooth Explainer for Retinopathy of Prematurity Classification](https://arxiv.org/abs/2512.08038)
*Elifnur Sunger,Tales Imbiriba,Peter Campbell,Deniz Erdogmus,Stratis Ioannidis,Jennifer Dy*

Main category: cs.CV

> The paper presents SSplain, an explainer method that generates realistic and structured explanations for ROP classification from fundus images by ensuring smoothness and sparsity, overcoming limitations of existing methods.

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind this study is to improve the explainability of neural network models used for medical diagnosis, particularly in the context of Retinopathy of Prematurity (ROP). Existing explainers fail to generate explanations that preserve input image structures, which is important for trust and understanding.

**Method:** This paper introduces Sparse and Smooth Explainer (SSplain), a method that generates pixel-wise explanations while preserving the structures of fundus images by enforcing smoothness and sparsity. It solves an optimization problem with combinatorial constraints using the ADMM method.

**Result:** Experimental results on ROP classification from fundus images indicate that SSplain outperforms other explainers in terms of accuracy and explanation smoothness. The method also identifies features consistent with clinical knowledge, supporting its interpretability in a medical context.

**Conclusion:** SSplain demonstrates superior performance in terms of post-hoc accuracy and smoothness analyses compared to common explainers. It identifies discriminative features consistent with what clinicians look for in diagnosing ROP, and shows good generalization when applied to other datasets.

**Abstract:** Neural networks are frequently used in medical diagnosis. However, due to their black-box nature, model explainers are used to help clinicians understand better and trust model outputs. This paper introduces an explainer method for classifying Retinopathy of Prematurity (ROP) from fundus images. Previous methods fail to generate explanations that preserve input image structures such as smoothness and sparsity. We introduce Sparse and Smooth Explainer (SSplain), a method that generates pixel-wise explanations while preserving image structures by enforcing smoothness and sparsity. This results in realistic explanations to enhance the understanding of the given black-box model. To achieve this goal, we define an optimization problem with combinatorial constraints and solve it using the Alternating Direction Method of Multipliers (ADMM). Experimental results show that SSplain outperforms commonly used explainers in terms of both post-hoc accuracy and smoothness analyses. Additionally, SSplain identifies features that are consistent with domain-understandable features that clinicians consider as discriminative factors for ROP. We also show SSplain's generalization by applying it to additional publicly available datasets. Code is available at https://github.com/neu-spiral/SSplain.

</details>


### [26] [Lost in Translation, Found in Embeddings: Sign Language Translation and Alignment](https://arxiv.org/abs/2512.08040)
*Youngjoon Jang,Liliane Momeni,Zifan Jiang,Joon Son Chung,Gül Varol,Andrew Zisserman*

Main category: cs.CV

> 本研究提出了一种用于手语翻译和手语-字幕对齐的统一模型。该模型基于轻量级视觉主干、滑动感知映射网络和多任务可扩展训练策略，并在两个数据集上实现了最佳性能。

<details>
  <summary>Details</summary>

**Motivation:** 为了开发一个统一的模型，用于手语理解和转换连续的手语视频为口语文本，并进行手语与字幕的时间对齐，这对于实际交流、大规模语料库构建和教育应用都是至关重要的。

**Method:** 我们的方法基于三个组成部分：(i) 一个轻量级视觉主干，它从人体关键点和唇区图像中捕获手动和非手动提示，同时保护签署者的隐私；(ii) 一个滑动感知映射网络，它将连续的视觉特征聚合到单词级嵌入中，以桥接视觉-文本差距；(iii) 一种多任务可扩展训练策略，同时优化SLT和SSA，加强语言和时间对齐。

**Result:** 借助这种多语言预训练和强大的模型设计，我们在具有挑战性的BOBSL（BSL）数据集中手语翻译（SLT）和手语-字幕对齐（SSA）任务上达到了最佳结果。我们的模型还在How2Sign（ASL）数据集上展示了强大的零样本泛化和精细调整的手语翻译性能。

**Conclusion:** 该模型不仅在手语翻译和时间对齐上展现出优异的性能，并且还展示了跨不同手语语言的可扩展翻译能力。

**Abstract:** Our aim is to develop a unified model for sign language understanding, that performs sign language translation (SLT) and sign-subtitle alignment (SSA). Together, these two tasks enable the conversion of continuous signing videos into spoken language text and also the temporal alignment of signing with subtitles -- both essential for practical communication, large-scale corpus construction, and educational applications. To achieve this, our approach is built upon three components: (i) a lightweight visual backbone that captures manual and non-manual cues from human keypoints and lip-region images while preserving signer privacy; (ii) a Sliding Perceiver mapping network that aggregates consecutive visual features into word-level embeddings to bridge the vision-text gap; and (iii) a multi-task scalable training strategy that jointly optimises SLT and SSA, reinforcing both linguistic and temporal alignment. To promote cross-linguistic generalisation, we pretrain our model on large-scale sign-text corpora covering British Sign Language (BSL) and American Sign Language (ASL) from the BOBSL and YouTube-SL-25 datasets. With this multilingual pretraining and strong model design, we achieve state-of-the-art results on the challenging BOBSL (BSL) dataset for both SLT and SSA. Our model also demonstrates robust zero-shot generalisation and finetuned SLT performance on How2Sign (ASL), highlighting the potential of scalable translation across different sign languages.

</details>


### [27] [Towards Sustainable Universal Deepfake Detection with Frequency-Domain Masking](https://arxiv.org/abs/2512.08042)
*Chandler Timm C. Doloriel,Habib Ullah,Kristian Hovde Liland,Fadi Al Machot,Ngai-Man Cheung*

Main category: cs.CV

> 本论文提出一种基于频率掩码的训练策略用于深度伪造检测，该方法显著提升了在不同生成模型间泛化能力和在模型剪枝下的性能，减少了计算开销，是向可持续、广泛应用的深度伪造检测迈进的关键步骤。

<details>
  <summary>Details</summary>

**Motivation:** 研究的动机在于提供一种能够检测AI生成图像的强大且通用的方法，这些图像覆盖了各种生成模型，包括未来的、未知的模型。同时，该方法需在减少计算开销的同时，保持高检测效率，以满足大规模深度伪造图像筛查的需求。

**Method:** 本论文探索了一种基于频率域的掩码训练策略，用于深度伪造图像检测。这种方法引入了随机掩码和几何变换，尤其侧重于频率掩码，因为其具有优异的泛化性能。与传统依赖空间特征或大规模预训练模型的方法不同，这种方法更注重频率掩码。

**Result:** 实验结果表明，频率掩码技术不仅提高了对多种生成器的检测准确率，还能在显著模型剪枝情况下保持良好的性能。该方法在基于GAN和扩散模型生成的图像数据集上达到最新的泛化性能，并在结构化剪枝下保持一致性鲁棒性。

**Conclusion:** 研究结果强调了频率掩码技术作为可持续且通用的深度伪造检测解决方案的潜力。这种方法不仅可以提高检测准确性，还能在减少计算资源的条件下保持良好的性能。

**Abstract:** Universal deepfake detection aims to identify AI-generated images across a broad range of generative models, including unseen ones. This requires robust generalization to new and unseen deepfakes, which emerge frequently, while minimizing computational overhead to enable large-scale deepfake screening, a critical objective in the era of Green AI. In this work, we explore frequency-domain masking as a training strategy for deepfake detectors. Unlike traditional methods that rely heavily on spatial features or large-scale pretrained models, our approach introduces random masking and geometric transformations, with a focus on frequency masking due to its superior generalization properties. We demonstrate that frequency masking not only enhances detection accuracy across diverse generators but also maintains performance under significant model pruning, offering a scalable and resource-conscious solution. Our method achieves state-of-the-art generalization on GAN- and diffusion-generated image datasets and exhibits consistent robustness under structured pruning. These results highlight the potential of frequency-based masking as a practical step toward sustainable and generalizable deepfake detection. Code and models are available at: [https://github.com/chandlerbing65nm/FakeImageDetection](https://github.com/chandlerbing65nm/FakeImageDetection).

</details>


### [28] [Mask to Adapt: Simple Random Masking Enables Robust Continual Test-Time Learning](https://arxiv.org/abs/2512.08048)
*Chandler Timm C. Doloriel*

Main category: cs.CV

> 我们提出了Mask to Adapt（M2A）方法，它使用随机遮蔽视图和一致性及熵最小化目标进行有效测试时间适应。实验证明了这种方法的有效性和鲁棒性。

<details>
  <summary>Details</summary>

**Motivation:** 我们的动机是探讨是否需要定制的遮蔽设计，或者在强大的损坏下，简单的随机遮蔽时间表是否足够。这项研究是基于掩码图像建模理论进行的。

**Method:** 我们提出了Mask to Adapt（M2A），一种简单的持续测试时间适应方法，它通过生成一系列简短的遮蔽视图（空间或频率）并采用两种适应目标：一种是遮蔽一致性损失，它使不同视图的预测对齐；另一种是熵最小化损失，鼓励产生自信的输出。我们研究了两种常见的遮蔽家族及其子类型（空间遮蔽：块与像素；频率遮蔽：全部与高频与低频）。

**Result:** 在CIFAR10C/CIFAR100C/ImageNetC（严重程度5）上，M2A(Spatial)的平均错误率达到8.3%/19.8%/39.2%，超越或匹配强基线，而M2A(Frequency)则落后。实验表明简单的随机遮蔽时间表，加上一致性及熵目标，足可以达成有效的测试时间适应。

**Conclusion:** 这些结果表明，结合一致性及熵目标，一个简单的随机遮蔽时间表足以驱动有效的测试时间适应，而无需依赖不确定性信号或注意力信号。

**Abstract:** Distribution shifts at test time degrade image classifiers. Recent continual test-time adaptation (CTTA) methods use masking to regulate learning, but often depend on calibrated uncertainty or stable attention scores and introduce added complexity. We ask: do we need custom-made masking designs, or can a simple random masking schedule suffice under strong corruption? We introduce Mask to Adapt (M2A), a simple CTTA approach that generates a short sequence of masked views (spatial or frequency) and adapts with two objectives: a mask consistency loss that aligns predictions across different views and an entropy minimization loss that encourages confident outputs. Motivated by masked image modeling, we study two common masking families -- spatial masking and frequency masking -- and further compare subtypes within each (spatial: patch vs.\ pixel; frequency: all vs.\ low vs.\ high). On CIFAR10C/CIFAR100C/ImageNetC (severity~5), M2A (Spatial) attains 8.3\%/19.8\%/39.2\% mean error, outperforming or matching strong CTTA baselines, while M2A (Frequency) lags behind. Ablations further show that simple random masking is effective and robust. These results indicate that a simple random masking schedule, coupled with consistency and entropy objectives, is sufficient to drive effective test-time adaptation without relying on uncertainty or attention signals.

</details>


### [29] [Identification of Deforestation Areas in the Amazon Rainforest Using Change Detection Models](https://arxiv.org/abs/2512.08075)
*Christian Massao Konishi,Helio Pedrini*

Main category: cs.CV

> 本文通过评估多种变化检测模型，并结合预处理、后处理及模型组合技术，有效地监测了亚马逊森林的砍伐情况，取得了与其他近期研究相当的结果。

<details>
  <summary>Details</summary>

**Motivation:** 本文旨在填补现有文献中的空白，解决现有模型在处理亚马逊森林地区监测时的有效性不足和缺乏标准化方法的问题。

**Method:** 文中提到了使用基于Transformers的自注意力机制和其他全卷积模型进行变化检测，并测试了预处理和后处理技术，如基于连通组件大小的过滤、纹理替换和图像增强，以及模型组合策略。

**Result:** 研究结果表明，通过组合不同的模型，可以达到与文献中其他近期工作相当的F1分数，即80.41%。

**Conclusion:** 采用不同的预处理和后处理技术，以及模型组合策略，可以显著提高单个模型的有效性，并达到优秀的森林监测效果。

**Abstract:** The preservation of the Amazon Rainforest is one of the global priorities in combating climate change, protecting biodiversity, and safeguarding indigenous cultures. The Satellite-based Monitoring Project of Deforestation in the Brazilian Legal Amazon (PRODES), a project of the National Institute for Space Research (INPE), stands out as a fundamental initiative in this effort, annually monitoring deforested areas not only in the Amazon but also in other Brazilian biomes. Recently, machine learning models have been developed using PRODES data to support this effort through the comparative analysis of multitemporal satellite images, treating deforestation detection as a change detection problem. However, existing approaches present significant limitations: models evaluated in the literature still show unsatisfactory effectiveness, many do not incorporate modern architectures, such as those based on self-attention mechanisms, and there is a lack of methodological standardization that allows direct comparisons between different studies. In this work, we address these gaps by evaluating various change detection models in a unified dataset, including fully convolutional models and networks incorporating self-attention mechanisms based on Transformers. We investigate the impact of different pre- and post-processing techniques, such as filtering deforested areas predicted by the models based on the size of connected components, texture replacement, and image enhancements; we demonstrate that such approaches can significantly improve individual model effectiveness. Additionally, we test different strategies for combining the evaluated models to achieve results superior to those obtained individually, reaching an F1-score of 80.41%, a value comparable to other recent works in the literature.

</details>


### [30] [CVP: Central-Peripheral Vision-Inspired Multimodal Model for Spatial Reasoning](https://arxiv.org/abs/2512.08135)
*Zeyuan Chen,Xiang Zhang,Haiyang Xu,Jianwen Xie,Zhuowen Tu*

Main category: cs.CV

> 本文提出了一种受人类视觉启发的多模态框架CVP，通过引入两个互补组件，该模型在3D场景理解任务中表现优异。

<details>
  <summary>Details</summary>

**Motivation:** 现有的方法主要依赖于无结构的表示方式，如点云、体素或补丁特征，并通过坐标嵌入隐式地注入场景上下文，但这种方式往往导致空间推理能力有限。因此，本研究旨在通过引入新的组件来改进这一问题。

**Method:** 本研究提出了一种受中央和周边视觉启发的框架（CVP），用于空间推理的多模态模型。此模型包含了两个互补的组件：目标亲和力标记，类似于中央视觉，引导模型关注查询相关的目标；以及以自我为中心的网格，类似于周边视觉，捕捉全局场景环境和空间布局。这些组件协同工作，以实现对复杂3D环境的结构化和上下文感知的理解。

**Result:** 实验结果显示，CVP在一系列3D场景理解基准测试中达到了最先进的性能。

**Conclusion:** 引入了两个互补的组件，使大容量多模态模型能够达到结构化和上下文感知的3D环境理解，从而在3D场景理解任务中取得了最佳性能。

**Abstract:** We present a central-peripheral vision-inspired framework (CVP), a simple yet effective multimodal model for spatial reasoning that draws inspiration from the two types of human visual fields -- central vision and peripheral vision. Existing approaches primarily rely on unstructured representations, such as point clouds, voxels, or patch features, and inject scene context implicitly via coordinate embeddings. However, this often results in limited spatial reasoning capabilities due to the lack of explicit, high-level structural understanding. To address this limitation, we introduce two complementary components into a Large Multimodal Model-based architecture: target-affinity token, analogous to central vision, that guides the model's attention toward query-relevant objects; and allocentric grid, akin to peripheral vision, that captures global scene context and spatial arrangements. These components work in tandem to enable structured, context-aware understanding of complex 3D environments. Experiments show that CVP achieves state-of-the-art performance across a range of 3D scene understanding benchmarks.

</details>


### [31] [Fourier-RWKV: A Multi-State Perception Network for Efficient Image Dehazing](https://arxiv.org/abs/2512.08161)
*Lirong Zheng,Yanshan Li,Rui Yu,Kaihao Zhang*

Main category: cs.CV

> 本研究提出Fourier-RWKV框架，提高图像去雾的效率和效果，通过结合多态感知范式解决了变压器方法的二次计算复杂度问题，实现了先进的去雾性能并减少了计算开销。

<details>
  <summary>Details</summary>

**Motivation:** 研究的动机在于解决变压器基方法在处理非均匀雾化条件下的实时部署的二次计算复杂度问题，通过引入Fourier-RWKV框架提供一个在恢复质量和实际效率之间的有利权衡。

**Method:** 本研究提出Fourier-RWKV框架，通过结合多态感知范式实现线性复杂度的全面雾化降解建模。该模型包括三种感知状态：空间形式感知（通过对Deformable Quad-directional Token Shift操作实现，动态调整感受野以适应局部雾化变化）、频域感知（通过将WKV注意力机制扩展到傅里叶域的Fourier Mix模块实现，保留长距离依赖性的同时缓解空间衰减）、语义关系感知（通过利用动态语义核融合（DSK-Fusion）协同编码器和解码器特征的语义桥接模块（SBM）实现，以精确对齐编码器-解码器特征并抑制伪影）。

**Result:** 广泛的实验表明，Fourier-RWKV在多个基准上实现了先进的去雾性能，并显著减少了计算开销，证明了其在处理多种雾化场景中的有效性。

**Conclusion:** Fourier-RWKV框架展示了在不同雾化情境中实现高效的去雾效果，通过综合运用多态感知策略显著提高了图像去雾的质量和效率，同时减小了计算负担。

**Abstract:** Image dehazing is crucial for reliable visual perception, yet it remains highly challenging under real-world non-uniform haze conditions. Although Transformer-based methods excel at capturing global context, their quadratic computational complexity hinders real-time deployment. To address this, we propose Fourier Receptance Weighted Key Value (Fourier-RWKV), a novel dehazing framework based on a Multi-State Perception paradigm. The model achieves comprehensive haze degradation modeling with linear complexity by synergistically integrating three distinct perceptual states: (1) Spatial-form Perception, realized through the Deformable Quad-directional Token Shift (DQ-Shift) operation, which dynamically adjusts receptive fields to accommodate local haze variations; (2) Frequency-domain Perception, implemented within the Fourier Mix block, which extends the core WKV attention mechanism of RWKV from the spatial domain to the Fourier domain, preserving the long-range dependencies essential for global haze estimation while mitigating spatial attenuation; (3) Semantic-relation Perception, facilitated by the Semantic Bridge Module (SBM), which utilizes Dynamic Semantic Kernel Fusion (DSK-Fusion) to precisely align encoder-decoder features and suppress artifacts. Extensive experiments on multiple benchmarks demonstrate that Fourier-RWKV delivers state-of-the-art performance across diverse haze scenarios while significantly reducing computational overhead, establishing a favorable trade-off between restoration quality and practical efficiency. Code is available at: https://github.com/Dilizlr/Fourier-RWKV.

</details>


### [32] [Accuracy Does Not Guarantee Human-Likeness in Monocular Depth Estimators](https://arxiv.org/abs/2512.08163)
*Yuki Kubota,Taiki Fukiage*

Main category: cs.CV

> 研究分析了深度估计模型的准确性和与人类相似度之间的关系，发现二者虽然有误差相似之处，但准确性的提升并不一定带来人类行为的相似性，并强调了开发多方面、以人为本评估标准的必要性。

<details>
  <summary>Details</summary>

**Motivation:** 尽管深度神经网络在基于物理的基准测试中达到了超人的准确性，但模型表示与人类感知对齐仍然是一个关键挑战。这项研究旨在探讨模型准确性与人类行为相似性之间的权衡关系，特别是在依赖传感器真实值而非人类感知估计的自然室外场景中的深度估计。

**Method:** 研究了69个单目深度估计模型在KITTI数据集上的准确性和与人类感知相似度之间的关系，并通过仿射拟合分解预测误差以理解误差模式。

**Result:** 通过仿射拟合的方法分解预测误差，揭示了模型准确性和与人类感知相似度之间复杂的权衡关系。

**Conclusion:** 研究揭示了尽管人类和DNN在深度估计中共享某些偏差，但模型准确性和人类相似度之间存在不同的权衡关系。这意味着提高准确性并不一定意味着行为更接近人类，强调了需要发展多方面的、以人为本的评估标准。

**Abstract:** Monocular depth estimation is a fundamental capability for real-world applications such as autonomous driving and robotics. Although deep neural networks (DNNs) have achieved superhuman accuracy on physical-based benchmarks, a key challenge remains: aligning model representations with human perception, a promising strategy for enhancing model robustness and interpretability. Research in object recognition has revealed a complex trade-off between model accuracy and human-like behavior, raising a question whether a similar divergence exist in depth estimation, particularly for natural outdoor scenes where benchmarks rely on sensor-based ground truth rather than human perceptual estimates. In this study, we systematically investigated the relationship between model accuracy and human similarity across 69 monocular depth estimators using the KITTI dataset. To dissect the structure of error patterns on a factor-by-factor basis, we applied affine fitting to decompose prediction errors into interpretable components. Intriguingly, our results reveal while humans and DNNs share certain estimation biases (positive error correlations), we observed distinct trade-off relationships between model accuracy and human similarity. This finding indicates that improving accuracy does not necessarily lead to more human-like behavior, underscoring the necessity of developing multifaceted, human-centric evaluations beyond traditional accuracy.

</details>


### [33] [GeoLoom: High-quality Geometric Diagram Generation from Textual Input](https://arxiv.org/abs/2512.08180)
*Xiaojing Wei,Ting Zhang,Wei He,Jingdong Wang,Hua Huang*

Main category: cs.CV

> 提出GeoLoom框架，用于将自然语言几何描述转化为高质量几何图，表现出优于现有方法的结构保真度。

<details>
  <summary>Details</summary>

**Motivation:** 高精度几何图生成具有严格空间准确性要求，同时提供了明确约束条件。

**Method:** GeoLoom框架，包括自然语言到生成导向形式语言GeoLingua的自动形式化模块以及将形式约束映射到精确坐标的坐标求解器。

**Result:** 实验结果表明GeoLoom在结构保真度上优于现有模型。

**Conclusion:** 研究成果为可解释和可扩展的图生成提供了一个原则性的基础。

**Abstract:** High-quality geometric diagram generation presents both a challenge and an opportunity: it demands strict spatial accuracy while offering well-defined constraints to guide generation. Inspired by recent advances in geometry problem solving that employ formal languages and symbolic solvers for enhanced correctness and interpretability, we propose GeoLoom, a novel framework for text-to-diagram generation in geometric domains. GeoLoom comprises two core components: an autoformalization module that translates natural language into a specifically designed generation-oriented formal language GeoLingua, and a coordinate solver that maps formal constraints to precise coordinates using the efficient Monte Carlo optimization. To support this framework, we introduce GeoNF, a dataset aligning natural language geometric descriptions with formal GeoLingua descriptions. We further propose a constraint-based evaluation metric that quantifies structural deviation, offering mathematically grounded supervision for iterative refinement. Empirical results demonstrate that GeoLoom significantly outperforms state-of-the-art baselines in structural fidelity, providing a principled foundation for interpretable and scalable diagram generation.

</details>


### [34] [Animal Re-Identification on Microcontrollers](https://arxiv.org/abs/2512.08198)
*Yubo Chen,Di Zhao,Yun Sing Koh,Talia Xu*

Main category: cs.CV

> 研究设计了一种适用于低功耗边缘节点设备的动物重识别框架，通过优化神经网络架构和引入数据高效的微调策略，能在MCU设备上实现高效的全设备上的推理。

<details>
  <summary>Details</summary>

**Motivation:** 提升野生动物监测和精准畜牧业管理中的动物重识别能力，特别是在低带宽环境下，设计适用于低功耗设备的高效模型。

**Method:** 研究首先分析了现有动物重识别模型与MCU硬件的性能差距，接着通过系统化缩小卷积神经网络架构的规模，专为低分辨率输入设计了高精度模型，同时提出一种数据高效的微调策略。

**Result:** 研究设计并评估了一个适用于微控制器类硬件设备的紧凑型动物重识别框架。框架采用了系统化的卷积神经网络架构优化和数据高效的微调策略，并证明在实际牛只数据集上可实现高精度的全设备上的推理，展示出可实际部署的潜力。

**Conclusion:** 研究的紧凑模型在多个公开的动物重识别数据集上达到竞争力的检索准确性，同时模型大小减少了两个数量级，在自收集的牛只数据集上全设备上推理只需要少量的精度下降。

**Abstract:** Camera-based animal re-identification (Animal Re-ID) can support wildlife monitoring and precision livestock management in large outdoor environments with limited wireless connectivity. In these settings, inference must run directly on collar tags or low-power edge nodes built around microcontrollers (MCUs), yet most Animal Re-ID models are designed for workstations or servers and are too large for devices with small memory and low-resolution inputs. We propose an on-device framework. First, we characterise the gap between state-of-the-art Animal Re-ID models and MCU-class hardware, showing that straightforward knowledge distillation from large teachers offers limited benefit once memory and input resolution are constrained. Second, guided by this analysis, we design a high-accuracy Animal Re-ID architecture by systematically scaling a CNN-based MobileNetV2 backbone for low-resolution inputs. Third, we evaluate the framework with a real-world dataset and introduce a data-efficient fine-tuning strategy to enable fast adaptation with just three images per animal identity at a new site. Across six public Animal Re-ID datasets, our compact model achieves competitive retrieval accuracy while reducing model size by over two orders of magnitude. On a self-collected cattle dataset, the deployed model performs fully on-device inference with only a small accuracy drop and unchanged Top-1 accuracy relative to its cluster version. We demonstrate that practical, adaptable Animal Re-ID is achievable on MCU-class devices, paving the way for scalable deployment in real field environments.

</details>


### [35] [Blur2Sharp: Human Novel Pose and View Synthesis with Generative Prior Refinement](https://arxiv.org/abs/2512.08215)
*Chia-Hern Lai,I-Hsuan Lo,Yen-Ku Yeh,Thanh-Nguyen Truong,Ching-Chun Huang*

Main category: cs.CV

> 本文提出了一种名为Blur2Sharp的新框架，用于从单一参考视图生成逼真、几何结构一致的多视角图像。这种方法通过结合3D感知的神经渲染和扩散模型，解决了几何不一致和图像模糊的问题。

<details>
  <summary>Details</summary>

**Motivation:** 当前方法生成多视角图像时在几何一致性或图像逼真度上存在问题，尤其是面对多种视角和复杂动作时，导致输出模糊。Blur2Sharp旨在解决这一问题。

**Method:** Blur2Sharp框架采用3D感知的神经渲染和扩散模型相结合的方式，先通过Human NeRF模型生成几何一致的多视角渲染图，随后通过扩散模型基于这些渲染图进一步优化生成图像，保留细节并维护结构精度。此外，该方法通过层次化的特征融合，加入来自参数化SMPL模型的纹理、法线和语义先验来提高全局一致性和局部细节准确性。

**Result:** 通过实验展示了Blur2Sharp在生成真实感强烈且几何结构一致性好的图像方面的优越性，尤其是在处理复杂场景（如宽松服装和遮挡）时表现尤为突出。

**Conclusion:** 实验结果表明，Blur2Sharp在生成新型姿态和视角图像任务中超越了现有的先进技术，特别是在涉及宽松服装和遮挡的复杂场景中表现出色。

**Abstract:** The creation of lifelike human avatars capable of realistic pose variation and viewpoint flexibility remains a fundamental challenge in computer vision and graphics. Current approaches typically yield either geometrically inconsistent multi-view images or sacrifice photorealism, resulting in blurry outputs under diverse viewing angles and complex motions. To address these issues, we propose Blur2Sharp, a novel framework integrating 3D-aware neural rendering and diffusion models to generate sharp, geometrically consistent novel-view images from only a single reference view. Our method employs a dual-conditioning architecture: initially, a Human NeRF model generates geometrically coherent multi-view renderings for target poses, explicitly encoding 3D structural guidance. Subsequently, a diffusion model conditioned on these renderings refines the generated images, preserving fine-grained details and structural fidelity. We further enhance visual quality through hierarchical feature fusion, incorporating texture, normal, and semantic priors extracted from parametric SMPL models to simultaneously improve global coherence and local detail accuracy. Extensive experiments demonstrate that Blur2Sharp consistently surpasses state-of-the-art techniques in both novel pose and view generation tasks, particularly excelling under challenging scenarios involving loose clothing and occlusions.

</details>


### [36] [VisKnow: Constructing Visual Knowledge Base for Object Understanding](https://arxiv.org/abs/2512.08221)
*Ziwei Yao,Qiyang Wan,Ruiping Wang,Xilin Chen*

Main category: cs.CV

> 本文提出Visual Knowledge Base结构化多模态对象知识为图，以及VisKnow框架，用于自动构建对象理解的知识库。并通过AnimalKB案例展示了其提升对象级别视觉任务的效果。

<details>
  <summary>Details</summary>

**Motivation:** 构建一个系统且组织良好的知识库，能够综合处理关于对象类别的深入理解，提供不仅仅是一类标签的输出，而是涉及到组件、外观特征、类别间关系、背景知识等多层次的全面感知。

**Method:** 本论文提出了名为VisKnow的框架，该框架通过结合专家设计和大规模模型应用，从多元模态数据中提取构筑对象级别知识。同时提出了视觉知识库，将多模态对象知识结构化为图的形式。

**Result:** 通过AnimalKB实验，展示了增强的对象级视觉任务表现，如零样本识别和细粒度VQA，并且可用作知识图谱完成和部分分割的挑战性基准测试。

**Conclusion:** 该研究展示了自动构建视觉知识库在推进视觉理解和实际应用中的潜力。

**Abstract:** Understanding objects is fundamental to computer vision. Beyond object recognition that provides only a category label as typical output, in-depth object understanding represents a comprehensive perception of an object category, involving its components, appearance characteristics, inter-category relationships, contextual background knowledge, etc. Developing such capability requires sufficient multi-modal data, including visual annotations such as parts, attributes, and co-occurrences for specific tasks, as well as textual knowledge to support high-level tasks like reasoning and question answering. However, these data are generally task-oriented and not systematically organized enough to achieve the expected understanding of object categories. In response, we propose the Visual Knowledge Base that structures multi-modal object knowledge as graphs, and present a construction framework named VisKnow that extracts multi-modal, object-level knowledge for object understanding. This framework integrates enriched aligned text and image-source knowledge with region annotations at both object and part levels through a combination of expert design and large-scale model application. As a specific case study, we construct AnimalKB, a structured animal knowledge base covering 406 animal categories, which contains 22K textual knowledge triplets extracted from encyclopedic documents, 420K images, and corresponding region annotations. A series of experiments showcase how AnimalKB enhances object-level visual tasks such as zero-shot recognition and fine-grained VQA, and serves as challenging benchmarks for knowledge graph completion and part segmentation. Our findings highlight the potential of automatically constructing visual knowledge bases to advance visual understanding and its practical applications. The project page is available at https://vipl-vsu.github.io/VisKnow.

</details>


### [37] [SOP^2: Transfer Learning with Scene-Oriented Prompt Pool on 3D Object Detection](https://arxiv.org/abs/2512.08223)
*Ching-Hung Cheng,Hsiu-Fu Wu,Bing-Chen Wu,Khanh-Phong Bui,Van-Tin Luu,Ching-Chun Huang*

Main category: cs.CV

> 该论文探索了在3D物体检测中常用提示调整方法的有效性，特别是通过大规模Waymo数据集训练的基础模型适应其他场景。提出了面向场景的提示池(SOP^2)，展示了提示池在3D物体检测中的有效性，并启发未来研究者探索三维领域的提示潜力。

<details>
  <summary>Details</summary>

**Motivation:** 随着大型语言模型（如GPT-3）的兴起，该论文旨在探索这些模型在NLP领域之外，特别是在3D物体检测中的应用潜力。通过转移学习技术调整预训练模型以适应新任务是本文的核心动机之一。

**Method:** 本文研究了提示令牌和提示生成器对模型在3D物体检测中的影响，并提出了一种称为面向场景的提示池(SOP^2)的新方法，用于改进3D物体检测任务的性能。

**Result:** 研究表明，通过提示池可以在3D物体检测中达到有效结果，适应预训练模型以在不同场景中表现良好的能力得到了验证。

**Conclusion:** 研究证明了在3D物体检测中使用提示池方法的有效性，这激发了未来研究者继续探索3D领域中提示技术的潜力并应用于实际问题。

**Abstract:** With the rise of Large Language Models (LLMs) such as GPT-3, these models exhibit strong generalization capabilities. Through transfer learning techniques such as fine-tuning and prompt tuning, they can be adapted to various downstream tasks with minimal parameter adjustments. This approach is particularly common in the field of Natural Language Processing (NLP). This paper aims to explore the effectiveness of common prompt tuning methods in 3D object detection. We investigate whether a model trained on the large-scale Waymo dataset can serve as a foundation model and adapt to other scenarios within the 3D object detection field. This paper sequentially examines the impact of prompt tokens and prompt generators, and further proposes a Scene-Oriented Prompt Pool (\textbf{SOP$^2$}). We demonstrate the effectiveness of prompt pools in 3D object detection, with the goal of inspiring future researchers to delve deeper into the potential of prompts in the 3D field.

</details>


### [38] [New VVC profiles targeting Feature Coding for Machines](https://arxiv.org/abs/2512.08227)
*Md Eimran Hossain Eimon,Ashan Perera,Juan Merlos,Velibor Adzic,Hari Kalva*

Main category: cs.CV

> 本文探讨了使用Versatile Video Coding (VVC) 对于MPEG-AI Feature Coding for Machines (FCM) 标准下中间特征的压缩方法，提出了三种轻量级VVC配置文件（Fast, Faster, Fastest），它们在不同压缩比下达到了不同程度的速度和效率平衡。

<details>
  <summary>Details</summary>

**Motivation:** 传统的视频编解码技术为了保持感知质量，使用了人类视觉系统的模型。然而，在分割推理系统中，这些假设不再适用，因为中间特征是抽象的、稀疏的且特定任务相关的，本文旨在探索VVC对于这种中间特征的压缩方法。

**Method:** 通过工具级别的分析了解各个编码组件对压缩效率和下游视觉任务准确性的影响，基于这些见解，提出了三种轻量级的VVC配置文件。

**Result:** Fast配置文件提供了2.96%的BD-Rate增益，并减少了21.8%的编码时间。Faster配置文件实现了1.85%的BD-Rate增益，编码时间减少了51.5%。Fastest配置文件将编码时间减少了95.6%，仅在BD-Rate上了损失了1.71%。

**Conclusion:** 提出的三种VVC配置文件能够适应不同的应用需求，提供了压缩率和编码速度的有效折衷。这些发现可以帮助选择合适的压缩策略，以便在不同的设置了优化中间特征的传输。

**Abstract:** Modern video codecs have been extensively optimized to preserve perceptual quality, leveraging models of the human visual system. However, in split inference systems-where intermediate features from neural network are transmitted instead of pixel data-these assumptions no longer apply. Intermediate features are abstract, sparse, and task-specific, making perceptual fidelity irrelevant. In this paper, we investigate the use of Versatile Video Coding (VVC) for compressing such features under the MPEG-AI Feature Coding for Machines (FCM) standard. We perform a tool-level analysis to understand the impact of individual coding components on compression efficiency and downstream vision task accuracy. Based on these insights, we propose three lightweight essential VVC profiles-Fast, Faster, and Fastest. The Fast profile provides 2.96% BD-Rate gain while reducing encoding time by 21.8%. Faster achieves a 1.85% BD-Rate gain with a 51.5% speedup. Fastest reduces encoding time by 95.6% with only a 1.71% loss in BD-Rate.

</details>


### [39] [MM-CoT:A Benchmark for Probing Visual Chain-of-Thought Reasoning in Multimodal Models](https://arxiv.org/abs/2512.08228)
*Jusheng Zhang,Kaitong Cai,Xiaoyang Guo,Sidi Liu,Qinhan Lv,Ruiqi Chen,Jing Yang,Yijia Fan,Xiaofei Sun,Jian Wang,Ziliang Chen,Liang Lin,Keze Wang*

Main category: cs.CV

> 研究引入了MM-CoT，一种专门测试多模态模型链式思维推理的视觉基础和逻辑连贯性的新基准，揭示了性能上的差距，并为未来模型开发提供了依据。

<details>
  <summary>Details</summary>

**Motivation:** 现有基准侧重于生成推理链，而不是验证其准确性和逻辑有效性。研究旨在填补验证方面的空白。

**Method:** 介绍了一种新的诊断基准MM-CoT，用于测试多模态模型在链式思维推理中的视觉基础和逻辑连贯性。模型需要选择唯一符合视觉一致性（步骤基于可观察证据）和逻辑连贯性（因果和常识有效性）两个正交约束的事件链。

**Result:** 通过在MM-CoT上评估领先视觉语言模型，结果表明尽管现有最先进系统的生成流畅性很高，但在现实推理准确性方面的表现却较差。

**Conclusion:** MM-CoT验证了低相关性与现有基准，确认它测量的是视觉基础和逻辑推理的独特组合。这为开发未来能够忠实且连贯地在视觉世界中进行推理的模型提供了基础。

**Abstract:** The ability to perform Chain-of-Thought (CoT) reasoning marks a major milestone for multimodal models (MMs), enabling them to solve complex visual reasoning problems. Yet a critical question remains: is such reasoning genuinely grounded in visual evidence and logically coherent? Existing benchmarks emphasize generation but neglect verification, i.e., the capacity to assess whether a reasoning chain is both visually consistent and logically valid. To fill this gap, we introduce MM-CoT, a diagnostic benchmark specifically designed to probe the visual grounding and logical coherence of CoT reasoning in MMs. Instead of generating free-form explanations, models must select the sole event chain that satisfies two orthogonal constraints: (i) visual consistency, ensuring all steps are anchored in observable evidence, and (ii) logical coherence, ensuring causal and commonsense validity. Adversarial distractors are engineered to violate one of these constraints, exposing distinct reasoning failures. We evaluate leading vision-language models on MM-CoT and find that even the most advanced systems struggle, revealing a sharp discrepancy between generative fluency and true reasoning fidelity. MM-CoT shows low correlation with existing benchmarks, confirming that it measures a unique combination of visual grounding and logical reasoning. This benchmark provides a foundation for developing future models that reason not just plausibly, but faithfully and coherently within the visual world.

</details>


### [40] [Geometry-Aware Sparse Depth Sampling for High-Fidelity RGB-D Depth Completion in Robotic Systems](https://arxiv.org/abs/2512.08229)
*Tony Salloom,Dandi Zhou,Xinhai Sun*

Main category: cs.CV

> 本文提出了一种基于法线的稀疏深度采样策略，结合Marigold-DC扩散型深度补全模型，在NYU Depth v2数据集上的实验表明，该方法可提升深度图准确性，减少伪影，并更好地模拟真实传感器行为。

<details>
  <summary>Details</summary>

**Motivation:** 现有深度补全pipeline的主要局限性是稀疏深度的真实感不足。现有方法通常从密集的地面真值深度中以均匀随机的方式选择稀疏像素，而忽略了实际传感器表现出的几何依赖性和空间非均匀可靠性。

**Method:** 本文提出了一种基于法线的稀疏深度采样策略，利用PCA方法估计RGB-D点云的表面法线，从而计算每个像素的深度可靠性度量。稀疏深度样本根据这个可靠性分布进行抽取。

**Result:** 实验结果表明，这种几何感知的稀疏深度提高了准确性，减少了边缘和不连续区域的伪影，并产生了更符合真实传感器行为的训练条件。

**Conclusion:** 该研究证明了使用基于表面法线的稀疏深度采样策略能够产生更高质量的深度图，并更好地模拟真实环境中的传感器行为。

**Abstract:** Accurate three-dimensional perception is essential for modern industrial robotic systems that perform manipulation, inspection, and navigation tasks. RGB-D and stereo vision sensors are widely used for this purpose, but the depth maps they produce are often noisy, incomplete, or biased due to sensor limitations and environmental conditions. Depth completion methods aim to generate dense, reliable depth maps from RGB images and sparse depth input. However, a key limitation in current depth completion pipelines is the unrealistic generation of sparse depth: sparse pixels are typically selected uniformly at random from dense ground-truth depth, ignoring the fact that real sensors exhibit geometry-dependent and spatially nonuniform reliability. In this work, we propose a normal-guided sparse depth sampling strategy that leverages PCA-based surface normal estimation on the RGB-D point cloud to compute a per-pixel depth reliability measure. The sparse depth samples are then drawn according to this reliability distribution. We integrate this sampling method with the Marigold-DC diffusion-based depth completion model and evaluate it on NYU Depth v2 using the standard metrics. Experiments show that our geometry-aware sparse depth improves accuracy, reduces artifacts near edges and discontinuities, and produces more realistic training conditions that better reflect real sensor behavior.

</details>


### [41] [FastBEV++: Fast by Algorithm, Deployable by Design](https://arxiv.org/abs/2512.08237)
*Yuanpeng Chen,Hui Song,Wei Tao,ShanHui Mo,Shuang Zhang,Xiao Hua,TianKun Zhao*

Main category: cs.CV

> FastBEV++是一款旨在解决高性能和车辆部署效率之间的矛盾的框架，通过创新的算法和设计，实现了高效和准确的Bird's-Eye-View感知。

<details>
  <summary>Details</summary>

**Motivation:** 解决相机仅依靠的Bird's-Eye-View(BEV)感知技术在追求高性能与车辆部署时的冲突问题。

**Method:** FastBEV++采用创新的视图转换范式，将复杂的投影过程转换为简单的Index-Gather-Reshape流水线，并支持深度感知融合机制以增强几何准确性。

**Result:** 在nuScenes基准测试中，FastBEV++达到了0.359 NDS的业界领先性能，同时在Tesla T4等汽车级硬件上实现了超过134 FPS的实时性能。

**Conclusion:** FastBEV++提供了一个无需使用定制插件的高性能和高效的解决方案，是生产自动驾驶系统设计中的成熟和可扩展的选择。

**Abstract:** The advancement of camera-only Bird's-Eye-View(BEV) perception is currently impeded by a fundamental tension between state-of-the-art performance and on-vehicle deployment tractability. This bottleneck stems from a deep-rooted dependency on computationally prohibitive view transformations and bespoke, platform-specific kernels. This paper introduces FastBEV++, a framework engineered to reconcile this tension, demonstrating that high performance and deployment efficiency can be achieved in unison via two guiding principles: Fast by Algorithm and Deployable by Design. We realize the "Deployable by Design" principle through a novel view transformation paradigm that decomposes the monolithic projection into a standard Index-Gather-Reshape pipeline. Enabled by a deterministic pre-sorting strategy, this transformation is executed entirely with elementary, operator native primitives (e.g Gather, Matrix Multiplication), which eliminates the need for specialized CUDA kernels and ensures fully TensorRT-native portability. Concurrently, our framework is "Fast by Algorithm", leveraging this decomposed structure to seamlessly integrate an end-to-end, depth-aware fusion mechanism. This jointly learned depth modulation, further bolstered by temporal aggregation and robust data augmentation, significantly enhances the geometric fidelity of the BEV representation.Empirical validation on the nuScenes benchmark corroborates the efficacy of our approach. FastBEV++ establishes a new state-of-the-art 0.359 NDS while maintaining exceptional real-time performance, exceeding 134 FPS on automotive-grade hardware (e.g Tesla T4). By offering a solution that is free of custom plugins yet highly accurate, FastBEV++ presents a mature and scalable design philosophy for production autonomous systems. The code is released at: https://github.com/ymlab/advanced-fastbev

</details>


### [42] [HybridToken-VLM: Hybrid Token Compression for Vision-Language Models](https://arxiv.org/abs/2512.08240)
*Jusheng Zhang,Xiaoyang Guo,Kaitong Cai,Qinhan Lv,Yijia Fan,Wenhao Chai,Jian Wang,Keze Wang*

Main category: cs.CV

> HTC-VLM框架将语义和外观分离，通过双通道设计有效压缩视觉建模，并保持了较高的性能表现。

<details>
  <summary>Details</summary>

**Motivation:** 视觉语言模型已经转变了多模态推理，但是将数百个视觉补丁标记输入到语言模型中会导致计算成本的大幅增加。传统的压缩方法要么削弱高层次语义，要么丢失细粒度的细节，难以兼顾效率和保真度。

**Method:** 提出了HTC-VLM框架，该框架通过连续路径传递细粒度细节，离散路径传递符号锚点，实现了语义和外观的分离。详细来说，连续路径使用ViT补丁，离散路径使用MGVQ量化为四个标记。通过解缠注意力掩模和瓶颈压缩，这些路径融合成一个580标记的混合序列，并最终压缩为一个单词汇标记，确保了有效且基础的表示。

**Result:** HTC-VLM在七个基准测试（GQA, VQAv2, MMBench, MME, POPE, SEED-Bench, ScienceQA-Image）中平均保持了87.2%的表现，优于连续基准线的81.0%，并且实现了580到1的比例压缩。注意力分析显示压缩的标记优先考虑离散锚点，验证了其语义引导的有效性。

**Conclusion:** 这项工作展示了最小化混合设计可以解决效率和保真度的困境，并推动可扩展的视觉语言模型的发展。

**Abstract:** Vision-language models (VLMs) have transformed multimodal reasoning, but feeding hundreds of visual patch tokens into LLMs incurs quadratic computational costs, straining memory and context windows. Traditional approaches face a trade-off: continuous compression dilutes high-level semantics such as object identities, while discrete quantization loses fine-grained details such as textures. We introduce HTC-VLM, a hybrid framework that disentangles semantics and appearance through dual channels, i.e., a continuous pathway for fine-grained details via ViT patches and a discrete pathway for symbolic anchors using MGVQ quantization projected to four tokens. These are fused into a 580-token hybrid sequence and compressed into a single voco token via a disentanglement attention mask and bottleneck, ensuring efficient and grounded representations. HTC-VLM achieves an average performance retention of 87.2 percent across seven benchmarks (GQA, VQAv2, MMBench, MME, POPE, SEED-Bench, ScienceQA-Image), outperforming the leading continuous baseline at 81.0 percent with a 580-to-1 compression ratio. Attention analyses show that the compressed token prioritizes the discrete anchor, validating its semantic guidance. Our work demonstrates that a minimalist hybrid design can resolve the efficiency-fidelity dilemma and advance scalable VLMs.

</details>


### [43] [Beyond Real Weights: Hypercomplex Representations for Stable Quantization](https://arxiv.org/abs/2512.08524)
*Jawad Ibn Ahad,Maisha Rahman,Amrijit Biswas,Muhammad Rafsan Kabir,Robin Krambroeckers,Sifat Momen,Nabeel Mohammed,Shafin Rahman*

Main category: cs.CV

> A progressive reparameterization method using PHM layers, reducing model parameters and inference latency in multimodal language models without compromising performance.

<details>
  <summary>Details</summary>

**Motivation:** To address the computational heaviness and inefficiency in deploying large-scale multimodal language models, which require high parameter capacity to align visual features with linguistic representations.

**Method:** We introduce a progressive reparameterization strategy using compact Parameterized Hypercomplex Multiplication (PHM) layers and a residual interpolation schedule with lightweight reconstruction and knowledge distillation losses to compress multimodal language models (MLLMs).

**Result:** The method yields substantial parameter and FLOP reductions while preserving strong multimodal alignment and enabling faster inference without degrading output quality. The approach was evaluated on multiple vision-language models (VLMs).

**Conclusion:** Our method delivers significant reductions in model size and inference latency while maintaining performance comparable to the base models, offering an efficient path for multimodal reasoning and complementing low-bit quantization techniques.

**Abstract:** Multimodal language models (MLLMs) require large parameter capacity to align high-dimensional visual features with linguistic representations, making them computationally heavy and difficult to deploy efficiently. We introduce a progressive reparameterization strategy that compresses these models by gradually replacing dense feed-forward network blocks with compact Parameterized Hypercomplex Multiplication (PHM) layers. A residual interpolation schedule, together with lightweight reconstruction and knowledge distillation losses, ensures that the PHM modules inherit the functional behavior of their dense counterparts during training. This transition yields substantial parameter and FLOP reductions while preserving strong multimodal alignment, enabling faster inference without degrading output quality. We evaluate the approach on multiple vision-language models (VLMs). Our method maintains performance comparable to the base models while delivering significant reductions in model size and inference latency. Progressive PHM substitution thus offers an architecture-compatible path toward more efficient multimodal reasoning and complements existing low-bit quantization techniques.

</details>


### [44] [Residual-SwinCA-Net: A Channel-Aware Integrated Residual CNN-Swin Transformer for Malignant Lesion Segmentation in BUSI](https://arxiv.org/abs/2512.08243)
*Saeeda Naz,Saddam Hussain Khan*

Main category: cs.CV

> 提出了一种新颖的深度混合Residual-SwinCA-Net分割框架，用于乳腺病变分割，并在BUSI数据集上达到了99.29%的平均准确率和0.9041的Dice系数。

<details>
  <summary>Details</summary>

**Motivation:** 为了解决乳腺病变分割的挑战，即提取局部相关且鲁棒的特征，并学习全局依赖性。

**Method:** 结合残差CNN模块和定制的Swin Transformer块，使用Laplacian-of-Gaussian算子来增强组织连贯性和突出细微结构过渡，应用边界导向算子来保持恶性病灶轮廓的形态完整性。同时，采用逐步减少特征图的收缩策略，每个解码器层次增加了MSCAS模块，在编码器显著图上选择性增强，并引入Pixel-Attention模块来突出恶性病灶像素。

**Result:** 在BUSI数据集上，Residual-SwinCA-Net框架的平均准确率为99.29%，IoU为98.74%，Dice系数为0.9041。

**Conclusion:** 新技术提高了乳腺病变的诊断性能，加强了临床决策的及时性。

**Abstract:** A novel deep hybrid Residual-SwinCA-Net segmentation framework is proposed in the study for addressing such challenges by extracting locally correlated and robust features, incorporating residual CNN modules. Furthermore, for learning global dependencies, Swin Transformer blocks are customized using internal residual pathways, which reinforce gradient stability, refine local patterns, and facilitate global feature fusion. Formerly, for enhancing tissue continuity, ultrasound noise suppressions, and accentuating fine structural transitions Laplacian-of-Gaussian regional operator is applied, and for maintaining the morphological integrity of malignant lesion contours, a boundary-oriented operator has been incorporated. Subsequently, a contraction strategy was applied stage-wise by progressively reducing features-map progressively for capturing scale invariance and enhancing the robustness of structural variability. In addition, each decoder level prior augmentation integrates a new Multi-Scale Channel Attention and Squeezing (MSCAS) module. The MSCAS selectively emphasizes encoder salient maps, retains discriminative global context, and complementary local structures with minimal computational cost while suppressing redundant activations. Finally, the Pixel-Attention module encodes class-relevant spatial cues by adaptively weighing malignant lesion pixels while suppressing background interference. The Residual-SwinCA-Net and existing CNNs/ViTs techniques have been implemented on the publicly available BUSI dataset. The proposed Residual-SwinCA-Net framework outperformed and achieved 99.29% mean accuracy, 98.74% IoU, and 0.9041 Dice for breast lesion segmentation. The proposed Residual-SwinCA-Net framework improves the BUSI lesion diagnostic performance and strengthens timely clinical decision-making.

</details>


### [45] [Pose-Based Sign Language Spotting via an End-to-End Encoder Architecture](https://arxiv.org/abs/2512.08738)
*Samuel Ebimobowei Johnny,Blessed Guda,Emmanuel Enejo Aaron,Assane Gueye*

Main category: cs.CV

> The paper proposes an end-to-end model for Sign Language Spotting, using pose keypoints for reduced computation and noise mitigation, achieving notable results on a standard dataset.

<details>
  <summary>Details</summary>

**Motivation:** The paper aims to address the challenge of detecting the presence or absence of a query sign within a sequence of continuous signs, a largely unexplored problem in sign language retrieval.

**Method:** Our architecture employs an encoder-only backbone with a binary classification head, directly operating on pose keypoints extracted from sign videos.

**Result:** The proposed method achieves 61.88% accuracy and 60.00% F1-score on the Word Presence Prediction dataset from the WSLP 2025 shared task.

**Conclusion:** The posed-based framework proposed in the paper demonstrates effectiveness for the task of Sign Language Spotting and lays the groundwork for future work in sign language retrieval.

**Abstract:** Automatic Sign Language Recognition (ASLR) has emerged as a vital field for bridging the gap between deaf and hearing communities. However, the problem of sign-to-sign retrieval or detecting a specific sign within a sequence of continuous signs remains largely unexplored. We define this novel task as Sign Language Spotting. In this paper, we present a first step toward sign language retrieval by addressing the challenge of detecting the presence or absence of a query sign video within a sentence-level gloss or sign video. Unlike conventional approaches that rely on intermediate gloss recognition or text-based matching, we propose an end-to-end model that directly operates on pose keypoints extracted from sign videos. Our architecture employs an encoder-only backbone with a binary classification head to determine whether the query sign appears within the target sequence. By focusing on pose representations instead of raw RGB frames, our method significantly reduces computational cost and mitigates visual noise. We evaluate our approach on the Word Presence Prediction dataset from the WSLP 2025 shared task, achieving 61.88\% accuracy and 60.00\% F1-score. These results demonstrate the effectiveness of our pose-based framework for Sign Language Spotting, establishing a strong foundation for future research in automatic sign language retrieval and verification. Code is available at https://github.com/EbimoJohnny/Pose-Based-Sign-Language-Spotting

</details>


### [46] [Distilling Future Temporal Knowledge with Masked Feature Reconstruction for 3D Object Detection](https://arxiv.org/abs/2512.08247)
*Haowen Zheng,Hu Zhu,Lu Deng,Weihao Gu,Yang Yang,Yanyan Liang*

Main category: cs.CV

> 开发了一种新的时空知识蒸馏技术FTKD，显著提升了在线模型在3D物体检测中的表现，特别是在未来帧知识的转移上，减少了未来的不确定性，且不增加推理成本。

<details>
  <summary>Details</summary>

**Motivation:** 现有的知识蒸馏方法主要关注于空间特征蒸馏或时间关系蒸馏，这些方法容易忽略未来帧，导致在线模型无法有效学习未来知识。因此，提出FTKD旨在弥补这一不足，提高在线模型的学习效果。

**Method:** 提出了一种基于稀疏查询的未来时空知识蒸馏（FTKD）方法，该方法不需要严格的帧对齐，能够从离线教师模型向在线学生模型有效转移未来帧的知识。具体地，提出了一个具有未来感知特性的特征重构策略，鼓励学生模型捕捉未来特征。此外，还引入了未来的指导日志蒸馏，利用教师模型稳定的前景和背景上下文。

**Result:** FTKD应用于两个高性能的3D物体检测基线上，在nuScenes数据集上实现了最高1.3 mAP和1.3 NDS的提升，同时提供了最准确的速度估计，且不增加推理成本。

**Conclusion:** 通过引入未来的时空知识蒸馏方法，有效提升了在线模型在3D物体检测中的表现，尤其是在未来帧信息的利用上取得了显著进步。

**Abstract:** Camera-based temporal 3D object detection has shown impressive results in autonomous driving, with offline models improving accuracy by using future frames. Knowledge distillation (KD) can be an appealing framework for transferring rich information from offline models to online models. However, existing KD methods overlook future frames, as they mainly focus on spatial feature distillation under strict frame alignment or on temporal relational distillation, thereby making it challenging for online models to effectively learn future knowledge. To this end, we propose a sparse query-based approach, Future Temporal Knowledge Distillation (FTKD), which effectively transfers future frame knowledge from an offline teacher model to an online student model. Specifically, we present a future-aware feature reconstruction strategy to encourage the student model to capture future features without strict frame alignment. In addition, we further introduce future-guided logit distillation to leverage the teacher's stable foreground and background context. FTKD is applied to two high-performing 3D object detection baselines, achieving up to 1.3 mAP and 1.3 NDS gains on the nuScenes dataset, as well as the most accurate velocity estimation, without increasing inference cost.

</details>


### [47] [Query-aware Hub Prototype Learning for Few-Shot 3D Point Cloud Semantic Segmentation](https://arxiv.org/abs/2512.08253)
*YiLin Zhou,Lili Wei,Zheming Xu,Ziyi Chen,Congyan Lang*

Main category: cs.CV

> 我们提出了一种名为QHP的新方法，它显式地建模了支持集和查询集之间的语义关联，以克服现有方法中仅使用支撑集生成原型所引起的原型偏差问题。

<details>
  <summary>Details</summary>

**Motivation:** 现有的基于度量的原型学习方法只从支撑集中生成原型，忽略了原型与查询集的相关性，这会导致原型偏差从而降低分割性能。我们的动机是解决这个问题，提高少样本3D点云语义分割性能。

**Method:** 我们提出了一种名为Query-aware Hub Prototype (QHP)的学习方法，该方法显式地建模了支持集和查询集之间的语义关联。特别地，我们提出了一种Hub Prototype Generation (HPG)模块，通过构建连接查询点和支撑点的二分图，识别频繁连接的支撑枢纽，并生成与查询相关的原型，以更好地捕捉跨集语义。此外，我们还引入了一个Prototype Distribution Optimization (PDO)模块，通过纯净度重权对比损失来优化原型分布，进一步减少了不良枢纽和模糊原型对性能的影响。

**Result:** 实验表明，QHP在S3DIS和ScanNet数据集上显著优于现有方法，能有效缩小原型和查询集之间的语义差距，在少样本3D点云语义分割任务中表现优异。

**Conclusion:** 通过引入Query-aware Hub Prototype (QHP)的学习方法，包括Hub Prototype Generation (HPG)模块和Prototype Distribution Optimization (PDO)模块，我们的方法能够更好地解决少样本3D点云语义分割问题，并在实验中取得了超越现有方法的效果。

**Abstract:** Few-shot 3D point cloud semantic segmentation (FS-3DSeg) aims to segment novel classes with only a few labeled samples. However, existing metric-based prototype learning methods generate prototypes solely from the support set, without considering their relevance to query data. This often results in prototype bias, where prototypes overfit support-specific characteristics and fail to generalize to the query distribution, especially in the presence of distribution shifts, which leads to degraded segmentation performance. To address this issue, we propose a novel Query-aware Hub Prototype (QHP) learning method that explicitly models semantic correlations between support and query sets. Specifically, we propose a Hub Prototype Generation (HPG) module that constructs a bipartite graph connecting query and support points, identifies frequently linked support hubs, and generates query-relevant prototypes that better capture cross-set semantics. To further mitigate the influence of bad hubs and ambiguous prototypes near class boundaries, we introduce a Prototype Distribution Optimization (PDO) module, which employs a purity-reweighted contrastive loss to refine prototype representations by pulling bad hubs and outlier prototypes closer to their corresponding class centers. Extensive experiments on S3DIS and ScanNet demonstrate that QHP achieves substantial performance gains over state-of-the-art methods, effectively narrowing the semantic gap between prototypes and query sets in FS-3DSeg.

</details>


### [48] [SFP: Real-World Scene Recovery Using Spatial and Frequency Priors](https://arxiv.org/abs/2512.08254)
*Yun Liu,Tao Li,Cosmin Ancuti,Wenqi Ren,Weisi Lin*

Main category: cs.CV

> 本文提出了利用空域和频域先验来恢复场景的方法（SFP），该方法在多种退化条件下的恢复效果得到验证。

<details>
  <summary>Details</summary>

**Motivation:** 现存的场景恢复技术存在依赖单一先验无法处理复叠加成和在合成数据上训练的模型泛化能力差的问题。

**Method:** Structure

**Result:** {
  "tldr": "本文提出了一种用于真实场景恢复的空域和频域先验方法（SFP），该方法通过利用空域和频域中的观察先验来恢复散射退化场景，并通过加权融合策略得到最终恢复结果。实验表明该方法在各种退化条件下有效且优越。", 
  "motivation": "现有的场景恢复方法要么依赖单一先验，无法处理多种退化，要么使用复杂网络在合成数据上训练，对现实世界的多样化场景泛化能力差。", 
  "method": "在空域上，观察到退化图像的逆图与场景传输图在频谱方向上具有投影相似性，基于此先验来估计传输图，并恢复散射退化场景。在频域上，基于提出的两个新先验来估计两个参数，构建自适应频率增强掩膜。最后，采用加权融合策略来结合空域恢复、频域增强及输入图像的显著特征得到最终恢复结果。", 
  "result": "大量的实验评估展示了SFP方法在各种退化条件下进行场景恢复的有效性和优越性。", 
  "conclusion": "此工作演示了通过空域和频域结合的方法能够有效提升复杂退化场景的恢复效果。" 
}

**Conclusion:** 研究展示结合空域和频域的恢复手段能显著提高场景恢复效果。

**Abstract:** Scene recovery serves as a critical task for various computer vision applications. Existing methods typically rely on a single prior, which is inherently insufficient to handle multiple degradations, or employ complex network architectures trained on synthetic data, which suffer from poor generalization for diverse real-world scenarios. In this paper, we propose Spatial and Frequency Priors (SFP) for real-world scene recovery. In the spatial domain, we observe that the inverse of the degraded image exhibits a projection along its spectral direction that resembles the scene transmission. Leveraging this spatial prior, the transmission map is estimated to recover the scene from scattering degradation. In the frequency domain, a mask is constructed for adaptive frequency enhancement, with two parameters estimated using our proposed novel priors. Specifically, one prior assumes that the mean intensity of the degraded image's direct current (DC) components across three channels in the frequency domain closely approximates that of each channel in the clear image. The second prior is based on the observation that, for clear images, the magnitude of low radial frequencies below 0.001 constitutes approximately 1% of the total spectrum. Finally, we design a weighted fusion strategy to integrate spatial-domain restoration, frequency-domain enhancement, and salient features from the input image, yielding the final recovered result. Extensive evaluations demonstrate the effectiveness and superiority of our proposed SFP for scene recovery under various degradation conditions.

</details>


### [49] [RLCNet: An end-to-end deep learning framework for simultaneous online calibration of LiDAR, RADAR, and Camera](https://arxiv.org/abs/2512.08262)
*Hafeez Husain Cholakkal,Stefano Arrigoni,Francesco Braghin*

Main category: cs.CV

> 本文介绍了RLCNet，一种用于实时在线校准多模态传感器的深度学习框架，它在实际部署中表现出强大的性能，并通过实验验证了其优越的准确性和鲁棒性。

<details>
  <summary>Details</summary>

**Motivation:** 精确的传感器外参校准对于实现可靠的自动驾驶感知至关重要，但是由于机械振动和动态环境中的累积传感器漂移，校准仍然面临挑战。

**Method:** 该论文提出了RLCNet，一种用于实时在线校准多模态传感器（如激光雷达、雷达和摄像机）的端到端可训练的深度学习框架。它包含加权移动平均和异常值拒绝来支持实时操作。

**Result:** 在实际数据集上的测试表明，RLCNet能够提供强大的性能，具有减少预测噪声和提高漂移容忍度的能力。

**Conclusion:** 通过比较现有方法，RLCNet展示了它的优越准确性和鲁棒性。

**Abstract:** Accurate extrinsic calibration of LiDAR, RADAR, and camera sensors is essential for reliable perception in autonomous vehicles. Still, it remains challenging due to factors such as mechanical vibrations and cumulative sensor drift in dynamic environments. This paper presents RLCNet, a novel end-to-end trainable deep learning framework for the simultaneous online calibration of these multimodal sensors. Validated on real-world datasets, RLCNet is designed for practical deployment and demonstrates robust performance under diverse conditions. To support real-time operation, an online calibration framework is introduced that incorporates a weighted moving average and outlier rejection, enabling dynamic adjustment of calibration parameters with reduced prediction noise and improved resilience to drift. An ablation study highlights the significance of architectural choices, while comparisons with existing methods demonstrate the superior accuracy and robustness of the proposed approach.

</details>


### [50] [EgoX: Egocentric Video Generation from a Single Exocentric Video](https://arxiv.org/abs/2512.08269)
*Taewoong Kang,Kinam Kim,Dohyeon Kim,Minho Park,Junha Hyung,Jaegul Choo*

Main category: cs.CV

> EgoX框架利用视频扩散模型和几何引导的自注意力机制，有效解决了将第三人称视频转换为第一人称视频的挑战，生成具有高保真度的第一人称视频。

<details>
  <summary>Details</summary>

**Motivation:** 将第三人称视频转换为第一人称视频以实现更沉浸式体验，但因相机姿态变化大且视区重叠少而具有挑战性。

**Method:** EgoX采用轻量级LoRA适应预先训练的视频扩散模型，结合内外视角的先验知识，并引入几何引导的自注意力机制，以确保几何一致性及视觉保真度。

**Result:** EgoX能生成连贯且逼真的第一人称视频，并在未见过的和复杂环境的视频中显示较强的伸缩性和鲁棒性。

**Conclusion:** EgoX为从单个第三人称视频生成第一人称视频提供了一种有效且鲁棒的方法，具有几何一致性和高视觉保真度。

**Abstract:** Egocentric perception enables humans to experience and understand the world directly from their own point of view. Translating exocentric (third-person) videos into egocentric (first-person) videos opens up new possibilities for immersive understanding but remains highly challenging due to extreme camera pose variations and minimal view overlap. This task requires faithfully preserving visible content while synthesizing unseen regions in a geometrically consistent manner. To achieve this, we present EgoX, a novel framework for generating egocentric videos from a single exocentric input. EgoX leverages the pretrained spatio temporal knowledge of large-scale video diffusion models through lightweight LoRA adaptation and introduces a unified conditioning strategy that combines exocentric and egocentric priors via width and channel wise concatenation. Additionally, a geometry-guided self-attention mechanism selectively attends to spatially relevant regions, ensuring geometric coherence and high visual fidelity. Our approach achieves coherent and realistic egocentric video generation while demonstrating strong scalability and robustness across unseen and in-the-wild videos.

</details>


### [51] [PAVAS: Physics-Aware Video-to-Audio Synthesis](https://arxiv.org/abs/2512.08282)
*Oh Hyun-Bin,Yuhta Takida,Toshimitsu Uesaka,Tae-Hyun Oh,Yuki Mitsufuji*

Main category: cs.CV

> 本文提出了Physics-Aware Video-to-Audio Synthesis (PAVAS)，该方法将物理推理整合到隐变量扩散模型中，从而合成反映真实世界物理因素的声音。实验表明，PAVAS能够生成优于现有模型的音频。

<details>
  <summary>Details</summary>

**Motivation:** 现有的Video-to-Audio生成模型大多是基于外观驱动的，捕获视觉-声学相关性但没有考虑塑造现实世界声音的物理因素。

**Method:** 通过引入Physics-Driven Audio Adapter (Phy-Adapter) 来将物理因素考虑到Video-to-Audio (V2A) 生成过程中。Phy-Adapter 接收由Physical Parameter Estimator (PPE) 估计的物体级物理参数，PPE 利用Vision-Language Model (VLM) 推断移动物体的质量，以及基于分割的动态3D重建模块恢复其运动轨迹计算速度。这些物理线索能够帮助模型合成反映底层物理因素的声音。

**Result:** 实验表明，PAVAS在定量和定性评估中都优于现有的Video-to-Audio模型，能够生成物理上合理且感知一致的声音。

**Conclusion:** PAVAS通过引入物理推理到基于隐变量扩散的Video-to-Audio生成模型中，能够在生成的音频中综合底层物理因素。项目的演示视频可以在https://physics-aware-video-to-audio-synthesis.github.io上找到。

**Abstract:** Recent advances in Video-to-Audio (V2A) generation have achieved impressive perceptual quality and temporal synchronization, yet most models remain appearance-driven, capturing visual-acoustic correlations without considering the physical factors that shape real-world sounds. We present Physics-Aware Video-to-Audio Synthesis (PAVAS), a method that incorporates physical reasoning into a latent diffusion-based V2A generation through the Physics-Driven Audio Adapter (Phy-Adapter). The adapter receives object-level physical parameters estimated by the Physical Parameter Estimator (PPE), which uses a Vision-Language Model (VLM) to infer the moving-object mass and a segmentation-based dynamic 3D reconstruction module to recover its motion trajectory for velocity computation. These physical cues enable the model to synthesize sounds that reflect underlying physical factors. To assess physical realism, we curate VGG-Impact, a benchmark focusing on object-object interactions, and introduce Audio-Physics Correlation Coefficient (APCC), an evaluation metric that measures consistency between physical and auditory attributes. Comprehensive experiments show that PAVAS produces physically plausible and perceptually coherent audio, outperforming existing V2A models in both quantitative and qualitative evaluations. Visit https://physics-aware-video-to-audio-synthesis.github.io for demo videos.

</details>


### [52] [OpenSubject: Leveraging Video-Derived Identity and Diversity Priors for Subject-driven Image Generation and Manipulation](https://arxiv.org/abs/2512.08294)
*Yexin Liu,Manyuan Zhang,Yueze Wang,Hongyu Li,Dian Zheng,Weiming Zhang,Changsheng Lu,Xunliang Cai,Yan Feng,Peng Pei,Harry Yang*

Main category: cs.CV

> 研究提出了一种新的视频驱动大规模数据集OpenSubject，用于改善模型在复杂场景下的性能。

<details>
  <summary>Details</summary>

**Motivation:** 动机是解决现有模型在生成参考身份图像时容易偏离，以及在复杂场景下处理多个主体时遇到的挑战。

**Method:** 视频驱动的大规模数据集OpenSubject包含250万样本和435万张图像，以解决现有模型在复杂场景和多目标生成时的问题。此数据集通过四阶段管道构建，利用跨帧身份优先。包括视频筛选，跨帧主体挖掘和匹配，保持身份的参考图像合成，以及验证和注释。

**Result:** 实验结果表明，使用OpenSubject进行训练可以显著提升生成和操纵性能，特别是在复杂场景中。

**Conclusion:** 结论是，OpenSubject数据集能有效改善复杂场景中的多目标生成效果，提升生成和操纵性能。

**Abstract:** Despite the promising progress in subject-driven image generation, current models often deviate from the reference identities and struggle in complex scenes with multiple subjects. To address this challenge, we introduce OpenSubject, a video-derived large-scale corpus with 2.5M samples and 4.35M images for subject-driven generation and manipulation. The dataset is built with a four-stage pipeline that exploits cross-frame identity priors. (i) Video Curation. We apply resolution and aesthetic filtering to obtain high-quality clips. (ii) Cross-Frame Subject Mining and Pairing. We utilize vision-language model (VLM)-based category consensus, local grounding, and diversity-aware pairing to select image pairs. (iii) Identity-Preserving Reference Image Synthesis. We introduce segmentation map-guided outpainting to synthesize the input images for subject-driven generation and box-guided inpainting to generate input images for subject-driven manipulation, together with geometry-aware augmentations and irregular boundary erosion. (iv) Verification and Captioning. We utilize a VLM to validate synthesized samples, re-synthesize failed samples based on stage (iii), and then construct short and long captions. In addition, we introduce a benchmark covering subject-driven generation and manipulation, and then evaluate identity fidelity, prompt adherence, manipulation consistency, and background consistency with a VLM judge. Extensive experiments show that training with OpenSubject improves generation and manipulation performance, particularly in complex scenes.

</details>


### [53] [Terrain Diffusion: A Diffusion-Based Successor to Perlin Noise in Infinite, Real-Time Terrain Generation](https://arxiv.org/abs/2512.08309)
*Alexander Goslin*

Main category: cs.CV

> Terrain Diffusion是一种AI技术，它通过InfiniteDiffusion算法克服了传统过程噪声函数在真实性和大范围一致性上的局限，能够实时生成无缝且无限的地形，适用于大规模的世界生成。

<details>
  <summary>Details</summary>

**Motivation:** 尽管过程噪声函数如Perlin噪声具备快速处理和无限生成的优点，但这些方法在真实感和大范围一致性方面存在局限性。因此，本研究旨在探索创新的方法，弥补传统过程噪声函数在真实性上的不足。

**Method:** 本研究提出了Terrain Diffusion技术，它结合了扩散模型的保真度与过程噪声函数的特性，包括无缝无限扩展、种子一致性以及常数时间随机访问。其核心是名为InfiniteDiffusion的新算法，该算法能够实时无缝地合成无边界的地形。通过分层的扩散模型将行星级上下文和局部细节结合，同时紧凑的拉普拉斯编码确保了地球规模动态范围下的稳定输出。

**Result:** 该研究通过开放源代码的无限张量框架，使用常数内存操作无界张量，并通过少步一致性蒸馏实现高效生成。研究成果表明扩散模型可以作为过程世界生成的实用基础，能够合成完全一致、可控制且无界限的行星。

**Conclusion:** 这项工作是过程噪声函数的AI时代的继承者，有望在实时渲染、视频游戏以及虚拟世界生成等领域提供更真实的无限扩展地形的解决方案。

**Abstract:** For decades, procedural worlds have been built on procedural noise functions such as Perlin noise, which are fast and infinite, yet fundamentally limited in realism and large-scale coherence. We introduce Terrain Diffusion, an AI-era successor to Perlin noise that bridges the fidelity of diffusion models with the properties that made procedural noise indispensable: seamless infinite extent, seed-consistency, and constant-time random access. At its core is InfiniteDiffusion, a novel algorithm for infinite generation, enabling seamless, real-time synthesis of boundless landscapes. A hierarchical stack of diffusion models couples planetary context with local detail, while a compact Laplacian encoding stabilizes outputs across Earth-scale dynamic ranges. An open-source infinite-tensor framework supports constant-memory manipulation of unbounded tensors, and few-step consistency distillation enables efficient generation. Together, these components establish diffusion models as a practical foundation for procedural world generation, capable of synthesizing entire planets coherently, controllably, and without limits.

</details>


### [54] [GeoDM: Geometry-aware Distribution Matching for Dataset Distillation](https://arxiv.org/abs/2512.08317)
*Xuhui Li,Zhengquan Luo,Zihui Cui,Zhiqiang Xu*

Main category: cs.CV

> 提出了一种新方法GeoDM用于数据集精炼，该方法通过引入多样化的几何结构来适应数据的内在几何特征。实验表明，这种方法在提高模型性能的同时，能够更好地泛化并匹配原数据集的分布。

<details>
  <summary>Details</summary>

**Motivation:** 现有的分布匹配方法局限于欧几里得空间，只能捕捉线性结构并忽略了数据的内在几何结构。本文提出了一种基于几何感知的分布匹配框架GeoDM，旨在解决这一问题。

**Method:** GeoDM框架通过在欧几里得、双曲和球形流形的Cartesian积空间中操作，以捕捉平坦、层次和周期性结构。它引入了可学习的曲率和权重参数来适应底层数据几何结构，并设计了一种最优传输损失以增强分布匹配的保真度。

**Result:** 理论分析表明，产品空间中的几何感知分布匹配比欧几里得对应方法具有较小的泛化误差界限。实验结果表明，该算法优于现有的数据蒸馏方法，并且在各种单一几何结构的分布匹配策略中均有效。

**Conclusion:** GeoDM框架通过合并多种几何结构，提供了一种更有效的方式进行数据集精炼，相较于纯粹使用欧几里得空间的方法能够更准确地捕捉数据集固有的几何特征。

**Abstract:** Dataset distillation aims to synthesize a compact subset of the original data, enabling models trained on it to achieve performance comparable to those trained on the original large dataset. Existing distribution-matching methods are confined to Euclidean spaces, making them only capture linear structures and overlook the intrinsic geometry of real data, e.g., curvature. However, high-dimensional data often lie on low-dimensional manifolds, suggesting that dataset distillation should have the distilled data manifold aligned with the original data manifold. In this work, we propose a geometry-aware distribution-matching framework, called \textbf{GeoDM}, which operates in the Cartesian product of Euclidean, hyperbolic, and spherical manifolds, with flat, hierarchical, and cyclical structures all captured by a unified representation. To adapt to the underlying data geometry, we introduce learnable curvature and weight parameters for three kinds of geometries. At the same time, we design an optimal transport loss to enhance the distribution fidelity. Our theoretical analysis shows that the geometry-aware distribution matching in a product space yields a smaller generalization error bound than the Euclidean counterparts. Extensive experiments conducted on standard benchmarks demonstrate that our algorithm outperforms state-of-the-art data distillation methods and remains effective across various distribution-matching strategies for the single geometries.

</details>


### [55] [Detecting Dental Landmarks from Intraoral 3D Scans: the 3DTeethLand challenge](https://arxiv.org/abs/2512.08323)
*Achraf Ben-Hamadou,Nour Neifar,Ahmed Rekik,Oussama Smaoui,Firas Bouzguenda,Sergi Pujades,Niels van Nistelrooij,Shankeeth Vinayahalingam,Kaibo Shi,Hairong Jin,Youyi Zheng,Tibor Kubík,Oldřich Kodym,Petr Šilling,Kateřina Trávníčková,Tomáš Mojžiš,Jan Matula,Jeffry Hartanto,Xiaoying Zhu,Kim-Ngan Nguyen,Tudor Dascalu,Huikai Wu,and Weijie Liu,Shaojie Zhuang,Guangshun Wei,Yuanfeng Zhou*

Main category: cs.CV

> 论文介绍了3DTeethLand挑战，旨在从口腔3D扫描中检测牙齿地标，提供了首个公开数据集，促进牙齿地标检测领域的发展。

<details>
  <summary>Details</summary>

**Motivation:** 精确地识别牙齿地标是现代临床正畸中的关键任务。这种精确识别可以实现高级诊断、个性化治疗策略和临床牙科治疗过程的更有效监控。然而，由于牙齿复杂的几何形状和个人之间的显著差异，可能会出现几个重大挑战。因此，开发先进的技术，特别是在深度学习应用方面，对于精确和可靠地检测三维牙齿地标是必要的。

**Method:** 论文并未详细描述具体的方法，而是介绍了一个公开挑战——3DTeethLand挑战，旨在促进通过深度学习等先进方法来解决牙齿地标检测的问题。

**Result:** 该论文提出了一个关于牙齿地标检测的重要挑战，即3DTeethLand挑战，该挑战是在2024年与医学图像计算和计算机辅助干预国际会议（MICCAI）合作举办的。挑战的目标是从口腔内部的3D扫描中检测牙齿地标。这是首次公开可用于3D牙齿地标检测的数据集，为评估这一任务的最先进方法提供了宝贵的资源，并鼓励社区在具有重大临床意义的问题上提出方法贡献。

**Conclusion:** 通过举办3DTeethLand挑战，并提供首个公开可用的数据集，本论文旨在推动牙齿地标检测领域的发展，鼓励社区贡献新的方法，以解决具有重大临床意义的问题。

**Abstract:** Teeth landmark detection is a critical task in modern clinical orthodontics. Their precise identification enables advanced diagnostics, facilitates personalized treatment strategies, and supports more effective monitoring of treatment progress in clinical dentistry. However, several significant challenges may arise due to the intricate geometry of individual teeth and the substantial variations observed across different individuals. To address these complexities, the development of advanced techniques, especially through the application of deep learning, is essential for the precise and reliable detection of 3D tooth landmarks. In this context, the 3DTeethLand challenge was held in collaboration with the International Conference on Medical Image Computing and Computer-Assisted Intervention (MICCAI) in 2024, calling for algorithms focused on teeth landmark detection from intraoral 3D scans. This challenge introduced the first publicly available dataset for 3D teeth landmark detection, offering a valuable resource to assess the state-of-the-art methods in this task and encourage the community to provide methodological contributions towards the resolution of their problem with significant clinical implications.

</details>


### [56] [GeoDiffMM: Geometry-Guided Conditional Diffusion for Motion Magnification](https://arxiv.org/abs/2512.08325)
*Xuedeng Liu,Jiabao Guo,Zheng Zhang,Fei Wang,Zhi Liu,Dan Guo*

Main category: cs.CV

> 提出了GeoDiffMM，一种基于扩散的视频运动放大方法，通过光流作为几何线索，有效放大微小而细微的运动，同时保持场景的结构一致性和减少噪声影响。

<details>
  <summary>Details</summary>

**Motivation:** 现有的欧拉方法虽然通过解耦表示学习等手段缓解了放大导致的噪声问题，但在小位移运动中仍然难以从光子噪声中分离出微运动。本文提出的方法旨在解决这一问题。

**Method:** GeoDiffMM采用了一种基于扩散的Lagrangian VMM框架，使用光流作为几何线索，来实现结构一致的运动放大。它包括噪声自由光学流增强策略，合成无光子噪声的非刚性运动场作为监督信息；以及基于扩散的运动放大器，该放大器在去噪过程中以光流作为几何先验条件，并通过可学习的放大因子控制放大程度，从而有选择地放大与场景语义和结构一致的运动成分，同时抑制与内容无关的扰动。最后，通过基于流的视频合成将放大的运动映射回图像域。

**Result:** 实验结果表明，GeoDiffMM超越了现有最先进的方法，并显著提升了运动放大效果。

**Conclusion:** GeoDiffMM通过结合光流引导的数据增强和基于扩散的运动放大技术，能够在保持场景结构一致性的同时放大微小运动，并有效抑制噪声。

**Abstract:** Video Motion Magnification (VMM) amplifies subtle macroscopic motions to a perceptible level. Recently, existing mainstream Eulerian approaches address amplification-induced noise via decoupling representation learning such as texture, shape and frequancey schemes, but they still struggle to separate photon noise from true micro-motion when motion displacements are very small. We propose GeoDiffMM, a novel diffusion-based Lagrangian VMM framework conditioned on optical flow as a geometric cue, enabling structurally consistent motion magnification. Specifically, we design a Noise-free Optical Flow Augmentation strategy that synthesizes diverse nonrigid motion fields without photon noise as supervision, helping the model learn more accurate geometry-aware optial flow and generalize better. Next, we develop a Diffusion Motion Magnifier that conditions the denoising process on (i) optical flow as a geometry prior and (ii) a learnable magnification factor controlling magnitude, thereby selectively amplifying motion components consistent with scene semantics and structure while suppressing content-irrelevant perturbations. Finally, we perform Flow-based Video Synthesis to map the amplified motion back to the image domain with high fidelity. Extensive experiments on real and synthetic datasets show that GeoDiffMM outperforms state-of-the-art methods and significantly improves motion magnification.

</details>


### [57] [Low Rank Support Quaternion Matrix Machine](https://arxiv.org/abs/2512.08327)
*Wang Chen,Ziyan Luo,Shuangyue Wang*

Main category: cs.CV

>  đề xuất Low-rank Support Quaternion Matrix Machine (LSQMM) dùng đại số quaternion để xử lý hình ảnh màu, với qui tắc chuẩn я- nhân để thúc đẩy cấu trúc cấp thấp, đạt hiệu quả cao trong phân loại hình ảnh.

<details>
  <summary>Details</summary>

**Motivation:**  tính toán vectơ, ma trận, hoặc tensor thứ ba trong không gian thực để đề xuất một phương pháp phân loại mới cho hình ảnh màu.

**Method:**  cải tiến phương pháp phân loại hình ảnh màu bằng cách sử dụng đại số quaternion để đại diện cho kênh màu RGB, và áp dụng qui tắc chuẩn я- nhân trong qui hoạch я- để thúc đẩy cấu trúc cấp thấp liên quan đến kênh màu.

**Result:**  thí nghiệm cho thấy phương pháp mới đạt độ chính xác, độ ổn định và hiệu quả tính toán tốt hơn so với các phương pháp tiên tiến khác.

**Conclusion:**  phương pháp phân loại hình ảnh màu mới dựa trên đại số quaternion và qui tắc chuẩn я- nhân cho thấy hiệu quả cao trong phân loại hình ảnh màu.

**Abstract:** Input features are conventionally represented as vectors, matrices, or third order tensors in the real field, for color image classification. Inspired by the success of quaternion data modeling for color images in image recovery and denoising tasks, we propose a novel classification method for color image classification, named as the Low-rank Support Quaternion Matrix Machine (LSQMM), in which the RGB channels are treated as pure quaternions to effectively preserve the intrinsic coupling relationships among channels via the quaternion algebra. For the purpose of promoting low-rank structures resulting from strongly correlated color channels, a quaternion nuclear norm regularization term, serving as a natural extension of the conventional matrix nuclear norm to the quaternion domain, is added to the hinge loss in our LSQMM model. An Alternating Direction Method of Multipliers (ADMM)-based iterative algorithm is designed to effectively resolve the proposed quaternion optimization model. Experimental results on multiple color image classification datasets demonstrate that our proposed classification approach exhibits advantages in classification accuracy, robustness and computational efficiency, compared to several state-of-the-art methods using support vector machines, support matrix machines, and support tensor machines.

</details>


### [58] [Interpreting Structured Perturbations in Image Protection Methods for Diffusion Models](https://arxiv.org/abs/2512.08329)
*Michael R. Martin,Garrick Chan,Kwan-Liu Ma*

Main category: cs.CV

> 本研究采用统一框架分析图像保护机制Glaze和Nightshade，发现它们通过结构化特征变形而非语义上的重定位来实现图像保护。

<details>
  <summary>Details</summary>

**Motivation:** 研究的动机是对最近用于保护图像的机制（如Glaze和Nightshade）的内部结构、可检测性以及表征行为进行更深入的理解，因为这些研究目前知之甚少。

**Method:** 通过潜空间聚类、特征通道激活分析、遮挡空间敏感性映射以及频域特征等方法，本研究旨在系统地分析图像防护机制的内部结构、可检测性和表征行为。

**Result:** 研究结果显示，保护机制以一种结构化的、低熵的扰动形式存在于图像中，这些扰动与底层图像内容有着紧密的关联。保护图像保留了基于内容的特征组织，并具有特定的子结构，而不是导致全局表征的漂移。

**Conclusion:** 这项工作增强了对对抗性图像保护的理解，并为未来对抗生成式AI系统的防御和检测策略设计提供了信息。

**Abstract:** Recent image protection mechanisms such as Glaze and Nightshade introduce imperceptible, adversarially designed perturbations intended to disrupt downstream text-to-image generative models. While their empirical effectiveness is known, the internal structure, detectability, and representational behavior of these perturbations remain poorly understood. This study provides a systematic, explainable AI analysis using a unified framework that integrates white-box feature-space inspection and black-box signal-level probing. Through latent-space clustering, feature-channel activation analysis, occlusion-based spatial sensitivity mapping, and frequency-domain characterization, we show that protection mechanisms operate as structured, low-entropy perturbations tightly coupled to underlying image content across representational, spatial, and spectral domains. Protected images preserve content-driven feature organization with protection-specific substructure rather than inducing global representational drift. Detectability is governed by interacting effects of perturbation entropy, spatial deployment, and frequency alignment, with sequential protection amplifying detectable structure rather than suppressing it. Frequency-domain analysis shows that Glaze and Nightshade redistribute energy along dominant image-aligned frequency axes rather than introducing diffuse noise. These findings indicate that contemporary image protection operates through structured feature-level deformation rather than semantic dislocation, explaining why protection signals remain visually subtle yet consistently detectable. This work advances the interpretability of adversarial image protection and informs the design of future defenses and detection strategies for generative AI systems.

</details>
