{"id": "2506.20747", "categories": ["cs.CL", "68T50, 68T37", "I.2.7"], "pdf": "https://arxiv.org/pdf/2506.20747", "abs": "https://arxiv.org/abs/2506.20747", "authors": ["Chen Shen", "Sajjadur Rahman", "Estevam Hruschka"], "title": "Towards Probabilistic Question Answering Over Tabular Data", "comment": null, "summary": "Current approaches for question answering (QA) over tabular data, such as\nNL2SQL systems, perform well for factual questions where answers are directly\nretrieved from tables. However, they fall short on probabilistic questions\nrequiring reasoning under uncertainty. In this paper, we introduce a new\nbenchmark LUCARIO and a framework for probabilistic QA over large tabular data.\nOur method induces Bayesian Networks from tables, translates natural language\nqueries into probabilistic queries, and uses large language models (LLMs) to\ngenerate final answers. Empirical results demonstrate significant improvements\nover baselines, highlighting the benefits of hybrid symbolic-neural reasoning.", "AI": {"tldr": "本文提出了一种新的针对大表格数据的概率问题回答框架，通过贝叶斯网络和大型语言模型提升了在处理不确定性推理问题上的性能。", "motivation": "当前的问题回答方法，如NL2SQL系统，在回答可以直接从表格中检索到答案的事实性问题时表现出色，但在需要在不确定性情况下推理的概率性问题上表现较差。为此，本文引入了新的基准LUCARIO和一个针对大表格数据的概率问题回答框架。", "method": "我们的方法从表格中诱导出贝叶斯网络，将自然语言查询转换为概率查询，并使用大型语言模型生成最终答案。", "result": "实验结果显示，本文的方法相较基线方法有显著提升，突出了混合符号-神经推理的优势。", "conclusion": "研究证明了混合符号和神经方法在处理不确定性的概率问题回答任务中具备的优势。"}}
{"id": "2506.20793", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.20793", "abs": "https://arxiv.org/abs/2506.20793", "authors": ["Victor Ojewale", "Inioluwa Deborah Raji", "Suresh Venkatasubramanian"], "title": "Multi-lingual Functional Evaluation for Large Language Models", "comment": null, "summary": "Multi-lingual competence in large language models is often evaluated via\nstatic data benchmarks such as Belebele, M-MMLU and M-GSM. However, these\nevaluations often fail to provide an adequate understanding of the practical\nperformance and robustness of models across multi-lingual settings. In\nresponse, we create multi-lingual functional benchmarks -- Cross-Lingual Grade\nSchool Math Symbolic (CL-GSM Symbolic) and Cross-Lingual Instruction-Following\nEval (CL-IFEval)-- by translating existing functional benchmark templates from\nEnglish to five additional languages that span the range of resources available\nfor NLP: French, Spanish, Hindi, Arabic and Yoruba. Our results reveal that\nsome static multi-lingual benchmarks capture functional performance much more\nclosely than others (i.e. across models, there is a 24%, 17% and 18% decrease\nin performance between M-GSM and CL-GSM Symbolic in English, French and Spanish\nrespectively; similarly there's a 15 - 24% performance drop across languages\nbetween Belebele and CL-IFEval, and only a 0.5% to 3% performance drop between\nM-MMLU and CL-IFEval). Similarly, we find that model robustness across\nlanguages varies significantly, with certain languages (eg. Arabic, English)\nbeing the most consistently well performing across evaluation iterations.", "AI": {"tldr": "本文介绍了一种新的多语言功能基准测试方法，通过创建CL-GSM Symbolic和CL-IFEval基准测试来评估大型语言模型在多种语言设置下的实用性和稳定性。", "motivation": "由于静态数据基准测试往往无法充分评估大型语言模型在多语言环境中的实际表现和鲁棒性，因此提出了一种新的多语言功能基准测试方法。", "method": "创建了跨语言功能基准测试 -- CL-GSM 符号和 CL-IFEval，通过将现有功能基准模板从英语翻译成法语、西班牙语、印地语、阿拉伯语和约鲁巴语五种语言来实现。", "result": "研究结果显示，在某些静态多语言基准测试中捕捉到的功能性能更接近实际情况（例如，M-GSM和CL-GSM符号中，在英语、法语和西班牙语中，模型性能分别下降了24%、17%和18%；在Belebele和CL-IFEval之间，性能下降了15%到24%；而M-MMLU和CL-IFEval之间的性能下降仅为0.5%到3%）。同时发现，模型在不同语言中的健壮性显著不同，某些语言（例如阿拉伯语、英语）在评估迭代中表现最为一致。", "conclusion": "跨语言功能基准测试方法能够更好地评估大型语言模型在多语言环境中的性能和鲁棒性，相比静态多语言基准测试提供了更有价值的见解。"}}
{"id": "2506.20803", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.20803", "abs": "https://arxiv.org/abs/2506.20803", "authors": ["Chenglei Si", "Tatsunori Hashimoto", "Diyi Yang"], "title": "The Ideation-Execution Gap: Execution Outcomes of LLM-Generated versus Human Research Ideas", "comment": "main paper is 14 pages", "summary": "Large Language Models (LLMs) have shown promise in accelerating the\nscientific research pipeline. A key capability for this process is the ability\nto generate novel research ideas, and prior studies have found settings in\nwhich LLM-generated research ideas were judged as more novel than human-expert\nideas. However, a good idea should not simply appear to be novel, it should\nalso result in better research after being executed. To test whether\nAI-generated ideas lead to better research outcomes, we conduct an execution\nstudy by recruiting 43 expert researchers to execute randomly-assigned ideas,\neither written by experts or generated by an LLM. Each expert spent over 100\nhours implementing the idea and wrote a 4-page short paper to document the\nexperiments. All the executed projects are then reviewed blindly by expert NLP\nresearchers. Comparing the review scores of the same ideas before and after\nexecution, the scores of the LLM-generated ideas decrease significantly more\nthan expert-written ideas on all evaluation metrics (novelty, excitement,\neffectiveness, and overall; p < 0.05), closing the gap between LLM and human\nideas observed at the ideation stage. When comparing the aggregated review\nscores from the execution study, we even observe that for many metrics there is\na flip in rankings where human ideas score higher than LLM ideas. This\nideation-execution gap highlights the limitations of current LLMs in generating\ntruly effective research ideas and the challenge of evaluating research ideas\nin the absence of execution outcomes.", "AI": {"tldr": "该研究通过执行由专家或大语言模型生成的研究想法来测试AI生成的研究想法的质量，发现虽然某些情况下AI想法在构思阶段看起来更加新颖，但执行后的评价得分更低，人类想法在许多指标上得分更高，揭示了AI生成想法在实际应用中的局限性。", "motivation": "研究的动机是测试由AI生成的想法是否能够带来更好的研究成果，因为仅仅在构思阶段新颖未必就意味着在执行后会带来更好的研究成果。", "method": "研究通过招募43位专家研究人员来执行随机分配的想法（这些想法是由专家或大语言模型生成的）来测试AI生成的想法是否能够带来更好的研究成果。每个专家花了超过100小时来实现想法并编写了一份4页的短篇文档来记录实验结果。所有执行的项目随后由专家NLP研究人员进行匿名评审。", "result": "执行研究后，评估分数显示，大语言模型生成的想法在所有评价指标（新颖性、兴奋度、有效性、总体评价）上的得分显著下降，而且与人类想法在构思阶段观察到的差距有所缩小。对于许多评估标准，甚至出现人类想法的得分高于大语言模型想法的排名逆转现象。", "conclusion": "研究结果显示，现有的大语言模型在生成真正有效研究想法方面存在局限性，并且在没有执行结果的情况下评估研究想法存在挑战。"}}
{"id": "2506.20821", "categories": ["cs.CL", "cs.AI", "cs.CE", "68T50, 68T07 (Primary) 68P20, 91G15, 91G70, 68U10 (Secondary)", "I.2.7; I.2.10; H.3.3; H.2.8; I.5.4; J.1"], "pdf": "https://arxiv.org/pdf/2506.20821", "abs": "https://arxiv.org/abs/2506.20821", "authors": ["Chinmay Gondhalekar", "Urjitkumar Patel", "Fang-Chun Yeh"], "title": "MultiFinRAG: An Optimized Multimodal Retrieval-Augmented Generation (RAG) Framework for Financial Question Answering", "comment": "Preprint Copy", "summary": "Financial documents--such as 10-Ks, 10-Qs, and investor presentations--span\nhundreds of pages and combine diverse modalities, including dense narrative\ntext, structured tables, and complex figures. Answering questions over such\ncontent often requires joint reasoning across modalities, which strains\ntraditional large language models (LLMs) and retrieval-augmented generation\n(RAG) pipelines due to token limitations, layout loss, and fragmented\ncross-modal context. We introduce MultiFinRAG, a retrieval-augmented generation\nframework purpose-built for financial QA. MultiFinRAG first performs multimodal\nextraction by grouping table and figure images into batches and sending them to\na lightweight, quantized open-source multimodal LLM, which produces both\nstructured JSON outputs and concise textual summaries. These outputs, along\nwith narrative text, are embedded and indexed with modality-aware similarity\nthresholds for precise retrieval. A tiered fallback strategy then dynamically\nescalates from text-only to text+table+image contexts when necessary, enabling\ncross-modal reasoning while reducing irrelevant context. Despite running on\ncommodity hardware, MultiFinRAG achieves 19 percentage points higher accuracy\nthan ChatGPT-4o (free-tier) on complex financial QA tasks involving text,\ntables, images, and combined multimodal reasoning.", "AI": {"tldr": "研究者开发了MultiFinRAG系统，解决金融文档多模态问答问题，其专用框架优于ChatGPT-4o (免费版)。", "motivation": "由于金融文档内容通常很复杂，结合了叙述性文本、表格、图表等，跨这些模态进行联合推理挑战了传统的大型语言模型和增强检索的生成框架。这些传统模型在处理多模态内容时面临标记限制、布局损失和碎片化的跨模态上下文等问题。因此，研究者引入MultiFinRAG来解决这些问题。", "method": "MultiFinRAG采用了一种增强检索的生成框架，专为金融问答设计。该方法首先通过分批处理表格和图像，利用轻量级、量化开源多模态语言模型生成结构化JSON输出和简洁的文本摘要。然后，将这些输出、叙述性文本与模态感知的相似度阈值嵌入并索引，以实现精确检索。接着，采用分层回退策略，根据需要从文本扩展到文本、表格和图像的上下文，从而在减少无关上下文的同时实现跨模态推理。", "result": "尽管是在通用硬件上运行的，但MultiFinRAG在需要文本、表格、图像以及多模态推理的复杂金融问答任务上，比ChatGPT-4o (免费版)准确率高出19个百分点。", "conclusion": "该研究开发了MultiFinRAG框架来解决金融文档的跨模态问答问题，通过嵌入和索引多模态内容，实现高效精准的检索，并通过层级回退策略降低无关上下文，从而取得更高的准确率，表现优于传统的大模型。"}}
{"id": "2506.20741", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.20741", "abs": "https://arxiv.org/abs/2506.20741", "authors": ["Qin Ren", "Yifan Wang", "Ruogu Fang", "Haibin Ling", "Chenyu You"], "title": "OTSurv: A Novel Multiple Instance Learning Framework for Survival Prediction with Heterogeneity-aware Optimal Transport", "comment": null, "summary": "Survival prediction using whole slide images (WSIs) can be formulated as a\nmultiple instance learning (MIL) problem. However, existing MIL methods often\nfail to explicitly capture pathological heterogeneity within WSIs, both\nglobally -- through long-tailed morphological distributions, and locally\nthrough -- tile-level prediction uncertainty. Optimal transport (OT) provides a\nprincipled way of modeling such heterogeneity by incorporating marginal\ndistribution constraints. Building on this insight, we propose OTSurv, a novel\nMIL framework from an optimal transport perspective. Specifically, OTSurv\nformulates survival predictions as a heterogeneity-aware OT problem with two\nconstraints: (1) global long-tail constraint that models prior morphological\ndistributions to avert both mode collapse and excessive uniformity by\nregulating transport mass allocation, and (2) local uncertainty-aware\nconstraint that prioritizes high-confidence patches while suppressing noise by\nprogressively raising the total transport mass. We then recast the initial OT\nproblem, augmented by these constraints, into an unbalanced OT formulation that\ncan be solved with an efficient, hardware-friendly matrix scaling algorithm.\nEmpirically, OTSurv sets new state-of-the-art results across six popular\nbenchmarks, achieving an absolute 3.6% improvement in average C-index. In\naddition, OTSurv achieves statistical significance in log-rank tests and offers\nhigh interpretability, making it a powerful tool for survival prediction in\ndigital pathology. Our codes are available at\nhttps://github.com/Y-Research-SBU/OTSurv.", "AI": {"tldr": "OTSurv, an innovative survival prediction tool for whole slide images, uses optimal transport with specific constraints to overcome the limitations of prior methods, achieving better results.", "motivation": "Existing MIL methods often fail to adequately model the heterogeneity within WSIs, both globally and locally, which OTSurv aims to address.", "method": "OTSurv is a novel multiple instance learning (MIL) framework that uses optimal transport (OT) to predict survival using whole slide images (WSIs). It includes a global constraint to model prior morphological distributions and avoid mode collapse or excessive uniformity, and a local constraint that enhances high-confidence patches while suppressing noise.", "result": "OTSurv outperformed existing methods with a significant 3.6% improvement in average C-index across six popular benchmarks. It also passed log-rank tests and provided high interpretability.", "conclusion": "OTSurv proves superior in survival prediction for digital pathology due to its ability to effectively model the heterogeneity within WSIs, setting a new standard in the field."}}
{"id": "2506.20822", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.20822", "abs": "https://arxiv.org/abs/2506.20822", "authors": ["Quintin Myers", "Yanjun Gao"], "title": "Uncovering Hidden Violent Tendencies in LLMs: A Demographic Analysis via Behavioral Vignettes", "comment": "Under review", "summary": "Large language models (LLMs) are increasingly proposed for detecting and\nresponding to violent content online, yet their ability to reason about morally\nambiguous, real-world scenarios remains underexamined. We present the first\nstudy to evaluate LLMs using a validated social science instrument designed to\nmeasure human response to everyday conflict, namely the Violent Behavior\nVignette Questionnaire (VBVQ). To assess potential bias, we introduce\npersona-based prompting that varies race, age, and geographic identity within\nthe United States. Six LLMs developed across different geopolitical and\norganizational contexts are evaluated under a unified zero-shot setting. Our\nstudy reveals two key findings: (1) LLMs surface-level text generation often\ndiverges from their internal preference for violent responses; (2) their\nviolent tendencies vary across demographics, frequently contradicting\nestablished findings in criminology, social science, and psychology.", "AI": {"tldr": "研究采用VBVQ评估了来自不同背景的六种大型语言模型对日常冲突的暴力反应，发现它们倾向于暴力反应的内在偏好与其生成文本并不一致，且这种倾向在不同人群中有所差异，与已有社会科学研究结果相左。", "motivation": "大型语言模型被越来越多地用于检测和回应网络上的暴力内容，然而，这些模型在处理现实生活中的道德模糊情境的能力尚未得到充分研究。", "method": "本研究采用了一种验证过的社会科学工具——暴力行为小品问卷（VBVQ），用于评估六个来自不同地缘政治和组织背景的大型语言模型（LLMs）对暴力内容的检测与反应能力。研究通过引入基于人物身份的提示词，变异种族、年龄和地理身份来评估潜在偏见。", "result": "研究揭示了两个关键发现：(1) LLMs生成的表面文本往往与其倾向于暴力反应的内在偏好相悖；(2) 面对暴力情况，这些模型表现出的倾向性在不同人群中有所不同，且经常与犯罪学、社会科学和心理学中的既有发现相悖。", "conclusion": "这项研究强调了在使用大型语言模型评估和响应暴力行为时需要谨慎，并指出这些模型在不同人群中的表现可能存在与现有社会科学研究相悖的趋势，需要进一步研究。"}}
{"id": "2506.20756", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.20756", "abs": "https://arxiv.org/abs/2506.20756", "authors": ["Haodong Li", "Chen Wang", "Jiahui Lei", "Kostas Daniilidis", "Lingjie Liu"], "title": "StereoDiff: Stereo-Diffusion Synergy for Video Depth Estimation", "comment": "Work done in Nov. 2024. Project page: https://stereodiff.github.io/", "summary": "Recent video depth estimation methods achieve great performance by following\nthe paradigm of image depth estimation, i.e., typically fine-tuning pre-trained\nvideo diffusion models with massive data. However, we argue that video depth\nestimation is not a naive extension of image depth estimation. The temporal\nconsistency requirements for dynamic and static regions in videos are\nfundamentally different. Consistent video depth in static regions, typically\nbackgrounds, can be more effectively achieved via stereo matching across all\nframes, which provides much stronger global 3D cues. While the consistency for\ndynamic regions still should be learned from large-scale video depth data to\nensure smooth transitions, due to the violation of triangulation constraints.\nBased on these insights, we introduce StereoDiff, a two-stage video depth\nestimator that synergizes stereo matching for mainly the static areas with\nvideo depth diffusion for maintaining consistent depth transitions in dynamic\nareas. We mathematically demonstrate how stereo matching and video depth\ndiffusion offer complementary strengths through frequency domain analysis,\nhighlighting the effectiveness of their synergy in capturing the advantages of\nboth. Experimental results on zero-shot, real-world, dynamic video depth\nbenchmarks, both indoor and outdoor, demonstrate StereoDiff's SoTA performance,\nshowcasing its superior consistency and accuracy in video depth estimation.", "AI": {"tldr": "论文提出了一种新的视频深度估计方法StereoDiff，该方法结合立体匹配和视频深度扩散技术，以满足视频中静态和动态部分的独特要求，实验结果显示了其优越性能。", "motivation": "论文指出视频深度估计并非图像深度估计的简单延伸，静态与动态区域在视频中对时间一致性有不同的要求。因此提出了一种新的方法来弥补这一差距。", "method": "StereoDiff, 一种两阶段的视频深度估计器，结合了主要用于静态区域的立体匹配与用于维持动态区域深度连续性的视频深度扩散方法。", "result": "通过零样本实验，展示了StereoDiff在室内和室外动态视频深度基准上的优异表现。", "conclusion": "实验结果显示，StereoDiff在零样本、现实世界的动态视频深度数据集上取得了SOTA性能，证明了其在视频深度估计中的一致性和准确性方面的优势。"}}
{"id": "2506.20876", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.20876", "abs": "https://arxiv.org/abs/2506.20876", "authors": ["Sebastian Joseph", "Lily Chen", "Barry Wei", "Michael Mackert", "Iain J. Marshall", "Paul Pu Liang", "Ramez Kouzy", "Byron C. Wallace", "Junyi Jessy Li"], "title": "Decide less, communicate more: On the construct validity of end-to-end fact-checking in medicine", "comment": null, "summary": "Technological progress has led to concrete advancements in tasks that were\nregarded as challenging, such as automatic fact-checking. Interest in adopting\nthese systems for public health and medicine has grown due to the high-stakes\nnature of medical decisions and challenges in critically appraising a vast and\ndiverse medical literature. Evidence-based medicine connects to every\nindividual, and yet the nature of it is highly technical, rendering the medical\nliteracy of majority users inadequate to sufficiently navigate the domain. Such\nproblems with medical communication ripens the ground for end-to-end\nfact-checking agents: check a claim against current medical literature and\nreturn with an evidence-backed verdict. And yet, such systems remain largely\nunused. To understand this, we present the first study examining how clinical\nexperts verify real claims from social media by synthesizing medical evidence.\nIn searching for this upper-bound, we reveal fundamental challenges in\nend-to-end fact-checking when applied to medicine: Difficulties connecting\nclaims in the wild to scientific evidence in the form of clinical trials;\nambiguities in underspecified claims mixed with mismatched intentions; and\ninherently subjective veracity labels. We argue that fact-checking should be\napproached and evaluated as an interactive communication problem, rather than\nan end-to-end process.", "AI": {"tldr": "该研究探讨了临床专家如何验证来自社交媒体的医学主张，揭示医学领域中端到端事实核查的根本挑战，并建议将事实核查视为一种交互通信问题而非端到端的过程来处理。", "motivation": "由于医学决策的重要性以及批判性评估大量且多样化的医学文献的挑战，对于采用这些系统的兴趣日益增长。然而，证据基础医学的高度技术性使得大多数用户的医学素养不足以充分应对。这些问题为端到端事实核查代理系统提供了成长空间，但这类系统依然未被广泛使用。本研究旨在探讨医学领域中端到端事实核查的根本挑战。", "method": "通过研究临床专家如何验证来自社交媒体的真实主张并合成医学证据，本研究寻求定义一个医学端到端事实核查的上限。", "result": "<tool_call>", "conclusion": "本研究揭示了端到端事实核查应用于医学时所面临的根本挑战，并提出应该将事实核查视为一种交互式的通信问题，而不是单纯的端到端过程。"}}
{"id": "2506.20757", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2506.20757", "abs": "https://arxiv.org/abs/2506.20757", "authors": ["Zhiyuan Wu", "Yongqiang Zhao", "Shan Luo"], "title": "ConViTac: Aligning Visual-Tactile Fusion with Contrastive Representations", "comment": null, "summary": "Vision and touch are two fundamental sensory modalities for robots, offering\ncomplementary information that enhances perception and manipulation tasks.\nPrevious research has attempted to jointly learn visual-tactile representations\nto extract more meaningful information. However, these approaches often rely on\ndirect combination, such as feature addition and concatenation, for modality\nfusion, which tend to result in poor feature integration. In this paper, we\npropose ConViTac, a visual-tactile representation learning network designed to\nenhance the alignment of features during fusion using contrastive\nrepresentations. Our key contribution is a Contrastive Embedding Conditioning\n(CEC) mechanism that leverages a contrastive encoder pretrained through\nself-supervised contrastive learning to project visual and tactile inputs into\nunified latent embeddings. These embeddings are used to couple visual-tactile\nfeature fusion through cross-modal attention, aiming at aligning the unified\nrepresentations and enhancing performance on downstream tasks. We conduct\nextensive experiments to demonstrate the superiority of ConViTac in real world\nover current state-of-the-art methods and the effectiveness of our proposed CEC\nmechanism, which improves accuracy by up to 12.0% in material classification\nand grasping prediction tasks.", "AI": {"tldr": "The paper introduces ConViTac, a new network that uses a Contrastive Embedding Conditioning mechanism to better align and integrate visual and tactile information, showing up to 12% improvement in accuracy for material classification and grasping prediction tasks compared to state-of-the-art methods.", "motivation": "The primary motivation is to address the limitations of previous joint learning approaches of visual-tactile representations, which tend to result in poor feature integration due to direct methods of modality fusion.", "method": "Our key contribution is a Contrastive Embedding Conditioning (CEC) mechanism that leverages a contrastive encoder pretrained through self-supervised contrastive learning to project visual and tactile inputs into unified latent embeddings. These embeddings are used to couple visual-tactile feature fusion through cross-modal attention, aiming at aligning the unified representations and enhancing performance on downstream tasks.", "result": "We conduct extensive experiments to demonstrate the superiority of ConViTac in real world over current state-of-the-art methods and the effectiveness of our proposed CEC mechanism, which improves accuracy by up to 12.0% in material classification and grasping prediction tasks.", "conclusion": "The paper concludes by presenting the ConViTac network with its CEC mechanism, which enhances the alignment of features during the fusion of visual and tactile inputs, leading to improved performance across various tasks compared to existing methods."}}
{"id": "2506.20917", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.20917", "abs": "https://arxiv.org/abs/2506.20917", "authors": ["Zhengyan Shi"], "title": "Optimising Language Models for Downstream Tasks: A Post-Training Perspective", "comment": "PhD Thesis", "summary": "Language models (LMs) have demonstrated remarkable capabilities in NLP, yet\nadapting them efficiently and robustly to specific tasks remains challenging.\nAs their scale and complexity grow, fine-tuning LMs on labelled data often\nunderutilizes available unlabelled data, leads to overfitting on small\ntask-specific sets, and imposes significant computational costs. These\nlimitations hamper their application to the open-ended landscape of real-world\nlanguage tasks.\n  This thesis proposes a series of methods to better adapt LMs to downstream\napplications. First, we explore strategies for extracting task-relevant\nknowledge from unlabelled data, introducing a novel continued pre-training\ntechnique that outperforms state-of-the-art semi-supervised approaches. Next,\nwe present a parameter-efficient fine-tuning method that substantially reduces\nmemory and compute costs while maintaining competitive performance. We also\nintroduce improved supervised fine-tuning methods that enable LMs to better\nfollow instructions, especially when labelled data is scarce, enhancing their\nperformance across a range of NLP tasks, including open-ended generation.\nFinally, we develop new evaluation methods and benchmarks, such as multi-hop\nspatial reasoning tasks, to assess LM capabilities and adaptation more\ncomprehensively.\n  Through extensive empirical studies across diverse NLP tasks, our results\ndemonstrate that these approaches substantially improve LM robustness,\nefficiency, and generalization, making them more adaptable to a broad range of\napplications. These advances mark a significant step towards more robust and\nefficient LMs, bringing us closer to the goal of artificial general\nintelligence.", "AI": {"tldr": "The thesis focuses on improving the adaptability of language models to downstream NLP tasks through innovative training and fine-tuning methods, and improved evaluation metrics.", "motivation": "The goal is to develop strategies to leverage unlabelled data for task-relevant knowledge extraction, lower the computational requirements for fine-tuning large language models, and improve the generalization ability of these models to a wider spectrum of real-world NLP tasks.", "method": "Our paper introduces several methods to improve the adaptation of language models to different NLP tasks. These include a new continued pre-training method that utilizes unlabelled data more effectively, a parameter-efficient fine-tuning technique that reduces resource consumption, and enhanced supervised fine-tuning that boosts performance in following instructions and handling open-ended tasks. Additionally, we propose new evaluation benchmarks to comprehensively assess model capabilities.", "result": "Empirical studies across various NLP tasks show that the proposed methods enhance the robustness, efficiency, and generalization of language models, making them better suited for diverse applications.", "conclusion": "The adopted methods are significant improvements towards achieving more robust, efficient, and generalized language models, pushing the field closer to realizing artificial general intelligence."}}
{"id": "2506.20786", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.20786", "abs": "https://arxiv.org/abs/2506.20786", "authors": ["Connor Ludwig", "Khashayar Namdar", "Farzad Khalvati"], "title": "AI-Driven MRI-based Brain Tumour Segmentation Benchmarking", "comment": null, "summary": "Medical image segmentation has greatly aided medical diagnosis, with U-Net\nbased architectures and nnU-Net providing state-of-the-art performance. There\nhave been numerous general promptable models and medical variations introduced\nin recent years, but there is currently a lack of evaluation and comparison of\nthese models across a variety of prompt qualities on a common medical dataset.\nThis research uses Segment Anything Model (SAM), Segment Anything Model 2 (SAM\n2), MedSAM, SAM-Med-3D, and nnU-Net to obtain zero-shot inference on the BraTS\n2023 adult glioma and pediatrics dataset across multiple prompt qualities for\nboth points and bounding boxes. Several of these models exhibit promising Dice\nscores, particularly SAM and SAM 2 achieving scores of up to 0.894 and 0.893,\nrespectively when given extremely accurate bounding box prompts which exceeds\nnnU-Net's segmentation performance. However, nnU-Net remains the dominant\nmedical image segmentation network due to the impracticality of providing\nhighly accurate prompts to the models. The model and prompt evaluation, as well\nas the comparison, are extended through fine-tuning SAM, SAM 2, MedSAM, and\nSAM-Med-3D on the pediatrics dataset. The improvements in point prompt\nperformance after fine-tuning are substantial and show promise for future\ninvestigation, but are unable to achieve better segmentation than bounding\nboxes or nnU-Net.", "AI": {"tldr": "本研究比较了不同模型在BraTS 2023数据集上的分割性能，发现SAM和SAM 2在极高准确度边界框提示下超越nnU-Net，但考虑到实际应用中提示准确度的限制，nnU-Net仍是主导模型。儿科数据集上的模型微调显示点提示性能有所改善。", "motivation": "当前缺乏对现有的一般提示模型及医学变体模型在公共医学数据集上进行的广泛提示质量的评估和比较。研究旨在填补这一空白，并进一步通过模型微调来扩展此评价。", "method": "该研究使用Segment Anything Model (SAM), Segment Anything Model 2 (SAM 2), MedSAM, SAM-Med-3D 和 nnU-Net，在BraTS 2023成人神经胶质瘤和儿科数据集上对多种提示质量（包括点提示和边界框提示）进行零样本推理进行评估和比较。", "result": "研究发现SAM和SAM 2在极高准确度边界框提示条件下表现出色，Dice分数最高达到0.894和0.893，分别超过nnU-Net的分割性能。然而，由于提供极高准确度提示的不可行性，nnU-Net依然占据了医学图像分割的主导地位。通过儿科数据集上的模型微调，点提示性能有所提升，显示出未来研究的潜力，但未能超越边界框或nnU-Net的分割性能。", "conclusion": "尽管SAM系列模型在高准确度提示下表现出卓越的分割性能，但鉴于实际应用中提示准确度的限制，nnU-Net依然保持其作为医学图像分割模型的优势地位。未来可以通过优化提示策略和继续微调模型来探索进一步改进。"}}
{"id": "2506.20920", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.20920", "abs": "https://arxiv.org/abs/2506.20920", "authors": ["Guilherme Penedo", "Hynek Kydlíček", "Vinko Sabolčec", "Bettina Messmer", "Negar Foroutan", "Amir Hossein Kargaran", "Colin Raffel", "Martin Jaggi", "Leandro Von Werra", "Thomas Wolf"], "title": "FineWeb2: One Pipeline to Scale Them All -- Adapting Pre-Training Data Processing to Every Language", "comment": null, "summary": "Pre-training state-of-the-art large language models (LLMs) requires vast\namounts of clean and diverse text data. While the open development of large\nhigh-quality English pre-training datasets has seen substantial recent\nprogress, training performant multilingual LLMs remains a challenge, in large\npart due to the inherent difficulty of tailoring filtering and deduplication\npipelines to a large number of languages. In this work, we introduce a new\npre-training dataset curation pipeline based on FineWeb that can be\nautomatically adapted to support any language. We extensively ablate our\npipeline design choices on a set of nine diverse languages, guided by a set of\nmeaningful and informative evaluation tasks that were chosen through a novel\nselection process based on measurable criteria. Ultimately, we show that our\npipeline can be used to create non-English corpora that produce more performant\nmodels than prior datasets. We additionally introduce a straightforward and\nprincipled approach to rebalance datasets that takes into consideration both\nduplication count and quality, providing an additional performance uplift.\nFinally, we scale our pipeline to over 1000 languages using almost 100 Common\nCrawl snapshots to produce FineWeb2, a new 20 terabyte (5 billion document)\nmultilingual dataset which we release along with our pipeline, training, and\nevaluation codebases.", "AI": {"tldr": "我们的工作介绍了一种可自动适应任何语言的多语言预训练数据集策划流程，并展示了通过该流程创建的数据集能够训练出比现有数据集更好的模型。", "motivation": "尽管高质量的大规模英语预训练数据集的公开开发取得了重大进展，但训练性能优异的多语言预训练模型仍然是一项挑战，主要是因为很难将过滤和重复数据删除流程定制为适用于大量语言。", "method": "我们介绍了一种基于FineWeb的新预训练数据集策划流程，该流程可以自动适应任何语言。我们在九种多样化语言的设置下，通过一系列有意义且具有信息量的评估任务指导，对我们的流程设计选择进行了广泛的消融研究。", "result": "我们的流程可用于创建非英语语料库，与之前的语料库相比，可以产生性能更优异的模型。我们还通过一种简单且合理的方法来重新平衡数据集，考虑重复次数和质量，提供了额外的性能提升。我们最终将流程扩展到超过1000种语言，使用了近100个Common Crawl快照，创建了FineWeb2，这是一个新的20 TB（50亿文档）多语言数据集。", "conclusion": "本研究提出了一个广泛适用于超过1000种语言的预训练数据集生成流程，而且我们还发布了FineWeb2数据集，包含50亿份文档，为多语言大模型的训练提供了宝贵的资源。"}}
{"id": "2506.20795", "categories": ["cs.CV", "cs.HC", "cs.RO", "I.2.10; I.2.9; I.5.4; I.4.8; I.4.9; H.1.2"], "pdf": "https://arxiv.org/pdf/2506.20795", "abs": "https://arxiv.org/abs/2506.20795", "authors": ["Stephanie Käs", "Anton Burenko", "Louis Markert", "Onur Alp Culha", "Dennis Mack", "Timm Linder", "Bastian Leibe"], "title": "How do Foundation Models Compare to Skeleton-Based Approaches for Gesture Recognition in Human-Robot Interaction?", "comment": null, "summary": "Gestures enable non-verbal human-robot communication, especially in noisy\nenvironments like agile production. Traditional deep learning-based gesture\nrecognition relies on task-specific architectures using images, videos, or\nskeletal pose estimates as input. Meanwhile, Vision Foundation Models (VFMs)\nand Vision Language Models (VLMs) with their strong generalization abilities\noffer potential to reduce system complexity by replacing dedicated\ntask-specific modules. This study investigates adapting such models for\ndynamic, full-body gesture recognition, comparing V-JEPA (a state-of-the-art\nVFM), Gemini Flash 2.0 (a multimodal VLM), and HD-GCN (a top-performing\nskeleton-based approach). We introduce NUGGET, a dataset tailored for\nhuman-robot communication in intralogistics environments, to evaluate the\ndifferent gesture recognition approaches. In our experiments, HD-GCN achieves\nbest performance, but V-JEPA comes close with a simple, task-specific\nclassification head - thus paving a possible way towards reducing system\ncomplexity, by using it as a shared multi-task model. In contrast, Gemini\nstruggles to differentiate gestures based solely on textual descriptions in the\nzero-shot setting, highlighting the need of further research on suitable input\nrepresentations for gestures.", "AI": {"tldr": "研究比较了不同模型在动态全身手势识别的效果，发现HD-GCN性能最佳，V-JEPA接近，提出Gemini在手势识别方面需要改进。", "motivation": "传统基于深度学习的手势识别依赖特定任务的架构，视觉基础模型（VFMs）和视觉语言模型（VLMs）凭借强大的泛化能力，可能减少系统的复杂性，取代特定的任务模块。", "method": "本研究比较了V-JEPA（一种先进视觉基础模型）、Gemini Flash 2.0（一种多模式视觉语言模型）和HD-GCN（一种性能领先的基于骨架的方法）在动态全身手势识别中的效果。", "result": "实验结果显示，HD-GCN实现了最佳性能，而V-JEPA通过简单的任务特定分类头取得了接近的性能，表明其作为多任务模型可以减少系统复杂性。Gemini在仅依靠文本描述识别手势的零样本设置下表现不佳。", "conclusion": "该研究促进了探索适用的手势识别输入表示的进一步研究。"}}
{"id": "2506.20923", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.20923", "abs": "https://arxiv.org/abs/2506.20923", "authors": ["Xinping Zhao", "Xinshuo Hu", "Zifei Shan", "Shouzheng Huang", "Yao Zhou", "Zetian Sun", "Zhenyu Liu", "Dongfang Li", "Xinyuan Wei", "Qian Chen", "Youcheng Pan", "Yang Xiang", "Meishan Zhang", "Haofen Wang", "Jun Yu", "Baotian Hu", "Min Zhang"], "title": "KaLM-Embedding-V2: Superior Training Techniques and Data Inspire A Versatile Embedding Model", "comment": "Technical Report; 26 pages 12 tables 1 figure. arXiv admin note:\n  substantial text overlap with arXiv:2501.01028", "summary": "In this paper, we propose KaLM-Embedding-V2, a versatile and compact\nembedding model, which achieves impressive performance in general-purpose text\nembedding tasks by leveraging superior training techniques and data. Our key\ninnovations include: (1) To better align the architecture with representation\nlearning, we remove the causal attention mask and adopt a fully bidirectional\ntransformer with simple yet effective mean-pooling to produce fixed-length\nembeddings; (2) We employ a multi-stage training pipeline: (i) pre-training on\nlarge-scale weakly supervised open-source corpora; (ii) fine-tuning on\nhigh-quality retrieval and non-retrieval datasets; and (iii) model-soup\nparameter averaging for robust generalization. Besides, we introduce a\nfocal-style reweighting mechanism that concentrates learning on difficult\nsamples and an online hard-negative mixing strategy to continuously enrich hard\nnegatives without expensive offline mining; (3) We collect over 20 categories\nof data for pre-training and 100 categories of data for fine-tuning, to boost\nboth the performance and generalization of the embedding model. Extensive\nevaluations on the Massive Text Embedding Benchmark (MTEB) Chinese and English\nshow that our model significantly outperforms others of comparable size, and\ncompetes with 3x, 14x, 18x, and 26x larger embedding models, setting a new\nstandard for a versatile and compact embedding model with less than 1B\nparameters.", "AI": {"tldr": "KaLM-Embedding-V2 is introduced, a compact embedding model that achieves high performance across a variety of tasks through architectural modifications and advanced training methods, outperforming similar models and competing with much larger models.", "motivation": "To create a compact yet versatile embedding model that surpasses existing models in performance and generalization across various text embedding tasks.", "method": "The paper introduces KaLM-Embedding-V2, which modifies the architecture to fully bidirectional transformer with mean-pooling. It uses a multi-stage training pipeline including pre-training on large, weakly supervised data, fine-tuning on retrieval and non-retrieval datasets, and parameter averaging. Additionally, it introduces a focal-style reweighting and online hard-negative mixing.", "result": "The model shows superior performance on the MTEB for both Chinese and English, outperforming similar-sized models and competing with much larger models with up to 26x more parameters.", "conclusion": "KaLM-Embedding-V2 sets a new standard for compact embedding models by leveraging innovative architectural adjustments, training techniques, and comprehensive data categories for pre-training and fine-tuning, achieving state-of-the-art results on the MTEB with less than 1 billion parameters."}}
{"id": "2506.20832", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.20832", "abs": "https://arxiv.org/abs/2506.20832", "authors": ["Cansu Korkmaz", "Ahmet Murat Tekalp", "Zafer Dogan"], "title": "Leveraging Vision-Language Models to Select Trustworthy Super-Resolution Samples Generated by Diffusion Models", "comment": "14 pages, 9 figures, 5 tables, accepted to IEEE Transactions on\n  Circuits and Systems for Video Technology", "summary": "Super-resolution (SR) is an ill-posed inverse problem with many feasible\nsolutions consistent with a given low-resolution image. On one hand, regressive\nSR models aim to balance fidelity and perceptual quality to yield a single\nsolution, but this trade-off often introduces artifacts that create ambiguity\nin information-critical applications such as recognizing digits or letters. On\nthe other hand, diffusion models generate a diverse set of SR images, but\nselecting the most trustworthy solution from this set remains a challenge. This\npaper introduces a robust, automated framework for identifying the most\ntrustworthy SR sample from a diffusion-generated set by leveraging the semantic\nreasoning capabilities of vision-language models (VLMs). Specifically, VLMs\nsuch as BLIP-2, GPT-4o, and their variants are prompted with structured queries\nto assess semantic correctness, visual quality, and artifact presence. The\ntop-ranked SR candidates are then ensembled to yield a single trustworthy\noutput in a cost-effective manner. To rigorously assess the validity of\nVLM-selected samples, we propose a novel Trustworthiness Score (TWS) a hybrid\nmetric that quantifies SR reliability based on three complementary components:\nsemantic similarity via CLIP embeddings, structural integrity using SSIM on\nedge maps, and artifact sensitivity through multi-level wavelet decomposition.\nWe empirically show that TWS correlates strongly with human preference in both\nambiguous and natural images, and that VLM-guided selections consistently yield\nhigh TWS values. Compared to conventional metrics like PSNR, LPIPS, which fail\nto reflect information fidelity, our approach offers a principled, scalable,\nand generalizable solution for navigating the uncertainty of the diffusion SR\nspace. By aligning outputs with human expectations and semantic correctness,\nthis work sets a new benchmark for trustworthiness in generative SR.", "AI": {"tldr": "本论文提出了一种利用视觉语言模型评估和选择扩散模型生成的超分辨率图像中最值得信赖的图像的方法，从而提高信息的清晰度和可靠性。", "motivation": "解决扩散模型生成的超分辨率图像集中选择最可靠解决方案的挑战，同时避免在重要应用中出现信息模糊问题。", "method": "通过利用视觉语言模型（VLM）的语义推理能力来识别从扩散模型生成的超分辨率图像集合中最值得信任的图像。具体来说，使用BLIP-2和GPT-4o等VLM通过结构化的查询来评估语义正确性、视觉质量和伪影存在。", "result": "提出了一种新的可信度分数（TWS），该分数基于语义相似度、结构完整性和伪影敏感性三个互补成分，能够与人类的选择偏好保持高度相关，并确保VLM指导选择的一致高质量。", "conclusion": "与传统的PSNR和LPIPS度量相比，本文的方法提供了一种更为合理、可扩展且通用的解决方案，为生成式的超分辨率图像的可靠性设定了新基准。"}}
