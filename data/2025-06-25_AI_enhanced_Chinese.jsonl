{"id": "2506.18919", "categories": ["cs.CL", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.18919", "abs": "https://arxiv.org/abs/2506.18919", "authors": ["Hexiang Gu", "Qifan Yu", "Saihui Hou", "Zhiqin Fang", "Huijia Wu", "Zhaofeng He"], "title": "MemeMind: A Large-Scale Multimodal Dataset with Chain-of-Thought Reasoning for Harmful Meme Detection", "comment": null, "summary": "The rapid development of social media has intensified the spread of harmful\ncontent. Harmful memes, which integrate both images and text, pose significant\nchallenges for automated detection due to their implicit semantics and complex\nmultimodal interactions. Although existing research has made progress in\ndetection accuracy and interpretability, the lack of a systematic, large-scale,\ndiverse, and highly explainable dataset continues to hinder further advancement\nin this field. To address this gap, we introduce MemeMind, a novel dataset\nfeaturing scientifically rigorous standards, large scale, diversity, bilingual\nsupport (Chinese and English), and detailed Chain-of-Thought (CoT) annotations.\nMemeMind fills critical gaps in current datasets by offering comprehensive\nlabeling and explicit reasoning traces, thereby providing a solid foundation\nfor enhancing harmful meme detection. In addition, we propose an innovative\ndetection framework, MemeGuard, which effectively integrates multimodal\ninformation with reasoning process modeling, significantly improving models'\nability to understand and identify harmful memes. Extensive experiments\nconducted on the MemeMind dataset demonstrate that MemeGuard consistently\noutperforms existing state-of-the-art methods in harmful meme detection tasks.", "AI": {"tldr": "本文引入了MemeMind数据集，填补了现有有害模因检测研究中的空白，并提出了一种新的检测框架MemeGuard，能在有害模因检测任务中显著优于现有的最先进的方法。", "motivation": "现有研究虽然在检测准确性和可解释性方面取得了进展，但由于缺乏系统性、大规模、多样性和高度解释性的数据集，进一步的发展受到阻碍。为了填补这一空白，作者引入了MemeMind数据集。", "method": "提出了一个创新的检测框架MemeGuard，该框架有效整合了多模态信息与推理过程建模，显著提高了模型对有害模因的理解和识别能力。", "result": "在MemeMind数据集上的广泛实验证明，MemeGuard在有害模因检测任务中始终超越现有的最先进的方法。", "conclusion": "MemeMind数据集为有害模因检测提供了坚实的基础，而MemeGuard框架通过整合多模态信息与推理过程建模提高了对有害模因的检测能力。这些努力显著改善了有害模因的自动检测。"}}
{"id": "2506.18998", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.18998", "abs": "https://arxiv.org/abs/2506.18998", "authors": ["Sahil Kale", "Vijaykant Nadadur"], "title": "Mirage of Mastery: Memorization Tricks LLMs into Artificially Inflated Self-Knowledge", "comment": "Accepted to the Pre-ACL Workshop 2025, Copenhagen", "summary": "When artificial intelligence mistakes memorization for intelligence, it\ncreates a dangerous mirage of reasoning. Existing studies treat memorization\nand self-knowledge deficits in LLMs as separate issues and do not recognize an\nintertwining link that degrades the trustworthiness of LLM responses. In our\nstudy, we utilize a novel framework to ascertain if LLMs genuinely learn\nreasoning patterns from training data or merely memorize them to assume\ncompetence across problems of similar complexity focused on STEM domains. Our\nanalysis shows a noteworthy problem in generalization: LLMs draw confidence\nfrom memorized solutions to infer a higher self-knowledge about their reasoning\nability, which manifests as an over 45% inconsistency in feasibility\nassessments when faced with self-validated, logically coherent task\nperturbations. This effect is most pronounced in science and medicine domains,\nwhich tend to have maximal standardized jargon and problems, further confirming\nour approach. Significant wavering within the self-knowledge of LLMs also shows\nflaws in current architectures and training patterns, highlighting the need for\ntechniques that ensure a balanced, consistent stance on models' perceptions of\ntheir own knowledge for maximum AI explainability and trustworthiness. Our code\nand results are available publicly at\nhttps://github.com/knowledge-verse-ai/LLM-Memorization_SK_Eval-.", "AI": {"tldr": "研究揭示了LLMs过度依赖记忆而不是推理的问题，这导致了自我认知的不一致，并表明当前架构和训练模式存在缺陷。", "motivation": "当前研究将LLMs的记忆问题和自我知识缺陷视为独立问题，没有意识到这两种问题之间有交织的联系，这会降低LLMs响应的信任度。作者动机在于揭示这一尚未被充分认识的联系。", "method": "采用了一种新框架来确定LLMs是真正从训练数据中学习推理模式，还是仅仅记忆它们以在类似复杂度的STEM领域问题上表现得像是有竞争力的。", "result": "研究揭示了显著的泛化问题：LLMs从记忆的解决方案中获得信心，假设自己对推理能力有更高的自我认知，进而导致面对逻辑一致的任务扰动时，可行性的评估不一致超过45%。这一现象在科学和医学领域尤为明显。", "conclusion": "这项研究表明，当前的AI架构和训练模式存在着缺陷，需要更有保障的方法来确保模型感知自身知识的一致性和可靠性，以提高AI解释能力和信任度。"}}
{"id": "2506.19004", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.19004", "abs": "https://arxiv.org/abs/2506.19004", "authors": ["Brian Siyuan Zheng", "Alisa Liu", "Orevaoghene Ahia", "Jonathan Hayase", "Yejin Choi", "Noah A. Smith"], "title": "Broken Tokens? Your Language Model can Secretly Handle Non-Canonical Tokenizations", "comment": "preprint", "summary": "Modern tokenizers employ deterministic algorithms to map text into a single\n\"canonical\" token sequence, yet the same string can be encoded as many\nnon-canonical tokenizations using the tokenizer vocabulary. In this work, we\ninvestigate the robustness of LMs to text encoded with non-canonical\ntokenizations entirely unseen during training. Surprisingly, when evaluated\nacross 20 benchmarks, we find that instruction-tuned models retain up to 93.4%\nof their original performance when given a randomly sampled tokenization, and\n90.8% with character-level tokenization. We see that overall stronger models\ntend to be more robust, and robustness diminishes as the tokenization departs\nfarther from the canonical form. Motivated by these results, we then identify\nsettings where non-canonical tokenization schemes can *improve* performance,\nfinding that character-level segmentation improves string manipulation and code\nunderstanding tasks by up to +14%, and right-aligned digit grouping enhances\nlarge-number arithmetic by +33%. Finally, we investigate the source of this\nrobustness, finding that it arises in the instruction-tuning phase. We show\nthat while both base and post-trained models grasp the semantics of\nnon-canonical tokenizations (perceiving them as containing misspellings), base\nmodels try to mimic the imagined mistakes and degenerate into nonsensical\noutput, while post-trained models are committed to fluent responses. Overall,\nour findings suggest that models are less tied to their tokenizer than\npreviously believed, and demonstrate the promise of intervening on tokenization\nat inference time to boost performance.", "AI": {"tldr": "研究表明，在未见过的非规范分词情况下，指令调整过的语言模型依然保持了较高的性能，并通过特定分词策略能进一步改善特定类型任务的性能。", "motivation": "动机在于探究语言模型面对未训练过的非规范分词时的鲁棒性，并验证通过改变分词方式在推理时提高模型性能的可能性。", "method": "该研究通过实验评估了在未见过的非规范分词方法下语言模型（LMs）的鲁棒性，包括随机采样分词和字符级分词。", "result": "研究结果显示，指令调整后的模型在面对随机采样分词时还能保留高达93.4%的原有性能，字符级分词时则是90.8%。此外，发现字符级分词能提高字符串操作和代码理解任务的性能达14%，而右对齐数字分组能提高大数运算的性能达33%。", "conclusion": "该研究表明模型的性能并不像以前认为的那样依赖于特定的分词器，通过在推理时干预分词，有机会提升模型性能。"}}
{"id": "2506.19028", "categories": ["cs.CL", "cs.AI", "cs.CY", "68T50", "I.2.7"], "pdf": "https://arxiv.org/pdf/2506.19028", "abs": "https://arxiv.org/abs/2506.19028", "authors": ["Weijie Xu", "Yiwen Wang", "Chi Xue", "Xiangkun Hu", "Xi Fang", "Guimin Dong", "Chandan K. Reddy"], "title": "Quantifying Fairness in LLMs Beyond Tokens: A Semantic and Statistical Perspective", "comment": "29 pages, 9 figures, 15 tables", "summary": "Large Language Models (LLMs) often generate responses with inherent biases,\nundermining their reliability in real-world applications. Existing evaluation\nmethods often overlook biases in long-form responses and the intrinsic\nvariability of LLM outputs. To address these challenges, we propose\nFiSCo(Fine-grained Semantic Computation), a novel statistical framework to\nevaluate group-level fairness in LLMs by detecting subtle semantic differences\nin long-form responses across demographic groups. Unlike prior work focusing on\nsentiment or token-level comparisons, FiSCo goes beyond surface-level analysis\nby operating at the claim level, leveraging entailment checks to assess the\nconsistency of meaning across responses. We decompose model outputs into\nsemantically distinct claims and apply statistical hypothesis testing to\ncompare inter- and intra-group similarities, enabling robust detection of\nsubtle biases. We formalize a new group counterfactual fairness definition and\nvalidate FiSCo on both synthetic and human-annotated datasets spanning gender,\nrace, and age. Experiments show that FiSco more reliably identifies nuanced\nbiases while reducing the impact of stochastic LLM variability, outperforming\nvarious evaluation metrics.", "AI": {"tldr": "FiSCo is a new statistical framework to detect subtle biases in long-form responses from LLMs, outperforming previous methods by analyzing semantic differences at the claim level and reducing the effect of model variability.", "motivation": "The motivation behind FiSCo is to tackle the inherent biases in LLM-generated responses, which are insufficiently addressed by existing evaluation methods. These methods often overlook biases in long-form responses and the intrinsic variability of LLM outputs.", "method": "FiSCo(Fine-grained Semantic Computation) is described as a novel statistical framework designed to evaluate group-level fairness in Large Language Models (LLMs) by detecting subtle semantic differences in long-form responses across demographic groups. It decomposes model outputs into semantically distinct claims and applies statistical hypothesis testing to compare inter- and intra-group similarities.", "result": "Experiments demonstrate that FiSCo reliably identifies nuanced biases while minimizing the impact of stochastic variability in LLM outputs, outperforming existing evaluation metrics.", "conclusion": "The conclusion drawn is that FiSCo is a robust method for detecting biases in LLM-generated long-form responses, providing a significant improvement over previous evaluation methods."}}
{"id": "2506.18922", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2506.18922", "abs": "https://arxiv.org/abs/2506.18922", "authors": ["Yiran Zhou", "Yingyu Wang", "Shoudong Huang", "Liang Zhao"], "title": "Correspondence-Free Multiview Point Cloud Registration via Depth-Guided Joint Optimisation", "comment": "8 pages, accepted for publication in IEEE/RSJ International\n  Conference on Intelligent Robots and Systems (IROS 2025)", "summary": "Multiview point cloud registration is a fundamental task for constructing\nglobally consistent 3D models. Existing approaches typically rely on feature\nextraction and data association across multiple point clouds; however, these\nprocesses are challenging to obtain global optimal solution in complex\nenvironments. In this paper, we introduce a novel correspondence-free multiview\npoint cloud registration method. Specifically, we represent the global map as a\ndepth map and leverage raw depth information to formulate a non-linear least\nsquares optimisation that jointly estimates poses of point clouds and the\nglobal map. Unlike traditional feature-based bundle adjustment methods, which\nrely on explicit feature extraction and data association, our method bypasses\nthese challenges by associating multi-frame point clouds with a global depth\nmap through their corresponding poses. This data association is implicitly\nincorporated and dynamically refined during the optimisation process. Extensive\nevaluations on real-world datasets demonstrate that our method outperforms\nstate-of-the-art approaches in accuracy, particularly in challenging\nenvironments where feature extraction and data association are difficult.", "AI": {"tldr": "本文提出了一种无需特征提取和数据关联的新型多视角点云配准方法，通过全局深度图和点云姿态进行优化，避免了在复杂环境中难以获得全局最优解的问题。", "motivation": "点云多视角配准是构建全局一致3D模型的基本任务。现有方法通常依赖于特征提取和数据关联，这些过程在复杂环境中难以获得全局最优解。本文旨在解决这一挑战。", "method": "本文提出了一种无对应点的多视角点云配准方法。具体来说，通过将全局地图表示为深度图，并利用原始深度信息，我们将问题表述为一个非线性最小二乘优化问题，该问题能够联合估计点云的姿态和全局地图。通过利用全局深度图和对应姿态关联多帧点云，我们避免了传统的基于特征的捆绑调整方法中显式的特征提取和数据关联的困难。此数据关联在优化过程中被隐式地包含并动态调整。", "result": "在真实数据集上的广泛评估表明，所提方法在准确性上优于最先进的方法，特别是在特征提取和数据关联困难的环境中。", "conclusion": "所提出的方法通过将全局地图表示为深度图并通过点云的姿态与全局深度图关联，有效解决了传统方法在复杂环境中遇到的问题，并在准确性上优于现有方法。"}}
{"id": "2506.19037", "categories": ["cs.CL", "cs.AI", "cs.IT", "cs.LG", "cs.NE", "math.IT"], "pdf": "https://arxiv.org/pdf/2506.19037", "abs": "https://arxiv.org/abs/2506.19037", "authors": ["Omer Luxembourg", "Haim Permuter", "Eliya Nachmani"], "title": "Plan for Speed -- Dilated Scheduling for Masked Diffusion Language Models", "comment": null, "summary": "Masked diffusion language models (MDLM) have shown strong promise for\nnon-autoregressive text generation, yet existing samplers act as implicit\nplanners, selecting tokens to unmask via denoiser confidence or entropy scores.\nSuch heuristics falter under parallel unmasking - they ignore pairwise\ninteractions between tokens and cannot account for dependencies when unmasking\nmultiple positions at once, limiting their inference time to traditional\nauto-regressive (AR) models. We introduce the Dilated-scheduled Unmasking\nStrategy (DUS), an inference-only, planner-model-free method that requires no\nadditional training. DUS leverages a first-order Markov assumption to partition\nsequence positions into dilation-based groups of non-adjacent tokens, enabling\nindependent, parallel unmasking steps that respect local context that minimizes\nthe joint entropy of each iteration step. Unlike semi-AR block approaches\n(e.g., LLADA and Dream) that still invoke the denoiser per block, DUS reduces\nthe number of denoiser calls to O(log B) per generation block - yielding\nsubstantial speedup over the O(B) run time of state-of-the-art diffusion\nmodels, where B is the block size in the semi-AR inference process. In\nexperiments on math (GSM8K) and code completion (Humaneval, MBPP) benchmarks -\ndomains suited to non-ordinal generation - DUS improves scores over parallel\nconfidence-based planner, without modifying the underlying denoiser. DUS offers\na lightweight, budget-aware approach to efficient, high-quality text\ngeneration, paving the way to unlock the true capabilities of MDLMs.", "AI": {"tldr": "文章提出了DUS方法，一种优化非自回归文本生成过程中并行去掩码策略的新方法，相比现有方法提高了生成速度和质量，特别是在数学和代码完成任务中表现优异。", "motivation": "文章旨在解决现有的非自回归文本生成模型在并行去掩码过程中的不足，这些不足在于忽略了词之间的成对交互作用，不能考虑到同时去掩码多个位置时的依赖关系。", "method": "DUS方法采用了一种推理专用的、无需额外训练的计划器模型，该方法基于一阶马尔可夫假设，将序列位置划分为不相邻的分组，以实现独立并行的去掩码步骤，减少denoiser的调用次数，从而提高生成速度。", "result": "在数学和代码完成基准测试中，DUS方法的表现优于基于并行置信度的计划器方法，并且没有修改底层的denoiser。", "conclusion": "DUS作为一种轻量级、预算感知的高效高质量文本生成方法，为释放MDLM模型的真实能力奠定了基础。"}}
{"id": "2506.18924", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.18924", "abs": "https://arxiv.org/abs/2506.18924", "authors": ["Ammar K Al Mhdawi", "Nonso Nnamoko", "Safanah Mudheher Raafat", "M. K. S. Al-Mhdawi", "Amjad J Humaidi"], "title": "Connecting Vision and Emissions: A Behavioural AI Approach to Carbon Estimation in Road Design", "comment": null, "summary": "We present an enhanced YOLOv8 real time vehicle detection and classification\nframework, for estimating carbon emissions in urban environments. The system\nenhances YOLOv8 architecture to detect, segment, and track vehicles from live\ntraffic video streams. Once a vehicle is localized, a dedicated deep\nlearning-based identification module is employed to recognize license plates\nand classify vehicle types. Since YOLOv8 lacks the built-in capacity for fine\ngrained recognition tasks such as reading license plates or determining vehicle\nattributes beyond class labels, our framework incorporates a hybrid pipeline\nwhere each detected vehicle is tracked and its bounding box is cropped and\npassed to a deep Optical Character Recognition (OCR) module. This OCR system,\ncomposed of multiple convolutional neural network (CNN) layers, is trained\nspecifically for character-level detection and license plate decoding under\nvaried conditions such as motion blur, occlusion, and diverse font styles.\nAdditionally, the recognized plate information is validated using a real time\nAPI that cross references with an external vehicle registration database to\nensure accurate classification and emission estimation. This multi-stage\napproach enables precise, automated calculation of per vehicle carbon\nemissions. Extensive evaluation was conducted using a diverse vehicle dataset\nenriched with segmentation masks and annotated license plates. The YOLOv8\ndetector achieved a mean Average Precision (mAP@0.5) of approximately 71% for\nbounding boxes and 70% for segmentation masks. Character level OCR accuracy\nreached up to 99% with the best performing CNN model. These results affirm the\nfeasibility of combining real time object detection with deep OCR for practical\ndeployment in smart transportation systems, offering a scalable solution for\nautomated, vehicle specific carbon emission monitoring.", "AI": {"tldr": "本文提出了一种增强的YOLOv8实时车辆检测与分类框架，用于估算城市环境中的碳排放。通过YOLOv8架构增强车辆的检测、分割和跟踪功能，识别车牌并分类车辆类型，结合OCR系统进行字符级检测和解码，实时API验证车牌信息以确保准确分类和排放估算。实验结果展示该框架具有高效的检测准确率和字符级OCR精准度。", "motivation": "本研究旨在开发一种结合实时车辆检测和深度OCR的方法，以实现城市环境中车辆碳排放的自动化精准监测。", "method": "使用增强的YOLOv8架构提升车辆检测、分割和跟踪功能，结合深度OCR系统进行车牌识别，采用实时API验证车牌信息以提高准确性。", "result": "系统实现了71%的平均精度（mAP@0.5）对于检测边界框和70%对于分割掩码的mAP，字符级别OCR的精度高达99%。", "conclusion": "结果证实了将实时对象检测与深度OCR结合在智慧交通系统中实际部署的可行性，提供了一种用于自动的、车辆特定的碳排放监测的可扩展解决方案。"}}
{"id": "2506.19058", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.19058", "abs": "https://arxiv.org/abs/2506.19058", "authors": ["Mike Zhang", "Rob van der Goot"], "title": "NLPnorth @ TalentCLEF 2025: Comparing Discriminative, Contrastive, and Prompt-Based Methods for Job Title and Skill Matching", "comment": "TalentCLEF 2025", "summary": "Matching job titles is a highly relevant task in the computational job market\ndomain, as it improves e.g., automatic candidate matching, career path\nprediction, and job market analysis. Furthermore, aligning job titles to job\nskills can be considered an extension to this task, with similar relevance for\nthe same downstream tasks. In this report, we outline NLPnorth's submission to\nTalentCLEF 2025, which includes both of these tasks: Multilingual Job Title\nMatching, and Job Title-Based Skill Prediction. For both tasks we compare\n(fine-tuned) classification-based, (fine-tuned) contrastive-based, and\nprompting methods. We observe that for Task A, our prompting approach performs\nbest with an average of 0.492 mean average precision (MAP) on test data,\naveraged over English, Spanish, and German. For Task B, we obtain an MAP of\n0.290 on test data with our fine-tuned classification-based approach.\nAdditionally, we made use of extra data by pulling all the language-specific\ntitles and corresponding \\emph{descriptions} from ESCO for each job and skill.\nOverall, we find that the largest multilingual language models perform best for\nboth tasks. Per the provisional results and only counting the unique teams, the\nranking on Task A is 5$^{\\text{th}}$/20 and for Task B 3$^{\\text{rd}}$/14.", "AI": {"tldr": "本文描述了NLPnorth在TalentCLEF 2025中的提交，包括多语言职位匹配和基于职位的技能预测，其中采用了各种NLP技术，并展示了大型多语言模型在两项任务中的优越性。", "motivation": "研究多语言职位匹配和基于职位的技能预测任务，旨在改进自动候选人匹配、职业路径预测和就业市场分析。", "method": "该研究采用了分类方法、对比方法和提示方法，特别是对于任务A，提示方法表现出色，而对于任务B，则是微调过的分类方法表现较好。", "result": "在任务A中，提示方法的平均精确度（MAP）为0.492；在任务B中，微调分类方法的MAP为0.290。另外，还使用了来自ESCO的额外数据以增强模型性能。", "conclusion": "大型多语言语言模型在这两项任务中表现最佳。根据中期结果，单独计算的独特团队排名中，任务A排名第5/20，任务B排名第3/14。"}}
{"id": "2506.18925", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.18925", "abs": "https://arxiv.org/abs/2506.18925", "authors": ["Tahereh Zarrat Ehsan", "Michael Tangermann", "Yağmur Güçlütürk", "Bastiaan R. Bloem", "Luc J. W. Evers"], "title": "Interpretable and Granular Video-Based Quantification of Motor Characteristics from the Finger Tapping Test in Parkinson Disease", "comment": null, "summary": "Accurately quantifying motor characteristics in Parkinson disease (PD) is\ncrucial for monitoring disease progression and optimizing treatment strategies.\nThe finger-tapping test is a standard motor assessment. Clinicians visually\nevaluate a patient's tapping performance and assign an overall severity score\nbased on tapping amplitude, speed, and irregularity. However, this subjective\nevaluation is prone to inter- and intra-rater variability, and does not offer\ninsights into individual motor characteristics captured during this test. This\npaper introduces a granular computer vision-based method for quantifying PD\nmotor characteristics from video recordings. Four sets of clinically relevant\nfeatures are proposed to characterize hypokinesia, bradykinesia, sequence\neffect, and hesitation-halts. We evaluate our approach on video recordings and\nclinical evaluations of 74 PD patients from the Personalized Parkinson Project.\nPrincipal component analysis with varimax rotation shows that the video-based\nfeatures corresponded to the four deficits. Additionally, video-based analysis\nhas allowed us to identify further granular distinctions within sequence effect\nand hesitation-halts deficits. In the following, we have used these features to\ntrain machine learning classifiers to estimate the Movement Disorder Society\nUnified Parkinson Disease Rating Scale (MDS-UPDRS) finger-tapping score.\nCompared to state-of-the-art approaches, our method achieves a higher accuracy\nin MDS-UPDRS score prediction, while still providing an interpretable\nquantification of individual finger-tapping motor characteristics. In summary,\nthe proposed framework provides a practical solution for the objective\nassessment of PD motor characteristics, that can potentially be applied in both\nclinical and remote settings. Future work is needed to assess its\nresponsiveness to symptomatic treatment and disease progression.", "AI": {"tldr": "本文提出了一种基于计算机视觉的方法，用于量化帕金森病患者的运动特征，通过该方法预测评分的准确率高于目前最先进的方法，同时提供深入了解个体运动特征的能力。", "motivation": "准确量化帕金森病的运动特征对于监测疾病进展和优化治疗策略至关重要。现有的手指敲击测试依赖于临床医生的主观评估，这种评估容易受到评分者间和评分者内变异的影响，且无法提供测试过程中捕捉到的个体运动特征的具体见解。", "method": "本论文提出了一种基于计算机视觉的细化方法，用于从视频记录中量化帕金森病的运动特征。研究提出了四个临床相关的特征集来描述运动迟缓、动作缓慢、序列效应和犹豫停顿。", "result": "通过使用主要成分分析和方差极大化旋转，他们的视频特征与四个缺陷相对应，并且还能够识别序列效应和犹豫停顿缺陷更细粒度的区分。此外，在预测MDS-UPDRS评分这一任务中，与最先进的方法相比，该方法达到了更高的准确率，并且仍能提供对个体手指敲击运动特征的可解释量化。", "conclusion": "本论文定义了一种实用框架，用于帕金森病运动特性的客观评估，该框架可能同时应用于临床和远程环境中。未来的研究需要评估其对症状治疗和疾病进展的敏感性。"}}
{"id": "2506.19073", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.19073", "abs": "https://arxiv.org/abs/2506.19073", "authors": ["Jackson Trager", "Francielle Vargas", "Diego Alves", "Matteo Guida", "Mikel K. Ngueajio", "Ameeta Agrawal", "Flor Plaza-del-Arco", "Yalda Daryanai", "Farzan Karimi-Malekabadi"], "title": "MFTCXplain: A Multilingual Benchmark Dataset for Evaluating the Moral Reasoning of LLMs through Hate Speech Multi-hop Explanation", "comment": "Under Review", "summary": "Ensuring the moral reasoning capabilities of Large Language Models (LLMs) is\na growing concern as these systems are used in socially sensitive tasks.\nNevertheless, current evaluation benchmarks present two major shortcomings: a\nlack of annotations that justify moral classifications, which limits\ntransparency and interpretability; and a predominant focus on English, which\nconstrains the assessment of moral reasoning across diverse cultural settings.\nIn this paper, we introduce MFTCXplain, a multilingual benchmark dataset for\nevaluating the moral reasoning of LLMs via hate speech multi-hop explanation\nusing Moral Foundation Theory (MFT). The dataset comprises 3,000 tweets across\nPortuguese, Italian, Persian, and English, annotated with binary hate speech\nlabels, moral categories, and text span-level rationales. Empirical results\nhighlight a misalignment between LLM outputs and human annotations in moral\nreasoning tasks. While LLMs perform well in hate speech detection (F1 up to\n0.836), their ability to predict moral sentiments is notably weak (F1 < 0.35).\nFurthermore, rationale alignment remains limited mainly in underrepresented\nlanguages. These findings show the limited capacity of current LLMs to\ninternalize and reflect human moral reasoning.", "AI": {"tldr": "本文提出MFTCXplain，一个多语言数据集，用于评测LLMs在基于Moral Foundation Theory的多跳解释性道德推理方面的表现，发现虽然LLMs在仇恨言论检测上表现良好，但在道德推理尤其是非主流语言上的表现较弱。", "motivation": "研究动机在于解决现有评估基准在评价LLMs的道德推理能力时存在的两大不足：缺乏对道德分类的注释解释，这限制了透明度和可解释性；以及主要聚焦于英语，限制了跨文化背景下的道德推理评价。", "method": "本研究提出了一种多语言基准数据集MFTCXplain，用于通过基于道德基础理论（MFT）的仇恨言论多跳解释来评估大型语言模型（LLMs）的道德推理能力。数据集包括葡萄牙语、意大利语、波斯语和英语的3000条推文，并注释有二元仇恨言论标签、道德类别和文本片段级别的解释。", "result": "实证结果显示，在道德推理任务中，LLMs的输出与人类注释之间存在一定程度上的不一致。LLMs在仇恨言论检测中表现良好（最高F1值达0.836），但在预测道德情绪方面能力较弱（F1值低于0.35）。此外，在代表性较低的语言中，解释的对齐度依然有限。", "conclusion": "研究发现目前的LLMs在内化和反映人类道德推理方面的能力有限。"}}
{"id": "2506.18930", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.18930", "abs": "https://arxiv.org/abs/2506.18930", "authors": ["Chong Di", "Shuwang Zhou", "Da Chen", "Jean-Marie Mirebeau", "Minglei Shu", "Laurent D. Cohen"], "title": "Reinforcement Learning-Based Dynamic Grouping for Tubular Structure Tracking", "comment": null, "summary": "The computation of minimal paths for the applications in tracking tubular\nstructures such as blood vessels and roads is challenged by complex\nmorphologies and environmental variations. Existing approaches can be roughly\ncategorized into two research lines: the point-wise based models and the\nsegment-wise based models. Although segment-wise approaches have obtained\npromising results in many scenarios, they often suffer from computational\ninefficiency and heavily rely on a prescribed prior to fit the target elongated\nshapes. We propose a novel framework that casts segment-wise tracking as a\nMarkov Decision Process (MDP), enabling a reinforcement learning approach. Our\nmethod leverages Q-Learning to dynamically explore a graph of segments,\ncomputing edge weights on-demand and adaptively expanding the search space.\nThis strategy avoids the high cost of a pre-computed graph and proves robust to\nincomplete initial information. Experimental reuslts on typical tubular\nstructure datasets demonstrate that our method significantly outperforms\nstate-of-the-art point-wise and segment-wise approaches. The proposed method\neffectively handles complex topologies and maintains global path coherence\nwithout depending on extensive prior structural knowledge.", "AI": {"tldr": "本文提出了一种利用Q-Learning进行动态段追踪的新框架，适用于管状结构的追踪，效果显著优于现有方法。", "motivation": "现有的基于点和基于段的方法在处理管状结构的追踪问题时，效率低或需要大量的先验知识。", "method": "本文提出了一种将基于段的追踪视为马尔可夫决策过程（MDP）的新框架，利用Q-Learning动态探索一个段的图，按需计算边的权重并自适应扩展搜索空间。", "result": "实验结果表明，该方法显著优于现有的基于点和基于段的方法，在处理复杂的管状结构时，效果显著。", "conclusion": "该方法有效地处理了复杂的拓扑结构，并保持了全局路径的一致性，无需依赖大量的结构性先验知识。"}}
{"id": "2506.19089", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.19089", "abs": "https://arxiv.org/abs/2506.19089", "authors": ["Nathaniel Getachew", "Abulhair Saparov"], "title": "Language Models Might Not Understand You: Evaluating Theory of Mind via Story Prompting", "comment": "14 pages, 11 figures", "summary": "We introduce $\\texttt{StorySim}$, a programmable framework for synthetically\ngenerating stories to evaluate the theory of mind (ToM) and world modeling (WM)\ncapabilities of large language models (LLMs). Unlike prior benchmarks that may\nsuffer from contamination in pretraining data, $\\texttt{StorySim}$ produces\nnovel, compositional story prompts anchored by a highly controllable\n$\\texttt{Storyboard}$, enabling precise manipulation of character perspectives\nand events. We use this framework to design first- and second-order ToM tasks\nalongside WM tasks that control for the ability to track and model mental\nstates. Our experiments across a suite of state-of-the-art LLMs reveal that\nmost models perform better on WM tasks than ToM tasks, and that models tend to\nperform better reasoning with humans compared to inanimate objects.\nAdditionally, our framework enabled us to find evidence of heuristic behavior\nsuch as recency bias and an over-reliance on earlier events in the story. All\ncode for generating data and evaluations is freely available.", "AI": {"tldr": "本文介绍了一个用于评估大型语言模型ToM和WM能力的可编程框架\\texttt{StorySim}，实验表明该框架能揭示模型在不同任务上的表现差异及其行为偏差。", "motivation": "大型语言模型的评估通常受到之前基准测试使用被预训练数据污染的故事的限制。为了精确评估这些模型的ToM和WM能力，需要一个能生成可控和新颖故事的框架。", "method": "我们提出了一个名为\\texttt{StorySim}的可编程框架，用于生成故事以评估大型语言模型的“心灵理论”(ToM)和世界观建模(WM)能力。通过一个高度可控的\\texttt{Storyboard}，该框架可以精确操控角色视角和事件，从而避免之前基准测试中的预训练数据污染问题。", "result": "实验表明，大多数模型在WM任务上的表现优于ToM任务，并且在处理人类对象时的推理能力优于无生命对象。此外，模型在处理故事时存在偏差，比如近因偏差和过度依赖故事早期情节的现象。", "conclusion": "通过\\texttt{StorySim}框架，研究者和开发者可以更好地理解大型语言模型在理解和预测复杂社会情境中的表现，并为未来发展提供了方向。"}}
{"id": "2506.18938", "categories": ["cs.CV", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2506.18938", "abs": "https://arxiv.org/abs/2506.18938", "authors": ["Yanke Wang", "Yu Hin Ng", "Haobo Liang", "Ching-Wei Chang", "Hao Chen"], "title": "Bird's-eye view safety monitoring for the construction top under the tower crane", "comment": null, "summary": "The tower crane is involving more automated and intelligent operation\nprocedure, and importantly, the application of automation technologies to the\nsafety issues is imperative ahead of the utilization of any other advances.\nAmong diverse risk management tasks on site, it is essential to protect the\nhuman workers on the workspace between the tower crane and constructed building\ntop area (construction top) from the bird's-eye view, especially with Modular\nIntegrated Construction (MiC) lifted. Also, the camera and Light Detection And\nRanging (LiDAR) can capture abundant 3D information on site, which is however\nyet made the best use. Considering the safety protection for humans and tower\ncranes, we present an AI-based fully automated safety monitoring system for\ntower crane lifting from the bird's-eye view, surveilling to shield the human\nworkers on the construction top and avoid cranes' collision by alarming the\ncrane operator. The system achieved a 3D data fusion for localization of humans\nand MiCs by integrating the captured information from camera and LiDAR. The\nstate-of-the-art methods were explored and implemented into our proposed\nsoftware pipeline coupled with the hardware and display systems. Furthermore,\nwe conducted an analysis of the components in the pipeline to verify the\naccuracy and effectiveness of the involved methods. The display and\nvisualization on the real site proved that our system can serve as a valuable\nsafety monitoring toolkit on site.", "AI": {"tldr": "本文提出了一种基于AI的全自动安全监控系统，用于塔式起重机的鸟瞰视角吊装监测，确保工人安全，避免碰撞。", "motivation": "鉴于塔式起重机操作中越来越多的自动化和智能化程序，以及自动化技术在安全问题中的重要性，特别是保护塔式起重机与建筑顶部区域之间的工人的安全，在采用任何其他先进功能之前，这变得尤为迫切。", "method": "我们提出了一种基于AI的全自动安全监控系统，用于从鸟瞰视角监测塔式起重机的吊装过程。该系统整合了摄像头和LiDAR收集的信息，实现了3D数据融合，用于定位人类工人和MiCs（模块化集成建筑），并警报起重机操作员避免碰撞和保护工人的安全。", "result": "通过对所涉及方法的组件分析，验证了系统的准确性和有效性。实际施工现场的显示和可视化证明了该系统可以作为有价值的工地安全监控工具包。", "conclusion": "我们展示了在塔式起重机操作中融合摄像头和LiDAR的3D信息，提出了一个软硬件结合的安全监控系统，并在实际工地现场展示了它的应用价值。"}}
{"id": "2506.19113", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.19113", "abs": "https://arxiv.org/abs/2506.19113", "authors": ["Ramaravind K. Mothilal", "Joanna Roy", "Syed Ishtiaque Ahmed", "Shion Guha"], "title": "Human-Aligned Faithfulness in Toxicity Explanations of LLMs", "comment": "21 pages, 5 figures, 7 tables", "summary": "The discourse around toxicity and LLMs in NLP largely revolves around\ndetection tasks. This work shifts the focus to evaluating LLMs' reasoning about\ntoxicity -- from their explanations that justify a stance -- to enhance their\ntrustworthiness in downstream tasks. Despite extensive research on\nexplainability, it is not straightforward to adopt existing methods to evaluate\nfree-form toxicity explanation due to their over-reliance on input text\nperturbations, among other challenges. To account for these, we propose a\nnovel, theoretically-grounded multi-dimensional criterion, Human-Aligned\nFaithfulness (HAF), that measures the extent to which LLMs' free-form toxicity\nexplanations align with those of a rational human under ideal conditions. We\ndevelop six metrics, based on uncertainty quantification, to comprehensively\nevaluate \\haf of LLMs' toxicity explanations with no human involvement, and\nhighlight how \"non-ideal\" the explanations are. We conduct several experiments\non three Llama models (of size up to 70B) and an 8B Ministral model on five\ndiverse toxicity datasets. Our results show that while LLMs generate plausible\nexplanations to simple prompts, their reasoning about toxicity breaks down when\nprompted about the nuanced relations between the complete set of reasons, the\nindividual reasons, and their toxicity stances, resulting in inconsistent and\nnonsensical responses. We open-source our code and LLM-generated explanations\nat https://github.com/uofthcdslab/HAF.", "AI": {"tldr": "研究提出了一种新的评估标准HAF来衡量大语言模型(LLMs)生成的毒性问题解释与理想条件下的人类解释的一致性，并通过实验表明，LLMs在处理复杂毒性问题推理时存在局限性。", "motivation": "讨论围绕着NLP中的大语言模型(LLMs)的毒性问题大多集中在检测任务上，本研究将关注点转向评估LLMs关于毒性的推理，特别是它们解释支持一种立场的方式，以提高它们在下游任务中的可信度。", "method": "提出了一种新的理论基础的多维标准Human-Aligned Faithfulness (HAF)，用于度量大语言模型(LLMs)生成的关于毒性问题的自由形式解释与理想条件下的人类解释的一致性。开发了六个基于不确定性量化指标来全面评估LLMs的毒性解释的可信度，这些指标无需人类参与，可以衡量解释的“非理想程度”。", "result": "在三个Llama模型（最大规模70B）和一个8B Ministral模型上，使用五个不同的毒性数据集进行实验，结果显示，虽然LLMs可以生成对简单提示的合理解释，但它们在处理复杂的、有关毒性问题的相关因素时，推理能力会下降，可能导致不一致和不合逻辑的回应。", "conclusion": "研究揭示了大语言模型在处理复杂和多维度的毒性问题推理时的局限性，即使在生成看似合理的解释时，也可能出现推理断裂，产生不一致和不合理的回应。"}}
{"id": "2506.18939", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.18939", "abs": "https://arxiv.org/abs/2506.18939", "authors": ["Rui An", "Yifeng Zhang", "Ziran Liang", "Wenqi Fan", "Yuxuan Liang", "Xuequn Shang", "Qing Li"], "title": "Damba-ST: Domain-Adaptive Mamba for Efficient Urban Spatio-Temporal Prediction", "comment": null, "summary": "Training urban spatio-temporal foundation models that generalize well across\ndiverse regions and cities is critical for deploying urban services in unseen\nor data-scarce regions. Recent studies have typically focused on fusing\ncross-domain spatio-temporal data to train unified Transformer-based models.\nHowever, these models suffer from quadratic computational complexity and high\nmemory overhead, limiting their scalability and practical deployment. Inspired\nby the efficiency of Mamba, a state space model with linear time complexity, we\nexplore its potential for efficient urban spatio-temporal prediction. However,\ndirectly applying Mamba as a spatio-temporal backbone leads to negative\ntransfer and severe performance degradation. This is primarily due to\nspatio-temporal heterogeneity and the recursive mechanism of Mamba's hidden\nstate updates, which limit cross-domain generalization. To overcome these\nchallenges, we propose Damba-ST, a novel domain-adaptive Mamba-based model for\nefficient urban spatio-temporal prediction. Damba-ST retains Mamba's linear\ncomplexity advantage while significantly enhancing its adaptability to\nheterogeneous domains. Specifically, we introduce two core innovations: (1) a\ndomain-adaptive state space model that partitions the latent representation\nspace into a shared subspace for learning cross-domain commonalities and\nindependent, domain-specific subspaces for capturing intra-domain\ndiscriminative features; (2) three distinct Domain Adapters, which serve as\ndomain-aware proxies to bridge disparate domain distributions and facilitate\nthe alignment of cross-domain commonalities. Extensive experiments demonstrate\nthe generalization and efficiency of Damba-ST. It achieves state-of-the-art\nperformance on prediction tasks and demonstrates strong zero-shot\ngeneralization, enabling seamless deployment in new urban environments without\nextensive retraining or fine-tuning.", "AI": {"tldr": "本研究提出Damba-ST模型，利用线性复杂度优势同时提高城市时空预测的跨域泛化能力，通过划分共享和独立的子空间和使用域适配器，实现了高效且准确的预测，并且具备在新城市环境中的零样本泛化能力。", "motivation": "研究动机在于解决现有的基于Transformer的模型在处理城市时空预测任务时计算复杂度高和内存开销大的问题。同时，直接应用Mamba模型又会由于时空异质性和隐藏状态更新的递归机制导致模型性能下降。因此，提出了一种改进的模型来克服这些挑战。", "method": "我们提出了一种名为Damba-ST的新模型，旨在通过两种核心创新来提高跨域泛化能力。首先，模型采用域自适应状态空间模型，将潜在表示空间划分为学习跨域共同特征的共享子空间和捕获域内辨别特征的特定子空间。其次，引入了三种不同的域适配器，作为域感知代理，用于弥合不同域之间的分布差距并促进跨域共同特征的对齐。", "result": "实验结果表明，Damba-ST模型具有良好的跨域泛化能力和高效性。它在预测任务中达到了最先进的性能水平，并展示了强大的零样本泛化能力。", "conclusion": "研究结论为，Damba-ST模型能够有效地进行城市时空预测，并且具备零样本泛化能力，能够在新的城市环境中无缝部署，无需大量的再训练或微调。"}}
{"id": "2506.19159", "categories": ["cs.CL", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.19159", "abs": "https://arxiv.org/abs/2506.19159", "authors": ["Yun Tang", "Eesung Kim", "Vijendra Raj Apsingekar"], "title": "Enhanced Hybrid Transducer and Attention Encoder Decoder with Text Data", "comment": "Accepted by Interspeech2025", "summary": "A joint speech and text optimization method is proposed for hybrid transducer\nand attention-based encoder decoder (TAED) modeling to leverage large amounts\nof text corpus and enhance ASR accuracy. The joint TAED (J-TAED) is trained\nwith both speech and text input modalities together, while it only takes speech\ndata as input during inference. The trained model can unify the internal\nrepresentations from different modalities, and be further extended to\ntext-based domain adaptation. It can effectively alleviate data scarcity for\nmismatch domain tasks since no speech data is required. Our experiments show\nJ-TAED successfully integrates speech and linguistic information into one\nmodel, and reduce the WER by 5.8 ~12.8% on the Librispeech dataset. The model\nis also evaluated on two out-of-domain datasets: one is finance and another is\nnamed entity focused. The text-based domain adaptation brings 15.3% and 17.8%\nWER reduction on those two datasets respectively.", "AI": {"tldr": "A hybrid TAED model, trained jointly with speech and text, enhances ASR accuracy and reduces WER across various datasets.", "motivation": "The goal is to leverage large amounts of text corpus and enhance ASR accuracy.", "method": "A joint speech and text optimization method is proposed for hybrid transducer and attention-based encoder decoder (TAED) modeling.", "result": "The J-TAED reduces WER by 5.8 ~12.8% on the Librispeech dataset and 15.3% and 17.8% WER reduction on out-of-domain datasets.", "conclusion": "The proposed method unifies internal representations from different modalities and can be extended to text-based domain adaptation with significant WER reduction."}}
{"id": "2506.18943", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.18943", "abs": "https://arxiv.org/abs/2506.18943", "authors": ["Andrew Kiruluta", "Priscilla Burity"], "title": "From Pixels and Words to Waves: A Unified Framework for Spectral Dictionary vLLMs", "comment": null, "summary": "Vision-language models (VLMs) unify computer vision and natural language\nprocessing in a single architecture capable of interpreting and describing\nimages. Most state-of-the-art systems rely on two computationally intensive\ncomponents: convolutions in the vision encoder and quadratic self-attention for\nmultimodal fusion. This work removes both by introducing a spectral dictionary\ntoken mixer, which represents each image patch or wordpiece as a sparse\ncombination of learnable frequency atoms. Our 1.1B-parameter prototype,\nSDict-VLM, achieves BLEU-4 of 39.2, CIDEr of 127.5, and SPICE of 27.0 on\nMS-COCO captioning, along with 50.3 percent accuracy on VQAv2. These results\nclose approximately 85 percent of the performance gap to BLIP-2 while using 60\npercent fewer parameters, 2.3 times less peak GPU memory, and 2.2 times faster\ninference than PaLI-3. To our knowledge, this is the first VLM to eliminate\nboth convolutions and self-attention while matching mid-scale transformer\nbaselines. In addition to its O(L log L) complexity, the shared frequency\ndictionary enables transparent cross-modal alignment and offers a tunable\ntrade-off between accuracy and compute, paving the way for efficient and\ninterpretable VLMs.", "AI": {"tldr": "提出SDict-VLM，通过频谱字典标记混合器去除卷积和二次自我注意机制，实现了性能接近BLIP-2但参数少60%，GPU内存需求少2.3倍，推理速度提高2.2倍。", "motivation": "大多数最先进的系统依赖于计算密集型组件，比如视觉编码器中的卷积和多模态融合的二次自我注意。这项工作通过移除这两个组件来提高效率和可解释性。", "method": "引入了频谱字典标记混合器，每个图像块或单词片段表示为可学习频率原子的稀疏组合，从而消除了视觉编码器中的卷积和多模态融合的二次自我注意。", "result": "11亿参数的原型模型SDict-VLM在MS-COCO标题生成任务上达到了BLEU-4 39.2，CIDEr 127.5，SPICE 27.0，并在VQAv2任务上的准确率为50.3%。", "conclusion": "这是首个消除卷积和自注意力机制的同时匹配中型变压器基线的视觉-语言模型。其O(L log L)复杂度，共享频率字典有利于透明的跨模态对齐，并可以在精度和计算之间提供可调的权衡，为高效的和可解释的视觉-语言模型铺平了道路。"}}
{"id": "2506.19187", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.19187", "abs": "https://arxiv.org/abs/2506.19187", "authors": ["Christopher Toukmaji", "Jeffrey Flanigan"], "title": "Prompt, Translate, Fine-Tune, Re-Initialize, or Instruction-Tune? Adapting LLMs for In-Context Learning in Low-Resource Languages", "comment": "Accepted to ACL GEM 2025", "summary": "LLMs are typically trained in high-resource languages, and tasks in\nlower-resourced languages tend to underperform the higher-resource language\ncounterparts for in-context learning. Despite the large body of work on\nprompting settings, it is still unclear how LLMs should be adapted\ncross-lingually specifically for in-context learning in the low-resource target\nlanguages. We perform a comprehensive study spanning five diverse target\nlanguages, three base LLMs, and seven downstream tasks spanning over 4,100 GPU\ntraining hours (9,900+ TFLOPs) across various adaptation techniques: few-shot\nprompting, translate-test, fine-tuning, embedding re-initialization, and\ninstruction fine-tuning. Our results show that the few-shot prompting and\ntranslate-test settings tend to heavily outperform the gradient-based\nadaptation methods. To better understand this discrepancy, we design a novel\nmetric, Valid Output Recall (VOR), and analyze model outputs to empirically\nattribute the degradation of these trained models to catastrophic forgetting.\nTo the extent of our knowledge, this is the largest study done on in-context\nlearning for low-resource languages with respect to train compute and number of\nadaptation techniques considered. We make all our datasets and trained models\navailable for public use.", "AI": {"tldr": "本文进行了一场大规模的关于低资源语言上下文学习的研究，比较了不同的适应技术，并提出了一个新的度量标准来解释模型训练中的性能下降问题。", "motivation": "尽管有大量关于提示设置的工作，但仍不清楚LLMs应如何跨语言适应低资源目标语言的上下文学习。本文旨在解决这一问题。", "method": "本文通过全面研究跨越五种目标语言、三种基础语言模型和七个下游任务，使用4100个GPU训练小时（超过9900 TFLOPs），比较了几种跨语言适应技术：少样本提示、翻译测试、微调、嵌入重新初始化和指令微调。", "result": "研究结果表明，少样本提示和翻译测试设置明显优于基于梯度的适应方法。本文还提出了一种新的度量标准VOR，分析模型输出表明训练模型的性能下降主要是由灾难性遗忘引起的。", "conclusion": "这是首次在低资源语言的上下文学习研究中涉及如此多的适应技术以及如此高的训练计算量，所有数据集和训练模型都将公开供使用。"}}
{"id": "2506.18946", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.18946", "abs": "https://arxiv.org/abs/2506.18946", "authors": ["Zhe Dong", "Yuzhe Sun", "Tianzhu Liu", "Yanfeng Gu"], "title": "DiffRIS: Enhancing Referring Remote Sensing Image Segmentation with Pre-trained Text-to-Image Diffusion Models", "comment": null, "summary": "Referring remote sensing image segmentation (RRSIS) enables the precise\ndelineation of regions within remote sensing imagery through natural language\ndescriptions, serving critical applications in disaster response, urban\ndevelopment, and environmental monitoring. Despite recent advances, current\napproaches face significant challenges in processing aerial imagery due to\ncomplex object characteristics including scale variations, diverse\norientations, and semantic ambiguities inherent to the overhead perspective. To\naddress these limitations, we propose DiffRIS, a novel framework that harnesses\nthe semantic understanding capabilities of pre-trained text-to-image diffusion\nmodels for enhanced cross-modal alignment in RRSIS tasks. Our framework\nintroduces two key innovations: a context perception adapter (CP-adapter) that\ndynamically refines linguistic features through global context modeling and\nobject-aware reasoning, and a progressive cross-modal reasoning decoder (PCMRD)\nthat iteratively aligns textual descriptions with visual regions for precise\nsegmentation. The CP-adapter bridges the domain gap between general\nvision-language understanding and remote sensing applications, while PCMRD\nenables fine-grained semantic alignment through multi-scale feature\ninteraction. Comprehensive experiments on three benchmark datasets-RRSIS-D,\nRefSegRS, and RISBench-demonstrate that DiffRIS consistently outperforms\nexisting methods across all standard metrics, establishing a new\nstate-of-the-art for RRSIS tasks. The significant performance improvements\nvalidate the effectiveness of leveraging pre-trained diffusion models for\nremote sensing applications through our proposed adaptive framework.", "AI": {"tldr": "提出DiffRIS框架，利用预训练的文本到图像扩散模型提高遥感图像语义理解，实验表明该框架在基准数据集上表现优于现有方法。", "motivation": "遥感图像分割存在尺度变化、多样方向和语义歧义等挑战。DiffRIS旨在通过跨模态对齐提升自然语言描述的遥感区域精确划分。", "method": "DiffRIS框架，包含上下文感知适配器（CP-adapter）和渐进跨模态推理解码器（PCMRD），利用预训练的文本到图像扩散模型进行语义理解，提高跨模态对齐。CP-adapter通过全局上下文建模和对象感知推理动态调整语言特征，PCMRD通过多尺度特征交互实现细粒度的语义对齐。", "result": "在RRSIS-D、RefSegRS和RISBench三个基准数据集上，DiffRIS在所有标准指标上均优于现有方法，建立了远程感知图像分割任务的新标准。", "conclusion": "实验表明，通过适应性框架利用预训练扩散模型显著提升了遥感任务的表现。DiffRIS框架在远程感知图像分割中表现出色，是一项重要进步。"}}
{"id": "2506.19209", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.19209", "abs": "https://arxiv.org/abs/2506.19209", "authors": ["Yichen Tang", "Weihang Su", "Yujia Zhou", "Yiqun Liu", "Min Zhang", "Shaoping Ma", "Qingyao Ai"], "title": "Augmenting Multi-Agent Communication with State Delta Trajectory", "comment": "22 pages, 5 figures", "summary": "Multi-agent techniques such as role playing or multi-turn debates have been\nshown to be effective in improving the performance of large language models\n(LLMs) in downstream tasks. Despite their differences in workflows, existing\nLLM-based multi-agent systems mostly use natural language for agent\ncommunication. While this is appealing for its simplicity and interpretability,\nit also introduces inevitable information loss as one model must down sample\nits continuous state vectors to concrete tokens before transferring them to the\nother model. Such losses are particularly significant when the information to\ntransfer is not simple facts, but reasoning logics or abstractive thoughts. To\ntackle this problem, we propose a new communication protocol that transfers\nboth natural language tokens and token-wise state transition trajectory from\none agent to another. Particularly, compared to the actual state value, we find\nthat the sequence of state changes in LLMs after generating each token can\nbetter reflect the information hidden behind the inference process, so we\npropose a State Delta Encoding (SDE) method to represent state transition\ntrajectories. The experimental results show that multi-agent systems with SDE\nachieve SOTA performance compared to other communication protocols,\nparticularly in tasks that involve complex reasoning. This shows the potential\nof communication augmentation for LLM-based multi-agent systems.", "AI": {"tldr": "研究提出了一种增强大型语言模型多智能体系统性能的新通信协议，通过传输自然语言标记加上状态转换轨迹来减少信息损失，并在实验中达到了最先进性能。", "motivation": "为解决现有LLM-based多智能体系统通过自然语言进行通信时不可避免的信息损失问题，特别是在需要传递复杂的推理逻辑或抽象思维时损失尤其显著。", "method": "我们提出了一种新的通信协议，该协议传输自然语言标记和标记级别的状态转换轨迹，以改进基于大型语言模型（LLM）的多智能体系统之间的通信。我们发现了LLMs在生成每个标记后的状态变化序列可以更好地反映推理过程中的隐藏信息，因此提出了状态增量编码（SDE）方法来表示状态转换轨迹。", "result": "实验结果显示，采用SDE的多智能体系统在与其他通信协议比较时，特别是涉及复杂的推理任务时，可以达到最先进（SOTA）的性能。", "conclusion": "这表明基于LLM的多智能体系统可以通过通信增强来提高性能，特别是在需要处理复杂推理时。"}}
{"id": "2506.18985", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.18985", "abs": "https://arxiv.org/abs/2506.18985", "authors": ["Guanxi Shen"], "title": "GLIMPSE: Gradient-Layer Importance Mapping for Prompted Visual Saliency Explanation for Generative LVLMs", "comment": null, "summary": "Recent advances in large vision language models (LVLMs) have unlocked\nunprecedented capabilities in generating coherent responses from visual inputs.\nHowever, interpreting where LVLMs direct their visual attention while\ngenerating free-form textual responses remains a significant challenge, yet is\nessential for understanding model behavior, diagnosing hallucination, exposing\nbias and ensuring transparency. We introduce GLIMPSE (Gradient-Layer Importance\nMapping for Prompted Visual Saliency Explanation), a lightweight,\nmodel-agnostic framework for visualizing the salient image regions that LVLMs\nrely upon during open-ended visual question answering (VQA), while concurrently\nrevealing the multimodal textual saliency. GLIMPSE fuses gradient-weighted\nattention, adaptive layer propagation, and weighted token aggregation to\nproduce holistic response-level attribution heat maps for interpreting\ncross-modal reasoning, outperforming prior interpretability methods in\nhuman-alignment. We demonstrate an analytic explainable AI (XAI) approach using\nGLIMPSE to uncover fine-grained insights into LVLM cross-modal attribution,\ntrace token-level reasoning dynamics, and analyze systematic human-attention\nmisalignment, hallucination, and bias.", "AI": {"tldr": "我们介绍了GLIMPSE，这是一款用来解析LVLMs在生成文本应答时如何分配视觉注意力的框架，它优于之前的解释方法，可以生成跨模态推理的归属热图，提供更细粒度的洞察力，追踪令牌级别的推理动态。", "motivation": "尽管大视觉语言模型（LVLMs）解锁了从视觉输入生成连贯反应的前所未有的能力，但解释这些模型在生成自由形式的文本应答时如何分配视觉注意力仍是重要挑战。理解模型行为、诊断幻觉、暴露偏见和确保透明度都需要解决这一问题。", "method": "我们提出了GLIMPSE，这是一个轻量级、模型不可知的框架，用于在开放式的视觉问答任务中可视化LVLMs依赖的显著图像区域，同时揭示多模态文本的显着性。GLIMPSE融合了基于梯度加权的注意力、自适应层传播和加权令牌聚合，生成跨模态推理的全面响应级归属热图。", "result": "与以往的可解释性方法相比，GLIMPSE能够更好地生成跨模态推理的归属热图，与人类的理解更加一致。它提供了一个可以追踪推理动态，以及分析系统性人类注意力不一致，幻觉及偏见的分析性可解释AI方法。", "conclusion": "我们通过GLIMPSE展示了分析可解释AI（XAI）的方法，用于揭示大视觉语言模型（LVLMs）跨模态归属的细粒度洞察，追踪令牌级别的推理动态，以及分析系统性的人类注意力不一致、幻觉和偏见。"}}
{"id": "2506.19258", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.19258", "abs": "https://arxiv.org/abs/2506.19258", "authors": ["Rasiq Hussain", "Jerry Ma", "Rithik Khandelwal", "Joshua Oltmanns", "Mehak Gupta"], "title": "Personality Prediction from Life Stories using Language Models", "comment": "13 pages, 5 figures", "summary": "Natural Language Processing (NLP) offers new avenues for personality\nassessment by leveraging rich, open-ended text, moving beyond traditional\nquestionnaires. In this study, we address the challenge of modeling long\nnarrative interview where each exceeds 2000 tokens so as to predict Five-Factor\nModel (FFM) personality traits. We propose a two-step approach: first, we\nextract contextual embeddings using sliding-window fine-tuning of pretrained\nlanguage models; then, we apply Recurrent Neural Networks (RNNs) with attention\nmechanisms to integrate long-range dependencies and enhance interpretability.\nThis hybrid method effectively bridges the strengths of pretrained transformers\nand sequence modeling to handle long-context data. Through ablation studies and\ncomparisons with state-of-the-art long-context models such as LLaMA and\nLongformer, we demonstrate improvements in prediction accuracy, efficiency, and\ninterpretability. Our results highlight the potential of combining\nlanguage-based features with long-context modeling to advance personality\nassessment from life narratives.", "AI": {"tldr": "研究提出了一种结合滑动窗口微调预训练语言模型提取上下文嵌入和使用具有注意力机制的循环神经网络整合长途依赖性的两步方法，用于预测来自长篇叙述性访谈文本的五大人格特征，展示了预测准确率、效率和可解释性的提升。", "motivation": "动机在于利用自然语言处理（NLP）提供的新途径，通过对长篇叙述性访谈文本的分析来评估人格特质，超越了传统的问卷形式，主要解决长文本处理中的挑战。", "method": "采用两步方法：首先使用滑动窗口微调预训练语言模型来提取上下文嵌入；然后应用具有注意力机制的循环神经网络（RNNs）来整合长途依赖性并提高解释性。", "result": "通过对模型进行消融研究并与当前最先进的长文本处理模型LLaMA和Longformer进行比较，展示了在预测准确率、效率和解释性上的改进。", "conclusion": "研究结果强调了将基于语言的特征与长文本建模相结合以从生命叙述中推进人格评估的潜力。"}}
{"id": "2506.18999", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.18999", "abs": "https://arxiv.org/abs/2506.18999", "authors": ["Yuan Yao", "Yicong Hong", "Difan Liu", "Long Mai", "Feng Liu", "Jiebo Luo"], "title": "Diffusion Transformer-to-Mamba Distillation for High-Resolution Image Generation", "comment": null, "summary": "The quadratic computational complexity of self-attention in diffusion\ntransformers (DiT) introduces substantial computational costs in\nhigh-resolution image generation. While the linear-complexity Mamba model\nemerges as a potential alternative, direct Mamba training remains empirically\nchallenging. To address this issue, this paper introduces diffusion\ntransformer-to-mamba distillation (T2MD), forming an efficient training\npipeline that facilitates the transition from the self-attention-based\ntransformer to the linear complexity state-space model Mamba. We establish a\ndiffusion self-attention and Mamba hybrid model that simultaneously achieves\nefficiency and global dependencies. With the proposed layer-level teacher\nforcing and feature-based knowledge distillation, T2MD alleviates the training\ndifficulty and high cost of a state space model from scratch. Starting from the\ndistilled 512$\\times$512 resolution base model, we push the generation towards\n2048$\\times$2048 images via lightweight adaptation and high-resolution\nfine-tuning. Experiments demonstrate that our training path leads to low\noverhead but high-quality text-to-image generation. Importantly, our results\nalso justify the feasibility of using sequential and causal Mamba models for\ngenerating non-causal visual output, suggesting the potential for future\nexploration.", "AI": {"tldr": "该论文提出了扩散Transformer到Mamba知识蒸馏方法，建立了一个高效训练管线来解决直接训练Mamba模型的挑战性问题，展示了其在生成高分辨率图像时的有效性和未来探索潜力。", "motivation": "解决扩散变换器中自注意力机制带来的二次计算复杂度问题，减少高分辨率图像生成中的计算成本。直接训练Mamba模型仍然具有挑战性，提出一种有效的方法从基于自注意力的变压器过渡到线性复杂度的状态空间模型Mamba。", "method": "通过提出扩散自注意力和Mamba混合模型来实现在保持全局依赖性的同时提高效率。通过逐层教师强制和基于特征的知识提炼，T2MD解决了从零开始训练状态空间模型的困难和高昂成本问题。", "result": "实验表明，所提出的训练路径实现了低开销的高质量文本到图像生成，并且结果证明了使用顺序和因果Mamba模型生成非因果视觉输出的可行性。", "conclusion": "文本展示了一条从原始512x512分辨率的基础模型开始，通过轻量级适应和高分辨率微调推进生成至2048x2048图像的有效训练路径，证明了这种训练路径的有效性和未来探索的潜力。"}}
{"id": "2506.19262", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.19262", "abs": "https://arxiv.org/abs/2506.19262", "authors": ["Yuchang Zhu", "Zhonghua zhen", "Qunshu Lin", "Haotong Wei", "Xiaolong Sun", "Zixuan Yu", "Minghao Liu", "Zibin Zheng", "Liang Chen"], "title": "What Matters in LLM-generated Data: Diversity and Its Effect on Model Fine-Tuning", "comment": "Ongoing work", "summary": "With the remarkable generative capabilities of large language models (LLMs),\nusing LLM-generated data to train downstream models has emerged as a promising\napproach to mitigate data scarcity in specific domains and reduce\ntime-consuming annotations. However, recent studies have highlighted a critical\nissue: iterative training on self-generated data results in model collapse,\nwhere model performance degrades over time. Despite extensive research on the\nimplications of LLM-generated data, these works often neglect the importance of\ndata diversity, a key factor in data quality. In this work, we aim to\nunderstand the implications of the diversity of LLM-generated data on\ndownstream model performance. Specifically, we explore how varying levels of\ndiversity in LLM-generated data affect downstream model performance.\nAdditionally, we investigate the performance of models trained on data that\nmixes different proportions of LLM-generated data, which we refer to as\nsynthetic data. Our experimental results show that, with minimal distribution\nshift, moderately diverse LLM-generated data can enhance model performance in\nscenarios with insufficient labeled data, whereas highly diverse generated data\nhas a negative impact. We hope our empirical findings will offer valuable\nguidance for future studies on LLMs as data generators.", "AI": {"tldr": "本文研究发现，适度多样性的LLM生成数据在标注数据不足的情境中可以改善模型性能，而高度多样性的生成数据则有负面影响。", "motivation": "尽管有关LLM生成数据的研究广泛存在，但这些研究往往忽视了数据多样性这一关键因素的重要性。本文旨在探讨LLM生成数据的多样性对下游模型性能的影响。", "method": "通过探索LLM生成数据的不同多样性水平对下游模型性能的影响，以及研究在合成数据中混合不同比例的LLM生成数据的模型性能。", "result": "实验显示，具有适度多样性的由LLM生成的数据可以提高在标注数据不足的情况下训练的模型的性能，而高度多样性的生成数据则会对模型性能产生负面影响。", "conclusion": "实验结果表明，在分布偏移最小的情况下，适度多样性的LLM生成数据可以提升模型性能，尤其是在标注数据不足的情境中，而高度多样性的生成数据有负面作用。"}}
{"id": "2506.19022", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.19022", "abs": "https://arxiv.org/abs/2506.19022", "authors": ["Jinlong Li", "Dong Zhao", "Qi Zang", "Zequn Jie", "Lin Ma", "Nicu Sebe"], "title": "Orthogonal Projection Subspace to Aggregate Online Prior-knowledge for Continual Test-time Adaptation", "comment": null, "summary": "Continual Test Time Adaptation (CTTA) is a task that requires a source\npre-trained model to continually adapt to new scenarios with changing target\ndistributions. Existing CTTA methods primarily focus on mitigating the\nchallenges of catastrophic forgetting and error accumulation. Though there have\nbeen emerging methods based on forgetting adaptation with parameter-efficient\nfine-tuning, they still struggle to balance competitive performance and\nefficient model adaptation, particularly in complex tasks like semantic\nsegmentation. In this paper, to tackle the above issues, we propose a novel\npipeline, Orthogonal Projection Subspace to aggregate online Prior-knowledge,\ndubbed OoPk. Specifically, we first project a tuning subspace orthogonally\nwhich allows the model to adapt to new domains while preserving the knowledge\nintegrity of the pre-trained source model to alleviate catastrophic forgetting.\nThen, we elaborate an online prior-knowledge aggregation strategy that employs\nan aggressive yet efficient image masking strategy to mimic potential target\ndynamism, enhancing the student model's domain adaptability. This further\ngradually ameliorates the teacher model's knowledge, ensuring high-quality\npseudo labels and reducing error accumulation. We demonstrate our method with\nextensive experiments that surpass previous CTTA methods and achieve\ncompetitive performances across various continual TTA benchmarks in semantic\nsegmentation tasks.", "AI": {"tldr": "本文提出了一种新的持续测试时间适应性方法OoPk，通过正交投影子空间缓解灾难性遗忘，以及使用有效的图像掩码策略来增强领域适应性，并在语义分割任务实验中取得了优异的表现。", "motivation": "现有持续测试时间适应性方法主要集中在缓解灾难性遗忘和误差积累问题，但它们在平衡性能和模型适应效率方面遇到挑战，特别是在复杂任务如语义分割中表现突出。因此，本文提出了一种新的方法来解决这些问题。", "method": "OoPk方法首先通过正交投影子空间来适应新领域，同时保留预训练模型的知识完整性，以缓解灾难性遗忘问题。然后通过一种激进但高效的图像掩码策略来模仿潜在目标动态变化，增强学生模型的领域适应性，并逐步改进教师模型的知识，从而减少误差累积，确保高质量的伪标签。", "result": "实验结果表明，OoPk方法在语义分割任务的持续测试时间适应性基准测试中，超越了之前的持续测试时间适应性方法，实现了有竞争力的表现。", "conclusion": "总体来说，OoPk方法通过正交投影子空间和高效的在线先验知识聚集策略，能够有效缓解灾难性遗忘和误差累积问题，并在语义分割持续测试时间适应性任务中展示出了优越的性能。"}}
{"id": "2506.19279", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.19279", "abs": "https://arxiv.org/abs/2506.19279", "authors": ["Zhiyang Qi", "Keiko Takamizo", "Mariko Ukiyo", "Michimasa Inaba"], "title": "EmoStage: A Framework for Accurate Empathetic Response Generation via Perspective-Taking and Phase Recognition", "comment": null, "summary": "The rising demand for mental health care has fueled interest in AI-driven\ncounseling systems. While large language models (LLMs) offer significant\npotential, current approaches face challenges, including limited understanding\nof clients' psychological states and counseling stages, reliance on\nhigh-quality training data, and privacy concerns associated with commercial\ndeployment. To address these issues, we propose EmoStage, a framework that\nenhances empathetic response generation by leveraging the inference\ncapabilities of open-source LLMs without additional training data. Our\nframework introduces perspective-taking to infer clients' psychological states\nand support needs, enabling the generation of emotionally resonant responses.\nIn addition, phase recognition is incorporated to ensure alignment with the\ncounseling process and to prevent contextually inappropriate or inopportune\nresponses. Experiments conducted in both Japanese and Chinese counseling\nsettings demonstrate that EmoStage improves the quality of responses generated\nby base models and performs competitively with data-driven methods.", "AI": {"tldr": "EmoStage is a framework designed to improve the empathetic response generation of open-source language models in AI-driven counseling systems by inferring clients' psychological states and aligning with appropriate counseling stages.", "motivation": "The increasing demand for mental health care and the limitations of current AI-driven counseling systems, such as the lack of understanding of clients' psychological states, the reliance on high-quality training data, and privacy issues when deploying commercially, motivated the development of EmoStage.", "method": "EmoStage employs perspective-taking to infer psychological states, supports clients' needs, and integrates phase recognition to ensure responses align with the counseling process and are contextually appropriate.", "result": "Experiments in Japanese and Chinese counseling contexts showed EmoStage enhancing the quality of responses by base models and performing competitively with data-driven methods.", "conclusion": "The framework effectively enhances the capability of AI-driven counseling by improving the relevance and emotional resonance of responses without the need for additional training data, thus addressing key challenges in the field."}}
{"id": "2506.19065", "categories": ["cs.CV", "cs.DL"], "pdf": "https://arxiv.org/pdf/2506.19065", "abs": "https://arxiv.org/abs/2506.19065", "authors": ["Guang Yang", "Victoria Ebert", "Nazif Tamer", "Luiza Pozzobon", "Noah A. Smith"], "title": "LEGATO: Large-scale End-to-end Generalizable Approach to Typeset OMR", "comment": null, "summary": "We propose Legato, a new end-to-end transformer model for optical music\nrecognition (OMR). Legato is the first large-scale pretrained OMR model capable\nof recognizing full-page or multi-page typeset music scores and the first to\ngenerate documents in ABC notation, a concise, human-readable format for\nsymbolic music. Bringing together a pretrained vision encoder with an ABC\ndecoder trained on a dataset of more than 214K images, our model exhibits the\nstrong ability to generalize across various typeset scores. We conduct\nexperiments on a range of datasets and demonstrate that our model achieves\nstate-of-the-art performance. Given the lack of a standardized evaluation for\nend-to-end OMR, we comprehensively compare our model against the previous state\nof the art using a diverse set of metrics.", "AI": {"tldr": "Legato is a new transformer model that excels in recognizing and outputting multi-page music scores in ABC notation, setting a new benchmark in Optical Music Recognition.", "motivation": "To develop a model capable of recognizing comprehensive music scores and generating output in a widely accepted notation format.", "method": "Legato combines a pretrained vision encoder with a decoder trained to produce ABC notation.", "result": "<tool_call>\r\n{{\"name\": \"Structure\", \"arguments\": {\"tldr\": \"Legato is a new transformer model designed for optical music recognition, which can recognize full-page or multi-page typeset music scores and generate documents in ABC notation. It outperforms existing models across various datasets.\", \"motivation\": \"To create a new OMR model that can process complex, multi-page sheet music and output in a human-readable, concise format not previously addressed by other models.\", \"method\": \"Developed a pretrained vision encoder and an ABC decoder trained on a large dataset of more than 214K images.\", \"result\": \"The model exhibited strong generalization performance across different types of music scores and achieved state-of-the-art performance.\", \"conclusion\": \"Legato fills a gap in the current OMR landscape by being the first model to work with full-page/multi-page music scores and produce ABC notation output, offering a significant advancement in the field of OMR.\"}}}\r\n</tool_call>", "conclusion": "This analysis provides a summary of the paper's key points, presenting the motivation, methodology, results, and conclusions drawn from Legato, a new model in the field of Optical Music Recognition."}}
{"id": "2506.19315", "categories": ["cs.CL", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.19315", "abs": "https://arxiv.org/abs/2506.19315", "authors": ["Tzu-Hsuan Yang", "Yue-Yang He", "Berlin Chen"], "title": "JCAPT: A Joint Modeling Approach for CAPT", "comment": "Submitted to the ISCA SLaTE-2025 Workshop", "summary": "Effective pronunciation feedback is critical in second language (L2)\nlearning, for which computer-assisted pronunciation training (CAPT) systems\noften encompass two key tasks: automatic pronunciation assessment (APA) and\nmispronunciation detection and diagnosis (MDD). Recent work has shown that\njoint modeling of these two tasks can yield mutual benefits. Our unified\nframework leverages Mamba, a selective state space model (SSM), while\nintegrating phonological features and think token strategies to jointly enhance\ninterpretability and fine-grained temporal reasoning in APA and MDD. To our\nknowledge, this is the first study to combine phonological attribution,\nSSM-based modeling, and prompting in CAPT. A series of experiments conducted on\nthe speechocean762 benchmark demonstrate that our model consistently\noutperforms prior methods, particularly on the MDD task.", "AI": {"tldr": "研究展示了如何使用Mamba选择性状态空间模型，结合音系特征和think token策略，提升计算机辅助发音训练系统中的自动发音评估和发音错误检测与诊断功能。", "motivation": "研究动机在于探索如何利用先进的模型和策略来增强计算机辅助发音训练系统的自动发音评估和发音错误检测与诊断功能。", "method": "该研究采用了Mamba选择性状态空间模型，并整合了音系特征和think token策略，旨在提高APA和MDD的性能。", "result": "该研究提出了一个统一框架，使用Mamba选择性状态空间模型，结合音系特征和think token策略来共同提升自动发音评估（APA）和发音错误检测与诊断（MDD）的可解释性和细粒度时间推理。这是首个将音系归因、基于SSM的建模和提示机制结合在计算机辅助发音训练（CAPT）中的研究。在speechocean762基准测试中的多个实验表明，该模型在APA和MDD任务上均表现出色，尤其在MDD任务上有显著提升。", "conclusion": "该研究证明了所提出的框架在自动发音评估和发音错误检测与诊断任务中的有效性，特别是在MDD任务上优于之前的模型。"}}
{"id": "2506.19072", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.19072", "abs": "https://arxiv.org/abs/2506.19072", "authors": ["Yimu Wang", "Mozhgan Nasr Azadani", "Sean Sedwards", "Krzysztof Czarnecki"], "title": "HAWAII: Hierarchical Visual Knowledge Transfer for Efficient Vision-Language Models", "comment": "Work in progress", "summary": "Improving the visual understanding ability of vision-language models (VLMs)\nis crucial for enhancing their performance across various tasks. While using\nmultiple pretrained visual experts has shown great promise, it often incurs\nsignificant computational costs during training and inference. To address this\nchallenge, we propose HAWAII, a novel framework that distills knowledge from\nmultiple visual experts into a single vision encoder, enabling it to inherit\nthe complementary strengths of several experts with minimal computational\noverhead. To mitigate conflicts among different teachers and switch between\ndifferent teacher-specific knowledge, instead of using a fixed set of adapters\nfor multiple teachers, we propose to use teacher-specific Low-Rank Adaptation\n(LoRA) adapters with a corresponding router. Each adapter is aligned with a\nspecific teacher, avoiding noisy guidance during distillation. To enable\nefficient knowledge distillation, we propose fine-grained and coarse-grained\ndistillation. At the fine-grained level, token importance scores are employed\nto emphasize the most informative tokens from each teacher adaptively. At the\ncoarse-grained level, we summarize the knowledge from multiple teachers and\ntransfer it to the student using a set of general-knowledge LoRA adapters with\na router. Extensive experiments on various vision-language tasks demonstrate\nthe superiority of HAWAII, compared to the popular open-source VLMs.", "AI": {"tldr": "HAWAII is a new framework that aggregates knowledge from multiple visual experts into a single vision encoder, using teacher-specific LoRA adapters and a router for efficient and selective knowledge distillation.", "motivation": "To enhance VLMs' visual understanding capability efficiently without increasing computational costs.", "method": "HAWAII uses fine-grained and coarse-grained distillation methods along with teacher-specific Low-Rank Adaptation (LoRA) adapters and a router to avoid conflicts and to transfer knowledge selectively.", "result": "Experiments show HAWAII performs better than popular VLMs across various tasks.", "conclusion": "The framework demonstrates a promising method to improve VLMs efficiently with less computational cost."}}
{"id": "2506.19352", "categories": ["cs.CL", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2506.19352", "abs": "https://arxiv.org/abs/2506.19352", "authors": ["Jisu Shin", "Juhyun Oh", "Eunsu Kim", "Hoyun Song", "Alice Oh"], "title": "Spotting Out-of-Character Behavior: Atomic-Level Evaluation of Persona Fidelity in Open-Ended Generation", "comment": "Findings of ACL 2025; github repo:\n  https://github.com/ddindidu/atomic-persona-evaluation/", "summary": "Ensuring persona fidelity in large language models (LLMs) is essential for\nmaintaining coherent and engaging human-AI interactions. However, LLMs often\nexhibit Out-of-Character (OOC) behavior, where generated responses deviate from\nan assigned persona, leading to inconsistencies that affect model reliability.\nExisting evaluation methods typically assign single scores to entire responses,\nstruggling to capture subtle persona misalignment, particularly in long-form\ntext generation. To address this limitation, we propose an atomic-level\nevaluation framework that quantifies persona fidelity at a finer granularity.\nOur three key metrics measure the degree of persona alignment and consistency\nwithin and across generations. Our approach enables a more precise and\nrealistic assessment of persona fidelity by identifying subtle deviations that\nreal users would encounter. Through our experiments, we demonstrate that our\nframework effectively detects persona inconsistencies that prior methods\noverlook. By analyzing persona fidelity across diverse tasks and personality\ntypes, we reveal how task structure and persona desirability influence model\nadaptability, highlighting challenges in maintaining consistent persona\nexpression.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2506.19079", "categories": ["cs.CV", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2506.19079", "abs": "https://arxiv.org/abs/2506.19079", "authors": ["Iosif Tsangko", "Andreas Triantafyllopoulos", "Adem Abdelmoula", "Adria Mallol-Ragolta", "Bjoern W. Schuller"], "title": "Reading Smiles: Proxy Bias in Foundation Models for Facial Emotion Recognition", "comment": null, "summary": "Foundation Models (FMs) are rapidly transforming Affective Computing (AC),\nwith Vision Language Models (VLMs) now capable of recognising emotions in zero\nshot settings. This paper probes a critical but underexplored question: what\nvisual cues do these models rely on to infer affect, and are these cues\npsychologically grounded or superficially learnt? We benchmark varying scale\nVLMs on a teeth annotated subset of AffectNet dataset and find consistent\nperformance shifts depending on the presence of visible teeth. Through\nstructured introspection of, the best-performing model, i.e., GPT-4o, we show\nthat facial attributes like eyebrow position drive much of its affective\nreasoning, revealing a high degree of internal consistency in its\nvalence-arousal predictions. These patterns highlight the emergent nature of\nFMs behaviour, but also reveal risks: shortcut learning, bias, and fairness\nissues especially in sensitive domains like mental health and education.", "AI": {"tldr": "本文探究了视觉语言模型在情绪识别中依赖的视觉线索，发现模型依赖的面部特征如眉毛等展示了内部推理的一致性，但也提出了模型可能导致的偏差和公平性问题。", "motivation": "研究动机在于探索视觉语言模型如何在零样本设置下识别情绪，特别是它们依赖的心理学基础的视觉线索还是表面学习的线索，以及这些线索可能引发的风险，比如偏差和公平性问题。", "method": "本文通过在AffectNet数据集的牙齿标注子集上对不同规模的视觉语言模型（VLMs）进行基准测试，并通过结构化的 introspection 方法分析了表现最好的模型GPT-4o的内部推理机制，探讨了这些模型在情绪识别中依赖的视觉线索及其心理基础或表面学习特性。", "result": "研究发现模型的性能在不同情况下有显著变化，尤其与人物是否露出牙齿有关。通过深入分析，研究者指出模型的情绪推理主要依赖如眉毛位置这样的面部特征，揭示了模型内部逻辑的内在一致性。", "conclusion": "研究结果表明了基础模型行为的涌现性质，但也揭示了在心理健康和教育等敏感领域使用这些模型时可能面临的挑战和风险，包括捷径学习、偏差以及公平性问题。"}}
{"id": "2506.19382", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.19382", "abs": "https://arxiv.org/abs/2506.19382", "authors": ["Ruben Härle", "Felix Friedrich", "Manuel Brack", "Stephan Wäldchen", "Björn Deiseroth", "Patrick Schramowski", "Kristian Kersting"], "title": "Measuring and Guiding Monosemanticity", "comment": null, "summary": "There is growing interest in leveraging mechanistic interpretability and\ncontrollability to better understand and influence the internal dynamics of\nlarge language models (LLMs). However, current methods face fundamental\nchallenges in reliably localizing and manipulating feature representations.\nSparse Autoencoders (SAEs) have recently emerged as a promising direction for\nfeature extraction at scale, yet they, too, are limited by incomplete feature\nisolation and unreliable monosemanticity. To systematically quantify these\nlimitations, we introduce Feature Monosemanticity Score (FMS), a novel metric\nto quantify feature monosemanticity in latent representation. Building on these\ninsights, we propose Guided Sparse Autoencoders (G-SAE), a method that\nconditions latent representations on labeled concepts during training. We\ndemonstrate that reliable localization and disentanglement of target concepts\nwithin the latent space improve interpretability, detection of behavior, and\ncontrol. Specifically, our evaluations on toxicity detection, writing style\nidentification, and privacy attribute recognition show that G-SAE not only\nenhances monosemanticity but also enables more effective and fine-grained\nsteering with less quality degradation. Our findings provide actionable\nguidelines for measuring and advancing mechanistic interpretability and control\nof LLMs.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2506.19087", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.19087", "abs": "https://arxiv.org/abs/2506.19087", "authors": ["Bowen Zhang", "Jesse T. Boulerice", "Nikhil Kuniyil", "Charvi Mendiratta", "Satish Kumar", "Hila Shamon", "B. S. Manjunath"], "title": "RareSpot: Spotting Small and Rare Wildlife in Aerial Imagery with Multi-Scale Consistency and Context-Aware Augmentation", "comment": "Accepted to the CVPR 2025 Workshop on Computer Vision for Animal\n  Behavior Tracking and Modeling (CV4Animals)", "summary": "Automated detection of small and rare wildlife in aerial imagery is crucial\nfor effective conservation, yet remains a significant technical challenge.\nPrairie dogs exemplify this issue: their ecological importance as keystone\nspecies contrasts sharply with their elusive presence--marked by small size,\nsparse distribution, and subtle visual features--which undermines existing\ndetection approaches. To address these challenges, we propose RareSpot, a\nrobust detection framework integrating multi-scale consistency learning and\ncontext-aware augmentation. Our multi-scale consistency approach leverages\nstructured alignment across feature pyramids, enhancing fine-grained object\nrepresentation and mitigating scale-related feature loss. Complementarily,\ncontext-aware augmentation strategically synthesizes challenging training\ninstances by embedding difficult-to-detect samples into realistic environmental\ncontexts, significantly boosting model precision and recall. Evaluated on an\nexpert-annotated prairie dog drone imagery benchmark, our method achieves\nstate-of-the-art performance, improving detection accuracy by over 35% compared\nto baseline methods. Importantly, it generalizes effectively across additional\nwildlife datasets, demonstrating broad applicability. The RareSpot benchmark\nand approach not only support critical ecological monitoring but also establish\na new foundation for detecting small, rare species in complex aerial scenes.", "AI": {"tldr": "This paper proposes a new framework called RareSpot, designed to improve the detection of small, rare wildlife in aerial imagery, enhancing accuracy significantly over baseline methods and providing a broad application across multiple wildlife datasets.", "motivation": "The motivation is to effectively detect small and rare wildlife in aerial imagery, which is important for conservation purposes but presents significant technical challenges.", "method": "RareSpot, a robust detection framework integrating multi-scale consistency learning and context-aware augmentation, is proposed. This approach leverages structured alignment across feature pyramids and strategically synthesizes challenging training instances to enhance fine-grained object representation and boost detection performance.", "result": "Evaluated on an expert-annotated prairie dog drone imagery benchmark, the method achieves state-of-the-art performance, improving detection accuracy by over 35% compared to baseline methods.", "conclusion": "The study not only contributes to ecological monitoring but also sets a new standard for detecting small, rare species in complex aerial scenes."}}
{"id": "2506.19399", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.19399", "abs": "https://arxiv.org/abs/2506.19399", "authors": ["Ruihan Hu", "Yu-Ming Shang", "Jiankun Peng", "Wei Luo", "Yazhe Wang", "Xi Zhang"], "title": "Automated Detection of Pre-training Text in Black-box LLMs", "comment": "13 pages", "summary": "Detecting whether a given text is a member of the pre-training data of Large\nLanguage Models (LLMs) is crucial for ensuring data privacy and copyright\nprotection. Most existing methods rely on the LLM's hidden information (e.g.,\nmodel parameters or token probabilities), making them ineffective in the\nblack-box setting, where only input and output texts are accessible. Although\nsome methods have been proposed for the black-box setting, they rely on massive\nmanual efforts such as designing complicated questions or instructions. To\naddress these issues, we propose VeilProbe, the first framework for\nautomatically detecting LLMs' pre-training texts in a black-box setting without\nhuman intervention. VeilProbe utilizes a sequence-to-sequence mapping model to\ninfer the latent mapping feature between the input text and the corresponding\noutput suffix generated by the LLM. Then it performs the key token\nperturbations to obtain more distinguishable membership features. Additionally,\nconsidering real-world scenarios where the ground-truth training text samples\nare limited, a prototype-based membership classifier is introduced to alleviate\nthe overfitting issue. Extensive evaluations on three widely used datasets\ndemonstrate that our framework is effective and superior in the black-box\nsetting.", "AI": {"tldr": "VeilProbe是一个自动检测大型语言模型预训练文本的框架，适用于黑箱设置，能够有效解决数据隐私和版权保护问题。", "motivation": "现有的许多方法依赖于语言模型的隐藏信息，因此在黑箱设置下无效。VeilProbe旨在自动检测语言模型的预训练文本，无需人工干预，特别是在黑箱设置下，这对于确保数据隐私和版权保护至关重要。", "method": "VeilProbe利用序列到序列映射模型来推断输入文本和大型语言模型生成的对应输出后缀之间的潜在映射特征。通过执行关键标记扰动以获得更易区分的成员特征。为了缓解过拟合问题，还引入了一个原型基础成员分类器。", "result": "在三个广泛使用的数据集上的广泛评估证明了该框架的有效性和优越性。", "conclusion": "实验表明，VeilProbe框架在黑箱设置下是有效且优于其他方法的。"}}
{"id": "2506.19103", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.19103", "abs": "https://arxiv.org/abs/2506.19103", "authors": ["Ilia Beletskii", "Andrey Kuznetsov", "Aibek Alanov"], "title": "Inverse-and-Edit: Effective and Fast Image Editing by Cycle Consistency Models", "comment": "The code of our method is available on GitHub at\n  https://github.com/ControlGenAI/Inverse-and-Edit", "summary": "Recent advances in image editing with diffusion models have achieved\nimpressive results, offering fine-grained control over the generation process.\nHowever, these methods are computationally intensive because of their iterative\nnature. While distilled diffusion models enable faster inference, their editing\ncapabilities remain limited, primarily because of poor inversion quality.\nHigh-fidelity inversion and reconstruction are essential for precise image\nediting, as they preserve the structural and semantic integrity of the source\nimage. In this work, we propose a novel framework that enhances image inversion\nusing consistency models, enabling high-quality editing in just four steps. Our\nmethod introduces a cycle-consistency optimization strategy that significantly\nimproves reconstruction accuracy and enables a controllable trade-off between\neditability and content preservation. We achieve state-of-the-art performance\nacross various image editing tasks and datasets, demonstrating that our method\nmatches or surpasses full-step diffusion models while being substantially more\nefficient. The code of our method is available on GitHub at\nhttps://github.com/ControlGenAI/Inverse-and-Edit.", "AI": {"tldr": "", "motivation": "", "method": "", "result": "{\n  \"tldr\": \"该框架通过使用一致性模型，能在四个步骤内完成高质量的图像编辑，同时最大限度地保留原始图像的结构和语义信息，提供编辑能力和内容保留性之间的可控权衡。\", \n  \"motivation\": \"鉴于目前的图像编辑模型计算成本高且重建质量不佳，作者旨在解决这些问题，通过引入一致性模型优化，提高图像反转和编辑的质量，并使编辑过程更加高效。\", \n  \"method\": \"通过采用一种循环一致性优化策略，提高图像的重建准确性，同时提供性能优化，达到了可编辑性和内容保持性之间的平衡。\", \n  \"result\": \"此框架在不同的图像编辑任务和数据集上达到了最先进的性能表现，并且其效率远超全步扩散模型。\", \n  \"conclusion\": \"研究表明，他们提出的方法不但能够达到与全步扩散模型相匹配或更优的性能，还具有显著的效率优势。该框架为高精度图像编辑提供了一种新的、可行的方法。\"}\n}", "conclusion": ""}}
{"id": "2506.19418", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.19418", "abs": "https://arxiv.org/abs/2506.19418", "authors": ["Yingji Zhang", "Marco Valentino", "Danilo S. Carvalho", "André Freitas"], "title": "Learning to Disentangle Latent Reasoning Rules with Language VAEs: A Systematic Study", "comment": null, "summary": "Incorporating explicit reasoning rules within the latent space of language\nmodels (LMs) offers a promising pathway to enhance generalisation,\ninterpretability, and controllability. While current Transformer-based language\nmodels have shown strong performance on Natural Language Inference (NLI) tasks,\nthey often rely on memorisation rather than rule-based inference. This work\ninvestigates how reasoning rules can be explicitly embedded and memorised\nwithin the LMs through Language Variational Autoencoders (VAEs). We propose a\ncomplete pipeline for learning reasoning rules within Transformer-based\nlanguage VAEs. This pipeline encompasses three rule-based reasoning tasks, a\nsupporting theoretical framework, and a practical end-to-end architecture. The\nexperiment illustrates the following findings: Disentangled reasoning: Under\nexplicit signal supervision, reasoning rules - viewed as functional mappings -\ncan be disentangled within the encoder's parametric space. This separation\nresults in distinct clustering of rules in the output feature space. Prior\nknowledge injection: injecting reasoning information into the Query enables the\nmodel to more effectively retrieve the stored value Value from memory based on\nKey. This approach offers a simple method for integrating prior knowledge into\ndecoder-only language models. Performance bottleneck: In mathematical reasoning\ntasks using Qwen2.5(0.5B), increasing sample count doesn't improve performance\nbeyond a point. Moreover, ffn layers are better than attention layers at\npreserving the separation of reasoning rules in the model's parameters.", "AI": {"tldr": "研究提出了一种在语言VAE中学习推理规则的方法，提升了模型的推理能力，并发现增加样本数量对数学推理任务的效果改善有限。", "motivation": "为了改善当前Transformer模型在自然语言推断任务上依赖记忆而非基于规则推理的问题，本研究探讨了如何通过显式信号监督在语言模型的潜在空间中拆分和记忆推理规则，提高了模型的通用性、可解释性和可控性。", "method": "本研究提出了一种完整的管道，用于在基于Transformer的语言变分自编码器（VAEs）中学习推理规则，涵盖了三种基于规则的推理任务、理论框架和实用的端到端架构。", "result": "实验表明，在显式信号监督下，推理规则可以在编码器的参数空间中被分离出来，形成输出特征空间中具体的规则聚类。将推理信息注入查询（Query）可以帮助模型更有效地根据密钥（Key）检索记忆中的值（Value）。发现对于基于Qwen2.5（0.5B）的数学推理任务，增加样本数量并不能无限提升性能，并且前馈神经网络层（FFN）比注意力层在保持模型参数中推理规则的分离方面表现更好。", "conclusion": "注入推理信息可以有效提升模型对知识的记忆与检索能力，而FFN层比注意力层更适合在语言模型参数中保持推理规则的分离。"}}
{"id": "2506.19117", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.19117", "abs": "https://arxiv.org/abs/2506.19117", "authors": ["Christina Ourania Tze", "Daniel Dauner", "Yiyi Liao", "Dzmitry Tsishkou", "Andreas Geiger"], "title": "PrITTI: Primitive-based Generation of Controllable and Editable 3D Semantic Scenes", "comment": "Project page: https://raniatze.github.io/pritti/", "summary": "Large-scale 3D semantic scene generation has predominantly relied on\nvoxel-based representations, which are memory-intensive, bound by fixed\nresolutions, and challenging to edit. In contrast, primitives represent\nsemantic entities using compact, coarse 3D structures that are easy to\nmanipulate and compose, making them an ideal representation for this task. In\nthis paper, we introduce PrITTI, a latent diffusion-based framework that\nleverages primitives as the main foundational elements for generating\ncompositional, controllable, and editable 3D semantic scene layouts. Our method\nadopts a hybrid representation, modeling ground surfaces in a rasterized format\nwhile encoding objects as vectorized 3D primitives. This decomposition is also\nreflected in a structured latent representation that enables flexible scene\nmanipulation of ground and object components. To overcome the orientation\nambiguities in conventional encoding methods, we introduce a stable\nCholesky-based parameterization that jointly encodes object size and\norientation. Experiments on the KITTI-360 dataset show that PrITTI outperforms\na voxel-based baseline in generation quality, while reducing memory\nrequirements by up to $3\\times$. In addition, PrITTI enables direct\ninstance-level manipulation of objects in the scene and supports a range of\ndownstream applications, including scene inpainting, outpainting, and\nphoto-realistic street-view synthesis.", "AI": {"tldr": "PrITTI, a latent diffusion-based framework, uses primitives as foundational components for generating 3D semantic scenes. It improves over voxel-based approaches by being more memory-efficient and easier to manipulate, offering a flexible solution for scene editing and various applications.", "motivation": "The motivation arises from the challenges of memory intensiveness, fixed resolution limitations, and difficulty in editing of voxel-based representations in 3D semantic scene generation. Primitives, offering a compact and easy to manipulate solution, are considered as a suitable alternative.", "method": "Our method adopts a hybrid representation, modeling ground surfaces in a rasterized format while encoding objects as vectorized 3D primitives. This decomposition is also reflected in a structured latent representation that enables flexible scene manipulation. A Cholesky-based parameterization that jointly encodes object size and orientation is introduced to overcome orientation ambiguities.", "result": "Experiments on the KITTI-360 dataset show that PrITTI outperforms a voxel-based baseline in generation quality, while reducing memory requirements by up to $3\times$. PrITTI also supports scene manipulation and various applications like scene inpainting and photo-realistic street-view synthesis.", "conclusion": "PrITTI demonstrates its effectiveness in generating high-quality 3D semantic scene layouts efficiently and with reduced memory usage. It provides a flexible and editable solution by leveraging a hybrid representation based on primitives, significantly outperforming voxel-based models."}}
{"id": "2506.19467", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.19467", "abs": "https://arxiv.org/abs/2506.19467", "authors": ["Jingwei Ni", "Yu Fan", "Vilém Zouhar", "Donya Rooein", "Alexander Hoyle", "Mrinmaya Sachan", "Markus Leippold", "Dirk Hovy", "Elliott Ash"], "title": "Can Large Language Models Capture Human Annotator Disagreements?", "comment": "Preprint Under Review", "summary": "Human annotation variation (i.e., annotation disagreements) is common in NLP\nand often reflects important information such as task subjectivity and sample\nambiguity. While Large Language Models (LLMs) are increasingly used for\nautomatic annotation to reduce human effort, their evaluation often focuses on\npredicting the majority-voted \"ground truth\" labels. It is still unclear,\nhowever, whether these models also capture informative human annotation\nvariation. Our work addresses this gap by extensively evaluating LLMs' ability\nto predict annotation disagreements without access to repeated human labels.\nOur results show that LLMs struggle with modeling disagreements, which can be\noverlooked by majority label-based evaluations. Notably, while RLVR-style\n(Reinforcement learning with verifiable rewards) reasoning generally boosts LLM\nperformance, it degrades performance in disagreement prediction. Our findings\nhighlight the critical need for evaluating and improving LLM annotators in\ndisagreement modeling. Code and data at\nhttps://github.com/EdisonNi-hku/Disagreement_Prediction.", "AI": {"tldr": "研究评估了大型语言模型预测注释分歧的能力，发现这些模型在建模分歧方面存在困难，而这一方面在基于多数标签的评估中可能被忽视。", "motivation": "研究动机在于填补大型语言模型在预测注释分歧方面能力评估的空白，因为现有评估往往侧重于预测多数投票的“地面实情”标签，而忽视了模型是否捕捉到了重要的注释分歧。", "method": "通过广泛的评估，研究主要探讨了在没有重复人类标签访问的情况下，大型语言模型预测注释分歧的能力。", "result": "研究结果表明，大型语言模型在预测注释分歧方面存在困难。值得注意的是，虽然基于可验证奖励的强化学习（RLVR）推理通常会提升大型语言模型的性能，但在分歧预测方面的性能却有所下降。", "conclusion": "研究强调了评估和改进大型语言模型在分歧预测建模方面的重要性。这表明需要进一步的工作来提高这些模型的性能，特别是在处理注释分歧的问题上。"}}
{"id": "2506.19154", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.19154", "abs": "https://arxiv.org/abs/2506.19154", "authors": ["Mahdi Falaki", "Maria A. Amer"], "title": "Lightweight RGB-T Tracking with Mobile Vision Transformers", "comment": null, "summary": "Single-modality object tracking (e.g., RGB-only) encounters difficulties in\nchallenging imaging conditions, such as low illumination and adverse weather\nconditions. To solve this, multimodal tracking (e.g., RGB-T models) aims to\nleverage complementary data such as thermal infrared features. While recent\nVision Transformer-based multimodal trackers achieve strong performance, they\nare often computationally expensive due to large model sizes. In this work, we\npropose a novel lightweight RGB-T tracking algorithm based on Mobile Vision\nTransformers (MobileViT). Our tracker introduces a progressive fusion framework\nthat jointly learns intra-modal and inter-modal interactions between the\ntemplate and search regions using separable attention. This design produces\neffective feature representations that support more accurate target\nlocalization while achieving a small model size and fast inference speed.\nCompared to state-of-the-art efficient multimodal trackers, our model achieves\ncomparable accuracy while offering significantly lower parameter counts (less\nthan 4 million) and the fastest GPU inference speed of 122 frames per second.\nThis paper is the first to propose a tracker using Mobile Vision Transformers\nfor RGB-T tracking and multimodal tracking at large. Tracker code and model\nweights will be made publicly available upon acceptance.", "AI": {"tldr": "提出了一种基于MobileViT的轻量级RGB-T跟踪算法，通过渐进融合框架提高了目标定位准确性，同时降低模型参数量和加快推理速度。", "motivation": "传统的单模态目标跟踪算法在低光照和恶劣天气条件下遇到困难。尽管基于Vision Transformers的多模态跟踪器性能较强，但模型尺寸较大，计算昂贵。本研究旨在解决此问题，并提出轻量级的解决方案。", "method": "基于Mobile Vision Transformers (MobileViT)的轻量级RGB-T跟踪算法，提出了一种渐进融合框架，通过可分离注意力机制，在模板和搜索区域之间同时学习单模态和多模态交互。", "result": "该算法在保持小模型尺寸和快速推理速度的同时，实现了有效的特征表示和更准确的目标定位。相比高效的多模态跟踪器，该模型具有相当的准确性，参数量少于4百万，GPU推理速度达到每秒122帧。", "conclusion": "本研究首次提出了使用Mobile Vision Transformers的RGB-T跟踪算法，该算法在保持高效性能的同时大大降低了计算成本。后续代码和模型权重将公开发布。"}}
{"id": "2506.19468", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.19468", "abs": "https://arxiv.org/abs/2506.19468", "authors": ["Wenhan Han", "Yifan Zhang", "Zhixun Chen", "Binbin Liu", "Haobin Lin", "Bingni Zhang", "Taifeng Wang", "Mykola Pechenizkiy", "Meng Fang", "Yin Zheng"], "title": "MuBench: Assessment of Multilingual Capabilities of Large Language Models Across 61 Languages", "comment": null, "summary": "Multilingual large language models (LLMs) are advancing rapidly, with new\nmodels frequently claiming support for an increasing number of languages.\nHowever, existing evaluation datasets are limited and lack cross-lingual\nalignment, leaving assessments of multilingual capabilities fragmented in both\nlanguage and skill coverage. To address this, we introduce MuBench, a benchmark\ncovering 61 languages and evaluating a broad range of capabilities. We evaluate\nseveral state-of-the-art multilingual LLMs and find notable gaps between\nclaimed and actual language coverage, particularly a persistent performance\ndisparity between English and low-resource languages. Leveraging MuBench's\nalignment, we propose Multilingual Consistency (MLC) as a complementary metric\nto accuracy for analyzing performance bottlenecks and guiding model\nimprovement. Finally, we pretrain a suite of 1.2B-parameter models on English\nand Chinese with 500B tokens, varying language ratios and parallel data\nproportions to investigate cross-lingual transfer dynamics.", "AI": {"tldr": "本文介绍了MuBench基准测试，发现多语言大语言模型中的性能差距，并提出多语言一致性作为补充指标。", "motivation": "现有的评估数据集有限，缺乏跨语言的对齐，导致对多语言能力的评估在语言和技能覆盖方面分散。为了解决这个问题，我们提出了MuBench。", "method": "我们介绍了一个涵盖61种语言的基准测试MuBench，用于评估广泛的能力范围。我们评估了几种先进的多语言大语言模型，并发现声称的语言覆盖范围与实际覆盖范围之间存在显著差距，特别是在英语和其他低资源语言之间存在持续的性能差异。", "result": "利用MuBench的对齐特性，我们提出了多语言一致性（MLC）作为补充指标，用于分析性能瓶颈并指导模型改进。最后，我们在英语和中文上进行了预训练，使用500B的token量，变化语言比例和平行数据的比例来研究跨语言转换动力学。", "conclusion": "通过引入MuBench和多语言一致性指标，我们可以更好地评估和提高多语言大语言模型的性能，特别是在低资源语言方面。"}}
{"id": "2506.19168", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.19168", "abs": "https://arxiv.org/abs/2506.19168", "authors": ["Mert Can Cakmak", "Nitin Agarwal", "Diwash Poudel"], "title": "PRISM: Perceptual Recognition for Identifying Standout Moments in Human-Centric Keyframe Extraction", "comment": null, "summary": "Online videos play a central role in shaping political discourse and\namplifying cyber social threats such as misinformation, propaganda, and\nradicalization. Detecting the most impactful or \"standout\" moments in video\ncontent is crucial for content moderation, summarization, and forensic\nanalysis. In this paper, we introduce PRISM (Perceptual Recognition for\nIdentifying Standout Moments), a lightweight and perceptually-aligned framework\nfor keyframe extraction. PRISM operates in the CIELAB color space and uses\nperceptual color difference metrics to identify frames that align with human\nvisual sensitivity. Unlike deep learning-based approaches, PRISM is\ninterpretable, training-free, and computationally efficient, making it well\nsuited for real-time and resource-constrained environments. We evaluate PRISM\non four benchmark datasets: BBC, TVSum, SumMe, and ClipShots, and demonstrate\nthat it achieves strong accuracy and fidelity while maintaining high\ncompression ratios. These results highlight PRISM's effectiveness in both\nstructured and unstructured video content, and its potential as a scalable tool\nfor analyzing and moderating harmful or politically sensitive media in online\nplatforms.", "AI": {"tldr": "PRISM, a lightweight framework for keyframe extraction, uses perceptual color difference metrics in the CIELAB color space. It is training-free, computationally efficient, and effective in both structured and unstructured video content.", "motivation": "The central role of online videos in shaping political discourse and amplifying cyber social threats makes it crucial to detect impactful moments in video content for content moderation, summarization, and forensic analysis.", "method": "PRISM (Perceptual Recognition for Identifying Standout Moments) is introduced as a lightweight and perceptually-aligned framework for keyframe extraction. It operates in the CIELAB color space and uses perceptual color difference metrics, making it interpretable, training-free, and computationally efficient.", "result": "PRISM achieves strong accuracy and fidelity while maintaining high compression ratios on four benchmark datasets: BBC, TVSum, SumMe, and ClipShots.", "conclusion": "PRISM is effective in both structured and unstructured video content, with strong accuracy and high compression ratios, and has potential as a scalable tool for analyzing and moderating harmful or politically sensitive media."}}
{"id": "2506.19483", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.19483", "abs": "https://arxiv.org/abs/2506.19483", "authors": ["Marcos Estecha-Garitagoitia", "Chen Zhang", "Mario Rodríguez-Cantelar", "Luis Fernando D'Haro"], "title": "Commonsense Generation and Evaluation for Dialogue Systems using Large Language Models", "comment": null, "summary": "This paper provides preliminary results on exploring the task of performing\nturn-level data augmentation for dialogue system based on different types of\ncommonsense relationships, and the automatic evaluation of the generated\nsynthetic turns. The proposed methodology takes advantage of the extended\nknowledge and zero-shot capabilities of pretrained Large Language Models (LLMs)\nto follow instructions, understand contextual information, and their\ncommonsense reasoning capabilities. The approach draws inspiration from\nmethodologies like Chain-of-Thought (CoT), applied more explicitly to the task\nof prompt-based generation for dialogue-based data augmentation conditioned on\ncommonsense attributes, and the automatic evaluation of the generated\ndialogues.\n  To assess the effectiveness of the proposed approach, first we extracted 200\nrandomly selected partial dialogues, from 5 different well-known dialogue\ndatasets, and generate alternative responses conditioned on different event\ncommonsense attributes. This novel dataset allows us to measure the proficiency\nof LLMs in generating contextually relevant commonsense knowledge, particularly\nup to 12 different specific ATOMIC [10] database relations. Secondly, we\npropose an evaluation framework to automatically detect the quality of the\ngenerated dataset inspired by the ACCENT [26] metric, which offers a nuanced\napproach to assess event commonsense. However, our method does not follow\nACCENT's complex eventrelation tuple extraction process. Instead, we propose an\ninstruction-based prompt for each commonsense attribute and use\nstate-of-the-art LLMs to automatically detect the original attributes used when\ncreating each augmented turn in the previous step.\n  Preliminary results suggest that our approach effectively harnesses LLMs\ncapabilities for commonsense reasoning and evaluation in dialogue systems.", "AI": {"tldr": "This paper explores the use of Large Language Models for turn-level data augmentation in dialogue systems, focusing on generating synthetic dialogue based on commonsense reasoning, and proposes an automated evaluation method inspired by ACCENT but simplified for practical use.", "motivation": "The motivation is to explore the use of LLMs for turn-level data augmentation and automated evaluation in dialogue systems based on commonsense reasoning, addressing the need for more contextually relevant synthetic data.", "method": "The method leverages Large Language Models (LLMs) for generating synthetic dialogue turns conditioned on commonsense attributes through a prompt-based approach inspired by Chain-of-Thought (CoT). It evaluates the synthetic dialogues using an auto-detection framework based on instructions for specific commonsense attributes, without the need for complex tuple extraction.", "result": "The preliminary results indicate that the proposed method can effectively augment dialogue datasets with contextually relevant synthetic turns using LLMs and can automatically detect the quality of the generated data based on commonsense attributes.", "conclusion": "The conclusion is that the method successfully exploits the capabilities of LLMs for commonsense reasoning and evaluation in dialogue systems, providing a promising foundation for further research in this area."}}
{"id": "2506.19174", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.19174", "abs": "https://arxiv.org/abs/2506.19174", "authors": ["Jialu Pi", "Juan Maria Farina", "Rimita Lahiri", "Jiwoong Jeong", "Archana Gurudu", "Hyung-Bok Park", "Chieh-Ju Chao", "Chadi Ayoub", "Reza Arsanjani", "Imon Banerjee"], "title": "MOSCARD -- Causal Reasoning and De-confounding for Multimodal Opportunistic Screening of Cardiovascular Adverse Events", "comment": null, "summary": "Major Adverse Cardiovascular Events (MACE) remain the leading cause of\nmortality globally, as reported in the Global Disease Burden Study 2021.\nOpportunistic screening leverages data collected from routine health check-ups\nand multimodal data can play a key role to identify at-risk individuals. Chest\nX-rays (CXR) provide insights into chronic conditions contributing to major\nadverse cardiovascular events (MACE), while 12-lead electrocardiogram (ECG)\ndirectly assesses cardiac electrical activity and structural abnormalities.\nIntegrating CXR and ECG could offer a more comprehensive risk assessment than\nconventional models, which rely on clinical scores, computed tomography (CT)\nmeasurements, or biomarkers, which may be limited by sampling bias and single\nmodality constraints. We propose a novel predictive modeling framework -\nMOSCARD, multimodal causal reasoning with co-attention to align two distinct\nmodalities and simultaneously mitigate bias and confounders in opportunistic\nrisk estimation. Primary technical contributions are - (i) multimodal alignment\nof CXR with ECG guidance; (ii) integration of causal reasoning; (iii) dual\nback-propagation graph for de-confounding. Evaluated on internal, shift data\nfrom emergency department (ED) and external MIMIC datasets, our model\noutperformed single modality and state-of-the-art foundational models - AUC:\n0.75, 0.83, 0.71 respectively. Proposed cost-effective opportunistic screening\nenables early intervention, improving patient outcomes and reducing\ndisparities.", "AI": {"tldr": "MOSCARD是一个新的预测建模框架，该框架通过共注意机制将胸部X光片（CXR）和心电图（ECG）的数据对齐，并同时减轻偏见和混杂因素，提高预测准确性。", "motivation": "机会性筛查利用常规健康检查中收集的数据，多模态数据可以在识别有风险的个人方面发挥作用。MOSCARD框架旨在提供比传统的依靠临床评分、CT测量或生物标志物更为全面的风险评估。", "method": "我们提出了一种新颖的预测建模框架MOSCARD，该框架通过共注意机制将胸部X光片（CXR）和心电图（ECG）的数据对齐，并同时减轻偏见和混杂因素。主要的技术贡献有：(i) 在ECG指导下对CXR进行多模态对齐；(ii) 集成因果推理；(iii) 使用双重反向传播图去除混杂因素。", "result": "在内部和急诊部的数据以及外部的MIMIC数据集上进行了评估，我们的模型在准确性上超过了单模态和其他最先进的模型 - AUC分别为0.75、0.83、0.71。", "conclusion": "该提出的成本效益高、机会性筛查方法可以实现早期干预，改善患者的结果并减少健康不平等。"}}
{"id": "2506.19484", "categories": ["cs.CL", "cs.AI", "cs.HC", "K.3.2; I.2.6; H.4.m"], "pdf": "https://arxiv.org/pdf/2506.19484", "abs": "https://arxiv.org/abs/2506.19484", "authors": ["Russell Beale"], "title": "Dialogic Pedagogy for Large Language Models: Aligning Conversational AI with Proven Theories of Learning", "comment": null, "summary": "Large Language Models (LLMs) are rapidly transforming education by enabling\nrich conversational learning experiences. This article provides a comprehensive\nreview of how LLM-based conversational agents are being used in higher\neducation, with extensions to secondary and lifelong learning contexts. We\nsynthesize existing literature on LLMs in education and theories of\nconversational and dialogic pedagogy - including Vygotsky's sociocultural\nlearning (scaffolding and the Zone of Proximal Development), the Socratic\nmethod, and Laurillard's conversational framework - and examine how prompting\nstrategies and retrieval-augmented generation (RAG) can align LLM behaviors\nwith these pedagogical theories, and how it can support personalized, adaptive\nlearning. We map educational theories to LLM capabilities, highlighting where\nLLM-driven dialogue supports established learning principles and where it\nchallenges or falls short of traditional pedagogical assumptions. Notable gaps\nin applying prior theories to LLMs are identified, such as the models tendency\nto provide direct answers instead of fostering co-construction of knowledge,\nand the need to account for the constant availability and broad but non-human\nexpertise of LLM tutors. In response, we propose practical strategies to better\nalign LLM interactions with sound pedagogy - for example, designing prompts\nthat encourage Socratic questioning, scaffolded guidance, and student\nreflection, as well as integrating retrieval mechanisms to ensure accuracy and\ncontextual relevance. Our aim is to bridge the gap between educational theory\nand the emerging practice of AI-driven conversational learning, offering\ninsights and tools for making LLM-based dialogues more educationally productive\nand theory-aligned.", "AI": {"tldr": "本文综述了大型语言模型在教育中的应用，探讨了这些模型如何与对话教学法相结合，支持个性化和适应性学习。文章还指出了应用现有理论到大型语言模型时出现的差距，并提出了一些策略来更好地对齐AI驱动的对话学习和教育理论。", "motivation": "文章的动机是为了理解和评估大型语言模型在教育中的潜力，特别是在通过对话支持个性化和适应性学习方面，以及如何将这些模型与现有教育理论相结合。", "method": "通过综合现有文献，这种方法结合了大型语言模型的能力与对话教学法理论，比如维果茨基的社会文化学习理论、苏格拉底方法和劳里拉对话教学框架，来探索两者如何相互作用。", "result": "研究表明，虽然大型语言模型能支持部分教育原则，但也显示出一些不足，如倾向直接提供答案而非支持知识共建。文章提出了改进策略，以期更好地应对这些挑战。", "conclusion": "大型语言模型在教育中有巨大的潜力，但要实现其优势，仍需要根据教育原则设计对话策略并整合检索机制以确保信息的精准和情境相关性，从而使得AI驱动的对话学习更加高效和理论对齐。"}}
{"id": "2506.19204", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.19204", "abs": "https://arxiv.org/abs/2506.19204", "authors": ["Muhammed Patel", "Javier Noa Turnes", "Jayden Hsiao", "Linlin Xu", "David Clausi"], "title": "OpenWildlife: Open-Vocabulary Multi-Species Wildlife Detector for Geographically-Diverse Aerial Imagery", "comment": null, "summary": "We introduce OpenWildlife (OW), an open-vocabulary wildlife detector designed\nfor multi-species identification in diverse aerial imagery. While existing\nautomated methods perform well in specific settings, they often struggle to\ngeneralize across different species and environments due to limited taxonomic\ncoverage and rigid model architectures. In contrast, OW leverages\nlanguage-aware embeddings and a novel adaptation of the Grounding-DINO\nframework, enabling it to identify species specified through natural language\ninputs across both terrestrial and marine environments. Trained on 15 datasets,\nOW outperforms most existing methods, achieving up to \\textbf{0.981} mAP50 with\nfine-tuning and \\textbf{0.597} mAP50 on seven datasets featuring novel species.\nAdditionally, we introduce an efficient search algorithm that combines\nk-nearest neighbors and breadth-first search to prioritize areas where social\nspecies are likely to be found. This approach captures over \\textbf{95\\%} of\nspecies while exploring only \\textbf{33\\%} of the available images. To support\nreproducibility, we publicly release our source code and dataset splits,\nestablishing OW as a flexible, cost-effective solution for global biodiversity\nassessments.", "AI": {"tldr": "OpenWildlife (OW) is an open-vocabulary wildlife detector designed for multi-species identification in diverse aerial imagery, offering superior performance and flexibility over existing methods.", "motivation": "The motivation behind OW is to address the limitations of existing automated wildlife detection methods which perform well in specific settings but struggle to generalize across different species and environments.", "method": "_OW leverages language-aware embeddings and a novel adaptation of the Grounding-DINO framework, and introduces an efficient search algorithm that combines k-nearest neighbors and breadth-first search to identify species in aerial imagery.", "result": "OW achieves up to 0.981 mAP50 with fine-tuning and 0.597 mAP50 on seven datasets featuring novel species, capturing over 95% of species while exploring only 33% of the available images.", "conclusion": "The conclusion is that OW, being a flexible solution, outperforms most existing methods and can be utilized for cost-effective global biodiversity assessments."}}
{"id": "2506.19492", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.19492", "abs": "https://arxiv.org/abs/2506.19492", "authors": ["Shu Yang", "Junchao Wu", "Xuansheng Wu", "Derek Wong", "Ninhao Liu", "Di Wang"], "title": "Is Long-to-Short a Free Lunch? Investigating Inconsistency and Reasoning Efficiency in LRMs", "comment": null, "summary": "Large Reasoning Models (LRMs) have achieved remarkable performance on complex\ntasks by engaging in extended reasoning before producing final answers, yet\nthis strength introduces the risk of overthinking, where excessive token\ngeneration occurs even for simple tasks. While recent work in efficient\nreasoning seeks to reduce reasoning length while preserving accuracy, it\nremains unclear whether such optimization is truly a free lunch. Drawing on the\nintuition that compressing reasoning may reduce the robustness of model\nresponses and lead models to omit key reasoning steps, we investigate whether\nefficient reasoning strategies introduce behavioral inconsistencies. To\nsystematically assess this, we introduce $ICBENCH$, a benchmark designed to\nmeasure inconsistency in LRMs across three dimensions: inconsistency across\ntask settings (ITS), inconsistency between training objectives and learned\nbehavior (TR-LB), and inconsistency between internal reasoning and\nself-explanations (IR-SE). Applying $ICBENCH$ to a range of open-source LRMs,\nwe find that while larger models generally exhibit greater consistency than\nsmaller ones, they all display widespread \"scheming\" behaviors, including\nself-disagreement, post-hoc rationalization, and the withholding of reasoning\ncues. Crucially, our results demonstrate that efficient reasoning strategies\nsuch as No-Thinking and Simple Token-Budget consistently increase all three\ndefined types of inconsistency. These findings suggest that although efficient\nreasoning enhances token-level efficiency, further investigation is imperative\nto ascertain whether it concurrently introduces the risk of models evading\neffective supervision.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2506.19208", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.19208", "abs": "https://arxiv.org/abs/2506.19208", "authors": ["Xiaolei Diao", "Rite Bo", "Yanling Xiao", "Lida Shi", "Zhihan Zhou", "Hao Xu", "Chuntao Li", "Xiongfeng Tang", "Massimo Poesio", "Cédric M. John", "Daqian Shi"], "title": "Ancient Script Image Recognition and Processing: A Review", "comment": null, "summary": "Ancient scripts, e.g., Egyptian hieroglyphs, Oracle Bone Inscriptions, and\nAncient Greek inscriptions, serve as vital carriers of human civilization,\nembedding invaluable historical and cultural information. Automating ancient\nscript image recognition has gained importance, enabling large-scale\ninterpretation and advancing research in archaeology and digital humanities.\nWith the rise of deep learning, this field has progressed rapidly, with\nnumerous script-specific datasets and models proposed. While these scripts vary\nwidely, spanning phonographic systems with limited glyphs to logographic\nsystems with thousands of complex symbols, they share common challenges and\nmethodological overlaps. Moreover, ancient scripts face unique challenges,\nincluding imbalanced data distribution and image degradation, which have driven\nthe development of various dedicated methods. This survey provides a\ncomprehensive review of ancient script image recognition methods. We begin by\ncategorizing existing studies based on script types and analyzing respective\nrecognition methods, highlighting both their differences and shared strategies.\nWe then focus on challenges unique to ancient scripts, systematically examining\ntheir impact and reviewing recent solutions, including few-shot learning and\nnoise-robust techniques. Finally, we summarize current limitations and outline\npromising future directions. Our goal is to offer a structured, forward-looking\nperspective to support ongoing advancements in the recognition, interpretation,\nand decipherment of ancient scripts.", "AI": {"tldr": "A comprehensive review of ancient script image recognition methods, categorizing studies by script type, examining common challenges and solutions, and highlighting future research directions.", "motivation": "To provide a structured and forward-looking perspective on ancient script image recognition, aiding in the large-scale interpretation and research advancements in archaeology and digital humanities.", "method": "Ancient script image recognition methods are categorized based on script types, such as phonographic and logographic systems, and the recognition methods for each type are discussed. The paper also examines challenges specific to ancient scripts, such as imbalanced data and image degradation, and reviews recent solutions including few-shot learning and noise-robust techniques.", "result": "The paper provides a comprehensive review of the current state of ancient script image recognition, covering a range of methodologies and challenges. It also discusses the commonalities and differences among various script recognition methods.", "conclusion": "The review highlights the need for further research to address the limitations in ancient script recognition, such as improving robustness against image degradation and handling imbalanced data, and suggests future research directions."}}
{"id": "2506.19505", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.19505", "abs": "https://arxiv.org/abs/2506.19505", "authors": ["Zeyu Li", "Chuanfu Xiao", "Yang Wang", "Xiang Liu", "Zhenheng Tang", "Baotong Lu", "Mao Yang", "Xinyu Chen", "Xiaowen Chu"], "title": "AnTKV: Anchor Token-Aware Sub-Bit Vector Quantization for KV Cache in Large Language Models", "comment": null, "summary": "Quantization has emerged as an effective and lightweight solution to reduce\nthe memory footprint of the KV cache in Large Language Models (LLMs).\nNevertheless, minimizing the performance degradation caused by ultra-low-bit KV\ncache quantization remains a significant challenge. We observe that quantizing\nthe KV cache of different tokens has varying impacts on the quality of\nattention outputs. To systematically investigate this phenomenon, we perform\nforward error propagation analysis on attention and propose the Anchor Score\n(AnS) that quantifies the sensitivity of each token's KV cache to\nquantization-induced error. Our analysis reveals significant disparities in AnS\nacross tokens, suggesting that preserving a small subset with full precision\n(FP16) of high-AnS tokens can greatly mitigate accuracy loss in aggressive\nquantization scenarios. Based on this insight, we introduce AnTKV, a novel\nframework that leverages Anchor Token-aware Vector Quantization to compress the\nKV cache. Furthermore, to support efficient deployment, we design and develop a\ntriton kernel that is fully compatible with FlashAttention, enabling fast\nonline Anchor Token selection. AnTKV enables LLaMA-3-8B to handle context\nlengths up to 840K tokens on a single 80GB A100 GPU, while achieving up to 3.5x\nhigher decoding throughput compared to the FP16 baseline. Our experiment\nresults demonstrate that AnTKV matches or outperforms prior works such as KIVI,\nSKVQ, KVQuant, and CQ under 4-bit settings. More importantly, AnTKV achieves\nsignificantly lower perplexity under ultra-low-bit quantization on Mistral-7B,\nwith only 6.32 at 1-bit and 8.87 at 0.375-bit, compared to the FP16 baseline of\n4.73.", "AI": {"tldr": "通过Anchor Score（AnS）量化每个token KV缓存对量化引起的误差的敏感性，并提出AnTKV框架来压缩KV缓存，实现高效的token选择和内存压缩，同时保持较高的解码吞吐量和较低的困惑度。", "motivation": "在减少大型语言模型内存占用的同时，最小化超低比特KV缓存量化带来的性能下降是一个重大挑战。研究了不同token的KV缓存量化对注意力输出质量的影响。", "method": "提出了一种名为AnTKV的新框架，该框架利用Anchor Token感知的向量量化来压缩KV缓存，并设计了一个与FlashAttention完全兼容的triton内核，以实现高效的在线Anchor Token选择。", "result": "AnTKV能使LLaMA-3-8B在单一80GB A100 GPU上处理长达840K tokens的上下文长度，解码吞吐量相比FP16基线提升了3.5倍，且在极致低比特量化下比FP16基线有更低的困惑度。", "conclusion": "AnTKV在4位量化设置下匹配或超越了以前的工作，包括KIVI，SKVQ，KVQuant和CQ，并在Mistral-7B上实现了显著更低的困惑度。"}}
{"id": "2506.19217", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.19217", "abs": "https://arxiv.org/abs/2506.19217", "authors": ["Sunggu Kyung", "Hyungbin Park", "Jinyoung Seo", "Jimin Sung", "Jihyun Kim", "Dongyeong Kim", "Wooyoung Jo", "Yoojin Nam", "Sangah Park", "Taehee Kwon", "Sang Min Lee", "Namkug Kim"], "title": "MedErr-CT: A Visual Question Answering Benchmark for Identifying and Correcting Errors in CT Reports", "comment": "14 pages, 5 figures, submitted to CVPR 2025", "summary": "Computed Tomography (CT) plays a crucial role in clinical diagnosis, but the\ngrowing demand for CT examinations has raised concerns about diagnostic errors.\nWhile Multimodal Large Language Models (MLLMs) demonstrate promising\ncomprehension of medical knowledge, their tendency to produce inaccurate\ninformation highlights the need for rigorous validation. However, existing\nmedical visual question answering (VQA) benchmarks primarily focus on simple\nvisual recognition tasks, lacking clinical relevance and failing to assess\nexpert-level knowledge. We introduce MedErr-CT, a novel benchmark for\nevaluating medical MLLMs' ability to identify and correct errors in CT reports\nthrough a VQA framework. The benchmark includes six error categories - four\nvision-centric errors (Omission, Insertion, Direction, Size) and two lexical\nerror types (Unit, Typo) - and is organized into three task levels:\nclassification, detection, and correction. Using this benchmark, we\nquantitatively assess the performance of state-of-the-art 3D medical MLLMs,\nrevealing substantial variation in their capabilities across different error\ntypes. Our benchmark contributes to the development of more reliable and\nclinically applicable MLLMs, ultimately helping reduce diagnostic errors and\nimprove accuracy in clinical practice. The code and datasets are available at\nhttps://github.com/babbu3682/MedErr-CT.", "AI": {"tldr": "本文介绍了MedErr-CT，一个评估医疗MLLMs能力识别和纠正CT报告错误的新型基准，说明了其任务类别及评估方法，展示了最先进的医疗3D MLLMs在不同错误类型的性能差异。", "motivation": "尽管多模态大型语言模型(MLLMs)对医学知识的理解表现出前景，但产生不准确信息的倾向凸显了严谨验证的必要性。现有医疗视觉问题回答(VQA)基准主要集中在简单的视觉识别任务上，缺乏临床相关性，未能评估专家级知识。", "method": "引入了MedErr-CT，这是一个评估医疗MLLMs能力的新基准，通过VQA框架来识别和纠正CT报告中的错误。基准包括六类错误：四个视觉中心错误（遗漏、插入、方向、大小）和两类词汇错误（单位，拼写）。这些任务分为三个级别：分类、检测和纠正。", "result": "使用该基准，我们定量评估了最先进的3D医疗MLLMs的性能，揭示了它们在不同错误类型上的能力存在显著差异。", "conclusion": "本文的基准有助于开发更可靠且适用于临床的MLLMs，最终有助于减少临床实践中诊断错误并提高准确性。"}}
{"id": "2506.19512", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.19512", "abs": "https://arxiv.org/abs/2506.19512", "authors": ["Ashish Chouhan", "Michael Gertz"], "title": "heiDS at ArchEHR-QA 2025: From Fixed-k to Query-dependent-k for Retrieval Augmented Generation", "comment": "12 pages, 2 figures, 6 tables, Workshop on BioNLP and Shared Tasks at\n  ACL 2025", "summary": "This paper presents the approach of our team called heiDS for the ArchEHR-QA\n2025 shared task. A pipeline using a retrieval augmented generation (RAG)\nframework is designed to generate answers that are attributed to clinical\nevidence from the electronic health records (EHRs) of patients in response to\npatient-specific questions. We explored various components of a RAG framework,\nfocusing on ranked list truncation (RLT) retrieval strategies and attribution\napproaches. Instead of using a fixed top-k RLT retrieval strategy, we employ a\nquery-dependent-k retrieval strategy, including the existing surprise and\nautocut methods and two new methods proposed in this work, autocut* and elbow.\nThe experimental results show the benefits of our strategy in producing factual\nand relevant answers when compared to a fixed-$k$.", "AI": {"tldr": "本文介绍了一种针对患者的电子健康记录(EHRs)中的临床证据提出答案的方法，使用了自适应的检索增强生成(RAG)框架，并在实验中验证了其产生的答案比固定检索策略更相关和事实性。", "motivation": "本文的动机在于提升对患者特定问题的回答质量，通过优化检索策略来提取更准确和相关的临床证据。", "method": "本文描述了一个使用检索增强生成（RAG）框架的方法，该框架通过从患者的电子健康记录（EHRs）中提取临床证据来生成针对患者特定问题的答案。团队研究了RAG框架的不同组成部分，特别是排名列表截断（RLT）检索策略和归因方法。团队提出了autocut*和elbow两种新方法，与现有的surprise和autocut方法结合形成了一种根据查询自适应的-k检索策略。", "result": "实验结果表明，与固定-k检索策略相比，这种方法能够产生更事实和相关性的答案。", "conclusion": "自适应-k检索策略相较于传统的固定-k策略更能够产生具有事实性的答案，验证了优化检索策略在提升RAG框架性能中的有效性。"}}
{"id": "2506.19225", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.19225", "abs": "https://arxiv.org/abs/2506.19225", "authors": ["Minghao Qin", "Xiangrui Liu", "Zhengyang Liang", "Yan Shu", "Huaying Yuan", "Juenjie Zhou", "Shitao Xiao", "Bo Zhao", "Zheng Liu"], "title": "Video-XL-2: Towards Very Long-Video Understanding Through Task-Aware KV Sparsification", "comment": "12 pages, 5 Figure, 3 Table", "summary": "Multi-modal large language models (MLLMs) models have made significant\nprogress in video understanding over the past few years. However, processing\nlong video inputs remains a major challenge due to high memory and\ncomputational costs. This makes it difficult for current models to achieve both\nstrong performance and high efficiency in long video understanding. To address\nthis challenge, we propose Video-XL-2, a novel MLLM that delivers superior\ncost-effectiveness for long-video understanding based on task-aware KV\nsparsification. The proposed framework operates with two key steps: chunk-based\npre-filling and bi-level key-value decoding. Chunk-based pre-filling divides\nthe visual token sequence into chunks, applying full attention within each\nchunk and sparse attention across chunks. This significantly reduces\ncomputational and memory overhead. During decoding, bi-level key-value decoding\nselectively reloads either dense or sparse key-values for each chunk based on\nits relevance to the task. This approach further improves memory efficiency and\nenhances the model's ability to capture fine-grained information. Video-XL-2\nachieves state-of-the-art performance on various long video understanding\nbenchmarks, outperforming existing open-source lightweight models. It also\ndemonstrates exceptional efficiency, capable of processing over 10,000 frames\non a single NVIDIA A100 (80GB) GPU and thousands of frames in just a few\nseconds.", "AI": {"tldr": "Introduces Video-XL-2, a novel multi-modal large language model that significantly improves long video understanding performance and efficiency through task-aware KV sparsification.", "motivation": "to address the challenge of processing long video inputs with high memory and computational costs.", "method": "chunk-based pre-filling and bi-level key-value decoding to reduce computational and memory overhead and improve model efficiency in long video understanding.", "result": "Video-XL-2 achieves state-of-the-art performance on various long video understanding benchmarks and shows exceptional efficiency.", "conclusion": "Video-XL-2 effectively balances long video understanding performance and computational efficiency by employing task-aware KV sparsification techniques."}}
{"id": "2506.19525", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.19525", "abs": "https://arxiv.org/abs/2506.19525", "authors": ["Natalia Bobkova", "Laura Zanella-Calzada", "Anyes Tafoughalt", "Raphaël Teboul", "François Plesse", "Félix Gaschi"], "title": "Automatic Posology Structuration : What role for LLMs?", "comment": null, "summary": "Automatically structuring posology instructions is essential for improving\nmedication safety and enabling clinical decision support. In French\nprescriptions, these instructions are often ambiguous, irregular, or\ncolloquial, limiting the effectiveness of classic ML pipelines. We explore the\nuse of Large Language Models (LLMs) to convert free-text posologies into\nstructured formats, comparing prompt-based methods and fine-tuning against a\n\"pre-LLM\" system based on Named Entity Recognition and Linking (NERL). Our\nresults show that while prompting improves performance, only fine-tuned LLMs\nmatch the accuracy of the baseline. Through error analysis, we observe\ncomplementary strengths: NERL offers structural precision, while LLMs better\nhandle semantic nuances. Based on this, we propose a hybrid pipeline that\nroutes low-confidence cases from NERL (<0.8) to the LLM, selecting outputs\nbased on confidence scores. This strategy achieves 91% structuration accuracy\nwhile minimizing latency and compute. Our results show that this hybrid\napproach improves structuration accuracy while limiting computational cost,\noffering a scalable solution for real-world clinical use.", "AI": {"tldr": "该研究探索了大型语言模型（LLMs）在结构化自由文本剂量指令方面的效用，发现微调后的LLMs在准确性上可以匹敌NERL基线系统，并提出了一种基于置信分数选择输出的混合管道，实现了91%的结构化准确性，同时减少了延迟和计算成本。", "motivation": "自动结构化剂量指令对于提高药物安全性和支持临床决策非常重要。尤其是，在法语处方中，这些指令常常语义模糊、不规则或口语化，这限制了传统机器学习流水线的有效性。", "method": "研究方法包括使用大型语言模型（LLMs）将自由文本剂量指令转换为结构化的格式，并将其与基于命名实体识别和链接（NERL）的“前LLM”系统进行比较。研究人员测试了基于提示的方法和微调LLM的效果。", "result": "研究结果表明，尽管提示方法提升了性能，只有微调的LLMs才能达到基线的准确性。错误分析指出，NERL在结构准确性方面更强，而LLMs在处理语义细微差别方面更优。", "conclusion": "基于此，研究者提出了一种混合管道，将NERL输出低于0.8置信度的低置信度情况进行LLM处理，并基于置信得分选择输出，该策略达到了91%的结构化准确性，同时最小化了延迟和计算量。这种混合方法提高了结构准确性，同时限制了计算成本，为临床实际使用提供了一种可行的解决方案。"}}
{"id": "2506.19257", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.19257", "abs": "https://arxiv.org/abs/2506.19257", "authors": ["Yinan Xia", "Yilei Jiang", "Yingshui Tan", "Xiaoyong Zhu", "Xiangyu Yue", "Bo Zheng"], "title": "MSR-Align: Policy-Grounded Multimodal Alignment for Safety-Aware Reasoning in Vision-Language Models", "comment": null, "summary": "Vision-Language Models (VLMs) have achieved remarkable progress in multimodal\nreasoning tasks through enhanced chain-of-thought capabilities. However, this\nadvancement also introduces novel safety risks, as these models become\nincreasingly vulnerable to harmful multimodal prompts that can trigger\nunethical or unsafe behaviors. Existing safety alignment approaches, primarily\ndesigned for unimodal language models, fall short in addressing the complex and\nnuanced threats posed by multimodal inputs. Moreover, current safety datasets\nlack the fine-grained, policy-grounded reasoning required to robustly align\nreasoning-capable VLMs. In this work, we introduce {MSR-Align}, a high-quality\nMultimodal Safety Reasoning dataset tailored to bridge this gap. MSR-Align\nsupports fine-grained, deliberative reasoning over standardized safety policies\nacross both vision and text modalities. Our data generation pipeline emphasizes\nmultimodal diversity, policy-grounded reasoning, and rigorous quality filtering\nusing strong multimodal judges. Extensive experiments demonstrate that\nfine-tuning VLMs on MSR-Align substantially improves robustness against both\ntextual and vision-language jailbreak attacks, while preserving or enhancing\ngeneral reasoning performance. MSR-Align provides a scalable and effective\nfoundation for advancing the safety alignment of reasoning-capable VLMs. Our\ndataset is made publicly available at\nhttps://huggingface.co/datasets/Leigest/MSR-Align.", "AI": {"tldr": "The paper presents MSR-Align, a dataset aimed at improving the safety and robustness of multimodal Vision-Language Models against harmful prompts.", "motivation": "The motivation is to address the increasing safety risks brought by multimodal inputs to reasoning-capable VLMs, as existing safety alignment approaches are inadequate for these complex scenarios.", "method": "The paper introduces MSR-Align, a dataset designed to enhance the safety and robustness of Vision-Language Models (VLMs) by addressing multimodal safety reasoning. It emphasizes multimodal diversity, policy-grounded reasoning, and quality filtering.", "result": "Experiments show that fine-tuning VLMs on MSR-Align improves their robustness against both textual and visual-textual jailbreak attacks, without compromising their general reasoning capabilities.", "conclusion": "MSR-Align is a critical resource for advancing the safety alignment of reasoning-capable VLMs and is openly shared for further research."}}
{"id": "2506.19527", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.19527", "abs": "https://arxiv.org/abs/2506.19527", "authors": ["Kelin Fu", "Kaigui Bian"], "title": "KnowMap: Efficient Knowledge-Driven Task Adaptation for LLMs", "comment": null, "summary": "While Large Language Models (LLMs) possess significant capabilities in\nopen-world agent tasks, they also face challenges in rapidly adapting to new,\nspecialized tasks due to their reliance on static pre-trained knowledge.\nTraditional methods such as fine-tuning are often costly, data-intensive, and\nmay lead to \"catastrophic forgetting.\" Therefore, we present KnowMap, a novel\napproach that dynamically constructs a knowledge base from environmental and\nexperiential data. KnowMap fine-tunes a small knowledge-embedding model to\nequip a larger LLM with valuable task-specific knowledge. Our experiments on\nthe ScienceWorld benchmark demonstrate 17.71% improvement for the performance\nof gpt-4-turbo model. KnowMap not only provides an efficient and effective\nmeans for LLM task-adapting, but also highlights how integrating environmental\nand experiential knowledge can enhance LLMs' reasoning capabilities.", "AI": {"tldr": "KnowMap通过动态构建知识库并微调模型，解决了大型语言模型在适应新任务时的挑战，提高了模型性能。", "motivation": "由于大型语言模型在快速适应新任务时存在挑战，传统方法如微调成本高昂且可能导致灾难性遗忘，因此提出KnowMap方法。", "method": "KnowMap采用动态构建知识库的方法，从环境和经验数据中提取，通过微调一个小模型来增强大型语言模型的特定任务知识。", "result": "实验表明，在ScienceWorld基准测试中，KnowMap使gpt-4-turbo模型的性能提高了17.71%。", "conclusion": "KnowMap不仅为大型语言模型的快速适应任务提供了一种有效的方法，而且还展示了集成环境和经验知识如何增强模型的推理能力。"}}
{"id": "2506.19261", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.19261", "abs": "https://arxiv.org/abs/2506.19261", "authors": ["Quang-Binh Nguyen", "Trong-Vu Hoang", "Ngoc-Do Tran", "Tam V. Nguyen", "Minh-Triet Tran", "Trung-Nghia Le"], "title": "Automated Image Recognition Framework", "comment": "ICCCI 2025", "summary": "While the efficacy of deep learning models heavily relies on data, gathering\nand annotating data for specific tasks, particularly when addressing novel or\nsensitive subjects lacking relevant datasets, poses significant time and\nresource challenges. In response to this, we propose a novel Automated Image\nRecognition (AIR) framework that harnesses the power of generative AI. AIR\nempowers end-users to synthesize high-quality, pre-annotated datasets,\neliminating the necessity for manual labeling. It also automatically trains\ndeep learning models on the generated datasets with robust image recognition\nperformance. Our framework includes two main data synthesis processes, AIR-Gen\nand AIR-Aug. The AIR-Gen enables end-users to seamlessly generate datasets\ntailored to their specifications. To improve image quality, we introduce a\nnovel automated prompt engineering module that leverages the capabilities of\nlarge language models. We also introduce a distribution adjustment algorithm to\neliminate duplicates and outliers, enhancing the robustness and reliability of\ngenerated datasets. On the other hand, the AIR-Aug enhances a given dataset,\nthereby improving the performance of deep classifier models. AIR-Aug is\nparticularly beneficial when users have limited data for specific tasks.\nThrough comprehensive experiments, we demonstrated the efficacy of our\ngenerated data in training deep learning models and showcased the system's\npotential to provide image recognition models for a wide range of objects. We\nalso conducted a user study that achieved an impressive score of 4.4 out of\n5.0, underscoring the AI community's positive perception of AIR.", "AI": {"tldr": "本研究提出了一种名为AIR的框架，利用生成式AI技术，实现了高质量数据集的自动合成与深度学习模型的自动化训练，尤其适用于没有充足数据集的新型或敏感主题，实验证明了其有效性和潜在价值。", "motivation": "鉴于深度学习模型的效果很大程度上取决于数据，而针对特定任务，尤其是在没有相关数据集的新型或敏感主题上，收集和标注数据面临着时间和资源上的挑战。为了解决这些问题，本文提出了上述方法。", "method": "本文提出了一种名为自动图像识别（AIR）的框架，该框架利用生成式AI技术，使用户能够自动生成高质量且预先标注的数据集，从而避免了手动标注的需求。它还能够在生成的数据集上自动训练深度学习模型，实现强大的图像识别性能。该框架包含两个主要的数据合成过程：AIR-Gen 和 AIR-Aug。其中，AIR-Gen 能够使用户无缝生成符合其特定需求的数据集；AIR-Aug 则在给定数据集的基础上进行增强，特别是当用户面临特定任务数据不足时，该模块特别有用。此外，引入了一种新颖的自动化提示工程模块，利用大型语言模型的能力提高图像质量，并配有一个分布调整算法来消除重复和异常值，增强数据集的健壮性和可靠性。", "result": "通过全面的实验，展示了本文生成数据在训练深度学习模型上的有效性，也证明了该系统在为各种物体提供图像识别模型方面的潜力。", "conclusion": "本文提出了一种能够在生成的数据集上自动训练强大图像识别模型的框架，去除了手动标注数据集的需求，展示了其在提升深度学习模型性能方面的潜在价值。"}}
{"id": "2506.19548", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2506.19548", "abs": "https://arxiv.org/abs/2506.19548", "authors": ["Devesh Pant", "Rishi Raj Grandhe", "Vipin Samaria", "Mukul Paul", "Sudhir Kumar", "Saransh Khanna", "Jatin Agrawal", "Jushaan Singh Kalra", "Akhil VSSG", "Satish V Khalikar", "Vipin Garg", "Himanshu Chauhan", "Pranay Verma", "Neha Khandelwal", "Soma S Dhavala", "Minesh Mathew"], "title": "Health Sentinel: An AI Pipeline For Real-time Disease Outbreak Detection", "comment": null, "summary": "Early detection of disease outbreaks is crucial to ensure timely intervention\nby the health authorities. Due to the challenges associated with traditional\nindicator-based surveillance, monitoring informal sources such as online media\nhas become increasingly popular. However, owing to the number of online\narticles getting published everyday, manual screening of the articles is\nimpractical. To address this, we propose Health Sentinel. It is a multi-stage\ninformation extraction pipeline that uses a combination of ML and non-ML\nmethods to extract events-structured information concerning disease outbreaks\nor other unusual health events-from online articles. The extracted events are\nmade available to the Media Scanning and Verification Cell (MSVC) at the\nNational Centre for Disease Control (NCDC), Delhi for analysis, interpretation\nand further dissemination to local agencies for timely intervention. From April\n2022 till date, Health Sentinel has processed over 300 million news articles\nand identified over 95,000 unique health events across India of which over\n3,500 events were shortlisted by the public health experts at NCDC as potential\noutbreaks.", "AI": {"tldr": "Health Sentinel 是一个用于自动从在线文章中提取疾病爆发相关事件的多阶段信息提取流程，已处理超过3亿篇文章并识别出9.5万个健康事件。", "motivation": "由于传统基于指标的监控在疾病爆发早期检测中的挑战，本研究旨在通过从在线媒体自动提取相关事件，以支持公共卫生机构的及时干预。", "method": "研究提出了一种结合机器学习和非机器学习方法的多阶段信息提取管道。", "result": "自2022年4月起，Health Sentinel 已处理超过3亿篇新闻文章，识别出9.5万个健康事件，其中3500个被公共卫生专家选作潜在爆发事件。", "conclusion": "这种方法有效地辅助了疾病控制中心识别和响应潜在的疾病爆发，提高了公共卫生监控的效率和及时性。"}}
{"id": "2506.19263", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.19263", "abs": "https://arxiv.org/abs/2506.19263", "authors": ["Rui Huang", "Jincheng Zeng", "Sen Gao", "Yan Xing"], "title": "3D-SSM: A Novel 3D Selective Scan Module for Remote Sensing Change Detection", "comment": null, "summary": "Existing Mamba-based approaches in remote sensing change detection have\nenhanced scanning models, yet remain limited by their inability to capture\nlong-range dependencies between image channels effectively, which restricts\ntheir feature representation capabilities. To address this limitation, we\npropose a 3D selective scan module (3D-SSM) that captures global information\nfrom both the spatial plane and channel perspectives, enabling a more\ncomprehensive understanding of the data.Based on the 3D-SSM, we present two key\ncomponents: a spatiotemporal interaction module (SIM) and a multi-branch\nfeature extraction module (MBFEM). The SIM facilitates bi-temporal feature\nintegration by enabling interactions between global and local features across\nimages from different time points, thereby enhancing the detection of subtle\nchanges. Meanwhile, the MBFEM combines features from the frequency domain,\nspatial domain, and 3D-SSM to provide a rich representation of contextual\ninformation within the image. Our proposed method demonstrates favourable\nperformance compared to state-of-the-art change detection methods on five\nbenchmark datasets through extensive experiments. Code is available at\nhttps://github.com/VerdantMist/3D-SSM", "AI": {"tldr": "该论文提出了一种新型3D选择性扫描模块及其配套组件，以提高遥感图像变化检测的质量和效率。", "motivation": "现有基于Mamba的遥感变化检测方法虽然改进了扫描模型，但在捕捉图像通道之间的长范围依赖方面存在不足，限制了特征表示能力。为解决此问题，该论文提出了新的方法。", "method": "该论文提出了一种3D选择性扫描模块（3D-SSM），该模块从空间和通道角度捕捉全局信息。基于3D-SSM，论文设计了两个关键组件：空间-时间交互模块（SIM）和多分支特征提取模块（MBFEM）。SIM通过不同时间点图像之间的全局和局部特征交互，促进生物特征的融合，从而增强细小变化的检测能力。而MBFEM结合了频率域、空间域和3D-SSM的特征，提供丰富的上下文信息表达。", "result": "实验表明，该论文提出的方法在五个基准数据集上相较于目前最先进的变化检测方法表现出优越的性能。", "conclusion": "所提出的方法通过3D-SSM、SIM和MBFEM的结合，在捕捉图像间的变化和提供丰富上下文信息方面展现出优越性能。"}}
{"id": "2506.19549", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.19549", "abs": "https://arxiv.org/abs/2506.19549", "authors": ["Debabrata Mahapatra", "Shubham Agarwal", "Apoorv Saxena", "Subrata Mitra"], "title": "RCStat: A Statistical Framework for using Relative Contextualization in Transformers", "comment": null, "summary": "Prior work on input-token importance in auto-regressive transformers has\nrelied on Softmax-normalized attention weights, which obscure the richer\nstructure of pre-Softmax query-key logits. We introduce RCStat, a statistical\nframework that harnesses raw attention logits via Relative Contextualization\n(RC), a random variable measuring contextual alignment between token segments,\nand derive an efficient upper bound for RC. We demonstrate two applications:\n(i) Key-Value compression, where RC-based thresholds drive adaptive key-value\neviction for substantial cache reduction with minimal quality loss; and (ii)\nAttribution, where RC yields higher-fidelity token-, sentence-, and chunk-level\nexplanations than post-Softmax methods. Across question answering,\nsummarization, and attribution benchmarks, RCStat achieves significant\nempirical gains, delivering state-of-the-art compression and attribution\nperformance without any model retraining.", "AI": {"tldr": "本文提出了RCStat框架，使用Raw attention logits来测量token片段之间的上下文对齐，适用于压缩和归因两个应用场景，无需模型重新训练即可达到最先进的压缩和归因性能。", "motivation": "之前的自回归transformers研究依赖于Softmax注意力权重，但无法充分反映预Softmax查询-键的丰富结构。", "method": "RCStat通过Relative Contextualization(RC)测量原始注意力logits来获得上下文对齐，通过两个应用示范框架的有效性：键值压缩和归因。", "result": "在问答、总结和归因基准测试中，RCStat取得了显著的实证收益。", "conclusion": "RCStat是基于Raw attention logits的统计框架，提出了有效且高效的策略，并在多个应用场景中取得了优于以往的方法的性能。"}}
{"id": "2506.19267", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.19267", "abs": "https://arxiv.org/abs/2506.19267", "authors": ["Weichen Zhang", "Dong Xu", "Wanli Ouyang", "Wen Li"], "title": "Self-Paced Collaborative and Adversarial Network for Unsupervised Domain Adaptation", "comment": null, "summary": "This paper proposes a new unsupervised domain adaptation approach called\nCollaborative and Adversarial Network (CAN), which uses the\ndomain-collaborative and domain-adversarial learning strategy for training the\nneural network. The domain-collaborative learning aims to learn domain-specific\nfeature representation to preserve the discriminability for the target domain,\nwhile the domain adversarial learning aims to learn domain-invariant feature\nrepresentation to reduce the domain distribution mismatch between the source\nand target domains. We show that these two learning strategies can be uniformly\nformulated as domain classifier learning with positive or negative weights on\nthe losses. We then design a collaborative and adversarial training scheme,\nwhich automatically learns domain-specific representations from lower blocks in\nCNNs through collaborative learning and domain-invariant representations from\nhigher blocks through adversarial learning. Moreover, to further enhance the\ndiscriminability in the target domain, we propose Self-Paced CAN (SPCAN), which\nprogressively selects pseudo-labeled target samples for re-training the\nclassifiers. We employ a self-paced learning strategy to select pseudo-labeled\ntarget samples in an easy-to-hard fashion. Comprehensive experiments on\ndifferent benchmark datasets, Office-31, ImageCLEF-DA, and VISDA-2017 for the\nobject recognition task, and UCF101-10 and HMDB51-10 for the video action\nrecognition task, show our newly proposed approaches achieve the\nstate-of-the-art performance, which clearly demonstrates the effectiveness of\nour proposed approaches for unsupervised domain adaptation.", "AI": {"tldr": "本文提出CAN和SPCAN方法以解决无监督领域自适应问题，通过领域协作与对抗学习，提升了物体与视频动作识别任务的性能。", "motivation": "为了提高无监督领域自适应的效果，特别是解决领域间分布不匹配的问题。", "method": "该研究提出了一种名为协作对抗网络（CAN）的新无监督领域自适应方法。CAN利用领域协作和领域对抗的学习策略来训练神经网络。领域协作学习旨在学习领域特定的特征表示以保留目标领域的判别性，而领域对抗学习旨在学习领域不变的特征表示以减少源领域和目标领域之间的分布不匹配。研究将这两种学习策略统一为带有正负权重损失的领域分类器学习。通过协作学习，CAN能够在CNN的较低层次自动学习到领域特定的表示，而通过对抗学习，则能在较高层次学习到领域不变的表示。此外，为增强目标领域的判别性，研究还提出了逐级CAN（SPCAN），逐步选择伪标签的目标样本用于再训练分类器。研究采用了逐级学习策略，按从易到难的方式选择伪标签目标样本。", "result": "实验结果表明，提出的CAN及SPCAN方法在Office-31、ImageCLEF-DA、VISDA-2017数据集上的物体识别任务与UCF101-10、HMDB51-10数据集上的视频动作识别任务中达到了最先进的性能，证明了方法的有效性。", "conclusion": "CAN和SPCAN方法对于无监督领域自适应问题展示了显著的优势，特别是在减少领域间分布不匹配和增强目标领域判别性方面。"}}
{"id": "2506.19571", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.19571", "abs": "https://arxiv.org/abs/2506.19571", "authors": ["Lorenzo Proietti", "Stefano Perrella", "Roberto Navigli"], "title": "Has Machine Translation Evaluation Achieved Human Parity? The Human Reference and the Limits of Progress", "comment": "Accepted at ACL 2025 Main Conference. 24 pages", "summary": "In Machine Translation (MT) evaluation, metric performance is assessed based\non agreement with human judgments. In recent years, automatic metrics have\ndemonstrated increasingly high levels of agreement with humans. To gain a\nclearer understanding of metric performance and establish an upper bound, we\nincorporate human baselines in the MT meta-evaluation, that is, the assessment\nof MT metrics' capabilities. Our results show that human annotators are not\nconsistently superior to automatic metrics, with state-of-the-art metrics often\nranking on par with or higher than human baselines. Despite these findings\nsuggesting human parity, we discuss several reasons for caution. Finally, we\nexplore the broader implications of our results for the research field, asking:\nCan we still reliably measure improvements in MT evaluation? With this work, we\naim to shed light on the limits of our ability to measure progress in the\nfield, fostering discussion on an issue that we believe is crucial to the\nentire MT evaluation community.", "AI": {"tldr": "本研究探讨了机器翻译评估指标与人类评估的一致性，并提出即使某些指标表现接近或优于人类基线，也需要注意一些潜在的问题。", "motivation": "为了更清晰地理解MT评估指标的表现，从而建立一个性能上限，作者将人类基线纳入到了MT评估指标的评估之中。", "method": "本研究通过引入人类基线来评估机器翻译（MT）评估指标的表现，并设定了MT评估指标能力评估的上限。", "result": "研究结果显示人类注释者并不总是比自动指标优越，最先进的指标经常与人类基线持平甚至高于人类基线。", "conclusion": "尽管研究结果表明指标与人类表现持平，但本文提出了一些谨慎的理由，并探讨该结果对MT评估研究领域的更广泛影响，导致关于MT评估中衡量进展的可靠性的讨论。"}}
{"id": "2506.19283", "categories": ["cs.CV", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2506.19283", "abs": "https://arxiv.org/abs/2506.19283", "authors": ["Xiangbo Gao", "Yuheng Wu", "Xuewen Luo", "Keshu Wu", "Xinghao Chen", "Yuping Wang", "Chenxi Liu", "Yang Zhou", "Zhengzhong Tu"], "title": "AirV2X: Unified Air-Ground Vehicle-to-Everything Collaboration", "comment": null, "summary": "While multi-vehicular collaborative driving demonstrates clear advantages\nover single-vehicle autonomy, traditional infrastructure-based V2X systems\nremain constrained by substantial deployment costs and the creation of\n\"uncovered danger zones\" in rural and suburban areas. We present\nAirV2X-Perception, a large-scale dataset that leverages Unmanned Aerial\nVehicles (UAVs) as a flexible alternative or complement to fixed Road-Side\nUnits (RSUs). Drones offer unique advantages over ground-based perception:\ncomplementary bird's-eye-views that reduce occlusions, dynamic positioning\ncapabilities that enable hovering, patrolling, and escorting navigation rules,\nand significantly lower deployment costs compared to fixed infrastructure. Our\ndataset comprises 6.73 hours of drone-assisted driving scenarios across urban,\nsuburban, and rural environments with varied weather and lighting conditions.\nThe AirV2X-Perception dataset facilitates the development and standardized\nevaluation of Vehicle-to-Drone (V2D) algorithms, addressing a critical gap in\nthe rapidly expanding field of aerial-assisted autonomous driving systems. The\ndataset and development kits are open-sourced at\nhttps://github.com/taco-group/AirV2X-Perception.", "AI": {"tldr": "本研究通过创建AirV2X-Perception数据集，利用无人机辅助驾驶弥补传统V2X系统在部署成本和覆盖范围上的局限，推动了交通工具到无人机（V2D）算法的研究和应用。", "motivation": "面对多功能车辆协同驾驶系统在单车辆自动驾驶上的优势，以及传统基于基础设施的V2X系统高昂的部署成本和在乡村及郊区形成的“暴露危险区”，本研究旨在通过无人机提供一个灵活的解决方案，从而推动自主驾驶系统的发展。", "method": "通过收集并整理6.73小时在不同环境下的无人机辅助驾驶场景，构建了一个涵盖多种天气和光照条件的大规模数据集。", "result": "此论文提出了一种名为AirV2X-Perception的大规模数据集，该数据集利用无人机作为地面道路基础设施的灵活替代或补充。无人机提供了独特的鸟瞰视角，可以减少遮挡，且具有悬停、巡逻和护送导航的能力。此数据集包括了在城市、郊区和农村环境中的6.73小时的无人机辅助驾驶场景。这项研究有助于开发和标准化评估车辆到无人机（V2D）算法，填补了空中辅助自动驾驶系统的空白。", "conclusion": "研究结果表明，利用无人机辅助的驾驶系统在减少部署成本、提高视域和驾驶安全性方面具有显著优势，同时填补了相关领域的空白，推动了相关算法的发展和标准化评估。"}}
{"id": "2506.19599", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.19599", "abs": "https://arxiv.org/abs/2506.19599", "authors": ["Zhenke Duan", "Jiqun Pan", "Jiani Tu", "Xiaoyi Wang", "Yanqing Wang"], "title": "ECCoT: A Framework for Enhancing Effective Cognition via Chain of Thought in Large Language Model", "comment": null, "summary": "In the era of large-scale artificial intelligence, Large Language Models\n(LLMs) have made significant strides in natural language processing. However,\nthey often lack transparency and generate unreliable outputs, raising concerns\nabout their interpretability. To address this, the Chain of Thought (CoT)\nprompting method structures reasoning into step-by-step deductions. Yet, not\nall reasoning chains are valid, and errors can lead to unreliable conclusions.\nWe propose ECCoT, an End-to-End Cognitive Chain of Thought Validation\nFramework, to evaluate and refine reasoning chains in LLMs. ECCoT integrates\nthe Markov Random Field-Embedded Topic Model (MRF-ETM) for topic-aware CoT\ngeneration and Causal Sentence-BERT (CSBert) for causal reasoning alignment. By\nfiltering ineffective chains using structured ordering statistics, ECCoT\nimproves interpretability, reduces biases, and enhances the trustworthiness of\nLLM-based decision-making. Key contributions include the introduction of ECCoT,\nMRF-ETM for topic-driven CoT generation, and CSBert for causal reasoning\nenhancement. Code is released at: https://github.com/erwinmsmith/ECCoT.git.", "AI": {"tldr": "本文提出ECCoT框架来评估和改进LLMs的推理链，增强其解释性并减少偏见，通过MRF-ETM和CSBert技术实现。", "motivation": "尽管LLMs在自然语言处理方面取得了显著进展，但它们缺乏透明度，生成的输出不可靠，使得解释性成为一个重要问题。研究动机是解决这一问题，提高LLMs输出的可靠性和解释性。", "method": "本研究提出了一种名为ECCoT（End-to-End Cognitive Chain of Thought Validation Framework）的端到端认知链验证框架，用以评估和优化LLMs的推理链。该框架包含两个关键组件：基于主题的推理链生成（Markov Random Field-Embedded Topic Model，MRF-ETM）和因果推理对齐（Causal Sentence-BERT，CSBert）。通过结构化顺序统计来过滤无效推理链，ECCoT改进了LLMs的可解释性，减少了偏见，并增强了基于LLMs的决策可信度。", "result": "ECCoT框架增强了LLMs在生成可靠的、有理由支持的输出时的能力，提高了决策的可信度和可解释性。此框架的实现在https://github.com/erwinmsmith/ECCoT.git可以找到。", "conclusion": "通过引入ECCoT、MRF-ETM和CSBert技术，本研究提供了提高LLMs推理链质量的有效方法，从而提高了机器学习系统的整体性能和可靠性，尤其是在自然语言处理领域。"}}
{"id": "2506.19288", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2506.19288", "abs": "https://arxiv.org/abs/2506.19288", "authors": ["Runwei Guan", "Ningwei Ouyang", "Tianhao Xu", "Shaofeng Liang", "Wei Dai", "Yafeng Sun", "Shang Gao", "Songning Lai", "Shanliang Yao", "Xuming Hu", "Ryan Wen Liu", "Yutao Yue", "Hui Xiong"], "title": "Da Yu: Towards USV-Based Image Captioning for Waterway Surveillance and Scene Understanding", "comment": "14 pages, 13 figures", "summary": "Automated waterway environment perception is crucial for enabling unmanned\nsurface vessels (USVs) to understand their surroundings and make informed\ndecisions. Most existing waterway perception models primarily focus on\ninstance-level object perception paradigms (e.g., detection, segmentation).\nHowever, due to the complexity of waterway environments, current perception\ndatasets and models fail to achieve global semantic understanding of waterways,\nlimiting large-scale monitoring and structured log generation. With the\nadvancement of vision-language models (VLMs), we leverage image captioning to\nintroduce WaterCaption, the first captioning dataset specifically designed for\nwaterway environments. WaterCaption focuses on fine-grained, multi-region\nlong-text descriptions, providing a new research direction for visual\ngeo-understanding and spatial scene cognition. Exactly, it includes 20.2k\nimage-text pair data with 1.8 million vocabulary size. Additionally, we propose\nDa Yu, an edge-deployable multi-modal large language model for USVs, where we\npropose a novel vision-to-language projector called Nano Transformer Adaptor\n(NTA). NTA effectively balances computational efficiency with the capacity for\nboth global and fine-grained local modeling of visual features, thereby\nsignificantly enhancing the model's ability to generate long-form textual\noutputs. Da Yu achieves an optimal balance between performance and efficiency,\nsurpassing state-of-the-art models on WaterCaption and several other captioning\nbenchmarks.", "AI": {"tldr": "研究开发了WaterCaption数据集和Da Yu模型，旨在提高无人水面车辆对复杂水道环境的理解能力。Da Yu模型通过Nano Transformer Adaptor（NTA）实现了高效且精准的视觉到语言转换，生成长文本描述。", "motivation": "目前水道感知模型主要集中在实例级对象感知范式上，而忽视了全局语义理解，限制了大规模监控和结构化日志的生成。此研究旨在通过引入新的数据集WaterCaption和模型Da Yu，推动水道环境认知领域的发展。", "method": "本研究利用视觉-语言模型（VLMs）的优势，提出了WaterCaption数据集，专注于细化、多区域长文本描述，旨在改进水域环境的全局语义理解。此外，提出了一种边缘可部署的多模态大型语言模型Da Yu，其中包含了一种名为Nano Transformer Adaptor (NTA) 的新视觉到语言投射器，用以平衡计算效率与视觉特征的全局和局部建模能力。", "result": "Da Yu在WaterCaption数据集以及若干其他描述基准上表现优于现有先进技术，证明了其强大的视觉到语言转换能力。", "conclusion": "本研究表明，通过利用视觉-语言模型，可以实现对复杂水道环境的高级理解和自动描述，这为无人水面车辆的自主感知提供了新的研究方向。"}}
{"id": "2506.19603", "categories": ["cs.CL", "cs.SI"], "pdf": "https://arxiv.org/pdf/2506.19603", "abs": "https://arxiv.org/abs/2506.19603", "authors": ["Tom Marzea", "Abraham Israeli", "Oren Tsur"], "title": "Social Hatred: Efficient Multimodal Detection of Hatemongers", "comment": "To be published in WOAH, July 2025. arXiv admin note: text overlap\n  with arXiv:2409.14464", "summary": "Automatic detection of online hate speech serves as a crucial step in the\ndetoxification of the online discourse. Moreover, accurate classification can\npromote a better understanding of the proliferation of hate as a social\nphenomenon. While most prior work focus on the detection of hateful utterances,\nwe argue that focusing on the user level is as important, albeit challenging.\nIn this paper we consider a multimodal aggregative approach for the detection\nof hate-mongers, taking into account the potentially hateful texts, user\nactivity, and the user network. Evaluating our method on three unique datasets\nX (Twitter), Gab, and Parler we show that processing a user's texts in her\nsocial context significantly improves the detection of hate mongers, compared\nto previously used text and graph-based methods. We offer comprehensive set of\nresults obtained in different experimental settings as well as qualitative\nanalysis of illustrative cases. Our method can be used to improve the\nclassification of coded messages, dog-whistling, and racial gas-lighting, as\nwell as to inform intervention measures. Moreover, we demonstrate that our\nmultimodal approach performs well across very different content platforms and\nover large datasets and networks.", "AI": {"tldr": "本文提出了一种多模态聚合方法来检测传播仇恨的用户，该方法考虑了潜在的仇恨文本、用户活动和用户网络。通过在Twitter、Gab和Parler三个数据集上的评估，结果表明，与之前的方法相比，我们的方法在检测仇恨传播者方面更有效。", "motivation": "大多数先前的工作集中在检测仇恨言论上，而本文则认为从用户层面进行检测同样重要，且更具挑战性。准确地分类用户可以帮助我们更好地理解仇恨言论的社会现象。", "method": "本文提出的方法是一种多模态聚合法，考虑了潜在的仇恨文本、用户活动和用户网络三个方面的信息。", "result": "本文在Twitter、Gab和Parler三个数据集上对方法进行了评估。结果显示，与之前使用的基于文本和基于图的方法相比，本文的方法显著提高了检测仇恨传播者的性能。", "conclusion": "本文的方法可用于改善被编码信息、吹口哨行为和种族灯光语言的分类，也可用于告知干预措施。此外，本文的方法在不同类型的内容平台上表现良好。"}}
{"id": "2506.19291", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.19291", "abs": "https://arxiv.org/abs/2506.19291", "authors": ["Xiaoyuan Wang", "Yizhou Zhao", "Botao Ye", "Xiaojun Shan", "Weijie Lyu", "Lu Qi", "Kelvin C. K. Chan", "Yinxiao Li", "Ming-Hsuan Yang"], "title": "HoliGS: Holistic Gaussian Splatting for Embodied View Synthesis", "comment": null, "summary": "We propose HoliGS, a novel deformable Gaussian splatting framework that\naddresses embodied view synthesis from long monocular RGB videos. Unlike prior\n4D Gaussian splatting and dynamic NeRF pipelines, which struggle with training\noverhead in minute-long captures, our method leverages invertible Gaussian\nSplatting deformation networks to reconstruct large-scale, dynamic environments\naccurately. Specifically, we decompose each scene into a static background plus\ntime-varying objects, each represented by learned Gaussian primitives\nundergoing global rigid transformations, skeleton-driven articulation, and\nsubtle non-rigid deformations via an invertible neural flow. This hierarchical\nwarping strategy enables robust free-viewpoint novel-view rendering from\nvarious embodied camera trajectories by attaching Gaussians to a complete\ncanonical foreground shape (\\eg, egocentric or third-person follow), which may\ninvolve substantial viewpoint changes and interactions between multiple actors.\nOur experiments demonstrate that \\ourmethod~ achieves superior reconstruction\nquality on challenging datasets while significantly reducing both training and\nrendering time compared to state-of-the-art monocular deformable NeRFs. These\nresults highlight a practical and scalable solution for EVS in real-world\nscenarios. The source code will be released.", "AI": {"tldr": "HoliGS是一种新的可变形高斯散点框架，能有效地从长单目RGB视频中进行实体视图合成，解决了现有方法在长时间捕捉中的训练负担问题。", "motivation": "为了从长时间的单目RGB视频中解决实体视图合成问题，HoliGS旨在减少训练负担，并准确地重建大规模动态环境。", "method": "HoliGS采用可变形的高斯散点框架，通过逆高斯散点变形网络，将场景分解为静态背景和随时间变化的对象，每个对象由学习到的高斯原语表示，这些原语可以进行全局刚性变换、骨架驱动的变形和细微的非刚性变形。", "result": "实验表明，与最先进的一元可变形NeRF相比，HoliGS在具有挑战性的数据集上实现了优越的重建质量，并显著减少了训练和渲染时间。", "conclusion": "HoliGS 提供了一种在实际场景中进行EVS的实用且可扩展的解决方案，通过显著降低训练和渲染时间，实现了高质量的重建。"}}
{"id": "2506.19607", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.19607", "abs": "https://arxiv.org/abs/2506.19607", "authors": ["Juraj Vladika", "Ihsan Soydemir", "Florian Matthes"], "title": "Correcting Hallucinations in News Summaries: Exploration of Self-Correcting LLM Methods with External Knowledge", "comment": "Accepted to FEVER @ ACL 2025", "summary": "While large language models (LLMs) have shown remarkable capabilities to\ngenerate coherent text, they suffer from the issue of hallucinations --\nfactually inaccurate statements. Among numerous approaches to tackle\nhallucinations, especially promising are the self-correcting methods. They\nleverage the multi-turn nature of LLMs to iteratively generate verification\nquestions inquiring additional evidence, answer them with internal or external\nknowledge, and use that to refine the original response with the new\ncorrections. These methods have been explored for encyclopedic generation, but\nless so for domains like news summarization. In this work, we investigate two\nstate-of-the-art self-correcting systems by applying them to correct\nhallucinated summaries using evidence from three search engines. We analyze the\nresults and provide insights into systems' performance, revealing interesting\npractical findings on the benefits of search engine snippets and few-shot\nprompts, as well as high alignment of G-Eval and human evaluation.", "AI": {"tldr": "研究採用了兩種最先進的自我修正方法來修正幻覺化的新聞摘要，通過利用多種搜尋引擎來進行補充與校正，并揭示了搜尋引擎摘要和少量示例提示的益處。", "motivation": "大型語言模型在生成連貫文本方面展示出強大能力，但事實上不準確的陳述問題仍然存在。自我修正方法被視為一種很有前景的解決幻覺問題的方案，使用該方法的先前研究主要集中在百科全書式內容生成，對於新領域如新聞摘要的應用則相對較少。", "method": "本研究採用了兩種最先進的自我修正系統，通過使用三種搜尋引擎提供的證據來修正幻覺化的摘要。研究設計利用了LLMs的多輪特性來迭代生成驗證問題，並利用內外部知識來回答這些問題，進而修訂原始回應以加以糾正。", "result": "研究結果揭示了各種背景下系統表現的情況，特別是在搜索引擎片段和少量提示調用的重要性以及G-Eval與人類評估之間的高度一致性上。", "conclusion": "研究強調了將自我修正技術應用於幻覺文本問題，特別是對於新聞摘要的新領域，並且提供了一些關於搜索引擎技術和提示設計方面的實用見解。"}}
{"id": "2506.19300", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.19300", "abs": "https://arxiv.org/abs/2506.19300", "authors": ["Kai Zhao", "Wubang Yuan", "Zheng Wang", "Guanyi Li", "Xiaoqiang Zhu", "Deng-ping Fan", "Dan Zeng"], "title": "Open-Vocabulary Camouflaged Object Segmentation with Cascaded Vision Language Models", "comment": null, "summary": "Open-Vocabulary Camouflaged Object Segmentation (OVCOS) seeks to segment and\nclassify camouflaged objects from arbitrary categories, presenting unique\nchallenges due to visual ambiguity and unseen categories.Recent approaches\ntypically adopt a two-stage paradigm: first segmenting objects, then\nclassifying the segmented regions using Vision Language Models (VLMs).However,\nthese methods (1) suffer from a domain gap caused by the mismatch between VLMs'\nfull-image training and cropped-region inference, and (2) depend on generic\nsegmentation models optimized for well-delineated objects, making them less\neffective for camouflaged objects.Without explicit guidance, generic\nsegmentation models often overlook subtle boundaries, leading to imprecise\nsegmentation.In this paper,we introduce a novel VLM-guided cascaded framework\nto address these issues in OVCOS.For segmentation, we leverage the Segment\nAnything Model (SAM), guided by the VLM.Our framework uses VLM-derived features\nas explicit prompts to SAM, effectively directing attention to camouflaged\nregions and significantly improving localization accuracy.For classification,\nwe avoid the domain gap introduced by hard cropping.Instead, we treat the\nsegmentation output as a soft spatial prior via the alpha channel, which\nretains the full image context while providing precise spatial guidance,\nleading to more accurate and context-aware classification of camouflaged\nobjects.The same VLM is shared across both segmentation and classification to\nensure efficiency and semantic consistency.Extensive experiments on both OVCOS\nand conventional camouflaged object segmentation benchmarks demonstrate the\nclear superiority of our method, highlighting the effectiveness of leveraging\nrich VLM semantics for both segmentation and classification of camouflaged\nobjects.", "AI": {"tldr": "提出了一种VLM引导的级联框架来改进OVCOS，通过显式提示SAM和软空间先验来提高分割和分类精度。", "motivation": "当前OVCOS方法存在领域差距和通用分割模型不适应隐蔽物体的问题，这导致分割边界不准确。本文旨在通过引入VLM引导的方法来改进这些问题，提高对隐蔽物体的分割和分类效果。", "method": "本研究提出了一种创新的VLM引导级联框架，用于解决开放词汇隐藏物体分割（OVCOS）中的问题。该框架利用Segment Anything Model (SAM)，并通过VLM提取的特征作为显式提示来引导SAM，以提高对隐蔽区域的定位精度。在分类阶段，避免了由于硬裁剪引入的领域差距，而是通过将分割输出视为软空间先验（通过alpha通道）来保留完整的图像上下文，从而实现更准确的分类。", "result": "实验结果表明，该方法在OVCOS和传统的隐蔽物体分割基准测试上均表现出优越性，尤其是在利用丰富VLM语义信息进行分割和分类方面。", "conclusion": "通过实验证明，所提出的方法在处理隐蔽物体时提供了显著的分割和分类性能，证实了在OVCOS中利用VLM语义信息的有效性。"}}
{"id": "2506.19652", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.19652", "abs": "https://arxiv.org/abs/2506.19652", "authors": ["Lucie Galland", "Catherine Pelachaud", "Florian Pecune"], "title": "Tailored Conversations beyond LLMs: A RL-Based Dialogue Manager", "comment": null, "summary": "In this work, we propose a novel framework that integrates large language\nmodels (LLMs) with an RL-based dialogue manager for open-ended dialogue with a\nspecific goal. By leveraging hierarchical reinforcement learning to model the\nstructured phases of dialogue and employ meta-learning to enhance adaptability\nacross diverse user profiles, our approach enhances adaptability and\nefficiency, enabling the system to learn from limited data, transition fluidly\nbetween dialogue phases, and personalize responses to heterogeneous patient\nneeds. We apply our framework to Motivational Interviews, aiming to foster\nbehavior change, and demonstrate that the proposed dialogue manager outperforms\na state-of-the-art LLM baseline in terms of reward, showing a potential benefit\nof conditioning LLMs to create open-ended dialogue systems with specific goals.", "AI": {"tldr": "研究提出了一种结合LLMs与RL的对话管理系统，专注于提高开放式对话的效率与个性化适应性，特别是针对动机性访谈场景中患者需求的个性化响应，实验证明其在奖励方面优于现有一流的LLM模型。", "motivation": "这项工作的动机在于开发一个能够适应多样化用户的开放式对话系统，特别适用于需要特定目标的场景，如鼓励行为改变的动机性访谈。", "method": "本研究提出了一种结合大型语言模型（LLMs）和基于强化学习（RL）的对话管理框架，用于具有特定目标的开放式对话。通过使用分层强化学习来模拟对话的结构性阶段，并运用元学习来提高对不同用户配置文件的适应性，该方法提高了适应性和效率，使得系统可以基于有限的数据学习，并在对话阶段间流畅转换，针对不同的患者需求个性化响应。", "result": "研究将其框架应用于动机性访谈，通过实验证明，提出的对话管理器在奖励指标上超过了最先进的LLM基线，展示了为特定目标调整LLM创建开放式对话系统的潜在优势。", "conclusion": "通过将大型语言模型与适当的对话管理策略结合，可以有效改善开放式对话系统的性能，特别是在需要根据特定目标调整对话以满足不同用户需求的场景中，该方法显示出优于当前最佳模型的潜力。"}}
{"id": "2506.19306", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.19306", "abs": "https://arxiv.org/abs/2506.19306", "authors": ["Jean-Paul Ainam", "Rahul", "Lora Cavuoto", "Matthew Hackett", "Jack Norfleet", "Suvranu De"], "title": "Airway Skill Assessment with Spatiotemporal Attention Mechanisms Using Human Gaze", "comment": "13 pages, 6 figures, 14 equations,", "summary": "Airway management skills are critical in emergency medicine and are typically\nassessed through subjective evaluation, often failing to gauge competency in\nreal-world scenarios. This paper proposes a machine learning-based approach for\nassessing airway skills, specifically endotracheal intubation (ETI), using\nhuman gaze data and video recordings. The proposed system leverages an\nattention mechanism guided by the human gaze to enhance the recognition of\nsuccessful and unsuccessful ETI procedures. Visual masks were created from gaze\npoints to guide the model in focusing on task-relevant areas, reducing\nirrelevant features. An autoencoder network extracts features from the videos,\nwhile an attention module generates attention from the visual masks, and a\nclassifier outputs a classification score. This method, the first to use human\ngaze for ETI, demonstrates improved accuracy and efficiency over traditional\nmethods. The integration of human gaze data not only enhances model performance\nbut also offers a robust, objective assessment tool for clinical skills,\nparticularly in high-stress environments such as military settings. The results\nshow improvements in prediction accuracy, sensitivity, and trustworthiness,\nhighlighting the potential for this approach to improve clinical training and\npatient outcomes in emergency medicine.", "AI": {"tldr": "研究提出了一种通过眼动数据与视频记录结合，基于机器学习方法评估ETI技能的系统，该系统提高了预测准确性、灵敏度及可信度，并验证了其在临床培训和提升病人结局上的潜力。", "motivation": "气道管理的技能在急诊医学中至关重要，但其通常通过主观评估来进行，这往往无法准确衡量在实际操作中的胜任程度。本研究旨在针对这一问题，提出一种客观且高效的评估方法。", "method": "本研究提出了一种基于机器学习的方法来评估气道管理技能，特别是经口/鼻气管插管(ETI)技能。该方法利用了人体眼动数据和视频记录，通过眼动数据引导的注意力机制来提高成功与不成功ETI程序的识别。视觉掩码由眼动点创建，以引导模型专注于任务相关的区域，减少不相关特征。使用自动编码器网络从视频中提取特征，注意力模块生成自视觉掩码的注意力，分类器输出分类得分。", "result": "研究结果表明，该方法在预测准确性、灵敏度和可信度方面都有所提高，表现出比传统方法更高的准确性和效率。", "conclusion": "该研究是一种创新的方法，首次结合了人体眼动数据用于ETI评估，能有效提高模型性能，提供一种强大的客观评估临床技能工具，特别是在高强度环境下如军队应用，具有显著的应用潜力。"}}
{"id": "2506.19733", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.19733", "abs": "https://arxiv.org/abs/2506.19733", "authors": ["Chuxuan Hu", "Yuxuan Zhu", "Antony Kellermann", "Caleb Biddulph", "Suppakit Waiwitlikhit", "Jason Benn", "Daniel Kang"], "title": "Breaking Barriers: Do Reinforcement Post Training Gains Transfer To Unseen Domains?", "comment": "9 pages, 4 figures, 2 tables", "summary": "Reinforcement post training (RPT) has recently shown promise in improving the\nreasoning abilities of large language models (LLMs). However, it remains\nunclear how well these improvements generalize to new domains, as prior work\nevaluates RPT models on data from the same domains used for fine-tuning. To\nunderstand the generalizability of RPT, we conduct two studies. (1)\nObservational: We compare a wide range of open-weight RPT models against their\ncorresponding base models across multiple domains, including both seen and\nunseen domains in their fine-tuning data. (2) Interventional: we fine-tune LLMs\nwith RPT on single domains and evaluate their performance across multiple\ndomains. Both studies converge on the same conclusion that, although RPT brings\nsubstantial gains on tasks similar to the fine-tuning data, the gains\ngeneralize inconsistently and can vanish on domains with different reasoning\npatterns.", "AI": {"tldr": "研究发现，强化后训练在相似任务上表现优异，但在推理模式不同的领域中泛化效果有限。", "motivation": "研究RPT在不同领域的泛化能力，因为先前的研究仅在同样用于微调的数据集上评估了RPT模型。", "method": "采用两种方法研究强化后训练（RPT）在不同领域的泛化能力。第一种是观察性研究，比较了多种开放权重的RPT模型与其基准模型在见过和未见过领域的表现。第二种是干预性研究，对RPT模型在单个领域进行微调，并评估其在多个领域的性能。", "result": "研究表明，尽管RPT在与微调数据相似的任务中带来了显著的提升，但这种提升在具有不同推理模式的领域中的泛化效果并不一致，甚至可能消失。", "conclusion": "RPT对于相似任务的提升显著，但在不同推理模式的领域中的泛化能力不强。"}}
{"id": "2506.19312", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.19312", "abs": "https://arxiv.org/abs/2506.19312", "authors": ["Junsei Tokumitsu", "Yuiga Wada"], "title": "Capturing Fine-Grained Alignments Improves 3D Affordance Detection", "comment": "MVA 2025 (Oral)", "summary": "In this work, we address the challenge of affordance detection in 3D point\nclouds, a task that requires effectively capturing fine-grained alignments\nbetween point clouds and text. Existing methods often struggle to model such\nalignments, resulting in limited performance on standard benchmarks. A key\nlimitation of these approaches is their reliance on simple cosine similarity\nbetween point cloud and text embeddings, which lacks the expressiveness needed\nfor fine-grained reasoning. To address this limitation, we propose LM-AD, a\nnovel method for affordance detection in 3D point clouds. Moreover, we\nintroduce the Affordance Query Module (AQM), which efficiently captures\nfine-grained alignment between point clouds and text by leveraging a pretrained\nlanguage model. We demonstrated that our method outperformed existing\napproaches in terms of accuracy and mean Intersection over Union on the 3D\nAffordanceNet dataset.", "AI": {"tldr": "该研究提出LM-AD方法，用于解决在3D点云中检测可用性的问题，这需要准确捕捉点云与文本之间精细对齐。通过引入Affordance Query Module (AQM)，该方法利用预训练语言模型更有效地捕捉对齐，提高了在3D AffordanceNet数据集上的准确率和平均交并比。", "motivation": "现有的方法往往依赖于简单的余弦相似性来建模3D点云和文本之间的对齐，这种方法缺乏精细推理所需的表达能力，限制了它们在标准基准测试中的表现。该研究旨在解决这些问题。", "method": "提出了LM-AD方法和Affordance Query Module (AQM)，通过利用预训练的语言模型，更有效地捕捉点云和文本之间的精细对齐。", "result": "在3D AffordanceNet数据集上，该方法在准确率和平均交并比方面超越了现有的方法。", "conclusion": "研究结果表明，LM-AD和AQM方法能够改善3D点云中可用性检测任务的性能。"}}
{"id": "2506.19750", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.19750", "abs": "https://arxiv.org/abs/2506.19750", "authors": ["Takashi Nishibayashi", "Seiji Kanazawa", "Kumpei Yamada"], "title": "Evaluating Rare Disease Diagnostic Performance in Symptom Checkers: A Synthetic Vignette Simulation Approach", "comment": null, "summary": "Background: Symptom Checkers (SCs) provide users with personalized medical\ninformation. To prevent performance degradation from algorithm updates, SC\ndevelopers must evaluate diagnostic performance changes for individual diseases\nbefore deployment. However, acquiring sufficient evaluation data for rare\ndiseases is difficult, and manually creating numerous clinical vignettes is\ncostly and impractical. Objective: This study proposes and validates a novel\nSynthetic Vignette Simulation Approach to evaluate diagnostic performance\nchanges for individual rare diseases following SC algorithm updates. Methods:\nWe used disease-phenotype annotations from the Human Phenotype Ontology (HPO),\na knowledge database for rare diseases, to generate synthetic vignettes. With\nthese, we simulated SC interviews to estimate the impact of algorithm updates\non real-world diagnostic performance. The method's effectiveness was evaluated\nretrospectively by comparing estimated values with actual metric changes using\nthe R 2(R-squared) coefficient. Results: The experiment included eight past SC\nalgorithm updates. For updates on diseases with frequency information in HPO\n(n=5), the R^2 for recall@8 change was 0.831 (p=0.031), and for precision@8\nchange, it was 0.78 (p=0.047), indicating the method can predict\npost-deployment performance. In contrast, large prediction errors occurred for\ndiseases without frequency information (n=3), highlighting its importance. The\nmanual effort to map HPO phenotypes to SC symptoms was approximately 2 hours\nper disease. Conclusions: Our method enables pre-deployment evaluation of SC\nalgorithm changes for individual rare diseases using a publicly available,\nexpert-created knowledge base. This transparent and low-cost approach allows\ndevelopers to efficiently improve diagnostic performance for rare diseases,\npotentially enhancing support for early diagnosis.", "AI": {"tldr": "研究提出一种使用HPO生成合成案例来评估SC算法更新对罕见疾病诊断性能影响的方法，并验证了其有效性。", "motivation": "由于获取罕见疾病的评估数据困难且成本高昂，本研究目的验证一种合成案例模拟方法，以在SC算法更新后评估罕见疾病的诊断性能变化。", "method": "此研究提出使用疾病表型注释从人类表型本体(HPO)中生成合成临床案例来评估症状检查器(SC)算法更新对罕见疾病诊断性能的影响。", "result": "实验包括了八个SC算法更新。对于在HPO中有频率信息的疾病，回忆率（@8）变化的R^2值为0.831（p=0.031），精度（@8）变化的R^2值为0.78（p=0.047）。但对于没有频率信息的疾病，预测误差较大。", "conclusion": "研究提出的方法可以使用公开可用、专家创建的知识库，在罕见疾病上实现算法更新前的诊断性能评估，这提供了一种透明且低成本的途径增强早期诊断的支持。"}}
{"id": "2506.19316", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.19316", "abs": "https://arxiv.org/abs/2506.19316", "authors": ["Weichen Zhang", "Dong Xu", "Jing Zhang", "Wanli Ouyang"], "title": "Progressive Modality Cooperation for Multi-Modality Domain Adaptation", "comment": null, "summary": "In this work, we propose a new generic multi-modality domain adaptation\nframework called Progressive Modality Cooperation (PMC) to transfer the\nknowledge learned from the source domain to the target domain by exploiting\nmultiple modality clues (\\eg, RGB and depth) under the multi-modality domain\nadaptation (MMDA) and the more general multi-modality domain adaptation using\nprivileged information (MMDA-PI) settings. Under the MMDA setting, the samples\nin both domains have all the modalities. In two newly proposed modules of our\nPMC, the multiple modalities are cooperated for selecting the reliable\npseudo-labeled target samples, which captures the modality-specific information\nand modality-integrated information, respectively. Under the MMDA-PI setting,\nsome modalities are missing in the target domain. Hence, to better exploit the\nmulti-modality data in the source domain, we further propose the PMC with\nprivileged information (PMC-PI) method by proposing a new multi-modality data\ngeneration (MMG) network. MMG generates the missing modalities in the target\ndomain based on the source domain data by considering both domain distribution\nmismatch and semantics preservation, which are respectively achieved by using\nadversarial learning and conditioning on weighted pseudo semantics. Extensive\nexperiments on three image datasets and eight video datasets for various\nmulti-modality cross-domain visual recognition tasks under both MMDA and\nMMDA-PI settings clearly demonstrate the effectiveness of our proposed PMC\nframework.", "AI": {"tldr": "A multi-modality domain adaptation framework (PMC) is proposed to improve cross-domain visual recognition tasks, showing effectiveness in both conventional MMDA and MMDA-PI settings with extensive dataset experiments.", "motivation": "The motivation is to improve knowledge transfer between domains by exploiting multi-modality clues effectively under different adaptation scenarios.", "method": "The paper proposes a Progressive Modality Cooperation (PMC) framework for multi-modality domain adaptation (MMDA) and multi-modality domain adaptation using privileged information (MMDA-PI) settings. In MMDA, PMC utilizes two modules that work with multiple modalities to select reliable pseudo-labeled target samples. In MMDA-PI, PMC-PI is introduced using a Multi-Modality Data Generation (MMG) network to generate missing modalities in the target domain based on source domain data, considering domain distribution mismatch and semantics preservation, the latter achieved through adversarial learning and weighted pseudo semantics conditioning.", "result": "Experiments on image and video datasets show PMC's effectiveness across various multi-modality cross-domain visual recognition tasks under both MMDA and MMDA-PI settings.", "conclusion": "The PMC framework effectively addresses the challenges of multi-modality domain adaptation, providing a robust method for cross-domain visual recognition tasks with multiple modalities, even when some modalities are missing in the target domain."}}
{"id": "2506.19753", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.19753", "abs": "https://arxiv.org/abs/2506.19753", "authors": ["Omar A. Essameldin", "Ali O. Elbeih", "Wael H. Gomaa", "Wael F. Elsersy"], "title": "Arabic Dialect Classification using RNNs, Transformers, and Large Language Models: A Comparative Analysis", "comment": null, "summary": "The Arabic language is among the most popular languages in the world with a\nhuge variety of dialects spoken in 22 countries. In this study, we address the\nproblem of classifying 18 Arabic dialects of the QADI dataset of Arabic tweets.\nRNN models, Transformer models, and large language models (LLMs) via prompt\nengineering are created and tested. Among these, MARBERTv2 performed best with\n65% accuracy and 64% F1-score. Through the use of state-of-the-art\npreprocessing techniques and the latest NLP models, this paper identifies the\nmost significant linguistic issues in Arabic dialect identification. The\nresults corroborate applications like personalized chatbots that respond in\nusers' dialects, social media monitoring, and greater accessibility for Arabic\ncommunities.", "AI": {"tldr": "本研究使用RNN、Transformer和大规模语言模型对18种阿拉伯语方言进行分类，其中MARBERTv2表现最佳，准确率为65%，F1得分为64%。该研究对社交媒体监控和个人化方言聊天机器人等领域有潜在应用。", "motivation": "鉴于阿拉伯语是世界上使用最广泛的语言之一，拥有在22个国家中使用的大量方言，该研究旨在解决对这些方言进行分类的问题。", "method": "本研究通过创建并测试了RNN模型、Transformer模型以及通过提示工程的大规模语言模型(LLMs)来解决QADI数据集中18种阿拉伯语方言的分类问题。", "result": "研究结果表明，MARBERTv2模型表现最佳，准确率为65%，F1得分为64%。", "conclusion": "本研究表明了利用先进的预处理技术和最新的NLP模型来区分阿拉伯语方言的重要性，并且该研究对开发针对特定方言的定制聊天机器人、社交媒体监控以及提高阿拉伯社群的可达性等方面有潜在的应用价值。"}}
{"id": "2506.19320", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.19320", "abs": "https://arxiv.org/abs/2506.19320", "authors": ["Yuang Yao", "Ruiqi Wu", "Yi Zhou", "Tao Zhou"], "title": "Continual Retinal Vision-Language Pre-training upon Incremental Imaging Modalities", "comment": "Accepted by MICCAI 2025", "summary": "Traditional fundus image analysis models focus on single-modal tasks,\nignoring fundus modality complementarity, which limits their versatility.\nRecently, retinal foundation models have emerged, but most still remain\nmodality-specific. Integrating multiple fundus imaging modalities into a single\nfoundation model is valuable. However, in dynamic environments, data from\ndifferent modalities often arrive incrementally, necessitating continual\npre-training. To address this, we propose RetCoP, the first continual\nvision-language pre-training framework in the fundus domain, which\nincrementally integrates image and text features from different imaging\nmodalities into a single unified foundation model. To mitigate catastrophic\nforgetting in continual pre-training, we introduce a rehearsal strategy\nutilizing representative image-text pairs and an off-diagonal information\ndistillation approach. The former allows the model to revisit knowledge from\nprevious stages, while the latter explicitly preserves the alignment between\nimage and text representations. Experiments show that RetCoP outperforms all\nthe compared methods, achieving the best generalization and lowest forgetting\nrate. The code can be found at https://github.com/Yuang-Yao/RetCoP.", "AI": {"tldr": "提出RetCoP框架，实现眼底多模态信息的持续学习与整合，解决模型遗忘问题，实验验证其优越性能。", "motivation": "传统的基金图像分析模型专注于单模态任务，忽视了基金图模态互补性，这限制了它们的多功能性。在动态环境中，不同模态的数据往往逐步到来，需要持续的预训练。", "method": "提出RetCoP，首个眼底领域的持续视觉语言预训练框架，该框架可以增量地将来自不同成像模式的图像和文本特征整合到单一基础模型中。为减轻持续预训练中的灾难性遗忘，采用了代表性的图像-文本对作为复习策略，并引入了非对角信息蒸馏方法。前者允许模型回顾之前阶段的知识，后者则明确保持了图像与文本表示之间的对齐。", "result": "实验表明，RetCoP优于所有对比方法，实现了最佳泛化能力，且忘记率最低。", "conclusion": "提出的方法在实验中表现良好，展示出在持续预训练下保持高性能和低遗忘率的能力，证明了RetCoP在眼底图像分析领域的潜力和效果。"}}
{"id": "2506.19761", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.19761", "abs": "https://arxiv.org/abs/2506.19761", "authors": ["Martin Ratajczak", "Jean-Philippe Robichaud", "Jennifer Drexler Fox"], "title": "Accurate, fast, cheap: Choose three. Replacing Multi-Head-Attention with Bidirectional Recurrent Attention for Long-Form ASR", "comment": "Accepted to Interspeech 2025", "summary": "Long-form speech recognition is an application area of increasing research\nfocus. ASR models based on multi-head attention (MHA) are ill-suited to\nlong-form ASR because of their quadratic complexity in sequence length. We\nbuild on recent work that has investigated linear complexity recurrent\nattention (RA) layers for ASR. We find that bidirectional RA layers can match\nthe accuracy of MHA for both short- and long-form applications. We present a\nstrong limited-context attention (LCA) baseline, and show that RA layers are\njust as accurate while being more efficient. We develop a long-form training\nparadigm which further improves RA performance, leading to better accuracy than\nLCA with 44% higher throughput. We also present Direction Dropout, a novel\nregularization method that improves accuracy, provides fine-grained control of\nthe accuracy/throughput trade-off of bidirectional RA, and enables a new\nalternating directions decoding mode with even higher throughput.", "AI": {"tldr": "该论文研究了长文本语音识别问题，通过使用线性复杂度递归注意力层（RA），取得了比多头注意力模型更高的效率和相当的准确率。此外，提出了一种新的训练范式和Direction Dropout正则化方法，进一步提升了模型性能和吞吐量。", "motivation": "多头注意力（MHA）模型在长文本语音识别应用中存在二次复杂度问题，效率低下。因此，该论文致力于寻找更高效且准确的解决方案。", "method": "构建了双向递归注意力（RA）层，并提出了一种新的训练范式，此外还提出了一个新的正则化方法：Direction Dropout。", "result": "双向RA层的表现优于限制上下文注意力（LCA），提高了44%的吞吐量，并通过Direction Dropout进一步优化了准确率和吞吐量之间的平衡。", "conclusion": "RA层展现出比MHA更高的效率和不亚于MHA的准确率，尤其是在长文本语音识别任务上。结合Direction Dropout使其性能更加优越。"}}
{"id": "2506.19324", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.19324", "abs": "https://arxiv.org/abs/2506.19324", "authors": ["Mingcheng Qu", "Guang Yang", "Donglin Di", "Yue Gao", "Tonghua Su", "Yang Song", "Lei Fan"], "title": "Memory-Augmented Incomplete Multimodal Survival Prediction via Cross-Slide and Gene-Attentive Hypergraph Learning", "comment": "accepted by MICCAI2025 code: https://github.com/MCPathology/M2Surv", "summary": "Multimodal pathology-genomic analysis is critical for cancer survival\nprediction. However, existing approaches predominantly integrate formalin-fixed\nparaffin-embedded (FFPE) slides with genomic data, while neglecting the\navailability of other preservation slides, such as Fresh Froze (FF) slides.\nMoreover, as the high-resolution spatial nature of pathology data tends to\ndominate the cross-modality fusion process, it hinders effective multimodal\nfusion and leads to modality imbalance challenges between pathology and\ngenomics. These methods also typically require complete data modalities,\nlimiting their clinical applicability with incomplete modalities, such as\nmissing either pathology or genomic data. In this paper, we propose a\nmultimodal survival prediction framework that leverages hypergraph learning to\neffectively integrate multi-WSI information and cross-modality interactions\nbetween pathology slides and genomics data while addressing modality imbalance.\nIn addition, we introduce a memory mechanism that stores previously learned\npaired pathology-genomic features and dynamically compensates for incomplete\nmodalities. Experiments on five TCGA datasets demonstrate that our model\noutperforms advanced methods by over 2.3% in C-Index. Under incomplete modality\nscenarios, our approach surpasses pathology-only (3.3%) and gene-only models\n(7.9%). Code: https://github.com/MCPathology/M2Surv", "AI": {"tldr": "该论文提出了一种利用超图学习整合多张全视野数字病理图像信息和病理切片与基因组数据之间跨模态交互的多模态生存预测框架，并引入记忆机制以应对数据模态不完整的问题。", "motivation": "现有的方法主要结合福尔马林固定石蜡包埋切片和基因组数据进行分析，忽略了新鲜冷冻切片，并且由于高分辨率空间性质路径学数据在跨模态融合过程中的主导地位，导致模态不平衡。", "method": "提出了一种基于超图学习的多模态生存预测框架，并引入记忆机制来存储之前学习的配对病理基因组特征，以补偿不完整的模态。", "result": "在五个TCGA数据集上的实验表明，他们的模型在C-Index指标上比先进方法高出至少2.3%。在模态不完全的情况下，他们的方法分别比单独病理模型和基因模型高出3.3%和7.9%。", "conclusion": "所提出的方法有效地解决了模态不平衡问题，并且在同一数据缺失的情况下，显示了比单一模态更高的生存预测准确性。"}}
