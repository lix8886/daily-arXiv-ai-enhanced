{"id": "2511.08589", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.08589", "abs": "https://arxiv.org/abs/2511.08589", "authors": ["Violet B", "John M. Conroy", "Sean Lynch", "Danielle M", "Neil P. Molino", "Aaron Wiechmann", "Julia S. Yang"], "title": "Where did you get that? Towards Summarization Attribution for Analysts", "comment": null, "summary": "Analysts require attribution, as nothing can be reported without knowing the source of the information. In this paper, we will focus on automatic methods for attribution, linking each sentence in the summary to a portion of the source text, which may be in one or more documents. We explore using a hybrid summarization, i.e., an automatic paraphrase of an extractive summary, to ease attribution. We also use a custom topology to identify the proportion of different categories of attribution-related errors.", "AI": {"tldr": "The paper focuses on developing automatic methods for attributing sentences in summaries to their source texts using a hybrid summarization technique and custom topology.", "motivation": "The motivation is to improve accuracy in attributing information sources in summaries, enabling more reliable reporting.", "method": "They employ a hybrid summarization method combining automatic paraphrasing with extractive techniques, and utilize a custom topology for identifying and categorizing attribution errors.", "result": "The method helps in easing the attribution process, and the custom topology aids in analyzing the errors related to attribution.", "conclusion": "The proposed approach can enhance automatic attribution in summaries, improving the reliability of the reported information."}}
{"id": "2511.08590", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.08590", "abs": "https://arxiv.org/abs/2511.08590", "authors": ["Encheng Xie", "Yihang Sun", "Tao Feng", "Jiaxuan You"], "title": "GMTRouter: Personalized LLM Router over Multi-turn User Interactions", "comment": "Preprint", "summary": "Large Language Model (LLM) routing has demonstrated strong capability in balancing response quality with computational cost. As users exhibit diverse preferences, personalization has attracted increasing attention in LLM routing, since even identical queries may require different models to generate responses tailored to individual needs. However, existing approaches are not fully personalized and often fail to capture the complex interactions between specific users and LLMs. Moreover, user preference data is typically scarce, noisy, and inconsistent in format, which limits the effectiveness of methods that rely solely on user-specific data. To address these challenges, we propose GMTRouter, which represents multi-turn user-LLM interactions as a heterogeneous graph with four node types: user, LLM, query, and response, thereby preserving the rich relational structure of the interaction. Through a tailored message-passing mechanism, GMTRouter learns to capture user preferences from few-shot data within a lightweight inductive graph learning framework, enabling effective personalization. Extensive experiments demonstrate that GMTRouter consistently outperforms strong baselines, achieving 0.9 to 21.6 percent higher accuracy and 0.006 to 0.309 higher AUC across multiple datasets. More importantly, we demonstrate that GMTRouter can adapt to new users and evolving preferences using only few-shot data, without extensive fine-tuning. The code for GMTRouter is publicly available at https://github.com/ulab-uiuc/GMTRouter.", "AI": {"tldr": "本文提出GMTRouter，通过异构图模型捕获少量数据情况下的用户偏好，实现了有效的个性化响应生成，在多个数据集上超越了现有方法。", "motivation": "尽管大型语言模型路由在平衡响应质量和计算成本方面表现强大，但现有方法尚未完全个性化，且难以捕捉用户和模型间的复杂交互。此外，用户偏好数据稀少、嘈杂且格式不统一，使得仅依赖用户特定数据的方法效果受限。", "method": "GMTRouter采用异构图表示用户和大语言模型（LLM）之间的多轮交互，包含四种节点类型：用户、LLM、查询和响应。通过定制的消息传递机制，在轻量级归纳图学习框架中仅使用少量数据学习用户偏好。", "result": "实验结果显示，GMTRouter相较强基线方法在准确性上高出0.9到21.6个百分点，在AUC上提高了0.006到0.309，且能利用少量数据适应新用户和变化中的偏好。", "conclusion": "GMTRouter不仅提升了语言模型路由的个性化能力，还能用少量数据快速适应新用户或用户偏好的变化，无需进行大量的微调。相关代码已开源。"}}
{"id": "2511.08592", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.08592", "abs": "https://arxiv.org/abs/2511.08592", "authors": ["Azza Bouleimen", "Giordano De Marzo", "Taehee Kim", "Nicol`o Pagan", "Hannah Metzler", "Silvia Giordano", "David Garcia"], "title": "The Collective Turing Test: Large Language Models Can Generate Realistic Multi-User Discussions", "comment": null, "summary": "Large Language Models (LLMs) offer new avenues to simulate online communities and social media. Potential applications range from testing the design of content recommendation algorithms to estimating the effects of content policies and interventions. However, the validity of using LLMs to simulate conversations between various users remains largely untested. We evaluated whether LLMs can convincingly mimic human group conversations on social media. We collected authentic human conversations from Reddit and generated artificial conversations on the same topic with two LLMs: Llama 3 70B and GPT-4o. When presented side-by-side to study participants, LLM-generated conversations were mistaken for human-created content 39\\% of the time. In particular, when evaluating conversations generated by Llama 3, participants correctly identified them as AI-generated only 56\\% of the time, barely better than random chance. Our study demonstrates that LLMs can generate social media conversations sufficiently realistic to deceive humans when reading them, highlighting both a promising potential for social simulation and a warning message about the potential misuse of LLMs to generate new inauthentic social media content.", "AI": {"tldr": "此研究展示了大规模语言模型（LLMs）在生成社交媒体对话方面的潜力及潜在风险，这些对话足以欺骗人类读者39%的时间，特别是LLama 3生成的对话有超过40%的时间被误认为是人类所创。", "motivation": "研究动机在于评估大规模语言模型模拟人类社交媒体对话的有效性和逼真度。", "method": "研究收集了来自Reddit的真实对话，并使用Llama 3和GPT-4o生成同一主题的人工对话。将这些对话并行展示给参与者评估其真实性。", "result": "研究结果表明，LLM生成的对话39%的时间被误认为是人类创作的。具体来说，参与者只有56%的时间正确识别Llama 3生成的对话为AI创作的。", "conclusion": "这项研究展示了LLMs在模拟社交媒体对话方面的潜力和可能的新应用，同时也提出了关于这些模型可能被滥用来生成不真实社交媒体内容的警告。"}}
{"id": "2511.08593", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.08593", "abs": "https://arxiv.org/abs/2511.08593", "authors": ["Abha Jha", "Abel Salinas", "Fred Morstatter"], "title": "Knowledge Graph Analysis of Legal Understanding and Violations in LLMs", "comment": null, "summary": "The rise of Large Language Models (LLMs) offers transformative potential for interpreting complex legal frameworks, such as Title 18 Section 175 of the US Code, which governs biological weapons. These systems hold promise for advancing legal analysis and compliance monitoring in sensitive domains. However, this capability comes with a troubling contradiction: while LLMs can analyze and interpret laws, they also demonstrate alarming vulnerabilities in generating unsafe outputs, such as actionable steps for bioweapon creation, despite their safeguards. To address this challenge, we propose a methodology that integrates knowledge graph construction with Retrieval-Augmented Generation (RAG) to systematically evaluate LLMs' understanding of this law, their capacity to assess legal intent (mens rea), and their potential for unsafe applications. Through structured experiments, we assess their accuracy in identifying legal violations, generating prohibited instructions, and detecting unlawful intent in bioweapons-related scenarios. Our findings reveal significant limitations in LLMs' reasoning and safety mechanisms, but they also point the way forward. By combining enhanced safety protocols with more robust legal reasoning frameworks, this research lays the groundwork for developing LLMs that can ethically and securely assist in sensitive legal domains - ensuring they act as protectors of the law rather than inadvertent enablers of its violation.", "AI": {"tldr": "通过结合知识图谱与检索增强生成技术，评估大型语言模型在敏感法律领域的应用能力与安全风险，并探索在未来如何确保其安全性和合规性。", "motivation": "解决大型语言模型在生成不安全内容方面的脆弱性问题。", "method": "提出了一种结合知识图谱构建与检索增强生成（RAG）的方法，以系统地评估大型语言模型（LLMs）对生物武器相关法律的理解、评估法律意图（mens rea）的能力以及潜在的不安全应用能力。", "result": "发现大型语言模型在逻辑推理和安全机制方面存在明显缺陷，但同时也指出了改进的方向。", "conclusion": "研究为开发能够在敏感法律领域中既能保护法律又不会无意中成为违法行为的促进因素的更安全和道德的大规模语言模型奠定了基础。"}}
{"id": "2511.08609", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.08609", "abs": "https://arxiv.org/abs/2511.08609", "authors": ["I. Bailo", "F. Buonora", "G. Ciarfaglia", "L. T. Consoli", "A. Evangelista", "M. Gabusi", "M. Ghiani", "C. Petracca Ciavarella", "F. Picariello", "F. Sarcina", "F. Tuosto", "V. Zullo", "L. Airoldi", "G. Bruno", "D. D. Gobbo", "S. Pezzenati", "G. A. Tona"], "title": "Case Study: Transformer-Based Solution for the Automatic Digitization of Gas Plants", "comment": null, "summary": "The energy transition is a key theme of the last decades to determine a future of eco-sustainability, and an area of such importance cannot disregard digitization, innovation and the new technological tools available. This is the context in which the Generative Artificial Intelligence models described in this paper are positioned, developed by Engineering Ingegneria Informatica SpA in order to automate the plant structures acquisition of SNAM energy infrastructure, a leading gas transportation company in Italy and Europe. The digitization of a gas plant consists in registering all its relevant information through the interpretation of the related documentation. The aim of this work is therefore to design an effective solution based on Artificial Intelligence techniques to automate the extraction of the information necessary for the digitization of a plant, in order to streamline the daily work of MGM users. The solution received the P&ID of the plant as input, each one in pdf format, and uses OCR, Vision LLM, Object Detection, Relational Reasoning and optimization algorithms to return an output consisting of two sets of information: a structured overview of the relevant design data and the hierarchical framework of the plant. To achieve convincing results, we extend a state-of-the-art model for Scene Graph Generation introducing a brand new Transformer architecture with the aim of deepening the analysis of the complex relations between the plant's components. The synergistic use of the listed AI-based technologies allowed to overcome many obstacles arising from the high variety of data, due to the lack of standardization. An accuracy of 91\\% has been achieved in the extraction of textual information relating to design data. Regarding the plants topology, 93\\% of components are correctly identified and the hierarchical structure is extracted with an accuracy around 80\\%.", "AI": {"tldr": "本文提出了一种基于人工智能的技术方案，用于自动解析气体工厂图纸（P&ID），旨在提高气体工厂结构的数字化水平，显著提高了信息提取的准确率。", "motivation": "本研究旨在设计一种有效的基于人工智能的技术方案，以自动化提取气体工厂结构的必要信息，从而简化MGM用户的日常工作流程。在能源转型和数字化的大背景下，这是一个重要的应用领域。", "method": "本文提出了一种基于人工智能的技术方案，用于自动提取气体工厂结构的相关信息，以促进其数字化。该方案以工厂P&ID图纸为输入，使用OCR、视觉LLM、对象检测、关系推理和优化算法作为技术手段。为了提高复杂组件间关系分析的深度，作者扩展了一个现有场景图生成模型，引入了一种全新的Transformer架构。", "result": "实验结果表明，该模型在提取设计数据相关的文本信息方面达到了91%的准确率，正确识别了93%的工厂组件，并以约80%的准确率提取了工厂的层级结构。", "conclusion": "透过此方案的成功应用，此研究显示人工智能技术在处理高度多样的数据并促进企业实现其结构数字化方面，展现出巨大潜力。"}}
{"id": "2511.08594", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.08594", "abs": "https://arxiv.org/abs/2511.08594", "authors": ["Stewart Slocum", "Asher Parker-Sartori", "Dylan Hadfield-Menell"], "title": "Diverse Preference Learning for Capabilities and Alignment", "comment": null, "summary": "The ability of LLMs to represent diverse perspectives is critical as they increasingly impact society. However, recent studies reveal that alignment algorithms such as RLHF and DPO significantly reduce the diversity of LLM outputs. Not only do aligned LLMs generate text with repetitive structure and word choice, they also approach problems in more uniform ways, and their responses reflect a narrower range of societal perspectives. We attribute this problem to the KL divergence regularizer employed in preference learning algorithms. This causes the model to systematically overweight majority opinions and sacrifice diversity in its outputs. To address this, we propose Soft Preference Learning, which decouples the entropy and cross-entropy terms in the KL penalty - allowing for fine-grained control over LLM generation diversity. From a capabilities perspective, LLMs trained using Soft Preference Learning attain higher accuracy on difficult repeated sampling tasks and produce outputs with greater semantic and lexical diversity. From an alignment perspective, they are capable of representing a wider range of societal viewpoints and display improved logit calibration. Notably, Soft Preference Learning resembles, but is a Pareto improvement over, standard temperature scaling.", "AI": {"tldr": "提出了Soft Preference Learning方法，解决了现有偏好学习算法导致的大规模语言模型输出多样性降低的问题，同时保持或提升了模型的准确性和对社会观点的广泛表示能力。", "motivation": "现有的对齐算法如RLHF和DPO减少了大规模语言模型输出的多样性，使得模型生成的文本结构和词汇选择重复，反映了较窄的社会视角。此问题归因于偏好学习算法中的KL散度正则化项，导致模型显著倾向于多数意见，牺牲了输出多样性。", "method": "提出Soft Preference Learning方法，该方法将KL罚函数中的熵和交叉熵项解耦，实现对语言模型生成多样性更精细的控制。", "result": "使用Soft Preference Learning训练的语言模型在困难的重复采样任务中达到了更高的准确性，并产生了语义和词汇多样性的输出，能够表示更广泛的社会观点，且显示出改进的逻辑校准能力。", "conclusion": "Soft Preference Learning不仅提升了语言模型的能力，也改善了其对齐特性，较标准温度缩放方法为帕累托改进。"}}
{"id": "2511.08613", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2511.08613", "abs": "https://arxiv.org/abs/2511.08613", "authors": ["Dogucan Yaman", "Fevziye Irem Eyiokur", "Hazım Kemal Ekenel", "Alexander Waibel"], "title": "Assessing Identity Leakage in Talking Face Generation: Metrics and Evaluation Framework", "comment": null, "summary": "Inpainting-based talking face generation aims to preserve video details such as pose, lighting, and gestures while modifying only lip motion, often using an identity reference image to maintain speaker consistency. However, this mechanism can introduce lip leaking, where generated lips are influenced by the reference image rather than solely by the driving audio. Such leakage is difficult to detect with standard metrics and conventional test setup. To address this, we propose a systematic evaluation methodology to analyze and quantify lip leakage. Our framework employs three complementary test setups: silent-input generation, mismatched audio-video pairing, and matched audio-video synthesis. We also introduce derived metrics including lip-sync discrepancy and silent-audio-based lip-sync scores. In addition, we study how different identity reference selections affect leakage, providing insights into reference design. The proposed methodology is model-agnostic and establishes a more reliable benchmark for future research in talking face generation.", "AI": {"tldr": "本研究旨在解决说话脸生成中唇泄漏问题，通过系统评估框架及衍生指标，分析了不同身份参考对泄漏的影响，为未来研究提供了可靠基准。", "motivation": "传统的测试设置难以检测唇部泄漏问题，即生成的嘴唇受到参考图像的影响，而不是仅仅受到驱动音频的影响。为解决此问题，需要提出新的评估方法。", "method": "本研究提出了一种系统的评估方法来分析和量化唇部泄漏问题，采用三种互补测试设置：无声输入生成、不同步音视频配对和同步音视频合成。此外，还引入了衍生指标，包括唇音不同步度和无声音频下的唇音同步得分。", "result": "研究结果表明不同身份参考的选择会如何影响泄漏，为参考设计提供了洞见，并建立了更可靠的说话人脸生成未来研究基准。", "conclusion": "该研究提出的方法是模型无关的，有助于建立更可靠的评估基准，为未来说话人脸生成的研究提供了重要指导。"}}
{"id": "2511.08595", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.08595", "abs": "https://arxiv.org/abs/2511.08595", "authors": ["Joongho Kim", "Xirui Huang", "Zarreen Reza", "Gabriel Grand", "Kevin Zhu", "Ryan Lagasse"], "title": "Chopping Trees: Semantic Similarity Based Dynamic Pruning for Tree-of-Thought Reasoning", "comment": "39th Conference on Neural Information Processing Systems (NeurIPS 2025) Workshop on Efficient Reasoning", "summary": "Tree-of-Thought (ToT) reasoning boosts the problem-solving abilities of Large Language Models (LLMs) but is computationally expensive due to semantic redundancy, where distinct branches explore equivalent reasoning paths. We introduce Semantic Similarity-Based Dynamic Pruning (SSDP), a lightweight method that, to the best of our knowledge, is the first framework to integrate online semantic merging into parallelized tree search, enabling the clustering and pruning of redundant steps in real time. Across reasoning benchmarks, including GSM8K and MATH500, SSDP achieves up to a 2.3x speedup over state-of-the-art tree-search baselines while maintaining competitive accuracy (typically within 5% of the strongest baseline) and reducing the number of explored nodes by 85-90%, demonstrating a practical approach to efficient, scalable LLM reasoning. The implementation of SSDP is publicly available at https://github.com/kimjoonghokim/SSDP.", "AI": {"tldr": "引入了SSDP方法，减少大规模语言模型推理中的语义冗余，实现了显著的速度提升和节点减少。", "motivation": "为了提高大规模语言模型的推理能力，我们试图解决树式思维推理中的计算成本高昂问题，特别是语义冗余问题。", "method": "我们介绍了一种轻量级方法，称为基于语义相似性的动态剪枝(SSDP)，该方法首次将在线语义合并集成到并行树搜索中，从而能够实现实时冗余步骤的聚类和剪枝。", "result": "实验结果表明，SSDP在GSM8K和MATH500等推理基准测试上，相比最先进的树搜索基线，实现了最高2.3倍的速度提升，同时保持了竞争性准确性（通常在最强基线的5%以内），并将探索节点数量减少了85-90%。", "conclusion": "SSDP提供了一种实用的方法，以实现大规模语言模型的高效、可扩展推理。"}}
{"id": "2511.08615", "categories": ["cs.CV", "cs.IT", "cs.LG", "cs.RO", "eess.IV"], "pdf": "https://arxiv.org/pdf/2511.08615", "abs": "https://arxiv.org/abs/2511.08615", "authors": ["Kosta Dakic", "Kanchana Thilakarathna", "Rodrigo N. Calheiros", "Teng Joon Lim"], "title": "A Multi-Drone Multi-View Dataset and Deep Learning Framework for Pedestrian Detection and Tracking", "comment": "Introduction of the MATRIX Dataset, featuring synchronized footage from eight drones in an urban environment with comprehensive annotations for detection and tracking, available at https://github.com/KostaDakic/MATRIX/tree/main", "summary": "Multi-drone surveillance systems offer enhanced coverage and robustness for pedestrian tracking, yet existing approaches struggle with dynamic camera positions and complex occlusions. This paper introduces MATRIX (Multi-Aerial TRacking In compleX environments), a comprehensive dataset featuring synchronized footage from eight drones with continuously changing positions, and a novel deep learning framework for multi-view detection and tracking. Unlike existing datasets that rely on static cameras or limited drone coverage, MATRIX provides a challenging scenario with 40 pedestrians and a significant architectural obstruction in an urban environment. Our framework addresses the unique challenges of dynamic drone-based surveillance through real-time camera calibration, feature-based image registration, and multi-view feature fusion in bird's-eye-view (BEV) representation. Experimental results demonstrate that while static camera methods maintain over 90\\% detection and tracking precision and accuracy metrics in a simplified MATRIX environment without an obstruction, 10 pedestrians and a much smaller observational area, their performance significantly degrades in the complex environment. Our proposed approach maintains robust performance with $\\sim$90\\% detection and tracking accuracy, as well as successfully tracks $\\sim$80\\% of trajectories under challenging conditions. Transfer learning experiments reveal strong generalization capabilities, with the pretrained model achieving much higher detection and tracking accuracy performance compared to training the model from scratch. Additionally, systematic camera dropout experiments reveal graceful performance degradation, demonstrating practical robustness for real-world deployments where camera failures may occur. The MATRIX dataset and framework provide essential benchmarks for advancing dynamic multi-view surveillance systems.", "AI": {"tldr": "The paper introduces the MATRIX dataset and a deep learning framework for dynamic multi-drone pedestrian tracking, demonstrating superior performance and robustness in complex scenarios compared to static camera methods.", "motivation": "The motivation behind this research is to improve pedestrian tracking in complex urban environments with multi-drone surveillance systems that can handle dynamic camera positions and occlusions, addressing the limitations of existing static camera and limited drone coverage methods.", "method": "This paper presents the MATRIX (Multi-Aerial TRacking In compleX environments) dataset and a novel deep learning framework for multi-view pedestrian detection and tracking under dynamic, drone-based surveillance conditions. The framework includes real-time camera calibration, feature-based image registration, and multi-view feature fusion in a bird's-eye-view (BEV) representation.", "result": "Experiments show that static camera methods maintain high precision in a simplified environment but fail in complex settings. In contrast, the proposed approach offers robust performance with approximately 90% detection and tracking accuracy, and 80% trajectory tracking accuracy under challenging conditions, and demonstrates significant generalizability.", "conclusion": "The paper concludes that the proposed framework and the MATRIX dataset provide a significant benchmark for advancing multi-view surveillance systems, particularly in dynamic and challenging urban environments."}}
{"id": "2511.08596", "categories": ["cs.CL", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2511.08596", "abs": "https://arxiv.org/abs/2511.08596", "authors": ["Arka Dutta", "Sujan Dutta", "Rijul Magu", "Soumyajit Datta", "Munmun De Choudhury", "Ashiqur R. KhudaBukhsh"], "title": "What About the Scene with the Hitler Reference? HAUNT: A Framework to Probe LLMs' Self-consistency Via Adversarial Nudge", "comment": null, "summary": "Hallucinations pose a critical challenge to the real-world deployment of large language models (LLMs) in high-stakes domains. In this paper, we present a framework for stress testing factual fidelity in LLMs in the presence of adversarial nudge. Our framework consists of three steps. In the first step, we instruct the LLM to produce sets of truths and lies consistent with the closed domain in question. In the next step, we instruct the LLM to verify the same set of assertions as truths and lies consistent with the same closed domain. In the final step, we test the robustness of the LLM against the lies generated (and verified) by itself. Our extensive evaluation, conducted using five widely known proprietary LLMs across two closed domains of popular movies and novels, reveals a wide range of susceptibility to adversarial nudges: \\texttt{Claude} exhibits strong resilience, \\texttt{GPT} and \\texttt{Grok} demonstrate moderate resilience, while \\texttt{Gemini} and \\texttt{DeepSeek} show weak resilience. Considering that a large population is increasingly using LLMs for information seeking, our findings raise alarm.", "AI": {"tldr": "本文开发了一个框架来评估大型语言模型对抗性提示的抵抗力，通过在两个封闭领域对五种语言模型进行测试，研究揭示了它们在事实准确性方面表现出不同的抵抗力。", "motivation": "大型语言模型在高风险领域的实际应用中面临着幻觉（即产生不准确或虚假信息）的挑战。为了应对这一问题，作者提出了一个事实忠实度的压力测试框架。", "method": "本文提出了一种框架，用于测试大型语言模型在面对对抗性提示时对事实准确性的压力测试。此框架包含三个步骤：生成真相和谎言，验证生成的真相和谎言，以及测试模型对生成的谎言的鲁棒性。", "result": "作者对五个知名的大型语言模型在电影和小说两个封闭领域进行了广泛的评估，结果显示：Claude表现出较强的抗干扰能力，GPT和Grok表现出中等的抗干扰能力，而Gemini和DeepSeek则显示出较弱的抗干扰能力。", "conclusion": "鉴于越来越多的人群正在使用大型语言模型来寻找信息，本文的研究结果对语言模型在实际应用中的依赖性发出警告。"}}
{"id": "2511.08628", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.08628", "abs": "https://arxiv.org/abs/2511.08628", "authors": ["Xuan Yu", "Tianyang Xu"], "title": "Learning Topology-Driven Multi-Subspace Fusion for Grassmannian Deep Network", "comment": "9 pages, 2 figures, accepted at AAAI 2026", "summary": "Grassmannian manifold offers a powerful carrier for geometric representation learning by modelling high-dimensional data as low-dimensional subspaces. However, existing approaches predominantly rely on static single-subspace representations, neglecting the dynamic interplay between multiple subspaces critical for capturing complex geometric structures. To address this limitation, we propose a topology-driven multi-subspace fusion framework that enables adaptive subspace collaboration on the Grassmannian. Our solution introduces two key innovations: (1) Inspired by the Kolmogorov-Arnold representation theorem, an adaptive multi-subspace modelling mechanism is proposed that dynamically selects and weights task-relevant subspaces via topological convergence analysis, and (2) a multi-subspace interaction block that fuses heterogeneous geometric representations through Fréchet mean optimisation on the manifold. Theoretically, we establish the convergence guarantees of adaptive subspaces under a projection metric topology, ensuring stable gradient-based optimisation. Practically, we integrate Riemannian batch normalisation and mutual information regularisation to enhance discriminability and robustness. Extensive experiments on 3D action recognition (HDM05, FPHA), EEG classification (MAMEM-SSVEPII), and graph tasks demonstrate state-of-the-art performance. Our work not only advances geometric deep learning but also successfully adapts the proven multi-channel interaction philosophy of Euclidean networks to non-Euclidean domains, achieving superior discriminability and interpretability.", "AI": {"tldr": "本文提出了一种新的拓扑驱动的多子空间融合框架，通过自适应子空间选择和加权以及多子空间交互融合，提高了几何表示学习在Grassmann流形上的性能，实验中展现出了顶尖的表现。", "motivation": "Grassmann流形为几何表征学习提供了强大的载体，但它通常使用静态的单一子空间表示，这限制了捕捉复杂几何结构的能力。本文旨在通过引入多子空间动态交互的概念来克服这一限制。", "method": "本文提出了一种基于拓扑驱动的多子空间融合框架，该框架能够在Grassmann流形上实现自适应子空间协作。该方法包括两个关键创新点：一是受到Kolmogorov-Arnold表示定理的启发，提出了一种自适应多子空间建模机制，通过拓扑收敛性分析动态选择和加权任务相关的子空间；二是引入了一个多子空间交互块，通过流形上的Fréchet均值优化融合异质几何表示。", "result": "理论上，本文证明了自适应子空间在投影度量拓扑下的收敛性，确保了基于梯度的优化的稳定性。此外，实验结果表明，该方法在3D动作识别、EEG分类和图任务中均达到了最先进的性能。", "conclusion": "本文的工作不仅推进了几何深度学习领域的发展，还将欧几里得网络中的多通道交互理念成功延伸至非欧几里得领域，方法不仅具备优越的分类能力，也增强了解释性。"}}
{"id": "2511.08597", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.08597", "abs": "https://arxiv.org/abs/2511.08597", "authors": ["Heehwan Kim", "Sungjune Park", "Daeseon Choi"], "title": "Self-HarmLLM: Can Large Language Model Harm Itself?", "comment": null, "summary": "Large Language Models (LLMs) are generally equipped with guardrails to block the generation of harmful responses. However, existing defenses always assume that an external attacker crafts the harmful query, and the possibility of a model's own output becoming a new attack vector has not been sufficiently explored. In this study, we propose the Self-HarmLLM scenario, which uses a Mitigated Harmful Query (MHQ) generated by the same model as a new input. An MHQ is an ambiguous query whose original intent is preserved while its harmful nature is not directly exposed. We verified whether a jailbreak occurs when this MHQ is re-entered into a separate session of the same model. We conducted experiments on GPT-3.5-turbo, LLaMA3-8B-instruct, and DeepSeek-R1-Distill-Qwen-7B under Base, Zero-shot, and Few-shot conditions. The results showed up to 52% transformation success rate and up to 33% jailbreak success rate in the Zero-shot condition, and up to 65% transformation success rate and up to 41% jailbreak success rate in the Few-shot condition. By performing both prefix-based automated evaluation and human evaluation, we found that the automated evaluation consistently overestimated jailbreak success, with an average difference of 52%. This indicates that automated evaluation alone is not accurate for determining harmfulness. While this study is a toy-level study based on a limited query set and evaluators, it proves that our method can still be a valid attack scenario. These results suggest the need for a fundamental reconsideration of guardrail design and the establishment of a more robust evaluation methodology.", "AI": {"tldr": "研究提出Self-HarmLLM情景，展示了大模型自身生成的有害查询重新输入模型时的潜在风险，表明现有护栏设计的不足，建议改进评估方法。", "motivation": "现有防御假设外部攻击者伪造有害查询，但尚未充分探讨模型自身输出成为新的攻击载体的可能性。", "method": "本研究提出了一种名为Self-HarmLLM的情景，其中使用了由同一模型生成的缓冲有害查询(MHQ)作为新输入。MHQ是一种保持其原始意图但不直接暴露有害性质的模糊查询。", "result": "研究发现，在零样本条件下，最高可达52%的转换成功率和33%的绕过成功几率；在少样本条件下，最高可达65%的转换成功率和41%的绕过成功几率。同时，自动化评估的准确性值得怀疑。", "conclusion": "研究结果表明需要重新考虑护栏设计和建立更稳健的评估方法。"}}
{"id": "2511.08633", "categories": ["cs.CV", "cs.AI", "cs.GR", "cs.LG", "cs.MM"], "pdf": "https://arxiv.org/pdf/2511.08633", "abs": "https://arxiv.org/abs/2511.08633", "authors": ["Assaf Singer", "Noam Rotstein", "Amir Mann", "Ron Kimmel", "Or Litany"], "title": "Time-to-Move: Training-Free Motion Controlled Video Generation via Dual-Clock Denoising", "comment": null, "summary": "Diffusion-based video generation can create realistic videos, yet existing image- and text-based conditioning fails to offer precise motion control. Prior methods for motion-conditioned synthesis typically require model-specific fine-tuning, which is computationally expensive and restrictive. We introduce Time-to-Move (TTM), a training-free, plug-and-play framework for motion- and appearance-controlled video generation with image-to-video (I2V) diffusion models. Our key insight is to use crude reference animations obtained through user-friendly manipulations such as cut-and-drag or depth-based reprojection. Motivated by SDEdit's use of coarse layout cues for image editing, we treat the crude animations as coarse motion cues and adapt the mechanism to the video domain. We preserve appearance with image conditioning and introduce dual-clock denoising, a region-dependent strategy that enforces strong alignment in motion-specified regions while allowing flexibility elsewhere, balancing fidelity to user intent with natural dynamics. This lightweight modification of the sampling process incurs no additional training or runtime cost and is compatible with any backbone. Extensive experiments on object and camera motion benchmarks show that TTM matches or exceeds existing training-based baselines in realism and motion control. Beyond this, TTM introduces a unique capability: precise appearance control through pixel-level conditioning, exceeding the limits of text-only prompting. Visit our project page for video examples and code: https://time-to-move.github.io/.", "AI": {"tldr": "文章提出了TTM，一种无训练、可插入的框架，用于基于图像到视频的扩散模型的视频生成，提供精准的运动和外观控制技术。", "motivation": "现有的基于图像和文本条件的方法不能提供精确的运动控制，而现有的基于运动合成的方法通常需要模型特定的微调，成本高且有限制。TTM旨在解决这些问题，提供一种无训练、易于使用的视频生成解决方案。", "method": "介绍了一种名为Time-to-Move (TTM) 的无训练、可插入框架，用于基于图像到视频的扩散模型进行动作和外观控制的视频生成。该方法通过用户友好的操作如剪切和拖拽或基于深度的重投影，获取粗略参考动画，将它们作为粗略动作线索，并对现有的SDEdit机制进行适应。引入了双时钟去噪机制，该机制根据区域情况，对用户指定的动作区域进行强对齐，而在其他地方保持灵活性，以平衡用户的意图与自然动态之间的关系。", "result": "实验结果表明，TTM在物体和相机运动的基准测试中，无论是现实感还是运动控制都达到了或超过了现有的基于训练的基线方法。此外，TTM通过像素级条件控制提供精确的外观控制，超越了仅基于文本提示的限制。", "conclusion": "TTM提供了一种无训练成本、可插入的精动作和外观控制的视频生成方法，与任何后端兼容，无需额外训练或运行时间成本。"}}
{"id": "2511.08598", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.08598", "abs": "https://arxiv.org/abs/2511.08598", "authors": ["Yanhong Li", "Tianyang Xu", "Kenan Tang", "Karen Livescu", "David McAllester", "Jiawei Zhou"], "title": "OKBench: Democratizing LLM Evaluation with Fully Automated, On-Demand, Open Knowledge Benchmarking", "comment": null, "summary": "Knowledge-intensive question answering is central to large language models (LLMs) and is typically assessed using static benchmarks derived from sources like Wikipedia and textbooks. However, these benchmarks fail to capture evolving knowledge in a dynamic world, and centralized curation struggles to keep pace with rapid LLM advancements. To address these drawbacks, we propose Open Knowledge Bench (OKBench), a fully automated framework for generating high-quality, dynamic knowledge benchmarks on demand. Focusing on the news domain where knowledge updates daily, OKBench is an agentic framework that automates the sourcing, creation, validation, and distribution of benchmarks. Our approach democratizes benchmark creation and facilitates thorough evaluation of retrieval-augmented methods by reducing overlap with pretraining data. We evaluate our framework on a wide range open-source and proprietary LLMs of various sizes and configurations, both with and without retrieval over freshly generated knowledge. Our results reveal distinct model behaviors when confronted with new information and highlight how retrieval narrows the performance gap between small and large models. These findings underscore the importance of evaluating LLMs on evolving knowledge benchmarks.", "AI": {"tldr": "本文提出OKBench，一个自动化生成高质量、动态知识基准的框架，用于评估和验证LLMs在快速更新的知识领域中的表现，强调了使用动态基准评估LLMs的重要性。", "motivation": "现有的基于维基百科和教科书等来源的静态基准无法捕捉到动态世界中的知识变化，因此提出OKBench来解决这一问题，同时实现基准创建的民主化，支持检索增强方法的全面评估。", "method": "提出了一个完全自动化的框架Open Knowledge Bench (OKBench)，用于按需生成高质量的动态知识基准。该框架在新闻领域中通过自动化信息源获取、基准创建、验证和分发过程来应对知识的快速更新。", "result": "实验结果表明不同大小和配置的开源与专有LLMs在面对新信息时表现出不同的行为，检索可以缩小小模型和大模型之间的性能差距。", "conclusion": "研究强调了在动态知识基准上评估LLMs的重要性。"}}
{"id": "2511.08634", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.08634", "abs": "https://arxiv.org/abs/2511.08634", "authors": ["Gen Yang", "Zhipeng Deng", "Junfeng Man"], "title": "CADIC: Continual Anomaly Detection Based on Incremental Coreset", "comment": "12 pages, 8 figures", "summary": "The primary objective of Continual Anomaly Detection (CAD) is to learn the normal patterns of new tasks under dynamic data distribution assumptions while mitigating catastrophic forgetting. Existing embedding-based CAD approaches continuously update a memory bank with new embeddings to adapt to sequential tasks. However, these methods require constructing class-specific sub-memory banks for each task, which restricts their flexibility and scalability. To address this limitation, we propose a novel CAD framework where all tasks share a unified memory bank. During training, the method incrementally updates embeddings within a fixed-size coreset, enabling continuous knowledge acquisition from sequential tasks without task-specific memory fragmentation. In the inference phase, anomaly scores are computed via a nearest-neighbor matching mechanism, achieving state-of-the-art detection accuracy. We validate the method through comprehensive experiments on MVTec AD and Visa datasets. Results show that our approach outperforms existing baselines, achieving average image-level AUROC scores of 0.972 (MVTec AD) and 0.891 (Visa). Notably, on a real-world electronic paper dataset, it demonstrates 100% accuracy in anomaly sample detection, confirming its robustness in practical scenarios. The implementation will be open-sourced on GitHub.", "AI": {"tldr": "我们提出了一种新的CAD框架，通过共享一个统一的记忆库克服了现有方法的限制，提高了连续学习新任务和检测异常的能力，实验结果验证了其优越的表现。", "motivation": "现有基于嵌入的CAD方法需要为每个任务构建类特定的子记忆库，这限制了它们的灵活性和可扩展性。本研究旨在解决这一局限性，提高CAD在动态数据分布假设下的表现。", "method": "我们的方法提出了一种新的连续异常检测(CAD)框架，所有任务共享一个统一的记忆库。在训练过程中，该方法通过增量更新固定大小核集中（coreset）的嵌入来持续获取知识，而无需任务特定的记忆碎片化。在推理阶段，通过最近邻匹配机制计算异常分数，实现了最先进的检测准确性。", "result": "在MVTec AD和Visa数据集上进行的实验证明，我们提出的方法优于现有的基线方法，分别实现了平均图像级别AUROC分数0.972和0.891，在真实世界电子纸数据集中实现了100%的异常样本检测准确性。", "conclusion": "我们的CAD框架在通用性的提高和性能的优越性方面显示出了极大的前景，这对于在复杂的、动态变化的数据环境中正确检测异常内容是至关重要的。方法将开源。"}}
{"id": "2511.08600", "categories": ["cs.CL", "cs.HC"], "pdf": "https://arxiv.org/pdf/2511.08600", "abs": "https://arxiv.org/abs/2511.08600", "authors": ["Yilan Liu"], "title": "Retrieval-Augmented Generation of Pediatric Speech-Language Pathology vignettes: A Proof-of-Concept Study", "comment": "37 pages, 3 figures", "summary": "Clinical vignettes are essential educational tools in speech-language pathology (SLP), but manual creation is time-intensive. While general-purpose large language models (LLMs) can generate text, they lack domain-specific knowledge, leading to hallucinations and requiring extensive expert revision. This study presents a proof-of-concept system integrating retrieval-augmented generation (RAG) with curated knowledge bases to generate pediatric SLP case materials. A multi-model RAG-based system was prototyped integrating curated domain knowledge with engineered prompt templates, supporting five commercial (GPT-4o, Claude 3.5 Sonnet, Gemini 2.5 Pro) and open-source (Llama 3.2, Qwen 2.5-7B) LLMs. Seven test scenarios spanning diverse disorder types and grade levels were systematically designed. Generated cases underwent automated quality assessment using a multi-dimensional rubric evaluating structural completeness, internal consistency, clinical appropriateness, and IEP goal/session note quality. This proof-of-concept demonstrates technical feasibility for RAG-augmented generation of pediatric SLP vignettes. Commercial models showed marginal quality advantages, but open-source alternatives achieved acceptable performance, suggesting potential for privacy-preserving institutional deployment. Integration of curated knowledge bases enabled content generation aligned with professional guidelines. Extensive validation through expert review, student pilot testing, and psychometric evaluation is required before educational or research implementation. Future applications may extend to clinical decision support, automated IEP goal generation, and clinical reflection training.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2511.08640", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.08640", "abs": "https://arxiv.org/abs/2511.08640", "authors": ["Xingcheng Liu", "Bin Rao", "Yanchen Guan", "Chengyue Wang", "Haicheng Liao", "Jiaxun Zhang", "Chengyu Lin", "Meixin Zhu", "Zhenning Li"], "title": "Predict and Resist: Long-Term Accident Anticipation under Sensor Noise", "comment": "accepted by the Fortieth AAAI Conference on Artificial Intelligence (AAAI-26)", "summary": "Accident anticipation is essential for proactive and safe autonomous driving, where even a brief advance warning can enable critical evasive actions. However, two key challenges hinder real-world deployment: (1) noisy or degraded sensory inputs from weather, motion blur, or hardware limitations, and (2) the need to issue timely yet reliable predictions that balance early alerts with false-alarm suppression. We propose a unified framework that integrates diffusion-based denoising with a time-aware actor-critic model to address these challenges. The diffusion module reconstructs noise-resilient image and object features through iterative refinement, preserving critical motion and interaction cues under sensor degradation. In parallel, the actor-critic architecture leverages long-horizon temporal reasoning and time-weighted rewards to determine the optimal moment to raise an alert, aligning early detection with reliability. Experiments on three benchmark datasets (DAD, CCD, A3D) demonstrate state-of-the-art accuracy and significant gains in mean time-to-accident, while maintaining robust performance under Gaussian and impulse noise. Qualitative analyses further show that our model produces earlier, more stable, and human-aligned predictions in both routine and highly complex traffic scenarios, highlighting its potential for real-world, safety-critical deployment.", "AI": {"tldr": "A unified framework combining diffusion denoising and time-aware actor-critic modeling effectively enhances accident anticipation in autonomous driving by improving sensor data quality and issuing timely, reduced-false-alarm warnings.", "motivation": "The motivation is to improve accident anticipation in autonomous vehicles by overcoming the challenges of noisy sensor data and the need for balanced, timely alerts, which are crucial for proactive and safe driving.", "method": "We propose a unified framework integrating diffusion-based denoising and a time-aware actor-critic model to enhance accident anticipation in autonomous driving by handling noisy sensory inputs and issuing timely, reliable alerts.", "result": "Experiments show state-of-the-art accuracy and significant improvements in mean time-to-accident across three datasets, with robust performance under various noise types and human-aligned predictions in complex traffic scenarios.", "conclusion": "The proposed model achieves earlier, more stable, and reliable accident anticipation, demonstrating its potential for real-world deployment in safety-critical autonomous driving applications."}}
{"id": "2511.08601", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.08601", "abs": "https://arxiv.org/abs/2511.08601", "authors": ["Nazmoon Falgunee Moon"], "title": "Evaluating DisCoCirc in Translation Tasks & its Limitations: A Comparative Study Between Bengali & English", "comment": "17 pages, 21 figures, 3 tables", "summary": "In [4], the authors present the DisCoCirc (Distributed Compositional Circuits) formalism for the English language, a grammar-based framework derived from the production rules that incorporates circuit-like representations in order to give a precise categorical theoretical structure to the language. In this paper, we extend this approach to develop a similar framework for Bengali and apply it to translation tasks between English and Bengali. A central focus of our work lies in reassessing the effectiveness of DisCoCirc in reducing language bureaucracy. Unlike the result suggested in [5], our findings indicate that although it works well for a large part of the language, it still faces limitations due to the structural variation of the two languages. We discuss the possible methods that might handle these shortcomings and show that, in practice, DisCoCirc still struggles even with relatively simple sentences. This divergence from prior claims not only highlights the framework's constraints in translation but also suggest scope for future improvement. Apart from our primary focus on English-Bengali translation, we also take a short detour to examine English conjunctions, following [1], showing a connection between conjunctions and Boolean logic.", "AI": {"tldr": "This study extends DisCoCirc formalism to Bengali, focusing on translation. It shows that while effective, the framework faces limitations due to the languages' structural differences.", "motivation": "The motivation is to explore the effectiveness of the DisCoCirc formalism in reducing language bureaucracy and to assess its applicability to other languages like Bengali.", "method": "Content discusses extending DisCoCirc formalism from English to Bengali, focusing on translation between the two languages and reassessing the effectiveness of DisCoCirc.", "result": "The framework works well for a large part of the language but faces limitations due to structural variation between the languages. It struggles even with relatively simple sentences.", "conclusion": "The study highlights the constraints of the DisCoCirc framework in translation tasks due to language structural differences but also suggests potential areas for future improvement."}}
{"id": "2511.08651", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.08651", "abs": "https://arxiv.org/abs/2511.08651", "authors": ["Hae-Won Jo", "Yeong-Jun Cho"], "title": "RS-Net: Context-Aware Relation Scoring for Dynamic Scene Graph Generation", "comment": null, "summary": "Dynamic Scene Graph Generation (DSGG) models how object relations evolve over time in videos. However, existing methods are trained only on annotated object pairs and lack guidance for non-related pairs, making it difficult to identify meaningful relations during inference. In this paper, we propose Relation Scoring Network (RS-Net), a modular framework that scores the contextual importance of object pairs using both spatial interactions and long-range temporal context. RS-Net consists of a spatial context encoder with learnable context tokens and a temporal encoder that aggregates video-level information. The resulting relation scores are integrated into a unified triplet scoring mechanism to enhance relation prediction. RS-Net can be easily integrated into existing DSGG models without architectural changes. Experiments on the Action Genome dataset show that RS-Net consistently improves both Recall and Precision across diverse baselines, with notable gains in mean Recall, highlighting its ability to address the long-tailed distribution of relations. Despite the increased number of parameters, RS-Net maintains competitive efficiency, achieving superior performance over state-of-the-art methods.", "AI": {"tldr": "论文提出了一种用于评估视频中物体关系的重要性的模块化框架RS-Net，该框架能够改善现有动态场景图生成模型的性能，尤其是在处理物体关系时显示出更好的性能。", "motivation": "由于现有方法仅基于标注的物体对进行训练，并且缺乏对非相关对的指导，使物体关系的识别在推理阶段变得困难。因此，该论文提出了RS-Net。", "method": "RS-Net是一个模块化框架，通过空间交互和长时间范围的时间上下文对对象对之间的上下文重要性进行评分。该框架包含一个带有可学习上下文标记的空间上下文编码器和一个聚合视频级信息的时间编码器。所得的关系评分被整合到一个统一的三元组评分机制中以增强关系预测。", "result": "实验结果表明，RS-Net能在Action Genome数据集上一致性地提高召回率和精确度，尤其是在平均召回率上显示出显著提升。此举证明RS-Net能有效解决关系分布长尾的问题。", "conclusion": "RS-Net可以轻松地与现有的DSGG模型集成，而无需改变架构。尽管参数数量有所增加，但RS-Net仍保持竞争力，其性能优于现有的方法。"}}
{"id": "2511.08605", "categories": ["cs.CL", "cs.CY", "cs.HC", "cs.MA", "cs.MM"], "pdf": "https://arxiv.org/pdf/2511.08605", "abs": "https://arxiv.org/abs/2511.08605", "authors": ["Azmine Toushik Wasi", "Wahid Faisal", "Mst Rafia Islam"], "title": "Mina: A Multilingual LLM-Powered Legal Assistant Agent for Bangladesh for Empowering Access to Justice", "comment": null, "summary": "Bangladesh's low-income population faces major barriers to affordable legal advice due to complex legal language, procedural opacity, and high costs. Existing AI legal assistants lack Bengali-language support and jurisdiction-specific adaptation, limiting their effectiveness. To address this, we developed Mina, a multilingual LLM-based legal assistant tailored for the Bangladeshi context. It employs multilingual embeddings and a RAG-based chain-of-tools framework for retrieval, reasoning, translation, and document generation, delivering context-aware legal drafts, citations, and plain-language explanations via an interactive chat interface. Evaluated by law faculty from leading Bangladeshi universities across all stages of the 2022 and 2023 Bangladesh Bar Council Exams, Mina scored 75-80% in Preliminary MCQs, Written, and simulated Viva Voce exams, matching or surpassing average human performance and demonstrating clarity, contextual understanding, and sound legal reasoning. These results confirm its potential as a low-cost, multilingual AI assistant that automates key legal tasks and scales access to justice, offering a real-world case study on building domain-specific, low-resource systems and addressing challenges of multilingual adaptation, efficiency, and sustainable public-service AI deployment.", "AI": {"tldr": "研发出了针对孟加拉国的多语言法律助手Mina，该系统进行了多次测试并在模拟考试中表现良好，显示出其具备自动执行关键法律任务和扩大法律服务的潜力。", "motivation": "孟加拉国低收入群体在获得负担得起的法律咨询方面面临障碍，现有AI法律助手缺乏孟加拉语支持和具体的司法区域适应性。为了应对这些挑战，本研究旨在提供一个解决方案。", "method": "开发了Mina，一个多语言大型语言模型（LLM）基础的法律助手，专为孟加拉国的背景定制。它采用多语言嵌入和基于RAG的工具链框架进行检索、推理、翻译和文档生成，提供互动聊天界面，能够提供语境感知的法律草稿、引用和通俗解释。", "result": "通过孟加拉国领先大学的法学教授对2022年和2023年孟加拉国律师公会考试的各阶段的评估，Mina在初步选择题、笔试和模拟口头考试中得分75-80%，达到或超过了平均人类表现，展示了清晰度、语境理解能力和良好的法律推理能力。", "conclusion": "研究结果验证了低资源、多语言AI助手在自动执行主要法律任务和扩大司法准入方面的潜力。Mina作为一个真实世界的案例，展示了构建特定领域系统并在多语言适应性、效率和可持续公共服务AI部署方面克服挑战的可能性。"}}
{"id": "2511.08666", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.08666", "abs": "https://arxiv.org/abs/2511.08666", "authors": ["Joseph Fioresi", "Ishan Rajendrakumar Dave", "Mubarak Shah"], "title": "Privacy Beyond Pixels: Latent Anonymization for Privacy-Preserving Video Understanding", "comment": null, "summary": "We introduce a novel formulation of visual privacy preservation for video foundation models that operates entirely in the latent space. While spatio-temporal features learned by foundation models have deepened general understanding of video content, sharing or storing these extracted visual features for downstream tasks inadvertently reveals sensitive personal information like skin color, gender, or clothing. Current privacy preservation methods focus on input-pixel-level anonymization, which requires retraining the entire utility video model and results in task-specific anonymization, making them unsuitable for recent video foundational models. To address these challenges, we introduce a lightweight Anonymizing Adapter Module (AAM) that removes private information from video features while retaining general task utility. AAM can be applied in a plug-and-play fashion to frozen video encoders, minimizing the computational burden of finetuning and re-extracting features. Our framework employs three newly designed training objectives: (1) a clip-level self-supervised privacy objective to reduce mutual information between static clips, (2) a co-training objective to retain utility across seen tasks, and (3) a latent consistency loss for generalization on unseen tasks. Our extensive evaluations demonstrate a significant 35% reduction in privacy leakage while maintaining near-baseline utility performance across various downstream tasks: Action Recognition (Kinetics400, UCF101, HMDB51), Temporal Action Detection (THUMOS14), and Anomaly Detection (UCF-Crime). We also provide an analysis on anonymization for sensitive temporal attribute recognition. Additionally, we propose new protocols for assessing gender bias in action recognition models, showing that our method effectively mitigates such biases and promotes more equitable video understanding.", "AI": {"tldr": "This research presents an Anonymizing Adapter Module (AAM) to preserve privacy in video foundation models by removing private information from video features while retaining utility, reducing privacy leakage significantly without detracting from task performance.", "motivation": "The motivation behind this research is to address the privacy risks associated with sharing or storing visual features extracted from video content, such as sensitive personal information, while also preserving the utility of these features for various downstream tasks.", "method": "Our method introduces a lightweight Anonymizing Adapter Module (AAM) that can be applied to frozen video encoders to remove private information from video features without the need for retraining the entire model. It employs a clip-level self-supervised privacy objective, a co-training objective, and a latent consistency loss.", "result": "The results demonstrate a 35% reduction in privacy leakage while maintaining near-baseline utility performance across various downstream tasks, including Action Recognition, Temporal Action Detection, and Anomaly Detection, and effectively mitigate gender bias in action recognition models.", "conclusion": "The conclusion is that the proposed lightweight AAM effectively mitigates privacy risks and maintains utility performance without requiring expensive retraining, making it suitable for recent video foundational models."}}
{"id": "2511.08614", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.08614", "abs": "https://arxiv.org/abs/2511.08614", "authors": ["Sergey K. Aityan", "Abdolreza Mosaddegh", "Rolando Herrero", "Haitham Tayyar", "Jiang Han", "Vikram Sawant", "Qi Chen", "Rishabh Jain", "Aruna Senthamaraikannan", "Stephen Wood", "Manuel Mersini", "Rita Lazzaro", "Mario Balzaneli", "Nicola Iacovazzo", "Ciro Gargiulo Isacco"], "title": "A Super-Learner with Large Language Models for Medical Emergency Advising", "comment": "12 pages, 3 figures, 2 tables", "summary": "Medical decision-support and advising systems are critical for emergency physicians to quickly and accurately assess patients' conditions and make diagnosis. Artificial Intelligence (AI) has emerged as a transformative force in healthcare in recent years and Large Language Models (LLMs) have been employed in various fields of medical decision-support systems. We studied responses of a group of different LLMs to real cases in emergency medicine. The results of our study on five most renown LLMs showed significant differences in capabilities of Large Language Models for diagnostics acute diseases in medical emergencies with accuracy ranging between 58% and 65%. This accuracy significantly exceeds the reported accuracy of human doctors. We built a super-learner MEDAS (Medical Emergency Diagnostic Advising System) of five major LLMs - Gemini, Llama, Grok, GPT, and Claude). The super-learner produces higher diagnostic accuracy, 70%, even with a quite basic meta-learner. However, at least one of the integrated LLMs in the same super-learner produces 85% correct diagnoses. The super-learner integrates a cluster of LLMs using a meta-learner capable of learning different capabilities of each LLM to leverage diagnostic accuracy of the model by collective capabilities of all LLMs in the cluster. The results of our study showed that aggregated diagnostic accuracy provided by a meta-learning approach exceeds that of any individual LLM, suggesting that the super-learner can take advantage of the combined knowledge of the medical datasets used to train the group of LLMs.", "AI": {"tldr": "研究了五种知名大型语言模型在紧急医学病例中的诊断准确率，并构建了一个超级学习者MEDAS系统，结合这些模型的集体能力，提高了诊断的准确性。", "motivation": "评估大型语言模型在紧急医学诊断中的有效性，探索如何通过组合多个模型提高诊断准确率。", "method": "选择了五种知名大型语言模型（Gemini, Llama, Grok, GPT, 和 Claude），研究其在真实紧急医学案例中的反应。构建了一个超级学习者MEDAS系统，该系统使用元学习器整合这些模型的诊断能力。", "result": "五种大型语言模型的诊断准确率在58%至65%之间，显著超过了人类医生的诊断准确率。超级学习者系统的诊断准确率为70%，而群组中的一个模型达到了85%的准确率。", "conclusion": "超级学习者系统的诊断准确率优于任何一个单独的大型语言模型，这表明这种组合模型的方法可以有效利用医学数据集中的知识。"}}
{"id": "2511.08704", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.08704", "abs": "https://arxiv.org/abs/2511.08704", "authors": ["Xinchen Yan", "Chen Liang", "Lijun Yu", "Adams Wei Yu", "Yifeng Lu", "Quoc V. Le"], "title": "Rethinking generative image pretraining: How far are we from scaling up next-pixel prediction?", "comment": null, "summary": "This paper investigates the scaling properties of autoregressive next-pixel prediction, a simple, end-to-end yet under-explored framework for unified vision models. Starting with images at resolutions of 32x32, we train a family of Transformers using IsoFlops profiles across compute budgets up to 7e19 FLOPs and evaluate three distinct target metrics: next-pixel prediction objective, ImageNet classification accuracy, and generation quality measured by Fr'echet Distance. First, optimal scaling strategy is critically task-dependent. At a fixed 32x32 resolution alone, the optimal scaling properties for image classification and image generation diverge, where generation optimal setup requires the data size grow three to five times faster than for the classification optimal setup. Second, as image resolution increases, the optimal scaling strategy indicates that the model size must grow much faster than data size. Surprisingly, by projecting our findings, we discover that the primary bottleneck is compute rather than the amount of training data. As compute continues to grow four to five times annually, we forecast the feasibility of pixel-by-pixel modeling of images within the next five years.", "AI": {"tldr": "This study examines the scaling properties for different tasks within a unified vision model framework using autoregressive next-pixel prediction. Findings suggest that task-specific optimal scaling strategies are needed and compute power is the key bottleneck for future models.", "motivation": "The motivation is to investigate the scaling properties of autoregressive next-pixel prediction across different tasks within a unified vision model framework.", "method": "This paper uses a family of Transformers with IsoFlops profiles to train and evaluate models at various resolutions and FLOPs budgets. The models are tested on next-pixel prediction, ImageNet classification accuracy, and generation quality.", "result": "The study finds that the optimal scaling strategy for tasks like image classification and image generation differs significantly. It also reveals that as resolution increases, the model size needs to grow faster than the data size, and that compute is the main limiting factor rather than data.", "conclusion": "The paper concludes that with the annual compute growth rate, pixel-by-pixel modeling of images could become feasible in the next five years."}}
{"id": "2511.08620", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.08620", "abs": "https://arxiv.org/abs/2511.08620", "authors": ["Yibai Liu", "Shihang Wang", "Zeming Liu", "Zheming Song", "Junzhe Wang", "Jingjing Liu", "Qingjie Liu", "Yunhong Wang"], "title": "Learn More, Forget Less: A Gradient-Aware Data Selection Approach for LLM", "comment": "Under review", "summary": "Despite large language models (LLMs) have achieved impressive achievements across numerous tasks, supervised fine-tuning (SFT) remains essential for adapting these models to specialized domains. However, SFT for domain specialization can be resource-intensive and sometimes leads to a deterioration in performance over general capabilities due to catastrophic forgetting (CF). To address these issues, we propose a self-adaptive gradient-aware data selection approach (GrADS) for supervised fine-tuning of LLMs, which identifies effective subsets of training data by analyzing gradients obtained from a preliminary training phase. Specifically, we design self-guided criteria that leverage the magnitude and statistical distribution of gradients to prioritize examples that contribute the most to the model's learning process. This approach enables the acquisition of representative samples that enhance LLMs understanding of domain-specific tasks. Through extensive experimentation with various LLMs across diverse domains such as medicine, law, and finance, GrADS has demonstrated significant efficiency and cost-effectiveness. Remarkably, utilizing merely 5% of the selected GrADS data, LLMs already surpass the performance of those fine-tuned on the entire dataset, and increasing to 50% of the data results in significant improvements! With catastrophic forgetting substantially mitigated simultaneously. We will release our code for GrADS later.", "AI": {"tldr": "本文提出了一种名为GrADS的自我适应梯度感知数据选择方法，旨在提高大型语言模型在特定领域的效率和效果，同时减少资源需求和避免灾难性遗忘。", "motivation": "鉴于LLMs在许多任务中表现出色，但针对特定领域的监督微调(SFT)既耗资源且可能导致因灾难性遗忘而降低模型性能的问题，本文旨在提出一种解决方案。", "method": "本文提出了一种自我适应的梯度感知数据选择方法(GrADS)，通过分析初步训练阶段获得的梯度来识别有效的训练子集，以此选择对模型学习过程贡献最大的样本，提高LLMs对特定领域任务的理解能力。", "result": "实验显示，仅使用5%的通过GrADS选出的数据，LLMs的性能就已经超过了使用整个数据集微调的模型，当数据量增至50%时，性能显著提升。同时，灾难性遗忘问题得到了大幅缓解。", "conclusion": "GrADS展示出显著的有效性和成本效益，且通过减少灾难性遗忘，能有效提升模型在不同领域的性能。"}}
{"id": "2511.08711", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.08711", "abs": "https://arxiv.org/abs/2511.08711", "authors": ["Abhipsa Basu", "Aviral Gupta", "Abhijnya Bhat", "R. Venkatesh Babu"], "title": "Harnessing Diffusion-Generated Synthetic Images for Fair Image Classification", "comment": null, "summary": "Image classification systems often inherit biases from uneven group representation in training data. For example, in face datasets for hair color classification, blond hair may be disproportionately associated with females, reinforcing stereotypes. A recent approach leverages the Stable Diffusion model to generate balanced training data, but these models often struggle to preserve the original data distribution. In this work, we explore multiple diffusion-finetuning techniques, e.g., LoRA and DreamBooth, to generate images that more accurately represent each training group by learning directly from their samples. Additionally, in order to prevent a single DreamBooth model from being overwhelmed by excessive intra-group variations, we explore a technique of clustering images within each group and train a DreamBooth model per cluster. These models are then used to generate group-balanced data for pretraining, followed by fine-tuning on real data. Experiments on multiple benchmarks demonstrate that the studied finetuning approaches outperform vanilla Stable Diffusion on average and achieve results comparable to SOTA debiasing techniques like Group-DRO, while surpassing them as the dataset bias severity increases.", "AI": {"tldr": "研究通过扩散微调技术如LoRA和DreamBooth生成平衡训练数据，旨在减少图像分类中的类别偏差，实验表明这种方法优于传统稳定扩散技术并在数据偏差严重时表现更好。", "motivation": "图像分类系统常常继承了训练数据中不同组表示不均一带来的偏差。例如，用于发色分类的面部数据集中，金发可能与女性更有关联，从而强化了刻板印象。最近的一种方法使用稳定扩散模型来生成平衡的训练数据，但这些模型往往难以保持原有的数据分布。", "method": "该研究探索了多种扩散微调技术，例如LoRA和DreamBooth，以生成更准确表示每个训练组的图像。此外，为了防止单个DreamBooth模型被过多的组内变化所影响，该研究采用了将每个组内的图像聚类，并为每个聚类训练一个DreamBooth模型的方法。这些模型生成的组平衡数据用于预训练，随后在真实数据上进行微调。", "result": "在多个基准上的实验表明，所研究的微调方法在平均上优于普通的稳定扩散方法，并且达到了与最先进的去偏技术如Group-DRO相当的结果。随着数据集偏差严重程度的增加，这些方法甚至超过了Group-DRO等技术。", "conclusion": "研究展示了通过使用多种扩散微调技术，能够在保持原有数据分布的同时，生成平衡的训练数据。这种方法能够有效地减少类别偏差，并在多种基准测试中取得了优异的效果。"}}
{"id": "2511.08636", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.08636", "abs": "https://arxiv.org/abs/2511.08636", "authors": ["Mohaiminul Islam Bhuiyan", "Nur Shazwani Kamarudin", "Nur Hafieza Ismail"], "title": "Detecting Suicidal Ideation in Text with Interpretable Deep Learning: A CNN-BiGRU with Attention Mechanism", "comment": "6 pages, 4 figures, 2025 IEEE 9th International Conference on Software Engineering & Computer Systems", "summary": "Worldwide, suicide is the second leading cause of death for adolescents with past suicide attempts to be an important predictor for increased future suicides. While some people with suicidal thoughts may try to suppress them, many signal their intentions in social media platforms. To address these issues, we propose a new type of hybrid deep learning scheme, i.e., the combination of a CNN architecture and a BiGRU technique, which can accurately identify the patterns of suicidal ideation from SN datasets. Also, we apply Explainable AI methods using SHapley Additive exPlanations to interpret the prediction results and verifying the model reliability. This integration of CNN local feature extraction, BiGRU bidirectional sequence modeling, attention mechanisms, and SHAP interpretability provides a comprehensive framework for suicide detection. Training and evaluation of the system were performed on a publicly available dataset. Several performance metrics were used for evaluating model performance. Our method was found to have achieved 93.97 accuracy in experimental results. Comparative study to different state-of-the-art Machine Learning and DL models and existing literature demonstrates the superiority of our proposed technique over all the competing methods.", "AI": {"tldr": "本文提出了一种结合CNN与BiGRU的混合深度学习方案，用于准确识别社交网络数据中自杀意念的模式，并使用SHAP方法解释预测结果。实验结果表明该方法准确率达到93.97%。", "motivation": "鉴于自杀是全球青少年第二大死因，且社交媒体可以作为自杀信号的重要来源，本文旨在开发一种准确预测自杀意念的工具。", "method": "本文采用CNN和BiGRU的深度学习架构结合注意力机制，通过SHAP解释预测结果的可解释人工智能方法。", "result": "在公开数据集上的实验表明，该方法实现了93.97%的准确率，并且在性能上优于现有的机器学习和深度学习模型。", "conclusion": "结合CNN、BiGRU和SHAP的混合深度学习方法在自杀意念检测中显示出卓越的性能和可解释性。"}}
{"id": "2511.08748", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.08748", "abs": "https://arxiv.org/abs/2511.08748", "authors": ["Estefania Talavera", "Deblina Bhattacharjee", "Himangi Mittal", "Mengwei Ren", "Karen Sanchez", "Carla Muntean", "JungEun Kim", "Mona Jalal"], "title": "WiCV at CVPR 2025: The Women in Computer Vision Workshop", "comment": null, "summary": "The Women in Computer Vision Workshop (WiCV@CVPR 2025) was held in conjunction with the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR 2025) in Nashville, Tennessee, United States. This report presents an overview of the workshop program, participation statistics, mentorship outcomes, and historical trends from previous WiCV editions. The goal is to document the impact and evolution of WiCV as a reference for future editions and for other initiatives aimed at advancing diversity, equity, and inclusion within the AI and computer vision communities. WiCV@CVPR 2025 marked the 16th edition of this long-standing event dedicated to increasing the visibility, inclusion, and professional growth of women and underrepresented minorities in the computer vision community. This year's workshop featured 14 accepted papers in the CVPR Workshop Proceedings out of 32 full-paper submissions. Five of these were selected for oral presentations, while all 14 were also presented as posters, along with 36 extended abstract posters accepted from 62 short-paper submissions, which are not included in the proceedings. The mentoring program matched 80 mentees with 37 mentors from both academia and industry. The 2025 edition attracted over 100 onsite participants, fostering rich technical and networking interactions across all career stages. Supported by 10 sponsors and approximately $44,000 USD in travel grants and diversity awards, WiCV continued its mission to empower emerging researchers and amplify diverse voices in computer vision.", "AI": {"tldr": "WiCV@CVPR 2025 在田纳西州纳什维尔举行，记录了工作坊的项目概览、参与统计数据、导师成果和历史趋势，以推进人工智能和计算机视觉社区内的多样性、公平性和包容性。", "motivation": "记录 WiCV 的影响和演变，为未来版提供参考，并推动 AI 和计算机视觉社区内的多元包容性。", "method": "通过报告工作坊的项目概览、参与统计数据、导师成果和历史趋势来分析 WiCV 的影响。", "result": "WiCV@CVPR 2025 接受了 14 篇论文和 36 个延长摘要，有 100 多名现场参与者，80 名被指导者与来自学术界和行业的 37 名导师匹配。", "conclusion": "WiCV 继续为新兴研究人员赋能，并放大计算机视觉领域的多元声音。"}}
{"id": "2511.08798", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.08798", "abs": "https://arxiv.org/abs/2511.08798", "authors": ["Manan Suri", "Puneet Mathur", "Nedim Lipka", "Franck Dernoncourt", "Ryan A. Rossi", "Dinesh Manocha"], "title": "Structured Uncertainty guided Clarification for LLM Agents", "comment": null, "summary": "LLM agents extend large language models with tool-calling capabilities, but ambiguous user instructions often lead to incorrect invocations and task failures. We introduce a principled formulation of structured uncertainty over tool-call parameters, modeling joint tool-argument clarification as a POMDP with Expected Value of Perfect Information (EVPI) objective for optimal question selection and aspect-based cost modeling to prevent redundancy. Our SAGE-Agent leverages this structured uncertainty to achieve superior efficiency: increasing coverage on ambiguous tasks by 7-39\\% while reducing clarification questions by 1.5-2.7$\\times$ compared to strong prompting and uncertainty-based baselines. We present ClarifyBench, the first multi-turn tool-augmented disambiguation benchmark with realistic LLM-based user simulation across diverse domains including document editing, vehicle control, and travel booking. Additionally, we demonstrate that structured uncertainty provides effective training signals for reinforcement learning, boosting When2Call accuracy from 36.5\\% to 65.2\\% (3B model) and 36.7\\% to 62.9\\% (7B model) through uncertainty-weighted GRPO training. These results establish structured uncertainty as a principled, efficient approach for tool-augmented agents, improving both task success and interaction efficiency in real-world scenarios.", "AI": {"tldr": "本文提出了一种基于POMDP和EVPI的结构化不确定性模型，通过SAGE-Agent实现了更高效的模糊任务处理，提高了覆盖率并减少了问题数量。结构化不确定性还提供了有效的强化学习训练信号，提高了工具调用的准确性。", "motivation": "解决LLM代理因用户指令模糊而导致的工具调用错误和任务失败问题。", "method": "提出了一种基于POMDP和EVPI目标的结构化不确定性模型，用于改进LLM代理的工具调用参数澄清过程。此外，还提出了一种基于方面的成本建模方法，以避免冗余。SAGE-Agent利用这种结构化不确定性模型，在处理模糊任务时，相较于强提示和基于不确定性基准的方法，提高了7-39%的覆盖率并减少了1.5-2.7倍的问题数量。", "result": "与基准方法相比，SAGE-Agent将模糊任务的覆盖率提高了7-39%，减少了1.5-2.7倍的澄清问题数量。通过不确定性加权的GRPO训练，当2调用的准确性从36.5%提高到65.2%（3B模型）和36.7%提高到62.9%（7B模型）。", "conclusion": "结构化不确定性为工具增强代理提供了一种高效的原则性方法，提升了实际场景中的任务成功率和交互效率。"}}
{"id": "2511.08809", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.08809", "abs": "https://arxiv.org/abs/2511.08809", "authors": ["Abu Taib Mohammed Shahjahan", "A. Ben Hamza"], "title": "Adaptive graph Kolmogorov-Arnold network for 3D human pose estimation", "comment": null, "summary": "Graph convolutional network (GCN)-based methods have shown strong performance in 3D human pose estimation by leveraging the natural graph structure of the human skeleton. However, their local receptive field limits their ability to capture long-range dependencies essential for handling occlusions and depth ambiguities. They also exhibit spectral bias, which prioritizes low-frequency components while struggling to model high-frequency details. In this paper, we introduce PoseKAN, an adaptive graph Kolmogorov-Arnold Network (KAN), framework that extends KANs to graph-based learning for 2D-to-3D pose lifting from a single image. Unlike GCNs that use fixed activation functions, KANs employ learnable functions on graph edges, allowing data-driven, adaptive feature transformations. This enhances the model's adaptability and expressiveness, making it more expressive in learning complex pose variations. Our model employs multi-hop feature aggregation, ensuring the body joints can leverage information from both local and distant neighbors, leading to improved spatial awareness. It also incorporates residual PoseKAN blocks for deeper feature refinement, and a global response normalization for improved feature selectivity and contrast. Extensive experiments on benchmark datasets demonstrate the competitive performance of our model against state-of-the-art methods.", "AI": {"tldr": "A new model called PoseKAN is proposed to address the limitations of GCNs in 3D human pose estimation. PoseKAN extends the KAN framework to graph-based learning, using learnable functions for improved feature transformation and aggregation, as well as a global response normalization for better feature selectivity and contrast.", "motivation": "The motivation behind this paper is to improve upon the limitations of Graph Convolutional Networks (GCN) in 3D human pose estimation. GCNs are limited by their local receptive fields and spectral bias, which hinder their ability to capture important long-range dependencies and model high-frequency details, thus making it difficult to accurately handle occlusions and depth ambiguities.", "method": "An adaptive graph Kolmogorov-Arnold Network (PoseKAN) framework is introduced to enhance 2D-to-3D pose lifting from a single image. PoseKAN extends the use of KANs to graph-based learning, employing learnable functions on graph edges rather than fixed activation functions, to better handle complex pose variations. Multi-hop feature aggregation and residual PoseKAN blocks are used to refine features, and global response normalization is applied to improve feature selectivity and contrast.", "result": "Extensive experiments on benchmark datasets show that the PoseKAN model performs competitively against state-of-the-art methods in 3D human pose estimation tasks, indicating its effectiveness in handling complex pose variations and improving spatial awareness.", "conclusion": "The proposed PoseKAN model, which enhances adaptive feature transformations through learnable functions in a graph-based setting and employs techniques like multi-hop feature aggregation and residual blocks, demonstrates competitive performance in 2D-to-3D pose lifting against state-of-the-art methods. This underscores its potential for improving 3D human pose estimation tasks, particularly in handling complex pose variations, occlusions, and depth ambiguities."}}
{"id": "2511.08806", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.08806", "abs": "https://arxiv.org/abs/2511.08806", "authors": ["Varada Khanna", "Nilay Bhatt", "Ikgyu Shin", "Sule Tinaz", "Yang Ren", "Hua Xu", "Vipina K. Keloth"], "title": "Toward Automated Cognitive Assessment in Parkinson's Disease Using Pretrained Language Models", "comment": "15 pages, 4 figures, 1 table. Varada Khanna and Nilay Bhatt are co-first authors. Sule Tinaz and Hua Xu are co-senior authors. Corresponding author: Vipina K. Keloth (vipina.kuttichikeloth@yale.edu)", "summary": "Understanding how individuals with Parkinson's disease (PD) describe cognitive experiences in their daily lives can offer valuable insights into disease-related cognitive and emotional changes. However, extracting such information from unstructured patient narratives is challenging due to the subtle, overlapping nature of cognitive constructs. This study developed and evaluated natural language processing (NLP) models to automatically identify categories that reflect various cognitive processes from de-identified first-person narratives. Three model families, a Bio_ClinicalBERT-based span categorization model for nested entity recognition, a fine-tuned Meta-Llama-3-8B-Instruct model using QLoRA for instruction following, and GPT-4o mini evaluated under zero- and few-shot settings, were compared on their performance on extracting seven categories. Our findings indicated that model performance varied substantially across categories and model families. The fine-tuned Meta-Llama-3-8B-Instruct achieved the highest overall F1-scores (0.74 micro-average and 0.59 macro-average), particularly excelling in context-dependent categories such as thought and social interaction. Bio_ClinicalBERT exhibited high precision but low recall and performed comparable to Llama for some category types such as location and time but failed on other categories such as thought, emotion and social interaction. Compared to conventional information extraction tasks, this task presents a greater challenge due to the abstract and overlapping nature of narrative accounts of complex cognitive processes. Nonetheless, with continued refinement, these NLP systems hold promise for enabling low-burden, longitudinal monitoring of cognitive function and serving as a valuable complement to formal neuropsychological assessments in PD.", "AI": {"tldr": "本研究开发了自然语言处理模型，用以自动识别帕金森病患者自述中反映不同类型认知过程的类别，并评估了其表现。其中，经过微调的Meta-Llama-3-8B-Instruct模型在识别与情境依赖类别上表现最好，但整体任务由于叙述中复杂认知过程的抽象性和重叠性而具有挑战。", "motivation": "理解帕金森病患者如何描述日常生活中的认知体验，可以获得宝贵的信息，但从中提取信息具有挑战性。", "method": "本研究开发并评估了自然语言处理模型，包括基于Bio_ClinicalBERT的嵌套实体识别模型、使用QLoRA的Meta-Llama-3-8B-Instruct指令跟随模型和零样本/少量样本的GPT-4o mini，以识别患者的叙述中的七个类别。", "result": "研究表明，不同类别和模型之间的性能差异明显。其中，Meta-Llama-3-8B-Instruct模型在识别情境相关类别如思考和社会互动方面表现最佳，而Bio_ClinicalBERT模型在部分类别上表现较好，但在思考、情绪和社交互动类别上表现不佳。", "conclusion": "尽管任务挑战性较大，但这些自然语言处理系统有望成为低负担、纵向监测认知功能的重要工具，补充正式的神经心理学评估。"}}
{"id": "2511.08810", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.08810", "abs": "https://arxiv.org/abs/2511.08810", "authors": ["Jingjie He", "Weijie Liang", "Zihan Shan", "Matthew Caesar"], "title": "SIFT-Graph: Benchmarking Multimodal Defense Against Image Adversarial Attacks With Robust Feature Graph", "comment": "Accepted by ICCV2025 Workshop, short paper", "summary": "Adversarial attacks expose a fundamental vulnerability in modern deep vision models by exploiting their dependence on dense, pixel-level representations that are highly sensitive to imperceptible perturbations. Traditional defense strategies typically operate within this fragile pixel domain, lacking mechanisms to incorporate inherently robust visual features. In this work, we introduce SIFT-Graph, a multimodal defense framework that enhances the robustness of traditional vision models by aggregating structurally meaningful features extracted from raw images using both handcrafted and learned modalities. Specifically, we integrate Scale-Invariant Feature Transform keypoints with a Graph Attention Network to capture scale and rotation invariant local structures that are resilient to perturbations. These robust feature embeddings are then fused with traditional vision model, such as Vision Transformer and Convolutional Neural Network, to form a unified, structure-aware and perturbation defensive model. Preliminary results demonstrate that our method effectively improves the visual model robustness against gradient-based white box adversarial attacks, while incurring only a marginal drop in clean accuracy.", "AI": {"tldr": "提出SIFT-Graph框架，通过结合SIFT关键点和图注意力网络，将鲁棒特征与传统视觉模型结合，以增强视觉模型对抗攻击的防御能力。", "motivation": "现代深度视觉模型依赖于对轻微扰动极端敏感的密集像素级表示，容易受到对抗攻击。传统防御策略通常在脆弱的像素域内工作，缺乏将固有鲁棒视觉特征整合的机制。", "method": "SIFT-Graph，一种多模态防御框架，通过整合从原始图像中提取的结构上具有重要意义的手工和学习模式特征来增强传统视觉模型的鲁棒性。具体来说，SIFT-Graph 将尺度不变特征变换关键点与图注意力网络相结合，以捕捉对扰动有抵抗力的尺度和旋转不变局部结构。这些鲁棒特征嵌入与传统视觉模型（如视觉变换器和卷积神经网络）融合，形成结构感知和抗扰模型。", "result": "初步结果显示，该方法可以有效提高视觉模型对抗梯度基白盒对抗攻击的鲁棒性，同时仅导致清洁精度略有下降。", "conclusion": "SIFT-Graph能够增强对对抗攻击的防御能力，同时保持较高的清洁样本准确性，是一种有效的防御机制。"}}
{"id": "2511.08813", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.08813", "abs": "https://arxiv.org/abs/2511.08813", "authors": ["Farah Binta Haque", "Md Yasin", "Shishir Saha", "Md Shoaib Akhter Rafi", "Farig Sadeque"], "title": "BNLI: A Linguistically-Refined Bengali Dataset for Natural Language Inference", "comment": null, "summary": "Despite the growing progress in Natural Language Inference (NLI) research, resources for the Bengali language remain extremely limited. Existing Bengali NLI datasets exhibit several inconsistencies, including annotation errors, ambiguous sentence pairs, and inadequate linguistic diversity, which hinder effective model training and evaluation. To address these limitations, we introduce BNLI, a refined and linguistically curated Bengali NLI dataset designed to support robust language understanding and inference modeling. The dataset was constructed through a rigorous annotation pipeline emphasizing semantic clarity and balance across entailment, contradiction, and neutrality classes. We benchmarked BNLI using a suite of state-of-the-art transformer-based architectures, including multilingual and Bengali-specific models, to assess their ability to capture complex semantic relations in Bengali text. The experimental findings highlight the improved reliability and interpretability achieved with BNLI, establishing it as a strong foundation for advancing research in Bengali and other low-resource language inference tasks.", "AI": {"tldr": "为解决现有孟加拉语NLI数据集在有效性及多样性上的局限性，研究引入了BNLI数据集，提高了模型捕获孟加拉语文本复杂语义关系的能力。", "motivation": "现有的孟加拉语NLI数据集存在一些不一致，包括注释错误、模糊语句对以及语言多样性不足，这阻碍了有效模型训练和评估。", "method": "通过严格的注释流程构建了BNLI，强调语义清晰并在蕴涵、矛盾和中立类别之间保持平衡。", "result": "实验结果表明，使用BNLI可以实现更高的可靠性和可解释性，为推进孟加拉语及其他低资源语言的推理任务研究奠定了坚实基础。", "conclusion": "BNLI作为经过精心策划的孟加拉语NLI数据集，为强大语言理解和推理建模提供了支持。"}}
{"id": "2511.08823", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.08823", "abs": "https://arxiv.org/abs/2511.08823", "authors": ["Wonbong Jang", "Jonathan Tremblay", "Lourdes Agapito"], "title": "DT-NVS: Diffusion Transformers for Novel View Synthesis", "comment": "14 pages", "summary": "Generating novel views of a natural scene, e.g., every-day scenes both indoors and outdoors, from a single view is an under-explored problem, even though it is an organic extension to the object-centric novel view synthesis. Existing diffusion-based approaches focus rather on small camera movements in real scenes or only consider unnatural object-centric scenes, limiting their potential applications in real-world settings. In this paper we move away from these constrained regimes and propose a 3D diffusion model trained with image-only losses on a large-scale dataset of real-world, multi-category, unaligned, and casually acquired videos of everyday scenes. We propose DT-NVS, a 3D-aware diffusion model for generalized novel view synthesis that exploits a transformer-based architecture backbone. We make significant contributions to transformer and self-attention architectures to translate images to 3d representations, and novel camera conditioning strategies to allow training on real-world unaligned datasets. In addition, we introduce a novel training paradigm swapping the role of reference frame between the conditioning image and the sampled noisy input. We evaluate our approach on the 3D task of generalized novel view synthesis from a single input image and show improvements over state-of-the-art 3D aware diffusion models and deterministic approaches, while generating diverse outputs.", "AI": {"tldr": "Proposes DT-NVS, a 3D-aware diffusion model with transformer architecture for novel view synthesis from a single image, trained on real-world videos. Shows improvements over existing models and deterministic approaches.", "motivation": "To address the limitations of existing diffusion-based approaches restricted to small camera movements or unnatural object-centric scenes, aiming to enable more applications in real-world settings.", "method": "Generating novel views of natural scenes from a single view using a 3D diffusion model trained on a large-scale dataset of real-world videos with a transformer-based backbone. Novel camera conditioning strategies and a new training paradigm where the reference frame role is swapped between the conditioning image and the sampled noisy input are introduced.", "result": "Demonstrates improvements over state-of-the-art 3D aware diffusion models and deterministic approaches in the 3D task of novel view synthesis from a single input image.", "conclusion": "Successfully generates diverse outputs for novel view synthesis using a 3D diffusion model trained on unaligned real-world data, expanding the potential applications in real-world settings."}}
{"id": "2511.08835", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.08835", "abs": "https://arxiv.org/abs/2511.08835", "authors": ["Yejin Yoon", "Yuri Son", "Namyoung So", "Minseo Kim", "Minsoo Cho", "Chanhee Park", "Seungshin Lee", "Taeuk Kim"], "title": "Beyond Task-Oriented and Chitchat Dialogues: Proactive and Transition-Aware Conversational Agents", "comment": "accepted to EMNLP2025", "summary": "Conversational agents have traditionally been developed for either task-oriented dialogue (TOD) or open-ended chitchat, with limited progress in unifying the two. Yet, real-world conversations naturally involve fluid transitions between these modes. To address this gap, we introduce TACT (TOD-And-Chitchat Transition), a dataset designed for transition-aware dialogue modeling that incorporates structurally diverse and integrated mode flows. TACT supports both user- and agent-driven mode switches, enabling robust modeling of complex conversational dynamics. To evaluate an agent's ability to initiate and recover from mode transitions, we propose two new metrics -- Switch and Recovery. Models trained on TACT outperform baselines in both intent detection and mode transition handling. Moreover, applying Direct Preference Optimization (DPO) to TACT-trained models yields additional gains, achieving 75.74\\% joint mode-intent accuracy and a 70.1\\% win rate against GPT-4o in human evaluation. These results demonstrate that pairing structurally diverse data with DPO enhances response quality and transition control, paving the way for more proactive and transition-aware conversational agents.", "AI": {"tldr": "本文介绍了TACT数据集，用于支持任务导向和闲聊间的自然转换对话生成。通过该数据集进行训练并结合DPO优化，能够生成更高质量的对话回复并更好地控制模式转换。", "motivation": "传统的对话代理通常被开发成支持任务导向对话（TOD）或开放式闲聊对话，很少有统一这两种对话模式的工作。然而，现实世界的对话中两种模式的转换是自然而然发生的。为了填补这一研究空白，本文提出了TACT数据集。", "method": "本文提出TACT（TOD-And-Chitchat Transition）数据集，用于对话模式转换的建模。TACT支持用户驱动和代理驱动的模式切换，以准确模拟复杂的对话动态。同时，为评估模式转换能力，本文提出了两个新指标——Switch和Recovery。", "result": "在TACT数据集上训练的模型在意图识别和模式转换处理上明显优于基线模型。采用直接偏好优化（DPO）对TACT训练过的模型进行进一步优化，达到了75.74%的联合模式意图准确性以及与GPT-4o对比时的人类评估胜率为70.1%。", "conclusion": "本文的结果证明结构性多样化数据和DPO的结合可以显著提升对话代理的回复质量和模式转换控制能力，推动更主动和转换感知对话代理的发展。"}}
{"id": "2511.08833", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.08833", "abs": "https://arxiv.org/abs/2511.08833", "authors": ["Jiaxun Guo", "Manar Amayri", "Nizar Bouguila", "Xin Liu", "Wentao Fan"], "title": "Enhancing Rotation-Invariant 3D Learning with Global Pose Awareness and Attention Mechanisms", "comment": "14 pages, 6 gigures,AAAI 2026", "summary": "Recent advances in rotation-invariant (RI) learning for 3D point clouds typically replace raw coordinates with handcrafted RI features to ensure robustness under arbitrary rotations. However, these approaches often suffer from the loss of global pose information, making them incapable of distinguishing geometrically similar but spatially distinct structures. We identify that this limitation stems from the restricted receptive field in existing RI methods, leading to Wing-tip feature collapse, a failure to differentiate symmetric components (e.g., left and right airplane wings) due to indistinguishable local geometries. To overcome this challenge, we introduce the Shadow-informed Pose Feature (SiPF), which augments local RI descriptors with a globally consistent reference point (referred to as the 'shadow') derived from a learned shared rotation. This mechanism enables the model to preserve global pose awareness while maintaining rotation invariance. We further propose Rotation-invariant Attention Convolution (RIAttnConv), an attention-based operator that integrates SiPFs into the feature aggregation process, thereby enhancing the model's capacity to distinguish structurally similar components. Additionally, we design a task-adaptive shadow locating module based on the Bingham distribution over unit quaternions, which dynamically learns the optimal global rotation for constructing consistent shadows. Extensive experiments on 3D classification and part segmentation benchmarks demonstrate that our approach substantially outperforms existing RI methods, particularly in tasks requiring fine-grained spatial discrimination under arbitrary rotations.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2511.08866", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.08866", "abs": "https://arxiv.org/abs/2511.08866", "authors": ["Fuyi Yang", "Chenchen Ye", "Mingyu Derek Ma", "Yijia Xiao", "Matthew Yang", "Wei Wang"], "title": "BioVerge: A Comprehensive Benchmark and Study of Self-Evaluating Agents for Biomedical Hypothesis Generation", "comment": null, "summary": "Hypothesis generation in biomedical research has traditionally centered on uncovering hidden relationships within vast scientific literature, often using methods like Literature-Based Discovery (LBD). Despite progress, current approaches typically depend on single data types or predefined extraction patterns, which restricts the discovery of novel and complex connections. Recent advances in Large Language Model (LLM) agents show significant potential, with capabilities in information retrieval, reasoning, and generation. However, their application to biomedical hypothesis generation has been limited by the absence of standardized datasets and execution environments. To address this, we introduce BioVerge, a comprehensive benchmark, and BioVerge Agent, an LLM-based agent framework, to create a standardized environment for exploring biomedical hypothesis generation at the frontier of existing scientific knowledge. Our dataset includes structured and textual data derived from historical biomedical hypotheses and PubMed literature, organized to support exploration by LLM agents. BioVerge Agent utilizes a ReAct-based approach with distinct Generation and Evaluation modules that iteratively produce and self-assess hypothesis proposals. Through extensive experimentation, we uncover key insights: 1) different architectures of BioVerge Agent influence exploration diversity and reasoning strategies; 2) structured and textual information sources each provide unique, critical contexts that enhance hypothesis generation; and 3) self-evaluation significantly improves the novelty and relevance of proposed hypotheses.", "AI": {"tldr": "The paper introduces BioVerge and BioVerge Agent to improve biomedical hypothesis generation using Large Language Models, addressing the limitations of traditional methods and the lack of standardized databases and execution environments.", "motivation": "To enhance biomedical hypothesis discovery, which is often hampered by the reliance on single data types and the lack of comprehensive, standardized environments for large language models.", "method": "Developing BioVerge, a benchmark dataset, and BioVerge Agent, an LLM-based framework that uses a ReAct-based approach with Generation and Evaluation modules for iterative hypothesis generation and assessment.", "result": "The paper reveals that architecture variations affect exploration diversity and reasoning strategies, structured and unstructured data contribute differently to hypothesis creation, and self-assessment boosts the novelty and relevance of hypotheses.", "conclusion": "BioVerge and BioVerge Agent provide a novel, standardized approach to biomedical hypothesis generation using LLMs, improving the diversity and relevance of generated hypotheses."}}
{"id": "2511.08872", "categories": ["cs.CV", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2511.08872", "abs": "https://arxiv.org/abs/2511.08872", "authors": ["Hu Cui", "Wenqiang Hua", "Renjing Huang", "Shurui Jia", "Tessai Hayama"], "title": "SasMamba: A Lightweight Structure-Aware Stride State Space Model for 3D Human Pose Estimation", "comment": "8pages, WACV2026 accepted", "summary": "Recently, the Mamba architecture based on State Space Models (SSMs) has gained attention in 3D human pose estimation due to its linear complexity and strong global modeling capability. However, existing SSM-based methods typically apply manually designed scan operations to flatten detected 2D pose sequences into purely temporal sequences, either locally or globally. This approach disrupts the inherent spatial structure of human poses and entangles spatial and temporal features, making it difficult to capture complex pose dependencies. To address these limitations, we propose the Skeleton Structure-Aware Stride SSM (SAS-SSM), which first employs a structure-aware spatiotemporal convolution to dynamically capture essential local interactions between joints, and then applies a stride-based scan strategy to construct multi-scale global structural representations. This enables flexible modeling of both local and global pose information while maintaining linear computational complexity. Built upon SAS-SSM, our model SasMamba achieves competitive 3D pose estimation performance with significantly fewer parameters compared to existing hybrid models. The source code is available at https://hucui2022.github.io/sasmamba_proj/.", "AI": {"tldr": "提出了SAS-SSM，改进了处理人体姿态估计的方法，能够在保持线性计算复杂度的情况下灵活建模局部和全局姿态信息，并且在参数数量大幅减少的情况下仍获得了与现有混合模型竞争的3D姿态估计性能。", "motivation": "现有的基于SSM的方法通常应用手动设计的扫描操作来将检测到的2D姿态序列扁平化为纯粹的时间序列，这会破坏人体姿态的固有空间结构，并纠缠空间和时间特征，使得捕捉复杂的姿态依赖关系变得困难。", "method": "我们提出了Skeleton Structure-Aware Stride SSM (SAS-SSM)，首先使用对结构敏感的时空卷积来动态捕捉各个关节间的必要局部交互，然后应用基于步幅的扫描策略来构建多尺度全局结构表示。", "result": "基于SAS-SSM构建的模型SasMamba在3D姿态估计性能上可与其他现有混合模型竞争，但参数要少得多。", "conclusion": "SAS-SSM能够灵活地对局部和全局姿态信息进行建模，同时保持线性计算复杂度。"}}
{"id": "2511.08877", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.08877", "abs": "https://arxiv.org/abs/2511.08877", "authors": ["Junichiro Niimi"], "title": "Hallucinate or Memorize? The Two Sides of Probabilistic Learning in Large Language Models", "comment": null, "summary": "Large language models (LLMs) have been increasingly applied to a wide range of tasks, from natural language understanding to code generation. While they have also been used to assist in citation recommendation, the hallucination of non-existent papers remains a major issue. Building on prior studies, this study hypothesizes that an LLM's ability to correctly produce bibliographic records depends on whether the underlying knowledge is generated or memorized, with highly cited papers (i.e., more frequently appear in the pretraining corpus) showing lower hallucination rates. We therefore assume citation count as a proxy for training data redundancy (i.e., the frequency with which a given bibliographic record appears in the pretraining corpus) and investigate how citation frequency affects hallucinated references in LLM outputs. Using GPT-4.1, we generated and manually verified 100 citations across twenty computer-science domains, and measured factual consistency via cosine similarity between generated and authentic metadata. The results revealed that (i) citation count is strongly correlated with factual accuracy, (ii) bibliographic information becomes almost verbatim memorized beyond roughly 1,000 citations, and (iii) memory interference occurs when multiple highly cited papers share similar content. These findings indicate a threshold where generalization shifts into memorization, with highly cited papers being nearly verbatim retained in the model.", "AI": {"tldr": "研究指出，大型语言模型生成的引文准确性与引用论文的引用次数有关，高引用率的论文被模型记忆得更加准确，但也可能引起记忆干扰。", "motivation": "研究动机是探索大型语言模型（LLMs）如何生成或记忆引文，以及高引用次数的论文与低幻影率之间的关系。", "method": "该研究使用GPT-4.1生成了100个引用，并手动验证了这些引用的准确性，同时通过比较生成的和真实的元数据之间的余弦相似性来测量其事实一致性。", "result": "结果发现：(i) 引用次数和事实准确性高度相关；(ii) 超过约1000次引用后，引文信息几乎被完全记忆；(iii) 当多篇高引用论文内容相似时，会出现记忆干扰。", "conclusion": "研究结果表明，引用次数与事实准确性密切相关，超过约1000次引用后，引文信息几乎完全被模型记忆。此外，当多篇高引用论文内容相似时，会出现记忆干扰现象。"}}
{"id": "2511.08883", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.08883", "abs": "https://arxiv.org/abs/2511.08883", "authors": ["Cheng Wang", "Shuisheng Zhou", "Fengjiao Peng", "Jin Sheng", "Feng Ye", "Yinli Dong"], "title": "Improve Contrastive Clustering Performance by Multiple Fusing-Augmenting ViT Blocks", "comment": null, "summary": "In the field of image clustering, the widely used contrastive learning networks improve clustering performance by maximizing the similarity between positive pairs and the dissimilarity of negative pairs of the inputs. Extant contrastive learning networks, whose two encoders often implicitly interact with each other by parameter sharing or momentum updating, may not fully exploit the complementarity and similarity of the positive pairs to extract clustering features from input data. To explicitly fuse the learned features of positive pairs, we design a novel multiple fusing-augmenting ViT blocks (MFAVBs) based on the excellent feature learning ability of Vision Transformers (ViT). Firstly, two preprocessed augmentions as positive pairs are separately fed into two shared-weight ViTs, then their output features are fused to input into a larger ViT. Secondly, the learned features are split into a pair of new augmented positive samples and passed to the next FAVBs, enabling multiple fusion and augmention through MFAVBs operations. Finally, the learned features are projected into both instance-level and clustering-level spaces to calculate the cross-entropy loss, followed by parameter updates by backpropagation to finalize the training process. To further enhance ability of the model to distinguish between similar images, our input data for the network we propose is preprocessed augmentions with features extracted from the CLIP pretrained model. Our experiments on seven public datasets demonstrate that MFAVBs serving as the backbone for contrastive clustering outperforms the state-of-the-art techniques in terms of clustering performance.", "AI": {"tldr": "一种基于Vision Transformers设计的多融合增强块（MFAVBs）用于提高图像聚类性能，实验表明该方法优于现有技术。", "motivation": "现有对比学习网络中的两个编码器通过参数共享或动量更新方式隐式交互，可能无法充分利用正样本对之间的互补性和相似性来从输入数据中提取聚类特征。", "method": "设计了一种基于Vision Transformers (ViT) 的新型多融合增强ViT块（MFAVBs）。首先，两个预处理的正样本输入分别通过两个共享权重的ViT提取特征，然后将其输出特征融合并输入更大规模的ViT。其次，学习到的特征被分割生成新的正样本对并传递给下一个FAVBs，实现MFAVBs操作的多次融合与增强。最后，学习到的特征投射到实例级和聚类级空间，计算交叉熵损失，通过反向传播更新参数完成训练过程。为了进一步增强模型区分相似图像的能力，提出的网络输入数据经过CLIP预训练模型特征提取的预处理增强。", "result": "实验结果表明，在七个公共数据集上，作为对比聚类骨干网络的MFAVBs在聚类性能方面优于最先进的技术。", "conclusion": "提出的方法通过多融合增强块的设计提高了基于对比学习的图像聚类性能，实验验证了这种方法的优势。"}}
{"id": "2511.08916", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.08916", "abs": "https://arxiv.org/abs/2511.08916", "authors": ["Yaxin Zhao", "Yu Zhang"], "title": "HalluClean: A Unified Framework to Combat Hallucinations in LLMs", "comment": null, "summary": "Large language models (LLMs) have achieved impressive performance across a wide range of natural language processing tasks, yet they often produce hallucinated content that undermines factual reliability. To address this challenge, we introduce HalluClean, a lightweight and task-agnostic framework for detecting and correcting hallucinations in LLM-generated text. HalluClean adopts a reasoning-enhanced paradigm, explicitly decomposing the process into planning, execution, and revision stages to identify and refine unsupported claims. It employs minimal task-routing prompts to enable zero-shot generalization across diverse domains, without relying on external knowledge sources or supervised detectors. We conduct extensive evaluations on five representative tasks-question answering, dialogue, summarization, math word problems, and contradiction detection. Experimental results show that HalluClean significantly improves factual consistency and outperforms competitive baselines, demonstrating its potential to enhance the trustworthiness of LLM outputs in real-world applications.", "AI": {"tldr": "提出了一种轻量级的、无关任务的框架HalluClean，用于检测和纠正LLM生成文本中的幻觉。实验显示，HalluClean显著提高了事实的一致性并优于竞争基线。", "motivation": "大型语言模型（LLMs）在自然语言处理任务中取得了令人印象深刻的表现，但它们经常会生成事实不准确的内容。这个框架旨在检测并纠正LLMs生成文本中的幻觉内容。", "method": "HalluClean采用了一个增强推理的范式，将过程明确地分解为规划、执行和修订阶段，以识别并精炼无根据的主张。它使用了最小的任务路由提示，使零样本泛化到各种不同的领域，无需依赖外部知识来源或监督式检测器。", "result": "实验结果表明，HalluClean显著提高了事实的一致性，并优于竞争基线，在对话、总结、数学问题解答和矛盾检测五个代表性任务上表现良好。", "conclusion": "HalluClean展示了在现实世界应用中提高LLM输出可信度的潜力。"}}
{"id": "2511.08896", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.08896", "abs": "https://arxiv.org/abs/2511.08896", "authors": ["Sanyukta Adap", "Ujjwal Baid", "Spyridon Bakas"], "title": "Classifying Histopathologic Glioblastoma Sub-regions with EfficientNet", "comment": null, "summary": "Glioblastoma (GBM) is the most common aggressive, fast-growing brain tumor, with a grim prognosis. Despite clinical diagnostic advancements, there have not been any substantial improvements to patient prognosis. Histopathological assessment of excised tumors is the first line of clinical diagnostic routine. We hypothesize that automated, robust, and accurate identification of distinct histological sub-regions within GBM could contribute to morphologically understanding this disease at scale. In this study, we designed a four-step deep learning approach to classify six (6) histopathological regions and quantitatively evaluated it on the BraTS-Path 2024 challenge dataset, which includes digitized Hematoxylin \\& Eosin (H\\&E) stained GBM tissue sections annotated for six distinct regions. We used the challenge's publicly available training dataset to develop and evaluate the effectiveness of several variants of EfficientNet architectures (i.e., B0, B1, B2, B3, B4). EfficientNet-B1 and EfficientNet-B4 achieved the best performance, achieving an F1 score of 0.98 in a 5-fold cross-validation configuration using the BraTS-Path training set. The quantitative performance evaluation of our proposed approach with EfficientNet-B1 on the BraTS-Path hold-out validation data and the final hidden testing data yielded F1 scores of 0.546 and 0.517, respectively, for the associated 6-class classification task. The difference in the performance on training, validation, and testing data highlights the challenge of developing models that generalize well to new data, which is crucial for clinical applications. The source code of the proposed approach can be found at the GitHub repository of Indiana University Division of Computational Pathology: https://github.com/IUCompPath/brats-path-2024-enet.", "AI": {"tldr": "研究采用EfficientNet模型来自动化分类胶质母细胞瘤的六个组织学区域，并在BraTS-Path 2024数据集上达到了较高的F1得分，但在测试数据上的得分较低，显示了泛化能力的挑战。", "motivation": "由于胶质母细胞瘤（GBM）的临床预后没有显著改善，该研究旨在通过自动化、鲁棒性和准确性的识别GBM中的不同组织学亚区域来增进对此疾病的形态学理解。", "method": "该研究设计了一个四步深度学习方法来分类六个（6个）组织学区域，并在BraTS-Path 2024挑战数据集上进行了定量评估。使用了EfficientNet架构的几个变体（B0、B1、B2、B3、B4）。", "result": "EfficientNet-B1和EfficientNet-B4取得了最佳性能，使用BraTS-Path训练集进行5折交叉验证的F1得分为0.98。在BraTS-Path验证数据和最终隐藏测试数据上，采用EfficientNet-B1的提出的模型的F1得分分别为0.546和0.517。", "conclusion": "该研究展示了通过深度学习来识别GBM中的组织学亚区域的潜力，但同时也表明了在新数据泛化能力上的挑战，这对临床应用至关重要。"}}
{"id": "2511.08923", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.08923", "abs": "https://arxiv.org/abs/2511.08923", "authors": ["Jingyu Liu", "Xin Dong", "Zhifan Ye", "Rishabh Mehta", "Yonggan Fu", "Vartika Singh", "Jan Kautz", "Ce Zhang", "Pavlo Molchanov"], "title": "TiDAR: Think in Diffusion, Talk in Autoregression", "comment": "NVIDIA-Tech Report", "summary": "Diffusion language models hold the promise of fast parallel generation, while autoregressive (AR) models typically excel in quality due to their causal structure aligning naturally with language modeling. This raises a fundamental question: can we achieve a synergy with high throughput, higher GPU utilization, and AR level quality? Existing methods fail to effectively balance these two aspects, either prioritizing AR using a weaker model for sequential drafting (speculative decoding), leading to lower drafting efficiency, or using some form of left-to-right (AR-like) decoding logic for diffusion, which still suffers from quality degradation and forfeits its potential parallelizability. We introduce TiDAR, a sequence-level hybrid architecture that drafts tokens (Thinking) in Diffusion and samples final outputs (Talking) AutoRegressively - all within a single forward pass using specially designed structured attention masks. This design exploits the free GPU compute density, achieving a strong balance between drafting and verification capacity. Moreover, TiDAR is designed to be serving-friendly (low overhead) as a standalone model. We extensively evaluate TiDAR against AR models, speculative decoding, and diffusion variants across generative and likelihood tasks at 1.5B and 8B scales. Thanks to the parallel drafting and sampling as well as exact KV cache support, TiDAR outperforms speculative decoding in measured throughput and surpasses diffusion models like Dream and Llada in both efficiency and quality. Most notably, TiDAR is the first architecture to close the quality gap with AR models while delivering 4.71x to 5.91x more tokens per second.", "AI": {"tldr": "TiDAR架构结合了并行扩散模型与自回归模型的优势，实现高质量的同时，大幅提升了生成效率。", "motivation": "解决现有方法未能在快速并行生成和高质量生成之间有效平衡的问题，提出TiDAR旨在实现高性能并行生成与自回归模型相当的质量水平。", "method": "TiDAR, 一种序列级混合架构，该架构利用扩散模型并行生成初始标记，并随后使用自回归模型采样最终输出，通过专门设计的结构化注意力掩码在单次前向传播中完成。", "result": "与自回归模型（AR）、投机解码和扩散模型（如Dream和Llada）相比，TiDAR在生成和可能性任务中展示了高通量、低计算开销和高效率的特性，特别是在1.5B和8B规模上。", "conclusion": "TiDAR首次在质量上接近自回归模型，同时实现了更高的通量，展示了4.71倍到5.91倍的每秒生成标记数量。"}}
{"id": "2511.08897", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.08897", "abs": "https://arxiv.org/abs/2511.08897", "authors": ["Mehdi Fatan Serj", "C. Alejandro Parraga", "Xavier Otazu"], "title": "Improving VisNet for Object Recognition", "comment": null, "summary": "Object recognition plays a fundamental role in how biological organisms perceive and interact with their environment. While the human visual system performs this task with remarkable efficiency, reproducing similar capabilities in artificial systems remains challenging. This study investigates VisNet, a biologically inspired neural network model, and several enhanced variants incorporating radial basis function neurons, Mahalanobis distance based learning, and retinal like preprocessing for both general object recognition and symmetry classification. By leveraging principles of Hebbian learning and temporal continuity associating temporally adjacent views to build invariant representations. VisNet and its extensions capture robust and transformation invariant features. Experimental results across multiple datasets, including MNIST, CIFAR10, and custom symmetric object sets, show that these enhanced VisNet variants substantially improve recognition accuracy compared with the baseline model. These findings underscore the adaptability and biological relevance of VisNet inspired architectures, offering a powerful and interpretable framework for visual recognition in both neuroscience and artificial intelligence.\n  Keywords: VisNet, Object Recognition, Symmetry Detection, Hebbian Learning, RBF Neurons, Mahalanobis Distance, Biologically Inspired Models, Invariant Representations", "AI": {"tldr": "本研究探讨了一个生物启发的神经网络模型VisNet及其增强变体，特别是在对象识别和对称性检测中的应用。通过引入特定技术，模型提高了识别准确率，研究表明其在AI领域尤其有显著意义。", "motivation": "研究动机在于，尽管人类视觉系统在对象识别方面表现出色，但在人造系统中复现这种能力仍然具有挑战性。研究旨在探讨生物启发模型VisNet及其变体在对象识别任务中的应用和改进潜力。", "method": "本研究使用了一种生物启发的神经网络模型VisNet和几种改进的变体，这些变体包括径向基函数神经元、基于马氏距离的学习方法和类似视网膜的预处理，用于一般对象识别和对称性分类。模型中采用了 Hebbian 学习原理和时间连续性，将相邻时间上的视图关联起来，建立不变表示。", "result": "实验结果表明，在MNIST、CIFAR10和自定义的对称对象集等多个数据集上，这些改进的VisNet变体相比基准模型显著提高了识别准确度。", "conclusion": "研究强调了VisNet启发模型的适应性和生物相关性，这些模型为神经科学和人工智能领域提供了强大的可解释框架，代表了视觉识别方向的重要进展。"}}
{"id": "2511.08949", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.08949", "abs": "https://arxiv.org/abs/2511.08949", "authors": ["Longfei Zuo", "Barbara Plank", "Siyao Peng"], "title": "EVADE: LLM-Based Explanation Generation and Validation for Error Detection in NLI", "comment": null, "summary": "High-quality datasets are critical for training and evaluating reliable NLP models. In tasks like natural language inference (NLI), human label variation (HLV) arises when multiple labels are valid for the same instance, making it difficult to separate annotation errors from plausible variation. An earlier framework VARIERR (Weber-Genzel et al., 2024) asks multiple annotators to explain their label decisions in the first round and flag errors via validity judgments in the second round. However, conducting two rounds of manual annotation is costly and may limit the coverage of plausible labels or explanations. Our study proposes a new framework, EVADE, for generating and validating explanations to detect errors using large language models (LLMs). We perform a comprehensive analysis comparing human- and LLM-detected errors for NLI across distribution comparison, validation overlap, and impact on model fine-tuning. Our experiments demonstrate that LLM validation refines generated explanation distributions to more closely align with human annotations, and that removing LLM-detected errors from training data yields improvements in fine-tuning performance than removing errors identified by human annotators. This highlights the potential to scale error detection, reducing human effort while improving dataset quality under label variation.", "AI": {"tldr": "研究提出EVADE框架，使用LLMs生成和验证解释，检测NLI任务中的错误。实验表明这种方法可减少人工工作，同时提高数据集质量。", "motivation": "高质数据集对于训练和评估可靠的自然语言处理(NLP)模型至关重要。然而，在自然语言推理(NLI)任务中，人类标签差异(HLV)使得区分标注错误和合理变体变得困难。", "method": "我们的研究提出了一个新的框架EVADE，利用大型语言模型(LLMs)生成和验证解释，以检测错误。该框架与之前的工作VARIERR相比，减少了进行两轮手动标注的需求。", "result": "实验结果表明，LLM验证可以使生成的解释分布更接近人类标注，并且在训练数据中移除LLM检测到的错误比移除人类标注者发现的错误能够提高微调性能。", "conclusion": "这项研究展示了利用LLM进行错误检测的潜力，能够降低人工投入并提高在标签变化情况下的数据集质量。"}}
{"id": "2511.08901", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.08901", "abs": "https://arxiv.org/abs/2511.08901", "authors": ["Riling Wei", "Kelu Yao", "Chuanguang Yang", "Jin Wang", "Zhuoyan Gao", "Chao Li"], "title": "Asymmetric Cross-Modal Knowledge Distillation: Bridging Modalities with Weak Semantic Consistency", "comment": "Accepted by AAAI-2026", "summary": "Cross-modal Knowledge Distillation has demonstrated promising performance on paired modalities with strong semantic connections, referred to as Symmetric Cross-modal Knowledge Distillation (SCKD). However, implementing SCKD becomes exceedingly constrained in real-world scenarios due to the limited availability of paired modalities. To this end, we investigate a general and effective knowledge learning concept under weak semantic consistency, dubbed Asymmetric Cross-modal Knowledge Distillation (ACKD), aiming to bridge modalities with limited semantic overlap. Nevertheless, the shift from strong to weak semantic consistency improves flexibility but exacerbates challenges in knowledge transmission costs, which we rigorously verified based on optimal transport theory. To mitigate the issue, we further propose a framework, namely SemBridge, integrating a Student-Friendly Matching module and a Semantic-aware Knowledge Alignment module. The former leverages self-supervised learning to acquire semantic-based knowledge and provide personalized instruction for each student sample by dynamically selecting the relevant teacher samples. The latter seeks the optimal transport path by employing Lagrangian optimization. To facilitate the research, we curate a benchmark dataset derived from two modalities, namely Multi-Spectral (MS) and asymmetric RGB images, tailored for remote sensing scene classification. Comprehensive experiments exhibit that our framework achieves state-of-the-art performance compared with 7 existing approaches on 6 different model architectures across various datasets.", "AI": {"tldr": "The paper presents SemBridge, an ACKD framework, which effectively handles scenarios with weak semantic alignment between modalities. It includes techniques for self-supervised learning and optimal transport to achieve state-of-the-art performance in cross-modal knowledge distillation.", "motivation": "The motivation behind the research is to address the limitations of Symmetric Cross-modal Knowledge Distillation (SCKD) in real-world applications due to the scarcity of paired modalities. The paper aims to develop a more flexible approach, Asymmetric Cross-modal Knowledge Distillation (ACKD), that can handle modalities with limited semantic overlap.", "method": "The paper proposes a framework called SemBridge, which integrates a Student-Friendly Matching module and a Semantic-aware Knowledge Alignment module to conduct Asymmetric Cross-modal Knowledge Distillation (ACKD) under weak semantic consistency. The Student-Friendly Matching module uses self-supervised learning to learn semantic-based knowledge and delivers personalized guidance for each student sample by adaptively choosing the relevant teacher samples. The Semantic-aware Knowledge Alignment module discovers the optimal transport path by using Lagrangian optimization.", "result": "The proposed SemBridge framework outperforms seven existing methods on six different model architectures across various datasets, demonstrating its effectiveness in remote sensing scene classification.", "conclusion": "The research concludes that the proposed SemBridge framework is capable of achieving state-of-the-art performance in ACKD under weak semantic consistency conditions. The framework's adaptability in selecting teacher samples and its use of optimal transport for knowledge alignment contribute to its success in remote sensing scene classification tasks."}}
{"id": "2511.08983", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.08983", "abs": "https://arxiv.org/abs/2511.08983", "authors": ["Shengmin Piao", "Sanghyun Park"], "title": "SpiralThinker: Latent Reasoning through an Iterative Process with Text-Latent Interleaving", "comment": null, "summary": "Recent advances in large reasoning models have been driven by reinforcement learning and test-time scaling, accompanied by growing interest in latent rather than purely textual reasoning. However, existing latent reasoning methods lack mechanisms to ensure stable evolution of latent representations and a systematic way to interleave implicit and explicit reasoning. We introduce SpiralThinker, a unified framework that performs iterative updates over latent representations, enabling extended implicit reasoning without generating additional tokens. A progressive alignment objective combined with structured annotations maintains coherence between latent and textual reasoning. Across mathematical, logical, and commonsense reasoning tasks, SpiralThinker achieves the best overall performance among latent reasoning approaches, consistently surpassing previous methods across all benchmarks. Detailed analyses reveal that both iteration and alignment are indispensable, the numbers of latent tokens and iterations exhibit dataset-specific optima, and appropriate alignment proves critical for an effective iterative process. Overall, SpiralThinker bridges iterative computation and latent reasoning, demonstrating that aligned iterative updates can reliably steer reasoning in the latent space.", "AI": {"tldr": "本文提出了SpiralThinker框架，该框架通过迭代更新潜在表示来进行扩展的隐式推理，同时保持潜在推理和文本推理之间的连贯性。实验结果表明，在数学、逻辑和常识推理任务中，SpiralThinker超越了现有的潜在推理方法。", "motivation": "现有的潜在推理方法在确保潜在表示的稳定进化以及隐式和显式推理之间的系统交织方面存在不足。", "method": "提出SpiralThinker框架，利用迭代更新潜在表示，融合渐进对齐目标和结构化注释来维持潜在和文本推理之间的连贯性。", "result": "在多个推理任务中，SpiralThinker在潜在推理方法中表现最佳，超过了现有的基准模型。", "conclusion": "SpiralThinker证明了经过对齐的迭代更新可以在潜在空间内可靠地指导推理过程，迭代和对齐都是关键因素。"}}
{"id": "2511.08903", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.08903", "abs": "https://arxiv.org/abs/2511.08903", "authors": ["Ibne Farabi Shihab", "Sanjeda Akter", "Anuj Sharma"], "title": "LLM-Guided Probabilistic Fusion for Label-Efficient Document Layout Analysis", "comment": null, "summary": "Document layout understanding remains data-intensive despite advances in semi-supervised learning. We present a framework that enhances semi-supervised detection by fusing visual predictions with structural priors from text-pretrained LLMs via principled probabilistic weighting. Given unlabeled documents, an OCR-LLM pipeline infers hierarchical regions which are combined with teacher detector outputs through inverse-variance fusion to generate refined pseudo-labels.Our method demonstrates consistent gains across model scales. With a lightweight SwiftFormer backbone (26M params), we achieve 88.2$\\pm$0.3 AP using only 5\\% labels on PubLayNet. When applied to document-pretrained LayoutLMv3 (133M params), our fusion framework reaches 89.7$\\pm$0.4 AP, surpassing both LayoutLMv3 with standard semi-supervised learning (89.1$\\pm$0.4 AP, p=0.02) and matching UDOP~\\cite{udop} (89.8 AP) which requires 100M+ pages of multimodal pretraining. This demonstrates that LLM structural priors are complementary to both lightweight and pretrained architectures. Key findings include: (1) learned instance-adaptive gating improves over fixed weights by +0.9 AP with data-dependent PAC bounds correctly predicting convergence; (2) open-source LLMs enable privacy-preserving deployment with minimal loss (Llama-3-70B: 87.1 AP lightweight, 89.4 AP with LayoutLMv3); (3) LLMs provide targeted semantic disambiguation (18.7\\% of cases, +3.8 AP gain) beyond simple text heuristics.Total system cost includes \\$12 for GPT-4o-mini API or 17 GPU-hours for local Llama-3-70B per 50K pages, amortized across training runs.", "AI": {"tldr": "", "motivation": "", "method": "Structure", "result": "{\n  \"tldr\": \"研究提出了一种通过将OCR-LLM流水线推断的分层区域与教师检测器输出相结合，生成精化的伪标记的框架，以增强半监督检测。该方法在轻量级SwiftFormer和预训练的LayoutLMv3模型上均表现出色，并且表明LLM结构先验对轻量级和预训练架构均有互补作用。\",\n  \"motivation\": \"尽管半监督学习有所进步，但文档布局理解仍需大量数据。研究旨在通过融合视觉预测和来自文本预训练语言模型的结构先验来增强半监督检测效果，减少所需标签数据量。\",\n  \"method\": \"该框架利用OCR-LLM流水线推断文档中的层级区域，然后结合教师检测器的输出通过逆方差融合生成精化的伪标签，以优化检测效果。\",\n  \"result\": \"该方法在使用少量标签数据的PubLayNet数据集上实现了显著的准确性提升。轻量级SwiftFormer模型仅使用5%的标签的情况下达到了88.2±0.3 AP,而预训练的LayoutLMv3达到了89.7±0.4 AP，超越了标准半监督学习的LayoutLMv3。\",\n  \"conclusion\": \"研究表明，LLM结构先验增强文档检测的有效性，特别是对于轻量级检测模型。该方法可以广泛应用，支持轻量级和预训练模型，同时保持较少的系统成本。此外，LLM提供了除简单文本规则外有针对性的语义区分能力。总体系统的成本包括API调用费用和GPU计算时间。\\n  \\n  实验结果证实了提出的方法不仅在精度上有显著提升，而且在不同模型类型上的表现稳定，并能够以边际成本部署。这表明研究提出方法在实际应用中有较高的实用价值和部署潜力。\"}\n}", "conclusion": ""}}
{"id": "2511.09003", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2511.09003", "abs": "https://arxiv.org/abs/2511.09003", "authors": ["Zhouxing Tan", "Ruochong Xiong", "Yulong Wan", "Jinlong Ma", "Hanlin Xue", "Qichun Deng", "Haifeng Jing", "Zhengtong Zhang", "Depei Liu", "Shiyuan Luo", "Junfei Liu"], "title": "Detecting Emotional Dynamic Trajectories: An Evaluation Framework for Emotional Support in Language Models", "comment": null, "summary": "Emotional support is a core capability in human-AI interaction, with applications including psychological counseling, role play, and companionship. However, existing evaluations of large language models (LLMs) often rely on short, static dialogues and fail to capture the dynamic and long-term nature of emotional support. To overcome this limitation, we shift from snapshot-based evaluation to trajectory-based assessment, adopting a user-centered perspective that evaluates models based on their ability to improve and stabilize user emotional states over time. Our framework constructs a large-scale benchmark consisting of 328 emotional contexts and 1,152 disturbance events, simulating realistic emotional shifts under evolving dialogue scenarios. To encourage psychologically grounded responses, we constrain model outputs using validated emotion regulation strategies such as situation selection and cognitive reappraisal. User emotional trajectories are modeled as a first-order Markov process, and we apply causally-adjusted emotion estimation to obtain unbiased emotional state tracking. Based on this framework, we introduce three trajectory-level metrics: Baseline Emotional Level (BEL), Emotional Trajectory Volatility (ETV), and Emotional Centroid Position (ECP). These metrics collectively capture user emotional dynamics over time and support comprehensive evaluation of long-term emotional support performance of LLMs. Extensive evaluations across a diverse set of LLMs reveal significant disparities in emotional support capabilities and provide actionable insights for model development.", "AI": {"tldr": "研究提出了一种通过轨迹评估大型语言模型（LLM）情感支持能力的方法，构建了一个大规模的模拟情境基准，并提出了新型情感支持效果度量。", "motivation": "现有的大型语言模型（LLM）评价通常依赖于简短且静态的对话，这未能捕捉情感支持在动态和长期交互中的作用。因此，研究将评估方式从基于快照转为基于轨迹，从用户角度评估模型在长时间内改善和稳定用户情感状态的能力。", "method": "本研究构建了一个大规模的基准测试，包含328种情感情境和1,152个扰动事件，以模拟现实中的情感变化。为了鼓励心理上合理回应，通过对模型输出进行基于验证的情感调节策略约束，例如情境选择和认知重构。用户情感轨迹被建模为一阶马尔可夫过程，并应用因果调整的情感估计来获取无偏的情感状态跟踪。", "result": "引入了三个轨迹级度量：基线情感水平（BEL）、情感轨迹波动（ETV）和情感质心位置（ECP），这些度量共同捕捉用户情感动力学特征，支持对LLM长期情感支持能力的全面评价。广泛的评价显示了不同LLM之间的显著差异，并为模型开发提供了实践性的见解。", "conclusion": "该研究提供了新的评价框架，能够更准确地评估LLM在长期情感支持中的表现，并指出不同模型之间存在显著差异，为后续LLM的发展提供了基于证据的指南。"}}
{"id": "2511.08904", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2511.08904", "abs": "https://arxiv.org/abs/2511.08904", "authors": ["Yating Liu", "Yan Lu"], "title": "Consistency Change Detection Framework for Unsupervised Remote Sensing Change Detection", "comment": "2025 IEEE International Conference on Multimedia and Expo (ICME)", "summary": "Unsupervised remote sensing change detection aims to monitor and analyze changes from multi-temporal remote sensing images in the same geometric region at different times, without the need for labeled training data. Previous unsupervised methods attempt to achieve style transfer across multi-temporal remote sensing images through reconstruction by a generator network, and then capture the unreconstructable areas as the changed regions. However, it often leads to poor performance due to generator overfitting. In this paper, we propose a novel Consistency Change Detection Framework (CCDF) to address this challenge. Specifically, we introduce a Cycle Consistency (CC) module to reduce the overfitting issues in the generator-based reconstruction. Additionally, we propose a Semantic Consistency (SC) module to enable detail reconstruction. Extensive experiments demonstrate that our method outperforms other state-of-the-art approaches.", "AI": {"tldr": "本文提出了一种新颖的一致性变化检测框架（CCDF），以应对生成器网络重建过程中过拟合的问题，实验表明，该方法优于其他现有方法。", "motivation": "由于先前借助生成器网络的无监督方法存在过拟合现象，导致变化检测的准确性降低，本文提出了一种新的方法，旨在优化这一过程，并提高变化检测的准确性与性能。", "method": "Structure", "result": "{\n  \"tldr\": \"本文提出了一种新颖的一致性变化检测框架（CCDF），以应对生成器网络重建过程中过拟合的问题，并引入循环一致性（CC）模块和语义一致性（SC）模块来提高变化检测的准确性。实验表明，该方法优于其他现有方法。\", \n  \"motivation\": \"先前无监督的方法通过生成器网络的重建技术跨多时相遥感图像进行样式迁移，并将不可重建的区域视为变化区域。然而，这种策略往往由于生成器网络的过拟合而导致性能不佳。为此，本文提出了改进的方法以提高变化检测的准确性与性能。\", \n  \"method\": \"本文设计了一个一致性变化检测框架（CCDF）。框架中包含两个模块：1. 循环一致性（CC）模块，旨在降低生成器网络在重建阶段的过拟合问题；2. 语义一致性（SC）模块，旨在提高细节重建的准确度。\", \n  \"result\": \"实验结果表明，所提出的方法优于现有的其他前沿技术方法。具体表现为变化检测的准确性与可靠性得到显著提升。\", \n  \"conclusion\": \"通过引入循环一致性（CC）模块和语义一致性（SC）模块，本文提出的方法CCDF能够在无监督的遥感变化检测中提供更好的性能表现，克服了传统方法存在的生成器网络过拟合问题，并提高了变化区域检测的准确性。\"}\n}", "conclusion": "通过循环一致性（CC）模块和语义一致性（SC）模块的应用，本文提出的一致性变化检测框架（CCDF），改进了无监督遥感变化检测的性能，提高了变化区域检测的准确性，克服了生成器模型的过拟合问题。"}}
{"id": "2511.09008", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.LO"], "pdf": "https://arxiv.org/pdf/2511.09008", "abs": "https://arxiv.org/abs/2511.09008", "authors": ["Sam Bayless", "Stefano Buliani", "Darion Cassel", "Byron Cook", "Duncan Clough", "Rémi Delmas", "Nafi Diallo", "Ferhat Erata", "Nick Feng", "Dimitra Giannakopoulou", "Aman Goel", "Aditya Gokhale", "Joe Hendrix", "Marc Hudak", "Dejan Jovanović", "Andrew M. Kent", "Benjamin Kiesl-Reiter", "Jeffrey J. Kuna", "Nadia Labai", "Joseph Lilien", "Divya Raghunathan", "Zvonimir Rakamarić", "Niloofar Razavi", "Michael Tautschnig", "Ali Torkamani", "Nathaniel Weir", "Michael W. Whalen", "Jianan Yao"], "title": "A Neurosymbolic Approach to Natural Language Formalization and Verification", "comment": "20 pages, 12 figures", "summary": "Large Language Models perform well at natural language interpretation and reasoning, but their inherent stochasticity limits their adoption in regulated industries like finance and healthcare that operate under strict policies. To address this limitation, we present a two-stage neurosymbolic framework that (1) uses LLMs with optional human guidance to formalize natural language policies, allowing fine-grained control of the formalization process, and (2) uses inference-time autoformalization to validate logical correctness of natural language statements against those policies. When correctness is paramount, we perform multiple redundant formalization steps at inference time, cross checking the formalizations for semantic equivalence. Our benchmarks demonstrate that our approach exceeds 99% soundness, indicating a near-zero false positive rate in identifying logical validity. Our approach produces auditable logical artifacts that substantiate the verification outcomes and can be used to improve the original text.", "AI": {"tldr": "本文提出了一种两阶段的神经符号框架，旨在解决大语言模型在受严格政策约束的行业内，因随机性而难以广泛应用的问题。该框架通过自然语言政策的形式化和推理时间自动形式化来确保逻辑正确性。", "motivation": "大语言模型虽然在自然语言解释和推理方面表现出色，但其固有的随机性限制了它们在金融和医疗等严格监管行业中的应用。", "method": "两阶段的神经符号框架，首先通过大语言模型（可选人类指导）将自然语言政策形式化，其次在推理时间进行自动形式化来验证自然语言陈述的逻辑正确性。", "result": "基准测试显示，本方法达到了超过99%的逻辑有效性的准确率，表明在识别逻辑有效性方面的假阳性率接近于零。", "conclusion": "该方法产生可审查的逻辑化文档，支持验证结果，并可用于改善原始文本。"}}
{"id": "2511.08908", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2511.08908", "abs": "https://arxiv.org/abs/2511.08908", "authors": ["Shuji Ono"], "title": "HitoMi-Cam: A Shape-Agnostic Person Detection Method Using the Spectral Characteristics of Clothing", "comment": "37 pages, 21 figures, 9 tables. Published in MDPI Journal of Imaging. Includes 1 supplementary video file (ancillary file)", "summary": "While convolutional neural network (CNN)-based object detection is widely used, it exhibits a shape dependency that degrades performance for postures not included in the training data. Building upon our previous simulation study published in this journal, this study implements and evaluates the spectral-based approach on physical hardware to address this limitation. Specifically, this paper introduces HitoMi-Cam, a lightweight and shape-agnostic person detection method that uses the spectral reflectance properties of clothing. The author implemented the system on a resource-constrained edge device without a GPU to assess its practical viability. The results indicate that a processing speed of 23.2 frames per second (fps) (253x190 pixels) is achievable, suggesting that the method can be used for real-time applications. In a simulated search and rescue scenario where the performance of CNNs declines, HitoMi-Cam achieved an average precision (AP) of 93.5%, surpassing that of the compared CNN models (best AP of 53.8%). Throughout all evaluation scenarios, the occurrence of false positives remained minimal. This study positions the HitoMi-Cam method not as a replacement for CNN-based detectors but as a complementary tool under specific conditions. The results indicate that spectral-based person detection can be a viable option for real-time operation on edge devices in real-world environments where shapes are unpredictable, such as disaster rescue.", "AI": {"tldr": "HitoMi-Cam offers a shape-agnostic person detection method using spectral reflectance, achieving high performance in unpredictable shape scenarios on resource-limited edge devices, complementary to CNN-based methods.", "motivation": "The motivation behind the study is to address the limitation of CNN-based object detection, which shows a dependency on shape, impairing its performance for postures not covered in training data.", "method": "The paper introduces HitoMi-Cam, a lightweight, shape-agnostic person detection method that leverages the spectral reflectance properties of clothing, implemented on a resource-constrained edge device without a GPU.", "result": "The spectral-based approach outperforms CNN models in shape-unpredictable environments, achieving a processing speed of 23.2 fps and an average precision of 93.5% in a simulated search and rescue scenario", "conclusion": "The study concludes that HitoMi-Cam is not a replacement for CNN-based detectors but complements them in specific conditions, indicating that real-time operation on edge devices is viable in unpredictable real-world environments."}}
