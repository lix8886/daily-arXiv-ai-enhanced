{"id": "2601.18845", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.18845", "abs": "https://arxiv.org/abs/2601.18845", "authors": ["Zeineb Dridi", "Jihen Bennaceur", "Amine Ben Hassouna"], "title": "Dynamic Mask-Based Backdoor Attack Against Vision AI Models: A Case Study on Mushroom Detection", "comment": null, "summary": "Deep learning has revolutionized numerous tasks within the computer vision field, including image classification, image segmentation, and object detection. However, the increasing deployment of deep learning models has exposed them to various adversarial attacks, including backdoor attacks. This paper presents a novel dynamic mask-based backdoor attack method, specifically designed for object detection models. We exploit a dataset poisoning technique to embed a malicious trigger, rendering any models trained on this compromised dataset vulnerable to our backdoor attack. We particularly focus on a mushroom detection dataset to demonstrate the practical risks posed by such attacks on critical real-life domains. Our work also emphasizes the importance of creating a detailed backdoor attack scenario to illustrate the significant risks associated with the outsourcing practice. Our approach leverages SAM, a recent and powerful image segmentation AI model, to create masks for dynamic trigger placement, introducing a new and stealthy attack method. Through extensive experimentation, we show that our sophisticated attack scenario maintains high accuracy on clean data with the YOLOv7 object detection model while achieving high attack success rates on poisoned samples. Our approach surpasses traditional methods for backdoor injection, which are based on static and consistent patterns. Our findings underscore the urgent need for robust countermeasures to protect deep learning models from these evolving adversarial threats.", "AI": {"tldr": "The paper introduces a new dynamic mask-based backdoor attack specifically for object detection models, demonstrating high attack success rates while maintaining model accuracy on clean data. This attack method uses SAM for dynamic trigger placement, improving stealth over traditional static pattern methods.", "motivation": "The motivation for this paper is to highlight the risks posed by backdoor attacks on object detection models and the need for better defense mechanisms. The authors aimed to show how a specific backdoor attack can be implemented effectively and how it can pose a serious threat, particularly in critical real-life applications such as mushroom detection.", "method": "Our approach uses a dataset poisoning method to embed a malicious trigger in images. It utilizes SAM, an image segmentation AI model, to dynamically place triggers in the images for object detection models, making the attack more stealthy compared to traditional backdoor attacks that use static patterns.", "result": "Experiments showed the proposed attack method maintained high accuracy on clean data with the YOLOv7 model while achieving high attack success rates on poisoned samples, indicating the effectiveness of the method in compromising the targeted models.", "conclusion": "The conclusion draws attention to the significant risks associated with outsourcing model training and the urgent need for robust countermeasures to defend against sophisticated and evolving adversarial attacks on deep learning models."}}
{"id": "2601.18849", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.18849", "abs": "https://arxiv.org/abs/2601.18849", "authors": ["Yuhui Zhang", "Hui Yu", "Wei Liang", "Sunjie Zhang"], "title": "Audio-Driven Talking Face Generation with Blink Embedding and Hash Grid Landmarks Encoding", "comment": null, "summary": "Dynamic Neural Radiance Fields (NeRF) have demonstrated considerable success in generating high-fidelity 3D models of talking portraits. Despite significant advancements in the rendering speed and generation quality, challenges persist in accurately and efficiently capturing mouth movements in talking portraits. To tackle this challenge, we propose an automatic method based on blink embedding and hash grid landmarks encoding in this study, which can substantially enhance the fidelity of talking faces. Specifically, we leverage facial features encoded as conditional features and integrate audio features as residual terms into our model through a Dynamic Landmark Transformer. Furthermore, we employ neural radiance fields to model the entire face, resulting in a lifelike face representation. Experimental evaluations have validated the superiority of our approach to existing methods.", "AI": {"tldr": "本文提出了一种基于眨眼嵌入和哈希网格地标编码的自动方法，以提高动态NeRF在捕捉说话人脸嘴部运动方面的准确性和效率。通过面部特征编码为条件特征，并将音频特征作为残差项整合到模型中，提高了生成说话人脸的逼真度。", "motivation": "虽然动态NeRF在生成高保真的3D人物肖像方面取得了成功，但在准确和高效地捕捉说话人脸的嘴部运动方面仍然存在挑战。", "method": "本文提出了一个基于眨眼嵌入和哈希网格地标编码的方法，并通过动态地标转换器将面部特征作为条件特征和音频特征作为残差项整合进模型，以提高说话人脸的逼真度。神经辐射场被用来建模整个脸部以呈现逼真的脸部表现。", "result": "实验结果验证了我们提出的方法优于现有的方法。", "conclusion": "通过引入基于眨眼嵌入和哈希网格地标编码的新方法，以及使用动态地标转换器和神经辐射场技术，本文成功提升了说话人脸模型的逼真度和性能。"}}
{"id": "2601.18851", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2601.18851", "abs": "https://arxiv.org/abs/2601.18851", "authors": ["Wei Liang", "Hui Yu", "Derui Ding", "Rachael E. Jack", "Philippe G. Schyns"], "title": "SelfieAvatar: Real-time Head Avatar reenactment from a Selfie Video", "comment": null, "summary": "Head avatar reenactment focuses on creating animatable personal avatars from monocular videos, serving as a foundational element for applications like social signal understanding, gaming, human-machine interaction, and computer vision. Recent advances in 3D Morphable Model (3DMM)-based facial reconstruction methods have achieved remarkable high-fidelity face estimation. However, on the one hand, they struggle to capture the entire head, including non-facial regions and background details in real time, which is an essential aspect for producing realistic, high-fidelity head avatars. On the other hand, recent approaches leveraging generative adversarial networks (GANs) for head avatar generation from videos can achieve high-quality reenactments but encounter limitations in reproducing fine-grained head details, such as wrinkles and hair textures. In addition, existing methods generally rely on a large amount of training data, and rarely focus on using only a simple selfie video to achieve avatar reenactment. To address these challenges, this study introduces a method for detailed head avatar reenactment using a selfie video. The approach combines 3DMMs with a StyleGAN-based generator. A detailed reconstruction model is proposed, incorporating mixed loss functions for foreground reconstruction and avatar image generation during adversarial training to recover high-frequency details. Qualitative and quantitative evaluations on self-reenactment and cross-reenactment tasks demonstrate that the proposed method achieves superior head avatar reconstruction with rich and intricate textures compared to existing approaches.", "AI": {"tldr": "A method for creating detailed head avatars from a single selfie video using 3DMM and StyleGAN, outperforming existing approaches in texture richness and detail.", "motivation": "The motivation is to overcome the limitations of existing methods in capturing entire heads and producing high-fidelity, detailed head avatars in real-time. It aims to achieve this with minimal data, such as a single selfie video.", "method": "The paper proposes a detailed head avatar reenactment method using a selfie video. It combines 3D Morphable Models (3DMMs) with a StyleGAN-based generator and incorporates mixed loss functions for foreground reconstruction and avatar image generation.", "result": "Qualitative and quantitative evaluations on self-reenactment and cross-reenactment tasks show that the proposed method achieves superior head avatar reconstruction with more intricate textures.", "conclusion": "The combined approach of 3DMMs and StyleGAN-based generator with mixed loss functions can generate high-fidelity, detailed head avatars suitable for real-time applications and diverse avatar reenactment tasks."}}
