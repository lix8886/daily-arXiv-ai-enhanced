<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 71]
- [cs.CV](#cs.CV) [Total: 67]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Bridging the Semantic Gap: Contrastive Rewards for Multilingual Text-to-SQL](https://arxiv.org/abs/2510.13827)
*Ashish Kattamuri,Ishita Prasad,Meetu Malhotra,Arpita Vats,Rahul Raja,Albert Lie*

Main category: cs.CL

> 我们的研究提出了一个结合GRPO和多语言对比奖励的新框架，以提高跨语言中SQL系统的性能，特别是在语义准确性和任务效率方面，并通过实验成功展示了其超越更大模型的效果，且仅使用少量训练样本。

<details>
  <summary>Details</summary>

**Motivation:** 文本到SQL方法仅关注可执行查询的评估，而忽视了语义对齐的挑战，包括查询的语义含义和执行结果的正确性。在从英语转换到其他语言时，即使执行准确性也显示出显著下降，平均每种非英语语言下降6个百分点。我们旨在解决这些挑战，并提高跨语言中SQL系统的语义准确性和任务效率。

**Method:** 我们的研究提出了一种新的框架，该框架结合了群组相对策略优化（GRPO）和多语言对比奖励信号，以提高跨语言环境中文本到SQL系统的任务效率和语义准确性。通过将基于语义相似性的奖励信号与GRPO结合，我们的方法使模型能够更好地匹配SQL生成与用户意图。

**Result:** 我们的方法通过在多语言蜘蛛数据集的7种语言上微调30亿参数的LLaMA-3模型，实现了高达87.4%的执行准确性（比零样本提高了26个百分点）和高达52.29%的语义准确性（比零样本提高了32.86个百分点）。添加GRPO框架中的对比奖励信号进一步提高了平均语义准确性至59.14%（提高了6.85个百分点，最多提高了10个百分点对于越南语）。

**Conclusion:** 实验结果显示，利用对比奖励信号微调的较小参数的30亿参数LLaMA模型，在执行准确性（零样本的8B模型为81.43%，而3B模型提高到88.86%）和语义准确性（68.57% vs. 59.14%）方面，优于更大尺寸的零样本8B LLaMA模型，并且所有这些改进仅使用了3000个强化学习训练样本就达到了。通过向文本到SQL系统添加对比奖励，我们可以在无需大型训练数据集的情况下显著提升其性能。

**Abstract:** Current Text-to-SQL methods are evaluated and only focused on executable
queries, overlooking the semantic alignment challenge -- both in terms of the
semantic meaning of the query and the correctness of the execution results.
Even execution accuracy itself shows significant drops when moving from English
to other languages, with an average decline of 6 percentage points across
non-English languages. We address these challenges by presenting a new
framework that combines Group Relative Policy Optimization (GRPO) within a
multilingual contrastive reward signal to enhance both task efficiency and
semantic accuracy in Text-to-SQL systems in cross-lingual scenarios. Our method
teaches models to obtain better correspondence between SQL generation and user
intent by combining a reward signal based on semantic similarity. On the
seven-language MultiSpider dataset, fine-tuning the LLaMA-3-3B model with GRPO
improved the execution accuracy up to 87.4 percent (+26 pp over zero-shot) and
semantic accuracy up to 52.29 percent (+32.86 pp). Adding our contrastive
reward signal in the GRPO framework further improved the average semantic
accuracy to 59.14 percent (+6.85 pp, up to +10 pp for Vietnamese). Our
experiments showcase that a smaller, parameter-efficient 3B LLaMA model
fine-tuned with our contrastive reward signal outperforms a much larger
zero-shot 8B LLaMA model, with an uplift of 7.43 pp in execution accuracy (from
81.43 percent on the 8B model to 88.86 percent on the 3B model), and nearly
matches its semantic accuracy (59.14 percent vs. 68.57 percent) -- all using
just 3,000 reinforcement learning training examples. These results demonstrate
how we can improve the performance of Text-to-SQL systems with contrastive
rewards for directed semantic alignment, without requiring large-scale training
datasets.

</details>


### [2] [From Explainability to Action: A Generative Operational Framework for Integrating XAI in Clinical Mental Health Screening](https://arxiv.org/abs/2510.13828)
*Ratna Kandala,Akshata Kishore Moharir,Divya Arvinda Nayak*

Main category: cs.CL

> 本文提出了一种名为Generative Operational Framework的新架构，旨在解决XAI技术在临床应用中的翻译问题，通过将XAI的输出与临床指南结合起来，生成易于理解的临床叙述。

<details>
  <summary>Details</summary>

**Motivation:** 现有的XAI技术，如SHAP和LIME，虽然在生成技术上忠实的输出方面表现出色，但无法提供临床相关的、可操作的见解，这些见解可以被临床医生使用或患者理解。这是导致技术透明度和人类实用性之间断层的主要障碍。

**Method:** 本文提出了Generative Operational Framework这一新颖的系统架构，该架构以大型语言模型（LLMs）为中心翻译引擎，它能够处理来自各种XAI工具的原始技术输出，并通过RAG与临床指南相结合，自动生成人类可读的、基于证据的临床叙述。

**Result:** 该框架能够直接解决主要的操作障碍，包括工作流程集成、偏见缓解和针对不同利益相关者的沟通。

**Conclusion:** 本文提供了一个战略路线图，旨在让该领域超越孤立数据点的生成，向在临床实践中提供集成的、可操作的和值得信赖的AI方向发展。

**Abstract:** Explainable Artificial Intelligence (XAI) has been presented as the critical
component for unlocking the potential of machine learning in mental health
screening (MHS). However, a persistent lab-to-clinic gap remains. Current XAI
techniques, such as SHAP and LIME, excel at producing technically faithful
outputs such as feature importance scores, but fail to deliver clinically
relevant, actionable insights that can be used by clinicians or understood by
patients. This disconnect between technical transparency and human utility is
the primary barrier to real-world adoption. This paper argues that this gap is
a translation problem and proposes the Generative Operational Framework, a
novel system architecture that leverages Large Language Models (LLMs) as a
central translation engine. This framework is designed to ingest the raw,
technical outputs from diverse XAI tools and synthesize them with clinical
guidelines (via RAG) to automatically generate human-readable, evidence-backed
clinical narratives. To justify our solution, we provide a systematic analysis
of the components it integrates, tracing the evolution from intrinsic models to
generative XAI. We demonstrate how this framework directly addresses key
operational barriers, including workflow integration, bias mitigation, and
stakeholder-specific communication. This paper also provides a strategic
roadmap for moving the field beyond the generation of isolated data points
toward the delivery of integrated, actionable, and trustworthy AI in clinical
practice.

</details>


### [3] [A Linguistics-Aware LLM Watermarking via Syntactic Predictability](https://arxiv.org/abs/2510.13829)
*Shinwoo Park,Hyejin Park,Hyeseon Ahn,Yo-Sub Han*

Main category: cs.CL

> 我们提出了STELA框架，通过调整文本中词性n元组的不确定性模态来平衡文本质量和检测鲁棒性，实现在多种语言上公共可验证的水印技术，提升检测效果。

<details>
  <summary>Details</summary>

**Motivation:** 随着大型语言模型的快速发展，可靠治理工具变得至关重要。公共可验证的水印技术对于建立一个可信赖的人工智能生态系统尤为重要。当前的挑战在于在文本质量和检测鲁棒性之间取得平衡。本研究旨在解决这一挑战。

**Method:** 我们介绍了STELA，这是一种新颖的框架，它根据语言的自由度来匹配水印强度。STELA利用词性（POS）n元模型的语义不确定性动态调节信号，使其在语法约束环境下减弱以保持文本质量，在语言更具灵活性的环境中增强以提高可检测性。

**Result:** 通过对分析性的英语、孤立性的汉语和聚合性的韩语进行广泛的实验，我们证明了STELA在检测鲁棒性方面优于之前的方法。

**Conclusion:** STELA提供了一种无需访问模型logits的公共验证检测方法，利用语言结构差异性来增强水印的公众可验证性，提高了水印技术的实际应用前景。

**Abstract:** As large language models (LLMs) continue to advance rapidly, reliable
governance tools have become critical. Publicly verifiable watermarking is
particularly essential for fostering a trustworthy AI ecosystem. A central
challenge persists: balancing text quality against detection robustness. Recent
studies have sought to navigate this trade-off by leveraging signals from model
output distributions (e.g., token-level entropy); however, their reliance on
these model-specific signals presents a significant barrier to public
verification, as the detection process requires access to the logits of the
underlying model. We introduce STELA, a novel framework that aligns watermark
strength with the linguistic degrees of freedom inherent in language. STELA
dynamically modulates the signal using part-of-speech (POS) n-gram-modeled
linguistic indeterminacy, weakening it in grammatically constrained contexts to
preserve quality and strengthen it in contexts with greater linguistic
flexibility to enhance detectability. Our detector operates without access to
any model logits, thus facilitating publicly verifiable detection. Through
extensive experiments on typologically diverse languages-analytic English,
isolating Chinese, and agglutinative Korean-we show that STELA surpasses prior
methods in detection robustness. Our code is available at
https://github.com/Shinwoo-Park/stela_watermark.

</details>


### [4] [Users as Annotators: LLM Preference Learning from Comparison Mode](https://arxiv.org/abs/2510.13830)
*Zhongze Cai,Xiaocheng Li*

Main category: cs.CL

> 本文介绍了一种新的收集用户对大型语言模型响应偏好数据的方法，通过用户在比较模式下的行为获取数据，并通过一种模型和算法来评估和过滤这些数据，以提高大型语言模型的对齐效果。

<details>
  <summary>Details</summary>

**Motivation:** 随着大型语言模型的普及，用户在日常交互中越来越多地贡献了偏好标签。虽然用户是评判对自己查询或提示的响应最合适的专家，但这些标签的质量控制存在不足。本文动机在于探讨一种新的方式来收集配对偏好数据，并解决质量控制问题。

**Method:** 本文提出了一种新的方法，通过用户在比较模式下的行为来收集配对偏好数据。具体方法是让两个不同的模型或同一模型的不同版本对同一提示生成两个不同响应，从而引入数据的不对称性，并通过所提出用户行为模型推断用户数据的质量。进而，采用期望最大化算法估计用户的潜在质量因素，并据此过滤用户标注数据。

**Result:** 通过使用本文的方法，可以有效提高大型语言模型的对齐效果，证明了用户行为模型与期望最大化算法在用户偏好数据的质量评估和过滤上的有效性。

**Conclusion:** 实验证明了本文提出的方法在捕捉用户行为和过滤数据方面的有效性，这对改进大型语言模型的对齐效果具有重要意义。

**Abstract:** Pairwise preference data have played an important role in the alignment of
large language models (LLMs). Each sample of such data consists of a prompt,
two different responses to the prompt, and a binary label indicating which of
the two responses is better. The labels are usually annotated by professional
human annotators. In this paper, we consider an alternative approach to collect
pairwise preference data -- user annotation from comparison mode. With the
increasingly wider adoption of LLMs among the population, users are
contributing more and more of their preference labels through their daily
interactions with the LLMs. The upside of such labels is that users are the
best experts in judging the responses to their own queries/prompts, but the
downside is the lack of quality control in these labels. In this paper, we
consider a new idea of generating two responses from two different models or
two different versions of the same model. The asymmetry allows us to make an
inference of the user's data quality through our proposed user behavior model.
We develop an expectation-maximization algorithm to estimate a latent quality
factor of the user, and filter users' annotation data accordingly. The
downstream task shows the effectiveness of our approach in both capturing the
user behavior and data filtering for LLM alignment.

</details>


### [5] [Informed Routing in LLMs: Smarter Token-Level Computation for Faster Inference](https://arxiv.org/abs/2510.13831)
*Chao Han,Yijuan Liang,Zihao Xuan,Daokuan Wu,Wei Zhang,Xiaoyu Shen*

Main category: cs.CL

> 本文通过引入轻量级特征预测器（LFF），提出了一种新的Informed Routing路由策略，以解决现有方法在模型组件选择时效率低下和信息丢失的问题，在保持模型精确度的同时大幅度减少了计算量。

<details>
  <summary>Details</summary>

**Motivation:** 由于现有基于贪婪路由的方法在选择token时存在短视的执行或跳过机制，导致不可逆的信息损失和次优的token选择，本文旨在通过提出新的路由策略来解决这些问题。

**Method:** 本文提出了一个名为Informed Routing的新范式，并设计了一个轻量级特征预测器（LFF），以评估每个token的重要性和可恢复性，从而在动态分配备份计算时，实现更加灵活且保持模型精度的执行或近似策略。

**Result:** 实验结果表明，该方法在不同稀疏级别下实现了性能和效率之间的最优平衡，并且即使不进行最终的LoRA微调，仍然可以匹配甚至超过需要全程微调的强大基线，同时训练时间减少了50%以上。

**Conclusion:** 实验表明本文方法在简约计算成本的同时保持了模型的高性能，证明了该新范式及LFF策略的有效性，并指出了相对于现有方法的独特优势。

**Abstract:** The deployment of large language models (LLMs) in real-world applications is
increasingly limited by their high inference cost. While recent advances in
dynamic token-level computation allocation attempt to improve efficiency by
selectively activating model components per token, existing methods rely on
greedy routing--a myopic execute-or-skip mechanism that often leads to
irreversible information loss and suboptimal token selection. This paper
introduces informed routing, a new paradigm that proactively addresses these
issues. The key insight is to assess not only a token's immediate importance
but also its recoverability, i.e., how well its transformation can be
approximated. To this end, we propose the Lightweight Feature Forecaster (LFF),
a small predictive module that estimates a unit's output before routing
decisions are made. This enables a flexible execute-or-approximate policy that
preserves model fidelity while drastically reducing computation. Extensive
experiments on both language modeling and reasoning tasks show that informed
routing achieves state-of-the-art efficiency-performance trade-offs across
multiple sparsity levels. Notably, even without final LoRA fine-tuning, our
method matches or surpasses strong baselines that require full fine-tuning, all
while reducing training time by over 50%. The code is available at:
https://github.com/EIT-NLP/informed-routing

</details>


### [6] [Entropy Meets Importance: A Unified Head Importance-Entropy Score for Stable and Efficient Transformer Pruning](https://arxiv.org/abs/2510.13832)
*Minsik Choi,Hyegang Son,Changhoon Kim,Young Geun Kim*

Main category: cs.CL

> 本文提出一种新的剪枝标准HIES（头部重要性-熵分数），结合头部重要性分数与注意力熵，为每个头部贡献提供更全面的证据。实验表明，基于HIES的剪枝方法相较于仅使用HIS的方法，在模型质量和稳定性上分别提高了15.2%和2.04倍，实现了显著的模型压缩，且不牺牲准确性或稳定性。

<details>
  <summary>Details</summary>

**Motivation:** Transformer模型虽然在NLP任务中表现出色，但其多层次及注意力头的结构特性引入了在推理和部署上的效率挑战。现有的基于梯度的剪枝方法虽然进展不错，但仍局限于捕捉梯度驱动的贡献，忽略了注意力模式的多样性。

**Method:** 作者提出新的剪枝标准HIES，该标准结合了头部重要性分数与注意力熵，以提供关于每个头部贡献的互补证据，从而更好地评估Transformer模型中各个注意力头的作用。

**Result:** 实验结果显示，基于HIES的剪枝方法相比仅使用HIS的方法，在模型质量和稳定性上分别提高了15.2%和2.04倍，实现了显著的模型压缩同时保持了模型的准确性和稳定性。

**Conclusion:** 研究结论是通过引入HIES标准，可以在不牺牲准确性或稳定性的条件下，有效地进行模型压缩，这是对现有剪枝方法的重要补充，并有望改善Transformer模型的效率。

**Abstract:** Transformer-based models have achieved remarkable performance in NLP tasks.
However, their structural characteristics-multiple layers and attention
heads-introduce efficiency challenges in inference and deployment. To address
these challenges, various pruning methods have recently been proposed. Notably,
gradient-based methods using Head Importance Scores (HIS) have gained traction
for interpretability, efficiency, and ability to identify redundant heads.
However, HIS alone has limitations as it captures only the gradient-driven
contribution, overlooking the diversity of attention patterns. To overcome
these limitations, we introduce a novel pruning criterion, HIES (Head
Importance-Entropy Score), which integrates head importance scores with
attention entropy, providing complementary evidence on per-head contribution.
Empirically, HIES-based pruning yields up to 15.2% improvement in model quality
and 2.04x improvement in stability over HIS-only methods, enabling substantial
model compression without sacrificing either accuracy or stability. Code will
be released upon publication.

</details>


### [7] [ConDABench: Interactive Evaluation of Language Models for Data Analysis](https://arxiv.org/abs/2510.13835)
*Avik Dutta,Priyanshu Gupta,Hosein Hasanbeig,Rahul Pratap Singh,Harshit Nigam,Sumit Gulwani,Arjun Radhakrishna,Gustavo Soares,Ashish Tiwari*

Main category: cs.CL

> ConDABench是一个生成对话数据分析基准并评估工具性能的框架，强调交互性的重要性和现有模型在复杂长期任务中的不足。

<details>
  <summary>Details</summary>

**Motivation:** 现实中，数据分析任务常常具有不明确的目标和不洁净的数据，用户交互对于理解用户意图和解决复杂任务是必要的。然而现有的评估LLMs的数据分析任务标准并没有覆盖这些复杂性或提供交互式支持。因此，需要一个能更好地支持交互性、模拟实际场景的测试平台。

**Method:** 引入了ConDABench框架用于生成对话数据分析基准并评估外部工具在生成的基准上的性能。这个框架由三部分组成：(a) 一个多功能代理工作流程，用于从描述公共数据集获得见解的文章中生成现实的基准；(b) 使用此工作流程生成的1,420个ConDA问题；(c) 一种评估工具，首次使得能够系统地对对话数据分析工具在生成的ConDA问题上的表现进行测试。

**Result:** 评估发现新一代模型在解决更多问题方面有进步，但在长篇对话和互动任务中表现不是最优，说明ConDABench对于未来模型改进的价值。

**Conclusion:** 评估结果显示，尽管新一代模型在解决更多案例方面表现更好，但在需要持续、长篇幅互动的任务方面并不一定表现得更好。ConDABench为模型构建者提供了衡量模型向真正协作的、能够完成复杂互动任务的发展方向的进展的途径。

**Abstract:** Real-world data analysis tasks often come with under-specified goals and
unclean data. User interaction is necessary to understand and disambiguate a
user's intent, and hence, essential to solving these complex tasks. Existing
benchmarks for evaluating LLMs on data analysis tasks do not capture these
complexities or provide first-class support for interactivity. We introduce
ConDABench, a framework for generating conversational data analysis (ConDA)
benchmarks and evaluating external tools on the generated benchmarks. \bench
consists of (a) a multi-agent workflow for generating realistic benchmarks from
articles describing insights gained from public datasets, (b) 1,420 ConDA
problems generated using this workflow, and (c) an evaluation harness that, for
the first time, makes it possible to systematically evaluate conversational
data analysis tools on the generated ConDA problems. Evaluation of
state-of-the-art LLMs on the benchmarks reveals that while the new generation
of models are better at solving more instances, they are not necessarily better
at solving tasks that require sustained, long-form engagement. ConDABench is an
avenue for model builders to measure progress towards truly collaborative
models that can complete complex interactive tasks.

</details>


### [8] [SIMBA UQ: Similarity-Based Aggregation for Uncertainty Quantification in Large Language Models](https://arxiv.org/abs/2510.13836)
*Debarun Bhattacharjya,Balaji Ganesan,Junkyu Lee,Radu Marinescu,Katsiaryna Mirylenka,Michael Glass,Xiao Shou*

Main category: cs.CL

> 本文研究了不确定性量化（UQ）技术的有效性，提出了一种非语言化相似性聚合框架及其基于该框架的具体新型技术，通过小规模训练集校准生成输出的置信度，实验证明这些方法优于现有基线方法。

<details>
  <summary>Details</summary>

**Motivation:** 研究的动机是探讨不确定性量化的有效性，尤其是那些非黑盒方法，这些方法不需要访问生成模型的内部信息，具有诸如系统变化鲁棒性、适应不同模型选择、成本减少和计算可行性的诸多实际优势。

**Method:** 本研究提出了一种基于非语言化相似性的聚合框架，该框架涵盖了适用于复杂生成任务的广泛不确定性量化方法，并引入了从该框架中派生的新型技术，这些技术使用小规模训练集训练置信度估计模型。

**Result:** 实证研究表明，所提出的基于相似性方法在问答、总结和文本到SQL等任务上比现有的基线方法具有更优的置信度校准能力。

**Conclusion:** 通过针对问答、总结和文本到SQL等多样化任务的数据集的实证研究，研究得出结论，所提出的基于相似性的方法能够比基线方法提供更好的校准置信度。

**Abstract:** When does a large language model (LLM) know what it does not know?
Uncertainty quantification (UQ) provides measures of uncertainty, such as an
estimate of the confidence in an LLM's generated output, and is therefore
increasingly recognized as a crucial component of trusted AI systems. Black-box
UQ methods do not require access to internal model information from the
generating LLM and therefore have numerous real-world advantages, such as
robustness to system changes, adaptability to choice of LLM, reduced costs, and
computational tractability. In this paper, we investigate the effectiveness of
UQ techniques that are primarily but not necessarily entirely black-box, where
the consistency between a generated output and other sampled generations is
used as a proxy for confidence in its correctness. We propose a high-level
non-verbalized similarity-based aggregation framework that subsumes a broad
swath of UQ approaches suitable for complex generative tasks, as well as
introduce specific novel techniques from the framework that train confidence
estimation models using small training sets. Through an empirical study with
datasets spanning the diverse tasks of question answering, summarization, and
text-to-SQL, we demonstrate that our proposed similarity-based methods can
yield better calibrated confidences than baselines.

</details>


### [9] [Seeing Hate Differently: Hate Subspace Modeling for Culture-Aware Hate Speech Detection](https://arxiv.org/abs/2510.13837)
*Weibin Cai,Reza Zafarani*

Main category: cs.CL

> 本文分析了仇恨言论检测中的挑战，并提出了一种文化感知框架，通过构建个体的仇恨子空间来解决这些问题，实验结果表明该方法优于最先进的方法。

<details>
  <summary>Details</summary>

**Motivation:** 现有的仇恨言论检测方法往往忽略了一些现实世界的复杂性：训练标签存在偏差，且不同文化背景的人对仇恨言论的解读也不同。我们分析了这些挑战，包括数据稀疏性、文化纠缠和标签模糊。

**Method:** 我们提出了一种文化感知框架，通过构建个体仇恨子空间来应对数据稀疏性问题。通过组合文化属性进行建模，并使用标签传播来应对文化纠缠和模糊标签的问题，以捕捉每种组合的独特特征。

**Result:** 实验结果表明，我们的方法在所有度量标准上的平均性能优于最先进的方法1.05%。

**Conclusion:** 所提出的文化感知方法能够更好地应对仇恨言论检测中的数据稀疏性、文化纠缠和标签模糊问题，从而提高分类性能。

**Abstract:** Hate speech detection has been extensively studied, yet existing methods
often overlook a real-world complexity: training labels are biased, and
interpretations of what is considered hate vary across individuals with
different cultural backgrounds. We first analyze these challenges, including
data sparsity, cultural entanglement, and ambiguous labeling. To address them,
we propose a culture-aware framework that constructs individuals' hate
subspaces. To alleviate data sparsity, we model combinations of cultural
attributes. For cultural entanglement and ambiguous labels, we use label
propagation to capture distinctive features of each combination. Finally,
individual hate subspaces, which in turn can further enhance classification
performance. Experiments show our method outperforms state-of-the-art by 1.05\%
on average across all metrics.

</details>


### [10] [Meronymic Ontology Extraction via Large Language Models](https://arxiv.org/abs/2510.13839)
*Dekai Zhang,Simone Conia,Antonio Rago*

Main category: cs.CL

> 本文提出了一种利用大型语言模型（LLMs）自动从原始评论文本中提取产品本体（以部分整体关系的形式，即meronymies）的方法，并展示了这种方法生成的本体比现有的BERT基线方法表现更优。

<details>
  <summary>Details</summary>

**Motivation:** 手动构建本体是一个耗时、成本高且繁重的过程，尤其是在电子商务领域，需要对大量的产品列表进行组织。而本体在现今数字化时代对于组织大量可用的非结构化文本信息至关重要。

**Method:** Structure

**Result:** 该方法所生成的本体在使用LLM作为评判标准时，超越了一个现有的BERT基线方法。

**Conclusion:** 本文为利用大型语言模型在本体抽取（无论是产品还是其它领域的本体）方面提供了基础。

**Abstract:** Ontologies have become essential in today's digital age as a way of
organising the vast amount of readily available unstructured text. In providing
formal structure to this information, ontologies have immense value and
application across various domains, e.g., e-commerce, where countless product
listings necessitate proper product organisation. However, the manual
construction of these ontologies is a time-consuming, expensive and laborious
process. In this paper, we harness the recent advancements in large language
models (LLMs) to develop a fully-automated method of extracting product
ontologies, in the form of meronymies, from raw review texts. We demonstrate
that the ontologies produced by our method surpass an existing, BERT-based
baseline when evaluating using an LLM-as-a-judge. Our investigation provides
the groundwork for LLMs to be used more generally in (product or otherwise)
ontology extraction.

</details>


### [11] [ADMIT: Few-shot Knowledge Poisoning Attacks on RAG-based Fact Checking](https://arxiv.org/abs/2510.13842)
*Yutao Wu,Xiao Liu,Yinghui Li,Yifeng Gao,Yifan Ding,Jiale Ding,Xiang Zheng,Xingjun Ma*

Main category: cs.CL

> 本文提出了ADMIT技术，该技术能在真实的事实核查系统中，以极低的投毒率成功执行投毒攻击，揭示了RAG系统在这一场景下的严重漏洞。

<details>
  <summary>Details</summary>

**Motivation:** 本文旨在研究知识投毒在事实核查场景中的应用，特别是在检索池中真实证据占主导地位的复杂情况下。该研究扩展了知识投毒的概念，用于应对更具挑战性的现实问题。

**Method:** 本文提出了一种名为ADMIT的对抗性多注入技术，它是一种少样本、语义一致性的投毒攻击方法，能够在不需要访问目标LLM和检索器的情况下，反转事实核查决策并产生欺骗性的解释。

**Result:** 实验结果表明，ADMIT在4种检索器、11种LLM和4个跨域基准上具有极高的攻击成功率（平均86%），即使在存在强有力反证的情况下依然有效。

**Conclusion:** ADMIT技术相较于现有最先进的攻击方法提高了11.2%的成功率，揭示了基于RAG的事实核查系统在现实中面临的重大安全隐患。

**Abstract:** Knowledge poisoning poses a critical threat to Retrieval-Augmented Generation
(RAG) systems by injecting adversarial content into knowledge bases, tricking
Large Language Models (LLMs) into producing attacker-controlled outputs
grounded in manipulated context. Prior work highlights LLMs' susceptibility to
misleading or malicious retrieved content. However, real-world fact-checking
scenarios are more challenging, as credible evidence typically dominates the
retrieval pool. To investigate this problem, we extend knowledge poisoning to
the fact-checking setting, where retrieved context includes authentic
supporting or refuting evidence. We propose \textbf{ADMIT}
(\textbf{AD}versarial \textbf{M}ulti-\textbf{I}njection \textbf{T}echnique), a
few-shot, semantically aligned poisoning attack that flips fact-checking
decisions and induces deceptive justifications, all without access to the
target LLMs, retrievers, or token-level control. Extensive experiments show
that ADMIT transfers effectively across 4 retrievers, 11 LLMs, and 4
cross-domain benchmarks, achieving an average attack success rate (ASR) of 86\%
at an extremely low poisoning rate of $0.93 \times 10^{-6}$, and remaining
robust even in the presence of strong counter-evidence. Compared with prior
state-of-the-art attacks, ADMIT improves ASR by 11.2\% across all settings,
exposing significant vulnerabilities in real-world RAG-based fact-checking
systems.

</details>


### [12] [Serialized EHR make for good text representations](https://arxiv.org/abs/2510.13843)
*Zhirong Chou,Quan Qin,Shi Li*

Main category: cs.CL

> 介绍SerialBEHRT，一种通过预训练结构化EHR序列来改进长期依赖关系捕获的医疗领域对齐基础模型。在抗生素耐药性预测任务上的测试结果显示，SerialBEHRT优于现有技术，强调了基础模型预训练中时间序列化的重要性。

<details>
  <summary>Details</summary>

**Motivation:** 现有方法难以处理电子健康记录（EHR）中的表格和事件性质与自然语言模型中的顺序先验之间的结构不匹配，从而限制了它们在捕捉患者就诊间的长期依赖性方面的效果。

**Method:** 开发SerialBEHRT模型，这个模型在结构化EHR序列上进行预训练，旨在编码临床事件中的时间与上下文关系，生成更丰富的患者表示。

**Result:** 在抗生素耐药性预测任务中进行广泛基准测试，发现SerialBEHRT比现有EHR表示策略表现更优且更稳定，特别是在抗生素管理上有临床意义。

**Conclusion:** 通过与现有方法的比较，证明了SerialBEHRT在捕捉临床事件时间与上下文信息方面的有效性。强调了医疗领域基础模型预训练中时间序列化的重要性。

**Abstract:** The emergence of foundation models in healthcare has opened new avenues for
learning generalizable representations from large scale clinical data. Yet,
existing approaches often struggle to reconcile the tabular and event based
nature of Electronic Health Records (EHRs) with the sequential priors of
natural language models. This structural mismatch limits their ability to
capture longitudinal dependencies across patient encounters. We introduce
SerialBEHRT, a domain aligned foundation model that extends SciBERT through
additional pretraining on structured EHR sequences. SerialBEHRT is designed to
encode temporal and contextual relationships among clinical events, thereby
producing richer patient representations. We evaluate its effectiveness on the
task of antibiotic susceptibility prediction, a clinically meaningful problem
in antibiotic stewardship. Through extensive benchmarking against state of the
art EHR representation strategies, we demonstrate that SerialBEHRT achieves
superior and more consistent performance, highlighting the importance of
temporal serialization in foundation model pretraining for healthcare.

</details>


### [13] [DynaSpec: Context-aware Dynamic Speculative Sampling for Large-Vocabulary Language Models](https://arxiv.org/abs/2510.13847)
*Jinbin Zhang,Nasib Ullah,Erik Schultheis,Rohit Babbar*

Main category: cs.CL

> 为解决LLM词汇扩充导致的令牌数量增多引起的问题，提出DynaSpec，一种基于上下文的动态短名单机制，有效提高了推测解码的速度和准确性。

<details>
  <summary>Details</summary>

**Motivation:** 随着大语言模型词汇表的扩大，候选标记数量大大增加，导致草案头部的参数数量增加，进而增加了延迟。现有方法虽然减少了草案的时间计算，但固定词汇表的方法是脆弱的，依赖于语料库频率列表，抑制了罕见或领域特定的标记，这限制了预期的每验证步标记数量。

**Method:** 提出了DynaSpec，这是一种基于上下文的动态短名单机制。具体来说，引入了轻量级的粗粒度元分类器，将上下文路由到少量的令牌簇中；所选最顶部的k个簇的并集构成草稿人的短名单，而验证保留全部词汇表和精确度。元分类器通过并行执行草案编码和元短名单选择来提前完成计算。

**Result:** 在标准推测解码基准上，相比固定短名单基线，观察到平均接受长度的一致提升。通过上下文依赖的选择，使小短名单能够在不降低接受度的情况下实现。

**Conclusion:** DynaSpec机制展示了稳健性，加快了草案生成，并且跨多样任务具有一般性。

**Abstract:** Speculative decoding (a.k.a. speculative sampling) has become a standard way
to accelerate LLM inference: a small drafter proposes multiple tokens and a
large target model verifies them once per speculation length. Recently, scaling
of the LLM vocabulary has pushed the number of tokens to grow substantially.
While verification over the full vocabulary leaves the target model largely
unaffected, the O(|V|d) parameters in the drafter's output head become a
latency bottleneck, slowing the entire pipeline. Contemporary methods (e.g.,
FR-Spec, VocabTrim) restrict the drafter's vocabulary to a fixed subset of the
target model's vocabulary, ranked in descending order of token frequency.
Although this reduces draft-time compute, it is brittle, since: (i) frequency
lists are corpus-dependent and require retuning to generalize, and (ii) static
shortlists suppress rare or domain-specific tokens, lowering the expected
number of tokens per verification step. We propose DynaSpec, a
context-dependent dynamic shortlisting mechanism that is robust, speeds up
drafting, and generalizes across diverse tasks. Concretely, we introduce
lightweight, coarse-grained meta-classifiers that route contexts to a small
number of token clusters; the union of the top-k selected clusters forms the
drafter's shortlist, while verification retains the full vocabulary and
exactness. The meta-classifier finishes its computation earlier than the
drafter's hidden state generation by exploiting parallel execution of draft
encoding and meta shortlisting on separate streams. On standard
speculative-decoding benchmarks, we observe consistent gains in mean accepted
length over fixed-shortlist baselines, while context-dependent selection
enables smaller shortlists without degrading acceptance.

</details>


### [14] [On-device System of Compositional Multi-tasking in Large Language Models](https://arxiv.org/abs/2510.13848)
*Ondrej Bohdal,Konstantinos Theodosiadis,Asterios Mpatziakas,Dimitris Filippidis,Iro Spyrou,Christos Zonios,Anastasios Drosou,Dimosthenis Ioannidis,Kyeng-Hun Lee,Jijoong Moon,Hyeonmok Ko,Mete Ozay,Umberto Michieli*

Main category: cs.CL

> 针对复杂多任务处理，我们提出了一种新的方法，通过在总结和翻译适配器之上添加可学习的投影层来提高效率，实验表明此方法适用于云和设备终端，具有高效操作的潜力。

<details>
  <summary>Details</summary>

**Motivation:** 现有的参数高效微调技术难以有效应对复杂任务的同时执行，如从长对话生成翻译摘要，因此我们提出了这个新的方法来解决这个挑战。

**Method:** 我们的方法是通过在总结和翻译适配器之上添加一个可学习的投影层来解决复杂任务的多任务处理问题，这种方式可以有效整合任务并减少计算开销。

**Result:** 实验结果显示，我们的解决方案在云环境和设备终端都表现良好且速度快，验证了我们方法的实用性。

**Conclusion:** 本研究提出的方法展示了在资源受限和需要高速操作的实际应用中的潜力，特别是在移动设备上运行时表现出色。

**Abstract:** Large language models (LLMs) are commonly adapted for diverse downstream
tasks via parameter-efficient fine-tuning techniques such as Low-Rank Adapters
(LoRA). While adapters can be combined to handle multiple tasks separately,
standard approaches struggle when targeting the simultaneous execution of
complex tasks, such as generating a translated summary from a long
conversation. To address this challenge, we propose a novel approach tailored
specifically for compositional multi-tasking scenarios involving summarization
and translation. Our technique involves adding a learnable projection layer on
top of the combined summarization and translation adapters. This design enables
effective integration while maintaining efficiency through reduced
computational overhead compared to alternative strategies requiring extensive
retraining or sequential processing. We demonstrate the practical viability of
our method within an on-device environment by developing an Android app capable
of executing compositional tasks seamlessly. Experimental results indicate our
solution performs well and is fast in both cloud-based and on-device
implementations, highlighting the potential benefits of adopting our framework
in real-world applications demanding high-speed operation alongside resource
constraints.

</details>


### [15] [Language steering in latent space to mitigate unintended code-switching](https://arxiv.org/abs/2510.13849)
*Andrey Goncharov,Nikolai Kondusov,Alexey Zaytsev*

Main category: cs.CL

> 论文介绍了一种新的方法——潜空间语言导向，以减少多语言大型语言模型中的代码切换现象，这种方法在推理时使用轻量级的方式控制语言身份，几乎不增加计算负担。

<details>
  <summary>Details</summary>

**Motivation:** 多语言大型语言模型常常因意外代码切换而降低下游任务的可靠性。

**Method:** 我们提出了一种称为潜空间语言导向的方法，通过PCA对平行翻译进行处理来识别语言方向，并通过调整词嵌入使其沿着这些轴移动以控制语言身份，从而减少多语言大型语言模型中的意外代码切换现象。

**Result:** 在Qwen2.5和Llama-3.2模型上，使用单一主成分实现了95-99%的语言分类准确率，并将下一词分布分歧减少了最多42%。此外，分析还揭示了语言表示在最终层附近几乎呈完美线性可分。

**Conclusion:** 提出的方法能够在不大幅提升计算成本的情况下，有效减少多语言模型的代码切换问题，并保持语义的一致性。实验结果显示了该方法在提高语言一致性方面的潜力。

**Abstract:** Multilingual Large Language Models (LLMs) often exhibit unintended
code-switching, reducing reliability in downstream tasks. We propose
latent-space language steering, a lightweight inference-time method that
identifies language directions via PCA on parallel translations and steers
token embeddings along these axes to control language identity. Our approach
mitigates code-switching while preserving semantics with negligible
computational overhead and requires only minimal parallel data for calibration.
Empirically, we achieve 95-99\% language classification accuracy using a single
principal component and reduce next-token distributional divergence by up to
42% across multiple language pairs on Qwen2.5 and Llama-3.2 models. We further
analyze the layer-wise evolution of language representations, revealing that
language identity concentrates in final layers with near-perfect linear
separability.

</details>


### [16] [Revisiting the UID Hypothesis in LLM Reasoning Traces](https://arxiv.org/abs/2510.13850)
*Minju Gwak,Guijin Son,Jaehyung Kim*

Main category: cs.CL

> 研究通过熵基度量分析大语言模型推理过程的信息流，发现成功的机器推理表现出非均匀的信息密度，挑战了关于机器推理的假设，并为设计可解释和适应性强的推理模型提供了新方向。

<details>
  <summary>Details</summary>

**Motivation:** 研究动机在于探索大语言模型（LLMs）的推理步骤是否有类似人类交流的信息密度均匀性，并挑战现有对于机器推理的假设。

**Method:** 该研究采用熵基度量方法，对LLMs中的推理过程中的信息流进行了分析，试图验证心理语言学中的均匀信息密度假设是否适用于LLMs的推理过程。

**Result:** 研究表明，成功推理的LLMs呈现出全局非均匀的信息密度特征，这与人类的交流方式形成鲜明对比。

**Conclusion:** 研究结果挑战了机器推理的假设，并为设计更加可解释和适应性的推理模型指明了新方向。

**Abstract:** Large language models (LLMs) often solve problems using step-by-step
Chain-of-Thought (CoT) reasoning, yet these intermediate steps are frequently
unfaithful or hard to interpret. Inspired by the Uniform Information Density
(UID) hypothesis in psycholinguistics -- which posits that humans communicate
by maintaining a stable flow of information -- we introduce entropy-based
metrics to analyze the information flow within reasoning traces. Surprisingly,
across three challenging mathematical benchmarks, we find that successful
reasoning in LLMs is globally non-uniform: correct solutions are characterized
by uneven swings in information density, in stark contrast to human
communication patterns. This result challenges assumptions about machine
reasoning and suggests new directions for designing interpretable and adaptive
reasoning models.

</details>


### [17] [EvoEdit: Evolving Null-space Alignment for Robust and Efficient Knowledge Editing](https://arxiv.org/abs/2510.13851)
*Sicheng Lyu,Yu Gu,Xinyu Wang,Jerry Huang,Sitao Luan,Yufei Cui,Xiao-Wen Chang,Peng Lu*

Main category: cs.CL

> EvoEdit, a new editing strategy for large language models, uses sequential null-space alignment to efficiently and stably introduce targeted updates without retraining, demonstrating superior performance and a significant speed improvement over existing methods.

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind this work is to overcome the limitations of existing LLM model editing techniques, which suffer from catastrophic interference when performing multiple updates. These issues lead to a degradation of previously integrated knowledge and require a more robust editing methodology.

**Method:** The paper introduces EvoEdit, a novel strategy for continuous updates to large language models (LLMs) without retraining. It uses sequential null-space alignment to preserve both the original and updated knowledge, mitigating catastrophic interference and maintaining output consistency across multiple edits.

**Result:** Evaluations on real-world sequential knowledge-editing benchmarks reveal that EvoEdit not only matches or outperforms state-of-the-art model editing techniques but also offers a significant speedup of up to 3.53 times.

**Conclusion:** The paper concludes by highlighting the success of EvoEdit in addressing the challenges in LLM editing, producing a more stable and efficient editing strategy with strong theoretical underpinnings. It emphasizes the importance of developing such principled approaches for managing dynamically evolving information.

**Abstract:** Large language models (LLMs) require continual updates to rectify outdated or
erroneous knowledge. Model editing has emerged as a compelling paradigm for
introducing targeted modifications without the computational burden of full
retraining. Existing approaches are mainly based on a locate-then-edit
framework. However, in sequential editing contexts, where multiple updates are
applied over time, they exhibit significant limitations and suffer from
catastrophic interference, i.e., new edits compromise previously integrated
updates and degrade preserved knowledge. To address these challenges, we
introduce EvoEdit, a novel editing strategy that mitigates catastrophic
interference through sequential null-space alignment, enabling stable and
efficient model editing. By performing sequential null-space alignment for each
incoming edit, EvoEdit preserves both original and previously modified
knowledge representations and maintains output invariance on preserved
knowledge even across long edit sequences, effectively mitigating interference.
Evaluations on real-world sequential knowledge-editing benchmarks show that
EvoEdit achieves better or comparable performance than prior state-of-the-art
locate-then-edit techniques, with up to 3.53 times speedup. Overall, these
results underscore the necessity of developing more principled approaches for
designing LLMs in dynamically evolving information settings, while providing a
simple yet effective solution with strong theoretical guarantees.

</details>


### [18] [ConsistencyAI: A Benchmark to Assess LLMs' Factual Consistency When Responding to Different Demographic Groups](https://arxiv.org/abs/2510.13852)
*Peter Banyas,Shristi Sharma,Alistair Simmons,Atharva Vispute*

Main category: cs.CL

> 本研究通过ConsistencyAI测试了大型语言模型在不同人设下的一致性。结果显示一致性因模型和主题而异。

<details>
  <summary>Details</summary>

**Motivation:** 为了评估和确保大型语言模型在面对不同用户群体时提供一致的事实信息，避免因用户特性而导致的事实偏差。

**Method:** 提出了ConsistencyAI，这是一个独立的基准测试，用于衡量大型语言模型（LLMs）在不同的人设下提供一致事实的能力。实验中，对19个LLMs进行了测试，请求每个模型为15个主题提供5个事实，并重复了100次查询，每次查询加入不同的人设背景。通过计算跨人设的cosine相似度加权平均得分来衡量一致性。

**Result:** 得分范围从0.7896到0.9065，平均分为0.8656，作为一致性基准阈值。xAI的Grok-3模型表现最一致，而一些轻量级模型排名较低。一致性还受到主题的影响，例如劳动力市场一致性最低，G7国家领导人一致性最高。

**Conclusion:** 提供了一种独立的基准测试方式，并发布了代码和互动演示，支持可重复性评估和鼓励设计不依赖于人设的提示策略。

**Abstract:** Is an LLM telling you different facts than it's telling me? This paper
introduces ConsistencyAI, an independent benchmark for measuring the factual
consistency of large language models (LLMs) for different personas.
ConsistencyAI tests whether, when users of different demographics ask identical
questions, the model responds with factually inconsistent answers. Designed
without involvement from LLM providers, this benchmark offers impartial
evaluation and accountability. In our experiment, we queried 19 LLMs with
prompts that requested 5 facts for each of 15 topics. We repeated this query
100 times for each LLM, each time adding prompt context from a different
persona selected from a subset of personas modeling the general population. We
processed the responses into sentence embeddings, computed cross-persona cosine
similarity, and computed the weighted average of cross-persona cosine
similarity to calculate factual consistency scores. In 100-persona experiments,
scores ranged from 0.9065 to 0.7896, and the mean was 0.8656, which we adopt as
a benchmark threshold. xAI's Grok-3 is most consistent, while several
lightweight models rank lowest. Consistency varies by topic: the job market is
least consistent, G7 world leaders most consistent, and issues like vaccines or
the Israeli-Palestinian conflict diverge by provider. These results show that
both the provider and the topic shape the factual consistency. We release our
code and interactive demo to support reproducible evaluation and encourage
persona-invariant prompting strategies.

</details>


### [19] [BenchPress: A Human-in-the-Loop Annotation System for Rapid Text-to-SQL Benchmark Curation](https://arxiv.org/abs/2510.13853)
*Fabian Wenz,Omar Bouattour,Devin Yang,Justin Choi,Cecil Gregg,Nesime Tatbul,Çağatay Demiralp*

Main category: cs.CL

> 本文介绍了一个名为BenchPress的人机协作系统，加速了特定领域文本到SQL基准的创建，显著减少了时间成本并提高了标注准确性和模型评估的鲁棒性。

<details>
  <summary>Details</summary>

**Motivation:** 由于手动注释SQL日志去识别其对应的自然语言问题非常费时且成本高昂，本文旨在通过一种新方法来克服这一挑战。

**Method:** BenchPress系统通过RAG和LLM为SQL日志产生自然语言描述，然后由人类专家进行选择、编辑或排序，以维持精确性和与企业领域的相关性。

**Result:** 本文提出了一种名为BenchPress的人机协作系统，旨在加速特定领域文本到SQL基准的创建。面对大型企业数据仓库中手动注释SQL日志的挑战，BenchPress利用检索增强生成(RAG)和大型语言模型(LLM)为给定的SQL查询产生多个自然语言描述。通过专家选择、排序或编辑描述，确保其准确性和领域的相符性。评估结果表明，结合人类验证与LLM生成的建议，可以显著减少创建高质量基准的时间和努力，从而提高标注的准确性和模型评估的鲁棒性。BenchPress在公共GitHub仓库可以免费获取，网址为：https://github.com/fabian-wenz/enterprise-txt2sql，也可通过官网访问：http://dsg-mcgraw.csail.mit.edu:5000。

**Conclusion:** 结合人类专家验证与LLM生成的建议，BenchPress能够大幅减少创建高质量文本到SQL基准的时间和努力，增强标注的准确性和模型评估的鲁棒性，提供了一种有效机制来评估文本到SQL模型在特定领域工作负载上的性能。

**Abstract:** Large language models (LLMs) have been successfully applied to many tasks,
including text-to-SQL generation. However, much of this work has focused on
publicly available datasets, such as Fiben, Spider, and Bird. Our earlier work
showed that LLMs are much less effective in querying large private enterprise
data warehouses and released Beaver, the first private enterprise text-to-SQL
benchmark. To create Beaver, we leveraged SQL logs, which are often readily
available. However, manually annotating these logs to identify which natural
language questions they answer is a daunting task. Asking database
administrators, who are highly trained experts, to take on additional work to
construct and validate corresponding natural language utterances is not only
challenging but also quite costly. To address this challenge, we introduce
BenchPress, a human-in-the-loop system designed to accelerate the creation of
domain-specific text-to-SQL benchmarks. Given a SQL query, BenchPress uses
retrieval-augmented generation (RAG) and LLMs to propose multiple natural
language descriptions. Human experts then select, rank, or edit these drafts to
ensure accuracy and domain alignment. We evaluated BenchPress on annotated
enterprise SQL logs, demonstrating that LLM-assisted annotation drastically
reduces the time and effort required to create high-quality benchmarks. Our
results show that combining human verification with LLM-generated suggestions
enhances annotation accuracy, benchmark reliability, and model evaluation
robustness. By streamlining the creation of custom benchmarks, BenchPress
offers researchers and practitioners a mechanism for assessing text-to-SQL
models on a given domain-specific workload. BenchPress is freely available via
our public GitHub repository at
https://github.com/fabian-wenz/enterprise-txt2sql and is also accessible on our
website at http://dsg-mcgraw.csail.mit.edu:5000.

</details>


### [20] [R2T: Rule-Encoded Loss Functions for Low-Resource Sequence Tagging](https://arxiv.org/abs/2510.13854)
*Mamadou K. Keita,Christopher Homan,Sebastien Diarra*

Main category: cs.CL

> 规则到标签（R2T）框架将语言规则整合到神经网络训练中，使用自适应损失函数，有效地处理词汇外词汇。实验表明，R2T使得在更少标签数据情况下仍能达到更好的准确率。

<details>
  <summary>Details</summary>

**Motivation:** 该研究旨在展示一种称为原则学习(PrL)的范式，其中模型是使用明确的任务约束进行训练，而不仅仅是使用标签示例。

**Method:** 提出了Rule-to-Tag (R2T) 框架，这是一种将多级语言规则直接整合入神经网络训练目标的混合方法。该框架的创新点在于其自适应损失函数，其中包含一个正则化项，可以训练模型以原则性的不确定性处理词汇外（OOV）词语。

**Result:** 实验结果表明，在扎尔马语词性标注任务中，只用未标注文本训练的R2T-BiLSTM模型达到了98.2%的准确率，超过了基于非洲BERTa微调的基线模型，仅使用300个标签句子进行训练。此外，研究还表明，在诸如命名实体识别（NER）等更复杂的任务中，R2T可以用作强有力的预训练步骤。

**Conclusion:** R2T框架展示了一种将语言规则和神经网络结合的新范式，显著提升了模型在词汇外词和标注数据有限情况下的表现。

**Abstract:** We introduce the Rule-to-Tag (R2T) framework, a hybrid approach that
integrates a multi-tiered system of linguistic rules directly into a neural
network's training objective. R2T's novelty lies in its adaptive loss function,
which includes a regularization term that teaches the model to handle
out-of-vocabulary (OOV) words with principled uncertainty. We frame this work
as a case study in a paradigm we call principled learning (PrL), where models
are trained with explicit task constraints rather than on labeled examples
alone. Our experiments on Zarma part-of-speech (POS) tagging show that the
R2T-BiLSTM model, trained only on unlabeled text, achieves 98.2% accuracy,
outperforming baselines like AfriBERTa fine-tuned on 300 labeled sentences. We
further show that for more complex tasks like named entity recognition (NER),
R2T serves as a powerful pre-training step; a model pre-trained with R2T and
fine-tuned on just 50 labeled sentences outperformes a baseline trained on 300.

</details>


### [21] [Harnessing Consistency for Robust Test-Time LLM Ensemble](https://arxiv.org/abs/2510.13855)
*Zhichen Zeng,Qi Yu,Xiao Lin,Ruizhong Qiu,Xuying Ning,Tianxin Wei,Yuchen Yan,Jingrui He,Hanghang Tong*

Main category: cs.CL

> 提出了CoRE技术，通过在大语言模型集成中利用模型一致性来提高鲁棒性。

<details>
  <summary>Details</summary>

**Motivation:** 尽管在提高集成质量方面有显著进展，但很少关注集成的鲁棒性问题，特别是在处理异构分词方案和不同模型专长造成的错误信号时。

**Method:** CoRE 使用了两种一致性机制：token 级别的一致性捕捉细粒度的分歧，模型级别的一致性捕捉全局的一致性。

**Result:** 通过广泛的实验，CoRE 在多种基准、模型组合和集成策略中都显示出了一致的性能提升和鲁棒性的提高。

**Conclusion:** CoRE 提出了一种利用模型一致性来增强大语言模型集成鲁棒性的方法，极大地改进了集成的表现和鲁棒性。

**Abstract:** Different large language models (LLMs) exhibit diverse strengths and
weaknesses, and LLM ensemble serves as a promising approach to integrate their
complementary capabilities. Despite substantial progress in improving ensemble
quality, limited attention has been paid to the robustness of ensembles against
potential erroneous signals, which often arise from heterogeneous tokenization
schemes and varying model expertise. Our analysis shows that ensemble failures
typically arise from both the token level and the model level: the former
reflects severe disagreement in token predictions, while the latter involves
low confidence and pronounced disparities among models. In light of this, we
propose CoRE, a plug-and-play technique that harnesses model consistency for
robust LLM ensemble, which can be seamlessly integrated with diverse ensemble
methods. Token-level consistency captures fine-grained disagreements by
applying a low-pass filter to downweight uncertain tokens with high
inconsistency, often due to token misalignment, thereby improving robustness at
a granular level. Model-level consistency models global agreement by promoting
model outputs with high self-confidence and minimal divergence from others,
enhancing robustness at a coarser level. Extensive experiments across diverse
benchmarks, model combinations, and ensemble strategies demonstrate that CoRE
consistently improves ensemble performance and robustness.

</details>


### [22] [Multimodal Retrieval-Augmented Generation with Large Language Models for Medical VQA](https://arxiv.org/abs/2510.13856)
*A H M Rezaul Karim,Ozlem Uzuner*

Main category: cs.CL

> 研究介绍了一种名为MasonNLP的系统，该系统在无需额外训练或复杂重排序的情况下，通过添加少量相关实例的方式，利用带有检索增强生成框架的通用域大规模语言模型来进行临床相关的伤病视觉问答，结果表明这种方法在多个评估指标上表现良好。

<details>
  <summary>Details</summary>

**Motivation:** 该研究旨在通过支持临床决策和患者护理的自然语言查询来增强医学视觉问答（MedVQA）。为了实现这一点，研究参与了MEDIQA-WV 2025共同任务，专门针对伤口护理VQA，要求系统从图像和患者查询中生成自由文本回复和结构化的伤口属性。

**Method:** MasonNLP系统采用了一种基于检索增强生成(RAG)框架的通用领域、指令调整大型语言模型，该框架整合了来自领域内数据的文本和图像实例，以提高输出的临床相关性、推理能力和遵循模式及响应质量。

**Result:** MasonNLP系统在19支队伍和51份提交物中排名第3，平均得分为41.37％。这表明，轻量级的RAG和通用目的的大型语言模型提供了一个简单的、有效的多模态临床NLP任务基线。

**Conclusion:** 研究表明，轻量级RAG结合通用目的大语言模型，作为一种在推理时间添加少量相关实例的简单方法，没有额外训练或复杂重排序，在多模态临床NLP任务上形成了一个简单而有效的基线。

**Abstract:** Medical Visual Question Answering (MedVQA) enables natural language queries
over medical images to support clinical decision-making and patient care. The
MEDIQA-WV 2025 shared task addressed wound-care VQA, requiring systems to
generate free-text responses and structured wound attributes from images and
patient queries. We present the MasonNLP system, which employs a
general-domain, instruction-tuned large language model with a
retrieval-augmented generation (RAG) framework that incorporates textual and
visual examples from in-domain data. This approach grounds outputs in
clinically relevant exemplars, improving reasoning, schema adherence, and
response quality across dBLEU, ROUGE, BERTScore, and LLM-based metrics. Our
best-performing system ranked 3rd among 19 teams and 51 submissions with an
average score of 41.37%, demonstrating that lightweight RAG with
general-purpose LLMs -- a minimal inference-time layer that adds a few relevant
exemplars via simple indexing and fusion, with no extra training or complex
re-ranking -- provides a simple and effective baseline for multimodal clinical
NLP tasks.

</details>


### [23] [ShishuLM: Lightweight Language Model with Hybrid Decoder-MLP Architecture and Paired Weight Sharing](https://arxiv.org/abs/2510.13860)
*Shivanshu Kumar,Gopalakrishnan Srinivasan*

Main category: cs.CL

> ShishuLM 使用新型架构减少了模型参数和KV缓存需求，显著提升了小规模语言模型的效率。

<details>
  <summary>Details</summary>

**Motivation:** 优化Transformer架构，降低内存和计算开销，同时不牺牲性能。

**Method:** 通过归一化和注意力计算的耦合，并用多层感知器替换Transformer模块，减少参数和KV缓存需求。

**Result:** 相比原模型，ShishuLM在内存需求上减少了25%，在训练和推断延迟上提升了40%。

**Conclusion:** 为构建更高效的小规模语言模型架构提供了见解，有助于提升基于AI的代理系统的性能。

**Abstract:** While the transformer architecture has achieved state-of-the-art performance
on natural language processing tasks, these models impose substantial memory
and computational overhead. Recent research has identified significant
architectural redundancies within these models, presenting opportunities for
optimization without compromising performance. Taking insights from research in
AI interpretability and inference-time layer pruning, we introduce an efficient
language model architecture, referred to as ShishuLM, which reduces both the
parameter count and Key-Value (KV) cache requirements. Given the increasing
importance of Small Language Models (SLMs) in agentic AI systems, we evaluate
our approach on two SLMs of different scales. Our analysis reveals that for
moderate-context scenarios, normalization coupled with attention computation is
roughly linear with the input, enabling entire transformer blocks to be
approximated through Multi-Layer Perceptrons (MLPs). Our results show that
ShishuLM provides up to 25% reduction in memory requirements and up to 40%
improvement in latency during both training and inference, compared to parent
models. Our experimental and analytical findings provide insights towards
building more efficient SLM architectures from a pre-training standpoint.

</details>


### [24] [Ensembling Large Language Models to Characterize Affective Dynamics in Student-AI Tutor Dialogues](https://arxiv.org/abs/2510.13862)
*Chenyu Zhang,Sharifa Alghowinem,Cynthia Breazeal*

Main category: cs.CL

> 本研究首次引入了基于集合型LLM的大规模情绪感知框架，用于分析AI辅助教学中的学生情绪动态，并通过分析与PyTutor互动的学生数据，发现情绪状态对学习体验的影响，特别是如何从消极情绪转变为积极情绪或保持情绪稳定。

<details>
  <summary>Details</summary>

**Motivation:** 尽管之前有研究探讨了大型语言模型(LLM)在教育中的影响，但是关于LLM辅助教学中情绪动态的理解还不充分。本研究的目标是填补这一空白，提出一种适用于大规模情绪感知的集合型LLM框架，增强负责任地将生成式AI整合进教育领域的对话。

**Method:** 本研究通过分析16,986条对话记录，这些记录来自于PyTutor与261名大学生在两个学期内的交流。研究使用Gemini、GPT-4o和Claude三个前沿LLM来生成零样本情绪标注，包括情绪效价、唤醒度和学习帮助度的情感标签及文本描述。通过模型间的加权融合和多数决议，生成稳定的情绪概况。

**Result:** 分析结果显示学生在与AI导师交互时通常表现出微小的积极情绪和中等唤醒度。尽管学习不总是顺畅的，但困惑和好奇心通常伴随着问题解决过程，沮丧虽然出现较少，但仍可能阻碍进展。情绪状态短暂存在，积极时刻比消极或中性时刻稍长，但积极情绪也很容易被干扰。但是，消极情绪往往能迅速解决，有时甚至直接转向积极情绪。中性时刻常作为转折点，更多的时候帮助学生情绪上升，这为导师提供了干预的机会。

**Conclusion:** 研究展示了集合型LLM框架在情绪感知中的潜力，并强调了针对学生情绪动态的情感洞察和教学干预的重要性。

**Abstract:** While recent studies have examined the leaning impact of large language model
(LLM) in educational contexts, the affective dynamics of LLM-mediated tutoring
remain insufficiently understood. This work introduces the first ensemble-LLM
framework for large-scale affect sensing in tutoring dialogues, advancing the
conversation on responsible pathways for integrating generative AI into
education by attending to learners' evolving affective states. To achieve this,
we analyzed two semesters' worth of 16,986 conversational turns exchanged
between PyTutor, an LLM-powered AI tutor, and 261 undergraduate learners across
three U.S. institutions. To investigate learners' emotional experiences, we
generate zero-shot affect annotations from three frontier LLMs (Gemini, GPT-4o,
Claude), including scalar ratings of valence, arousal, and
learning-helpfulness, along with free-text emotion labels. These estimates are
fused through rank-weighted intra-model pooling and plurality consensus across
models to produce robust emotion profiles. Our analysis shows that during
interaction with the AI tutor, students typically report mildly positive affect
and moderate arousal. Yet learning is not uniformly smooth: confusion and
curiosity are frequent companions to problem solving, and frustration, while
less common, still surfaces in ways that can derail progress. Emotional states
are short-lived--positive moments last slightly longer than neutral or negative
ones, but they are fragile and easily disrupted. Encouragingly, negative
emotions often resolve quickly, sometimes rebounding directly into positive
states. Neutral moments frequently act as turning points, more often steering
students upward than downward, suggesting opportunities for tutors to intervene
at precisely these junctures.

</details>


### [25] [Unlocking the Potential of Diffusion Language Models through Template Infilling](https://arxiv.org/abs/2510.13870)
*Junhoo Lee,Seungyeon Kim,Nojun Kwak*

Main category: cs.CL

> 本文提出了一种针对扩散语言模型（DLMs）生成过程的条件生成新方法——模板填充（TI），展示了该方法在数学推理和代码生成基准测试中的有效性，并且证明了其在多标记生成设置中的优势。

<details>
  <summary>Details</summary>

**Motivation:** 扩散语言模型（DLMs）作为一种有前途的自回归语言模型替代品已经出现，但其推理策略仍然局限于从自回归范式继承的基于前缀的提示。

**Method:** 我们提出了模板填充（TI）作为一种特制的条件生成方法，用于扩散语言模型（DLMs）。与传统的基于前缀提示的方法不同，TI 首先为目标响应生成一个结构模板，然后填充掩码段。此外，我们还引入了动态片段分配（DSA）来增强这种结构控制的灵活性，DSA 会根据生成置信度自适应调整片段长度。

**Result:** 我们在数学推理和代码生成基准测试中展示了我们方法的有效性，与基线相比取得了 17.01% 的一致提升。此外，我们还表明，TI 在多标记生成设置中提供了额外的优势，同时实现了生成速度的加速和生成质量的保持。

**Conclusion:** 本研究提出了一种基于模板填充的技术来改进扩散语言模型的生成质量，并展示了在多个基准测试中该方法的有效性，尤其是在多标记生成方面有所提高。

**Abstract:** Diffusion Language Models (DLMs) have emerged as a promising alternative to
Autoregressive Language Models, yet their inference strategies remain limited
to prefix-based prompting inherited from the autoregressive paradigm. In this
paper, we propose Template Infilling (TI), a tailored conditioning methodology
for DLMs' generation process. Unlike conventional prefix prompting, TI first
generates a structural template for the target response, then fills in the
masked segments. To enhance the flexibility of this structural control, we
introduce Dynamic Segment Allocation (DSA), which adaptively adjusts segment
lengths based on generation confidence. We demonstrate the effectiveness of our
approach on mathematical reasoning and code generation benchmarks, achieving
consistent improvements of 17.01$\%$p over baseline. Furthermore, we show that
TI provides additional advantages in multi-token generation settings, enabling
effective speedup while maintaining generation quality.

</details>


### [26] [Quechua Speech Datasets in Common Voice: The Case of Puno Quechua](https://arxiv.org/abs/2510.13871)
*Elwin Huaman,Wendi Huaman,Jorge Luis Huaman,Ninfa Quispe*

Main category: cs.CL

> 本文探究了将克丘亚语融入Common Voice的可行性，以Puno克丘亚语为例展示了成功的语音数据收集，并强调了Common Voice对促进低资源语言社区发展的重要性。

<details>
  <summary>Details</summary>

**Motivation:** 为了应对资源匮乏语言（如克丘亚语）在语音技术发展中的数据和资源匮乏问题，我们探索了Common Voice这一开放、社区驱动的数据集创建平台对促进这类语言发展的重要性。

**Method:** 本研究详细介绍了将17种克丘亚语纳入Common Voice的过程，并以Puno克丘亚语（ISO 639-3: qxp）为例，探讨了语言引入和语料收集的工作，包括阅读和自发语音数据。

**Result:** 结果显示，Common Voice现在托管了191.1小时的克丘亚语语音数据（其中86%已被验证），其中Puno克丘亚语贡献了12小时语音数据（77%已被验证）。

**Conclusion:** 本研究为低资源语言社区的包容性语音技术和数字化赋权作出了贡献，并提出了未来的研究议程，旨在解决技术挑战及社区参与和土著数据主权的伦理考虑。

**Abstract:** Under-resourced languages, such as Quechuas, face data and resource scarcity,
hindering their development in speech technology. To address this issue, Common
Voice presents a crucial opportunity to foster an open and community-driven
speech dataset creation. This paper examines the integration of Quechua
languages into Common Voice. We detail the current 17 Quechua languages,
presenting Puno Quechua (ISO 639-3: qxp) as a focused case study that includes
language onboarding and corpus collection of both reading and spontaneous
speech data. Our results demonstrate that Common Voice now hosts 191.1 hours of
Quechua speech (86\% validated), with Puno Quechua contributing 12 hours (77\%
validated), highlighting the Common Voice's potential. We further propose a
research agenda addressing technical challenges, alongside ethical
considerations for community engagement and indigenous data sovereignty. Our
work contributes towards inclusive voice technology and digital empowerment of
under-resourced language communities.

</details>


### [27] [FRACCO: A gold-standard annotated corpus of oncological entities with ICD-O-3.1 normalisation](https://arxiv.org/abs/2510.13873)
*Johann Pignat,Milena Vucetic,Christophe Gaudet-Blavignac,Jamil Zaghir,Amandine Stettler,Fanny Amrein,Jonatan Bonjour,Jean-Philippe Goldman,Olivier Michielin,Christian Lovis,Mina Bjelogrlic*

Main category: cs.CL

> 本文介绍了FRACCO，一个法语临床肿瘤学的注释语料库，以解决法语肿瘤学资源稀缺的问题，为法语文本中的命名实体识别和概念规范化提供了重要的参考标准。

<details>
  <summary>Details</summary>

**Motivation:** 鉴于开发针对临床文本的自然语言处理工具需要标注数据集，而法语肿瘤学资源相对稀缺，开发FRACCO的目标是弥补这一空白。

**Method:** 本文介绍了FRACCO（法语临床肿瘤学注释语料库）的开发，这是一个由1301份合成的法语文本病例组成的专家注释语料库。这些病例最初是从西班牙的CANTEMIST语料库翻译过来，作为FRASIMED倡议的一部分。每个文档都被标注了与形态学、解剖位置和组织分化相关的术语，参考了国际疾病分类-肿瘤编码(ICD-O)。同时还添加了一个额外的标注层，捕捉能够组合多个ICD-O元素的临床概念的规范表达。标注质量通过专家的审查得到保证：1301篇文本经过两位领域专家的手动标注实体跨度进行处理。总共生成了71127个ICD-O规范，通过自动匹配和由五名标注员团队的手动验证相结合的方式完成。

**Result:** 最终的数据集包含了399个唯一的形态代码（来自2549种不同的表达），272个解剖位置代码（来自3143种不同的表达），以及2043个唯一的复合表达（来自11144种不同的表达）。

**Conclusion:** 这个数据集为法语文本中肿瘤学领域的命名实体识别和概念规范化提供了一种参考标准。

**Abstract:** Developing natural language processing tools for clinical text requires
annotated datasets, yet French oncology resources remain scarce. We present
FRACCO (FRench Annotated Corpus for Clinical Oncology) an expert-annotated
corpus of 1301 synthetic French clinical cases, initially translated from the
Spanish CANTEMIST corpus as part of the FRASIMED initiative. Each document is
annotated with terms related to morphology, topography, and histologic
differentiation, using the International Classification of Diseases for
Oncology (ICD-O) as reference. An additional annotation layer captures
composite expression-level normalisations that combine multiple ICD-O elements
into unified clinical concepts. Annotation quality was ensured through expert
review: 1301 texts were manually annotated for entity spans by two domain
experts. A total of 71127 ICD-O normalisations were produced through a
combination of automated matching and manual validation by a team of five
annotators. The final dataset representing 399 unique morphology codes (from
2549 different expressions), 272 topography codes (from 3143 different
expressions), and 2043 unique composite expressions (from 11144 different
expressions). This dataset provides a reference standard for named entity
recognition and concept normalisation in French oncology texts.

</details>


### [28] [What Layers When: Learning to Skip Compute in LLMs with Residual Gates](https://arxiv.org/abs/2510.13876)
*Filipe Laitenberger,Dawid Kopiczko,Cees G. M. Snoek,Yuki M. Asano*

Main category: cs.CL

> GateSkip机制通过标记级层跳过操作改进了解码器式的语言模型的性能，并且在保持高准确率的同时，大幅度节省计算资源。

<details>
  <summary>Details</summary>

**Motivation:** 早期退出或基于路由器的深度混合模型因其不稳定性及需要大量的重新训练而为人所知。我们的工作中的平滑可微分门控机制可以稳定地在预训练模型上进行微调。这种方法旨在提高在长格式推理中的计算效率，同时也保持相对较高的准确率，并能够与量化、剪枝及自我投机性解码等方法结合使用。

**Method:** 我们引入了一种名为GateSkip的简单残差流门控机制，该机制可以在解码器式的语言模型中实现按逐个标记跳过层的操作。每个注意力/多层感知器分支都配备了一个sigmoid线性门，该门在分支输出重新进入残差流之前对其进行了压缩。在推理过程中，我们根据门值对标记进行排序，并使用每层预算跳过低重要性的标记。

**Result:** 该方法在长格式推理中能够节省高达15%的计算量，同时仍保留超过90%的基准准确率。在指令调优的模型中，我们在全计算量下看到准确度的提升，并接近50%的计算节省的同时，能够达到基准的精度。

**Conclusion:** 这种学习的门控机制提供了对Transformer模型信息流动的洞见，例如开始标记可以作为锚点，此外，该方法可以轻松地与其他方法结合使用，以进一步提高模型性能。

**Abstract:** We introduce GateSkip, a simple residual-stream gating mechanism that enables
token-wise layer skipping in decoder-only LMs. Each Attention/MLP branch is
equipped with a sigmoid-linear gate that condenses the branch's output before
it re-enters the residual stream. During inference we rank tokens by the gate
values and skip low-importance ones using a per-layer budget. While early-exit
or router-based Mixture-of-Depths models are known to be unstable and need
extensive retraining, our smooth, differentiable gates fine-tune stably on top
of pretrained models. On long-form reasoning, we save up to 15\% compute while
retaining over 90\% of baseline accuracy. On instruction-tuned models we see
accuracy gains at full compute and match baseline quality near 50\% savings.
The learned gates give insight into transformer information flow (e.g., BOS
tokens act as anchors), and the method combines easily with quantization,
pruning, and self-speculative decoding.

</details>


### [29] [TextBandit: Evaluating Probabilistic Reasoning in LLMs Through Language-Only Decision Tasks](https://arxiv.org/abs/2510.13878)
*Jimin Lim,Arjun Damerla,Arthur Jiang,Nam Le*

Main category: cs.CL

> 本研究通过一个新的基准测试，探索大型语言模型（LLMs）在多臂赌博机环境中仅基于自然语言进行决策的能力。尽管大多数LLMs的表现比不上标准算法，但Qwen3-4B取得了优越的表现，突显了语言模型在概率推理上的潜力。

<details>
  <summary>Details</summary>

**Motivation:** 大型语言模型（LLMs）已经显示出越来越能够执行推理任务，但它们仅使用自然语言在不确定性下进行连续决策的能力仍然没有得到充分的探索。本研究旨在探索LLMs能否仅通过自然语言处理在多臂赌博机环境中做出决策。

**Method:** 引入了一个新的基准，在这个基准中，大型语言模型(LLMs)仅通过纯文本反馈（如“你赢得了一个代币”）与多臂赌博机环境互动，没有访问数值提示或显式概率，导致模型仅根据语言线索推断潜在的奖励结构并相应地适应。

**Result:** 评估了四个开源的LLMs的性能，并将它们与标准决策算法（如汤普森抽样、ε-贪婪策略、上置信界算法）和随机选择进行比较。尽管大多数的LLMs的表现劣于基准，Qwen3-4B模型却取得了最佳选择率89.2%，显著超越了更大的LLMs和其他传统方法。

**Conclusion:** 研究结果表明，概率推理可以从语言本身出现，提出的基准测试可以作为评估在自然语言、非数值环境中决策能力的一个步骤。

**Abstract:** Large language models (LLMs) have shown to be increasingly capable of
performing reasoning tasks, but their ability to make sequential decisions
under uncertainty only using natural language remains underexplored. We
introduce a novel benchmark in which LLMs interact with multi-armed bandit
environments using purely textual feedback, "you earned a token", without
access to numerical cues or explicit probabilities, resulting in the model to
infer latent reward structures purely off linguistic cues and to adapt
accordingly. We evaluated the performance of four open-source LLMs and compare
their performance to standard decision-making algorithms such as Thompson
Sampling, Epsilon Greedy, Upper Confidence Bound (UCB), and random choice.
While most of the LLMs underperformed compared to the baselines, Qwen3-4B,
achieved the best-arm selection rate of 89.2% , which significantly
outperformed both the larger LLMs and traditional methods. Our findings suggest
that probabilistic reasoning is able to emerge from language alone, and we
present this benchmark as a step towards evaluating decision-making
capabilities in naturalistic, non-numeric contexts.

</details>


### [30] [Catch Your Breath: Adaptive Computation for Self-Paced Sequence Production](https://arxiv.org/abs/2510.13879)
*Alexandre Galashov,Matt Jones,Rosemary Ke,Yuan Cao,Vaishnavh Nagarajan,Michael C. Mozer*

Main category: cs.CL

> 研究提出了一种动态调整计算资源的语言模型训练方法CYB损失，该方法能够根据输入复杂度自主调整计算步数，实验显示这种方法减少了训练数据需求并提高了预测精度。

<details>
  <summary>Details</summary>

**Motivation:** 该研究的动机在于提高语言模型的效率，允许模型根据输入复杂度自主调整其计算步数，以达到更好的预测性能。

**Method:** 本文提出了一种新的训练目标，称为'Catch Your Breath'(CYB)损失，它允许语言模型动态调整每个输入令牌所需的计算步数。模型可以通过输出<不知道>来申请额外的计算步数。CYB损失包含三种方法：CYB-AP、CYB-VA和CYB-DP，分别解决了任何时刻都需要预测的问题、最大化预测准确性的变分方法以及基于计算预算施加的惩罚。

**Result:** 实验表明，CYB模型只需三分之一的训练数据即可达到基线模型（无暂停）的性能，并且少于使用具有暂停和交叉熵损失模型所用数据的一半。CYB模型请求额外步骤以提高准确性，并根据令牌复杂度和上下文调整处理时间。

**Conclusion:** 研究结果表明，使用CYB损失函数的模型能够有效地根据输入的复杂性和上下文调整其计算需求，减少训练数据的需求并提高预测精度。

**Abstract:** We explore a class of supervised training objectives that allow a language
model to dynamically and autonomously scale the number of compute steps used
for each input token. For any token, the model can request additional compute
steps by emitting a <don't know> output. If the model is granted a delay, a
specialized <pause> token is inserted at the next input step, providing the
model with additional compute resources to generate an output. The model can
request multiple pauses. To train the model to use <don't know> outputs
judiciously and to calibrate its uncertainty, we frame the selection of each
output token as a sequential-decision problem with a time cost. We refer to the
class of methods as $\textit{Catch Your Breath}$ losses and we study three
methods in this class: CYB-AP frames the model's task as anytime prediction,
where an output may be required at any step and accuracy is discounted over
time; CYB-VA is a variational approach that aims to maximize prediction
accuracy subject to a specified distribution over stopping times; and CYB-DP
imposes a penalty based on a computational budget. Through fine-tuning
experiments, we identify the best performing loss variant. The CYB model needs
only one third as much training data as the baseline (no pause) model needs to
achieve the same performance, and half as much data as a model with pauses and
a cross-entropy loss. We find that the CYB model requests additional steps when
doing so improves accuracy, and the model adapts its processing time to
token-level complexity and context. For example, it often pauses after plural
nouns like $\textit{patients}$ and $\textit{challenges}$ but never pauses after
the first token of contracted words like $\textit{wasn}$ and $\textit{didn}$,
and it shows high variability for ambiguous tokens like $\textit{won}$, which
could function as either a verb or part of a contraction.

</details>


### [31] [PAGE: Prompt Augmentation for text Generation Enhancement](https://arxiv.org/abs/2510.13880)
*Mauro Jose Pacchiotti,Luciana Ballejos,Mariel Ale*

Main category: cs.CL

> 研究提出了一种名为PAGE的框架，通过轻量级辅助模块如分类器来增强自然语言生成模型，提高文本生成的质量和可控性。

<details>
  <summary>Details</summary>

**Motivation:** 近年来，自然语言生成模型在文本生成任务中表现出色，但在面对特定任务或特定需求时，可能表现出较差的性能或需要大量额外数据的调整。本研究旨在通过一个简单的模块化架构来辅助这些模型，从而提高生成质量和适应不同任务的能力。

**Method:** 本研究提出了一种名为PAGE（通过提示增强进行文本生成增强的框架），该框架通过简单的辅助模块来辅助文本生成模型。这些辅助模块包括轻量级模型如分类器或提取器，可以从输入文本中提供推断结果。辅助模块的输出用于构建一个增强的输入，以提高生成的质量和可控性。

**Result:** 本研究通过在需求工程领域进行概念验证，展示了一个辅助模块（分类器）如何提升软件需求生成的质量。

**Conclusion:** PAGE框架通过模块化的设计，不需要额外的生成模型，可以更容易地适应不同的任务，从而有效地辅助自然语言生成模型，提高其性能和适用性。

**Abstract:** In recent years, natural language generative models have shown outstanding
performance in text generation tasks. However, when facing specific tasks or
particular requirements, they may exhibit poor performance or require
adjustments that demand large amounts of additional data. This work introduces
PAGE (Prompt Augmentation for text Generation Enhancement), a framework
designed to assist these models through the use of simple auxiliary modules.
These modules, lightweight models such as classifiers or extractors, provide
inferences from the input text. The output of these auxiliaries is then used to
construct an enriched input that improves the quality and controllability of
the generation. Unlike other generation-assistance approaches, PAGE does not
require auxiliary generative models; instead, it proposes a simpler, modular
architecture that is easy to adapt to different tasks. This paper presents the
proposal, its components and architecture, and reports a proof of concept in
the domain of requirements engineering, where an auxiliary module with a
classifier is used to improve the quality of software requirements generation.

</details>


### [32] [Too Open for Opinion? Embracing Open-Endedness in Large Language Models for Social Simulation](https://arxiv.org/abs/2510.13884)
*Bolei Ma,Yong Cao,Indira Sen,Anna-Carolina Haensch,Frauke Kreuter,Barbara Plank,Daniel Hershcovich*

Main category: cs.CL

> 文章呼吁在利用大型语言模型进行社会现象模拟时，放弃封闭格式，转而采用开放性自由文本格式，以充分利用LLMs的生成能力，增强研究的表达性、个性化和实用性。

<details>
  <summary>Details</summary>

**Motivation:** 当前大多数研究将LLMs的社会模拟限制在多选题或简答题等封闭格式中，这忽略了LLMs的生成性本质。本文旨在强调开放性（自由文本格式）在模拟中的重要性。

**Method:** 尚未提及具体方法，但强调了开放性文本格式在使用大语言模型（LLMs）进行社会模拟中的价值和必要性。

**Result:** 讨论了开放性在社会模拟测量与设计改进、未预期观点的探索、降低研究人员偏见等方面的好处，并提出了开放性有助于捕捉表达性和个体性，对预测试有帮助，增强方法学实用性等观点。

**Conclusion:** 呼吁采用能够充分利用LLMs开放生成性多样性的新实践和评估框架，推动自然语言处理（NLP）与社会科学的协同作用。

**Abstract:** Large Language Models (LLMs) are increasingly used to simulate public opinion
and other social phenomena. Most current studies constrain these simulations to
multiple-choice or short-answer formats for ease of scoring and comparison, but
such closed designs overlook the inherently generative nature of LLMs. In this
position paper, we argue that open-endedness, using free-form text that
captures topics, viewpoints, and reasoning processes "in" LLMs, is essential
for realistic social simulation. Drawing on decades of survey-methodology
research and recent advances in NLP, we argue why this open-endedness is
valuable in LLM social simulations, showing how it can improve measurement and
design, support exploration of unanticipated views, and reduce
researcher-imposed directive bias. It also captures expressiveness and
individuality, aids in pretesting, and ultimately enhances methodological
utility. We call for novel practices and evaluation frameworks that leverage
rather than constrain the open-ended generative diversity of LLMs, creating
synergies between NLP and social science.

</details>


### [33] [Order from Chaos: Comparative Study of Ten Leading LLMs on Unstructured Data Categorization](https://arxiv.org/abs/2510.13885)
*Ariel Kamen*

Main category: cs.CL

> 该研究比较了十种先进的大语言模型在非结构化文本分类上的表现，结果显示这些模型在经典标准和特定性能指标上都仅达到了中等水平，且存在幻觉和膨胀问题。GPT 120B在幻觉比率最低，集合方法改进了准确率和减少了幻觉，表明协调多个模型可能是未来改进的关键。

<details>
  <summary>Details</summary>

**Motivation:** 研究动机在于评估当前大语言模型在处理复杂文本分类任务中存在哪些局限性，并探索是否通过集合方法可以改善这些模型的性能。

**Method:** 该研究对比分析了十种最先进的大语言模型（LLMs），在使用交互式广告局（IAB）2.2分层分类法对非结构化文本进行分类时的表现。分析采用一个包含8,660个人工标注样本的统一数据集，并使用完全一致的零样本提示，以确保所有模型在方法上的一致性。评估指标包括四个经典指标——准确率、查准率、查全率和F1分数——以及三个针对LLM的指标：幻觉比率、膨胀比率和分类成本。

**Result:** 研究结果显示，目前的LLMs虽然在快速进步，但只达到了中等程度的经典性能，平均准确率为34%，查准率为42%，查全率为45%，F1得分为41%。幻觉和膨胀比率表明模型比人类标注者生成了更多的分类。评估的系统中，Gemini 1.5/2.0 Flash和GPT 20B/120B在成本与性能平衡方面得分最高，而GPT 120B在幻觉比率方面得分最低。研究发现，仅仅增加规模和改进架构并不能保证更好的分类准确性，因为这项任务要求将丰富的非结构化文本压缩到一个有限的分类体系中，这一过程对当前模型架构构成了挑战。为了克服这些限制，研究开发并测试了一种基于集合的方法，在这种方法中，多个LLMs作为独立专家进行分类，显著提高了准确率，减少了膨胀，并完全消除了幻觉。

**Conclusion:** 这些结果表明，协调多个模型的集合方法——而不是单纯的规模——可能是实现或超越大规模文本分类中人类专家表现最有效的途径。

**Abstract:** This study presents a comparative evaluation of ten state-of-the-art large
language models (LLMs) applied to unstructured text categorization using the
Interactive Advertising Bureau (IAB) 2.2 hierarchical taxonomy. The analysis
employed a uniform dataset of 8,660 human-annotated samples and identical
zero-shot prompts to ensure methodological consistency across all models.
Evaluation metrics included four classic measures - accuracy, precision,
recall, and F1-score - and three LLM-specific indicators: hallucination ratio,
inflation ratio, and categorization cost.
  Results show that, despite their rapid advancement, contemporary LLMs achieve
only moderate classic performance, with average scores of 34% accuracy, 42%
precision, 45% recall, and 41% F1-score. Hallucination and inflation ratios
reveal that models frequently overproduce categories relative to human
annotators. Among the evaluated systems, Gemini 1.5/2.0 Flash and GPT 20B/120B
offered the most favorable cost-to-performance balance, while GPT 120B
demonstrated the lowest hallucination ratio. The findings suggest that scaling
and architectural improvements alone do not ensure better categorization
accuracy, as the task requires compressing rich unstructured text into a
limited taxonomy - a process that challenges current model architectures.
  To address these limitations, a separate ensemble-based approach was
developed and tested. The ensemble method, in which multiple LLMs act as
independent experts, substantially improved accuracy, reduced inflation, and
completely eliminated hallucinations. These results indicate that coordinated
orchestration of models - rather than sheer scale - may represent the most
effective path toward achieving or surpassing human-expert performance in
large-scale text categorization.

</details>


### [34] [Reliable Fine-Grained Evaluation of Natural Language Math Proofs](https://arxiv.org/abs/2510.13888)
*Wenjie Ma,Andrei Cojocaru,Neel Kolhe,Bradley Louie,Robin Said Sharif,Haihan Zhang,Vincent Zhuang,Matei Zaharia,Sewon Min*

Main category: cs.CL

> 研究提出了一个系统化的方法来开发和验证评估器，用于对LLM生成的数学证明进行细致评分。建立了第一个专家标注的数据集ProofBench，并开发了评估器ProofGrader，有效评估生成的数学证明。

<details>
  <summary>Details</summary>

**Motivation:** 鉴于现有的大语言模型（LLMs）在数学推理上的进展主要集中在最终答案容易验证的任务上，生成和验证自然语言的数学证明仍然是一个开放的挑战。为此，缺乏可靠、细致的LLM生成数学证明评估器被视为一个关键缺口。

**Method:** 提出了一种系统化的方法论来开发和验证能对LLM生成的数学证明进行细致评分的评估器，评分采用0-7尺度。

**Result:** ProofGrader评估器通过综合运用强推理后端语言模型、参考解决方案和评分方案中的丰富背景信息，以及一个简单的集成方法，实现了与专家评分相比低至0.926的平均绝对误差（MAE），显著优于简单基准。

**Conclusion:** 在最佳之n选择任务中，ProofGrader展示了其实用价值，证明了其对促进下游证明生成的潜在影响。

**Abstract:** Recent advances in large language models (LLMs) for mathematical reasoning
have largely focused on tasks with easily verifiable final answers; however,
generating and verifying natural language math proofs remains an open
challenge. We identify the absence of a reliable, fine-grained evaluator for
LLM-generated math proofs as a critical gap. To address this, we propose a
systematic methodology for developing and validating evaluators that assign
fine-grained scores on a 0-7 scale to model-generated math proofs. To enable
this study, we introduce ProofBench, the first expert-annotated dataset of
fine-grained proof ratings, spanning 145 problems from six major math
competitions (USAMO, IMO, Putnam, etc) and 435 LLM-generated solutions from
Gemini-2.5-pro, o3, and DeepSeek-R1. %with expert gradings. Using ProofBench as
a testbed, we systematically explore the evaluator design space across key
axes: the backbone model, input context, instructions and evaluation workflow.
Our analysis delivers ProofGrader, an evaluator that combines a strong
reasoning backbone LM, rich context from reference solutions and marking
schemes, and a simple ensembling method; it achieves a low Mean Absolute Error
(MAE) of 0.926 against expert scores, significantly outperforming naive
baselines. Finally, we demonstrate its practical utility in a best-of-$n$
selection task: at $n=16$, ProofGrader achieves an average score of 4.14 (out
of 7), closing 78% of the gap between a naive binary evaluator (2.48) and the
human oracle (4.62), highlighting its potential to advance downstream proof
generation.

</details>


### [35] [A Survey on Collaborating Small and Large Language Models for Performance, Cost-effectiveness, Cloud-edge Privacy, and Trustworthiness](https://arxiv.org/abs/2510.13890)
*Fali Wang,Jihai Chen,Shuhua Yang,Ali Al-Lawati,Linli Tang,Hui Liu,Suhang Wang*

Main category: cs.CL

> Error

<details>
  <summary>Details</summary>

**Motivation:** Error

**Method:** Error

**Result:** Error

**Conclusion:** Error

**Abstract:** Large language models (LLMs) have advanced many domains and applications but
face high fine-tuning costs, inference latency, limited edge deployability, and
reliability concerns. Small language models (SLMs), compact, efficient, and
adaptable, offer complementary remedies. Recent work explores collaborative
frameworks that fuse SLMs' specialization and efficiency with LLMs'
generalization and reasoning to meet diverse objectives across tasks and
deployment scenarios. Motivated by these developments, this paper presents a
systematic survey of SLM-LLM collaboration organized by collaboration
objectives. We propose a taxonomy with four goals: performance enhancement,
cost-effectiveness, cloud-edge privacy, and trustworthiness. Within this
framework, we review representative methods, summarize design paradigms, and
outline open challenges and future directions toward efficient, secure, and
scalable SLM-LLM collaboration.

</details>


### [36] [The Harder The Better: Maintaining Supervised Fine-tuning Generalization with Less but Harder Data](https://arxiv.org/abs/2510.13892)
*Zhaoyang Shang,Sibo Wei,Jianbin Guo,Rui Zhou,Lifeng Dong,Yin Luo*

Main category: cs.CL

> The paper introduces THTB, a framework inspired by cognitive science for selecting and guiding the annotation of instruction data for specialized LLM fine-tuning, which significantly reduces the amount of required data while improving performance and generalization.

<details>
  <summary>Details</summary>

**Motivation:** Address the over-reliance on LLM's internal knowledge, weak interpretability, and limited generalization in the existing data selection methods for specialized fine-tuning by proposing a method that combines quality filtering with hardness scoring.

**Method:** Develop THTB, which prioritizes higher-level cognitive instructions based on intrinsic and extrinsic hardness scores, to select and guide the annotation of instruction data for efficient SFT.

**Result:** Experiments show models trained on a small subset of data using THTB outperform models trained on the full dataset and demonstrate superior generalization compared to LLM-only selection methods.

**Conclusion:** THTB outperforms existing methods for data selection and annotation guidance in specialized domains, reducing the reliance on large datasets while enhancing model performance and generalization.

**Abstract:** Large Language Models (LLMs) excel in general tasks, but adapting them to
specialized domains relies on high-quality supervised fine-tuning (SFT) data.
Although existing methods can identify subsets of high-quality data and reduce
training cost to some extent, their selection process still suffers from
over-reliance on LLMs' internal knowledge, weak interpretability, and limited
generalization. To address these limitations, we propose THTB (The Harder The
Better), a cognitive science-inspired framework for instruction data selection
and annotation guidance. THTB prioritizes higher-level cognitive instructions
by combining quality filtering with intrinsic and extrinsic hardness scoring,
offering interpretable and quantifiable criteria for efficient SFT, both in
data selection and annotation guidance. Experiments show that THTB enables
models trained on only 5% of the data to outperform full-dataset training,
while achieving superior generalization compared with LLM-only selection. In
addition, THTB provides effective annotation guidance in vertical domains,
enabling a model trained on just 2% of the data to surpass models trained on
much larger datasets, demonstrating strong potential for domain adaptation. Our
code, datasets, and models are available on
https://github.com/DYJG-research/THTB.

</details>


### [37] [Guarding the Guardrails: A Taxonomy-Driven Approach to Jailbreak Detection](https://arxiv.org/abs/2510.13893)
*Olga E. Sorokoletova,Francesco Giarrusso,Vincenzo Suriani,Daniele Nardi*

Main category: cs.CL

> 该研究通过红队挑战测试了越狱技术的效果，建立了包含50种策略的分类系统，并分析了多轮次意大利语对抗对话数据集，以研究对抗意图如何逐渐出现并绕过现有安全措施。

<details>
  <summary>Details</summary>

**Motivation:** 现有的防御措施主要集中在单轮次攻击上，其覆盖的语言种类有限，并且依赖于有限的分类系统，这些分类系统要么无法捕捉越狱策略的多样性，要么强调风险类别而非越狱技术。因此，该研究旨在推进对越狱技术有效性的理解。

**Method:** 该研究通过构建一个结构化的红队挑战来评估越狱技术的有效性。研究团队开发了一个包含50种越狱策略的综合分层分类系统，归纳为七大类，分别是伪装、说服、权限升级、认知过载、混淆、目标冲突和数据投毒。

**Result:** 研究结果不仅提供了关于不同攻击类型普遍性和成功率的见解，还展示了特定越狱策略如何利用模型的脆弱性并产生偏离。此外，该研究还为自动检测越狱活动提供了基准，并引入了一个新的意大利语多轮次对抗对话数据集，其中包含1364个数据点和分类系统注释。

**Conclusion:** 这项研究表明，现有的L大型语言模型面临由多种越狱策略组成的复杂威胁。通过提出的分类系统和新创建的数据集，研究界可以更好地识别和防御这些策略。此外，研究表明，基于分类指导的提示对于自动检出越狱攻击具有潜在的价值。

**Abstract:** Jailbreaking techniques pose a significant threat to the safety of Large
Language Models (LLMs). Existing defenses typically focus on single-turn
attacks, lack coverage across languages, and rely on limited taxonomies that
either fail to capture the full diversity of attack strategies or emphasize
risk categories rather than the jailbreaking techniques. To advance the
understanding of the effectiveness of jailbreaking techniques, we conducted a
structured red-teaming challenge. The outcome of our experiments are manifold.
First, we developed a comprehensive hierarchical taxonomy of 50 jailbreak
strategies, consolidating and extending prior classifications into seven broad
families, including impersonation, persuasion, privilege escalation, cognitive
overload, obfuscation, goal conflict, and data poisoning. Second, we analyzed
the data collected from the challenge to examine the prevalence and success
rates of different attack types, providing insights into how specific jailbreak
strategies exploit model vulnerabilities and induce misalignment. Third, we
benchmark a popular LLM for jailbreak detection, evaluating the benefits of
taxonomy-guided prompting for improving automatic detection. Finally, we
compiled a new Italian dataset of 1364 multi-turn adversarial dialogues,
annotated with our taxonomy, enabling the study of interactions where
adversarial intent emerges gradually and succeeds in bypassing traditional
safeguards.

</details>


### [38] [Attribution Quality in AI-Generated Content:Benchmarking Style Embeddings and LLM Judges](https://arxiv.org/abs/2510.13898)
*Misam Abbas*

Main category: cs.CL

> 这项研究旨在通过评估固定风格嵌入和指令调整的LLM评委对生成文本的归属准确性，揭示两者之间的互补优势。研究结果表明，归属判断是个多维问题，需要混合策略来解决。

<details>
  <summary>Details</summary>

**Motivation:** 随着大语言模型生成的文本愈发接近人类写作，将作者身份归属性质推进到了一个更具挑战性的阶段。这项研究旨在通过评估两种互补的归属机制，揭示LLM生成内容在不同文体中的表现差异，评估归属质量。

**Method:** 该研究采用固定风格嵌入和指令调整的LLM评委（GPT-4o）两种互补的归属机制，对Human AI Parallel Corpus这个包含600个平衡实例的数据集进行基准测试，这些实例涵盖了学术、新闻、小说、博客、口语记录和电视/电影剧本六个领域。每个实例包括一个由人类生成的提示，以及一个由人类书写的参考续写和一个由LLM生成的续写，后者可能来自GPT-4o或LLaMA-70B-Instruct。

**Result:** 风格嵌入基准在GPT续写上的总体准确性更高（82% vs. 68%），而LLM评委则在LLaMA续写上略好（85% vs. 81%），但这种差异并不具有统计学意义。关键的是，LLM评委在小说和学术文章方面显著优于风格嵌入，表明其对语义敏感；而嵌入模型在口语和脚本对话方面占优，反映出了结构方面的优势。

**Conclusion:** 该研究指出文体归属是一个多维度的问题，需要采用混合策略。通过提供GitHub上的代码和MIT许可下的Hugging Face上的衍生数据，研究人员为AI生成内容的归属质量评估提供了一种可重现的基准测试框架。

**Abstract:** Attributing authorship in the era of large language models (LLMs) is
increasingly challenging as machine-generated prose rivals human writing. We
benchmark two complementary attribution mechanisms , fixed Style Embeddings and
an instruction-tuned LLM judge (GPT-4o) on the Human AI Parallel Corpus, an
open dataset of 600 balanced instances spanning six domains (academic, news,
fiction, blogs, spoken transcripts, and TV/movie scripts). Each instance
contains a human prompt with both a gold continuation and an LLM-generated
continuation from either GPT-4o or LLaMA-70B-Instruct. The Style Embedding
baseline achieves stronger aggregate accuracy on GPT continuations (82 pct vs.
68 pct). The LLM Judge is slightly better than the Style embeddings on LLaMA
continuations (85 pct vs. 81 pct) but the results are not statistically
significant. Crucially, the LLM judge significantly outperforms in fiction and
academic prose, indicating semantic sensitivity, whereas embeddings dominate in
spoken and scripted dialogue, reflecting structural strengths. These
complementary patterns highlight attribution as a multidimensional problem
requiring hybrid strategies. To support reproducibility we provide code on
GitHub and derived data on Hugging Face under the MIT license. This open
framework provides a reproducible benchmark for attribution quality assessment
in AI-generated content, along with a review of related literature influencing
this work.

</details>


### [39] [Narrow Finetuning Leaves Clearly Readable Traces in Activation Differences](https://arxiv.org/abs/2510.13900)
*Julian Minder,Clément Dumas,Stewart Slocum,Helena Casademunt,Cameron Holmes,Robert West,Neel Nanda*

Main category: cs.CL

> 研究展示了窄域微调在大语言模型中异常属性的创建，并通过分析模型激活偏差来理解细调域，提出改进训练方式，指出细调模型作为广泛研究的代理可能不现实。

<details>
  <summary>Details</summary>

**Motivation:** 研究细调在狭窄领域中的强大偏差，旨在理解模型过拟合的问题，并提出改进训练方式的建议。同时，通过对这些细调模型的分析，警告AI安全和可解释性研究人员细调模型作为研究宽泛细调的替代方案可能不切实际。

**Method:** 通过分析细调后模型激活值的变化，特别是在随机文本前几个token的激活差异，研究团队能够理解细调领域中的偏差。通过简单的模型差异工具，这些偏差能够被找到，并通过将差异添加到模型激活值中来生成类似于细调数据格式和内容的文本。

**Result:** 研究创建了一个基于LLM的可解释性代理，该代理在理解细调领域时表现显著优于仅使用简单提示的基本代理。发现这些偏差反映了过拟合，预训练数据与细调语料库的混合能显著减少这些偏差，但仍可能存在遗留风险。

**Conclusion:** 该工作证明了窄域细调模型的训练目标在其激活中留下了显著的痕迹，提出了改进训练方式的方法，并强调了对窄域细调效应深入研究的必要性。

**Abstract:** Finetuning on narrow domains has become an essential tool to adapt Large
Language Models (LLMs) to specific tasks and to create models with known
unusual properties that are useful for research. We show that narrow finetuning
creates strong biases in LLM activations that can be interpreted to understand
the finetuning domain. These biases can be discovered using simple tools from
model diffing - the study of differences between models before and after
finetuning. In particular, analyzing activation differences on the first few
tokens of random text and steering by adding this difference to the model
activations produces text similar to the format and general content of the
finetuning data. We demonstrate that these analyses contain crucial information
by creating an LLM-based interpretability agent to understand the finetuning
domain. With access to the bias, the agent performs significantly better
compared to baseline agents using simple prompting. Our analysis spans
synthetic document finetuning for false facts, emergent misalignment,
subliminal learning, and taboo word guessing game models across different
architectures (Gemma, LLaMA, Qwen) and scales (1B to 32B parameters). We
suspect these biases reflect overfitting and find that mixing pretraining data
into the finetuning corpus largely removes them, though residual risks may
remain. Our work (1) demonstrates that narrowly finetuned models have salient
traces of their training objective in their activations and suggests ways to
improve how they are trained, (2) warns AI safety and interpretability
researchers that the common practice of using such models as a proxy for
studying broader finetuning (e.g., chat-tuning) might not be realistic, and (3)
highlights the need for deeper investigation into the effects of narrow
finetuning and development of truly realistic case studies for model-diffing,
safety and interpretability research.

</details>


### [40] [RAID: Refusal-Aware and Integrated Decoding for Jailbreaking LLMs](https://arxiv.org/abs/2510.13901)
*Tuan T. Nguyen,John Le,Thai T. Vu,Willy Susilo,Heath Cooper*

Main category: cs.CL

> 文章提出了RAID框架，通过对抗性后缀来研究大型语言模型的脆弱性，并通过实验显示，RAID比近期的白盒和黑盒基线方法具有更高的攻击成功率，同时减少了查询量和计算成本。这突显了嵌入空间正则化对于理解和缓解大型语言模型的监禁漏洞的重要性。

<details>
  <summary>Details</summary>

**Motivation:** 大型语言模型尽管在多种任务上表现优异，但仍然容易受到绕过安全机制的监禁攻击。研究的动机在于系统地探测这些模型的脆弱性。

**Method:** RAID（Refusal-Aware and Integrated Decoding）框架通过创建诱导受限内容同时保持流畅性的对抗性后缀来系统地探测这些弱点击。RAID将离散的标记放松为连续嵌入，并通过一个联合目标函数进行优化，该函数(i)鼓励受限响应；(ii)引入拒绝感知的正则化来引导激活远离嵌入空间中的拒绝方向；(iii)应用一致项以保持语义合理性和非冗余性。优化后，通过平衡嵌入亲和力和语言模型似然性的引导解码过程将嵌入映射回标记。

**Result:** 实验结果表明，在多个开源语言模型上，RAID的攻击成功率更高，查询次数更少，运算成本更低。

**Conclusion:** 研究强调了嵌入空间正则化对于理解并缓解大型语言模型的监禁漏洞的重要性。

**Abstract:** Large language models (LLMs) achieve impressive performance across diverse
tasks yet remain vulnerable to jailbreak attacks that bypass safety mechanisms.
We present RAID (Refusal-Aware and Integrated Decoding), a framework that
systematically probes these weaknesses by crafting adversarial suffixes that
induce restricted content while preserving fluency. RAID relaxes discrete
tokens into continuous embeddings and optimizes them with a joint objective
that (i) encourages restricted responses, (ii) incorporates a refusal-aware
regularizer to steer activations away from refusal directions in embedding
space, and (iii) applies a coherence term to maintain semantic plausibility and
non-redundancy. After optimization, a critic-guided decoding procedure maps
embeddings back to tokens by balancing embedding affinity with language-model
likelihood. This integration yields suffixes that are both effective in
bypassing defenses and natural in form. Experiments on multiple open-source
LLMs show that RAID achieves higher attack success rates with fewer queries and
lower computational cost than recent white-box and black-box baselines. These
findings highlight the importance of embedding-space regularization for
understanding and mitigating LLM jailbreak vulnerabilities.

</details>


### [41] [Investigating Political and Demographic Associations in Large Language Models Through Moral Foundations Theory](https://arxiv.org/abs/2510.13902)
*Nicole Smith-Vaniz,Harper Lyon,Lorraine Steigner,Ben Armstrong,Nicholas Mattei*

Main category: cs.CL

> 论文通过对比LLMs与人类的数据，评估了LLMs在回答道德问题时的固有偏见及能否准确代表不同的意识形态。

<details>
  <summary>Details</summary>

**Motivation:** 鉴于大型语言模型（LLMs）在日常生活中的普及及其在医疗、人际关系及法律等领域的角色，此研究旨在量化和探讨LLMs在回答涉及政治和道德问题时所显露的潜在偏见。

**Method:** 通过应用道德基础理论（MFT）来分析LLMs的道德偏见，该理论将人类道德推理分为五个维度：伤害、公平、群体忠诚、权威和纯洁。论文通过与现有的人类研究直接对比，探讨了LLMs在面对由角色扮演构建的不同意识形态时的回应特点。

**Result:** 通过对LLMs在直接回应及角色扮演等情境下的系统分析，研究为理解基于AI的回应是否存在政治和人口特征依赖提供了一些见解。

**Conclusion:** 研究表明，LLMs的回答在某种程度上体现了特定的政治意识形态倾向，同时也能够通过明确的提示和基于人口特征的角色扮演来表示这些意识形态的视角。

**Abstract:** Large Language Models (LLMs) have become increasingly incorporated into
everyday life for many internet users, taking on significant roles as advice
givers in the domains of medicine, personal relationships, and even legal
matters. The importance of these roles raise questions about how and what
responses LLMs make in difficult political and moral domains, especially
questions about possible biases. To quantify the nature of potential biases in
LLMs, various works have applied Moral Foundations Theory (MFT), a framework
that categorizes human moral reasoning into five dimensions: Harm, Fairness,
Ingroup Loyalty, Authority, and Purity. Previous research has used the MFT to
measure differences in human participants along political, national, and
cultural lines. While there has been some analysis of the responses of LLM with
respect to political stance in role-playing scenarios, no work so far has
directly assessed the moral leanings in the LLM responses, nor have they
connected LLM outputs with robust human data. In this paper we analyze the
distinctions between LLM MFT responses and existing human research directly,
investigating whether commonly available LLM responses demonstrate ideological
leanings: either through their inherent responses, straightforward
representations of political ideologies, or when responding from the
perspectives of constructed human personas. We assess whether LLMs inherently
generate responses that align more closely with one political ideology over
another, and additionally examine how accurately LLMs can represent ideological
perspectives through both explicit prompting and demographic-based
role-playing. By systematically analyzing LLM behavior across these conditions
and experiments, our study provides insight into the extent of political and
demographic dependency in AI-generated responses.

</details>


### [42] [Schema for In-Context Learning](https://arxiv.org/abs/2510.13905)
*Pan Chen,Shaohong Chen,Mark Wang,Shi Xuan Leong,Priscilla Fung,Varinia Bernales,Alan Aspuru-Guzik*

Main category: cs.CL

> 本文提出了SCHEMA ACTIVATED IN CONTEXT LEARNING方法，通过构建抽象化的模式来增强模型的推理能力，显著提升了大语言模型在处理新型问题上的性能。

<details>
  <summary>Details</summary>

**Motivation:** 传统的示例驱动的上下文学习缺乏显式的知识检索和转移模块，而受到认知科学中模式理论的启发，提出了一种新的方法SA-ICL，以提升模型的推理能力和性能。

**Method:** 通过提取先前示例中嵌入的认知过程的构建块来创建抽象化的模式，该模式是一种轻量级、结构化的模板，包含关键推理步骤及其关系，并用于增强模型在面对新问题时的推理过程。

**Result:** 实验结果表明，在GPQA数据集中的化学和物理问题上，SA-ICL能够大幅提高性能，最多提升36.19%，同时减少对多个示例的依赖并提高模型的可解释性。

**Conclusion:** 该研究不仅连接了各种上下文学习策略，例如模式启动和思考链提示，还为提升大语言模型的人类般推理能力铺平了新道路。

**Abstract:** In-Context Learning (ICL) enables transformer-based language models to adapt
to new tasks by conditioning on demonstration examples. However, traditional
example-driven in-context learning lacks explicit modules for knowledge
retrieval and transfer at the abstraction level. Inspired by cognitive science,
specifically schema theory, which holds that humans interpret new information
by activating pre-existing mental frameworks (schemas) to structure
understanding, we introduce SCHEMA ACTIVATED IN CONTEXT LEARNING (SA-ICL). This
framework extracts the representation of the building blocks of cognition for
the reasoning process instilled from prior examples, creating an abstracted
schema, a lightweight, structured template of key inferential steps and their
relationships, which is then used to augment a model's reasoning process when
presented with a novel question. We demonstrate that a broad range of large
language models (LLMs) lack the capacity to form and utilize internal
schema-based learning representations implicitly, but instead benefit
significantly from explicit schema-based scaffolding. Across chemistry and
physics questions from the GPQA dataset, our experiments show that SA-ICL
consistently boosts performance, up to 36.19 percent, when the single
demonstration example is of high quality, which simultaneously reduces reliance
on the number of demonstrations and enhances interpretability. SCHEMA ACTIVATED
IN CONTEXT LEARNING not only bridges disparate ICL strategies ranging from
pattern priming to Chain-of-Thought prompting, but also paves a new path for
enhancing human-like reasoning in LLMs.

</details>


### [43] [LLM Prompt Duel Optimizer: Efficient Label-Free Prompt Optimization](https://arxiv.org/abs/2510.13907)
*Yuanchen Wu,Saurabh Verma,Justin Lee,Fangzhou Xiong,Poppy Zhang,Amel Awadelkarim,Xu Chen,Yubai Yuan,Shawndra Hill*

Main category: cs.CL

> Error

<details>
  <summary>Details</summary>

**Motivation:** Error

**Method:** Error

**Result:** Error

**Conclusion:** Error

**Abstract:** Large language models (LLMs) are highly sensitive to their input prompts,
making prompt design a central challenge. While automatic prompt optimization
(APO) reduces manual engineering, most approaches assume access to ground-truth
references such as labeled validation data. In practice, however, collecting
high-quality labels is costly and slow. We propose the Prompt Duel Optimizer
(PDO), a sample-efficient framework for label-free prompt optimization. PDO
formulates the problem as a dueling-bandit setting, where supervision signal
comes from pairwise preference feedback provided by an LLM judge. The framework
combines Double Thompson Sampling (D-TS), which prioritizes informative prompt
comparisons, with Top-Performer Guided Mutation, which expands the candidate
pool by mutating high-performing prompts. PDO naturally operates in label-free
settings and can also incorporate partial labels to mitigate judge noise.
Experiments on BIG-bench Hard (BBH) and MS MARCO show that PDO consistently
outperforms baseline methods. Ablation studies further demonstrate the
effectiveness of both D-TS and prompt mutation.

</details>


### [44] [Interpreting the Latent Structure of Operator Precedence in Language Models](https://arxiv.org/abs/2510.13908)
*Dharunish Yugeswardeenoo,Harshil Nukala,Cole Blondin,Sean O Brien,Vasu Sharma,Kevin Zhu*

Main category: cs.CL

> 通过研究开源的LLaMA 3.2-3B模型，发现其中存在运算符优先级的线性编码，并提出了一种部分嵌入交换技术来调整运算符优先级。

<details>
  <summary>Details</summary>

**Motivation:** 探索大规模语言模型在执行算术任务时内部结构的作用。

**Method:** Structure

**Result:** {"tldr": "通过研究开源的LLaMA 3.2-3B模型，发现其中存在运算符优先级的线性编码，并提出了一种部分嵌入交换技术来调整运算符优先级。", "motivation": "探索大规模语言模型在执行算术任务时内部结构的作用。", "method": "使用包含三个操作数和两个运算符的算术表达式数据集，通过logit lens、线性分类探测和UMAP几何可视化等可解释性技术对残差流进行研究。", "result": "发现中间计算结果出现在残差流中，特别是在MLP模块之后，并且每个运算符的嵌入在注意力层之后线性编码了优先级。", "conclusion": "部分嵌入交换技术可以用来修改模型中的运算符优先级。"}

**Conclusion:** 部分嵌入交换技术可以用来修改模型中的运算符优先级。

**Abstract:** Large Language Models (LLMs) have demonstrated impressive reasoning
capabilities but continue to struggle with arithmetic tasks. Prior works
largely focus on outputs or prompting strategies, leaving the open question of
the internal structure through which models do arithmetic computation. In this
work, we investigate whether LLMs encode operator precedence in their internal
representations via the open-source instruction-tuned LLaMA 3.2-3B model. We
constructed a dataset of arithmetic expressions with three operands and two
operators, varying the order and placement of parentheses. Using this dataset,
we trace whether intermediate results appear in the residual stream of the
instruction-tuned LLaMA 3.2-3B model. We apply interpretability techniques such
as logit lens, linear classification probes, and UMAP geometric visualization.
Our results show that intermediate computations are present in the residual
stream, particularly after MLP blocks. We also find that the model linearly
encodes precedence in each operator's embeddings post attention layer. We
introduce partial embedding swap, a technique that modifies operator precedence
by exchanging high-impact embedding dimensions between operators.

</details>


### [45] [Knowledge Reasoning Language Model: Unifying Knowledge and Language for Inductive Knowledge Graph Reasoning](https://arxiv.org/abs/2510.13909)
*Xingrui Zhuo,Jiapu Wang,Gongqing Wu,Zhongyuan Wang,Jichen Zhang,Shirui Pan,Xindong Wu*

Main category: cs.CL

> 本文提出了KRLM模型，解决了LLM与KG上下文不完全协调的问题，提升了KGR的可信度。

<details>
  <summary>Details</summary>

**Motivation:** 解决LLM知识被稀疏KG上下文掩盖及生成式幻觉未完全约束的问题。目的在于提升归纳知识图推理中模型推理结果的可信度。

**Method:** Structure

**Result:** {
  "tldr": "本文提出了知识推理语言模型（KRLM），通过设计KRL指令格式和注意力层解决了将大型语言模型（LLM）知识与知识图谱（KG）上下文统一协调的问题，提高了归纳知识图推理（KGR）的可信度。", 
  "motivation": "旨在解决当前LLM-based KGFMs中LLM知识被稀疏KG上下文掩盖及生成式幻觉未完全约束的问题。", 
  "method": "提出KRL指令格式、KRL分词器和KRL注意力层以及结构感知的下一个实体预测器。", 
  "result": "在25个现实世界归纳KGR数据集中，KRLM在零样本推理和微调场景下表现出明显优势。", 
  "conclusion": "KRLM通过统一管理LLM知识和KG上下文，有效地提升了模型在开放领域的知识推理能力。"}
}

**Conclusion:** KRLM模型通过统一管理LLM知识与KG上下文，改进了开放领域知识推理能力。

**Abstract:** Inductive Knowledge Graph Reasoning (KGR) aims to discover facts in
open-domain KGs containing unknown entities and relations, which poses a
challenge for KGR models in comprehending uncertain KG components. Existing
studies have proposed Knowledge Graph Foundation Models (KGFMs) that learn
structural invariances across KGs to handle this uncertainty. Recently, Large
Language Models (LLMs) have demonstrated strong capabilities for open-domain
knowledge reasoning. As a result, the latest research has focused on LLM-based
KGFMs that integrate LLM knowledge with KG context for inductive KGR. However,
the intrinsic knowledge of LLMs may be overshadowed by sparse KG context,
leading to LLM knowledge distortion, which can cause irreversible damage to
model reasoning. Moreover, existing LLM-based KGR methods still struggle to
fully constrain generative hallucinations in LLMs, severely limiting the
credibility of reasoning results. To address these limitations, we propose a
Knowledge Reasoning Language Model (KRLM) that achieves unified coordination
between LLM knowledge and KG context throughout the KGR process. Specifically,
we design a Knowledge Reasoning Language (KRL) instruction format and a KRL
tokenizer to align LLM knowledge with KG representations. Then, we propose a
KRL attention layer that coordinates intrinsic LLM knowledge with additional KG
context through a dynamic knowledge memory mechanism. Finally, a
structure-aware next-entity predictor is proposed, which strictly constrains
the reasoning results within a trustworthy knowledge domain. Extensive
experimental results on 25 real-world inductive KGR datasets demonstrate the
significant superiority of the proposed KRLM\footnote{Our source codes are
available at https://anonymous.4open.science/r/KRLM-EA36 in both zero-shot
reasoning and fine-tuning scenarios.

</details>


### [46] [RAGCap-Bench: Benchmarking Capabilities of LLMs in Agentic Retrieval Augmented Generation Systems](https://arxiv.org/abs/2510.13910)
*Jingru Lin,Chen Zhang,Stephen Y. Liu,Haizhou Li*

Main category: cs.CL

> 提出了RAGCap-Bench来评估和改进代理RAG系统中的中间推理能力，并表明增强这些能力可以提高最终结果。

<details>
  <summary>Details</summary>

**Motivation:** 为了解决现有代理RAG系统在处理复杂多跳查询时遇到的问题，并探索改进中间推理能力的方法。

**Method:** 提出RAGCap-Bench，这是一个面向能力的基准测试，用于对代理RAG工作流中的中间任务进行细粒度评估。

**Result:** "慢思考"模型在RAGCap基准测试中表现更好，最终结果也更优。

**Conclusion:** 实验表明，增强这些中间能力的"慢思考"模型可以达到更好的端到端结果，验证了Benchmark的有效性。

**Abstract:** Retrieval-Augmented Generation (RAG) mitigates key limitations of Large
Language Models (LLMs)-such as factual errors, outdated knowledge, and
hallucinations-by dynamically retrieving external information. Recent work
extends this paradigm through agentic RAG systems, where LLMs act as agents to
iteratively plan, retrieve, and reason over complex queries. However, these
systems still struggle with challenging multi-hop questions, and their
intermediate reasoning capabilities remain underexplored. To address this, we
propose RAGCap-Bench, a capability-oriented benchmark for fine-grained
evaluation of intermediate tasks in agentic RAG workflows. We analyze outputs
from state-of-the-art systems to identify common tasks and the core
capabilities required for their execution, then construct a taxonomy of typical
LLM errors to design targeted evaluation questions. Experiments show that
"slow-thinking" models with stronger RAGCap performance achieve better
end-to-end results, underscoring the benchmark's validity and the importance of
enhancing these intermediate capabilities.

</details>


### [47] [AI Debaters are More Persuasive when Arguing in Alignment with Their Own Beliefs](https://arxiv.org/abs/2510.13912)
*María Victoria Carro,Denise Alejandra Mester,Facundo Nieto,Oscar Agustín Stanchi,Guido Ernesto Bergman,Mario Alejandro Leiva,Eitan Sprejer,Luca Nicolás Forziati Gangi,Francisca Gauna Selasco,Juan Gustavo Corvalán,Gerardo I. Simari,María Vanina Martinez*

Main category: cs.CL

> 研究探索了AI辩论在主观问题上的应用，发现语言模型更倾向于采纳与裁判形象一致的观点，顺序辩论存在偏见，模型在辩护与先验信念一致的观点时更具说服力，但与先验信念对立的辩论被评价为质量更高。

<details>
  <summary>Details</summary>

**Motivation:** 研究的动机在于探索AI辩论作为可扩展监督技术的核心前提，即说谎比反驳谎言更难。现有的辩论实验依赖于具有基础事实的数据集，忽略了说谎还要求相信所主张的内容是错误的这一主观维度。

**Method:** 该研究使用了辩论的方法，针对主观问题进行了实验，并在实验前测量了大型语言模型的先验信念。实验中，辩手可以选择他们偏好的立场，并面对一个与他们先验信念相冲突的裁判形象。研究实施并比较了两种辩论协议，即顺序辩论和同时辩论，以评估潜在的系统偏差。

**Result:** 主要发现包括：模型倾向于辩护与裁判形象一致的立场，顺序辩论引入了显著的偏差，有利于第二个辩手；当辩护与先验信念一致的立场时，模型更具说服力；悖论的是，与先验信念不一致的辩论在成对比较中被评为质量更高。

**Conclusion:** 该研究揭示了在人类与AI互动中关于语言模型说服力动力学的重要方面，可以指导人类裁判提供更高质量的训练信号，同时有助于创建更符合预期的AI系统。

**Abstract:** The core premise of AI debate as a scalable oversight technique is that it is
harder to lie convincingly than to refute a lie, enabling the judge to identify
the correct position. Yet, existing debate experiments have relied on datasets
with ground truth, where lying is reduced to defending an incorrect
proposition. This overlooks a subjective dimension: lying also requires the
belief that the claim defended is false. In this work, we apply debate to
subjective questions and explicitly measure large language models' prior
beliefs before experiments. Debaters were asked to select their preferred
position, then presented with a judge persona deliberately designed to conflict
with their identified priors. This setup tested whether models would adopt
sycophantic strategies, aligning with the judge's presumed perspective to
maximize persuasiveness, or remain faithful to their prior beliefs. We
implemented and compared two debate protocols, sequential and simultaneous, to
evaluate potential systematic biases. Finally, we assessed whether models were
more persuasive and produced higher-quality arguments when defending positions
consistent with their prior beliefs versus when arguing against them. Our main
findings show that models tend to prefer defending stances aligned with the
judge persona rather than their prior beliefs, sequential debate introduces
significant bias favoring the second debater, models are more persuasive when
defending positions aligned with their prior beliefs, and paradoxically,
arguments misaligned with prior beliefs are rated as higher quality in pairwise
comparison. These results can inform human judges to provide higher-quality
training signals and contribute to more aligned AI systems, while revealing
important aspects of human-AI interaction regarding persuasion dynamics in
language models.

</details>


### [48] [Synthesizing Agentic Data for Web Agents with Progressive Difficulty Enhancement Mechanisms](https://arxiv.org/abs/2510.13913)
*Shrey Pandit,Xuan-Phi Nguyen,Yifei Ming,Austin Xu,Jiayu Wang,Caiming Xiong,Shafiq Joty*

Main category: cs.CL

> A new data synthesis pipeline for web-based deep research agents increases the quality and difficulty control of generated datasets, leading to more effective and less repetitive tool-use in trained models.

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind this paper is to address the challenge of long-horizon reasoning and exploration in complex web-based tasks. Existing methods often lack control over data quality and difficulty, and complicate it further by combining data and training effects. The paper aims to create more effective, diverse datasets and ultimately train models that perform better and avoid repetitive actions.

**Method:** Web-based 'deep research' agents are created to solve complex question-answering tasks through long-horizon interactions with online tools. The paper proposes a two-pronged data synthesis pipeline that generates question-answer pairs by increasing task complexity until a baseline web agent fails, ensuring fine-grained control over difficulty and quality. The baseline agent is used to attempt questions, validate factuality, check for alternative answers, and enforce filtering.

**Result:** Experiments demonstrate that the proposed smaller dataset enables the training of more effective web agents than existing datasets, displaying a doubled diversity in tool-use actions, which allows for stronger performance and avoidance of repetitive tool-calling behaviors.

**Conclusion:** The data synthesis pipeline effectively generates more challenging and high-quality datasets that optimize the performance of long-horizon reasoning in web-based agents, resulting in more diverse and efficient models compared to traditional datasets.

**Abstract:** Web-based 'deep research' agents aim to solve complex question - answering
tasks through long-horizon interactions with online tools. These tasks remain
challenging, as the underlying language models are often not optimized for
long-horizon reasoning and exploration. Prior work has proposed workflows for
constructing instruction-tuning datasets, often leveraging knowledge graphs.
However, such methods typically lack fine-grained control over difficulty and
quality, yielding synthetic data that falls short of capturing the complexity
required for long-horizon reasoning. Furthermore, many studies conflate data
and training effects by comparing models trained under different optimization
recipes, making it difficult to isolate and evaluate the effectiveness of the
data itself. We introduce a two-pronged data synthesis pipeline that generates
question - answer pairs by progressively increasing task complexity until a
frontier baseline web agent fails. The baseline agent plays multiple roles in
this process: attempting the questions, validating factuality, checking for
alternative answers, and enforcing filtering. To evaluate the effectiveness of
our synthesis methods, we adopt a controlled training setup based on
distillation from strong web agents. Experiments across multiple web-based
benchmarks show that our dataset - despite being smaller - enables the training
of more effective web agents than existing datasets. In particular, our data
exhibits twice the diversity in tool-use actions, allowing models trained on it
to achieve stronger performance while avoiding repetitive tool-calling
behaviors.

</details>


### [49] [Readability $\ne$ Learnability: Rethinking the Role of Simplicity in Training Small Language Models](https://arxiv.org/abs/2510.13915)
*Ivan Lee,Taylor Berg-Kirkpatrick*

Main category: cs.CL

> 研究表明，与可读性相比，统计简洁性（如n-gram多样性）是预测小语言模型学习能力的更强指标。

<details>
  <summary>Details</summary>

**Motivation:** 挑战当前将小语言模型的连贯性归因于可读性的研究结论，并探索什么属性实际上支持小模型能力的出现。

**Method:** 通过构建具有匹配结构但可读性不同的合成数据集来测试可读性是否是预测小语言模型（SLMs）连贯性和学习效率的关键因素。

**Result:** 研究发现，可读性本身并不能很好地预测小语言模型的连贯性或学习效率。在复杂、成人级文本上训练的模型与在简化语言上训练的模型表现相当，甚至在训练过程中连贯性发展的速度更快。

**Conclusion:** 研究结果警告不要将语言模型训练人类化，即在没有经验依据的情况下将其与人类认知发展进行类比，并强调了精确推理哪些属性真正支持小模型能力出现的重要性。

**Abstract:** Recent studies suggest that very small language models (SLMs) can generate
surprisingly coherent text when trained on simplified, child-directed corpora
such as TinyStories. These findings have been interpreted as evidence that
readability -- characterized by accessible vocabulary, familiar narrative
structure, and simple syntax -- plays a key role in enabling such capabilities
to emerge. In this paper, we challenge that interpretation. We construct
synthetic datasets with matched structure but varied readability, and find that
readability alone does not predict coherence or learning efficiency in SLMs.
Models trained on complex, adult-level text perform comparably to those trained
on simplified language, and even exhibit faster development of coherence during
training. Instead, we show that statistical simplicity, as measured by n-gram
diversity, is a stronger predictor of learnability. Our findings caution
against the growing trend of anthropomorphizing language model training --
drawing parallels to human cognitive development without empirical basis -- and
argue for more precise reasoning about what properties actually support
capability emergence in small models.

</details>


### [50] [Element2Vec: Build Chemical Element Representation from Text for Property Prediction](https://arxiv.org/abs/2510.13916)
*Yuanhao Li,Keyuan Lai,Tianqi Wang,Qihao Liu,Jiawei Ma,Yuan-Chao Hu*

Main category: cs.CL

> 本研究提出Element2Vecto方法，通过语言模型将化学元素从自然语言中有效表示出来，支持自然科学领域的研究。方法生成单个通用向量和属性突出的向量集，并通过自注意力机制在测试时训练以减少预测误差。

<details>
  <summary>Details</summary>

**Motivation:** 针对传统方法在预测化学元素性质时的局限性，如难以表示所有特性并存在复杂关系的问题，本研究旨在利用先进的AI工具，特别是语言模型，提高预测的准确性和解释性。

**Method:** Structure

**Result:** {
  "tldr": "本研究提出Element2Vecto方法，通过语言模型将化学元素从自然语言中有效表示出来，支持自然科学领域的研究。方法生成单个通用向量和属性突出的向量集，并通过自注意力机制在测试时训练以减少预测误差。", 
  "motivation": "针对传统方法在预测化学元素性质时的局限性，如难以表示所有特性并存在复杂关系的问题，本研究旨在利用先进的AI工具，特别是语言模型，提高预测的准确性和解释性。", 
  "method": "Element2Vecto方法利用从Wikipedia提取的文本，通过语言模型生成化学元素的通用嵌入和属性高亮向量，并设计了基于自注意力的测试时训练方法以减少预测误差。", 
  "result": "虽然面临文本分布差异和数据稀缺的挑战，本研究的方法在处理元素性质预测上取得了有效进展。", 
  "conclusion": "Element2Vecto方法展现出在基于AI的材料科学发现领域的潜力和作用，为该方向的研究提供了新思路。"}
}

**Conclusion:** Element2Vecto方法展现出在基于AI的材料科学发现领域的潜力和作用，为该方向的研究提供了新思路。

**Abstract:** Accurate property data for chemical elements is crucial for materials design
and manufacturing, but many of them are difficult to measure directly due to
equipment constraints. While traditional methods use the properties of other
elements or related properties for prediction via numerical analyses, they
often fail to model complex relationships. After all, not all characteristics
can be represented as scalars. Recent efforts have been made to explore
advanced AI tools such as language models for property estimation, but they
still suffer from hallucinations and a lack of interpretability. In this paper,
we investigate Element2Vecto effectively represent chemical elements from
natural languages to support research in the natural sciences. Given the text
parsed from Wikipedia pages, we use language models to generate both a single
general-purpose embedding (Global) and a set of attribute-highlighted vectors
(Local). Despite the complicated relationship across elements, the
computational challenges also exist because of 1) the discrepancy in text
distribution between common descriptions and specialized scientific texts, and
2) the extremely limited data, i.e., with only 118 known elements, data for
specific properties is often highly sparse and incomplete. Thus, we also design
a test-time training method based on self-attention to mitigate the prediction
error caused by Vanilla regression clearly. We hope this work could pave the
way for advancing AI-driven discovery in materials science.

</details>


### [51] [Optimal Aggregation of LLM and PRM Signals for Efficient Test-Time Scaling](https://arxiv.org/abs/2510.13918)
*Peng Kuang,Yanli Wang,Xiaoyu Han,Yaowenqi Liu,Kaidi Xu,Haohan Wang*

Main category: cs.CL

> 本文基于理论框架设计了一种优化的权重聚合策略，有效整合了过程奖励模型和大型语言模型的信号，极大提升了TTS的效率。

<details>
  <summary>Details</summary>

**Motivation:** 面对PRM信号在某些基准测试中被简单的多数投票法超越的问题，该研究旨在探索更有效地利用PRM信号的方法，以提升测试时间扩展(TTS)的效果。

**Method:** 本文提出了一个理论框架来优化大型语言模型(LLM)和过程奖励模型(PRM)之间的信号结合，这个框架展示了最优策略是基于权重的响应聚合。通过估计捕捉模型间复杂交互的权重，来提升聚合策略的有效性。

**Result:** 实验结果显示，通过高效的预计算方法校准权重函数，可以在较少的计算资源下，显著提升TTS效率，优于常规加权多数投票法。

**Conclusion:** 研究表明，相较于仅仅增加测试时间的计算资源，投资于更智能地整合LLM和PRM信号的方法，将获得更好的性能提升。

**Abstract:** Process reward models (PRMs) are a cornerstone of test-time scaling (TTS),
designed to verify and select the best responses from large language models
(LLMs). However, this promise is challenged by recent benchmarks where simple
majority voting, which ignores PRM signals, occasionally outperforms standard
PRM-based selection. This raises a critical question: How can we effectively
utilize verification signals from PRMs for TTS? To address this, we start by
developing a theoretical framework for optimally combining signals from both
the LLM and the PRM. Our framework reveals that the optimal strategy is a
weighted aggregation of responses, a strategy whose effectiveness hinges on
estimating weights that capture the complex interplay between the models. Based
on our theoretical results, we empirically show that these optimal weighting
functions differ significantly across LLM-PRM pairs and, notably, often assign
substantial negative weights. Motivated by these insights, we propose efficient
pre-computation methods to calibrate these weighting functions. Extensive
experiments across 5 LLMs and 7 PRMs demonstrate that our calibration method
significantly boosts the TTS efficiency, surpassing the performance of vanilla
weighted majority voting while using only $21.3\%$ of the computation.
Ultimately, our work demonstrates that investing in a more intelligent
aggregation strategy can be a more convincing path to performance gains than
simply scaling test-time computation.

</details>


### [52] [FACTS: Table Summarization via Offline Template Generation with Agentic Workflows](https://arxiv.org/abs/2510.13920)
*Ye Yuan,Mohammad Amin Shabani,Siqi Liu*

Main category: cs.CL

> FACTS, a method for query-focused table summarization, uses offline template generation to offer faster, more accurate, and privacy-compliant summarization compared to existing methods.

<details>
  <summary>Details</summary>

**Motivation:** to overcome limitations of existing approaches such as costly fine-tuning, token-limit and efficiency issues, and lack of robustness and scalability

**Method:** agentic workflow, FACTS, which generates offline templates consisting of SQL queries and Jinja2 templates for natural language summarization

**Result:** FACTS outperforms baseline methods on widely-used benchmarks, indicating its practical value for query-focused table summarization

**Conclusion:** FACTS is a reliable and scalable solution that provides fast and privacy-compliant table summarization, enhancing access to insights beyond simple fact retrieval

**Abstract:** Query-focused table summarization requires generating natural language
summaries of tabular data conditioned on a user query, enabling users to access
insights beyond fact retrieval. Existing approaches face key limitations:
table-to-text models require costly fine-tuning and struggle with complex
reasoning, prompt-based LLM methods suffer from token-limit and efficiency
issues while exposing sensitive data, and prior agentic pipelines often rely on
decomposition, planning, or manual templates that lack robustness and
scalability. To mitigate these issues, we introduce an agentic workflow, FACTS,
a Fast, Accurate, and Privacy-Compliant Table Summarization approach via
Offline Template Generation. FACTS produces offline templates, consisting of
SQL queries and Jinja2 templates, which can be rendered into natural language
summaries and are reusable across multiple tables sharing the same schema. It
enables fast summarization through reusable offline templates, accurate outputs
with executable SQL queries, and privacy compliance by sending only table
schemas to LLMs. Evaluations on widely-used benchmarks show that FACTS
consistently outperforms baseline methods, establishing it as a practical
solution for real-world query-focused table summarization.

</details>


### [53] [An LLM-Powered AI Agent Framework for Holistic IoT Traffic Interpretation](https://arxiv.org/abs/2510.13925)
*Daniel Adu Worae,Spyridon Mastorakis*

Main category: cs.CL

> 本文提出了一种基于LLM的AI代理框架，通过整合多种技术，能够将原始数据包转换为结构化表示并进行交互式分析，实现对物联网网络流量数据的高效解读，实验表明该方法在多个评估指标上表现出色。

<details>
  <summary>Details</summary>

**Motivation:** 物联网（IoT）网络生成多样且数量巨大的流量数据，这些数据既反映了正常活动也反映了潜在威胁。对于这类遥测数据的有意义的见解需要跨层解释行为、协议和上下文，而不仅仅是孤立的检测。

**Method:** 本文提出了一种基于LLM（大型语言模型）的AI代理框架，该框架能够将原始数据包捕获转换为结构化且语义丰富的表示形式，以进行交互式分析。该框架整合了特征提取、基于转换器的异常检测、数据包和流总结、威胁情报丰富化以及检索增强型问题回答等功能。一个由大型语言模型引导的AI代理可以在索引的流量数据上进行推理，组装证据以产生精确且易于人类理解的解释。

**Result:** 实验结果表明，结合词汇和语义搜索以及重新排序的混合检索方法，相对于仅使用密集检索，显著提高了BLEU、ROUGE、METEOR和BERTScore等评估指标的结果。系统分析表明该框架在CPU、GPU和内存占用方面具有低开销。

**Conclusion:** 实验评估表明，结合词汇和语义搜索以及重新排序的混合检索方法在多个物联网捕获数据和六个开放模型上显著提高了BLEU、ROUGE、METEOR和BERTScore等指标的结果。系统分析进一步表明，该框架在CPU、GPU和内存占用方面具有低开销，表明该框架能够实现对物联网网络流量的全面而高效的解释。

**Abstract:** Internet of Things (IoT) networks generate diverse and high-volume traffic
that reflects both normal activity and potential threats. Deriving meaningful
insight from such telemetry requires cross-layer interpretation of behaviors,
protocols, and context rather than isolated detection. This work presents an
LLM-powered AI agent framework that converts raw packet captures into
structured and semantically enriched representations for interactive analysis.
The framework integrates feature extraction, transformer-based anomaly
detection, packet and flow summarization, threat intelligence enrichment, and
retrieval-augmented question answering. An AI agent guided by a large language
model performs reasoning over the indexed traffic artifacts, assembling
evidence to produce accurate and human-readable interpretations. Experimental
evaluation on multiple IoT captures and six open models shows that hybrid
retrieval, which combines lexical and semantic search with reranking,
substantially improves BLEU, ROUGE, METEOR, and BERTScore results compared with
dense-only retrieval. System profiling further indicates low CPU, GPU, and
memory overhead, demonstrating that the framework achieves holistic and
efficient interpretation of IoT network traffic.

</details>


### [54] [BioMedSearch: A Multi-Source Biomedical Retrieval Framework Based on LLMs](https://arxiv.org/abs/2510.13926)
*Congying Liu,Xingyuan Wei,Peipei Liu,Yiqing Shen,Yanxu Mao,Tiehan Cui*

Main category: cs.CL

> BioMedSearch提出了一种多来源生物医学信息检索框架，通过集成多个数据源来提高生物医学查询质量，实验结果显示其在复杂生物医学问答任务上显著优于现有方法。

<details>
  <summary>Details</summary>

**Motivation:** 由于大规模语言模型生成的生物医学内容缺乏科学严谨性，无法有效访问权威生物医学数据库，并且经常编造蛋白质功能、相互作用和结构细节，因此提出了BioMedSearch来解决这些问题并提高生物医学查询的准确性。

**Method:** BioMedSearch采用多来源的生物医学信息检索框架，基于大规模语言模型，并结合文献检索、蛋白质数据库及网络搜索来支持复杂生物医学查询的准确和高效处理。该方法通过子查询分解、关键词提取、任务图构建以及多来源信息过滤来生成高质量的问答结果。

**Result:** 实验结果显示，BioMedSearch在所有级别上都比基线模型提高了准确性。具体而言，Level 1准确率从59.1%提高到91.9%，Level 2从47.0%提高到81.0%，Level 3从36.3%提高到73.4%。

**Conclusion:** BioMedSearch通过多来源的生物医学信息检索框架显著提高了对复杂生物医学查询的质量和准确性。其性能在多层生物医学问答数据集BioMedMCQs上得到了验证。

**Abstract:** Biomedical queries often rely on a deep understanding of specialized
knowledge such as gene regulatory mechanisms and pathological processes of
diseases. They require detailed analysis of complex physiological processes and
effective integration of information from multiple data sources to support
accurate retrieval and reasoning. Although large language models (LLMs) perform
well in general reasoning tasks, their generated biomedical content often lacks
scientific rigor due to the inability to access authoritative biomedical
databases and frequently fabricates protein functions, interactions, and
structural details that deviate from authentic information. Therefore, we
present BioMedSearch, a multi-source biomedical information retrieval framework
based on LLMs. The method integrates literature retrieval, protein database and
web search access to support accurate and efficient handling of complex
biomedical queries. Through sub-queries decomposition, keywords extraction,
task graph construction, and multi-source information filtering, BioMedSearch
generates high-quality question-answering results. To evaluate the accuracy of
question answering, we constructed a multi-level dataset, BioMedMCQs,
consisting of 3,000 questions. The dataset covers three levels of reasoning:
mechanistic identification, non-adjacent semantic integration, and temporal
causal reasoning, and is used to assess the performance of BioMedSearch and
other methods on complex QA tasks. Experimental results demonstrate that
BioMedSearch consistently improves accuracy over all baseline models across all
levels. Specifically, at Level 1, the average accuracy increases from 59.1% to
91.9%; at Level 2, it rises from 47.0% to 81.0%; and at the most challenging
Level 3, the average accuracy improves from 36.3% to 73.4%. The code and
BioMedMCQs are available at: https://github.com/CyL-ucas/BioMed_Search

</details>


### [55] [LLMs Can Get "Brain Rot"!](https://arxiv.org/abs/2510.13928)
*Shuo Xing,Junyuan Hong,Yifan Wang,Runjin Chen,Zhenyu Zhang,Ananth Grama,Zhengzhong Tu,Zhangyang Wang*

Main category: cs.CL

> 研究证实，数据质量显著影响LLM的性能，垃圾数据会导致认知能力下降，包括推理和安全性能的下降。这一发现强调了持续数据质量改善和定期检查的重要性。

<details>
  <summary>Details</summary>

**Motivation:** 提出并测试了LLM脑衰退假设：持续暴露于网络垃圾文本会导致大型语言模型的长期认知能力下降。此研究旨在探讨数据质量对LLM性能的影响。

**Method:** 本研究通过在真实的Twitter/X语料库上构建垃圾数据集和反向控制数据集（通过M1[参与度]和M2[语义质量]两种正交操作化手段），进行了受控实验，以隔离数据质量的影响。

**Result:** 实验结果显示，与对照组相比，4个LLM模型在垃圾数据集上的连续预训练导致了在推理、长上下文理解、安全性和“暗特征”（如精神病、自恋）方面的非轻微下降。随着垃圾数据比例从0%上升到100%，在M1条件下，ARC-Challenge和RULER-CWE的性能分别从74.9下降到57.2、从84.4下降到52.3。

**Conclusion:** 研究结果提供了有力的、多视角的证据，表明数据质量是LLM能力衰减的因果驱动因素，这重新框定了持续预训练的数据筛选作为一种训练时的安全问题，并激发了对部署LLM进行常规“认知健康检查”的动机。

**Abstract:** We propose and test the LLM Brain Rot Hypothesis: continual exposure to junk
web text induces lasting cognitive decline in large language models (LLMs). To
causally isolate data quality, we run controlled experiments on real Twitter/X
corpora, constructing junk and reversely controlled datasets via two orthogonal
operationalizations: M1 (engagement degree) and M2 (semantic quality), with
matched token scale and training operations across conditions. Contrary to the
control group, continual pre-training of 4 LLMs on the junk dataset causes
non-trivial declines (Hedges' $g>0.3$) on reasoning, long-context
understanding, safety, and inflating "dark traits" (e.g., psychopathy,
narcissism). The gradual mixtures of junk and control datasets also yield
dose-response cognition decay: for example, under M1, ARC-Challenge with Chain
Of Thoughts drops $74.9 \rightarrow 57.2$ and RULER-CWE $84.4 \rightarrow 52.3$
as junk ratio rises from $0\%$ to $100\%$.
  Error forensics reveal several key insights. First, we identify
thought-skipping as the primary lesion: models increasingly truncate or skip
reasoning chains, explaining most of the error growth. Second, partial but
incomplete healing is observed: scaling instruction tuning and clean data
pre-training improve the declined cognition yet cannot restore baseline
capability, suggesting persistent representational drift rather than format
mismatch. Finally, we discover that the popularity, a non-semantic metric, of a
tweet is a better indicator of the Brain Rot effect than the length in M1.
Together, the results provide significant, multi-perspective evidence that data
quality is a causal driver of LLM capability decay, reframing curation for
continual pretraining as a \textit{training-time safety} problem and motivating
routine "cognitive health checks" for deployed LLMs.

</details>


### [56] [Robust or Suggestible? Exploring Non-Clinical Induction in LLM Drug-Safety Decisions](https://arxiv.org/abs/2510.13931)
*Siying Liu,Shisheng Zhang,Indu Bala*

Main category: cs.CL

> 该研究通过使用FAERS数据和基于人物框架的评估方法，发现大型语言模型在药物安全预测中存在系统性偏差，尤其影响弱势群体，并提出需要制定公平评估策略和缓解措施来应对这一问题。

<details>
  <summary>Details</summary>

**Motivation:** 该项研究的动机是探索大型语言模型在药物安全预测中的可靠性，特别关注这些模型是否会将临床无关的社会人口统计信息纳入不良事件预测中。

**Method:** 该研究使用了来自美国食品药品监督管理局不良事件报告系统（FAERS）的结构化数据以及基于人物框架的评估方法来评估两个先进的模型：ChatGPT-4o 和 Bio-Medical-Llama-3.8B，在不同的角色（全科医生、专科医生、患者）下进行评估，以模拟现实中根据用户类型区分访问权限的部署场景。

**Result:** 研究发现，在不良事件（AE）预测的准确性方面存在系统性差异。处于不利地位的群体（如低教育水平、住房不稳定）经常被分配更高的不良事件预测可能性，而较有利群体（如研究生学历、私人保险）则不然。此外，还识别了两种不同的偏差类型：明确偏差，即错误预测直接在推理过程中参考了人物属性；隐含偏差，即预测结果不一致，但未明确提及人物属性。

**Conclusion:** 研究结果揭示了使用大型语言模型进行药物警戒的严重风险，并强调需要制定公平性的评估协议和缓解策略，避免在临床部署中的潜在偏见。

**Abstract:** Large language models (LLMs) are increasingly applied in biomedical domains,
yet their reliability in drug-safety prediction remains underexplored. In this
work, we investigate whether LLMs incorporate socio-demographic information
into adverse event (AE) predictions, despite such attributes being clinically
irrelevant. Using structured data from the United States Food and Drug
Administration Adverse Event Reporting System (FAERS) and a persona-based
evaluation framework, we assess two state-of-the-art models, ChatGPT-4o and
Bio-Medical-Llama-3.8B, across diverse personas defined by education, marital
status, employment, insurance, language, housing stability, and religion. We
further evaluate performance across three user roles (general practitioner,
specialist, patient) to reflect real-world deployment scenarios where
commercial systems often differentiate access by user type. Our results reveal
systematic disparities in AE prediction accuracy. Disadvantaged groups (e.g.,
low education, unstable housing) were frequently assigned higher predicted AE
likelihoods than more privileged groups (e.g., postgraduate-educated, privately
insured). Beyond outcome disparities, we identify two distinct modes of bias:
explicit bias, where incorrect predictions directly reference persona
attributes in reasoning traces, and implicit bias, where predictions are
inconsistent, yet personas are not explicitly mentioned. These findings expose
critical risks in applying LLMs to pharmacovigilance and highlight the urgent
need for fairness-aware evaluation protocols and mitigation strategies before
clinical deployment.

</details>


### [57] [Big Reasoning with Small Models: Instruction Retrieval at Inference Time](https://arxiv.org/abs/2510.13935)
*Kenan Alkiek,David Jurgens,Vinod Vydiswaran*

Main category: cs.CL

> This paper presents a technique to enhance small language models (SLMs) with instruction retrieval, improving their performance in multi-step reasoning tasks across medical, legal, and mathematical domains without needing additional fine-tuning.

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to address the limitations of small language models in performing tasks that require multi-step reasoning or domain-specific knowledge by enhancing their capability through instruction intervention at inference time.

**Method:** Our method involves building an Instruction Corpus by grouping similar training questions and creating instructions via GPT-5. During inference, the Small Language Model (SLM) retrieves the most relevant instructions and follows their steps.

**Result:** The results show that SLMs with instruction retrieval exhibit significant performance improvements compared to those without retrieval, indicating the method's efficacy in boosting reasoning capabilities in domain-specific tasks.

**Conclusion:** The conclusion indicates that the instruction retrieval method consistently improves the performance of SLMs across various domains, with gains of 9.4% on MedQA, 7.9% on MMLU Law, and 5.1% on MathQA. It also highlights that the effectiveness of the method varies with the model family and the model's intrinsic reasoning ability.

**Abstract:** Can we bring large-scale reasoning to local-scale compute? Small language
models (SLMs) are increasingly attractive because they run efficiently on local
hardware, offering strong privacy, low cost, and reduced environmental impact.
Yet they often struggle with tasks that require multi-step reasoning or
domain-specific knowledge. We address this limitation through instruction
intervention at inference time, where an SLM retrieves structured reasoning
procedures rather than generating them from scratch. Our method builds an
Instruction Corpus by grouping similar training questions and creating
instructions via GPT-5. During inference, the SLM retrieves the most relevant
instructions and follows their steps. Unlike retrieval-augmented generation,
which retrieves text passages, instruction retrieval gives the model structured
guidance for reasoning. We evaluate this framework on MedQA (medical board
exams), MMLU Professional Law, and MathQA using models from 3B to 14B
parameters without any additional fine-tuning. Instruction retrieval yields
consistent gains: 9.4% on MedQA, 7.9% on MMLU Law, and 5.1% on MathQA. Concise
instructions outperform longer ones, and the magnitude of improvement depends
strongly on model family and intrinsic reasoning ability.

</details>


### [58] [FinDeepResearch: Evaluating Deep Research Agents in Rigorous Financial Analysis](https://arxiv.org/abs/2510.13936)
*Fengbin Zhu,Xiang Yao Ng,Ziyang Liu,Chang Liu,Xianwei Zeng,Chao Wang,Tianhui Tan,Xuan Yao,Pengyang Shao,Min Xu,Zixuan Wang,Jing Wang,Xin Lin,Junfeng Li,Jingxian Zhu,Yang Zhang,Wenjie Wang,Fuli Feng,Richang Hong,Huanbo Luan,Ke-Wei Huang,Tat-Seng Chua*

Main category: cs.CL

> 研究通过HisRubric框架和FinDeepResearch基准评估了16种方法在财务分析中的表现，包括6个DR代理、5个具有深度推理和搜索能力的LLM，以及5个仅具有深度推理能力的LLM。

<details>
  <summary>Details</summary>

**Motivation:** 当前文献在系统评估DR代理进行关键性研究分析能力方面做得不够。为填补这一空白，研究提出了HisRubric框架来评估其在公司财务分析中的能力。

**Method:** 提出HisRubric框架来评估由高级语言模型驱动的DR代理在公司财务分析方面的能力，该框架模拟了专业分析师的工作流程，包括数据识别、指标计算以及战略总结与解读。此外，基于该框架构建了一个名为FinDeepResearch的数据集，包含来自8个金融市场、4种语言的64家上市公司的财务数据，共计15,808项评估内容。

**Result:** 实验结果显示了不同方法在多样财务市场和语言下的优劣势，并提供了未来研究和开发的重要启示。

**Conclusion:** 该研究为评估DR代理和LLM在复杂财务分析任务中的表现提供了基准和实验依据，未来将公开发布此基准数据集和评估代码。

**Abstract:** Deep Research (DR) agents, powered by advanced Large Language Models (LLMs),
have recently garnered increasing attention for their capability in conducting
complex research tasks. However, existing literature lacks a rigorous and
systematic evaluation of DR Agent's capabilities in critical research analysis.
To address this gap, we first propose HisRubric, a novel evaluation framework
with a hierarchical analytical structure and a fine-grained grading rubric for
rigorously assessing DR agents' capabilities in corporate financial analysis.
This framework mirrors the professional analyst's workflow, progressing from
data recognition to metric calculation, and finally to strategic summarization
and interpretation. Built on this framework, we construct a FinDeepResearch
benchmark that comprises 64 listed companies from 8 financial markets across 4
languages, encompassing a total of 15,808 grading items. We further conduct
extensive experiments on the FinDeepResearch using 16 representative methods,
including 6 DR agents, 5 LLMs equipped with both deep reasoning and search
capabilities, and 5 LLMs with deep reasoning capabilities only. The results
reveal the strengths and limitations of these approaches across diverse
capabilities, financial markets, and languages, offering valuable insights for
future research and development. The benchmark and evaluation code will be made
publicly available.

</details>


### [59] [Readers Prefer Outputs of AI Trained on Copyrighted Books over Expert Human Writers](https://arxiv.org/abs/2510.13939)
*Tuhin Chakrabarty,Jane C. Ginsburg,Paramveer Dhillon*

Main category: cs.CL

> 通过对三个前沿AI模型与人类专家的对比研究发现，虽然AI在不需要专门训练的情况下生成的文本并不受专家欢迎，但在经过针对特定作者的微调后，AI生成的文本在风格忠实度和写作质量上甚至被专家评为优于人类专家的文本，这可能对版权法的合理使用条款产生影响。

<details>
  <summary>Details</summary>

**Motivation:** 研究旨在探究AI模型是否能够在模仿获奖作家的风格的同时，生成高质量的文学作品，特别是在训练集使用受版权保护书籍的情况下。

**Method:** 研究采用了预注册方法，比较了AI模型（ChatGPT，Claude和Gemini）与人类专家作家生成的文本，进行了盲审评价，并分析了未经训练和经过特定作者微调后的AI作品差异。

**Result:** 研究结果显示，未经专门训练的AI模型生成的文本在专家评审中严重受到不利评价，但在经过针对作者完整作品的微调后，AI作品在风格和质量上被专家评为优于人类作家。此外，微调后的AI文本被人类视为人工智能生成的概率大大降低。

**Conclusion:** 该研究表明，经过作者特定微调后的AI生成文本可能比传统人类作者更受读者欢迎，这可能对未来的版权问题和市场价值产生影响。

**Abstract:** The use of copyrighted books for training AI models has led to numerous
lawsuits from authors concerned about AI's ability to generate derivative
content.Yet it's unclear whether these models can generate high quality
literary text while emulating authors' styles. To answer this we conducted a
preregistered study comparing MFA-trained expert writers with three frontier AI
models: ChatGPT, Claude & Gemini in writing up to 450 word excerpts emulating
50 award-winning authors' diverse styles. In blind pairwise evaluations by 159
representative expert & lay readers, AI-generated text from in-context
prompting was strongly disfavored by experts for both stylistic fidelity
(OR=0.16, p<10^8) & writing quality (OR=0.13, p<10^7) but showed mixed results
with lay readers. However, fine-tuning ChatGPT on individual authors' complete
works completely reversed these findings: experts now favored AI-generated text
for stylistic fidelity (OR=8.16, p<10^13) & writing quality (OR=1.87, p=0.010),
with lay readers showing similar shifts. These effects generalize across
authors & styles. The fine-tuned outputs were rarely flagged as AI-generated
(3% rate v. 97% for in-context prompting) by best AI detectors. Mediation
analysis shows this reversal occurs because fine-tuning eliminates detectable
AI stylistic quirks (e.g., cliche density) that penalize in-context outputs.
While we do not account for additional costs of human effort required to
transform raw AI output into cohesive, publishable prose, the median
fine-tuning & inference cost of $81 per author represents a dramatic 99.7%
reduction compared to typical professional writer compensation. Author-specific
fine-tuning thus enables non-verbatim AI writing that readers prefer to expert
human writing, providing empirical evidence directly relevant to copyright's
fourth fair-use factor, the "effect upon the potential market or value" of the
source works.

</details>


### [60] [Less is More: Improving LLM Reasoning with Minimal Test-Time Intervention](https://arxiv.org/abs/2510.13940)
*Zhen Yang,Mingyang Zhang,Feng Chen,Ganggui Ding,Liang Hou,Xin Tao,Pengfei Wan,Ying-Cong Chen*

Main category: cs.CL

> 研究揭示了推理过程中不确定性是局部集中的，并提出了一种无训练框架MTI来提高推理准确性和稳定性，同时保持计算效率。

<details>
  <summary>Details</summary>

**Motivation:** 推理不确定性主要集中在一小部分高熵标记上，这导致了输出的正确性。因此，研究者提出MTI框架以减少计算开销并提高模型推理性能。

**Method:** 提出了一种无训练框架Minimal Test-Time Intervention (MTI)，包括：(i) 选择性CFG干预，在不确定位置应用无分类器指导；(ii) 轻量级负提示指导，重用主模型的KV缓存以高效地近似无条件解码。

**Result:** MTI方法在通用、编程和STEM任务上均带来了稳定提升，例如，在Qwen3-8B-Base的八个基准上平均提高了1.35%，在使用Qwen3-32B-Reasoning的AIME2024上提高了5%，同时保持了高效性。

**Conclusion:** MTI框架通过在推理阶段进行最小干预，有效提升了语言模型的推理准确性和稳定性，且计算开销小。

**Abstract:** Recent progress in large language models (LLMs) has focused on test-time
scaling to improve reasoning via increased inference computation, but often at
the cost of efficiency. We revisit test-time behavior and uncover a simple yet
underexplored phenomenon: reasoning uncertainty is highly localized-only a
small subset of high-entropy tokens dominantly affects output correctness.
Motivated by this, we propose Minimal Test-Time Intervention (MTI), a
training-free framework that enhances reasoning accuracy and stability with
minimal overhead. MTI includes: (i) Selective CFG intervention, applying
classifier-free guidance only at uncertain positions; and (ii) Lightweight
negative-prompt guidance, reusing the main model's KV cache to approximate
unconditional decoding efficiently. MTI yields consistent gains across general,
coding, and STEM tasks-e.g., +1.35% average improvement on eight benchmarks for
Qwen3-8B-Base and +5% on AIME2024 using Qwen3-32B-Reasoning-while remaining
highly efficient.

</details>


### [61] [Classifying and Addressing the Diversity of Errors in Retrieval-Augmented Generation Systems](https://arxiv.org/abs/2510.13975)
*Kin Kwan Leung,Mouloud Belbahri,Yi Sui,Alex Labach,Xueying Zhang,Stephen Rose,Jesse C. Cresswell*

Main category: cs.CL

> 本研究提出了一种针对检索增强生成(RAG)系统错误的新分类方法，并提供了一个标注了错误类型的错误响应数据集。同时提出了一个与分类方法相匹配的自评估方法，用于实践中跟踪和解决开发中的错误。

<details>
  <summary>Details</summary>

**Motivation:** 研究目的是为了更好地理解在实际应用中RAG系统的各种潜在错误并在开发中解决它们，从而提高系统的鲁棒性和性能。

**Method:** 研究提出一种针对RAG系统的错误分类，并给出错误类型的实例和应对建议。同时还构建了一个RAG错误响应的数据集，并提出了一个自评估方法来跟踪解决开发中的错误。

**Result:** 本研究构建了错误类型的分类，并提供了一个标注了错误类型的RAG数据集，同时提出了一个自评估方法，用于跟踪解决问题。

**Conclusion:** 本研究提供了一个新的RAG系统错误分类方法和数据分析，以及与之相匹配的自评估方法，这对于RAG系统的后续研究和应用有重要的参考价值。

**Abstract:** Retrieval-augmented generation (RAG) is a prevalent approach for building
LLM-based question-answering systems that can take advantage of external
knowledge databases. Due to the complexity of real-world RAG systems, there are
many potential causes for erroneous outputs. Understanding the range of errors
that can occur in practice is crucial for robust deployment. We present a new
taxonomy of the error types that can occur in realistic RAG systems, examples
of each, and practical advice for addressing them. Additionally, we curate a
dataset of erroneous RAG responses annotated by error types. We then propose an
auto-evaluation method aligned with our taxonomy that can be used in practice
to track and address errors during development. Code and data are available at
https://github.com/layer6ai-labs/rag-error-classification.

</details>


### [62] [The German Commons - 154 Billion Tokens of Openly Licensed Text for German Language Models](https://arxiv.org/abs/2510.13996)
*Lukas Gienapp,Christopher Schröder,Stefan Schweter,Christopher Akiki,Ferdinand Schlatt,Arden Zimmermann,Phillipe Genêt,Martin Potthast*

Main category: cs.CL

> 本文介绍了一个名为German Commons的德语文本集合，这是迄今为止最大的公开许可的德语文本集合，为开发开放的德语文本模型提供了资源和可能性。

<details>
  <summary>Details</summary>

**Motivation:** 解决大型语言模型开发过程中依赖的大型训练语料库中包含许可证状态不明的数据的问题，尤其是对于非英语语言，公开许可的文本仍然非常稀缺的问题。

**Method:** 系统地从有可验证许可证的41个数据源中收集文本，涵盖7个领域，包括法律、科学、文化、政治、新闻、经济和网络文本。通过质量过滤、去重和文本格式修复，确保文本质量的一致性。

**Result:** 创建了迄今为止最大的公开许可的德语文本集合——German Commons，包含了154.56亿个高质量的文本标记，用于语言模型的训练。

**Conclusion:** German Commons解决了公开许可德语预训练数据的关键缺口，使开发真正的开放的德语文本模型成为可能。同时，公布了用于语料库构建和数据过滤的代码，使German Commons具有完全可重复性和可扩展性。

**Abstract:** Large language model development relies on large-scale training corpora, yet
most contain data of unclear licensing status, limiting the development of
truly open models. This problem is exacerbated for non-English languages, where
openly licensed text remains critically scarce. We introduce the German
Commons, the largest collection of openly licensed German text to date. It
compiles data from 41 sources across seven domains, encompassing legal,
scientific, cultural, political, news, economic, and web text. Through
systematic sourcing from established data providers with verifiable licensing,
it yields 154.56 billion tokens of high-quality text for language model
training. Our processing pipeline implements comprehensive quality filtering,
deduplication, and text formatting fixes, ensuring consistent quality across
heterogeneous text sources. All domain subsets feature licenses of at least
CC-BY-SA 4.0 or equivalent, ensuring legal compliance for model training and
redistribution. The German Commons therefore addresses the critical gap in
openly licensed German pretraining data, and enables the development of truly
open German language models. We also release code for corpus construction and
data filtering tailored to German language text, rendering the German Commons
fully reproducible and extensible.

</details>


### [63] [CRaFT: An Explanation-Based Framework for Evaluating Cultural Reasoning in Multilingual Language Models](https://arxiv.org/abs/2510.14014)
*Shehenaz Hossain,Haithem Afli*

Main category: cs.CL

> 研究人员提出了CRaFT多语言评估框架，以评估大型语言模型在不同文化环境中的推理能力，结果显示语言模型的文化意识并非固有，而是通过语言框架产生的。

<details>
  <summary>Details</summary>

**Motivation:** 传统的评估方法主要基于输出的准确性进行评分，而忽略了在跨文化背景下推理的差异性。

**Method:** 介绍了一种基于解释的多语言评估框架CRaFT，该框架用于评估大型语言模型（LLMs）在跨文化语境中的推理能力。CRaFT框架基于文化通达性、偏差、一致性、语言适应性这四个可解释的指标对模型解释进行评估。

**Result:** 结果表明跨语言推理存在显著差异：阿拉伯语降低了通达性，孟加拉语增强了通达性，西班牙语相对稳定。GPT更适合跨语言适应，但在一致性方面较低；FANAR则表现出稳定但僵化的推理能力。

**Conclusion:** CRaFT提供了一种新的视角，用于在多语言环境中评估跨文化推理，并为构建具有文化适应能力的语言模型提供了可操作的见解。

**Abstract:** Correct answers do not necessarily reflect cultural understanding. We
introduce CRaFT, an explanation-based multilingual evaluation framework
designed to assess how large language models (LLMs) reason across cultural
contexts. Rather than scoring outputs solely based on accuracy, CRaFT evaluates
model explanations using four interpretable metrics: Cultural Fluency,
Deviation, Consistency, and Linguistic Adaptation. We apply the framework to 50
culturally grounded questions from the World Values Survey, translated into
Arabic, Bengali, and Spanish, and evaluate three models (GPT, DeepSeek, and
FANAR) across over 2,100 answer-explanation pairs. Results reveal significant
cross-lingual variation in reasoning: Arabic reduces fluency, Bengali enhances
it, and Spanish remains largely stable. While GPT adapts more effectively
across languages, it exhibits lower consistency; FANAR shows stable but rigid
reasoning. These findings suggest that cultural awareness in LLMs is not
intrinsic but emerges through linguistic framing. CRaFT offers a new lens for
evaluating cross-cultural reasoning in multilingual settings, providing
actionable insights for building culturally adaptive language models.

</details>


### [64] [Think Globally, Group Locally: Evaluating LLMs Using Multi-Lingual Word Grouping Games](https://arxiv.org/abs/2510.14030)
*César Guerra-Solano,Zhuochun Li,Xiang Lorraine Li*

Main category: cs.CL

> 研究开发了一种名为GlobalGroup的任务，用于评估大型语言模型在多种语言中的抽象推理能力，发现英语言模态通常表现更好，并且在开源和闭源模型中存在性能差距。

<details>
  <summary>Details</summary>

**Motivation:** 研究动机在于探索大型语言模型在抽象推理任务中因语言模态而产生的偏见，这是以前的研究较少涉及的领域。抽象推理在生活中至关重要，因为它涉及到跳出框框思考和识别使用模式来解决问题的能力。

**Method:** 此研究开发了一个名为GlobalGroup的任务，该任务基于纽约时报的Connection谜题，用于评估跨多种语言（包括英语、西班牙语、汉语、印地语和阿拉伯语）的抽象推理能力。研究设计了具有相同难度级别的游戏基准，以便更好地控制和对比不同模型的表现。

**Result:** 实验发现，在抽象推理任务中，英语言模态表现出更好的性能，并且在开源和闭源模型之间存在性能差异。

**Conclusion:** 通过这项研究，作者揭示了不同的语言模态对抽象推理表现的影响，尤其在现有开源和闭源模型中存在显著差异。

**Abstract:** Large language models (LLMs) can exhibit biases in reasoning capabilities due
to linguistic modality, performing better on tasks in one language versus
another, even with similar content. Most previous works evaluate this through
reasoning tasks where reliance on strategies or knowledge can ensure success,
such as in commonsense or math tasks. However, abstract reasoning is vital to
reasoning for everyday life, where people apply "out-of-the-box thinking" to
identify and use patterns for solutions, without a reliance on formulaic
approaches. Comparatively, little work has evaluated linguistic biases in this
task type. In this paper, we propose a task inspired by the New York Times
Connections: GlobalGroup, that evaluates models in an abstract reasoning task
across several languages. We constructed a game benchmark with five linguistic
backgrounds -- English, Spanish, Chinese, Hindi, and Arabic -- in both the
native language and an English translation for comparison. We also proposed
game difficulty measurements to evaluate models on games with similar
difficulty, enabling a more controlled comparison, which is particularly
important in reasoning evaluations. Through experimentation, we find English
modalities largely lead to better performance in this abstract reasoning task,
and performance disparities between open- and closed-source models.

</details>


### [65] [Quantifying Phonosemantic Iconicity Distributionally in 6 Languages](https://arxiv.org/abs/2510.14040)
*George Flint,Kaustubh Kislay*

Main category: cs.CL

> 研究通过分布的方法，量化分析了六种语言中的音义同构性，发现了新的对齐现象和跨语言模式，并验证了一些假设。

<details>
  <summary>Details</summary>

**Motivation:** 研究探讨了在大规模定量研究中，语音和语义之间的系统关系可以如何体现，包括已经确定和未被识别的现象。

**Method:** 采用分布的方法量化了六种不同语言（英语，西班牙语，印地语，芬兰语，土耳其语和泰米尔语）中的音义同构性。在每种语言中，通过一系列统计测量手段分析音素的语音和语义相似空间的对齐情况。

**Result:** 发现了文献中尚未被确定的一系列可解释的音义对齐现象，以及跨语言的模式。对五种已经假设的音义对齐现象进行了分析，发现对某些现象的支持，而对其他现象的结果则不一。

**Conclusion:** 研究结果表明，在几种不同语言中，存在有趣且可解释的音义同构现象，这为语言的音义系统性研究提供了新的数据和见解。

**Abstract:** Language is, as commonly theorized, largely arbitrary. Yet, systematic
relationships between phonetics and semantics have been observed in many
specific cases. To what degree could those systematic relationships manifest
themselves in large scale, quantitative investigations--both in previously
identified and unidentified phenomena? This work undertakes a distributional
approach to quantifying phonosemantic iconicity at scale across 6 diverse
languages (English, Spanish, Hindi, Finnish, Turkish, and Tamil). In each
language, we analyze the alignment of morphemes' phonetic and semantic
similarity spaces with a suite of statistical measures, and discover an array
of interpretable phonosemantic alignments not previously identified in the
literature, along with crosslinguistic patterns. We also analyze 5 previously
hypothesized phonosemantic alignments, finding support for some such alignments
and mixed results for others.

</details>


### [66] [ERGO: Entropy-guided Resetting for Generation Optimization in Multi-turn Language Models](https://arxiv.org/abs/2510.14077)
*Haziq Mohammad Khalid,Athikash Jeyaganthan,Timothy Do,Yicheng Fu,Sean O'Brien,Vasu Sharma,Kevin Zhu*

Main category: cs.CL

> Error

<details>
  <summary>Details</summary>

**Motivation:** Error

**Method:** Error

**Result:** Error

**Conclusion:** Error

**Abstract:** Large Language Models (LLMs) suffer significant performance degradation in
multi-turn conversations when information is presented incrementally. Given
that multi-turn conversations characterize everyday interactions with LLMs,
this degradation poses a severe challenge to real world usability. We
hypothesize that abrupt increases in model uncertainty signal misalignment in
multi-turn LLM interactions, and we exploit this insight to dynamically realign
conversational context. We introduce ERGO (Entropy-guided Resetting for
Generation Optimization), which continuously quantifies internal uncertainty
via Shannon entropy over next token distributions and triggers adaptive prompt
consolidation when a sharp spike in entropy is detected. By treating
uncertainty as a first class signal rather than a nuisance to eliminate, ERGO
embraces variability in language and modeling, representing and responding to
uncertainty. In multi-turn tasks with incrementally revealed instructions, ERGO
yields a 56.6% average performance gain over standard baselines, increases
aptitude (peak performance capability) by 24.7%, and decreases unreliability
(variability in performance) by 35.3%, demonstrating that uncertainty aware
interventions can improve both accuracy and reliability in conversational AI.

</details>


### [67] [DROID: Dual Representation for Out-of-Scope Intent Detection](https://arxiv.org/abs/2510.14110)
*Wael Rashwan,Hossam M. Zawbaa,Sourav Dutta,Haytham Assem*

Main category: cs.CL

> DROID is a compact end-to-end framework for detecting out-of-scope intents using dual encoders and a simple calibration method, outperforming current techniques.

<details>
  <summary>Details</summary>

**Motivation:** To address the challenge of detecting out-of-scope (OOS) user utterances in task-oriented dialogue systems, which is critical for robust open-set intent recognition.

**Method:** DROID (Dual Representation for Out-of-Scope Intent Detection) combines the Universal Sentence Encoder (USE) and a domain-adapted Transformer-based Denoising Autoencoder (TSDAE), with a lightweight branched classifier.

**Result:** DROID achieves macro-F1 improvements of 6--15% for known and 8--20% for OOS intents, with significant gains in low-resource settings, outperforming recent state-of-the-art baselines.

**Conclusion:** These results demonstrate that dual-encoder representations with simple calibration can provide a robust, scalable, and reliable OOS detection system for neural dialogue systems.

**Abstract:** Detecting out-of-scope (OOS) user utterances remains a key challenge in
task-oriented dialogue systems and, more broadly, in open-set intent
recognition. Existing approaches often depend on strong distributional
assumptions or auxiliary calibration modules. We present DROID (Dual
Representation for Out-of-Scope Intent Detection), a compact end-to-end
framework that combines two complementary encoders -- the Universal Sentence
Encoder (USE) for broad semantic generalization and a domain-adapted
Transformer-based Denoising Autoencoder (TSDAE) for domain-specific contextual
distinctions. Their fused representations are processed by a lightweight
branched classifier with a single calibrated threshold that separates in-domain
and OOS intents without post-hoc scoring. To enhance boundary learning under
limited supervision, DROID incorporates both synthetic and open-domain outlier
augmentation. Despite using only 1.5M trainable parameters, DROID consistently
outperforms recent state-of-the-art baselines across multiple intent
benchmarks, achieving macro-F1 improvements of 6--15% for known and 8--20% for
OOS intents, with the most significant gains in low-resource settings. These
results demonstrate that dual-encoder representations with simple calibration
can yield robust, scalable, and reliable OOS detection for neural dialogue
systems.

</details>


### [68] [Toward Cybersecurity-Expert Small Language Models](https://arxiv.org/abs/2510.14113)
*Matan Levi,Daniel Ohayon,Ariel Blobstein,Ravid Sagi,Ian Molloy,Yair Allouche*

Main category: cs.CL

> CyberPal 2.0是一个小型语言模型系列，专门为网络安全设计，参数规模从4B到20B不等。通过SecKnowledge 2.0生成的高质量数据集进行训练，CyberPal 2.0在网络安全任务中表现优异，超越多个前沿模型，尽管规模更小。

<details>
  <summary>Details</summary>

**Motivation:** 由于缺乏高质量的网络安全特定模型和训练数据集，大型语言模型（LLMs）在网络安全领域的应用滞后。为了弥补这一差距，提出了CyberPal 2.0，它是一个由4B-20B参数构成的小型语言模型系列，专门用于网络安全。

**Method:** CyberPal 2.0通过我们的数据增强和格式化管道SecKnowledge 2.0生成了一个丰富的网络安全指令数据集。这个管道结合了专家在环的推理格式指导以及LLM驱动的多步骤推理，提高了安全任务的推理轨迹的保真度。

**Result:** CyberPal 2.0在网络安全基准测试中表现优于其基线模型，同时大小仅为前沿模型的一小部分。在核心网络安全知识任务中，CyberPal 2.0的表现仅次于Sec-Gemini v1。在核心威胁调查任务中，最好的20B参数模型排名第一，而最小的4B参数模型排名第二。

**Conclusion:** 研究表明，CyberPal 2.0在网络安全特定任务中具有竞争力，特别是在网络安全知识任务和威胁调查任务中，表现远超多数前沿模型，并且其模型大小远小于前沿模型。

**Abstract:** Large language models (LLMs) are transforming everyday applications, yet
deployment in cybersecurity lags due to a lack of high-quality, domain-specific
models and training datasets. To address this gap, we present CyberPal 2.0, a
family of cybersecurity-expert small language models (SLMs) ranging from 4B-20B
parameters. To train CyberPal 2.0, we generate an enriched chain-of-thought
cybersecurity instruction dataset built with our data enrichment and formatting
pipeline, SecKnowledge 2.0, which integrates expert-in-the-loop steering of
reasoning formats alongside LLM-driven multi-step grounding, yielding
higher-fidelity, task-grounded reasoning traces for security tasks. Across
diverse cybersecurity benchmarks, CyberPal 2.0 consistently outperforms its
baselines and matches or surpasses various open and closed-source frontier
models, while remaining a fraction of their size. On core cyber threat
intelligence knowledge tasks, our models outperform almost all tested frontier
models, ranking second only to Sec-Gemini v1. On core threat-investigation
tasks, such as correlating vulnerabilities and bug tickets with weaknesses, our
best 20B-parameter model outperforms GPT-4o, o1, o3-mini, and Sec-Gemini v1,
ranking first, while our smallest 4B-parameter model ranks second.

</details>


### [69] [Building a Macedonian Recipe Dataset: Collection, Parsing, and Comparative Analysis](https://arxiv.org/abs/2510.14128)
*Darko Sasanski,Dimitar Peshevski,Riste Stojanov,Dimitar Trajanov*

Main category: cs.CL

> 本文通过网络爬虫构建了一个马其顿语食谱数据集，该数据集用于分析马其顿菜肴的成分组合，填补了该语言在食谱数据方面的研究空白。

<details>
  <summary>Details</summary>

**Motivation:** 开发该马其顿语食谱数据集的动机是为了弥补马其顿语食谱在数字研究中的不足，为研究语言未被充分代表的文化提供了新的资源。

**Method:** 通过网络爬虫和结构化解析系统性地构建了首个马其顿语食谱数据集，解决了处理异构成分描述（包括单位、数量和描述符标准化）的挑战。

**Result:** 通过运用点互信息和提升分数等度量工具，对成分频率和共现模式进行了探索性分析，突显了定义马其顿菜肴的独特成分组合。

**Conclusion:** 这项研究为数字化研究提供了宝贵的资源，有助于理解和研究马其顿等未被充分代表语言地区的食物文化特色。

**Abstract:** Computational gastronomy increasingly relies on diverse, high-quality recipe
datasets to capture regional culinary traditions. Although there are
large-scale collections for major languages, Macedonian recipes remain
under-represented in digital research. In this work, we present the first
systematic effort to construct a Macedonian recipe dataset through web scraping
and structured parsing. We address challenges in processing heterogeneous
ingredient descriptions, including unit, quantity, and descriptor
normalization. An exploratory analysis of ingredient frequency and
co-occurrence patterns, using measures such as Pointwise Mutual Information and
Lift score, highlights distinctive ingredient combinations that characterize
Macedonian cuisine. The resulting dataset contributes a new resource for
studying food culture in underrepresented languages and offers insights into
the unique patterns of Macedonian culinary tradition.

</details>


### [70] [RLSR: Reinforcement Learning with Supervised Reward Outperforms SFT in Instruction Following](https://arxiv.org/abs/2510.14200)
*Zhichao Wang,Andy Wong,Ruslan Belkin*

Main category: cs.CL

> 提出RLSR方法来强化基础模型的指令跟随能力，基于SFT数据集，并通过在Qwen-7B (INFINITY)上的实验表明，RLSR的表现超过了传统的SFT方法，特别是在与SFT结合时。

<details>
  <summary>Details</summary>

**Motivation:** 动机在于提升基础模型的指令跟随能力，减少不期望的响应，提高推理能力，并支持以最少的数据进行高效的领域适应。

**Method:** RLSR方法通过利用SFT数据集，结合RL框架来改善基础模型的指令跟随能力。具体来说，基模型对每个提示生成多个响应，奖励分数计算为生成响应与人类标签响应在语义嵌入空间中的余弦相似度。

**Result:** RLSR直接替换SFT时，在指令跟随基准上的表现优于SFT，例如Qwen-7B (INFINITY)使用RLSR (SB)获得的AlpacaEval胜率为26.34%，优于SFT的21.01%。同时，结合SFT与RLSR进一步提升了下游任务的表现，Qwen-7B (INFINITY)通过SFT + RLSR训练获得了30.73%的胜率。

**Conclusion:** RLSR方法通过改进基础模型的指令跟随能力，展现了相较于传统方法的优越性，特别是在与SFT相结合时，性能得到进一步提升。

**Abstract:** After the pretraining stage of LLMs, techniques such as SFT, RLHF, RLVR, and
RFT are applied to enhance instruction-following ability, mitigate undesired
responses, improve reasoning capability and enable efficient domain adaptation
with minimal data. SFT relies on the next-token prediction objective to
strengthen instruction following in a base model using a large corpus of
human-labeled responses. In contrast, RFT employs a RL-based approach to adapt
fine-tuned reasoning models to specific domains with limited supervision.
Inspired by RFT, we propose replacing SFT with RLSR to leverage the extensive
SFT dataset in an RL framework, thereby improving the base model's
instruction-following ability. In RLSR, the base model generates multiple
responses for each prompt, and reward scores are computed as the cosine
similarity in the semantic embedding space between the generated and
human-labeled responses. RLSR can be utilized in multiple ways. It can directly
replace SFT, achieving superior performance on instruction-following
benchmarks-for example, RLSR (SB) on Qwen-7B (INFINITY) achieved an AlpacaEval
win rate of 26.34%, surpassing SFT's 21.01%. Furthermore, combining SFT and
RLSR further enhances downstream task performance; Qwen-7B (INFINITY) achieved
a win rate of 30.73% when trained with SFT + RLSR.

</details>


### [71] [DPRF: A Generalizable Dynamic Persona Refinement Framework for Optimizing Behavior Alignment Between Personalized LLM Role-Playing Agents and Humans](https://arxiv.org/abs/2510.14205)
*Bingsheng Yao,Bo Sun,Yuanzhe Dong,Yuxuan Lu,Dakuo Wang*

Main category: cs.CL

> The paper introduces the Dynamic Persona Refinement Framework (DPRF) which enhances the alignment of large language model role-playing agents' behavior with target individuals by refining the persona profiles based on cognitive divergence analysis.

<details>
  <summary>Details</summary>

**Motivation:** The persona fidelity of large language model role-playing agents is often compromised by manually-created profiles lacking validation against target individuals.

**Method:** DPRF optimizes the alignment by iteratively identifying and addressing cognitive divergence through either free-form or theory-grounded structured analysis.

**Result:** Evaluation across four diverse behavior-prediction scenarios with five LLMs demonstrated consistent improvement in behavioral alignment using DPRF compared to baseline personas.

**Conclusion:** This work offers a robust method for developing high-fidelity persona profiles and improving the validity of applications like user simulation and personalized AI.

**Abstract:** The emerging large language model role-playing agents (LLM RPAs) aim to
simulate individual human behaviors, but the persona fidelity is often
undermined by manually-created profiles (e.g., cherry-picked information and
personality characteristics) without validating the alignment with the target
individuals. To address this limitation, our work introduces the Dynamic
Persona Refinement Framework (DPRF).DPRF aims to optimize the alignment of LLM
RPAs' behaviors with those of target individuals by iteratively identifying the
cognitive divergence, either through free-form or theory-grounded, structured
analysis, between generated behaviors and human ground truth, and refining the
persona profile to mitigate these divergences.We evaluate DPRF with five LLMs
on four diverse behavior-prediction scenarios: formal debates, social media
posts with mental health issues, public interviews, and movie reviews.DPRF can
consistently improve behavioral alignment considerably over baseline personas
and generalizes across models and scenarios.Our work provides a robust
methodology for creating high-fidelity persona profiles and enhancing the
validity of downstream applications, such as user simulation, social studies,
and personalized AI.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [72] [MultiFoodhat: A potential new paradigm for intelligent food quality inspection](https://arxiv.org/abs/2510.13889)
*Yue Hu,Guohang Zhuang*

Main category: cs.CV

> 本文研究介绍了MultiFoodChat框架，用于零样本食品识别，通过多代理间的协作推理提升了复杂食品场景的理解能力，展示了在智能食品质量检测中的潜力。

<details>
  <summary>Details</summary>

**Motivation:** 鉴于现有的监督模型依赖大量标记的数据集，并且在面对未见过的食品类别时具有有限的泛化能力。这项研究旨在克服这些问题。

**Method:** 研究引入了MultiFoodChat，一个基于对话的多代理推理框架，用于零样本食品识别。该框架集成了视觉语言模型（VLMs）和大型语言模型（LLMs），通过多轮次视图-文本对话，实现了协作推理。使用对象感知标记（OPT）捕捉细粒度的视觉属性，而交互推理代理（IRA）动态解释上下文线索以优化预测。

**Result:** 实验结果表明，MultiFoodChat在多个公开食品数据集上展示了优于现有无监督和少样本方法的识别准确性和可解释性。

**Conclusion:** 实验结果显示，MultiFoodChat在多个公开的食品数据集上，相较于现有的无监督和少样本方法，实现了更高的识别准确性和可解释性，突显了其作为智能食品质量检测新范例的潜力。

**Abstract:** Food image classification plays a vital role in intelligent food quality
inspection, dietary assessment, and automated monitoring. However, most
existing supervised models rely heavily on large labeled datasets and exhibit
limited generalization to unseen food categories. To overcome these challenges,
this study introduces MultiFoodChat, a dialogue-driven multi-agent reasoning
framework for zero-shot food recognition. The framework integrates
vision-language models (VLMs) and large language models (LLMs) to enable
collaborative reasoning through multi-round visual-textual dialogues. An Object
Perception Token (OPT) captures fine-grained visual attributes, while an
Interactive Reasoning Agent (IRA) dynamically interprets contextual cues to
refine predictions. This multi-agent design allows flexible and human-like
understanding of complex food scenes without additional training or manual
annotations. Experiments on multiple public food datasets demonstrate that
MultiFoodChat achieves superior recognition accuracy and interpretability
compared with existing unsupervised and few-shot methods, highlighting its
potential as a new paradigm for intelligent food quality inspection and
analysis.

</details>


### [73] [Post-surgical Endometriosis Segmentation in Laparoscopic Videos](https://arxiv.org/abs/2510.13899)
*Andreas Leibetseder,Klaus Schoeffmann,Jörg Keckstein,Simon Keckstein*

Main category: cs.CV

> 该论文提出了一种系统，用于自动识别和标注腹腔镜手术视频中的暗色子宫内膜异位症病灶，并提供检测摘要以改善视频浏览。

<details>
  <summary>Details</summary>

**Motivation:** 子宫内膜异位症的表现形式多样，使得其识别难度大，准确性低。该系统旨在协助妇科医生诊断子宫内膜异位症。

**Method:** 系统用于识别腹腔镜手术视频中频发的暗色子宫内膜异位症病灶，进而进行病灶区域标注，并生成检测摘要。

**Result:** 论文未提供具体结果数据，但描述了系统的功能与目标。

**Conclusion:** 该系统有望提高子宫内膜异位症诊断的准确性和效率，辅助医生更好地处理相关病症。

**Abstract:** Endometriosis is a common women's condition exhibiting a manifold visual
appearance in various body-internal locations. Having such properties makes its
identification very difficult and error-prone, at least for laymen and
non-specialized medical practitioners. In an attempt to provide assistance to
gynecologic physicians treating endometriosis, this demo paper describes a
system that is trained to segment one frequently occurring visual appearance of
endometriosis, namely dark endometrial implants. The system is capable of
analyzing laparoscopic surgery videos, annotating identified implant regions
with multi-colored overlays and displaying a detection summary for improved
video browsing.

</details>


### [74] [Efficient Few-Shot Learning in Remote Sensing: Fusing Vision and Vision-Language Models](https://arxiv.org/abs/2510.13993)
*Jia Yun Chua,Argyrios Zolotas,Miguel Arana-Catania*

Main category: cs.CV

> This research enhances remote sensing image analysis by integrating YOLO and Vision Language Models, significantly improving aircraft detection accuracy and scene understanding, especially in data-limited and degraded imaging conditions.

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to overcome the limitations of traditional vision models in remote sensing, which require a significant amount of labelled data and struggle with complex environmental contexts. VLMs offer a promising alternative, but their application in remote sensing is still underdeveloped and requires exploration.

**Method:** This work integrates YOLO for object detection with Vision Language Models (VLMs) such as LLaVA, ChatGPT, and Gemini to improve aircraft detection and scene understanding in remote sensing. It explores the potential of VLMs to enhance traditional vision models without the need for extensive labelled data.

**Result:** The integration of VLMs and YOLO resulted in significant performance improvements, achieving an average MAE improvement of 48.46% in accuracy for aircraft detection and counting. There was also a 6.17% enhancement in CLIPScore, indicative of better scene comprehension, even under data degradation conditions.

**Conclusion:** The study concludes that combining VLMs with traditional vision models can effectively enhance remote sensing image analysis, particularly in scenarios with limited labelled data. This method paves the way for more accurate and contextually rich image interpretation in remote sensing tasks.

**Abstract:** Remote sensing has become a vital tool across sectors such as urban planning,
environmental monitoring, and disaster response. While the volume of data
generated has increased significantly, traditional vision models are often
constrained by the requirement for extensive domain-specific labelled data and
their limited ability to understand the context within complex environments.
Vision Language Models offer a complementary approach by integrating visual and
textual data; however, their application to remote sensing remains
underexplored, particularly given their generalist nature. This work
investigates the combination of vision models and VLMs to enhance image
analysis in remote sensing, with a focus on aircraft detection and scene
understanding. The integration of YOLO with VLMs such as LLaVA, ChatGPT, and
Gemini aims to achieve more accurate and contextually aware image
interpretation. Performance is evaluated on both labelled and unlabelled remote
sensing data, as well as degraded image scenarios which are crucial for remote
sensing. The findings show an average MAE improvement of 48.46% across models
in the accuracy of aircraft detection and counting, especially in challenging
conditions, in both raw and degraded scenarios. A 6.17% improvement in
CLIPScore for comprehensive understanding of remote sensing images is obtained.
The proposed approach combining traditional vision models and VLMs paves the
way for more advanced and efficient remote sensing image analysis, especially
in few-shot learning scenarios.

</details>


### [75] [Finding Holes: Pathologist Level Performance Using AI for Cribriform Morphology Detection in Prostate Cancer](https://arxiv.org/abs/2510.13995)
*Kelvin Szolnoky,Anders Blilie,Nita Mulliqi,Toyonori Tsuzuki,Hemamali Samaratunga,Matteo Titus,Xiaoyi Ji,Sol Erika Boman,Einar Gudlaugsson,Svein Reidar Kjosavik,José Asenjo,Marcello Gambacorta,Paolo Libretti,Marcin Braun,Radisław Kordek,Roman Łowicki,Brett Delahunt,Kenneth A. Iczkowski,Theo van der Kwast,Geert J. L. H. van Leenders,Katia R. M. Leite,Chin-Chen Pan,Emiel Adrianus Maria Janssen,Martin Eklund,Lars Egevad,Kimmo Kartasalo*

Main category: cs.CV

> 研究描述了一种基于深度学习的方法，用于提高前列腺癌中筛状结构识别。模型在内部和外部验证中表现优异，优于多名病理学家的表现。这表明AI系统在前列腺肿瘤组织学评估中具有潜力。

<details>
  <summary>Details</summary>

**Motivation:** 我们的目标是开发并验证一种基于AI的系统，以提高前列腺癌筛检中识别筛状结构的能力。筛状形态是一种表明不良预后的组织学特征，目前这项特征报告不足，并且病理学家之间的观察差异显著。

**Method:** 我们开发了一种基于深度学习的模型，采用EfficientNetV2-S编码器和多实例学习进行整个滑片的分类任务。该模型在来自430位患者的640个数字化前列腺核心针活检中进行训练，并在两个验证集上进行了内部和外部验证。内部验证集包含开发集使用的实验室或扫描仪，而外部验证集则使用完全独立的仪器和实验室。

**Result:** 模型在内部验证中表现出色（AUC: 0.97, 95% CI: 0.95-0.99; Cohen's kappa: 0.81, 95% CI: 0.72-0.89），在外部验证中也表现稳健（AUC: 0.90, 95% CI: 0.86-0.93; Cohen's kappa: 0.55, 95% CI: 0.45-0.64）。通过对88个内部验证集的幻灯片的评审者间分析，模型达到了最高的平均一致性（Cohen's kappa: 0.66, 95% CI: 0.57-0.74），超过了所有九名病理学家评分的Cohen's kappas范围从0.35到0.62。

**Conclusion:** 我们的AI模型在前列腺癌中筛状形态的检测上达到了病理学家级别的表现。这种方法有望提高诊断的可靠性，标准化报告，并改善前列腺癌患者的治疗决策。

**Abstract:** Background: Cribriform morphology in prostate cancer is a histological
feature that indicates poor prognosis and contraindicates active surveillance.
However, it remains underreported and subject to significant interobserver
variability amongst pathologists. We aimed to develop and validate an AI-based
system to improve cribriform pattern detection.
  Methods: We created a deep learning model using an EfficientNetV2-S encoder
with multiple instance learning for end-to-end whole-slide classification. The
model was trained on 640 digitised prostate core needle biopsies from 430
patients, collected across three cohorts. It was validated internally (261
slides from 171 patients) and externally (266 slides, 104 patients from three
independent cohorts). Internal validation cohorts included laboratories or
scanners from the development set, while external cohorts used completely
independent instruments and laboratories. Annotations were provided by three
expert uropathologists with known high concordance. Additionally, we conducted
an inter-rater analysis and compared the model's performance against nine
expert uropathologists on 88 slides from the internal validation cohort.
  Results: The model showed strong internal validation performance (AUC: 0.97,
95% CI: 0.95-0.99; Cohen's kappa: 0.81, 95% CI: 0.72-0.89) and robust external
validation (AUC: 0.90, 95% CI: 0.86-0.93; Cohen's kappa: 0.55, 95% CI:
0.45-0.64). In our inter-rater analysis, the model achieved the highest average
agreement (Cohen's kappa: 0.66, 95% CI: 0.57-0.74), outperforming all nine
pathologists whose Cohen's kappas ranged from 0.35 to 0.62.
  Conclusion: Our AI model demonstrates pathologist-level performance for
cribriform morphology detection in prostate cancer. This approach could enhance
diagnostic reliability, standardise reporting, and improve treatment decisions
for prostate cancer patients.

</details>


### [76] [NAPPure: Adversarial Purification for Robust Image Classification under Non-Additive Perturbations](https://arxiv.org/abs/2510.14025)
*Junjie Nan,Jianing Li,Wei Chen,Mingkun Zhang,Xueqi Cheng*

Main category: cs.CV

> NAPPure is a new adversarial purification framework designed to handle non-additive adversarial perturbations, offering improved robustness in image classification models.

<details>
  <summary>Details</summary>

**Motivation:** Existing adversarial purification methods are mostly effective against additive adversarial perturbations but perform poorly against non-additive perturbations common in real-world scenarios, motivating the development of NAPPure.

**Method:** The paper proposes an extended adversarial purification framework, NAPPure, to effectively handle non-additive perturbations such as blur, occlusion, and distortion. It establishes the generation process of an adversarial image and disentangles the underlying clean image and perturbation parameters by maximizing the likelihood.

**Result:** Experiments on the GTSRB and CIFAR-10 datasets demonstrate that NAPPure improves the robustness of image classification models against non-additive adversarial perturbations.

**Conclusion:** The proposed NAPPure framework is a significant improvement over existing adversarial purification methods, especially in dealing with non-additive perturbations.

**Abstract:** Adversarial purification has achieved great success in combating adversarial
image perturbations, which are usually assumed to be additive. However,
non-additive adversarial perturbations such as blur, occlusion, and distortion
are also common in the real world. Under such perturbations, existing
adversarial purification methods are much less effective since they are
designed to fit the additive nature. In this paper, we propose an extended
adversarial purification framework named NAPPure, which can further handle
non-additive perturbations. Specifically, we first establish the generation
process of an adversarial image, and then disentangle the underlying clean
image and perturbation parameters through likelihood maximization. Experiments
on GTSRB and CIFAR-10 datasets show that NAPPure significantly boosts the
robustness of image classification models against non-additive perturbations.

</details>


### [77] [Vgent: Graph-based Retrieval-Reasoning-Augmented Generation For Long Video Understanding](https://arxiv.org/abs/2510.14032)
*Xiaoqian Shen,Wenxuan Zhang,Jun Chen,Mohamed Elhoseiny*

Main category: cs.CV

> 该论文介绍了针对长时间视频理解的一种新的图像检索和推理增强生成框架Vgent，解决了RAG应用于视频时面临的挑战，并在多个评估中优于现有方法。

<details>
  <summary>Details</summary>

**Motivation:** 由于LVLMs难以处理超出上下文窗口的密集视频令牌并保持长期序列信息，因此对于长时间视频的理解和推理提出了巨大的挑战。RAG虽然在处理长上下文的LLMs方面显示出了有效性，但在应用于长时间视频时遇到中断时间依赖关系和包含无关信息等问题，这些问题可能会妨碍准确推理。

**Method:** 该论文提出了一种名为Vgent的新框架，它基于图形的检索-推理增强生成方法，旨在改进LVLMs的长时间视频理解能力。框架包含两个关键创新：(i) 将视频表示为保留视频片段间语义关系的结构化图，以提高检索效率；(ii) 引入中间推理步骤，利用结构化验证减少检索噪声，并促进片段间相关信息的显式聚合，从而生成更精准和语境相关的响应。

**Result:** 该框架在三个长时间视频理解基准上使用各种开源LVLMs进行了全面评估，相比于基础模型在MLVU上整体性能提高了3.0%~5.4%，并且在视频RAG方法上超过了现有最先进方法8.6%。

**Conclusion:** Vgent框架通过引入视频片段间的语义关系图和中间推理步骤，改进了LVLMs在长时间视频理解中的能力，并在多个基准测试中表现出色。代码已公开。

**Abstract:** Understanding and reasoning over long videos pose significant challenges for
large video language models (LVLMs) due to the difficulty in processing
intensive video tokens beyond context window and retaining long-term sequential
information. Retrieval-Augmented Generation (RAG) has demonstrated
effectiveness in processing long context for Large Language Models (LLMs);
however, applying RAG to long video faces challenges such as disrupted temporal
dependencies and inclusion of irrelevant information that can hinder accurate
reasoning. To address these limitations, we propose Vgent, a novel graph-based
retrieval-reasoning-augmented generation framework to enhance LVLMs for long
video understanding. Our approach introduces two key innovations: (i) It
represents videos by structured graphs with semantic relationships across video
clips preserved to improve retrieval effectiveness. (ii) It introduces an
intermediate reasoning step to mitigate the reasoning limitation of LVLMs,
which leverages structured verification to reduce retrieval noise and
facilitate the explicit aggregation of relevant information across clips,
resulting in more accurate and context-aware responses. We comprehensively
evaluate our framework with various open-source LVLMs on three long-video
understanding benchmarks. Our approach yielded an overall performance
improvement of $3.0\%\sim 5.4\%$ over base models on MLVU, and outperformed
state-of-the-art video RAG methods by $8.6\%$. Our code is publicly available
at https://xiaoqian-shen.github.io/Vgent.

</details>


### [78] [Synchronization of Multiple Videos](https://arxiv.org/abs/2510.14051)
*Avihai Naaman,Ron Shapira Weber,Oren Freifeld*

Main category: cs.CV

> 文章提出了一种新的方法TPL，用于同步来自不同场景或生成的AI视频，这些视频通常具有复杂的时间错位问题。实验表明，该方法提高了同步的精准度，效率和鲁棒性。

<details>
  <summary>Details</summary>

**Motivation:** 当前视频同步的挑战在于不同场景或最近的生成 AI 视频之间的同步，因为这些视频存在多种主题、背景和非线性时间错位问题。

**Method:** 我们提出了一种基于原型的学习框架(Temporal Prototype Learning, TPL)，该框架可以从任意预训练模型提取的高维嵌入中构建共享的紧凑一维表示。TPL 通过学习统一的原型序列，锚定关键动作阶段，从而避免了耗时的成对匹配，稳健地对齐视频。

**Result:** 实验表明，TPL 提高了跨多个数据集的同步准确性、效率和鲁棒性，包括细粒度的帧检索和阶段分类任务。

**Conclusion:** TPL 是首个解决多个生成 AI 视频中同步问题的方法。我们的代码和一个新的多视频同步数据集可以在指定网址获得。

**Abstract:** Synchronizing videos captured simultaneously from multiple cameras in the
same scene is often easy and typically requires only simple time shifts.
However, synchronizing videos from different scenes or, more recently,
generative AI videos, poses a far more complex challenge due to diverse
subjects, backgrounds, and nonlinear temporal misalignment. We propose Temporal
Prototype Learning (TPL), a prototype-based framework that constructs a shared,
compact 1D representation from high-dimensional embeddings extracted by any of
various pretrained models. TPL robustly aligns videos by learning a unified
prototype sequence that anchors key action phases, thereby avoiding exhaustive
pairwise matching. Our experiments show that TPL improves synchronization
accuracy, efficiency, and robustness across diverse datasets, including
fine-grained frame retrieval and phase classification tasks. Importantly, TPL
is the first approach to mitigate synchronization issues in multiple generative
AI videos depicting the same action. Our code and a new multiple video
synchronization dataset are available at https://bgu-cs-vil.github.io/TPL/

</details>


### [79] [Capture, Canonicalize, Splat: Zero-Shot 3D Gaussian Avatars from Unstructured Phone Images](https://arxiv.org/abs/2510.14081)
*Emanuel Garbin,Guy Adam,Oded Krams,Zohar Barzelay,Eran Guendelman,Michael Schwarz,Moran Vatelmacher,Yigal Shenkman,Eli Peker,Itai Druker,Uri Patish,Yoav Blum,Max Bluvstein,Junxuan Li,Rawal Khirodkar,Shunsuke Saito*

Main category: cs.CV

> 该研究工作解决了现有3D头像生成方法的现实性和身份保留性的问题，提出了一种新颖的零样本生成流程，可以通过少量的无结构性手机图片来生成具有高度真实性和身份保留性的3D头像。

<details>
  <summary>Details</summary>

**Motivation:** 现有的方法存在单视图几何不一致和身份保存问题，以及训练在合成数据上的模型无法捕获高频细节，从而限制了现实主义表现。新的方法旨在解决这些问题，提高现实性和身份保留性。

**Method:** 该论文提出一种新的零样本流程，用于从少量无结构的手机照片生成具有高度现实性和身份保留性的3D头像。该方法通过引入生成正则化模块处理多个视角生成标准化一致的表示，并采用基于变压器的模型，在高保真度高斯散射头像数据集上进行训练。

**Result:** 该方法能够从无结构的照片中生成具有吸引人的逼真度的静物四分之三身体头像，并具备强大的身份保留性。

**Conclusion:** 通过采用新的“捕捉、正则化、散射”管道，该方法能够从无结构的照片生成高真实性且保持身份特征的3D头像，展示了在零样本情景下的优越性能。

**Abstract:** We present a novel, zero-shot pipeline for creating hyperrealistic,
identity-preserving 3D avatars from a few unstructured phone images. Existing
methods face several challenges: single-view approaches suffer from geometric
inconsistencies and hallucinations, degrading identity preservation, while
models trained on synthetic data fail to capture high-frequency details like
skin wrinkles and fine hair, limiting realism. Our method introduces two key
contributions: (1) a generative canonicalization module that processes multiple
unstructured views into a standardized, consistent representation, and (2) a
transformer-based model trained on a new, large-scale dataset of high-fidelity
Gaussian splatting avatars derived from dome captures of real people. This
"Capture, Canonicalize, Splat" pipeline produces static quarter-body avatars
with compelling realism and robust identity preservation from unstructured
photos.

</details>


### [80] [cubic: CUDA-accelerated 3D Bioimage Computing](https://arxiv.org/abs/2510.14143)
*Alexandr A. Kalinin,Anne E. Carpenter,Shantanu Singh,Matthew J. O'Meara*

Main category: cs.CV

> cubic is a Python library that offers GPU-accelerated alternatives to existing bioimage analysis tools, providing a scalable and efficient solution for handling large 2D and 3D datasets in biomedical research.

<details>
  <summary>Details</summary>

**Motivation:** To overcome the scalability and interoperability limitations of existing bioimage analysis tools and to facilitate the integration of GPU acceleration into bioimage analysis workflows, enhancing the efficiency and speed of processing large image datasets.

**Method:** cubic, an open-source Python library that provides GPU-accelerated alternatives for bioimage analysis, compatible with SciPy and scikit-image, ensuring efficient processing of large 2D and 3D image datasets.

**Result:** Significant speedups have been achieved in bioimage analysis workflows by benchmarking individual operations and reproducing existing deconvolution and segmentation pipelines using cubic, while maintaining the accuracy of the algorithms used.

**Conclusion:** cubic provides a robust and scalable solution for bioimage analysis that integrates well with the Python scientific computing ecosystem, allowing for both interactive exploration and high-throughput automated analysis workflows.

**Abstract:** Quantitative analysis of multidimensional biological images is useful for
understanding complex cellular phenotypes and accelerating advances in
biomedical research. As modern microscopy generates ever-larger 2D and 3D
datasets, existing computational approaches are increasingly limited by their
scalability, efficiency, and integration with modern scientific computing
workflows. Existing bioimage analysis tools often lack application programmable
interfaces (APIs), do not support graphics processing unit (GPU) acceleration,
lack broad 3D image processing capabilities, and/or have poor interoperability
for compute-heavy workflows. Here, we introduce cubic, an open-source Python
library that addresses these challenges by augmenting widely used SciPy and
scikit-image APIs with GPU-accelerated alternatives from CuPy and RAPIDS cuCIM.
cubic's API is device-agnostic and dispatches operations to GPU when data
reside on the device and otherwise executes on CPU, seamlessly accelerating a
broad range of image processing routines. This approach enables GPU
acceleration of existing bioimage analysis workflows, from preprocessing to
segmentation and feature extraction for 2D and 3D data. We evaluate cubic both
by benchmarking individual operations and by reproducing existing deconvolution
and segmentation pipelines, achieving substantial speedups while maintaining
algorithmic fidelity. These advances establish a robust foundation for
scalable, reproducible bioimage analysis that integrates with the broader
Python scientific computing ecosystem, including other GPU-accelerated methods,
enabling both interactive exploration and automated high-throughput analysis
workflows. cubic is openly available at
https://github$.$com/alxndrkalinin/cubic

</details>


### [81] [Virtually Being: Customizing Camera-Controllable Video Diffusion Models with Multi-View Performance Captures](https://arxiv.org/abs/2510.14179)
*Yuancheng Xu,Wenqi Xian,Li Ma,Julien Philip,Ahmet Levent Taşel,Yiwei Zhao,Ryan Burgert,Mingming He,Oliver Hermann,Oliver Pilarski,Rahul Garg,Paul Debevec,Ning Yu*

Main category: cs.CV

> 本文工作提出一种用于视频生成的框架，通过创新的数据管道实现多视角角色一致性及3D相机控制，显著提升视频质量与个性化能力，支持虚拟制作的关键功能。

<details>
  <summary>Details</summary>

**Motivation:** 动机在于提供强大的多视角身份保持、精确的相机控制以及光照适应性，以及基本的虚拟制作功能，例如多对象生成、场景和真实视频定制以及在定制过程中对运动和空间布局的控制。

**Method:** 本文提出了一种框架，通过定制数据管道实现视频扩散模型中的多视角角色一致性以及3D相机控制。角色一致性组件通过4D高斯点云法（4DGS）和视频重光照模型结合多种相机轨迹渲染的体积捕捉表演来进行训练。

**Result:** 实验表明，与现有方法相比，本文方法在视频质量、个性化精度以及相机控制和光照适应性方面有显著提升。

**Conclusion:** 研究成果推进了视频生成技术在虚拟制作中的整合，证明了创新数据处理和模型训练方法的有效性。

**Abstract:** We introduce a framework that enables both multi-view character consistency
and 3D camera control in video diffusion models through a novel customization
data pipeline. We train the character consistency component with recorded
volumetric capture performances re-rendered with diverse camera trajectories
via 4D Gaussian Splatting (4DGS), lighting variability obtained with a video
relighting model. We fine-tune state-of-the-art open-source video diffusion
models on this data to provide strong multi-view identity preservation, precise
camera control, and lighting adaptability. Our framework also supports core
capabilities for virtual production, including multi-subject generation using
two approaches: joint training and noise blending, the latter enabling
efficient composition of independently customized models at inference time; it
also achieves scene and real-life video customization as well as control over
motion and spatial layout during customization. Extensive experiments show
improved video quality, higher personalization accuracy, and enhanced camera
control and lighting adaptability, advancing the integration of video
generation into virtual production. Our project page is available at:
https://eyeline-labs.github.io/Virtually-Being.

</details>


### [82] [Joint Modeling of Big Five and HEXACO for Multimodal Apparent Personality-trait Recognition](https://arxiv.org/abs/2510.14203)
*Ryo Masumura,Shota Orihashi,Mana Ihori,Tomohiro Tanaka,Naoki Makishima,Taiga Yamane,Naotaka Kawata,Satoshi Suzuki,Taichi Katayama*

Main category: cs.CV

> Error

<details>
  <summary>Details</summary>

**Motivation:** Error

**Method:** Error

**Result:** Error

**Conclusion:** Error

**Abstract:** This paper proposes a joint modeling method of the Big Five, which has long
been studied, and HEXACO, which has recently attracted attention in psychology,
for automatically recognizing apparent personality traits from multimodal human
behavior. Most previous studies have used the Big Five for multimodal apparent
personality-trait recognition. However, no study has focused on apparent HEXACO
which can evaluate an Honesty-Humility trait related to displaced aggression
and vengefulness, social-dominance orientation, etc. In addition, the
relationships between the Big Five and HEXACO when modeled by machine learning
have not been clarified. We expect awareness of multimodal human behavior to
improve by considering these relationships. The key advance of our proposed
method is to optimize jointly recognizing the Big Five and HEXACO. Experiments
using a self-introduction video dataset demonstrate that the proposed method
can effectively recognize the Big Five and HEXACO.

</details>


### [83] [LOTA: Bit-Planes Guided AI-Generated Image Detection](https://arxiv.org/abs/2510.14230)
*Hongsong Wang,Renxi Cheng,Yang Zhang,Chaolei Han,Jie Gui*

Main category: cs.CV

> 本文提出了一种基于位平面图像处理方法来检测AI生成图像的新技术，该技术极大地提高了检测的准确率并大幅减少了计算资源的需求。

<details>
  <summary>Details</summary>

**Motivation:** 传统的基于图像重建误差的方法消耗大量计算资源且难以捕捉图像中内在的噪声特征，这促使了本文技术的研发。

**Method:** 本文利用位平面化处理技术提取噪声特征，并设计了最大梯度补丁选择方法以放大噪声信号。提出了基于噪声和引导噪声的两种分类器策略。

**Result:** 实验表明，该方法在GenImage基准测试中的平均准确率为98.9%，并具有优秀的跨生成器泛化能力。特别的，该方法从GAN到Diffusion和Diffusion到GAN的准确率分别超过了98.2%和99.2%，且误差提取速度快约一百倍。

**Conclusion:** 本文的方法不仅提升了AI生成图像检测的准确率，还大幅提升了处理速度。

**Abstract:** The rapid advancement of GAN and Diffusion models makes it more difficult to
distinguish AI-generated images from real ones. Recent studies often use
image-based reconstruction errors as an important feature for determining
whether an image is AI-generated. However, these approaches typically incur
high computational costs and also fail to capture intrinsic noisy features
present in the raw images. To solve these problems, we innovatively refine
error extraction by using bit-plane-based image processing, as lower bit planes
indeed represent noise patterns in images. We introduce an effective bit-planes
guided noisy image generation and exploit various image normalization
strategies, including scaling and thresholding. Then, to amplify the noise
signal for easier AI-generated image detection, we design a maximum gradient
patch selection that applies multi-directional gradients to compute the noise
score and selects the region with the highest score. Finally, we propose a
lightweight and effective classification head and explore two different
structures: noise-based classifier and noise-guided classifier. Extensive
experiments on the GenImage benchmark demonstrate the outstanding performance
of our method, which achieves an average accuracy of \textbf{98.9\%}
(\textbf{11.9}\%~$\uparrow$) and shows excellent cross-generator generalization
capability. Particularly, our method achieves an accuracy of over 98.2\% from
GAN to Diffusion and over 99.2\% from Diffusion to GAN. Moreover, it performs
error extraction at the millisecond level, nearly a hundred times faster than
existing methods. The code is at https://github.com/hongsong-wang/LOTA.

</details>


### [84] [PIA: Deepfake Detection Using Phoneme-Temporal and Identity-Dynamic Analysis](https://arxiv.org/abs/2510.14241)
*Soumyya Kanti Datta,Tanvi Ranga,Chengzhe Sun,Siwei Lyu*

Main category: cs.CV

> 本文开发了一个名为Phoneme-Temporal and Identity-Dynamic Analysis(PIA)的新框架，用于改进对现代深度造假视频的检测能力，特别是对于使用高级生成模型创建的视频。它通过整合多模态数据来识别细微的时间不一致性。

<details>
  <summary>Details</summary>

**Motivation:** 传统的深度造假检测方法依赖于手动设计的音素-口型对齐阈值、基本帧级一致性检查或单模态检测策略，这些对于advanced generative models生成的深度造假视频来说，检测性能不足。因为这些高级技术生成的每一帧非常完美，但会无意中产生微小的时间不一致性，这被传统的检测器所忽略。

**Method:** 本文提出了一种多模态的音视频框架，称为Phoneme-Temporal和Identity-Dynamic Analysis(PIA)，结合了语言、动态面部运动和面部识别线索，以识别现代深度造假视频中的细微改动。该方法使用了音素序列、唇部几何数据和高级面部身份嵌入。

**Result:** 该集成方法通过识别多个互补模态中的不一致性，显著提高了检测现代深度伪造视频中细微变动的能力。

**Conclusion:** 本研究通过创建一种能够检测多模态细微改变的新框架，为对抗深度造假视频提供了一个有效的解决方案。

**Abstract:** The rise of manipulated media has made deepfakes a particularly insidious
threat, involving various generative manipulations such as lip-sync
modifications, face-swaps, and avatar-driven facial synthesis. Conventional
detection methods, which predominantly depend on manually designed
phoneme-viseme alignment thresholds, fundamental frame-level consistency
checks, or a unimodal detection strategy, inadequately identify modern-day
deepfakes generated by advanced generative models such as GANs, diffusion
models, and neural rendering techniques. These advanced techniques generate
nearly perfect individual frames yet inadvertently create minor temporal
discrepancies frequently overlooked by traditional detectors. We present a
novel multimodal audio-visual framework, Phoneme-Temporal and Identity-Dynamic
Analysis(PIA), incorporating language, dynamic face motion, and facial
identification cues to address these limitations. We utilize phoneme sequences,
lip geometry data, and advanced facial identity embeddings. This integrated
method significantly improves the detection of subtle deepfake alterations by
identifying inconsistencies across multiple complementary modalities. Code is
available at https://github.com/skrantidatta/PIA

</details>


### [85] [Event Interval Modulation: A Novel Scheme for Event-based Optical Camera Communication](https://arxiv.org/abs/2510.14245)
*Miu Sumino,Mayu Ishii,Shun Kaizu,Daisuke Hisano,Yu Nakayama*

Main category: cs.CV

> 本文提出了一种新的调制方案EIM，用于基于事件的光相机通信系统中，通过实验验证提高了传输速率，达到了新的基准。

<details>
  <summary>Details</summary>

**Motivation:** 针对传统的基于帧的相机的可见光通信技术存在的低比特率和高处理负载的问题，提出了使用基于事件视觉传感器的OCC系统来提高通信性能。

**Method:** 本文提出了一个新的调制方案，称为事件间隔调制（EIM）方案，专门用于基于事件的光相机通信（OCC）。EIM通过利用事件之间的间隔来调制信息，从而提高传输速度。

**Result:** 实验结果显示，在10米的距离内室内环境下成功实现了28 kbps的传输速度，在50米的距离内实现了8.4 kbps的传输速度。这为基于事件的可见光通信系统设置了新的速率基准。

**Conclusion:** 研究提出了一种新的基于事件间隔调制的理论模型，并通过实验验证了其可行性，证明了EIM方案提高了基于事件的OCC系统的传输速度。

**Abstract:** Optical camera communication (OCC) represents a promising visible light
communication technology. Nonetheless, typical OCC systems utilizing
frame-based cameras are encumbered by limitations, including low bit rate and
high processing load. To address these issues, OCC system utilizing an
event-based vision sensor (EVS) as receivers have been proposed. The EVS
enables high-speed, low-latency, and robust communication due to its
asynchronous operation and high dynamic range. In existing event-based OCC
systems, conventional modulation schemes such as on-off keying (OOK) and pulse
position modulation have been applied, however, to the best of our knowledge,
no modulation method has been proposed that fully exploits the unique
characteristics of the EVS. This paper proposes a novel modulation scheme,
called the event interval modulation (EIM) scheme, specifically designed for
event-based OCC. EIM enables improvement in transmission speed by modulating
information using the intervals between events. This paper proposes a
theoretical model of EIM and conducts a proof-of-concept experiment. First, the
parameters of the EVS are tuned and customized to optimize the frequency
response specifically for EIM. Then, the maximum modulation order usable in EIM
is determined experimentally. We conduct transmission experiments based on the
obtained parameters. Finally, we report successful transmission at 28 kbps over
10 meters and 8.4 kbps over 50 meters in an indoor environment. This sets a new
benchmark for bit rate in event-based OCC systems.

</details>


### [86] [MACE: Mixture-of-Experts Accelerated Coordinate Encoding for Large-Scale Scene Localization and Rendering](https://arxiv.org/abs/2510.14251)
*Mingkai Liu,Dikai Fan,Haohua Que,Haojia Gao,Xiao Liu,Shuxue Peng,Meixia Lin,Shengyu Gu,Ruicong Ye,Wanli Qiu,Handong Yao,Ruopeng Zhang,Xianliang Huang*

Main category: cs.CV

> 提出MACE和ALF-LB方法，解决了大规模场景下的高效定位和高质量渲染问题。

<details>
  <summary>Details</summary>

**Motivation:** 针对大规模场景中高效定位和高质量渲染面临的计算成本问题，尤其是Scene Coordinate Regression方法在扩展到大规模场景时受到单个网络容量限制，作者提出了一种新的解决方案。

**Method:** 提出了基于多专家系统的加速坐标编码方法（MACE），采用门控网络隐式分类和选择子网络，并提出了无辅助损失的负载均衡策略（ALF-LB）来提高大规模场景的定位精度。

**Result:** {

**Conclusion:** 该框架在降低成本的同时保持了较高的精度，为大规模场景应用提供了一种高效的解决方案。

**Abstract:** Efficient localization and high-quality rendering in large-scale scenes
remain a significant challenge due to the computational cost involved. While
Scene Coordinate Regression (SCR) methods perform well in small-scale
localization, they are limited by the capacity of a single network when
extended to large-scale scenes. To address these challenges, we propose the
Mixed Expert-based Accelerated Coordinate Encoding method (MACE), which enables
efficient localization and high-quality rendering in large-scale scenes.
Inspired by the remarkable capabilities of MOE in large model domains, we
introduce a gating network to implicitly classify and select sub-networks,
ensuring that only a single sub-network is activated during each inference.
Furtheremore, we present Auxiliary-Loss-Free Load Balancing(ALF-LB) strategy to
enhance the localization accuracy on large-scale scene. Our framework provides
a significant reduction in costs while maintaining higher precision, offering
an efficient solution for large-scale scene applications. Additional
experiments on the Cambridge test set demonstrate that our method achieves
high-quality rendering results with merely 10 minutes of training.

</details>


### [87] [Identity-Preserving Image-to-Video Generation via Reward-Guided Optimization](https://arxiv.org/abs/2510.14255)
*Liao Shen,Wentao Jiang,Yiran Zhu,Tiezheng Ge,Zhiguo Cao,Bo Zheng*

Main category: cs.CV

> Error

<details>
  <summary>Details</summary>

**Motivation:** Error

**Method:** Error

**Result:** Error

**Conclusion:** Error

**Abstract:** Recent advances in image-to-video (I2V) generation have achieved remarkable
progress in synthesizing high-quality, temporally coherent videos from static
images. Among all the applications of I2V, human-centric video generation
includes a large portion. However, existing I2V models encounter difficulties
in maintaining identity consistency between the input human image and the
generated video, especially when the person in the video exhibits significant
expression changes and movements. This issue becomes critical when the human
face occupies merely a small fraction of the image. Since humans are highly
sensitive to identity variations, this poses a critical yet under-explored
challenge in I2V generation. In this paper, we propose Identity-Preserving
Reward-guided Optimization (IPRO), a novel video diffusion framework based on
reinforcement learning to enhance identity preservation. Instead of introducing
auxiliary modules or altering model architectures, our approach introduces a
direct and effective tuning algorithm that optimizes diffusion models using a
face identity scorer. To improve performance and accelerate convergence, our
method backpropagates the reward signal through the last steps of the sampling
chain, enabling richer gradient feedback. We also propose a novel facial
scoring mechanism that treats faces in ground-truth videos as facial feature
pools, providing multi-angle facial information to enhance generalization. A
KL-divergence regularization is further incorporated to stabilize training and
prevent overfitting to the reward signal. Extensive experiments on Wan 2.2 I2V
model and our in-house I2V model demonstrate the effectiveness of our method.
Our project and code are available at
\href{https://ipro-alimama.github.io/}{https://ipro-alimama.github.io/}.

</details>


### [88] [Identity-GRPO: Optimizing Multi-Human Identity-preserving Video Generation via Reinforcement Learning](https://arxiv.org/abs/2510.14256)
*Xiangyu Meng,Zixian Zhang,Zhenghao Zhang,Junchao Liao,Long Qin,Weizhi Wang*

Main category: cs.CV

> 提出了一种基于人类反馈的优化流程Identity-GRPO，以解决当前先进方法在多个人身份保持视频生成中的问题，并展示了显著的性能提升。

<details>
  <summary>Details</summary>

**Motivation:** 尽管先进的方法如VACE和Phantom在多样场景下的特定主体视频生成方面取得了进步，但它们在多个人身份保持方面存在挑战，特别是在复杂的动态互动场景中，保持多个角色的一致性尤为重要。

**Method:** 提出了一种名为Identity-GRPO的优化流程，该流程基于人类反馈驱动，专注于改进多人身份保持的视频生成。首先，构建了一个视频奖励模型，该模型基于大规模偏好数据集进行训练，数据集中包含人类标注数据和合成的失真数据，并重点标注了视频中人类一致性维持的成对标注信息。接着，采用了一种专门为多人一致性定制的GRPO变体，极大地提升了VACE和Phantom方法的效果。

**Result:** 通过广泛的消融研究，评估了注释质量和设计选择对策略优化的影响。实验结果显示，Identity-GRPO在人类一致性指标上比基线方法提高了最多18.9%，为将强化学习与个性化视频生成相结合提供了实际洞察。

**Conclusion:** Identity-GRPO通过引入人为标注数据到视频奖励模型中并采用专门的策略优化，成功提高了多个人身份保持视频生成的一致性。研究结果表明，这种方法在保持多人一致性的视频生成方面有着显著的效果，并为个性化视频生成提供有用见解。

**Abstract:** While advanced methods like VACE and Phantom have advanced video generation
for specific subjects in diverse scenarios, they struggle with multi-human
identity preservation in dynamic interactions, where consistent identities
across multiple characters are critical. To address this, we propose
Identity-GRPO, a human feedback-driven optimization pipeline for refining
multi-human identity-preserving video generation. First, we construct a video
reward model trained on a large-scale preference dataset containing
human-annotated and synthetic distortion data, with pairwise annotations
focused on maintaining human consistency throughout the video. We then employ a
GRPO variant tailored for multi-human consistency, which greatly enhances both
VACE and Phantom. Through extensive ablation studies, we evaluate the impact of
annotation quality and design choices on policy optimization. Experiments show
that Identity-GRPO achieves up to 18.9% improvement in human consistency
metrics over baseline methods, offering actionable insights for aligning
reinforcement learning with personalized video generation.

</details>


### [89] [MatchAttention: Matching the Relative Positions for High-Resolution Cross-View Matching](https://arxiv.org/abs/2510.14260)
*Tingman Yan,Tao Liu,Xilian Yang,Qunfei Zhao,Zeyang Xia*

Main category: cs.CV

> The paper presents a novel attention mechanism for cross-view matching that prioritizes efficiency and accuracy.

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to solve the problem of high-resolution image matching, which is limited by the quadratic complexity and lack of explicit matching constraints in current cross-attention mechanisms.

**Method:** The paper introduces MatchAttention, which uses relative positions to determine the attention sampling center given a query, enabling continuous and differentiable sliding-window attention. This mechanism is integrated into a hierarchical decoder (MatchDecoder) designed for cross-view matching tasks. Additionally, gated cross-MatchAttention and a consistency-constrained loss are proposed to handle occlusions.

**Result:** The proposed method (MatchStereo-B and MatchStereo-T) achieves top performance in stereo matching benchmarks including Middlebury, KITTI 2012, KITTI 2015, ETH3D, and Spring flow datasets, with excellent real-time processing capabilities for high-resolution images.

**Conclusion:** The paper concludes that the proposed models provide a significant improvement in cross-view matching by offering high accuracy and low computational complexity, making real-time, high-resolution, and high-accuracy matching possible.

**Abstract:** Cross-view matching is fundamentally achieved through cross-attention
mechanisms. However, matching of high-resolution images remains challenging due
to the quadratic complexity and lack of explicit matching constraints in the
existing cross-attention. This paper proposes an attention mechanism,
MatchAttention, that dynamically matches relative positions. The relative
position determines the attention sampling center of the key-value pairs given
a query. Continuous and differentiable sliding-window attention sampling is
achieved by the proposed BilinearSoftmax. The relative positions are
iteratively updated through residual connections across layers by embedding
them into the feature channels. Since the relative position is exactly the
learning target for cross-view matching, an efficient hierarchical cross-view
decoder, MatchDecoder, is designed with MatchAttention as its core component.
To handle cross-view occlusions, gated cross-MatchAttention and a
consistency-constrained loss are proposed. These two components collectively
mitigate the impact of occlusions in both forward and backward passes, allowing
the model to focus more on learning matching relationships. When applied to
stereo matching, MatchStereo-B ranked 1st in average error on the public
Middlebury benchmark and requires only 29ms for KITTI-resolution inference.
MatchStereo-T can process 4K UHD images in 0.1 seconds using only 3GB of GPU
memory. The proposed models also achieve state-of-the-art performance on KITTI
2012, KITTI 2015, ETH3D, and Spring flow datasets. The combination of high
accuracy and low computational complexity makes real-time, high-resolution, and
high-accuracy cross-view matching possible. Code is available at
https://github.com/TingmanYan/MatchAttention.

</details>


### [90] [Experimental Demonstration of Event-based Optical Camera Communication in Long-Range Outdoor Environment](https://arxiv.org/abs/2510.14266)
*Miu Sumino,Mayu Ishii,Shun Kaizu,Daisuke Hisano,Yu Nakayama*

Main category: cs.CV

> 首次报道了一种使用事件驱动视觉传感器的鲁棒解调方案，结合OOK和切换解调以及数字锁相环技术，显著降低了室外光学摄像通信系统在特定距离和波特率下的误码率。

<details>
  <summary>Details</summary>

**Motivation:** 为了提高光学摄像通信系统的解调鲁棒性，尤其是在户外环境下。

**Method:** 通过使用事件驱动视觉传感器结合OOK和切换解调以及数字锁相环的方法。

**Result:** 在户外实验中实现了在200米60kbps和400米30kbps的误码率低于10^-3的成果。

**Conclusion:** 提出了一种在户外实验中使用事件驱动视觉传感器结合OOK和切换解调以及数字锁相环的鲁棒解调方案，实现了在200米60kbps和400米30kbps的误码率低于10^-3的成果。

**Abstract:** We propose a robust demodulation scheme for optical camera communication
systems using an event-based vision sensor, combining OOK with toggle
demodulation and a digital phase-locked loop. This is the first report to
achieve a $\mathrm{BER} < 10^{-3}$ at 200m-60kbps and 400m-30kbps in outdoor
experiments.

</details>


### [91] [GauSSmart: Enhanced 3D Reconstruction through 2D Foundation Models and Geometric Filtering](https://arxiv.org/abs/2510.14270)
*Alexander Valverde,Brian Xu,Yuyin Zhou,Meng Xu,Hongyun Wang*

Main category: cs.CV

> This paper presents GauSSmart, a hybrid method that integrates 2D techniques and 3D Gaussian Splatting to improve scene reconstruction, demonstrating its advantage over existing approaches on multiple datasets.

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to address the limitations of Gaussian Splatting, which struggles to capture fine details or maintain realism in sparse coverage regions. The goal is to leverage the strengths of 2D computer vision techniques to enhance the capabilities of 3D Gaussian Splatting reconstruction.

**Method:** Our approach, named GauSSmart, integrates 2D computer vision techniques like convex filtering and semantic feature supervision from 2D foundational models (such as DINO) with 3D Gaussian Splatting reconstruction. It uses 2D segmentation priors and high-dimensional feature embeddings to guide densification and refinement of Gaussian splats, thereby enhancing coverage in underrepresented areas and preserving intricate structural details.

**Result:** We validate our method across three datasets, showing that GauSSmart outperforms existing Gaussian Splatting methods in most evaluated scenes.

**Conclusion:** The conclusion is that hybrid 2D-3D approaches have significant potential, and our work demonstrates that the integration of 2D and 3D techniques can overcome the limitations inherent to each individual approach.

**Abstract:** Scene reconstruction has emerged as a central challenge in computer vision,
with approaches such as Neural Radiance Fields (NeRF) and Gaussian Splatting
achieving remarkable progress. While Gaussian Splatting demonstrates strong
performance on large-scale datasets, it often struggles to capture fine details
or maintain realism in regions with sparse coverage, largely due to the
inherent limitations of sparse 3D training data.
  In this work, we propose GauSSmart, a hybrid method that effectively bridges
2D foundational models and 3D Gaussian Splatting reconstruction. Our approach
integrates established 2D computer vision techniques, including convex
filtering and semantic feature supervision from foundational models such as
DINO, to enhance Gaussian-based scene reconstruction. By leveraging 2D
segmentation priors and high-dimensional feature embeddings, our method guides
the densification and refinement of Gaussian splats, improving coverage in
underrepresented areas and preserving intricate structural details.
  We validate our approach across three datasets, where GauSSmart consistently
outperforms existing Gaussian Splatting in the majority of evaluated scenes.
Our results demonstrate the significant potential of hybrid 2D-3D approaches,
highlighting how the thoughtful combination of 2D foundational models with 3D
reconstruction pipelines can overcome the limitations inherent in either
approach alone.

</details>


### [92] [CLEAR: Causal Learning Framework For Robust Histopathology Tumor Detection Under Out-Of-Distribution Shifts](https://arxiv.org/abs/2510.14273)
*Kieu-Anh Truong Thi,Huy-Hieu Pham,Duc-Trong Le*

Main category: cs.CV

> 研究提出了一种基于因果推断的框架，利用语义特征减少混杂因素的影响，以缓解组织病理学领域的领域迁移问题。该方法在两个数据集上实现了相比现有方法的性能提升。

<details>
  <summary>Details</summary>

**Motivation:** 现有方法主要依赖于对齐特征分布或引入统计变化来建模统计相关性，但往往忽略因果关系。因此，本研究旨在通过因果推断解决因获取过程或数据源不同导致组织病理学领域位移的问题。

**Method:** 我们的方法基于因果推断框架，通过显式地纳入中介因素和观察到的组织切片来实施前门原则，以利用语义特征并减轻混杂因素的影响。

**Result:** 我们在CAMELYON17数据集和一个私有的组织病理学数据集上验证了我们的方法，取得了跨越未见过领域的一致性性能提升，这两个数据集上均有7%的性能提升，超过了现有基线。

**Conclusion:** 这些结果强调了因果推断作为解决组织病理学图像分析领域迁移的强大工具的潜力。

**Abstract:** Domain shift in histopathology, often caused by differences in acquisition
processes or data sources, poses a major challenge to the generalization
ability of deep learning models. Existing methods primarily rely on modeling
statistical correlations by aligning feature distributions or introducing
statistical variation, yet they often overlook causal relationships. In this
work, we propose a novel causal-inference-based framework that leverages
semantic features while mitigating the impact of confounders. Our method
implements the front-door principle by designing transformation strategies that
explicitly incorporate mediators and observed tissue slides. We validate our
method on the CAMELYON17 dataset and a private histopathology dataset,
demonstrating consistent performance gains across unseen domains. As a result,
our approach achieved up to a 7% improvement in both the CAMELYON17 dataset and
the private histopathology dataset, outperforming existing baselines. These
results highlight the potential of causal inference as a powerful tool for
addressing domain shift in histopathology image analysis.

</details>


### [93] [Watermarking for Factuality: Guiding Vision-Language Models Toward Truth via Tri-layer Contrastive Decoding](https://arxiv.org/abs/2510.14304)
*Kyungryul Back,Seongbeom Park,Milim Kim,Mincheol Kwon,SangHyeok Lee,Hyunyoung Lee,Junhee Cho,Seunghyun Park,Jinkyu Kim*

Main category: cs.CV

> 提出了一种无需训练的三层对比解码方法，使用水印来减少LVLMs产生的幻觉，生成更合理视觉结果的输出，并在多项公共基准测试中取得了先进的表现。

<details>
  <summary>Details</summary>

**Motivation:** 尽管大型视觉-语言模型(LVLMs)在许多多模态任务中表现出色，但在某些情况下，他们仍存在产生幻觉的问题，过于依赖单一模态或在训练数据上记忆而不是合理生成输出。为解决这个问题而提出的方法。

**Method:** 我们提出了一种无需训练的三层对比解码方法，该方法通过使用水印对比解码来生成最终输出，共包括三个步骤：(1) 从解码层中选择一个成熟的层和一个不成熟的层，(2) 使用与水印相关的问题来识别一个中转层，并判断该层是否视觉上合理，(3) 应用三层对比解码产生最终输出。

**Result:** 实验结果表明，使用我们的方法在减少幻觉方面作为LVLMs能够达到最先进的性能，并产生更具视觉合理性的响应。

**Conclusion:** 所提出的方法在公开基准如POPE，MME和AMBER上的测试显示，能够显著减少大型视觉-语言模型的幻觉问题，并生成更加合理的视觉产出。

**Abstract:** Large Vision-Language Models (LVLMs) have recently shown promising results on
various multimodal tasks, even achieving human-comparable performance in
certain cases. Nevertheless, LVLMs remain prone to hallucinations -- they often
rely heavily on a single modality or memorize training data without properly
grounding their outputs. To address this, we propose a training-free, tri-layer
contrastive decoding with watermarking, which proceeds in three steps: (1)
select a mature layer and an amateur layer among the decoding layers, (2)
identify a pivot layer using a watermark-related question to assess whether the
layer is visually well-grounded, and (3) apply tri-layer contrastive decoding
to generate the final output. Experiments on public benchmarks such as POPE,
MME and AMBER demonstrate that our method achieves state-of-the-art performance
in reducing hallucinations in LVLMs and generates more visually grounded
responses.

</details>


### [94] [A Multi-domain Image Translative Diffusion StyleGAN for Iris Presentation Attack Detection](https://arxiv.org/abs/2510.14314)
*Shivangi Yadav,Arun Ross*

Main category: cs.CV

> 本文介绍了一种新的深度学习框架MID-StyleGAN，用于生成高质量的眼部和人工攻击（PA）合成图像，以提高虹膜生物识别系统中的人工攻击检测（PAD）性能。这种方法不仅解决了数据稀缺问题，还显著提高了检测性能。

<details>
  <summary>Details</summary>

**Motivation:** 由于构建和拍摄人工攻击样本的难度大，使得虹膜生物识别系统中的人工攻击检测（PAD）训练和评估数据集稀缺，因此研究提出了MID-StyleGAN来生成高质量的合成样本，以缓解这一问题。

**Method:** 论文采用了扩散模型和生成对抗网络（GAN）结合的MID-StyleGAN框架，其多领域架构可以实现真实眼部图像与不同人工攻击领域的转换，采用了专门针对眼部数据自适应损失函数以保持领域一致性。

**Result:** 该论文提出了一种新的框架MID-StyleGAN，结合扩散模型和生成对抗网络（GAN）生成多领域（真实、打印眼睛和美容隐形眼镜）的眼部图像，以解决虹膜生物识别系统中存在的人工攻击检测（PAD）数据集缺乏问题。这种方法不仅生成了高质量的合成图像，还通过实验表明，使用这些数据能够显著提高PAD系统的性能，在LivDet2020数据集上的真正检测率提高了5.31%。

**Conclusion:** 研究展示了使用MID-StyleGAN生成的数据可以有效解决虹膜和眼生物识别领域中的人工攻击检测数据稀缺问题，从而显著提高了PAD系统的性能。

**Abstract:** An iris biometric system can be compromised by presentation attacks (PAs)
where artifacts such as artificial eyes, printed eye images, or cosmetic
contact lenses are presented to the system. To counteract this, several
presentation attack detection (PAD) methods have been developed. However, there
is a scarcity of datasets for training and evaluating iris PAD techniques due
to the implicit difficulties in constructing and imaging PAs. To address this,
we introduce the Multi-domain Image Translative Diffusion StyleGAN
(MID-StyleGAN), a new framework for generating synthetic ocular images that
captures the PA and bonafide characteristics in multiple domains such as
bonafide, printed eyes and cosmetic contact lens. MID-StyleGAN combines the
strengths of diffusion models and generative adversarial networks (GANs) to
produce realistic and diverse synthetic data. Our approach utilizes a
multi-domain architecture that enables the translation between bonafide ocular
images and different PA domains. The model employs an adaptive loss function
tailored for ocular data to maintain domain consistency. Extensive experiments
demonstrate that MID-StyleGAN outperforms existing methods in generating
high-quality synthetic ocular images. The generated data was used to
significantly enhance the performance of PAD systems, providing a scalable
solution to the data scarcity problem in iris and ocular biometrics. For
example, on the LivDet2020 dataset, the true detect rate at 1% false detect
rate improved from 93.41% to 98.72%, showcasing the impact of the proposed
method.

</details>


### [95] [Vision-Centric Activation and Coordination for Multimodal Large Language Models](https://arxiv.org/abs/2510.14349)
*Yunnan Wang,Fan Lu,Kecheng Zheng,Ziyuan Huang,Ziqiang Li,Wenjun Zeng,Xin Jin*

Main category: cs.CV

> 提出VaCo，通过多视觉基础模型中的视觉感知特征的视觉辨别对齐，优化多模态大语言模型的视觉感知能力，从而提高其在不同基准测试中的表现能力。

<details>
  <summary>Details</summary>

**Motivation:** 解决主流多模态大语言模型仅依赖文本信息忽视视觉关键信息的问题。

**Method:** 引入视觉辨别对齐，结合可学习的模块任务查询（MTQs）和视觉对齐层（VALs），并使用标记网关屏蔽（TGM）协调多个视觉基础模型之间的表示冲突。

**Result:** 实验结果显示，VaCo在不同基准测试中显著提高了多模态语言模型的性能。

**Conclusion:** 表明VaCo能有效提升多模态大语言模型的视觉理解能力。

**Abstract:** Multimodal large language models (MLLMs) integrate image features from visual
encoders with LLMs, demonstrating advanced comprehension capabilities. However,
mainstream MLLMs are solely supervised by the next-token prediction of textual
tokens, neglecting critical vision-centric information essential for analytical
abilities. To track this dilemma, we introduce VaCo, which optimizes MLLM
representations through Vision-Centric activation and Coordination from
multiple vision foundation models (VFMs). VaCo introduces visual discriminative
alignment to integrate task-aware perceptual features extracted from VFMs,
thereby unifying the optimization of both textual and visual outputs in MLLMs.
Specifically, we incorporate the learnable Modular Task Queries (MTQs) and
Visual Alignment Layers (VALs) into MLLMs, activating specific visual signals
under the supervision of diverse VFMs. To coordinate representation conflicts
across VFMs, the crafted Token Gateway Mask (TGM) restricts the information
flow among multiple groups of MTQs. Extensive experiments demonstrate that VaCo
significantly improves the performance of different MLLMs on various
benchmarks, showcasing its superior capabilities in visual comprehension.

</details>


### [96] [Leveraging Cycle-Consistent Anchor Points for Self-Supervised RGB-D Registration](https://arxiv.org/abs/2510.14354)
*Siddharth Tourani,Jayaram Reddy,Sarvesh Thakur,K Madhava Krishna,Muhammad Haris Khan,N Dinesh Reddy*

Main category: cs.CV

> Error

<details>
  <summary>Details</summary>

**Motivation:** Error

**Method:** Error

**Result:** Error

**Conclusion:** Error

**Abstract:** With the rise in consumer depth cameras, a wealth of unlabeled RGB-D data has
become available. This prompts the question of how to utilize this data for
geometric reasoning of scenes. While many RGB-D registration meth- ods rely on
geometric and feature-based similarity, we take a different approach. We use
cycle-consistent keypoints as salient points to enforce spatial coherence
constraints during matching, improving correspondence accuracy. Additionally,
we introduce a novel pose block that combines a GRU recurrent unit with
transformation synchronization, blending historical and multi-view data. Our
approach surpasses previous self- supervised registration methods on ScanNet
and 3DMatch, even outperforming some older supervised methods. We also
integrate our components into existing methods, showing their effectiveness.

</details>


### [97] [Spatial Preference Rewarding for MLLMs Spatial Understanding](https://arxiv.org/abs/2510.14374)
*Han Qiu,Peng Gao,Lewei Lu,Xiaoqin Zhang,Ling Shao,Shijian Lu*

Main category: cs.CV

> The paper introduces SPR, a new rewarding method that enhances multimodal language models' fine-grained spatial understanding by directly optimizing for detailed and accurate object localization in response to user needs.

<details>
  <summary>Details</summary>

**Motivation:** The motivation for this paper is to address the limitations of MLLMs in fine-grained spatial understanding, especially in providing detailed region descriptions and accurate object localization.

**Method:** Spatial Preference Rewarding (SPR) approach rewards MLLMs for detailed and accurately localized responses, using semantic and localization scores to evaluate and refine the descriptions.

**Result:** Experiments over standard benchmarks demonstrated that the SPR approach effectively enhances MLLM spatial understanding capabilities with minimal training overhead.

**Conclusion:** The SPR method improves MLLMs' spatial perception abilities by directly optimizing the responses for spatial detail and localization accuracy.

**Abstract:** Multimodal large language models~(MLLMs) have demonstrated promising spatial
understanding capabilities, such as referencing and grounding object
descriptions. Despite their successes, MLLMs still fall short in fine-grained
spatial perception abilities, such as generating detailed region descriptions
or accurately localizing objects. Additionally, they often fail to respond to
the user's requirements for desired fine-grained spatial understanding. This
issue might arise because existing approaches primarily focus on tuning MLLMs
to model pre-annotated instruction data to inject spatial knowledge, without
direct supervision of MLLMs' actual responses. We address this issue by SPR, a
Spatial Preference Rewarding~(SPR) approach that enhances MLLMs' spatial
capabilities by rewarding MLLMs' detailed responses with precise object
localization over vague or inaccurate responses. With randomly selected image
regions and region descriptions from MLLMs, SPR introduces semantic and
localization scores to comprehensively evaluate the text quality and
localization quality in MLLM-generated descriptions. We also refine the MLLM
descriptions with better localization accuracy and pair the best-scored
refinement with the initial descriptions of the lowest score for direct
preference optimization, thereby enhancing fine-grained alignment with visual
input. Extensive experiments over standard referring and grounding benchmarks
show that SPR improves MLLM spatial understanding capabilities effectively with
minimal overhead in training. Data and code will be released at
https://github.com/hanqiu-hq/SPR

</details>


### [98] [DOS: Directional Object Separation in Text Embeddings for Multi-Object Image Generation](https://arxiv.org/abs/2510.14376)
*Dongnam Byun,Jungwon Park,Jumgmin Ko,Changin Choi,Wonjong Rhee*

Main category: cs.CV

> 文章提出DOS方法，通过修改CLIP文本嵌入来改善多对象图像生成，实验证明它的成功率更高，且在人工评估中胜过四种竞争方法。

<details>
  <summary>Details</summary>

**Motivation:** 研究发现文本到图像生成模型在处理涉及多个对象的提示时存在困难，如对象忽略或对象混合。通过观察CLIP嵌入的两个关键点，动机是提出一种改进方法。

**Method:** 通过修改在文本到图像模型中使用的CLIP文本嵌入的三种类型，提出了DOS（方向性对象分离）方法，解决了多对象生成中的问题。

**Result:** 实验结果显示DOS在多对象图像生成中提高了成功率，减少了对象混合。在四个基准测试中，DOS在人工评估中获得的投票比四种竞争方法高出26.24%-43.04%。

**Conclusion:** DOS是一种实用且有效的方法，用于提高多对象图像生成的质量。

**Abstract:** Recent progress in text-to-image (T2I) generative models has led to
significant improvements in generating high-quality images aligned with text
prompts. However, these models still struggle with prompts involving multiple
objects, often resulting in object neglect or object mixing. Through extensive
studies, we identify four problematic scenarios, Similar Shapes, Similar
Textures, Dissimilar Background Biases, and Many Objects, where inter-object
relationships frequently lead to such failures. Motivated by two key
observations about CLIP embeddings, we propose DOS (Directional Object
Separation), a method that modifies three types of CLIP text embeddings before
passing them into text-to-image models. Experimental results show that DOS
consistently improves the success rate of multi-object image generation and
reduces object mixing. In human evaluations, DOS significantly outperforms four
competing methods, receiving 26.24%-43.04% more votes across four benchmarks.
These results highlight DOS as a practical and effective solution for improving
multi-object image generation.

</details>


### [99] [DRBD-Mamba for Robust and Efficient Brain Tumor Segmentation with Analytical Insights](https://arxiv.org/abs/2510.14383)
*Danish Ali,Ajmal Mian,Naveed Akhtar,Ghulam Mubashar Hassan*

Main category: cs.CV

> 本研究提出了DRBD-Mamba模型来提高脑肿瘤分割的准确性和效率，通过使用空间填充曲线和门控融合模块，该模型在计算效率和分割准确性上显著优于现有方法。

<details>
  <summary>Details</summary>

**Motivation:** 现有的基于Mamba的State Space Models在脑肿瘤分割中表现出色，但计算成本高且在BraTS数据集的不同分割上的鲁棒性未得到充分验证。

**Method:** 提出了一种新的双分辨率双向Mamba模型（DRBD-Mamba），并引入空间填充曲线以减少高成本的多轴特征扫描依赖，同时通过门控融合模块和量化块来增强特征表示。

**Result:** 该模型在分割准确性上都有所提高，特别是在BraTS2023的系统性分析中表现出了较强的鲁棒性和计算效率，比现有最佳方法提高了15倍的效率。

**Conclusion:** DRBD-Mamba模型在保持分割准确性的同时，提高了计算效率，显示出其在脑肿瘤分割应用中的优势。

**Abstract:** Accurate brain tumor segmentation is significant for clinical diagnosis and
treatment. It is challenging due to the heterogeneity of tumor subregions.
Mamba-based State Space Models have demonstrated promising performance.
However, they incur significant computational overhead due to sequential
feature computation across multiple spatial axes. Moreover, their robustness
across diverse BraTS data partitions remains largely unexplored, leaving a
critical gap in reliable evaluation. To address these limitations, we propose
dual-resolution bi-directional Mamba (DRBD-Mamba), an efficient 3D segmentation
model that captures multi-scale long-range dependencies with minimal
computational overhead. We leverage a space-filling curve to preserve spatial
locality during 3D-to-1D feature mapping, thereby reducing reliance on
computationally expensive multi-axial feature scans. To enrich feature
representation, we propose a gated fusion module that adaptively integrates
forward and reverse contexts, along with a quantization block that discretizes
features to improve robustness. In addition, we propose five systematic folds
on BraTS2023 for rigorous evaluation of segmentation techniques under diverse
conditions and present detailed analysis of common failure scenarios. On the
20\% test set used by recent methods, our model achieves Dice improvements of
0.10\% for whole tumor, 1.75\% for tumor core, and 0.93\% for enhancing tumor.
Evaluations on the proposed systematic five folds demonstrate that our model
maintains competitive whole tumor accuracy while achieving clear average Dice
gains of 0.86\% for tumor core and 1.45\% for enhancing tumor over existing
state-of-the-art. Furthermore, our model attains 15 times improvement in
efficiency while maintaining high segmentation accuracy, highlighting its
robustness and computational advantage over existing approaches.

</details>


### [100] [BoardVision: Deployment-ready and Robust Motherboard Defect Detection with YOLO+Faster-RCNN Ensemble](https://arxiv.org/abs/2510.14389)
*Brandon Hill,Kma Solaiman*

Main category: cs.CV

> 本论文介绍了BoardVision，一个用于检测主板组装缺陷的可重现框架，通过对比YOLOv7和Faster R-CNN的性能，提出了一种名为Confidence-Temporal Voting的轻量级集成方法，并评估了其在现实条件下，例如亮度和角度变化时的鲁棒性。

<details>
  <summary>Details</summary>

**Motivation:** 动机在于解决主板组装级缺陷检测的挑战，提供更可靠的高产量电子产品制造保障，而且现有的PCB检测大多集中在裸板或轨道级，对组装级缺陷检测的研究较少。

**Method:** 方法包括使用BoardVision框架，对比了YOLOv7和Faster R-CNN在MiracleFactory主板数据集上的表现，并提出了一种通过可解释规则平衡精确度和召回率的Confidence-Temporal Voting方法。

**Result:** 结果显示，通过集成方法，可以在保持高精确度的同时，增加召回率，同时还展示了在现实环境中的鲁棒性，尤其是在亮度、角度变化中稳定性。

**Conclusion:** 结论是，通过这些贡献，包括基准测试、提出的集成方案以及发布的GUI驱动检测工具，演示了如何将计算机视觉技术从基准测试过渡到实际的装配级主板制造的质量保证中。

**Abstract:** Motherboard defect detection is critical for ensuring reliability in
high-volume electronics manufacturing. While prior research in PCB inspection
has largely targeted bare-board or trace-level defects, assembly-level
inspection of full motherboards inspection remains underexplored. In this work,
we present BoardVision, a reproducible framework for detecting assembly-level
defects such as missing screws, loose fan wiring, and surface scratches. We
benchmark two representative detectors - YOLOv7 and Faster R-CNN, under
controlled conditions on the MiracleFactory motherboard dataset, providing the
first systematic comparison in this domain. To mitigate the limitations of
single models, where YOLO excels in precision but underperforms in recall and
Faster R-CNN shows the reverse, we propose a lightweight ensemble,
Confidence-Temporal Voting (CTV Voter), that balances precision and recall
through interpretable rules. We further evaluate robustness under realistic
perturbations including sharpness, brightness, and orientation changes,
highlighting stability challenges often overlooked in motherboard defect
detection. Finally, we release a deployable GUI-driven inspection tool that
bridges research evaluation with operator usability. Together, these
contributions demonstrate how computer vision techniques can transition from
benchmark results to practical quality assurance for assembly-level motherboard
manufacturing.

</details>


### [101] [DCMIL: A Progressive Representation Learning Model of Whole Slide Images for Cancer Prognosis Analysis](https://arxiv.org/abs/2510.14403)
*Chao Tu,Kun Huang,Jie Zhang,Qianjin Feng,Yu Zhang,Zhenyuan Ning*

Main category: cs.CV

> 研究提出了一种新的计算病理学算法DCMIL，该算法能够在没有大量标注的情况下处理数十亿像素的全幻灯片图像，以进行癌症预后。实验显示，该算法取得了优于传统模型的结果，并能提供预后相关的细粒度信息。

<details>
  <summary>Details</summary>

**Motivation:** 计算病理学作为一个蓬勃发展的学科，旨在利用全幻灯片图像（WSIs）量化形态异质性并为人类癌症开发客观的预后模式。然而，由于计算瓶颈和密集的手动注释稀缺，进展受到了阻碍。现有方法往往忽略了跨多倍放大WSIs的细粒度信息和肿瘤微环境的变化。

**Method:** 提出了一种称为双课程对比多实例学习（DCMIL）的易难渐进表示学习模型，该模型能直接将数十亿像素的全幻灯片图像转换为预后预测，不需要密集的标注。

**Result:** 在12种癌症类型（5954名患者，1254万块图像）上的广泛实验表明，DCMIL优于标准的基于WSI的预后模型。此外，DCMIL还可以识别出预后相关的微区域，并提供了稳健的实例不确定性估计，同时捕获正常和肿瘤组织之间的形态差异，有可能生成新的生物见解。

**Conclusion:** DCMIL模型解决了现有技术的缺点，能有效处理全幻灯片图像以用于癌症预后判断，并且显示出优于传统WSI预后模型的性能。

**Abstract:** The burgeoning discipline of computational pathology shows promise in
harnessing whole slide images (WSIs) to quantify morphological heterogeneity
and develop objective prognostic modes for human cancers. However, progress is
impeded by the computational bottleneck of gigapixel-size inputs and the
scarcity of dense manual annotations. Current methods often overlook
fine-grained information across multi-magnification WSIs and variations in
tumor microenvironments. Here, we propose an easy-to-hard progressive
representation learning model, termed dual-curriculum contrastive
multi-instance learning (DCMIL), to efficiently process WSIs for cancer
prognosis. The model does not rely on dense annotations and enables the direct
transformation of gigapixel-size WSIs into outcome predictions. Extensive
experiments on twelve cancer types (5,954 patients, 12.54 million tiles)
demonstrate that DCMIL outperforms standard WSI-based prognostic models.
Additionally, DCMIL identifies fine-grained prognosis-salient regions, provides
robust instance uncertainty estimation, and captures morphological differences
between normal and tumor tissues, with the potential to generate new biological
insights. All codes have been made publicly accessible at
https://github.com/tuuuc/DCMIL.

</details>


### [102] [Real-Time Neural Video Compression with Unified Intra and Inter Coding](https://arxiv.org/abs/2510.14431)
*Hui Xiang,Yifan Bian,Li Li,Jingran Wu,Xianguo Zhang,Dong Liu*

Main category: cs.CV

> Develops an advanced NVC framework that integrates intra and inter coding, improving on current limitations in NVC, achieving better compression efficiency and stability.

<details>
  <summary>Details</summary>

**Motivation:** To overcome the limitations of existing NVC schemes, including issues with disocclusion, new content, and interframe error propagation.

**Method:** The paper proposes a new NVC framework combining intra and inter coding within a single model, which adaptively performs intra/inter coding. It also introduces a simultaneous two-frame compression design to enhance interframe redundancy handling.

**Result:** Experimental results demonstrate an average of 10.7% BD-rate reduction compared to DCVC-RT, with more stable bitrate and quality per frame, and maintains real-time encoding/decoding performance.

**Conclusion:** The proposed unified intra and inter coding framework for NVC, along with the simultaneous two-frame compression method, effectively improves compression efficiency and stability compared to existing NVC technologies.

**Abstract:** Neural video compression (NVC) technologies have advanced rapidly in recent
years, yielding state-of-the-art schemes such as DCVC-RT that offer superior
compression efficiency to H.266/VVC and real-time encoding/decoding
capabilities. Nonetheless, existing NVC schemes have several limitations,
including inefficiency in dealing with disocclusion and new content, interframe
error propagation and accumulation, among others. To eliminate these
limitations, we borrow the idea from classic video coding schemes, which allow
intra coding within inter-coded frames. With the intra coding tool enabled,
disocclusion and new content are properly handled, and interframe error
propagation is naturally intercepted without the need for manual refresh
mechanisms. We present an NVC framework with unified intra and inter coding,
where every frame is processed by a single model that is trained to perform
intra/inter coding adaptively. Moreover, we propose a simultaneous two-frame
compression design to exploit interframe redundancy not only forwardly but also
backwardly. Experimental results show that our scheme outperforms DCVC-RT by an
average of 10.7\% BD-rate reduction, delivers more stable bitrate and quality
per frame, and retains real-time encoding/decoding performances. Code and
models will be released.

</details>


### [103] [Structured Universal Adversarial Attacks on Object Detection for Video Sequences](https://arxiv.org/abs/2510.14460)
*Sven Jacob,Weijia Shao,Gjergji Kasneci*

Main category: cs.CV

> 本研究提出了一种用于视频对象检测的最小失真通用对抗攻击方法，该方法使用了核范数正则化和自适应乐观指数梯度法，以提高攻击效果并保持较高隐蔽性。

<details>
  <summary>Details</summary>

**Motivation:** 虽然基于深度学习的对象检测器已经实现了令人印象深刻的表现，但它们仍然容易受到对抗性攻击的影响，特别是涉及通用扰动的攻击。我们的目标是提高在视频对象检测中的对抗性攻击的效果和隐蔽性。

**Method:** 我们提出了一种针对视频对象检测的最小失真通用对抗攻击方法，这种方法利用核范数正则化来促进集中在背景上的结构化扰动。为了有效地优化这一公式，我们采用了一种自适应、乐观的指数梯度法，这种方法增强了可扩展性和收敛性。

**Result:** 我们的实验结果表明，所提出的攻击方法在有效性方面优于低秩投影梯度下降和基于Frank-Wolfe的攻击方法，同时保持了高度的隐蔽性。

**Conclusion:** 该研究展示了如何通过引入结构化扰动以及优化方法来提高视频对象检测中对抗性攻击的有效性和隐蔽性。

**Abstract:** Video-based object detection plays a vital role in safety-critical
applications. While deep learning-based object detectors have achieved
impressive performance, they remain vulnerable to adversarial attacks,
particularly those involving universal perturbations. In this work, we propose
a minimally distorted universal adversarial attack tailored for video object
detection, which leverages nuclear norm regularization to promote structured
perturbations concentrated in the background. To optimize this formulation
efficiently, we employ an adaptive, optimistic exponentiated gradient method
that enhances both scalability and convergence. Our results demonstrate that
the proposed attack outperforms both low-rank projected gradient descent and
Frank-Wolfe based attacks in effectiveness while maintaining high stealthiness.
All code and data are publicly available at
https://github.com/jsve96/AO-Exp-Attack.

</details>


### [104] [Unsupervised Deep Generative Models for Anomaly Detection in Neuroimaging: A Systematic Scoping Review](https://arxiv.org/abs/2510.14462)
*Youwan Mahé,Elise Bannier,Stéphanie Leplaideur,Elisa Fromont,Francesca Galassi*

Main category: cs.CV

> 该研究综述了无监督深度生成模型在脑部影像异常检测中的应用，这些模型从健康数据中学习并识别异常，适用于罕见或异质性疾病。已发表的49项研究涵盖了使用生成对抗网络、变分自编码器等多种模型，整体结果令人鼓舞。未来研究需注重解剖学感知建模及临床验证。

<details>
  <summary>Details</summary>

**Motivation:** 文章旨在概述无监督深度生成模型在脑部影像异常检测中的最新研究进展，解决监督方法对标注数据量需求大和局限性问题。

**Method:** 通过对2018-2025年间发表的49篇相关研究的文章进行综合分析，包括不同类型的生成模型（变分自编码器、生成对抗网络、扩散模型）在MRI和CT影像中的应用。

**Result:** 综述发现生成模型在大型明显病变检测方面表现良好，并逐步提升对细微异常的检测能力。模型能生成可解释的伪健康重建图像，这对稀缺标注数据的情况下尤其重要。

**Conclusion:** 无监督深度生成模型在未来脑部异常检测中具有潜力，可支持半监督学习、新型影像生物标志物的发现及疾病偏离度的映射。未来需加强解剖学感知建模、基础模型的开发、适当的任务评估指标和严格的临床验证。

**Abstract:** Unsupervised deep generative models are emerging as a promising alternative
to supervised methods for detecting and segmenting anomalies in brain imaging.
Unlike fully supervised approaches, which require large voxel-level annotated
datasets and are limited to well-characterised pathologies, these models can be
trained exclusively on healthy data and identify anomalies as deviations from
learned normative brain structures. This PRISMA-guided scoping review
synthesises recent work on unsupervised deep generative models for anomaly
detection in neuroimaging, including autoencoders, variational autoencoders,
generative adversarial networks, and denoising diffusion models. A total of 49
studies published between 2018 - 2025 were identified, covering applications to
brain MRI and, less frequently, CT across diverse pathologies such as tumours,
stroke, multiple sclerosis, and small vessel disease. Reported performance
metrics are compared alongside architectural design choices. Across the
included studies, generative models achieved encouraging performance for large
focal lesions and demonstrated progress in addressing more subtle
abnormalities. A key strength of generative models is their ability to produce
interpretable pseudo-healthy (also referred to as counterfactual)
reconstructions, which is particularly valuable when annotated data are scarce,
as in rare or heterogeneous diseases. Looking ahead, these models offer a
compelling direction for anomaly detection, enabling semi-supervised learning,
supporting the discovery of novel imaging biomarkers, and facilitating within-
and cross-disease deviation mapping in unified end-to-end frameworks. To
realise clinical impact, future work should prioritise anatomy-aware modelling,
development of foundation models, task-appropriate evaluation metrics, and
rigorous clinical validation.

</details>


### [105] [Pruning Overparameterized Multi-Task Networks for Degraded Web Image Restoration](https://arxiv.org/abs/2510.14463)
*Thomas Katraouras,Dimitrios Rafailidis*

Main category: cs.CV

> 本文介绍了MIR-L模型，通过迭代剪枝策略实现了高效多任务图像恢复。

<details>
  <summary>Details</summary>

**Motivation:** 在线社交网络应用的有损操作常常导致图像质量退化，影响用户体验。虽然多任务图像恢复模型因其能够同时处理不同类型的图像退化而受到关注，但是这些模型往往参数过多，难以应用。因此，本文旨在解决模型的计算效率问题。

**Method:** 本文提出了一种多任务图像恢复模型压缩策略，名叫MIR-L。该模型使用迭代剪枝策略，通过多轮去除低幅度权重，并重置剩余权重到初始状态，从而发现稀疏子网络，这些子网络在高稀疏度下也能维持或超过现有最佳性能。

**Result:** 实验验证表明，在去雨、去雾、降噪任务的基准数据集上，MIR-L仅保留10%的模型参数，但仍然能保持高质量的图像恢复性能。

**Conclusion:** 实验结果表明，该方法在保证高质量图像恢复的同时，显著减少了计算量，提高了模型的实用性。代码，数据集和预训练模型已公开。

**Abstract:** Image quality is a critical factor in delivering visually appealing content
on web platforms. However, images often suffer from degradation due to lossy
operations applied by online social networks (OSNs), negatively affecting user
experience. Image restoration is the process of recovering a clean high-quality
image from a given degraded input. Recently, multi-task (all-in-one) image
restoration models have gained significant attention, due to their ability to
simultaneously handle different types of image degradations. However, these
models often come with an excessively high number of trainable parameters,
making them computationally inefficient. In this paper, we propose a strategy
for compressing multi-task image restoration models. We aim to discover highly
sparse subnetworks within overparameterized deep models that can match or even
surpass the performance of their dense counterparts. The proposed model, namely
MIR-L, utilizes an iterative pruning strategy that removes low-magnitude
weights across multiple rounds, while resetting the remaining weights to their
original initialization. This iterative process is important for the multi-task
image restoration model's optimization, effectively uncovering "winning
tickets" that maintain or exceed state-of-the-art performance at high sparsity
levels. Experimental evaluation on benchmark datasets for the deraining,
dehazing, and denoising tasks shows that MIR-L retains only 10% of the
trainable parameters while maintaining high image restoration performance. Our
code, datasets and pre-trained models are made publicly available at
https://github.com/Thomkat/MIR-L.

</details>


### [106] [Grazing Detection using Deep Learning and Sentinel-2 Time Series Data](https://arxiv.org/abs/2510.14493)
*Aleksis Pirinen,Delia Fano Yela,Smita Chakraborty,Erik Källman*

Main category: cs.CV

> 本研究利用CNN-LSTM模型和Sentinel-2卫星数据有效地区分过去几个月中的放牧和非放牧区域，有助于保护目标的土地使用监控。

<details>
  <summary>Details</summary>

**Motivation:** 研究旨在利用可扩展的方法监测放牧活动，同时支持农业生产和生物多样性保护，克服现有监测手段的局限。

**Method:** 使用Sentinel-2 L2A遥感数据的时间序列进行季节性放牧检测，通过训练CNN-LSTM模型来预测每块定义好的边界范围内是否发生了放牧行为。

**Result:** 该研究利用Sentinel-2 L2A时间序列遥感数据，通过训练卷积神经网络-长短期记忆（CNN-LSTM）模型来检测季节性放牧情况。模型在多重时间反射率特征上训练，实现了平均F1分数达到77%的准确率。在实际操作中，若土地管理者每年只能访问4%的区域，优先检查模型预测为未放牧的地区会比随机检查多出17.2倍的确切未放牧地点。这表明低成本的卫星数据可以有效地优化资源，以符合保护目标的土地使用监管。研究代码和模型已公开。

**Conclusion:** 结果表明，使用卫星数据可以可靠地指导资源，以优化保护性土地使用合规性，且研究结果已公开，便于大众使用和进一步研究。

**Abstract:** Grazing shapes both agricultural production and biodiversity, yet scalable
monitoring of where grazing occurs remains limited. We study seasonal grazing
detection from Sentinel-2 L2A time series: for each polygon-defined field
boundary, April-October imagery is used for binary prediction (grazed / not
grazed). We train an ensemble of CNN-LSTM models on multi-temporal reflectance
features, and achieve an average F1 score of 77 percent across five validation
splits, with 90 percent recall on grazed pastures. Operationally, if inspectors
can visit at most 4 percent of sites annually, prioritising fields predicted by
our model as non-grazed yields 17.2 times more confirmed non-grazing sites than
random inspection. These results indicate that coarse-resolution, freely
available satellite data can reliably steer inspection resources for
conservation-aligned land-use compliance. Code and models have been made
publicly available.

</details>


### [107] [Vision Mamba for Permeability Prediction of Porous Media](https://arxiv.org/abs/2510.14516)
*Ali Kashefi,Tapan Mukerji*

Main category: cs.CV

> 本文提出并分析了使用Vision Mamba架构预测三维多孔介质渗透率的方法，并展示了相比于传统ViTs和CNN模型，Vision Mamba的优越性能。

<details>
  <summary>Details</summary>

**Motivation:** 鉴于Vision Mamba在网络规模、计算和内存效率方面优于ViTs和CNN，并且参数量较少，本文探索了其在三维多孔介质渗透率预测中的应用潜力。

**Method:** 本文提出了首次使用Vision Mamba作为骨干网络来预测三维多孔介质渗透率的神经网络框架。并通过将Vision Mamba与ViT和CNN模型在多个方面进行性能比较，进行了消融研究以评估其各个组件对准确性的影响。

**Result:** 实验证明了Vision Mamba在三维多孔介质渗透率预测中相比ViTs和CNN具有明显优势。

**Conclusion:** 该研究提出的框架展示了Vision Mamba在三维多孔介质渗透率预测中的优越表现，并具备集成到大型视觉模型中的潜力。

**Abstract:** Vision Mamba has recently received attention as an alternative to Vision
Transformers (ViTs) for image classification. The network size of Vision Mamba
scales linearly with input image resolution, whereas ViTs scale quadratically,
a feature that improves computational and memory efficiency. Moreover, Vision
Mamba requires a significantly smaller number of trainable parameters than
traditional convolutional neural networks (CNNs), and thus, they can be more
memory efficient. Because of these features, we introduce, for the first time,
a neural network that uses Vision Mamba as its backbone for predicting the
permeability of three-dimensional porous media. We compare the performance of
Vision Mamba with ViT and CNN models across multiple aspects of permeability
prediction and perform an ablation study to assess the effects of its
components on accuracy. We demonstrate in practice the aforementioned
advantages of Vision Mamba over ViTs and CNNs in the permeability prediction of
three-dimensional porous media. We make the source code publicly available to
facilitate reproducibility and to enable other researchers to build on and
extend this work. We believe the proposed framework has the potential to be
integrated into large vision models in which Vision Mamba is used instead of
ViTs.

</details>


### [108] [Real-Time Surgical Instrument Defect Detection via Non-Destructive Testing](https://arxiv.org/abs/2510.14525)
*Qurrat Ul Ain,Atif Aftab Ahmed Jilani,Zunaira Shafqat,Nigar Azhar Butt*

Main category: cs.CV

> SurgScan, an AI-powered defect detection framework, is introduced to ensure high accuracy and industrial scalability in surgical instrument inspection, offering real-time defect identification with 99.3% accuracy, suitable for industrial deployment.

<details>
  <summary>Details</summary>

**Motivation:** The motivation for this study is the risk posed by defective surgical instruments due to manual inspection, which often leads to human errors and inconsistencies.

**Method:** The method involves using YOLOv8 for real-time defect classification in surgical instruments. A high-resolution image dataset consisting of 102,876 images is used for training.

**Result:** SurgScan achieves the highest accuracy of 99.3% and can infer defects in real-time at speeds of 4.2-5.8 ms per image, which is more accurate than state-of-the-art CNN architectures.

**Conclusion:** This study concludes that SurgScan provides a scalable, cost-effective AI solution for automated quality control, reducing reliance on manual inspection and ensuring compliance with ISO 13485 and FDA standards.

**Abstract:** Defective surgical instruments pose serious risks to sterility, mechanical
integrity, and patient safety, increasing the likelihood of surgical
complications. However, quality control in surgical instrument manufacturing
often relies on manual inspection, which is prone to human error and
inconsistency. This study introduces SurgScan, an AI-powered defect detection
framework for surgical instruments. Using YOLOv8, SurgScan classifies defects
in real-time, ensuring high accuracy and industrial scalability. The model is
trained on a high-resolution dataset of 102,876 images, covering 11 instrument
types and five major defect categories. Extensive evaluation against
state-of-the-art CNN architectures confirms that SurgScan achieves the highest
accuracy (99.3%) with real-time inference speeds of 4.2-5.8 ms per image,
making it suitable for industrial deployment. Statistical analysis demonstrates
that contrast-enhanced preprocessing significantly improves defect detection,
addressing key limitations in visual inspection. SurgScan provides a scalable,
cost-effective AI solution for automated quality control, reducing reliance on
manual inspection while ensuring compliance with ISO 13485 and FDA standards,
paving the way for enhanced defect detection in medical manufacturing.

</details>


### [109] [Noise Projection: Closing the Prompt-Agnostic Gap Behind Text-to-Image Misalignment in Diffusion Models](https://arxiv.org/abs/2510.14526)
*Yunze Tong,Didi Zhu,Zijing Hu,Jinluan Yang,Ziyu Zhao*

Main category: cs.CV

> The paper introduces a noise projector to improve alignment between generated images and text prompts in text-to-image generation by refining initial noise conditions during the inference process, leading to better alignment results compared to the existing methods.

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind the paper is to address the issue of misalignment between generated images and text prompts in text-to-image generation. The authors attribute this issue to a mismatch between training and inference processes.

**Method:** Our method involves a noise projector that refines initial noise based on the prompt before the denoising process. It uses token-level feedback from a vision-language model and a reward model to optimize the projector.

**Result:** Experiments demonstrated that their prompt-aware noise projection improves text-image alignment for different prompts.

**Conclusion:** The paper concludes that their proposed noise projector can effectively align generated images with text prompts without requiring reference images or handcrafted priors, and incurs minimal inference cost.

**Abstract:** In text-to-image generation, different initial noises induce distinct
denoising paths with a pretrained Stable Diffusion (SD) model. While this
pattern could output diverse images, some of them may fail to align well with
the prompt. Existing methods alleviate this issue either by altering the
denoising dynamics or by drawing multiple noises and conducting post-selection.
In this paper, we attribute the misalignment to a training-inference mismatch:
during training, prompt-conditioned noises lie in a prompt-specific subset of
the latent space, whereas at inference the noise is drawn from a
prompt-agnostic Gaussian prior. To close this gap, we propose a noise projector
that applies text-conditioned refinement to the initial noise before denoising.
Conditioned on the prompt embedding, it maps the noise to a prompt-aware
counterpart that better matches the distribution observed during SD training,
without modifying the SD model. Our framework consists of these steps: we first
sample some noises and obtain token-level feedback for their corresponding
images from a vision-language model (VLM), then distill these signals into a
reward model, and finally optimize the noise projector via a quasi-direct
preference optimization. Our design has two benefits: (i) it requires no
reference images or handcrafted priors, and (ii) it incurs small inference
cost, replacing multi-sample selection with a single forward pass. Extensive
experiments further show that our prompt-aware noise projection improves
text-image alignment across diverse prompts.

</details>


### [110] [PaddleOCR-VL: Boosting Multilingual Document Parsing via a 0.9B Ultra-Compact Vision-Language Model](https://arxiv.org/abs/2510.14528)
*Cheng Cui,Ting Sun,Suyin Liang,Tingquan Gao,Zelun Zhang,Jiaxuan Liu,Xueqing Wang,Changda Zhou,Hongen Liu,Manhui Lin,Yue Zhang,Yubo Zhang,Handong Zheng,Jing Zhang,Jun Zhang,Yi Liu,Dianhai Yu,Yanjun Ma*

Main category: cs.CV

> 本文提出了一种名为PaddleOCR-VL的文档解析模型，它整合了视觉和语言模型的优势，实现了紧凑性、多语言支持和快速准确的元素识别能力，并在多个公共和私有数据集上验证了其有效性和先进性。

<details>
  <summary>Details</summary>

**Motivation:** 该模型旨在提供一种在保持最小资源消耗的同时，支持多种语言（109种语言）和复杂元素（如文本、表格、公式和图表）识别能力的高效解决方案。

**Method:** 本文提出了PaddleOCR-VL，这是一个用于文档解析的最新且资源高效的模型。该模型的核心部件是PaddleOCR-VL-0.9B，这是一种简约而强大的视觉语言模型（VLM），它结合了类似于NaViT的动态分辨率视觉编码器和ERNIE-4.5-0.3B语言模型以实现精确的元素识别。

**Result:** 为了验证PaddleOCR-VL的性能，作者在广泛使用的公共基准测试和内部基准测试中进行了全面评估，结果显示，PaddleOCR-VL在页面级和元素级文档解析方面均达到了最新技术水平。该模型不仅显著优于现有解决方案，而且在顶级VLM中表现出强竞争力，同时还具备快速推理速度。

**Conclusion:** 鉴于其强大的功能性、高效性和快速推理速度，PaddleOCR-VL非常适于在现实世界场景中部署使用。

**Abstract:** In this report, we propose PaddleOCR-VL, a SOTA and resource-efficient model
tailored for document parsing. Its core component is PaddleOCR-VL-0.9B, a
compact yet powerful vision-language model (VLM) that integrates a NaViT-style
dynamic resolution visual encoder with the ERNIE-4.5-0.3B language model to
enable accurate element recognition. This innovative model efficiently supports
109 languages and excels in recognizing complex elements (e.g., text, tables,
formulas, and charts), while maintaining minimal resource consumption. Through
comprehensive evaluations on widely used public benchmarks and in-house
benchmarks, PaddleOCR-VL achieves SOTA performance in both page-level document
parsing and element-level recognition. It significantly outperforms existing
solutions, exhibits strong competitiveness against top-tier VLMs, and delivers
fast inference speeds. These strengths make it highly suitable for practical
deployment in real-world scenarios.

</details>


### [111] [Towards Generalist Intelligence in Dentistry: Vision Foundation Models for Oral and Maxillofacial Radiology](https://arxiv.org/abs/2510.14532)
*Xinrui Huang,Fan Xiao,Dongming He,Anqi Gao,Dandan Li,Xiaofan Zhang,Shaoting Zhang,Xudong Wang*

Main category: cs.CV

> DentVFM解决了现有人工智能系统在牙科领域中的单模态焦点和任务特定设计的局限性，通过自我监督学习训练多个视觉基础模型，展示了出色的跨领域诊断、分析、生物标志物识别和解剖标志点检测与分割的泛化能力。

<details>
  <summary>Details</summary>

**Motivation:** 现有的牙科AI系统由于单一模式焦点、特定任务设计以及依赖昂贵标注数据，限制了它们在多种临床场景中的推广。为了克服这些挑战，提出了DentVFM，旨在提供一种可适应性和标签效率更高的模型，以提高智能牙科医疗保健和解决全球口腔医疗保健中的关键缺口。

**Method:** 介绍了DentVFM，这是第一个专为牙科设计的视觉基础模型家族，它能够生成适用于多种牙科应用的多模态放射影像的任务无关视觉表示，并采用自我监督学习方法在DentVista上进行训练，这是一个大约包含160万张多模态放射影像的精心策划牙科成像数据集。DentVFM包括基于ViT架构的2D和3D变体。

**Result:** DentVFM展示了在多种牙科任务中的强大泛化能力，包括疾病诊断、治疗分析、生物标志物识别以及解剖标志点的检测和分割。

**Conclusion:** 实验表明，DentVFM显著优于监督、自我监督和弱监督基线模型，在泛化能力、标签效率和可扩展性方面表现出色。它还能够提供跨模态诊断，其结果比经验丰富的牙医更可靠，特别是在传统成像不可用的情况下。

**Abstract:** Oral and maxillofacial radiology plays a vital role in dental healthcare, but
radiographic image interpretation is limited by a shortage of trained
professionals. While AI approaches have shown promise, existing dental AI
systems are restricted by their single-modality focus, task-specific design,
and reliance on costly labeled data, hindering their generalization across
diverse clinical scenarios. To address these challenges, we introduce DentVFM,
the first family of vision foundation models (VFMs) designed for dentistry.
DentVFM generates task-agnostic visual representations for a wide range of
dental applications and uses self-supervised learning on DentVista, a large
curated dental imaging dataset with approximately 1.6 million multi-modal
radiographic images from various medical centers. DentVFM includes 2D and 3D
variants based on the Vision Transformer (ViT) architecture. To address gaps in
dental intelligence assessment and benchmarks, we introduce DentBench, a
comprehensive benchmark covering eight dental subspecialties, more diseases,
imaging modalities, and a wide geographical distribution. DentVFM shows
impressive generalist intelligence, demonstrating robust generalization to
diverse dental tasks, such as disease diagnosis, treatment analysis, biomarker
identification, and anatomical landmark detection and segmentation.
Experimental results indicate DentVFM significantly outperforms supervised,
self-supervised, and weakly supervised baselines, offering superior
generalization, label efficiency, and scalability. Additionally, DentVFM
enables cross-modality diagnostics, providing more reliable results than
experienced dentists in situations where conventional imaging is unavailable.
DentVFM sets a new paradigm for dental AI, offering a scalable, adaptable, and
label-efficient model to improve intelligent dental healthcare and address
critical gaps in global oral healthcare.

</details>


### [112] [Acquisition of interpretable domain information during brain MR image harmonization for content-based image retrieval](https://arxiv.org/abs/2510.14535)
*Keima Abe,Hayato Muraki,Shuhei Tomoshige,Kenichi Oishi,Hitoshi Iyatomi*

Main category: cs.CV

> 提出了PL-SE-ADA模型，旨在解决医学图像跨成像站点的域偏移问题，通过可解释的表示学习实现了医学图像的域调和，显著提升了疾病的分类准确性。

<details>
  <summary>Details</summary>

**Motivation:** 医疗影像如MR扫描由于扫描器和技术方案的不同，在不同成像站点之间存在领域差异，导致机器学习在疾病分类等任务中的性能下降，因此域调和成为一个关键研究领域。现有方法虽有效但缺乏在医学应用中至关重要的解释性。

**Method:** PL-SE-ADA包括两个编码器$f_E$和$f_{SE}$用于提取领域不变特征$\boldsymbol{z_u}$和领域特定特征$\boldsymbol{z_d}$，重建器$f_D$用于重构影像，一个领域预测器$g_D$。除了编码器和领域预测器之间的对抗训练，模型通过$\boldsymbol{z_u}$和$\boldsymbol{z_d}$的重建来学习重构输入影像$\boldsymbol{x}$，确保了调和度和信息丰富度。

**Result:** 与先前方法相比，PL-SE-ADA在图像重建、疾病分类和领域识别方面取得了相当或更好的表现。

**Conclusion:** PL-SE-ADA不仅可以可视化领域独立的和领域特异性的脑部特征，还可以提供整个框架的高可解释性。

**Abstract:** Medical images like MR scans often show domain shifts across imaging sites
due to scanner and protocol differences, which degrade machine learning
performance in tasks such as disease classification. Domain harmonization is
thus a critical research focus. Recent approaches encode brain images
$\boldsymbol{x}$ into a low-dimensional latent space $\boldsymbol{z}$, then
disentangle it into $\boldsymbol{z_u}$ (domain-invariant) and
$\boldsymbol{z_d}$ (domain-specific), achieving strong results. However, these
methods often lack interpretability$-$an essential requirement in medical
applications$-$leaving practical issues unresolved. We propose
Pseudo-Linear-Style Encoder Adversarial Domain Adaptation (PL-SE-ADA), a
general framework for domain harmonization and interpretable representation
learning that preserves disease-relevant information in brain MR images.
PL-SE-ADA includes two encoders $f_E$ and $f_{SE}$ to extract
$\boldsymbol{z_u}$ and $\boldsymbol{z_d}$, a decoder to reconstruct the image
$f_D$, and a domain predictor $g_D$. Beyond adversarial training between the
encoder and domain predictor, the model learns to reconstruct the input image
$\boldsymbol{x}$ by summing reconstructions from $\boldsymbol{z_u}$ and
$\boldsymbol{z_d}$, ensuring both harmonization and informativeness. Compared
to prior methods, PL-SE-ADA achieves equal or better performance in image
reconstruction, disease classification, and domain recognition. It also enables
visualization of both domain-independent brain features and domain-specific
components, offering high interpretability across the entire framework.

</details>


### [113] [Exploring Image Representation with Decoupled Classical Visual Descriptors](https://arxiv.org/abs/2510.14536)
*Chenyuan Qu,Hao Chen,Jianbo Jiao*

Main category: cs.CV

> 研究提出了VisualSplit框架，该框架将图像分解为经典视觉描述符，通过重建驱动的预训练方案学习捕捉描述符的实质，同时保持可解释性。这有助于高级视觉任务中的属性控制。

<details>
  <summary>Details</summary>

**Motivation:** 动机源于传统视觉描述符和现代深度学习模型内部表示之间的差距，旨在探索现代学习方法是否可以从经典视觉描述符中受益。

**Method:** 通过VisualSplit框架，图像被分解成独立的经典描述符，每个描述符被当作视觉知识的独立但互补的组成部分。通过重建驱动的预训练方案，VisualSplit学习捕捉每个视觉描述符的本质同时保留其可解释性。

**Result:** VisualSplit通过显式分解视觉属性，为各种高级视觉任务（包括图像生成和编辑）提供了有效的属性控制能力。

**Conclusion:** 这种方法超越了传统的分类和分割任务，展示了VisualSplit对于视觉理解的有效性。

**Abstract:** Exploring and understanding efficient image representations is a
long-standing challenge in computer vision. While deep learning has achieved
remarkable progress across image understanding tasks, its internal
representations are often opaque, making it difficult to interpret how visual
information is processed. In contrast, classical visual descriptors (e.g. edge,
colour, and intensity distribution) have long been fundamental to image
analysis and remain intuitively understandable to humans. Motivated by this
gap, we ask a central question: Can modern learning benefit from these
classical cues? In this paper, we answer it with VisualSplit, a framework that
explicitly decomposes images into decoupled classical descriptors, treating
each as an independent but complementary component of visual knowledge. Through
a reconstruction-driven pre-training scheme, VisualSplit learns to capture the
essence of each visual descriptor while preserving their interpretability. By
explicitly decomposing visual attributes, our method inherently facilitates
effective attribute control in various advanced visual tasks, including image
generation and editing, extending beyond conventional classification and
segmentation, suggesting the effectiveness of this new learning approach for
visual understanding. Project page: https://chenyuanqu.com/VisualSplit/.

</details>


### [114] [Exploring Cross-Modal Flows for Few-Shot Learning](https://arxiv.org/abs/2510.14543)
*Ziqi Jiang,Yanghao Wang,Long Chen*

Main category: cs.CV

> The paper introduces a novel multi-step adjustment method, FMA, for parameter-efficient fine-tuning in cross-modal tasks, demonstrating superior performance on complex datasets.

<details>
  <summary>Details</summary>

**Motivation:** All existing parameter-efficient fine-tuning (PEFT) methods only perform one-step adjustment, which is insufficient for complex datasets where features of different modalities are highly entangled.

**Method:** Proposes a model-agnostic multi-step adjustment approach called Flow Matching Alignment (FMA) to improve cross-modal feature alignment. This involves a fixed coupling strategy, a noise augmentation strategy, and an early-stopping solver.

**Result:** FMA consistently yields significant performance gains across various benchmarks and backbones, particularly on challenging datasets.

**Conclusion:** Introducing multi-step rectification through FMA can achieve more precise and robust cross-modal alignment, addressing limitations of existing one-step PEFT methods.

**Abstract:** Aligning features from different modalities, is one of the most fundamental
challenges for cross-modal tasks. Although pre-trained vision-language models
can achieve a general alignment between image and text, they often require
parameter-efficient fine-tuning (PEFT) for further adjustment. Today's PEFT
methods (e.g., prompt tuning, LoRA-based, or adapter-based) always selectively
fine-tune a subset of parameters, which can slightly adjust either visual or
textual features, and avoid overfitting. In this paper, we are the first to
highlight that all existing PEFT methods perform one-step adjustment. It is
insufficient for complex (or difficult) datasets, where features of different
modalities are highly entangled. To this end, we propose the first
model-agnostic multi-step adjustment approach by learning a cross-modal
velocity field: Flow Matching Alignment (FMA). Specifically, to ensure the
correspondence between categories during training, we first utilize a fixed
coupling strategy. Then, we propose a noise augmentation strategy to alleviate
the data scarcity issue. Finally, we design an early-stopping solver, which
terminates the transformation process earlier, improving both efficiency and
accuracy. Compared with one-step PEFT methods, FMA has the multi-step
rectification ability to achieve more precise and robust alignment. Extensive
results have demonstrated that FMA can consistently yield significant
performance gains across various benchmarks and backbones, particularly on
challenging datasets.

</details>


### [115] [Consistent text-to-image generation via scene de-contextualization](https://arxiv.org/abs/2510.14553)
*Song Tang,Peihao Gong,Kunyu Li,Kai Guo,Boyu Wang,Mao Ye,Jianwei Zhang,Xiatian Zhu*

Main category: cs.CV

> 本文提出一种名为Scene De-Contextualization (SDeC)的方法解决了一致性文本到图像生成中的身份偏移问题，这种方法通过编辑身份提示的嵌入来逆向化场景上下文的自然关联，提升了身份一致性同时保持场景多样性。方法无需预训练且具有高度灵活性，适用于实际情况。

<details>
  <summary>Details</summary>

**Motivation:** 解决文本到图像生成过程中存在的身份偏移问题，尤其是在身份保持的图像生成方面，前人的方法通常依赖于不切实际的前提，即提前知道所有目标场景。本文指出场景与身份之间的上下文关联是引起身份偏移的关键原因。

**Method:** 通过量化SVD方向的稳定性来重新加权相应的特征值，从而识别并抑制身份提示嵌入中的场景-身份关联。此方法称为Scene De-Contextualization (SDeC)，是一种训练自由的提示嵌入编辑方法。

**Result:** 实验结果表明，SDeC方法显著提升了身份一致性，同时保持场景多样性。这是一种高效且灵活的解决方案，适用于实际应用中缺乏先验知识或知识随时间变动的情况。

**Conclusion:** SDeC方法解决了一致性文本到图像生成中的身份偏移问题，是一种无需提前知道所有目标场景的训练无关的提示编辑方法，对实际应用具备高度灵活性和实用性。

**Abstract:** Consistent text-to-image (T2I) generation seeks to produce
identity-preserving images of the same subject across diverse scenes, yet it
often fails due to a phenomenon called identity (ID) shift. Previous methods
have tackled this issue, but typically rely on the unrealistic assumption of
knowing all target scenes in advance. This paper reveals that a key source of
ID shift is the native correlation between subject and scene context, called
scene contextualization, which arises naturally as T2I models fit the training
distribution of vast natural images. We formally prove the near-universality of
this scene-ID correlation and derive theoretical bounds on its strength. On
this basis, we propose a novel, efficient, training-free prompt embedding
editing approach, called Scene De-Contextualization (SDeC), that imposes an
inversion process of T2I's built-in scene contextualization. Specifically, it
identifies and suppresses the latent scene-ID correlation within the ID
prompt's embedding by quantifying the SVD directional stability to adaptively
re-weight the corresponding eigenvalues. Critically, SDeC allows for per-scene
use (one scene per prompt) without requiring prior access to all target scenes.
This makes it a highly flexible and general solution well-suited to real-world
applications where such prior knowledge is often unavailable or varies over
time. Experiments demonstrate that SDeC significantly enhances identity
preservation while maintaining scene diversity.

</details>


### [116] [Eyes Wide Open: Ego Proactive Video-LLM for Streaming Video](https://arxiv.org/abs/2510.14560)
*Yulin Zhang,Cheng Shi,Yang Wang,Sibei Yang*

Main category: cs.CV

> 本文提出了一种可以在自主感知和理解的基础上，针对不断变化的环境，主动提供帮助的AI模型。通过引入ESTP-Bench和ESTP-F1指标，设计了一套完整的模型框架，包括数据引擎，多阶段训练策略和主动动态压缩技术，并取得了优于多种基线模型的结果。

<details>
  <summary>Details</summary>

**Motivation:** 旨在开发一种能够在真实环境中，通过视频流输入主动理解和回应环境变化的AI助手，强调其主动连贯性，即时响应性和同步效率。

**Method:** 提出了一种包含数据引擎，多阶段训练策略，以及主动动态压缩技术的全面技术管道。

**Result:** 提出的模型在ESTP-Bench等评估基准上优于多个基线模型。

**Conclusion:** 研究表明，通过ESTP-Bench和ESTP-F1的评估方式，结合技术创新，可以有效地开发出主动感知和响应的AI助手。

**Abstract:** Envision an AI capable of functioning in human-like settings, moving beyond
mere observation to actively understand, anticipate, and proactively respond to
unfolding events. Towards this vision, we focus on the innovative task where,
given ego-streaming video input, an assistant proactively answers diverse,
evolving questions at the opportune moment, while maintaining synchronized
perception and reasoning. This task embodies three key properties: (1)
Proactive Coherence, (2) Just-in-Time Responsiveness, and (3) Synchronized
Efficiency. To evaluate and address these properties, we first introduce
ESTP-Bench (Ego Streaming Proactive Benchmark) alongside the ESTP-F1 metric-a
novel framework designed for their rigorous assessment. Secondly, we propose a
comprehensive technical pipeline to enable models to tackle this challenging
task. This pipeline comprises: (1) a data engine, (2) a multi-stage training
strategy, and (3) a proactive dynamic compression technique. Our proposed model
effectively addresses these critical properties while outperforming multiple
baselines across diverse online and offline benchmarks. Project
Page:https://zhangyl4.github.io/publications/eyes-wide-open/

</details>


### [117] [BalanceGS: Algorithm-System Co-design for Efficient 3D Gaussian Splatting Training on GPU](https://arxiv.org/abs/2510.14564)
*Junyi Wu,Jiaming Xu,Jinhao Li,Yongkang Zhou,Jiayi Pan,Xingyang Li,Guohao Dai*

Main category: cs.CV

> 本文提出了BalanceGS算法，从算法、系统及映射三个层面改进了3D Gaussian Splatting (3DGS)的训练效率，取得了1.44倍的训练加速性能。

<details>
  <summary>Details</summary>

**Motivation:** 旨在解决3D Gaussian Splatting (3DGS)传统训练流水线中的效率低问题，如高斯密度化中的密度分配不均、高斯投影中的计算负载不平衡和颜色点化过程中的片段式内存访问等问题。

**Method:** 该论文提出了BalanceGS算法，分为算法、系统及映射三个层面：1. 在算法层面，提出了基于任务负载的高斯密度控制，自动平衡点分布；2. 在系统层面，提出了基于相似性的高斯采样和合并策略，允许线程根据局部簇密度动态处理不同数量的高斯量；3. 在映射层面，提出了基于重排的内存访问映射策略，重构了RGB存储并实现共享内存中的批量加载。

**Result:** 实验表明，与3DGS相比，该方法在NVIDIA A100 GPU上的训练速度提高了1.44倍，并且几乎没有质量下降。

**Conclusion:** BalanceGS有效提高了3D Gaussian Splatting的训练效率，同时保持了高质量的重建结果。

**Abstract:** 3D Gaussian Splatting (3DGS) has emerged as a promising 3D reconstruction
technique. The traditional 3DGS training pipeline follows three sequential
steps: Gaussian densification, Gaussian projection, and color splatting.
Despite its promising reconstruction quality, this conventional approach
suffers from three critical inefficiencies: (1) Skewed density allocation
during Gaussian densification, (2) Imbalanced computation workload during
Gaussian projection and (3) Fragmented memory access during color splatting.
  To tackle the above challenges, we introduce BalanceGS, the algorithm-system
co-design for efficient training in 3DGS. (1) At the algorithm level, we
propose heuristic workload-sensitive Gaussian density control to automatically
balance point distributions - removing 80% redundant Gaussians in dense regions
while filling gaps in sparse areas. (2) At the system level, we propose
Similarity-based Gaussian sampling and merging, which replaces the static
one-to-one thread-pixel mapping with adaptive workload distribution - threads
now dynamically process variable numbers of Gaussians based on local cluster
density. (3) At the mapping level, we propose reordering-based memory access
mapping strategy that restructures RGB storage and enables batch loading in
shared memory.
  Extensive experiments demonstrate that compared with 3DGS, our approach
achieves a 1.44$\times$ training speedup on a NVIDIA A100 GPU with negligible
quality degradation.

</details>


### [118] [CALM-Net: Curvature-Aware LiDAR Point Cloud-based Multi-Branch Neural Network for Vehicle Re-Identification](https://arxiv.org/abs/2510.14576)
*Dongwook Lee,Sol Han,Jinwhan Kim*

Main category: cs.CV

> 本文提出了一种基于曲率意识的LiDAR点云多分支神经网络CALM-Net，用于车辆重新识别。实验表明，使用该模型可以有效提高车辆重新识别的精度。

<details>
  <summary>Details</summary>

**Motivation:** 本文旨在解决从三维点云中学习辨别性和互补特征以区分车辆的挑战。

**Method:** CALM-Net采用了多分支架构，结合了边缘卷积、点注意力机制以及描述点云局部表面变化的曲率嵌入。通过这些机制，模型能学习到更多的几何和上下文特征，非常适合用于车辆的重新识别。

**Result:** 在大规模nuScenes数据集上的实验评估证明，与我们研究中的最强基线相比，CALM-Net实现了平均重新识别准确率大约提升1.97个百分点。

**Conclusion:** 实验结果证实，在深度学习架构中结合曲率信息的有效性，并突显出多分支特征学习在基于LiDAR点云的车辆重新识别中的益处。

**Abstract:** This paper presents CALM-Net, a curvature-aware LiDAR point cloud-based
multi-branch neural network for vehicle re-identification. The proposed model
addresses the challenge of learning discriminative and complementary features
from three-dimensional point clouds to distinguish between vehicles. CALM-Net
employs a multi-branch architecture that integrates edge convolution, point
attention, and a curvature embedding that characterizes local surface variation
in point clouds. By combining these mechanisms, the model learns richer
geometric and contextual features that are well suited for the
re-identification task. Experimental evaluation on the large-scale nuScenes
dataset demonstrates that CALM-Net achieves a mean re-identification accuracy
improvement of approximately 1.97\% points compared with the strongest baseline
in our study. The results confirms the effectiveness of incorporating curvature
information into deep learning architectures and highlight the benefit of
multi-branch feature learning for LiDAR point cloud-based vehicle
re-identification.

</details>


### [119] [Talking Points: Describing and Localizing Pixels](https://arxiv.org/abs/2510.14583)
*Matan Rusanovsky,Shimon Malnick,Shai Avidan*

Main category: cs.CV

> 研究开发了一种新的关键点可视化理解框架，能够实现像素级的关键点定位，并通过合成的新数据集LlamaPointInPart验证了其有效性。

<details>
  <summary>Details</summary>

**Motivation:** 现有模型在跨模态理解方面取得了显著的成功，但仅限于对象级或区域级的基础，缺乏通过自然语言实现像素级关键点理解的能力。本研究旨在解决这一问题，提出了一种新的像素级接地方法。

**Method:** 本研究提出了一种新的框架，该框架包括两个互补组件：Point Descriptor（生成丰富、语境描述的个别关键点描述器）和Point Localizer（从这些描述中回归精确像素坐标的定位器）。该框架采用了丰富的上下文描述，并用于合成一个名为LlamaPointInPart的新数据集，其中包括20K+图像-关键点-描述三元组。为了进行跨类别的泛化，研究者们在一个被称为AP-10K的数据集上优化Point Descriptor。

**Result:** 实验显示，该框架在LlamaPointInPart数据集上的性能优于基线模型。

**Conclusion:** 这种双方向的方法不仅证明了其在关键点导向图像理解以及语言导向精确定位的潜力，而且公开的数据集和编码资源可在未来研究中被充分利用。

**Abstract:** Vision-language models have achieved remarkable success in cross-modal
understanding. Yet, these models remain limited to object-level or region-level
grounding, lacking the capability for pixel-precise keypoint comprehension
through natural language. We introduce a novel framework for pixel level
grounding. The framework consists of two complementary components: a Point
Descriptor that generates rich, contextual descriptions of individual
keypoints, and a Point Localizer that regresses precise pixel coordinates from
these descriptions. Unlike prior work that relies on templated prompts or
keypoint names, our approach produces free-form, coarse-to-fine descriptions
that situate keypoints within their visual context. Since there is no available
dataset to train such a system, we introduce LlamaPointInPart, a carefully
curated dataset of 20K+ image-keypoint-description triplets synthesized from
multiple vision-language models, capturing multi-scale information from
scene-level context to visual features around the keypoint. For cross-category
generalization, we optimize the Point Descriptor on AP-10K via GRPO, using the
frozen Point Localizer as a reward model to produce descriptions that maximize
localization accuracy. To evaluate our results we establish a new evaluation
protocol. Instead of comparing the text description produced by our method to
the ground truth, we use the localizer to determine how close is the predicted
point generated to the ground truth point. Experiments demonstrate superior
performance compared to baseline models on LlamaPointInPart.The bidirectional
nature of our framework should enable future applications in both
keypoint-guided image understanding and language-guided precise localization.
Our code and dataset are publicly available at
https://github.com/matanr/Talking_Points.

</details>


### [120] [STANCE: Motion Coherent Video Generation Via Sparse-to-Dense Anchored Encoding](https://arxiv.org/abs/2510.14588)
*Zhifei Chen,Tianshuo Xu,Leyi Wu,Luozhou Wang,Dongyu Yan,Zihan You,Wenting Luo,Guo Zhang,Yingcong Chen*

Main category: cs.CV

> STANCE, a new image-to-video generation framework, introduces Instance Cues and Dense RoPE to improve motion guidance and temporal consistency in video generation.

<details>
  <summary>Details</summary>

**Motivation:** Despite progress in video generation, maintaining coherent object motion and interactions is challenging, especially due to ineffective human-provided motion hints and issues with optimizing appearance and motion simultaneously.

**Method:** STANCE, a framework that includes Instance Cues and Dense RoPE to enhance video generation by improving motion guidance and temporal consistency.

**Result:** STANCE reduces depth ambiguity and improves temporal coherence without the need for per-frame trajectory scripts, leading to better motion control and appearance in generated videos.

**Conclusion:** The introduction of Instance Cues and Dense RoPE in the STANCE framework addresses key challenges in video generation, demonstrating potential for significant improvements in generated videos.

**Abstract:** Video generation has recently made striking visual progress, but maintaining
coherent object motion and interactions remains difficult. We trace two
practical bottlenecks: (i) human-provided motion hints (e.g., small 2D maps)
often collapse to too few effective tokens after encoding, weakening guidance;
and (ii) optimizing for appearance and motion in a single head can favor
texture over temporal consistency. We present STANCE, an image-to-video
framework that addresses both issues with two simple components. First, we
introduce Instance Cues -- a pixel-aligned control signal that turns sparse,
user-editable hints into a dense 2.5D (camera-relative) motion field by
averaging per-instance flow and augmenting with monocular depth over the
instance mask. This reduces depth ambiguity compared to 2D arrow inputs while
remaining easy to use. Second, we preserve the salience of these cues in token
space with Dense RoPE, which tags a small set of motion tokens (anchored on the
first frame) with spatial-addressable rotary embeddings. Paired with joint RGB
\(+\) auxiliary-map prediction (segmentation or depth), our model anchors
structure while RGB handles appearance, stabilizing optimization and improving
temporal coherence without requiring per-frame trajectory scripts.

</details>


### [121] [Hierarchical Re-Classification: Combining Animal Classification Models with Vision Transformers](https://arxiv.org/abs/2510.14594)
*Hugo Markoff,Jevgenijs Galaktionovs*

Main category: cs.CV

> 我们提出了一种改进的分层再分类系统，使用SpeciesNet和CLIP嵌入，可提高动物检测平台上的物种级别分类准确性。实验表明，该系统能够以较高的准确性恢复和再分类动物图像，提升了物种级别的识别精度。

<details>
  <summary>Details</summary>

**Motivation:** 当前最先进的动物分类模型（如SpeciesNet）虽然能提供数千个物种的预测结果，但它们使用的是保守的层次聚合策略，导致许多动物只被标记在高级分类别上而不是具体物种。我们的动机是改进动物识别的准确性，特别是达到物种级别的识别。

**Method:** 我们提出了一个结合了SpeciesNet EfficientNetV2-M预测、CLIP嵌入和度量学习的分层再分类系统来改进动物检测平台上的动物分类，提高了物种级别的识别精度。该系统包括五个阶段：高置信度接受、鸟类覆盖、中心点构建、三元组损失度量学习以及自适应余弦距离评分。

**Result:** 我们在LILA BC沙漠狮保护数据集的一个子集上评估了这种方法，该子集包含4,018张图像和15,031个检测结果。经过处理，我们从'空白'和'动物'标签中找回了761个鸟类检测结果，并以96.5%的准确性重新分类了标记为'动物'、'哺乳动物'或'空白'的456个检测结果。

**Conclusion:** 该方法显著提高了在图像分类中的物种级别的准确性和可靠性，这对于动物保护和研究工作非常关键。我们的系统是一大进步，更有潜力应用于更广泛的数据集中。

**Abstract:** State-of-the-art animal classification models like SpeciesNet provide
predictions across thousands of species but use conservative rollup strategies,
resulting in many animals labeled at high taxonomic levels rather than species.
We present a hierarchical re-classification system for the Animal Detect
platform that combines SpeciesNet EfficientNetV2-M predictions with CLIP
embeddings and metric learning to refine high-level taxonomic labels toward
species-level identification. Our five-stage pipeline (high-confidence
acceptance, bird override, centroid building, triplet-loss metric learning, and
adaptive cosine-distance scoring) is evaluated on a segment of the LILA BC
Desert Lion Conservation dataset (4,018 images, 15,031 detections). After
recovering 761 bird detections from "blank" and "animal" labels, we re-classify
456 detections labeled animal, mammal, or blank with 96.5% accuracy, achieving
species-level identification for 64.9 percent

</details>


### [122] [Zero-Shot Wildlife Sorting Using Vision Transformers: Evaluating Clustering and Continuous Similarity Ordering](https://arxiv.org/abs/2510.14596)
*Hugo Markoff,Jevgenijs Galaktionovs*

Main category: cs.CV

> 研究使用自监督视觉变压器对野生动物摄像头陷阱的图像进行零样本组织，在Animal Detect平台上的实验结果表明，使用DINOv2配合UMAP和GMM能够获得优秀的分类效果，并将该方法部署用于加速生物多样性监测的手动标注工作流程。

<details>
  <summary>Details</summary>

**Motivation:** 野生动物摄像头陷阱产生了数百万张野生动物图像，但许多数据集中包含现有的分类器中不存在的物种。为了处理这些未标记的图像，研究动机是评估零样本方法来组织这些图像，特别是在Animal Detect平台上的摄像头陷阱分析。

**Method:** 本研究在Animal Detect平台中使用自监督视觉变压器对未标记的野生动物图像进行组织，评估了零样本方法。研究比较了DBSCAN和GMM两种无监督聚类方法，三种架构（CLIP，DINOv2，MegaDescriptor），以及主成分分析（PCA）和均匀映射（UMAP）两种降维技术，并通过t-SNE投影实现连续的一维相似性排序。

**Result:** 在一个包含5种物种的测试数据集上，使用地面真实标签仅用于评估，DINOv2配合UMAP和GMM达到了88.6%的准确率（宏F1 = 0.874），一维排序对于哺乳动物和鸟类达到了88.2%的连贯性，而对于鱼类达到了95.2%的连贯性（基于1,500张图像）。

**Conclusion:** 基于这些发现，研究将连续相似性排序部署到生产中，这有助于快速的探索性分析，并加速了手动标注工作流程，以监测生物多样性。

**Abstract:** Camera traps generate millions of wildlife images, yet many datasets contain
species that are absent from existing classifiers. This work evaluates
zero-shot approaches for organizing unlabeled wildlife imagery using
self-supervised vision transformers, developed and tested within the Animal
Detect platform for camera trap analysis. We compare unsupervised clustering
methods (DBSCAN, GMM) across three architectures (CLIP, DINOv2, MegaDescriptor)
combined with dimensionality reduction techniques (PCA, UMAP), and we
demonstrate continuous 1D similarity ordering via t-SNE projection. On a
5-species test set with ground truth labels used only for evaluation, DINOv2
with UMAP and GMM achieves 88.6 percent accuracy (macro-F1 = 0.874), while 1D
sorting reaches 88.2 percent coherence for mammals and birds and 95.2 percent
for fish across 1,500 images. Based on these findings, we deployed continuous
similarity ordering in production, enabling rapid exploratory analysis and
accelerating manual annotation workflows for biodiversity monitoring.

</details>


### [123] [Knowledge-based Visual Question Answer with Multimodal Processing, Retrieval and Filtering](https://arxiv.org/abs/2510.14605)
*Yuyang Hong,Jiaqi Gu,Qi Yang,Lubin Fan,Yue Wu,Ying Wang,Kun Ding,Shiming Xiang,Jieping Ye*

Main category: cs.CV

> 本文提出了一种名为Wiki-PRF的三阶段方法，旨在改进知识基础视觉问答中多模态查询的质量和检索结果的相关性，通过处理、检索和过滤阶段增强了模型的推理和工具调用能力，实验结果表明该方法在两个基准数据集E-VQA和InfoSeek上取得了显著的性能提升。

<details>
  <summary>Details</summary>

**Motivation:** 为了克服现有的检索增强生成方法在知识基础视觉问答任务中的多模态查询质量和检索结果相关性的局限性，文章提出了一种新的三阶段方法。

**Method:** 提出的方法分为处理、检索和过滤三个阶段，通过动态调用视觉工具提取精确的多模态信息，结合视觉和文本特征实现多模态知识检索，并执行检索结果的相关性过滤。

**Result:** 实验结果在E-VQA和InfoSeek数据集上分别提升了36.0和42.8的准确性，达到了最新的性能水平。

**Conclusion:** 实验确实证明了Wiki-PRF方法的有效性，其在改善知识基础视觉问答质量方面显示出显著的进步。

**Abstract:** Knowledge-based visual question answering (KB-VQA) requires visual language
models (VLMs) to integrate visual understanding with external knowledge
retrieval. Although retrieval-augmented generation (RAG) achieves significant
advances in this task by combining knowledge-base querying, it still struggles
with the quality of multimodal queries and the relevance of retrieved results.
To overcome these challenges, we propose a novel three-stage method, termed
Wiki-PRF, including Processing, Retrieval and Filtering stages. The processing
stage dynamically invokes visual tools to extract precise multimodal
information for retrieval. The retrieval stage integrates visual and text
features to achieve multimodal knowledge retrieval. The filtering stage
performs relevance filtering and concentration on retrieval results. To this
end, we introduce a visual language model trained with answer accuracy and
format consistency as reward signals via a reinforcement learning manner. This
enhances the model's reasoning, tool invocation for accurate queries, and
filtering of irrelevant content. Experiments on benchmark datasets (E-VQA and
InfoSeek) show significant improvements~(36.0 and 42.8) in answer quality,
achieving state-of-the-art performance. Code is available at
https://github.com/cqu-student/Wiki-PRF

</details>


### [124] [Shot2Tactic-Caption: Multi-Scale Captioning of Badminton Videos for Tactical Understanding](https://arxiv.org/abs/2510.14617)
*Ning Ding,Keisuke Fujii,Toru Tamaki*

Main category: cs.CV

> 本文提出了一个名为Shot2Tactic-Caption的新框架，用于羽毛球视频的语义和时间多尺度描述，包括生成击球描述和战术描述，并引入了战术单元检测器和击球引导机制。

<details>
  <summary>Details</summary>

**Motivation:** 本文针对羽毛球比赛中战术理解的复杂性和动态性，提出了一种新的框架，以期提高对羽毛球视频在击球层面和战术层面描述的准确性和完整性。

**Method:** 本文提出了一个名为Shot2Tactic-Caption的新框架，用于羽毛球视频的语义和时间多尺度描述。该框架包括两个分支，每个分支都包含视觉编码器、时空变换编码器以及变换解码器，以生成击球描述和战术描述。为了支持战术描述，作者还引入了一个战术单元检测器，用于识别战术单位、战术类型和战术状态。此外，还引入了一个基于击球引导机制，通过交叉注意力机制将预测的战术类型和状态嵌入到解码器中，以便更准确地描述战术执行。

**Result:** 实验结果显示，本文提出的框架在生成击球和战术描述方面均表现有效。此外，消融研究表明基于ResNet50的时空编码器在性能上优于其他变体，且基于击球的提示结构化方法能生成更为连贯和准确的战术描述。

**Conclusion:** 本文所提出的Shot2Tactic-Caption框架在羽毛球视频多尺度描述任务上展现了良好的效果，特别是通过增加战术单元检测器和引入基于击球的引导机制，提升了战术描述的准确性和连贯性。

**Abstract:** Tactical understanding in badminton involves interpreting not only individual
actions but also how tactics are dynamically executed over time. In this paper,
we propose \textbf{Shot2Tactic-Caption}, a novel framework for semantic and
temporal multi-scale video captioning in badminton, capable of generating
shot-level captions that describe individual actions and tactic-level captions
that capture how these actions unfold over time within a tactical execution. We
also introduce the Shot2Tactic-Caption Dataset, the first badminton captioning
dataset containing 5,494 shot captions and 544 tactic captions.
Shot2Tactic-Caption adopts a dual-branch design, with both branches including a
visual encoder, a spatio-temporal Transformer encoder, and a Transformer-based
decoder to generate shot and tactic captions. To support tactic captioning, we
additionally introduce a Tactic Unit Detector that identifies valid tactic
units, tactic types, and tactic states (e.g., Interrupt, Resume). For tactic
captioning, we further incorporate a shot-wise prompt-guided mechanism, where
the predicted tactic type and state are embedded as prompts and injected into
the decoder via cross-attention. The shot-wise prompt-guided mechanism enables
our system not only to describe successfully executed tactics but also to
capture tactical executions that are temporarily interrupted and later resumed.
Experimental results demonstrate the effectiveness of our framework in
generating both shot and tactic captions. Ablation studies show that the
ResNet50-based spatio-temporal encoder outperforms other variants, and that
shot-wise prompt structuring leads to more coherent and accurate tactic
captioning.

</details>


### [125] [Efficient Video Sampling: Pruning Temporally Redundant Tokens for Faster VLM Inference](https://arxiv.org/abs/2510.14624)
*Natan Bagrov,Eugene Khvedchenia,Borys Tymchenko,Shay Aharon,Lior Kadoch,Tomer Keren,Ofri Masad,Yonatan Geifman,Ran Zilberstein,Tuomas Rintamaki,Matthieu Le,Andrew Tao*

Main category: cs.CV

> 提出Efficient Video Sampling (EVS) 方法，用于减少视频中的标记冗余，能够在较少损失准确性的前提下，大幅提高视频推理速度并解答长视频处理中的上下文限制和延迟问题。

<details>
  <summary>Details</summary>

**Motivation:** 现代语言模型处理密集帧序列的成本呈二次增长，导致对长时间视频处理时的上下文限制和延迟问题，因此提出该方法解决这一挑战。

**Method:** 通过识别并修剪连续帧之间不变的时空区域，减少视频中的标记冗余，此方法称为Efficient Video Sampling (EVS)，能够在保持位置身份、无需架构更改或重新训练的情况下，实现这一目标。

**Result:** EVS显著减少了标记数量，同时保持语义保真度，加快推理速度并允许更长的输入序列。EVS在推理时的应用可将大语言模型的时间到首个标记(TTFT)最多减少4倍，且准确性损失极小；结合使用随机修剪率的uptraining阶段，EVS可以产出对不同压缩程度具有鲁棒性的模型，并在激进修剪下保持全性能。

**Conclusion:** 实验广泛表明EVS在提高效率-准确性权衡方面始终具有改进，解锁可扩展的视频语言理解，同时不牺牲质量。

**Abstract:** Vision-language models (VLMs) have recently expanded from static image
understanding to video reasoning, but their scalability is fundamentally
limited by the quadratic cost of processing dense frame sequences. Long videos
often exceed the token budget of modern language models, leading to severe
context limitations and latency issues. We introduce Efficient Video Sampling
(EVS), a simple, plug-and-play method for reducing token redundancy in videos
by identifying and pruning temporally static patches -- spatial regions that
remain unchanged across consecutive frames. EVS preserves positional identity,
requires no architectural changes or retraining. We show that EVS substantially
reduces token count while maintaining semantic fidelity, enabling faster
inference and longer input sequences. Applied at inference time, EVS reduces
large language model (LLM) time-to-first-token (TTFT) by up to 4x with minimal
accuracy loss. When combined with an uptraining phase using stochastic pruning
rates, EVS yields models that are robust to varying compression levels and
retain full performance under aggressive pruning. Extensive experiments
demonstrate that EVS consistently improves efficiency-accuracy trade-offs,
unlocking scalable video-language understanding without sacrificing quality.

</details>


### [126] [Adapting Self-Supervised Representations as a Latent Space for Efficient Generation](https://arxiv.org/abs/2510.14630)
*Ming Gui,Johannes Schusterbauer,Timy Phan,Felix Krause,Josh Susskind,Miguel Angel Bautista,Björn Ommer*

Main category: cs.CV

> RepTok 是一种使用单个连续潜在标记表示图像的生成模型框架，通过余弦相似度损失保持潜在空间的几何特性。尽管方法简单且高效，但RepTok在各类图像生成任务中表现出优异性能。

<details>
  <summary>Details</summary>

**Motivation:** 开发一种更高效、更简单的生成模型方法，能够减少训练成本并保持良好的图像重建效果。

**Method:** 我们引入了表示标记化器（RepTok），这是一种生成模型框架，使用从自监督视觉变压器中获得的单个连续潜在标记来表示图像。在预训练的SSL编码器基础上，我们只微调语义标记嵌入，并将其与通过标准流匹配目标联合训练的生成解码器配对。为了保持原始SSL空间的有利几何特性，我们还添加了一种余弦相似度损失，以正则化适应的标记，确保潜在空间保持平滑且适合生成。

**Result:** RepTok 在类别条件下的 ImageNet 生成上取得了有竞争力的结果，并且能够自然扩展到文本到图像的合成，在 MS-COCO 上达到了有限训练预算下的零样本性能。

**Conclusion:** 研究结果表明，微调后的 SSL 表示作为紧凑且有效的潜在空间对于高效的生成建模具有潜力。

**Abstract:** We introduce Representation Tokenizer (RepTok), a generative modeling
framework that represents an image using a single continuous latent token
obtained from self-supervised vision transformers. Building on a pre-trained
SSL encoder, we fine-tune only the semantic token embedding and pair it with a
generative decoder trained jointly using a standard flow matching objective.
This adaptation enriches the token with low-level, reconstruction-relevant
details, enabling faithful image reconstruction. To preserve the favorable
geometry of the original SSL space, we add a cosine-similarity loss that
regularizes the adapted token, ensuring the latent space remains smooth and
suitable for generation. Our single-token formulation resolves spatial
redundancies of 2D latent spaces and significantly reduces training costs.
Despite its simplicity and efficiency, RepTok achieves competitive results on
class-conditional ImageNet generation and naturally extends to text-to-image
synthesis, reaching competitive zero-shot performance on MS-COCO under
extremely limited training budgets. Our findings highlight the potential of
fine-tuned SSL representations as compact and effective latent spaces for
efficient generative modeling.

</details>


### [127] [SteeringTTA: Guiding Diffusion Trajectories for Robust Test-Time-Adaptation](https://arxiv.org/abs/2510.14634)
*Jihyun Yu,Yoojin Oh,Wonho Bae,Mingyu Kim,Junhyug Noh*

Main category: cs.CV

> 本文提出SteeringTTA方法，通过结合累积的Top-K概率和熵计划，改进基于扩散的输入适配，从而提高模型鲁棒性，且在ImageNet-C上获得了优于基线的结果。

<details>
  <summary>Details</summary>

**Motivation:** 为了改善基于输入扩散的测试时间适应方法的鲁棒性和推广性，特别是在面临不同类型的失真时，并且避免依赖梯度引导的局限性。

**Method:** SteeringTTA提出了一种仅在推理阶段使用的框架，它通过结合累积的Top-K概率和熵计划，利用Feynman-Kac导向来引导基于扩散的输入适应，从而提高分类的鲁棒性。

**Result:** 在ImageNet-C数据集上，SteeringTTA在不进行任何模型更新或使用源数据的情况下，始终优于基线方法。

**Conclusion:** SteeringTTA通过在推理阶段引入多轨迹粒子和奖励机制，提高了测试时间适应方法的有效性和适应性。

**Abstract:** Test-time adaptation (TTA) aims to correct performance degradation of deep
models under distribution shifts by updating models or inputs using unlabeled
test data. Input-only diffusion-based TTA methods improve robustness for
classification to corruptions but rely on gradient guidance, limiting
exploration and generalization across distortion types. We propose SteeringTTA,
an inference-only framework that adapts Feynman-Kac steering to guide
diffusion-based input adaptation for classification with rewards driven by
pseudo-label. SteeringTTA maintains multiple particle trajectories, steered by
a combination of cumulative top-K probabilities and an entropy schedule, to
balance exploration and confidence. On ImageNet-C, SteeringTTA consistently
outperforms the baseline without any model updates or source data.

</details>


### [128] [In-Context Learning with Unpaired Clips for Instruction-based Video Editing](https://arxiv.org/abs/2510.14648)
*Xinyao Liao,Xianfang Zeng,Ziye Song,Zhoujie Fu,Gang Yu,Guosheng Lin*

Main category: cs.CV

> 本文提出了一种低成本的预训练策略，用于扩展指令基础的视频编辑能力，通过预训练和微调的结合，提高指令对齐度和视觉保真度。

<details>
  <summary>Details</summary>

**Motivation:** 视频编辑领域的指令基础方法扩展仍待探索，主要因为构建大规模配对视频编辑数据集的成本和复杂度问题。我们的目标是提出一种有效且经济的方法来解决这个问题。

**Method:** 我们提出了一种低成本的预训练策略，利用不配对的视频片段进行上下文学习，来扩展指令基础的视频编辑能力。首先在近100万个真实视频片段上预训练基础视频生成模型，学习基本的编辑概念，然后在少于15万个精选编辑配对上进行微调，以扩展更多编辑任务并提高编辑质量。

**Result:** 对比实验证明，我们方法在指令对齐度和视觉保真度方面超过了现有方法，分别提高了12%和15%。

**Conclusion:** 我们的方法可以通过结合大量不配对视频片段的预训练和少量高质量配对编辑数据的微调，有效地扩展视频的编辑能力。

**Abstract:** Despite the rapid progress of instruction-based image editing, its extension
to video remains underexplored, primarily due to the prohibitive cost and
complexity of constructing large-scale paired video editing datasets. To
address this challenge, we introduce a low-cost pretraining strategy for
instruction-based video editing that leverages in-context learning from
unpaired video clips. We show that pretraining a foundation video generation
model with this strategy endows it with general editing capabilities, such as
adding, replacing, or deleting operations, according to input editing
instructions. The pretrained model can then be efficiently refined with a small
amount of high-quality paired editing data. Built upon HunyuanVideoT2V, our
framework first pretrains on approximately 1M real video clips to learn basic
editing concepts, and subsequently fine-tunes on fewer than 150k curated
editing pairs to extend more editing tasks and improve the editing quality.
Comparative experiments show that our method surpasses existing
instruction-based video editing approaches in both instruction alignment and
visual fidelity, achieving a 12\% improvement in editing instruction following
and a 15\% improvement in editing quality.

</details>


### [129] [Decorrelation Speeds Up Vision Transformers](https://arxiv.org/abs/2510.14657)
*Kieran Carrigg,Rob van Gastel,Melda Yeghaian,Sander Dalm,Faysal Boughorbel,Marcel van Gerven*

Main category: cs.CV

> The integration of Decorrelated Backpropagation (DBP) into Masked Autoencoder (MAE) pre-training for vision transformers (ViTs) achieves faster training without losing stability, reducing both the training time and carbon emissions while enhancing performance in image segmentation.

<details>
  <summary>Details</summary>

**Motivation:** To tackle the high computational costs and complexity of MAE pre-training for ViTs in practical and industrial settings, leading to impractical long training times and high resource usage.

**Method:** Structure

**Result:** DBP reduces training time by 21.1% and carbon emissions by 21.4%, while also boosting segmentation mIoU by 1.1 points on datasets such as ImageNet-1K and ADE20K, demonstrating comparable gains with proprietary industrial data as well.

**Conclusion:** DBP significantly improves the efficiency and effectiveness of MAE pre-training for ViTs, making it a viable solution for real-world applications that are time- and resource-constrained.

**Abstract:** Masked Autoencoder (MAE) pre-training of vision transformers (ViTs) yields
strong performance in low-label regimes but comes with substantial
computational costs, making it impractical in time- and resource-constrained
industrial settings. We address this by integrating Decorrelated
Backpropagation (DBP) into MAE pre-training, an optimization method that
iteratively reduces input correlations at each layer to accelerate convergence.
Applied selectively to the encoder, DBP achieves faster pre-training without
loss of stability. On ImageNet-1K pre-training with ADE20K fine-tuning, DBP-MAE
reduces wall-clock time to baseline performance by 21.1%, lowers carbon
emissions by 21.4% and improves segmentation mIoU by 1.1 points. We observe
similar gains when pre-training and fine-tuning on proprietary industrial data,
confirming the method's applicability in real-world scenarios. These results
demonstrate that DBP can reduce training time and energy use while improving
downstream performance for large-scale ViT pre-training.

</details>


### [130] [EuroMineNet: A Multitemporal Sentinel-2 Benchmark for Spatiotemporal Mining Footprint Analysis in the European Union (2015-2024)](https://arxiv.org/abs/2510.14661)
*Weikang Yu,Vincent Nwazelibe,Xianping Ma,Xiaokang Zhang,Richard Gloaguen,Xiao Xiang Zhu,Pedram Ghamisi*

Main category: cs.CV

> 本文介绍了EuroMineNet，这是一个基于Sentinel-2多光谱影像构建的多时态采矿足迹监测数据集，支持可持续发展任务和GeoAI方法的发展。

<details>
  <summary>Details</summary>

**Motivation:** 采矿活动是工业和经济发展的重要组成部分，但也是导致环境退化的主要因素，如森林砍伐、土壤侵蚀和水污染等。现有的数据集在时间深度或地理范围上有限，因此需要一个更全面的基准数据集来实现持续的环境监测。

**Method:** 通过使用Sentinel-2多光谱影像，构建了一个名为EuroMineNet的多时态基准数据集，用于监测和绘制欧盟范围内133个采矿地点的采矿足迹变化。

**Result:** 该研究提供了从2015年至2024年的年度观测和专家验证注释，并支持了两个任务：多时态采矿足迹映射和跨时间变化检测。对20种最先进的深度学习模型的基准测试表明，GeoAI方法在识别长期环境变化方面有效，但在短时动态检测方面仍面临挑战。

**Conclusion:** 通过推进时间上一致且可解释的采矿监测，EuroMineNet有助于可持续土地管理、提高环境韧性，并推动GeoAI在社会和环境公益中的应用。

**Abstract:** Mining activities are essential for industrial and economic development, but
remain a leading source of environmental degradation, contributing to
deforestation, soil erosion, and water contamination. Sustainable resource
management and environmental governance require consistent, long-term
monitoring of mining-induced land surface changes, yet existing datasets are
often limited in temporal depth or geographic scope. To address this gap, we
present EuroMineNet, the first comprehensive multitemporal benchmark for mining
footprint mapping and monitoring based on Sentinel-2 multispectral imagery.
Spanning 133 mining sites across the European Union, EuroMineNet provides
annual observations and expert-verified annotations from 2015 to 2024, enabling
GeoAI-based models to analyze environmental dynamics at a continental scale. It
supports two sustainability-driven tasks: (1) multitemporal mining footprint
mapping for consistent annual land-use delineation, evaluated with a novel
Change-Aware Temporal IoU (CA-TIoU) metric, and (2) cross-temporal change
detection to capture both gradual and abrupt surface transformations.
Benchmarking 20 state-of-the-art deep learning models reveals that while GeoAI
methods effectively identify long-term environmental changes, challenges remain
in detecting short-term dynamics critical for timely mitigation. By advancing
temporally consistent and explainable mining monitoring, EuroMineNet
contributes to sustainable land-use management, environmental resilience, and
the broader goal of applying GeoAI for social and environmental good. We
release the codes and datasets by aligning with FAIR and the open science
paradigm at https://github.com/EricYu97/EuroMineNet.

</details>


### [131] [WeCKD: Weakly-supervised Chained Distillation Network for Efficient Multimodal Medical Imaging](https://arxiv.org/abs/2510.14668)
*Md. Abdur Rahman,Mohaimenul Azam Khan Raiaan,Sami Azam,Asif Karim,Jemima Beissbarth,Amanda Leach*

Main category: cs.CV

> This paper presents WeCKD, a new approach to knowledge distillation that improves upon traditional methods by structuring the knowledge transfer through a sequence of interconnected models, leading to better performance, especially in limited-data scenarios.

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind this research is to address the limitations of traditional knowledge distillation, which includes knowledge degradation, inefficient supervision, and the reliance on either very strong teacher models or large labeled datasets. The proposed method is particularly aimed at improving performance in real-world scenarios where data is limited.

**Method:** The paper introduces a novel method called Weakly-supervised Chain-based KD network (WeCKD). This method uses a structured sequence of connected models in a distillation chain, where each model learns and refines knowledge before passing it on. This approach enhances feature learning, reduces data dependency, and mitigates the limitations of one-step KD.

**Result:** The WeCKD method has been evaluated on four otoscopic imaging datasets and two other datasets, demonstrating it can match and often exceed the performance of existing supervised methods. It showed cumulative accuracy gains of up to +23% over a single backbone trained on limited data.

**Conclusion:** The conclusion is that the WeCKD method effectively addresses the challenges faced by traditional KD methods, enhancing feature learning, reducing data dependency, and achieving high performance in limited-data scenarios, thus being promising for real-world applications in medical imaging and beyond.

**Abstract:** Knowledge distillation (KD) has traditionally relied on a static
teacher-student framework, where a large, well-trained teacher transfers
knowledge to a single student model. However, these approaches often suffer
from knowledge degradation, inefficient supervision, and reliance on either a
very strong teacher model or large labeled datasets, which limits their
effectiveness in real-world, limited-data scenarios. To address these, we
present the first-ever Weakly-supervised Chain-based KD network (WeCKD) that
redefines knowledge transfer through a structured sequence of interconnected
models. Unlike conventional KD, it forms a progressive distillation chain,
where each model not only learns from its predecessor but also refines the
knowledge before passing it forward. This structured knowledge transfer further
enhances feature learning, reduces data dependency, and mitigates the
limitations of one-step KD. Each model in the distillation chain is trained on
only a fraction of the dataset and demonstrates that effective learning can be
achieved with minimal supervision. Extensive evaluations across four otoscopic
imaging datasets demonstrate that it not only matches but in many cases
surpasses the performance of existing supervised methods. Experimental results
on two other datasets further underscore its generalization across diverse
medical imaging modalities, including microscopic and magnetic resonance
imaging. Furthermore, our evaluations resulted in cumulative accuracy gains of
up to +23% over a single backbone trained on the same limited data, which
highlights its potential for real-world adoption.

</details>


### [132] [VTimeCoT: Thinking by Drawing for Video Temporal Grounding and Reasoning](https://arxiv.org/abs/2510.14672)
*Jinglei Zhang,Yuanfan Guo,Rolandos Alexandros Potamias,Jiankang Deng,Hang Xu,Chao Ma*

Main category: cs.CV

> 论文提出了VTimeCoT框架，一种用于视频时间定位和推理的工具，通过结合新颖的视觉工具和跨模态推理方法，提升了基于大规模语言模型的视频问答系统的表现。

<details>
  <summary>Details</summary>

**Motivation:** 大规模语言模型虽然在多模态应用中表现出了显著的潜力，但它们在视频时间定位和推理方面仍存在不足，这使得开发有效的视频理解系统遇到了挑战。为此，作者受到人类使用视频进度条进行视频理解方式的启发，提出了VTimeCoT框架。

**Method:** 该论文提出了一种名为VTimeCoT的无训练框架，用于提高视频中的时间定位和推理性能。该框架包含两个新颖的视觉工具：可插拔的时间条整合工具和高效率的强调工具。此外，还引入了跨模态推理的视时链式思考（visuotemporal CoT）过程，以解决传统基于文本的链式思考方法的局限性。

**Result:** 该方法在视频时间定位和基于推理的问题回答任务上，对Qwen2VL-7B和GPT4o基线模型的表现有了显著的提升。

**Conclusion:** 该框架实现了组合性和解释性强的推理过程，有效解决了视频时间定位和推理的问题，从而改善了基于大规模语言模型的视频问答系统的表现。

**Abstract:** In recent years, video question answering based on multimodal large language
models (MLLM) has garnered considerable attention, due to the benefits from the
substantial advancements in LLMs. However, these models have a notable
deficiency in the domains of video temporal grounding and reasoning, posing
challenges to the development of effective real-world video understanding
systems. Inspired by how humans use video players to interact with the progress
bar for video comprehension, we introduce VTimeCoT, a simple yet effective
training-free framework, designed for high-performance video grounding and
reasoning. The proposed framework incorporates two novel visual tools of the
progress bar: a plug-and-play progress bar integration tool and a
high-efficiency highlighting tool. In addition, to address the limitations of
conventional text-based chain-of-thought (CoT) approaches, we introduce a
visuotemporal CoT process that integrates cross-modality reasoning across both
video and text. Our approach demonstrates significant performance improvements
on both Qwen2VL-7B and GPT4o baselines in tasks of video temporal grounding and
reasoning-based question answering. Finally, we showcase that the proposed
framework achieves a compositional and interpretable reasoning process. Project
page: https://vtimecot.github.io

</details>


### [133] [Leveraging Learned Image Prior for 3D Gaussian Compression](https://arxiv.org/abs/2510.14705)
*Seungjoo Shin,Jaesik Park,Sunghyun Cho*

Main category: cs.CV

> 介绍了一种新的3D GS压缩框架，利用学习到的图像先验来恢复压缩质量，展示了优越的率失真性能和更好的渲染质量。

<details>
  <summary>Details</summary>

**Motivation:** 尽管现有的3DGS压缩技术在减少存储开销方面取得了显著的成效，但由于缺乏学习到的先验知识，使得在3DGS压缩任务中的率失真折衷进一步改进受限。因此，提出了一种新的3DGS压缩框架以解决此问题。

**Method:** 通过引入一种新的3DGS压缩框架来解决现有压缩技术中的缺点。该框架利用图像先验学习的能力来恢复压缩导致的质量下降。这个框架在初始压缩的高斯函数的基础上，引入了一个恢复网络来模拟压缩过程中的图像空间畸变。通过将粗糙渲染残差作为侧信息提供给恢复网络，提高率失真性能。恢复网络的监督来自于恢复的图像，用于进一步优化压缩高斯函数，实现更紧凑的表示和增强的渲染性能。

**Result:** 广泛的实验结果证明了框架的有效性，展示了优越的率失真性能，并在比最先进的3DGS压缩方法更少的存储空间要求下，实现了更好的渲染质量。

**Conclusion:** 该框架设计与现有的高斯压缩方法兼容，不仅验证了其优越的率失真性能，还在存储量大幅减少的情况下超越了现有最先进的3D GS压缩方法的渲染质量。

**Abstract:** Compression techniques for 3D Gaussian Splatting (3DGS) have recently
achieved considerable success in minimizing storage overhead for 3D Gaussians
while preserving high rendering quality. Despite the impressive storage
reduction, the lack of learned priors restricts further advances in the
rate-distortion trade-off for 3DGS compression tasks. To address this, we
introduce a novel 3DGS compression framework that leverages the powerful
representational capacity of learned image priors to recover
compression-induced quality degradation. Built upon initially compressed
Gaussians, our restoration network effectively models the compression artifacts
in the image space between degraded and original Gaussians. To enhance the
rate-distortion performance, we provide coarse rendering residuals into the
restoration network as side information. By leveraging the supervision of
restored images, the compressed Gaussians are refined, resulting in a highly
compact representation with enhanced rendering performance. Our framework is
designed to be compatible with existing Gaussian compression methods, making it
broadly applicable across different baselines. Extensive experiments validate
the effectiveness of our framework, demonstrating superior rate-distortion
performance and outperforming the rendering quality of state-of-the-art 3DGS
compression methods while requiring substantially less storage.

</details>


### [134] [Where are the Whales: A Human-in-the-loop Detection Method for Identifying Whales in High-resolution Satellite Imagery](https://arxiv.org/abs/2510.14709)
*Caleb Robinson,Kimberly T. Goetz,Christin B. Khan,Meredith Sackett,Kathleen Leonard,Rahul Dodhia,Juan M. Lavista Ferres*

Main category: cs.CV

> 本文提出了一种半自动方法来识别高分辨率卫星图像中的鲸鱼，能够在大规模图像中显著减少人工标注工作量，并提高监测效率。

<details>
  <summary>Details</summary>

**Motivation:** 改善鲸鱼群体监测效率，降低成本，解决大规模自动检测中的挑战，如缺乏标注图像、图像质量差异和环境条件变化、以及构建大规模机器学习管道的成本问题。

**Method:** 一种半自动的方法，使用统计异常检测法来标记高分辨率卫星图像中的可能鲸鱼检测点，并结合网络标注界面以供专家快速标注这些检测点。

**Result:** 在三个具有已知鲸鱼标注的基准场景上进行评估，召回率从90.3%到96.4%，同时将需要专家检查的区域减少到最高99.8% -- 在某些情况下，从超过1,000平方公里减少到不足2平方公里。

**Conclusion:** 该方法无需标注数据集训练，并提供了未来空间辅助海洋哺乳动物监测的可扩展的初始步骤。

**Abstract:** Effective monitoring of whale populations is critical for conservation, but
traditional survey methods are expensive and difficult to scale. While prior
work has shown that whales can be identified in very high-resolution (VHR)
satellite imagery, large-scale automated detection remains challenging due to a
lack of annotated imagery, variability in image quality and environmental
conditions, and the cost of building robust machine learning pipelines over
massive remote sensing archives. We present a semi-automated approach for
surfacing possible whale detections in VHR imagery using a statistical anomaly
detection method that flags spatial outliers, i.e. "interesting points". We
pair this detector with a web-based labeling interface designed to enable
experts to quickly annotate the interesting points. We evaluate our system on
three benchmark scenes with known whale annotations and achieve recalls of
90.3% to 96.4%, while reducing the area requiring expert inspection by up to
99.8% -- from over 1,000 sq km to less than 2 sq km in some cases. Our method
does not rely on labeled training data and offers a scalable first step toward
future machine-assisted marine mammal monitoring from space. We have open
sourced this pipeline at https://github.com/microsoft/whales.

</details>


### [135] [Camera Movement Classification in Historical Footage: A Comparative Study of Deep Video Models](https://arxiv.org/abs/2510.14713)
*Tingyu Lin,Armin Dadras,Florian Kleber,Robert Sablatnig*

Main category: cs.CV

> 本文评估了深度学习技术在历史影片拍摄运动分类上的应用，并发现Video Swin Transformer模型能有效处理历史视频。

<details>
  <summary>Details</summary>

**Motivation:** 此研究旨在系统性评估现有深度视频拍摄运动分类模型在历史影片材料上的表现，特别是面对低质量视频时的适应性和潜力。

**Method:** 本文通过对HISTORIAN数据集（包含专家标注的二战片段）上五种标准视频分类模型的评估，分析了深度视频拍摄运动分类（CMC）模型在历史影片材料上的应用情况。

**Result:** 评估结果表明，在训练数据有限的情况下，Video Swin Transformer模型能够达到80.25%的精度，证明了其良好的泛化能力。

**Conclusion:** 本研究指出了现有模型应用于低质量视频时所面临的挑战，并提倡未来研究结合多样化的输入模式和时间架构来进一步提升性能。

**Abstract:** Camera movement conveys spatial and narrative information essential for
understanding video content. While recent camera movement classification (CMC)
methods perform well on modern datasets, their generalization to historical
footage remains unexplored. This paper presents the first systematic evaluation
of deep video CMC models on archival film material. We summarize representative
methods and datasets, highlighting differences in model design and label
definitions. Five standard video classification models are assessed on the
HISTORIAN dataset, which includes expert-annotated World War II footage. The
best-performing model, Video Swin Transformer, achieves 80.25% accuracy,
showing strong convergence despite limited training data. Our findings
highlight the challenges and potential of adapting existing models to
low-quality video and motivate future work combining diverse input modalities
and temporal architectures.

</details>


### [136] [Cross-Layer Feature Self-Attention Module for Multi-Scale Object Detection](https://arxiv.org/abs/2510.14726)
*Dingzhou Xie,Rushi Lan,Cheng Pang,Enhao Ning,Jiahao Zeng,Wei Zheng*

Main category: cs.CV

> 文章提出了一种最先进的跨层特征自我注意模块，显著提升了对象检测的性能。

<details>
  <summary>Details</summary>

**Motivation:** 最近的对象检测方法通过利用注意力机制以提高特征可分辨性方面取得了显著进展。然而，大多数现有方法局限于优化单层特征或融合双层特征，忽视了多尺度表示中的丰富层间关系。这种局限性限制了它们捕捉全面上下文信息的能力，这一点对于检测具有大规模变化的对象至关重要。因此，本文提出了一种新的跨层特征自注意力模块（CFSAM），以全局地建模多尺度特征图中的局部和全局依赖关系。

**Method:** Structure

**Result:** {
  "tldr": "本文提出了一种新型的跨层特征自注意力模块（CFSAM），它能够全局地建模多尺度特征图中的局部和全局依赖关系，从而显著提升了对象检测性能。",
  "motivation": "现有的方法通常局限于优化单层或融合双层特征，忽视了多尺度表示中的丰富跨层依赖关系，限制了其捕捉全面上下文信息的能力，特别是对于检测具有大规模变化的对象。",
  "method": "CFSAM包含三个关键组成部分：卷积局部特征提取器，基于Transformer的全局建模单元，以及特征融合机制。当集成到SSD300框架中时，CFSAM显著提升了检测性能。",
  "result": "实验显示，在PASCAL VOC和COCO数据集上，CFSAM提高了78.6% mAP和52.1% mAP，分别比基线高出75.5%和43.1%，并加速了训练收敛。",
  "conclusion": "该模块在不显著增加计算负担的前提下，以显式跨层注意力建模方式推进了多尺度物体检测的发展。这项工作凸显了显式跨层注意建模在多尺度物体检测中的重要性。",
  "tool": "structure_extraction_tool"}
}

**Conclusion:** 我们提出的工作展示了显式跨层注意力建模在推进多尺度物体检测中的重要性。CFSAM模块能够在不显著增加计算负担的情况下，显著提升对象检测性能，并加速训练速度。

**Abstract:** Recent object detection methods have made remarkable progress by leveraging
attention mechanisms to improve feature discriminability. However, most
existing approaches are confined to refining single-layer or fusing dual-layer
features, overlooking the rich inter-layer dependencies across multi-scale
representations. This limits their ability to capture comprehensive contextual
information essential for detecting objects with large scale variations. In
this paper, we propose a novel Cross-Layer Feature Self-Attention Module
(CFSAM), which holistically models both local and global dependencies within
multi-scale feature maps. CFSAM consists of three key components: a
convolutional local feature extractor, a Transformer-based global modeling unit
that efficiently captures cross-layer interactions, and a feature fusion
mechanism to restore and enhance the original representations. When integrated
into the SSD300 framework, CFSAM significantly boosts detection performance,
achieving 78.6% mAP on PASCAL VOC (vs. 75.5% baseline) and 52.1% mAP on COCO
(vs. 43.1% baseline), outperforming existing attention modules. Moreover, the
module accelerates convergence during training without introducing substantial
computational overhead. Our work highlights the importance of explicit
cross-layer attention modeling in advancing multi-scale object detection.

</details>


### [137] [Free-Grained Hierarchical Recognition](https://arxiv.org/abs/2510.14737)
*Seulki Park,Zilin Wang,Stella X. Yu*

Main category: cs.CV

> 本文提出ImageNet-F基准和自由粒度学习方法，使用CLIP模拟人类标注行为产生的粗细混合标签，增强了硅基与视觉指导，提高了分层分类在实际约束下的表现。

<details>
  <summary>Details</summary>

**Motivation:** 现有的分层图像分类方法通常假设完整的细粒度标注，但在实际应用中这一假设很少成立。现实中的监督在颗粒度上会有差异，这受到图像质量、标注者的专业技能以及任务需求的影响。

**Method:** 本文采用的方法包括：使用CLIP模拟现实中更靠近人类标注行为的标签，开发自由粒度学习技术，利用视觉-语言模型的伪属性进行语义指导提升，以及通过半监督学习进行视觉指导。

**Result:** 本文提出了ImageNet-F，这是一个从ImageNet中精心挑选并按照认知启发的基本、中级和细粒度级别结构化的大型基准数据集。通过使用CLIP来模拟由人类标注行为产生的混杂粒度标签，从而更好地反映实际中的语义模糊性。同时，本文还提出了自由粒度学习方法，采用异构监督实例。方法上，借助视觉-语言模型的伪属性增强语义指导，并通过半监督学习提供视觉指导。这些方法以及强大的基线模型在混合监督下显著提高了性能，共同推动了在现实世界约束下的层次分类进步。

**Conclusion:** 通过引入ImageNet-F基准和新提出的方法，本文在现实世界的约束条件下显著推动了分层图像分类的发展，实现了在混合监督下的性能提升。

**Abstract:** Hierarchical image classification predicts labels across a semantic taxonomy,
but existing methods typically assume complete, fine-grained annotations, an
assumption rarely met in practice. Real-world supervision varies in
granularity, influenced by image quality, annotator expertise, and task
demands; a distant bird may be labeled Bird, while a close-up reveals Bald
eagle. We introduce ImageNet-F, a large-scale benchmark curated from ImageNet
and structured into cognitively inspired basic, subordinate, and fine-grained
levels. Using CLIP as a proxy for semantic ambiguity, we simulate realistic,
mixed-granularity labels reflecting human annotation behavior. We propose
free-grain learning, with heterogeneous supervision across instances. We
develop methods that enhance semantic guidance via pseudo-attributes from
vision-language models and visual guidance via semi-supervised learning. These,
along with strong baselines, substantially improve performance under mixed
supervision. Together, our benchmark and methods advance hierarchical
classification under real-world constraints.

</details>


### [138] [DEXTER: Diffusion-Guided EXplanations with TExtual Reasoning for Vision Models](https://arxiv.org/abs/2510.14741)
*Simone Carnemolla,Matteo Pennisi,Sarinda Samarasinghe,Giovanni Bellitto,Simone Palazzo,Daniela Giordano,Mubarak Shah,Concetto Spampinato*

Main category: cs.CV

> DEXTER是一种无数据框架，结合扩散模型和大规模语言模型生成视觉分类器的全局文本文档，无需访问训练数据即可解释模型决策过程。

<details>
  <summary>Details</summary>

**Motivation:** 为了建立透明和值得信赖的AI系统，理解机器学习模型的行为至关重要。DEXTER能够在无需访问训练数据或真实标签的情况下提供分类器决策过程的自然语言解释。

**Method:** DEXTER采用无数据框架，利用扩散模型和大规模语言模型生成视觉分类器的全局文本解释。通过优化文本提示以合成强烈激活目标分类器的类别条件图像，这些合成样本用于生成描述类别特定决策模式和偏差的详细自然语言报告。

**Result:** 定量和定性评估，包括用户研究，表明DEXTER生成准确且可解释的输出。在ImageNet、Waterbirds、CelebA和FairFaces上的实验确认了DEXTER在全局模型解释和类别级别偏差报告方面优于现有方法。

**Conclusion:** DEXTER在多种实验中证明了其灵活性和有效性，展示了在全局模型解释和类别特定偏差报告方面的优越性。

**Abstract:** Understanding and explaining the behavior of machine learning models is
essential for building transparent and trustworthy AI systems. We introduce
DEXTER, a data-free framework that employs diffusion models and large language
models to generate global, textual explanations of visual classifiers. DEXTER
operates by optimizing text prompts to synthesize class-conditional images that
strongly activate a target classifier. These synthetic samples are then used to
elicit detailed natural language reports that describe class-specific decision
patterns and biases. Unlike prior work, DEXTER enables natural language
explanation about a classifier's decision process without access to training
data or ground-truth labels. We demonstrate DEXTER's flexibility across three
tasks-activation maximization, slice discovery and debiasing, and bias
explanation-each illustrating its ability to uncover the internal mechanisms of
visual classifiers. Quantitative and qualitative evaluations, including a user
study, show that DEXTER produces accurate, interpretable outputs. Experiments
on ImageNet, Waterbirds, CelebA, and FairFaces confirm that DEXTER outperforms
existing approaches in global model explanation and class-level bias reporting.
Code is available at https://github.com/perceivelab/dexter.

</details>
