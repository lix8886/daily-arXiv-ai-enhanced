{"id": "2507.16922", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.16922", "abs": "https://arxiv.org/abs/2507.16922", "authors": ["Shmuel Amar", "Ori Shapira", "Aviv Slobodkin", "Ido Dagan"], "title": "A Unifying Scheme for Extractive Content Selection Tasks", "comment": null, "summary": "A broad range of NLP tasks involve selecting relevant text spans from given\nsource texts. Despite this shared objective, such \\textit{content selection}\ntasks have traditionally been studied in isolation, each with its own modeling\napproaches, datasets, and evaluation metrics. In this work, we propose\n\\textit{instruction-guided content selection (IGCS)} as a beneficial unified\nframework for such settings, where the task definition and any\ninstance-specific request are encapsulated as instructions to a language model.\nTo promote this framework, we introduce \\igcsbench{}, the first unified\nbenchmark covering diverse content selection tasks. Further, we create a large\ngeneric synthetic dataset that can be leveraged for diverse content selection\ntasks, and show that transfer learning with these datasets often boosts\nperformance, whether dedicated training for the targeted task is available or\nnot. Finally, we address generic inference time issues that arise in LLM-based\nmodeling of content selection, assess a generic evaluation metric, and overall\npropose the utility of our resources and methods for future content selection\nmodels. Models and datasets available at https://github.com/shmuelamar/igcs.", "AI": {"tldr": "本文提出了一种统一的NLP内容选择框架IGCS，创建了一个大型通用合成数据集，并展示了通过迁移学习可以改善模型在内容选择任务中的表现。", "motivation": "这篇文章旨在通过一个统一的框架来解决NLP领域的内容选择任务，这些任务通常各自为政，缺乏统一的方法论、数据集和评价标准。通过引入IGCS框架，希望可以将这些任务的研究结合起来，促进跨任务的学习和优化。", "method": "本文提出了基于指令的内容选择（IGCS）框架，该框架将任务定义和实例特定的请求封装为语言模型的指令，作为NLP任务中广泛存在的从给定源文本中选择相关文本片段问题的统一解决方案。此外，作者创建了一个大型的通用合成数据集，用于多种内容选择任务，并展示了这些数据集在没有特定任务训练时也能提升性能的迁移学习能力。", "result": "研究引入了首个统一的内容选择基准测试\textit{igcsbench}，并且发现迁移学习从大型通用合成数据集中往往可以获得更好的性能。同时，文章还解决了一些在基于大规模语言模型的内容选择时出现的推理时间问题。", "conclusion": "总的来说，研究提出了基于指令的内容选择框架IGCS，并引入了相关的资源和方法，为未来的内容选择模型提供了有用的工具和思路。"}}
{"id": "2507.16947", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.16947", "abs": "https://arxiv.org/abs/2507.16947", "authors": ["Robert Korom", "Sarah Kiptinness", "Najib Adan", "Kassim Said", "Catherine Ithuli", "Oliver Rotich", "Boniface Kimani", "Irene King'ori", "Stellah Kamau", "Elizabeth Atemba", "Muna Aden", "Preston Bowman", "Michael Sharman", "Rebecca Soskin Hicks", "Rebecca Distler", "Johannes Heidecke", "Rahul K. Arora", "Karan Singhal"], "title": "AI-based Clinical Decision Support for Primary Care: A Real-World Study", "comment": "Blog: https://openai.com/index/ai-clinical-copilot-penda-health/", "summary": "We evaluate the impact of large language model-based clinical decision\nsupport in live care. In partnership with Penda Health, a network of primary\ncare clinics in Nairobi, Kenya, we studied AI Consult, a tool that serves as a\nsafety net for clinicians by identifying potential documentation and clinical\ndecision-making errors. AI Consult integrates into clinician workflows,\nactivating only when needed and preserving clinician autonomy. We conducted a\nquality improvement study, comparing outcomes for 39,849 patient visits\nperformed by clinicians with or without access to AI Consult across 15 clinics.\nVisits were rated by independent physicians to identify clinical errors.\nClinicians with access to AI Consult made relatively fewer errors: 16% fewer\ndiagnostic errors and 13% fewer treatment errors. In absolute terms, the\nintroduction of AI Consult would avert diagnostic errors in 22,000 visits and\ntreatment errors in 29,000 visits annually at Penda alone. In a survey of\nclinicians with AI Consult, all clinicians said that AI Consult improved the\nquality of care they delivered, with 75% saying the effect was \"substantial\".\nThese results required a clinical workflow-aligned AI Consult implementation\nand active deployment to encourage clinician uptake. We hope this study\ndemonstrates the potential for LLM-based clinical decision support tools to\nreduce errors in real-world settings and provides a practical framework for\nadvancing responsible adoption.", "AI": {"tldr": "通过在肯尼亚诊所的真实环境中使用AI Consult，研究发现这款基于大型语言模型的临床决策工具能够减少诊断和治疗错误，提高医疗质量。", "motivation": "研究的动机在于探索AI支持的临床决策工具在真实世界中的应用效果，具体来说，通过质量改进研究来评估AI Consult如何帮助减少临床决策和文档制定中的错误。", "method": "本研究通过与肯尼亚内罗毕的Penda Health初级诊所网络合作，评估了大型语言模型支持的临床决策工具—AI Consult在实时照护环境中的影响。AI Consult工具旨在识别潜在的文档和临床决策错误，只在必要时激活，保护了临床医生的自主性。", "result": "在比较了15个诊所的39,849次患者访问后发现，使用AI Consult的临床医生出现的错误较少，即诊断错误减少了16%，治疗错误减少了13%。此外，所有使用AI Consult的临床医生认为该工具提高了他们提供的医疗质量，其中75%认为效果显著。", "conclusion": "研究表明，与临床工作流程相适应并积极部署的AI Consult可以提高医疗质量，并且减少诊断和治疗错误，为负责地采纳基于大型语言模型的临床决策支持工具提供了一个实用框架。"}}
{"id": "2507.16951", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.16951", "abs": "https://arxiv.org/abs/2507.16951", "authors": ["Shuyuan Lin", "Lei Duan", "Philip Hughes", "Yuxuan Sheng"], "title": "Harnessing RLHF for Robust Unanswerability Recognition and Trustworthy Response Generation in LLMs", "comment": null, "summary": "Conversational Information Retrieval (CIR) systems, while offering intuitive\naccess to information, face a significant challenge: reliably handling\nunanswerable questions to prevent the generation of misleading or hallucinated\ncontent. Traditional approaches often rely on external classifiers, which can\nintroduce inconsistencies with the core generative Large Language Models\n(LLMs). This paper introduces Self-Aware LLM for Unanswerability (SALU), a\nnovel approach that deeply integrates unanswerability detection directly within\nthe LLM's generative process. SALU is trained using a multi-task learning\nframework for both standard Question Answering (QA) and explicit abstention\ngeneration for unanswerable queries. Crucially, it incorporates a\nconfidence-score-guided reinforcement learning with human feedback (RLHF)\nphase, which explicitly penalizes hallucinated responses and rewards\nappropriate abstentions, fostering intrinsic self-awareness of knowledge\nboundaries. Through extensive experiments on our custom-built\nC-IR_Answerability dataset, SALU consistently outperforms strong baselines,\nincluding hybrid LLM-classifier systems, in overall accuracy for correctly\nanswering or abstaining from questions. Human evaluation further confirms\nSALU's superior reliability, achieving high scores in factuality, appropriate\nabstention, and, most importantly, a dramatic reduction in hallucination,\ndemonstrating its ability to robustly \"know when to say 'I don't know'.\"", "AI": {"tldr": "SALU, a novel method integrating unanswerability detection into LLMs, uses multi-task learning and RLHF to enhance reliability in conversational information retrieval.", "motivation": "To address the challenge of handling unanswerable questions in CIR systems and avoid generating misleading content.", "method": "SALU employs a multi-task learning framework and confidence-score-guided reinforcement learning with human feedback to train LLMs for QA and abstention.", "result": "SALU outperforms baselines, demonstrating high accuracy in answering questions or abstaining when necessary and significantly reducing hallucination.", "conclusion": "SALU improves the reliability of CIR systems by training LLMs to recognize their knowledge boundaries and appropriately abstain from unanswerable questions."}}
{"id": "2507.16971", "categories": ["cs.CL", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2507.16971", "abs": "https://arxiv.org/abs/2507.16971", "authors": ["Aleksandr Perevalov", "Andreas Both"], "title": "Text-to-SPARQL Goes Beyond English: Multilingual Question Answering Over Knowledge Graphs through Human-Inspired Reasoning", "comment": "During the final evaluation on the DBpedia- and Corporate-based KGQA\n  benchmarks within the Text2SPARQL challenge 2025, our approach took first\n  place among the other participants", "summary": "Accessing knowledge via multilingual natural-language interfaces is one of\nthe emerging challenges in the field of information retrieval and related ones.\nStructured knowledge stored in knowledge graphs can be queried via a specific\nquery language (e.g., SPARQL). Therefore, one needs to transform\nnatural-language input into a query to fulfill an information need. Prior\napproaches mostly focused on combining components (e.g., rule-based or\nneural-based) that solve downstream tasks and come up with an answer at the\nend. We introduce mKGQAgent, a human-inspired framework that breaks down the\ntask of converting natural language questions into SPARQL queries into modular,\ninterpretable subtasks. By leveraging a coordinated LLM agent workflow for\nplanning, entity linking, and query refinement - guided by an experience pool\nfor in-context learning - mKGQAgent efficiently handles multilingual KGQA.\nEvaluated on the DBpedia- and Corporate-based KGQA benchmarks within the\nText2SPARQL challenge 2025, our approach took first place among the other\nparticipants. This work opens new avenues for developing human-like reasoning\nsystems in multilingual semantic parsing.", "AI": {"tldr": "论文介绍了一种名为mKGQAgent的框架，旨在通过分解任务来将自然语言问题转化为SPARQL查询，特别适用于多语言知识图谱问答任务。该框架在Text2SPARQL挑战中取得了第一的成绩。", "motivation": "目前的方法多结合成多个组件来解决下游任务，缺乏对任务的分解和模块化处理。因此，提出了mKGQAgent来提升处理多语言知识图谱问答的能力。", "method": "Structure", "result": "{\n  \"tldr\": \"论文介绍了一种名为mKGQAgent的框架，旨在通过分解任务来将自然语言问题转化为SPARQL查询，特别适用于多语言知识图谱问答任务。该框架在Text2SPARQL挑战中取得了第一的成绩。\",\n  \"motivation\": \"目前的方法多结合成多个组件来解决下游任务，缺乏对任务的分解和模块化处理。因此，提出了mKGQAgent来提升处理多语言知识图谱问答的能力。\",\n  \"method\": \"mKGQAgent框架使用一个协调的LLM代理工作流程（包括规划、实体链接和查询优化）来处理任务。通过一个策略池进行上下文学习来指导多个子任务。\",\n  \"result\": \"在DBpedia和Corporate基准上的KGQA任务上，mKGQAgent在Text2SPARQL挑战2025中取得了最好的成绩。\",\n  \"conclusion\": \"这项工作为开发具有人类相似推理能力的多语言语义解析系统开辟了新的可能性。\">\n}\n", "conclusion": "这项工作为开发具有人类相似推理能力的多语言语义解析系统开辟了新的可能性。"}}
{"id": "2507.16849", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.16849", "abs": "https://arxiv.org/abs/2507.16849", "authors": ["Yi-Shan Chu", "Hsuan-Cheng Wei"], "title": "Post-Disaster Affected Area Segmentation with a Vision Transformer (ViT)-based EVAP Model using Sentinel-2 and Formosat-5 Imagery", "comment": null, "summary": "We propose a vision transformer (ViT)-based deep learning framework to refine\ndisaster-affected area segmentation from remote sensing imagery, aiming to\nsupport and enhance the Emergent Value Added Product (EVAP) developed by the\nTaiwan Space Agency (TASA). The process starts with a small set of manually\nannotated regions. We then apply principal component analysis (PCA)-based\nfeature space analysis and construct a confidence index (CI) to expand these\nlabels, producing a weakly supervised training set. These expanded labels are\nthen used to train ViT-based encoder-decoder models with multi-band inputs from\nSentinel-2 and Formosat-5 imagery. Our architecture supports multiple decoder\nvariants and multi-stage loss strategies to improve performance under limited\nsupervision. During the evaluation, model predictions are compared with\nhigher-resolution EVAP output to assess spatial coherence and segmentation\nconsistency. Case studies on the 2022 Poyang Lake drought and the 2023 Rhodes\nwildfire demonstrate that our framework improves the smoothness and reliability\nof segmentation results, offering a scalable approach for disaster mapping when\naccurate ground truth is unavailable.", "AI": {"tldr": "提出了一种基于视觉变换器的深度学习框架，用于从遥感图像中精炼灾害影响区域分割，支持台湾太空局开发的应急增值服务。使用少量手动标注区域，通过主成分分析和置信度指数扩展标签，训练多波段输入的视觉变换器模型，提高分割结果的平滑度和可靠性。案例研究证明了其在灾害制图中的可扩展性和有效性。", "motivation": "旨在改进遥感图像中灾害影响区域的分割，特别是在准确的实地数据不可用时支持更可靠的灾害测绘分析。", "method": "Structure", "result": "{\n  \"tldr\": \"提出了一种基于视觉变换器的深度学习框架，用于从遥感图像中精炼灾害影响区域分割，支持台湾太空局开发的应急增值服务。使用少量手动标注区域，通过主成分分析和置信度指数扩展标签，训练多波段输入的视觉变换器模型，提高分割结果的平滑度和可靠性。案例研究证明了其在灾害制图中的可扩展性和有效性。\", \n  \"motivation\": \"旨在改进遥感图像中灾害影响区域的分割，特别是在准确的实地数据不可用时支持更可靠的灾害测绘分析。\", \n  \"method\": \"采用视觉变换器框架配合主成分分析以及置信度指数方法进行弱监督学习，使用Sentinel-2和Formosat-5的多波段数据训练多解码器的视觉变换器模型，并采用多阶段损失策略。\", \n  \"result\": \"实验结果应用于2022年鄱阳湖干旱和2023年罗得岛火灾的案例研究中，提高了分割结果的空间一致性和可靠性。\", \n  \"conclusion\": \"提出的方法可以为灾害影响区域的分割提供一个有效的解决方案，特别是在准确的地面实况无法获得时提供了一种可扩展且可靠的灾害制图方法。\"}\n}", "conclusion": "提出的方法可以为灾害影响区域的分割提供一个有效的解决方案，特别是在准确的地面实况无法获得时提供了一种可扩展且可靠的灾害制图方法。"}}
{"id": "2507.16974", "categories": ["cs.CL", "cs.AI", "I.2.7; J.m"], "pdf": "https://arxiv.org/pdf/2507.16974", "abs": "https://arxiv.org/abs/2507.16974", "authors": ["Rishemjit Kaur", "Arshdeep Singh Bhankhar", "Surangika Ranathunga", "Jashanpreet Singh Salh", "Sudhir Rajput", "Vidhi", "Kashish Mahendra", "Bhavika Berwal", "Ritesh Kumar"], "title": "Leveraging Synthetic Data for Question Answering with Multilingual LLMs in the Agricultural Domain", "comment": "15 pages, 9 tables, Appendix A-K", "summary": "Enabling farmers to access accurate agriculture-related information in their\nnative languages in a timely manner is crucial for the success of the\nagriculture field. Although large language models (LLMs) can be used to\nimplement Question Answering (QA) systems, simply using publicly available\ngeneral-purpose LLMs in agriculture typically offer generic advisories, lacking\nprecision in local and multilingual contexts due to insufficient\ndomain-specific training and scarcity of high-quality, region-specific\ndatasets. Our study addresses these limitations by generating multilingual\nsynthetic agricultural datasets (English, Hindi, Punjabi) from\nagriculture-specific documents and fine-tuning language-specific LLMs. Our\nevaluation on curated multilingual datasets demonstrates significant\nimprovements in factual accuracy, relevance, and agricultural consensus for the\nfine-tuned models compared to their baseline counterparts. These results\nhighlight the efficacy of synthetic data-driven, language-specific fine-tuning\nas an effective strategy to improve the performance of LLMs in agriculture,\nespecially in multilingual and low-resource settings. By enabling more accurate\nand localized agricultural advisory services, this study provides a meaningful\nstep toward bridging the knowledge gap in AI-driven agricultural solutions for\ndiverse linguistic communities.", "AI": {"tldr": "研究生成多语言合成农业数据集并对语言特定的LLMs进行微调，显著提高了LLMs在农业领域尤其是多语言和低资源环境下的性能。", "motivation": "现有的公共通用大型语言模型在农业领域通常缺乏精确的地方性和多语言上下文建议，且在农业特定训练和高质量、地区特定数据方面不足。", "method": "通过生成多语言的合成农业数据集（英语、印地语、旁遮普语）来自农业特定文档，并对语言特定的大型语言模型（LLMs）进行微调。", "result": "在精心策划的多语言数据集上的评估显示，微调模型在事实准确性、相关性和农业共识方面比基线模型有显著提高。", "conclusion": "该研究通过合成数据驱动的、语言特定的微调策略，大幅提升了LLMs在农业领域表现，特别是在多语言和低资源设置中的性能。"}}
{"id": "2507.16850", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.16850", "abs": "https://arxiv.org/abs/2507.16850", "authors": ["Mohamed Adjel"], "title": "Toward a Real-Time Framework for Accurate Monocular 3D Human Pose Estimation with Geometric Priors", "comment": "IEEE ICRA 2025 (workshop: Enhancing Human Mobility: From Computer\n  Vision-Based Motion Tracking to Wearable Assistive Robot Control), May 2025,\n  Atlanta (Georgia), United States", "summary": "Monocular 3D human pose estimation remains a challenging and ill-posed\nproblem, particularly in real-time settings and unconstrained environments.\nWhile direct imageto-3D approaches require large annotated datasets and heavy\nmodels, 2D-to-3D lifting offers a more lightweight and flexible\nalternative-especially when enhanced with prior knowledge. In this work, we\npropose a framework that combines real-time 2D keypoint detection with\ngeometry-aware 2D-to-3D lifting, explicitly leveraging known camera intrinsics\nand subject-specific anatomical priors. Our approach builds on recent advances\nin self-calibration and biomechanically-constrained inverse kinematics to\ngenerate large-scale, plausible 2D-3D training pairs from MoCap and synthetic\ndatasets. We discuss how these ingredients can enable fast, personalized, and\naccurate 3D pose estimation from monocular images without requiring specialized\nhardware. This proposal aims to foster discussion on bridging data-driven\nlearning and model-based priors to improve accuracy, interpretability, and\ndeployability of 3D human motion capture on edge devices in the wild.", "AI": {"tldr": "提出一种结合实时2D关键点检测和几何感知的2D到3D提升方法，使用相机内在参数和个体特定的解剖学先验知识来改善单目3D人体姿态估计的准确性。", "motivation": "单目3D人体姿态估计算法在实时设置和不受限制的环境中仍然是一个具有挑战性和病态的问题。直接的图像到3D方法需要大量的标注数据集和重型模型，而2D到3D的提升提供了一个更加轻量级和灵活的替代方案，尤其是在增强先验知识的情况下。", "method": "本研究提出了一种框架，结合实时2D关键点检测与考虑几何的2D到3D提升方法，并明确利用已知的相机内在参数和个体特定的解剖学先验。这种方法基于自校准和生物力学约束逆运动学的最新进展，从MoCap和合成数据集中生成大规模、合理的2D-3D训练样本对。", "result": "这种方法可以实现从单目图像中快速、个性化且准确的3D姿态估计，而无需专门的硬件。", "conclusion": "该提案旨在促进以数据驱动的学习和模型基础的先验知识相结合的方法，以提高3D人体运动捕捉的准确性、可解释性和部署能力，特别是在边缘设备上的应用。"}}
{"id": "2507.16989", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.16989", "abs": "https://arxiv.org/abs/2507.16989", "authors": ["Giulio Pelosio", "Devesh Batra", "Noémie Bovey", "Robert Hankache", "Cristovao Iglesias", "Greig Cowan", "Raad Khraishi"], "title": "Obscured but Not Erased: Evaluating Nationality Bias in LLMs via Name-Based Bias Benchmarks", "comment": null, "summary": "Large Language Models (LLMs) can exhibit latent biases towards specific\nnationalities even when explicit demographic markers are not present. In this\nwork, we introduce a novel name-based benchmarking approach derived from the\nBias Benchmark for QA (BBQ) dataset to investigate the impact of substituting\nexplicit nationality labels with culturally indicative names, a scenario more\nreflective of real-world LLM applications. Our novel approach examines how this\nsubstitution affects both bias magnitude and accuracy across a spectrum of LLMs\nfrom industry leaders such as OpenAI, Google, and Anthropic. Our experiments\nshow that small models are less accurate and exhibit more bias compared to\ntheir larger counterparts. For instance, on our name-based dataset and in the\nambiguous context (where the correct choice is not revealed), Claude Haiku\nexhibited the worst stereotypical bias scores of 9%, compared to only 3.5% for\nits larger counterpart, Claude Sonnet, where the latter also outperformed it by\n117.7% in accuracy. Additionally, we find that small models retain a larger\nportion of existing errors in these ambiguous contexts. For example, after\nsubstituting names for explicit nationality references, GPT-4o retains 68% of\nthe error rate versus 76% for GPT-4o-mini, with similar findings for other\nmodel providers, in the ambiguous context. Our research highlights the stubborn\nresilience of biases in LLMs, underscoring their profound implications for the\ndevelopment and deployment of AI systems in diverse, global contexts.", "AI": {"tldr": "研究展示了小型和大型语言模型在潜在国籍偏见和准确性方面的差异，强调了偏见问题的重要性。", "motivation": "研究大型语言模型在没有显式的人口统计标记时仍然可能存在的国籍偏见问题。", "method": "提出了一种基于名字的基准测试方法，该方法源自于BBQ数据集，用于研究用文化指示性名字代替显式的国籍标签对偏见和准确性的影响。", "result": "小型模型在准确性方面表现较差且表现出更多的偏见。例如，在模糊的背景下，Claude Haiku的刻板印象偏见评分为9%，而更大的模型Claude Sonnet的评分为3.5%，且在准确性上高出117.7%。此外，小模型在这些模糊背景下保留了更大比例的现有错误。", "conclusion": "研究表明，即使使用了文化指示性名字代替国籍标签，大型语言模型中的偏见仍然存在，这强调了这种偏见对于开发和部署全球范围内的AI系统的重要影响。"}}
{"id": "2507.16851", "categories": ["cs.CV", "cs.NE", "eess.IV"], "pdf": "https://arxiv.org/pdf/2507.16851", "abs": "https://arxiv.org/abs/2507.16851", "authors": ["Zelong Liu", "Yuliang Gu", "Zhichao Sun", "Huachao Zhu", "Xin Xiao", "Bo Du", "Laurent Najman", "Yongchao Xu"], "title": "Coarse-to-fine crack cue for robust crack detection", "comment": null, "summary": "Crack detection is an important task in computer vision. Despite impressive\nin-dataset performance, deep learning-based methods still struggle in\ngeneralizing to unseen domains. The thin structure property of cracks is\nusually overlooked by previous methods. In this work, we introduce CrackCue, a\nnovel method for robust crack detection based on coarse-to-fine crack cue\ngeneration. The core concept lies on leveraging the thin structure property to\ngenerate a robust crack cue, guiding the crack detection. Specifically, we\nfirst employ a simple max-pooling and upsampling operation on the crack image.\nThis results in a coarse crack-free background, based on which a fine\ncrack-free background can be obtained via a reconstruction network. The\ndifference between the original image and fine crack-free background provides a\nfine crack cue. This fine cue embeds robust crack prior information which is\nunaffected by complex backgrounds, shadow, and varied lighting. As a\nplug-and-play method, we incorporate the proposed CrackCue into three advanced\ncrack detection networks. Extensive experimental results demonstrate that the\nproposed CrackCue significantly improves the generalization ability and\nrobustness of the baseline methods. The source code will be publicly available.", "AI": {"tldr": "CrackCue是一种新的裂缝检测方法，它利用裂缝的细长结构特性来生成更精确的裂缝提示，并通过整合到高级检测网络中显著提高裂缝检测的泛化能力和鲁棒性。", "motivation": "尽管深度学习方法在数据集内表现优异，但在未见领域仍存在泛化能力不足的问题，且之前的裂缝检测方法通常忽视了裂缝的细长结构特性，因此该论文旨在解决这些问题，提高裂缝检测的泛化能力和鲁棒性。", "method": "本论文提出的方法CrackCue首先采用了简单最大的池化和上采样操作来处理裂缝图像，以生成一个粗糙的无裂缝背景。随后，通过重建网络获取一个精细的无裂缝背景。原始图像与精细无裂缝背景之间的差异形成精细的裂缝提示，该裂缝提示嵌入了稳定的裂缝先验信息，不受复杂背景等影响。", "result": "主要内容：本篇论文提出了一种新的裂缝检测方法CrackCue，基于粗到细的裂缝提示生成。该方法利用裂缝的细长结构特性，首先使用简单的最大池化和上采样操作对裂缝图像进行处理，生成一个粗糙的无裂缝背景，然后通过一个重建网络获得一个更精细的无裂缝背景。原始图像与精细无裂缝背景之间的差异提供了一个精细的裂缝提示，这个提示包含稳定的裂缝先验信息，不受复杂背景、阴影和变化的光照的影响。作为即插即用的方法，CrackCue被整合到三个高级裂缝检测网络中，实验结果表明，CrackCue显著提高了基准方法的泛化能力和鲁棒性。该源代码将会公开。", "conclusion": "实验显示，通过将CrackCue整合到三个高级裂缝检测网络中，其显著提升了方法的泛化能力和鲁棒性。"}}
{"id": "2507.17009", "categories": ["cs.CL", "cs.IR", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2507.17009", "abs": "https://arxiv.org/abs/2507.17009", "authors": ["Ming Huang", "Zehan Li", "Yan Hu", "Wanjing Wang", "Andrew Wen", "Scott Lane", "Salih Selek", "Lokesh Shahani", "Rodrigo Machado-Vieira", "Jair Soares", "Hua Xu", "Hongfang Liu"], "title": "Multi-Label Classification with Generative AI Models in Healthcare: A Case Study of Suicidality and Risk Factors", "comment": null, "summary": "Suicide remains a pressing global health crisis, with over 720,000 deaths\nannually and millions more affected by suicide ideation (SI) and suicide\nattempts (SA). Early identification of suicidality-related factors (SrFs),\nincluding SI, SA, exposure to suicide (ES), and non-suicidal self-injury\n(NSSI), is critical for timely intervention. While prior studies have applied\nAI to detect SrFs in clinical notes, most treat suicidality as a binary\nclassification task, overlooking the complexity of cooccurring risk factors.\nThis study explores the use of generative large language models (LLMs),\nspecifically GPT-3.5 and GPT-4.5, for multi-label classification (MLC) of SrFs\nfrom psychiatric electronic health records (EHRs). We present a novel end to\nend generative MLC pipeline and introduce advanced evaluation methods,\nincluding label set level metrics and a multilabel confusion matrix for error\nanalysis. Finetuned GPT-3.5 achieved top performance with 0.94 partial match\naccuracy and 0.91 F1 score, while GPT-4.5 with guided prompting showed superior\nperformance across label sets, including rare or minority label sets,\nindicating a more balanced and robust performance. Our findings reveal\nsystematic error patterns, such as the conflation of SI and SA, and highlight\nthe models tendency toward cautious over labeling. This work not only\ndemonstrates the feasibility of using generative AI for complex clinical\nclassification tasks but also provides a blueprint for structuring unstructured\nEHR data to support large scale clinical research and evidence based medicine.", "AI": {"tldr": "研究使用GPT-3.5和GPT-4.5对自杀相关因素进行多标签分类，从EHRs中提取信息，展示了生成式AI在临床分类任务中的潜力。", "motivation": "研究动机是应对忽视自杀相关因素复杂性的二分类任务问题，提出了一种新的端到端生成式MLC流水线，并引入了先进的评估方法，如标签集级评估指标和多标签混淆矩阵进行错误分析。", "method": "该研究采用GPT-3.5和GPT-4.5等生成式大型语言模型（LLMs），用于从精神科电子健康记录（EHRs）中进行自杀相关因素（SrFs）的多标签分类（MLC）。", "result": "微调后的GPT-3.5达到了0.94的部分匹配准确率和0.91的F1分数，而GPT-4.5在引导提示下展示出了跨越多个标签集的优越性能，包括罕见或少数标签集，显示了更平衡和稳健的表现。", "conclusion": "该研究发现系统性错误模式，如自杀观念(SI)与自杀企图(SA)之间的混淆，并指出模型存在谨慎的过度标注倾向。这不仅展示了生成式AI在复杂临床分类任务中的可行性，还提供了一个结构化无结构EHR数据的蓝图，以支持大规模临床研究和基于证据的医学。"}}
{"id": "2507.16854", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.16854", "abs": "https://arxiv.org/abs/2507.16854", "authors": ["Xiaoqiang He"], "title": "CLAMP: Contrastive Learning with Adaptive Multi-loss and Progressive Fusion for Multimodal Aspect-Based Sentiment Analysis", "comment": null, "summary": "Multimodal aspect-based sentiment analysis(MABSA) seeks to identify aspect\nterms within paired image-text data and determine their fine grained sentiment\npolarities, representing a fundamental task for improving the effectiveness of\napplications such as product review systems and public opinion monitoring.\nExisting methods face challenges such as cross modal alignment noise and\ninsufficient consistency in fine-grained representations. While global modality\nalignment methods often overlook the connection between aspect terms and their\ncorresponding local visual regions, bridging the representation gap between\ntext and images remains a challenge. To address these limitations, this paper\nintroduces an end to end Contrastive Learning framework with Adaptive\nMulti-loss and Progressive Attention Fusion(CLAMP). The framework is composed\nof three novel modules: Progressive Attention Fusion network, Multi-task\nContrastive Learning, and Adaptive Multi-loss Aggregation. The Progressive\nAttention Fusion network enhances fine-grained alignment between textual\nfeatures and image regions via hierarchical, multi-stage cross modal\ninteractions, effectively suppressing irrelevant visual noise. Secondly,\nmulti-task contrastive learning combines global modal contrast and local\ngranularity alignment to enhance cross modal representation consistency.\nAdaptive Multi-loss Aggregation employs a dynamic uncertainty based weighting\nmechanism to calibrate loss contributions according to each task's uncertainty,\nthereby mitigating gradient interference. Evaluation on standard public\nbenchmarks demonstrates that CLAMP consistently outperforms the vast majority\nof existing state of the art methods.", "AI": {"tldr": "The paper presents CLAMP, a new framework for multimodal aspect-based sentiment analysis, which improves cross-modal alignment and reduces noise, outperforming many existing methods in benchmarks.", "motivation": "The motivation behind the paper is to address the limitations of current multimodal aspect-based sentiment analysis methods, particularly challenges related to cross-modal alignment noise and inconsistent fine-grained representations.", "method": "The paper introduces CLAMP, which includes three modules: Progressive Attention Fusion network, Multi-task Contrastive Learning, and Adaptive Multi-loss Aggregation. This framework aims to enhance fine-grained alignment and consistency between textual and visual data, mitigating issues such as irrelevant visual noise and gradient interference.", "result": "Evaluation results on standard public benchmarks indicate that CLAMP outperforms most existing state-of-the-art methods in multimodal aspect-based sentiment analysis.", "conclusion": "The conclusion of the paper is that CLAMP, an end-to-end framework with adaptive multi-loss and progressive attention fusion, effectively enhances cross-modal representation consistency and mitigates irrelevant visual noise, leading to improved performance in multimodal aspect-based sentiment analysis."}}
{"id": "2507.17015", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.17015", "abs": "https://arxiv.org/abs/2507.17015", "authors": ["Arduin Findeis", "Floris Weers", "Guoli Yin", "Ke Ye", "Ruoming Pang", "Tom Gunter"], "title": "Can External Validation Tools Improve Annotation Quality for LLM-as-a-Judge?", "comment": "Accepted at ACL 2025", "summary": "Pairwise preferences over model responses are widely collected to evaluate\nand provide feedback to large language models (LLMs). Given two alternative\nmodel responses to the same input, a human or AI annotator selects the \"better\"\nresponse. This approach can provide feedback for domains where other hard-coded\nmetrics are difficult to obtain (e.g., chat response quality), thereby helping\nmodel evaluation or training. However, for some domains high-quality pairwise\ncomparisons can be tricky to obtain - from AI and humans. For example, for\nresponses with many factual statements, annotators may disproportionately weigh\nwriting quality rather than underlying facts. In this work, we explore\naugmenting standard AI annotator systems with additional tools to improve\nperformance on three challenging response domains: long-form factual, math and\ncode tasks. We propose a tool-using agentic system to provide higher quality\nfeedback on these domains. Our system uses web-search and code execution to\nground itself based on external validation, independent of the LLM's internal\nknowledge and biases. We provide extensive experimental results evaluating our\nmethod across the three targeted response domains as well as general annotation\ntasks, using RewardBench (incl. AlpacaEval and LLMBar), RewardMath, as well as\nthree new datasets for domains with saturated pre-existing datasets. Our\nresults indicate that external tools can indeed improve performance in many,\nbut not all, cases. More generally, our experiments highlight the sensitivity\nof performance to simple parameters (e.g., prompt) and the need for improved\n(non-saturated) annotator benchmarks. We share our code at\nhttps://github.com/apple/ml-agent-evaluator.", "AI": {"tldr": "本研究提出了一种工具辅助的标注系统，通过引入网络搜索和代码执行工具，以提高在长篇事实性、数学和编程任务上的标注质量。", "motivation": "本研究旨在解决在某些领域难以获得高质量成对比较标注的问题，特别是针对含有大量事实陈述的响应。在这种情况下，标注者可能过多地重视写作质量而非内容的真实性。", "method": "本研究通过引入基于网络搜索和代码执行的工具，以提高在长篇事实性、数学和编程任务上的人工标注系统的性能。这些工具独立于语言模型的内部知识和偏见，旨在为这些特定领域提供更高质量的反馈。", "result": "实验结果显示，这些外部工具确实可以在许多情况下提升标注性能，但并非在所有情况下均有效。此外，实验还强调了性能对简单参数（如提示语）的敏感性和需要改进的标注基准。", "conclusion": "本研究的结果支持在特定任务中使用外部工具可以改善人工标注的质量，但也表明标注质量高度依赖于实验设计中的具体参数和非饱和标注基准。"}}
{"id": "2507.16856", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.16856", "abs": "https://arxiv.org/abs/2507.16856", "authors": ["Youngjin Na", "Sangheon Jeong", "Youngwan Lee"], "title": "SIA: Enhancing Safety via Intent Awareness for Vision-Language Models", "comment": "5 pages, 6 figures", "summary": "As vision-language models (VLMs) are increasingly deployed in real-world\napplications, new safety risks arise from the subtle interplay between images\nand text. In particular, seemingly innocuous inputs can combine to reveal\nharmful intent, leading to unsafe model responses. Despite increasing attention\nto multimodal safety, previous approaches based on post hoc filtering or static\nrefusal prompts struggle to detect such latent risks, especially when\nharmfulness emerges only from the combination of inputs. We propose SIA (Safety\nvia Intent Awareness), a training-free prompt engineering framework that\nproactively detects and mitigates harmful intent in multimodal inputs. SIA\nemploys a three-stage reasoning process: (1) visual abstraction via captioning,\n(2) intent inference through few-shot chain-of-thought prompting, and (3)\nintent-conditioned response refinement. Rather than relying on predefined rules\nor classifiers, SIA dynamically adapts to the implicit intent inferred from the\nimage-text pair. Through extensive experiments on safety-critical benchmarks\nincluding SIUO, MM-SafetyBench, and HoliSafe, we demonstrate that SIA achieves\nsubstantial safety improvements, outperforming prior methods. Although SIA\nshows a minor reduction in general reasoning accuracy on MMStar, the\ncorresponding safety gains highlight the value of intent-aware reasoning in\naligning VLMs with human-centric values.", "AI": {"tldr": "The paper introduces SIA, a dynamic, training-free framework for detecting and mitigating harmful intent in vision-language models by analyzing the implicit intent of image-text inputs.", "motivation": "To address the safety risks arising from the interplay between images and text in vision-language models, which cannot be adequately mitigated by previous approaches based on post hoc filtering or static refusal prompts.", "method": "SIA (Safety via Intent Awareness), a training-free prompt engineering framework that employs a three-stage reasoning process: (1) visual abstraction via captioning, (2) intent inference through few-shot chain-of-thought prompting, and (3) intent-conditioned response refinement.", "result": "SIA achieves substantial safety improvements on benchmarks like SIUO, MM-SafetyBench, and HoliSafe, while showing a minor reduction in general reasoning accuracy on MMStar compared to prior methods.", "conclusion": "SIA outperforms prior methods on safety-critical benchmarks, demonstrating the value of intent-aware reasoning in aligning vision-language models with human-centric values, despite a minor reduction in general reasoning accuracy."}}
{"id": "2507.17025", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.17025", "abs": "https://arxiv.org/abs/2507.17025", "authors": ["Soumen Sinha", "Shahryar Rahnamayan", "Azam Asilian Bidgoli"], "title": "Evolutionary Feature-wise Thresholding for Binary Representation of NLP Embeddings", "comment": null, "summary": "Efficient text embedding is crucial for large-scale natural language\nprocessing (NLP) applications, where storage and computational efficiency are\nkey concerns. In this paper, we explore how using binary representations\n(barcodes) instead of real-valued features can be used for NLP embeddings\nderived from machine learning models such as BERT. Thresholding is a common\nmethod for converting continuous embeddings into binary representations, often\nusing a fixed threshold across all features. We propose a Coordinate\nSearch-based optimization framework that instead identifies the optimal\nthreshold for each feature, demonstrating that feature-specific thresholds lead\nto improved performance in binary encoding. This ensures that the binary\nrepresentations are both accurate and efficient, enhancing performance across\nvarious features. Our optimal barcode representations have shown promising\nresults in various NLP applications, demonstrating their potential to transform\ntext representation. We conducted extensive experiments and statistical tests\non different NLP tasks and datasets to evaluate our approach and compare it to\nother thresholding methods. Binary embeddings generated using using optimal\nthresholds found by our method outperform traditional binarization methods in\naccuracy. This technique for generating binary representations is versatile and\ncan be applied to any features, not just limited to NLP embeddings, making it\nuseful for a wide range of domains in machine learning applications.", "AI": {"tldr": "This paper explores a method to improve the efficiency of text embeddings in NLP by using binary representations with feature-specific thresholds, demonstrating its superiority over traditional methods in terms of accuracy.", "motivation": "The motivation is to address the issues of storage and computational efficiency in large-scale NLP applications by using more efficient binary representations for text embeddings, which are typically real-valued.", "method": "The paper introduces a Coordinate Search-based optimization framework for identifying optimal thresholds to convert continuous embeddings, such as those from BERT, into binary representations. This method applies feature-specific thresholds to improve the performance of binary encoding.", "result": "The optimal barcode representations proposed in the paper showed improved performance in accuracy when compared to traditional binarization methods. The experiments encompassed various NLP tasks and datasets, indicating that the method can achieve better binary encoding and is superior in diverse application scenarios.", "conclusion": "The conclusion is that using feature-specific optimal thresholds in the Coordinate Search-based framework produces higher quality binary representations, which can lead to improved performance in NLP tasks and potentially across other domains in machine learning."}}
{"id": "2507.16861", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.16861", "abs": "https://arxiv.org/abs/2507.16861", "authors": ["Xiang Li"], "title": "Look Before You Fuse: 2D-Guided Cross-Modal Alignment for Robust 3D Detection", "comment": null, "summary": "Integrating LiDAR and camera inputs into a unified Bird's-Eye-View (BEV)\nrepresentation is crucial for enhancing 3D perception capabilities of\nautonomous vehicles. However, current methods are often affected by\nmisalignment between camera and LiDAR features. This misalignment leads to\ninaccurate depth supervision in camera branch and erroneous fusion during\ncross-modal feature aggregation. The root cause of this misalignment lies in\nprojection errors, stemming from minor extrinsic calibration inaccuracies and\nrolling shutter effect of LiDAR during vehicle motion. In this work, our key\ninsight is that these projection errors are predominantly concentrated at\nobject-background boundaries, which are readily identified by 2D detectors.\nBased on this, our main motivation is to utilize 2D object priors to pre-align\ncross-modal features before fusion. To address local misalignment, we propose\nPrior Guided Depth Calibration (PGDC), which leverages 2D priors to correct\nlocal misalignment and preserve correct cross-modal feature pairs. To resolve\nglobal misalignment, we introduce Discontinuity Aware Geometric Fusion (DAGF)\nto process calibrated results from PGDC, suppressing noise and explicitly\nenhancing sharp transitions at object-background boundaries. To effectively\nutilize these transition-aware depth representations, we incorporate Structural\nGuidance Depth Modulator (SGDM), using a gated attention mechanism to\nefficiently fuse aligned depth and image features. Our proposed method achieves\nstate-of-the-art performance on nuScenes validation dataset, with its mAP and\nNDS reaching 71.5% and 73.6% respectively.", "AI": {"tldr": "通过使用2D物体先验进行跨模态特征预对齐，该论文提出的方法在nuScenes数据集上实现了71.5%的mAP和73.6%的NDS，显著提升了3D感知性能。", "motivation": "论文的主要动机是使用2D物体先验来在跨模态特征融合之前预对齐激光雷达和摄像头输出，以解决因为投影误差导致的特征融合问题，这些误差主要出现在物体-背景边界。", "method": "论文的方法包括两个组成部分：Prior Guided Depth Calibration (PGDC)用于修正局部对齐中的误差，而Discontinuity Aware Geometric Fusion (DAGF)用于处理经过PGDC修正的结果，以增强边缘清晰度并减少噪声。", "result": "该论文提出了一种新的方法来解决在结合激光雷达和摄像头输入生成鸟瞰图（BEV）表示时出现的对齐问题，这些问题通常会导致融合过程中的深度监督不准确。提出的方法包括两步：首先利用2D物体先验来预对齐跨模态特征，解决局部对齐问题；其次通过一种新的几何融合方法解决全局对齐问题，以处理那些经过预对齐的特征，同时增强物体-背景边界上的明显过渡。实验结果表明，该方法在nuScenes验证数据集上达到了最先进的性能，mAP和NDS分别达到了71.5%和73.6%。", "conclusion": "这篇论文证明了通过先验引导深度校准和间断敏感几何融合的方法可以改善激光雷达和摄像头特征融合中的对齐问题，在nuScenes数据集上实现了state-of-the-art的性能。"}}
{"id": "2507.17147", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.17147", "abs": "https://arxiv.org/abs/2507.17147", "authors": ["Cheng Liu", "Yifei Lu", "Fanghua Ye", "Jian Li", "Xingyu Chen", "Feiliang Ren", "Zhaopeng Tu", "Xiaolong Li"], "title": "CogDual: Enhancing Dual Cognition of LLMs via Reinforcement Learning with Implicit Rule-Based Rewards", "comment": null, "summary": "Role-Playing Language Agents (RPLAs) have emerged as a significant\napplication direction for Large Language Models (LLMs). Existing approaches\ntypically rely on prompt engineering or supervised fine-tuning to enable models\nto imitate character behaviors in specific scenarios, but often neglect the\nunderlying \\emph{cognitive} mechanisms driving these behaviors. Inspired by\ncognitive psychology, we introduce \\textbf{CogDual}, a novel RPLA adopting a\n\\textit{cognize-then-respond } reasoning paradigm. By jointly modeling external\nsituational awareness and internal self-awareness, CogDual generates responses\nwith improved character consistency and contextual alignment. To further\noptimize the performance, we employ reinforcement learning with two\ngeneral-purpose reward schemes designed for open-domain text generation.\nExtensive experiments on the CoSER benchmark, as well as Cross-MR and\nLifeChoice, demonstrate that CogDual consistently outperforms existing\nbaselines and generalizes effectively across diverse role-playing tasks.", "AI": {"tldr": "本文提出CogDual，一种新的角色扮演语言代理，它通过认知-响应推理范式，结合情景意识和自我意识，使用强化学习优化性能。实验中，CogDual展现了卓越的性能。", "motivation": "现有的角色扮演语言代理（RPLAs）侧重于模仿特定场景中的角色行为，但忽略了这些行为背后的认知机制。本文旨在通过引入CogDual填补这一空白，并提高RPLAs的表现。", "method": "CogDual, 采用认知-响应推理范式，结合外部情景意识和内部自我意识来生成回答，以提高人物一致性和情境一致性。同时使用了强化学习和两个通用奖励机制来优化性能。", "result": "实验结果表明，CogDual在CoSER基准和Cross-MR以及LifeChoice数据集上的表现优于现有基线，并且在不同的角色扮演任务中展现了良好的泛化能力。", "conclusion": "CogDual的有效性和泛化能力表明，通过引入认知机制，角色扮演语言代理能够获得更好的表现。此方法提供了一个新的视角来改进大型语言模型的行为模拟。"}}
{"id": "2507.16863", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.16863", "abs": "https://arxiv.org/abs/2507.16863", "authors": ["Hongcheng Gao", "Zihao Huang", "Lin Xu", "Jingyi Tang", "Xinhao Li", "Yue Liu", "Haoyang Li", "Taihang Hu", "Minhua Lin", "Xinlong Yang", "Ge Wu", "Balong Bi", "Hongyu Chen", "Wentao Zhang"], "title": "Pixels, Patterns, but No Poetry: To See The World like Humans", "comment": null, "summary": "Achieving human-like perception and reasoning in Multimodal Large Language\nModels (MLLMs) remains a central challenge in artificial intelligence. While\nrecent research has primarily focused on enhancing reasoning capabilities in\nMLLMs, a fundamental question persists: Can Multimodal Large Language Models\ntruly perceive the world as humans do? This paper shifts focus from reasoning\nto perception. Rather than constructing benchmarks specifically for reasoning,\nwe introduce the Turing Eye Test (TET), a challenging perception-oriented\nbenchmark comprising four diagnostic tasks that evaluate MLLMs' performance on\nsynthetic images that humans process intuitively. Our findings reveal that\nstate-of-the-art MLLMs exhibit catastrophic failures on our perceptual tasks\ntrivial for humans. Both in-context learning and training on language\nbackbone-effective for previous benchmarks-fail to improve performance on our\ntasks, while fine-tuning the vision tower enables rapid adaptation, suggesting\nthat our benchmark poses challenges for vision tower generalization rather than\nfor the knowledge and reasoning capabilities of the language backbone-a key gap\nbetween current MLLMs and human perception. We release a representative subset\nof TET tasks in this version, and will introduce more diverse tasks and methods\nto enhance visual generalization in future work.", "AI": {"tldr": "本篇论文提出了Turing Eye Test (TET)，一个新的用于评估多模态语言模型视觉感知能力的基准测试，并揭示了当前模型在处理这些任务上的巨大差距。", "motivation": "该研究旨在转向多模态大型语言模型的感知能力研究，而不是以往主要关注的推理能力。特别是，研究关注模型是否能像人类一样“看到”世界，并揭示了当前最先进模型在这方面的严重不足。", "method": "本论文介绍了Turing Eye Test (TET)，一个旨在评估大型多模态语言模型视觉感知能力的新基准测试。TET包含了四个诊断任务，这些任务涉及的是人类可以直观处理的合成图像。", "result": "研究结果揭示，即使是当前最先进的MLLMs也在TET的感知任务上表现出灾难性的失败，这些任务对于人类来说却十分简单。进一步分析显示，对于语言主干训练或在上下文中的学习并不能改善在此任务上的表现。", "conclusion": "研究表明，视觉塔的微调能够快速提升在这些任务上的性能，暗示这些任务更多挑战的是视觉塔的泛化能力，而非语言主干的知识和推理能力。这揭示了当前MLLMs与人类感知能力之间的关键差距。"}}
{"id": "2507.17178", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.17178", "abs": "https://arxiv.org/abs/2507.17178", "authors": ["Zhiqiang Liu", "Enpei Niu", "Yin Hua", "Mengshu Sun", "Lei Liang", "Huajun Chen", "Wen Zhang"], "title": "SKA-Bench: A Fine-Grained Benchmark for Evaluating Structured Knowledge Understanding of LLMs", "comment": null, "summary": "Although large language models (LLMs) have made significant progress in\nunderstanding Structured Knowledge (SK) like KG and Table, existing evaluations\nfor SK understanding are non-rigorous (i.e., lacking evaluations of specific\ncapabilities) and focus on a single type of SK. Therefore, we aim to propose a\nmore comprehensive and rigorous structured knowledge understanding benchmark to\ndiagnose the shortcomings of LLMs. In this paper, we introduce SKA-Bench, a\nStructured Knowledge Augmented QA Benchmark that encompasses four widely used\nstructured knowledge forms: KG, Table, KG+Text, and Table+Text. We utilize a\nthree-stage pipeline to construct SKA-Bench instances, which includes a\nquestion, an answer, positive knowledge units, and noisy knowledge units. To\nevaluate the SK understanding capabilities of LLMs in a fine-grained manner, we\nexpand the instances into four fundamental ability testbeds: Noise Robustness,\nOrder Insensitivity, Information Integration, and Negative Rejection. Empirical\nevaluations on 8 representative LLMs, including the advanced DeepSeek-R1,\nindicate that existing LLMs still face significant challenges in understanding\nstructured knowledge, and their performance is influenced by factors such as\nthe amount of noise, the order of knowledge units, and hallucination\nphenomenon. Our dataset and code are available at\nhttps://github.com/Lza12a/SKA-Bench.", "AI": {"tldr": "本文介绍了SKA-Bench，这是一个包含四种常见的结构化知识形式的增强型QA基准测试，使用一种三阶段的管道来构建数据集，并评估出了LLMs在处理结构化知识时的能力和不足。", "motivation": "现有的针对结构化知识理解的评估方法是非严格的，缺乏对特定能力的评估，并且只侧重于单一类型的结构化知识。因此，我们旨在提出一个更为全面和严格的结构化知识理解基准测试，以诊断LLMs的不足之处。", "method": "我们引入了SKA-Bench，这是一个涵盖了四种常用的结构化知识形式的增强型QA基准测试：知识图谱(KG)、表格(Table)、KG+文本、Table+文本。我们采用三阶段管道来构建SKA-Bench实例，这些实例包括问题、答案、正向知识单元和噪声知识单元。为了细粒度地评估LLMs在理解结构化知识方面的能力，我们将其扩展为四个基本能力测试床：噪声鲁棒性、次序不敏感性、信息整合能力和否定拒绝能力。", "result": "在8种代表性的LLMs上进行的实验证明，现有的LLMs在理解结构化知识方面仍然面临重大挑战，其表现受到噪声量、知识单元顺序、幻觉现象等因素的影响。", "conclusion": "我们的数据集和代码可在GitHub上获取：https://github.com/Lza12a/SKA-Bench。实验结果表明，现有的LLMs在理解结构化知识方面还有很大的提升空间，研究这一领域仍有许多工作要做。"}}
{"id": "2507.16873", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.16873", "abs": "https://arxiv.org/abs/2507.16873", "authors": ["Jeongeun Lee", "Youngjae Yu", "Dongha Lee"], "title": "HIPPO-Video: Simulating Watch Histories with Large Language Models for Personalized Video Highlighting", "comment": "Accepted to COLM2025", "summary": "The exponential growth of video content has made personalized video\nhighlighting an essential task, as user preferences are highly variable and\ncomplex. Existing video datasets, however, often lack personalization, relying\non isolated videos or simple text queries that fail to capture the intricacies\nof user behavior. In this work, we introduce HIPPO-Video, a novel dataset for\npersonalized video highlighting, created using an LLM-based user simulator to\ngenerate realistic watch histories reflecting diverse user preferences. The\ndataset includes 2,040 (watch history, saliency score) pairs, covering 20,400\nvideos across 170 semantic categories. To validate our dataset, we propose\nHiPHer, a method that leverages these personalized watch histories to predict\npreference-conditioned segment-wise saliency scores. Through extensive\nexperiments, we demonstrate that our method outperforms existing generic and\nquery-based approaches, showcasing its potential for highly user-centric video\nhighlighting in real-world scenarios.", "AI": {"tldr": "本文提出了HIPPO-Video数据集和HiPHer方法，利用基于LLM的用户模拟器生成的观看历史预测个性化视频片段的显著性评分，实验结果表明其超越了现有的方法。", "motivation": "现有的视频数据集通常缺乏个性化，依赖于孤立的视频或简单的文本查询，无法捕捉到用户行为的复杂性。因此，为了应对用户个性化需求，文章提出了HIPPO-Video数据集和HiPHer方法。", "method": "利用基于LLM的用户模拟器生成反映多样化用户喜好的现实观看历史，以此创建了一个名为HIPPO-Video的新数据集，数据集中包含2040对（观看历史，显著性评分），涉及20400个视频，覆盖170个语义类别。此外，提出了HiPHer方法，利用这些个性化的观看历史来预测条件偏好下的分段显著性评分。", "result": "通过广泛的实验表明，相对于现有的通用和基于查询的方法，该方法表现更优，展示了其在现实场景中高度用户中心的视频高亮处理中的潜力。", "conclusion": "该研究展示了一种新的数据集和方法，能够更好地理解和预测用户对视频内容的个性化偏好，从而实现更高效的个性化视频高亮处理。"}}
{"id": "2507.17186", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.17186", "abs": "https://arxiv.org/abs/2507.17186", "authors": ["Lingfeng Zeng", "Fangqi Lou", "Zixuan Wang", "Jiajie Xu", "Jinyi Niu", "Mengping Li", "Yifan Dong", "Qi Qi", "Wei Zhang", "Ziwei Yang", "Jun Han", "Ruilun Feng", "Ruiqi Hu", "Lejie Zhang", "Zhengbo Feng", "Yicheng Ren", "Xin Guo", "Zhaowei Liu", "Dongpo Cheng", "Weige Cai", "Liwen Zhang"], "title": "FinGAIA: An End-to-End Benchmark for Evaluating AI Agents in Finance", "comment": null, "summary": "The booming development of AI agents presents unprecedented opportunities for\nautomating complex tasks across various domains. However, their multi-step,\nmulti-tool collaboration capabilities in the financial sector remain\nunderexplored. This paper introduces FinGAIA, an end-to-end benchmark designed\nto evaluate the practical abilities of AI agents in the financial domain.\nFinGAIA comprises 407 meticulously crafted tasks, spanning seven major\nfinancial sub-domains: securities, funds, banking, insurance, futures, trusts,\nand asset management. These tasks are organized into three hierarchical levels\nof scenario depth: basic business analysis, asset decision support, and\nstrategic risk management. We evaluated 10 mainstream AI agents in a zero-shot\nsetting. The best-performing agent, ChatGPT, achieved an overall accuracy of\n48.9\\%, which, while superior to non-professionals, still lags financial\nexperts by over 35 percentage points. Error analysis has revealed five\nrecurring failure patterns: Cross-modal Alignment Deficiency, Financial\nTerminological Bias, Operational Process Awareness Barrier, among others. These\npatterns point to crucial directions for future research. Our work provides the\nfirst agent benchmark closely related to the financial domain, aiming to\nobjectively assess and promote the development of agents in this crucial field.\nPartial data is available at https://github.com/SUFE-AIFLM-Lab/FinGAIA.", "AI": {"tldr": "This paper presents FinGAIA, a benchmark for evaluating AI agents in the financial sector. Though the best agent, ChatGPT, performed better than non-professionals, it still falls short of human experts, indicating room for improvement in AI capabilities in finance.", "motivation": "The motivation is to explore the multi-step, multi-tool collaboration capabilities of AI agents in the financial sector, which remains underexplored.", "method": "This paper introduces FinGAIA, an end-to-end benchmark for evaluating AI agents' practical abilities in the financial domain, which includes 407 tasks across seven major financial sub-domains.", "result": "The best-performing AI agent, ChatGPT, achieved an overall accuracy of 48.9% on FinGAIA, which is better than non-professionals but still significantly lower than financial experts.", "conclusion": "The research identifies five recurring failure patterns in the tested AI agents, pointing to directions for future research. The work aims to provide an objective assessment and promotion of AI agents in the financial domain."}}
{"id": "2507.16877", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.16877", "abs": "https://arxiv.org/abs/2507.16877", "authors": ["Yizhi Hu", "Zezhao Tian", "Xingqun Qi", "Chen Su", "Bingkun Yang", "Junhui Yin", "Muyi Sun", "Man Zhang", "Zhenan Sun"], "title": "ReMeREC: Relation-aware and Multi-entity Referring Expression Comprehension", "comment": "15 pages, 7 figures", "summary": "Referring Expression Comprehension (REC) aims to localize specified entities\nor regions in an image based on natural language descriptions. While existing\nmethods handle single-entity localization, they often ignore complex\ninter-entity relationships in multi-entity scenes, limiting their accuracy and\nreliability. Additionally, the lack of high-quality datasets with fine-grained,\npaired image-text-relation annotations hinders further progress. To address\nthis challenge, we first construct a relation-aware, multi-entity REC dataset\ncalled ReMeX, which includes detailed relationship and textual annotations. We\nthen propose ReMeREC, a novel framework that jointly leverages visual and\ntextual cues to localize multiple entities while modeling their\ninter-relations. To address the semantic ambiguity caused by implicit entity\nboundaries in language, we introduce the Text-adaptive Multi-entity Perceptron\n(TMP), which dynamically infers both the quantity and span of entities from\nfine-grained textual cues, producing distinctive representations. Additionally,\nour Entity Inter-relationship Reasoner (EIR) enhances relational reasoning and\nglobal scene understanding. To further improve language comprehension for\nfine-grained prompts, we also construct a small-scale auxiliary dataset,\nEntityText, generated using large language models. Experiments on four\nbenchmark datasets show that ReMeREC achieves state-of-the-art performance in\nmulti-entity grounding and relation prediction, outperforming existing\napproaches by a large margin.", "AI": {"tldr": "为了解决现有方法在处理多实体引用表达式理解中忽略复杂实体间关系的问题，本研究提出了一种新的方法ReMeREC，该方法结合视觉和文本信息进行多实体定位，并建模实体间的关系。实验结果表明该方法具有显著的性能优势。", "motivation": "现有的方法在处理单实体定位时忽略了多实体场景中复杂的实体间关系，这对准确性和可靠性造成了限制。此外，缺乏有高质量细粒度的、成对的图像-文本-关系注释的数据集阻碍了进一步的进展。", "method": "构建了一个关系感知的多实体引用表达式理解（REC）数据集称为ReMeX，包含了详细的关系和文本注释。提出了一种新的框架ReMeREC，该框架结合视觉和文本线索的同时建模实体间关系。提出了文本自适应多实体感知机（TMP）来从细粒度文本线索中动态推断实体的数量和范围，并引入了实体间关系推理器（EIR）来增强关系推理和全局场景理解。", "result": "实验结果显示ReMeREC在多实体定位和关系预测上取得了比现有方法更优越的性能。", "conclusion": "ReMeREC结合了视觉和文本线索来定位多个实体，同时建模这些实体之间的关系，从而解决了语言中由隐式实体边界引起的语义歧义问题，显著提高了多实体引用表达式理解的性能。"}}
{"id": "2507.17216", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.17216", "abs": "https://arxiv.org/abs/2507.17216", "authors": ["Giuseppe Russo", "Debora Nozza", "Paul Röttger", "Dirk Hovy"], "title": "The Pluralistic Moral Gap: Understanding Judgment and Value Differences between Humans and Large Language Models", "comment": "13 pages, 4 figures", "summary": "People increasingly rely on Large Language Models (LLMs) for moral advice,\nwhich may influence humans' decisions. Yet, little is known about how closely\nLLMs align with human moral judgments. To address this, we introduce the Moral\nDilemma Dataset, a benchmark of 1,618 real-world moral dilemmas paired with a\ndistribution of human moral judgments consisting of a binary evaluation and a\nfree-text rationale. We treat this problem as a pluralistic distributional\nalignment task, comparing the distributions of LLM and human judgments across\ndilemmas. We find that models reproduce human judgments only under high\nconsensus; alignment deteriorates sharply when human disagreement increases. In\nparallel, using a 60-value taxonomy built from 3,783 value expressions\nextracted from rationales, we show that LLMs rely on a narrower set of moral\nvalues than humans. These findings reveal a pluralistic moral gap: a mismatch\nin both the distribution and diversity of values expressed. To close this gap,\nwe introduce Dynamic Moral Profiling (DMP), a Dirichlet-based sampling method\nthat conditions model outputs on human-derived value profiles. DMP improves\nalignment by 64.3% and enhances value diversity, offering a step toward more\npluralistic and human-aligned moral guidance from LLMs.", "AI": {"tldr": "研究发现，LLMs在道德判断上仅在人类高度一致时与人类相一致，否则一致性较差。LLMs对道德价值观的依赖范围较狭窄。提出Dynamic Moral Profiling（DMP）方法，显著提高了模型的一致性和道德价值观的多样性。", "motivation": "鉴于人们越来越多地依赖大型语言模型（LLMs）来获取道德建议，但尚不清楚LLMs的评估结果与人类道德判断有多一致，我们开展了一项研究来解决这一问题。", "method": "我们构建了Moral Dilemma Dataset，包含1,618个真实世界道德困境，配以人类道德判断的分布（包括二元判断和自由文本理由）。我们将问题视为多元分布一致性任务，比较模型和人类在各个道德困境上的判断分布。我们还构建了一个60价值观的分类法，基于从3,783个价值表达中提取的道德价值，分析LLMs对人类道德价值观的依赖程度。", "result": "我们发现，只有在高度一致的情况下，模型才与人类判断相一致，当人类判断不一致时，一致性急剧下降。此外，LLMs依赖的道德价值观的范围比人类狭窄。我们提出了一种方法，Dynamic Moral Profiling（DMP）来解决这个问题，这极大地提高了一致性（提高了64.3%），并增强了道德价值观的多样性。", "conclusion": "研究揭示了LLMs在道德判断上的局限性以及与人类道德判断的一致性问题，并提出了一种新方法来改进模型的一致性和道德价值观的多样性。"}}
{"id": "2507.16878", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.16878", "abs": "https://arxiv.org/abs/2507.16878", "authors": ["Xuchen Li", "Xuzhao Li", "Shiyu Hu", "Kaiqi Huang", "Wentao Zhang"], "title": "CausalStep: A Benchmark for Explicit Stepwise Causal Reasoning in Videos", "comment": "Preprint, Under review", "summary": "Recent advances in large language models (LLMs) have improved reasoning in\ntext and image domains, yet achieving robust video reasoning remains a\nsignificant challenge. Existing video benchmarks mainly assess shallow\nunderstanding and reasoning and allow models to exploit global context, failing\nto rigorously evaluate true causal and stepwise reasoning. We present\nCausalStep, a benchmark designed for explicit stepwise causal reasoning in\nvideos. CausalStep segments videos into causally linked units and enforces a\nstrict stepwise question-answer (QA) protocol, requiring sequential answers and\npreventing shortcut solutions. Each question includes carefully constructed\ndistractors based on error type taxonomy to ensure diagnostic value. The\nbenchmark features 100 videos across six categories and 1,852 multiple-choice\nQA pairs. We introduce seven diagnostic metrics for comprehensive evaluation,\nenabling precise diagnosis of causal reasoning capabilities. Experiments with\nleading proprietary and open-source models, as well as human baselines, reveal\na significant gap between current models and human-level stepwise reasoning.\nCausalStep provides a rigorous benchmark to drive progress in robust and\ninterpretable video reasoning.", "AI": {"tldr": "本文提出CausalStep作为评估视频中因果推理能力的严格基准测试。", "motivation": "虽然LLMs在文本和图像领域提高了推理能力，但在视频推理上仍然面临巨大挑战。现有的视频基准测试主要评估浅层理解和推理，允许模型利用全局上下文，未能严格评估真正的因果和逐步推理。", "method": "本文提出了CausalStep基准测试，该测试专门设计用于评估视频中的显式逐步因果推理能力。它将视频分割成因果相关的单元，并采用严格的问答协议，要求按顺序作答并避免捷径解决方案。每个问题均包含基于错误类型分类精心构建的干扰项，以确保诊断价值。", "result": "实验使用了领先的专有和开源模型以及人类基线，揭示了当前模型和人级别的逐步推理能力之间存在显著差距。", "conclusion": "CausalStep为促进稳健和可解释的视频推理的进步提供了一个严格基准。"}}
{"id": "2507.17234", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.17234", "abs": "https://arxiv.org/abs/2507.17234", "authors": ["Kyeongkyu Lee", "Seonghwan Yoon", "Hongki Lim"], "title": "CLARIFID: Improving Radiology Report Generation by Reinforcing Clinically Accurate Impressions and Enforcing Detailed Findings", "comment": null, "summary": "Automatic generation of radiology reports has the potential to alleviate\nradiologists' significant workload, yet current methods struggle to deliver\nclinically reliable conclusions. In particular, most prior approaches focus on\nproducing fluent text without effectively ensuring the factual correctness of\nthe reports and often rely on single-view images, limiting diagnostic\ncomprehensiveness. We propose CLARIFID, a novel framework that directly\noptimizes diagnostic correctness by mirroring the two-step workflow of experts.\nSpecifically, CLARIFID (1) learns the logical flow from Findings to Impression\nthrough section-aware pretraining, (2) is fine-tuned with Proximal Policy\nOptimization in which the CheXbert F1 score of the Impression section serves as\nthe reward, (3) enforces reasoning-aware decoding that completes \"Findings\"\nbefore synthesizing the \"Impression\", and (4) fuses multiple chest X-ray views\nvia a vision-transformer-based multi-view encoder. During inference, we apply a\nreasoning-aware next-token forcing strategy followed by report-level\nre-ranking, ensuring that the model first produces a comprehensive Findings\nsection before synthesizing the Impression and thereby preserving coherent\nclinical reasoning. Experimental results on the MIMIC-CXR dataset demonstrate\nthat our method achieves superior clinical efficacy and outperforms existing\nbaselines on both standard NLG metrics and clinically aware scores.", "AI": {"tldr": "CLARIFID框架通过模拟专家的两步工作流实现诊断正确性，包括逻辑流程学习、策略优化微调、推理感知解码以及多视图融合，其在临床准确性上优于现有方法。", "motivation": "本文旨在提出一种优化诊断准确性的自动放射学报告生成功能框架，通过更合理的工作流来确保报告的事实准确性，并通过多视图图像以提高诊断的全面性。", "method": "CLARIFID采用先通过部分感知预训练学习从发现到印象的逻辑流程，接着利用CheXbert F1分数作为奖励信号通过近似策略优化进行微调，然后强制先完成“发现”部分再合成“印象”部分，最后通过基于视觉变换器的多视图编码器融合多个胸部X光视图。", "result": "实验结果表明，该方法在MIMIC-CXR数据集上的临床效果优于现有的基线方法。", "conclusion": "实验结果证明了CLARIFID在提高放射学报告生成的事实准确性及诊断全面性上的优越性。"}}
{"id": "2507.16880", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.16880", "abs": "https://arxiv.org/abs/2507.16880", "authors": ["Antoni Kowalczuk", "Dominik Hintersdorf", "Lukas Struppek", "Kristian Kersting", "Adam Dziedzic", "Franziska Boenisch"], "title": "Finding Dori: Memorization in Text-to-Image Diffusion Models Is Less Local Than Assumed", "comment": null, "summary": "Text-to-image diffusion models (DMs) have achieved remarkable success in\nimage generation. However, concerns about data privacy and intellectual\nproperty remain due to their potential to inadvertently memorize and replicate\ntraining data. Recent mitigation efforts have focused on identifying and\npruning weights responsible for triggering replication, based on the assumption\nthat memorization can be localized. Our research assesses the robustness of\nthese pruning-based approaches. We demonstrate that even after pruning, minor\nadjustments to text embeddings of input prompts are sufficient to re-trigger\ndata replication, highlighting the fragility of these defenses. Furthermore, we\nchallenge the fundamental assumption of memorization locality, by showing that\nreplication can be triggered from diverse locations within the text embedding\nspace, and follows different paths in the model. Our findings indicate that\nexisting mitigation strategies are insufficient and underscore the need for\nmethods that truly remove memorized content, rather than attempting to suppress\nits retrieval. As a first step in this direction, we introduce a novel\nadversarial fine-tuning method that iteratively searches for replication\ntriggers and updates the model to increase robustness. Through our research, we\nprovide fresh insights into the nature of memorization in text-to-image DMs and\na foundation for building more trustworthy and compliant generative AI.", "AI": {"tldr": "该研究展示了现有的剪枝方法在防止文本到图像扩散模型中的数据复制方面存在的脆弱性，并提出了一种新的对抗微调方法来增强模型的鲁棒性。", "motivation": "研究旨在评估剪枝方法在防止数据复制方面的有效性，并挑战记忆局部性的假设。", "method": "研究人员分析了剪枝后模型的脆弱性，并提出了一个新的对抗微调方法来增强模型的鲁棒性。", "result": "研究发现即使剪枝后，文本嵌入的一些小调整可以重新触发数据复制，并且复制可以从文本嵌入空间的多个位置触发。", "conclusion": "现有的缓解策略不足以防止数据复制，需要开发能够真正去除记忆内容的方法，研究提供了一个对抗微调方法作为第一步。"}}
{"id": "2507.17288", "categories": ["cs.CL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.17288", "abs": "https://arxiv.org/abs/2507.17288", "authors": ["Miaomiao Gao", "Xiaoxiao Xiang", "Yiwen Guo"], "title": "Triple X: A LLM-Based Multilingual Speech Recognition System for the INTERSPEECH2025 MLC-SLM Challenge", "comment": null, "summary": "This paper describes our Triple X speech recognition system submitted to Task\n1 of the Multi-Lingual Conversational Speech Language Modeling (MLC-SLM)\nChallenge. Our work focuses on optimizing speech recognition accuracy in\nmultilingual conversational scenarios through an innovative encoder-adapter-LLM\narchitecture. This framework harnesses the powerful reasoning capabilities of\ntext-based large language models while incorporating domain-specific\nadaptations. To further enhance multilingual recognition performance, we\nadopted a meticulously designed multi-stage training strategy leveraging\nextensive multilingual audio datasets. Experimental results demonstrate that\nour approach achieves competitive Word Error Rate (WER) performance on both dev\nand test sets, obtaining second place in the challenge ranking.", "AI": {"tldr": "本文描述了一个名为Triple X的语音识别系统，该系统在Multi-Lingual Conversational Speech Language Modeling挑战赛中表现出色，通过创新的架构和训练策略实现多语种对话的准确识别。", "motivation": "本文的工作重点是在多语种对话场景中优化语音识别的准确性。", "method": "此论文采用了一种创新的编码器-适配器-大语言模型框架，利用文本基础大型语言模型的强大推理能力，并结合领域特定的调整。同时，通过精心设计的多阶段训练策略和广泛的多语种音频数据集进一步提高多语种识别性能。", "result": "实验结果表明，该方法在开发集和测试集上的词错误率（WER）达到了具有竞争力的表现，并在挑战赛中排名第二。", "conclusion": "通过使用编码器-适配器-大型语言模型的框架及多阶段训练策略，该研究成功提高了多语种语音识别的性能。"}}
{"id": "2507.16886", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.16886", "abs": "https://arxiv.org/abs/2507.16886", "authors": ["Yaoyu Fang", "Jiahe Qian", "Xinkun Wang", "Lee A. Cooper", "Bo Zhou"], "title": "Sparser2Sparse: Single-shot Sparser-to-Sparse Learning for Spatial Transcriptomics Imputation with Natural Image Co-learning", "comment": "16 pages, 5 figure, under review", "summary": "Spatial transcriptomics (ST) has revolutionized biomedical research by\nenabling high resolution gene expression profiling within tissues. However, the\nhigh cost and scarcity of high resolution ST data remain significant\nchallenges. We present Single-shot Sparser-to-Sparse (S2S-ST), a novel\nframework for accurate ST imputation that requires only a single and low-cost\nsparsely sampled ST dataset alongside widely available natural images for\nco-training. Our approach integrates three key innovations: (1) a\nsparser-to-sparse self-supervised learning strategy that leverages intrinsic\nspatial patterns in ST data, (2) cross-domain co-learning with natural images\nto enhance feature representation, and (3) a Cascaded Data Consistent\nImputation Network (CDCIN) that iteratively refines predictions while\npreserving sampled gene data fidelity. Extensive experiments on diverse tissue\ntypes, including breast cancer, liver, and lymphoid tissue, demonstrate that\nour method outperforms state-of-the-art approaches in imputation accuracy. By\nenabling robust ST reconstruction from sparse inputs, our framework\nsignificantly reduces reliance on costly high resolution data, facilitating\npotential broader adoption in biomedical research and clinical applications.", "AI": {"tldr": "本文提出S2S-ST框架，通过稀疏采样空间转录组数据和自然图像共同训练，实现准确的空间转录组插补，从而减轻高分辨率数据成本高昂的问题。", "motivation": "空间转录组学虽然在生物医学研究中发挥了革命性的作用，但高分辨率空间转录组数据的成本高昂且稀缺，成为重要挑战。因此，作者提出S2S-ST框架来解决这一问题。", "method": "S2S-ST框架，包含三个关键创新：(1) 使用稀疏到稀疏的自监督学习策略，利用空间转录组数据中的内在空间模式；(2) 借助自然图像进行跨域协同学习以增强特征表示；(3) 提出级联数据一致性的插补网络（CDCIN）来迭代精炼预测结果同时保持采样基因数据的准确性。", "result": "在多种组织类型上的广泛实验表明，该方法在插补准确性方面优于最先进的方法。通过利用稀疏输入实现稳健的空间转录组重建，显著减少了对高成本高分辨率数据的依赖。", "conclusion": "S2S-ST框架能够以更低的成本实现高质量的空间转录组重建，促进在生物医学研究和临床应用中的更广泛应用。"}}
{"id": "2507.17399", "categories": ["cs.CL", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2507.17399", "abs": "https://arxiv.org/abs/2507.17399", "authors": ["Zhili Shen", "Chenxin Diao", "Pascual Merita", "Pavlos Vougiouklis", "Jeff Z. Pan"], "title": "Millions of $\\text{GeAR}$-s: Extending GraphRAG to Millions of Documents", "comment": "Accepted by SIGIR 2025 LiveRAG Challenge Program", "summary": "Recent studies have explored graph-based approaches to retrieval-augmented\ngeneration, leveraging structured or semi-structured information -- such as\nentities and their relations extracted from documents -- to enhance retrieval.\nHowever, these methods are typically designed to address specific tasks, such\nas multi-hop question answering and query-focused summarisation, and therefore,\nthere is limited evidence of their general applicability across broader\ndatasets. In this paper, we aim to adapt a state-of-the-art graph-based RAG\nsolution: $\\text{GeAR}$ and explore its performance and limitations on the\nSIGIR 2025 LiveRAG Challenge.", "AI": {"tldr": "研究者们研究了通过适应$\\text{GeAR}$这一最先进的基于图的RAG解决方案，来探讨其在SIIGR 2025 LiveRAG挑战中的可行性和局限性。", "motivation": "尽管目前的研究通过利用从文档中提取的实体及其关系等结构化或半结构化信息来改进检索，这些方法通常被设计用来解决特定任务，缺少证明其在更广泛数据集上的通用性的证据。", "method": "本研究旨在通过适应最先进的基于图的RAG（检索增强生成）解决方案：$\\text{GeAR}$来研究其在SIGIR 2025 LiveRAG挑战上的性能和局限性。", "result": "尚未提供具体研究结果。", "conclusion": "尚未提供具体结论。"}}
{"id": "2507.16940", "categories": ["cs.CV", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2507.16940", "abs": "https://arxiv.org/abs/2507.16940", "authors": ["Nima Fathi", "Amar Kumar", "Tal Arbel"], "title": "AURA: A Multi-Modal Medical Agent for Understanding, Reasoning & Annotation", "comment": "9 pages, 3 figures, International Conference on Medical Image\n  Computing and Computer-Assisted Intervention", "summary": "Recent advancements in Large Language Models (LLMs) have catalyzed a paradigm\nshift from static prediction systems to agentic AI agents capable of reasoning,\ninteracting with tools, and adapting to complex tasks. While LLM-based agentic\nsystems have shown promise across many domains, their application to medical\nimaging remains in its infancy. In this work, we introduce AURA, the first\nvisual linguistic explainability agent designed specifically for comprehensive\nanalysis, explanation, and evaluation of medical images. By enabling dynamic\ninteractions, contextual explanations, and hypothesis testing, AURA represents\na significant advancement toward more transparent, adaptable, and clinically\naligned AI systems. We highlight the promise of agentic AI in transforming\nmedical image analysis from static predictions to interactive decision support.\nLeveraging Qwen-32B, an LLM-based architecture, AURA integrates a modular\ntoolbox comprising: (i) a segmentation suite with phase grounding, pathology\nsegmentation, and anatomy segmentation to localize clinically meaningful\nregions; (ii) a counterfactual image-generation module that supports reasoning\nthrough image-level explanations; and (iii) a set of evaluation tools including\npixel-wise difference-map analysis, classification, and advanced\nstate-of-the-art components to assess diagnostic relevance and visual\ninterpretability.", "AI": {"tldr": "AURA是一个专为医学图像分析设计的新型视觉语言解释代理，使用了Qwen-32B大型语言模型架构，并结合了分段、反事实图像生成和评估模块，以适应复杂的临床任务。", "motivation": "当前基于大型语言模型的代理系统在各个领域已经显示出潜力，但在医学成像中的应用仍然有限。AURA旨在填补这一空白，提供透明、灵活且符合临床需求的人工智能系统。", "method": "通过使用Qwen-32B大型语言模型架构，AURA结合了一个包括分段套件、反事实图像生成模块和评估工具集的模块化工具箱，以支持对医学图像的综合分析、解释和评估。", "result": "AURA作为第一个用于医学图像全面分析、解释和评估的视觉语言性解释代理，展示了其在将医学图像分析从静态预测转变为交互式决策支持方面的潜力。", "conclusion": "AURA代表了更透明、更适应临床需求的AI系统的重要进步，并展示了代理AI在医学图像分析领域从静态预测到交互式决策支持的转变潜力。"}}
{"id": "2507.17409", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.17409", "abs": "https://arxiv.org/abs/2507.17409", "authors": ["Carlotta Quensel", "Neele Falk", "Gabriella Lapesa"], "title": "Investigating Subjective Factors of Argument Strength: Storytelling, Emotions, and Hedging", "comment": "Accepted to the 12th Workshop on Argument Mining (ArgMining) 2025", "summary": "In assessing argument strength, the notions of what makes a good argument are\nmanifold. With the broader trend towards treating subjectivity as an asset and\nnot a problem in NLP, new dimensions of argument quality are studied. Although\nstudies on individual subjective features like personal stories exist, there is\na lack of large-scale analyses of the relation between these features and\nargument strength. To address this gap, we conduct regression analysis to\nquantify the impact of subjective factors $-$ emotions, storytelling, and\nhedging $-$ on two standard datasets annotated for objective argument quality\nand subjective persuasion. As such, our contribution is twofold: at the level\nof contributed resources, as there are no datasets annotated with all studied\ndimensions, this work compares and evaluates automated annotation methods for\neach subjective feature. At the level of novel insights, our regression\nanalysis uncovers different patterns of impact of subjective features on the\ntwo facets of argument strength encoded in the datasets. Our results show that\nstorytelling and hedging have contrasting effects on objective and subjective\nargument quality, while the influence of emotions depends on their rhetoric\nutilization rather than the domain.", "AI": {"tldr": "本研究通过回归分析，量化了情绪、叙述和缓和对论据质量的影响，并发现这些因素在客观和主观论据质量上的影响存在差异。", "motivation": "尽管存在对个别主观特征的研究，但仍缺乏大规模地分析这些特征与论据强度之间关系的研究。", "method": "通过回归分析量化情绪、叙述和缓和等主观因素对客观论据质量和主观说服力的影响力。", "result": "研究结果表明，叙述和缓和在客观和主观的论据质量上具有相反的影响效果，而情绪的影响则取决于修辞利用方式，而非领域。", "conclusion": "该研究展示了不同主观特征对论据强度的不同影响模式，并对比评估了自动化标注方法。"}}
{"id": "2507.16946", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.16946", "abs": "https://arxiv.org/abs/2507.16946", "authors": ["Chiao-An Yang", "Kuan-Chuan Peng", "Raymond A. Yeh"], "title": "Toward Long-Tailed Online Anomaly Detection through Class-Agnostic Concepts", "comment": "This paper is accepted to ICCV 2025. The supplementary material is\n  included. The long-tailed online anomaly detection dataset is available at\n  https://doi.org/10.5281/zenodo.16283852", "summary": "Anomaly detection (AD) identifies the defect regions of a given image. Recent\nworks have studied AD, focusing on learning AD without abnormal images, with\nlong-tailed distributed training data, and using a unified model for all\nclasses. In addition, online AD learning has also been explored. In this work,\nwe expand in both directions to a realistic setting by considering the novel\ntask of long-tailed online AD (LTOAD). We first identified that the offline\nstate-of-the-art LTAD methods cannot be directly applied to the online setting.\nSpecifically, LTAD is class-aware, requiring class labels that are not\navailable in the online setting. To address this challenge, we propose a\nclass-agnostic framework for LTAD and then adapt it to our online learning\nsetting. Our method outperforms the SOTA baselines in most offline LTAD\nsettings, including both the industrial manufacturing and the medical domain.\nIn particular, we observe +4.63% image-AUROC on MVTec even compared to methods\nthat have access to class labels and the number of classes. In the most\nchallenging long-tailed online setting, we achieve +0.53% image-AUROC compared\nto baselines. Our LTOAD benchmark is released here:\nhttps://doi.org/10.5281/zenodo.16283852 .", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2507.17442", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.17442", "abs": "https://arxiv.org/abs/2507.17442", "authors": ["Shiting Chen", "Zijian Zhao", "Jinsong Chen"], "title": "Each to Their Own: Exploring the Optimal Embedding in RAG", "comment": null, "summary": "Recently, as Large Language Models (LLMs) have fundamentally impacted various\nfields, the methods for incorporating up-to-date information into LLMs or\nadding external knowledge to construct domain-specific models have garnered\nwide attention. Retrieval-Augmented Generation (RAG), serving as an\ninference-time scaling method, is notable for its low cost and minimal effort\nfor parameter tuning. However, due to heterogeneous training data and model\narchitecture, the variant embedding models used in RAG exhibit different\nbenefits across various areas, often leading to different similarity\ncalculation results and, consequently, varying response quality from LLMs. To\naddress this problem, we propose and examine two approaches to enhance RAG by\ncombining the benefits of multiple embedding models, named Mixture-Embedding\nRAG and Confident RAG. Mixture-Embedding RAG simply sorts and selects\nretrievals from multiple embedding models based on standardized similarity;\nhowever, it does not outperform vanilla RAG. In contrast, Confident RAG\ngenerates responses multiple times using different embedding models and then\nselects the responses with the highest confidence level, demonstrating average\nimprovements of approximately 10% and 5% over vanilla LLMs and RAG,\nrespectively. The consistent results across different LLMs and embedding models\nindicate that Confident RAG is an efficient plug-and-play approach for various\ndomains. We will release our code upon publication.", "AI": {"tldr": "本文提出两种方法增强 Retrieval-Augmented Generation (RAG) 的效果，分别是 Mixture-Embedding RAG 和 Confident RAG。结果显示 Confident RAG 相比于基本 RAG 和 LLM 在不同情况下均有明显的提升。", "motivation": "由于在 RAG 中使用不同的变体嵌入模型会导致相似度计算结果不同以及 LLM 产出的回答质量不一，因此研究者希望结合多个嵌入模型的优势来改善 RAG。", "method": "我们提出并评估了两种方法来改善 Retrieval-Augmented Generation (RAG)，即 Mixture-Embedding RAG 和 Confident RAG。Mixture-Embedding RAG 通过标准化相似度来排序并选择来自多个嵌入模型的检索结果，但其表现并未优于基本的 RAG。而 Confident RAG 则使用不同的嵌入模型多次生成响应，并选择置信度最高的响应。", "result": "实验结果显示，相比于基本的 LLM 和 RAG，Confident RAG 平均提升了约 10% 和 5% 的效果。其结果在不同 LLM 和嵌入模型下的一致性表明，Confident RAG 是一个有效的即插即用方法。", "conclusion": "Confident RAG 方法提供了一种有效的即插即用解决方案，以改善 RAG 不同领域应用时的回答质量。"}}
{"id": "2507.17000", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.17000", "abs": "https://arxiv.org/abs/2507.17000", "authors": ["Jacob Piland", "Chris Sweet", "Adam Czajka"], "title": "Divisive Decisions: Improving Salience-Based Training for Generalization in Binary Classification Tasks", "comment": null, "summary": "Existing saliency-guided training approaches improve model generalization by\nincorporating a loss term that compares the model's class activation map (CAM)\nfor a sample's true-class ({\\it i.e.}, correct-label class) against a human\nreference saliency map. However, prior work has ignored the false-class CAM(s),\nthat is the model's saliency obtained for incorrect-label class. We hypothesize\nthat in binary tasks the true and false CAMs should diverge on the important\nclassification features identified by humans (and reflected in human saliency\nmaps). We use this hypothesis to motivate three new saliency-guided training\nmethods incorporating both true- and false-class model's CAM into the training\nstrategy and a novel post-hoc tool for identifying important features. We\nevaluate all introduced methods on several diverse binary close-set and\nopen-set classification tasks, including synthetic face detection, biometric\npresentation attack detection, and classification of anomalies in chest X-ray\nscans, and find that the proposed methods improve generalization capabilities\nof deep learning models over traditional (true-class CAM only) saliency-guided\ntraining approaches. We offer source codes and model weights\\footnote{GitHub\nrepository link removed to preserve anonymity} to support reproducible\nresearch.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2507.17476", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.17476", "abs": "https://arxiv.org/abs/2507.17476", "authors": ["Alexander R. Fabbri", "Diego Mares", "Jorge Flores", "Meher Mankikar", "Ernesto Hernandez", "Dean Lee", "Bing Liu", "Chen Xing"], "title": "MultiNRC: A Challenging and Native Multilingual Reasoning Evaluation Benchmark for LLMs", "comment": null, "summary": "Although recent Large Language Models (LLMs) have shown rapid improvement on\nreasoning benchmarks in English, the evaluation of such LLMs' multilingual\nreasoning capability across diverse languages and cultural contexts remains\nlimited. Existing multilingual reasoning benchmarks are typically constructed\nby translating existing English reasoning benchmarks, biasing these benchmarks\ntowards reasoning problems with context in English language/cultures. In this\nwork, we introduce the Multilingual Native Reasoning Challenge (MultiNRC), a\nbenchmark designed to assess LLMs on more than 1,000 native, linguistic and\nculturally grounded reasoning questions written by native speakers in French,\nSpanish, and Chinese. MultiNRC covers four core reasoning categories:\nlanguage-specific linguistic reasoning, wordplay & riddles, cultural/tradition\nreasoning, and math reasoning with cultural relevance. For cultural/tradition\nreasoning and math reasoning with cultural relevance, we also provide English\nequivalent translations of the multilingual questions by manual translation\nfrom native speakers fluent in English. This set of English equivalents can\nprovide a direct comparison of LLM reasoning capacity in other languages vs.\nEnglish on the same reasoning questions. We systematically evaluate current 14\nleading LLMs covering most LLM families on MultiNRC and its English equivalent\nset. The results show that (1) current LLMs are still not good at native\nmultilingual reasoning, with none scoring above 50% on MultiNRC; (2) LLMs\nexhibit distinct strengths and weaknesses in handling linguistic, cultural, and\nlogical reasoning tasks; (3) Most models perform substantially better in math\nreasoning in English compared to in original languages (+10%), indicating\npersistent challenges with culturally grounded knowledge.", "AI": {"tldr": "本文介绍了Multilingual Native Reasoning Challenge（MultiNRC），一个评估大型语言模型在法语、西班牙语和中文中本土推理能力的新基准，分析结果显示当前LLMs在此方面表现不佳，特别是在文化相关的推理任务上。", "motivation": "现有的多语言推理基准通常通过翻译现有的英文推理基准得到，这导致评估偏向英文文化内容。因此，本文设计了一个全新的多语言推理基准测试，以全面评估语言模型在不同语言及文化背景下的推理能力。", "method": "引入了MultiNRC，一个由母语者设计的基准，涵盖了法语、西班牙语和中文中超过1,000个本土、语言和文化相关的推理问题。MultiNRC评估了语言特定的推理、文字游戏与谜语、文化/传统推理以及时带有文化关联的数学推理。对于某些题目，提供英文翻译版本让模型英文推理能力与其他语言对比。", "result": "对当下14种领先的大语言模型进行评估，在MultiNRC基准测试中，没有一种模型的得分超过50%。结果显示，模型在语言特定、文化相关的任务上存在显著差异，且在英文数学推理方面表现优于原始语言。", "conclusion": "当前的大语言模型在处理多语言背景下的本土推理任务上仍然面临挑战，尤其是文化和逻辑推理任务方面。这表明在跨语言和跨文化知识的理解上仍需进一步研究。"}}
{"id": "2507.17008", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.17008", "abs": "https://arxiv.org/abs/2507.17008", "authors": ["Gaston Gustavo Rios", "Pedro Dal Bianco", "Franco Ronchetti", "Facundo Quiroga", "Oscar Stanchi", "Santiago Ponte Ahón", "Waldo Hasperué"], "title": "Bringing Balance to Hand Shape Classification: Mitigating Data Imbalance Through Generative Models", "comment": "23 pages, 8 figures, to be published in Applied Soft Computing", "summary": "Most sign language handshape datasets are severely limited and unbalanced,\nposing significant challenges to effective model training. In this paper, we\nexplore the effectiveness of augmenting the training data of a handshape\nclassifier by generating synthetic data. We use an EfficientNet classifier\ntrained on the RWTH German sign language handshape dataset, which is small and\nheavily unbalanced, applying different strategies to combine generated and real\nimages. We compare two Generative Adversarial Networks (GAN) architectures for\ndata generation: ReACGAN, which uses label information to condition the data\ngeneration process through an auxiliary classifier, and SPADE, which utilizes\nspatially-adaptive normalization to condition the generation on pose\ninformation. ReACGAN allows for the generation of realistic images that align\nwith specific handshape labels, while SPADE focuses on generating images with\naccurate spatial handshape configurations. Our proposed techniques improve the\ncurrent state-of-the-art accuracy on the RWTH dataset by 5%, addressing the\nlimitations of small and unbalanced datasets. Additionally, our method\ndemonstrates the capability to generalize across different sign language\ndatasets by leveraging pose-based generation trained on the extensive HaGRID\ndataset. We achieve comparable performance to single-source trained classifiers\nwithout the need for retraining the generator.", "AI": {"tldr": "我们使用EfficientNet和两种GAN（ReACGAN与SPADE）生成合成数据来解决手语手势数据集的小规模和不平衡问题，提高了分类准确率并增强了跨数据集的泛化能力。", "motivation": "大多数手语手势数据集的规模有限且分布不均，这给模型训练带来了挑战。本文探讨了通过生成合成数据来增强训练集的效果，以解决此问题。", "method": "我们使用EfficientNet分类器，并结合两种不同的生成对抗网络（GAN）架构来生成合成数据以增强训练集：ReACGAN和SPADE。ReACGAN通过辅助分类器使用标签信息来条件生成过程，而SPADE利用空间自适应归一化来限制手部形状配置。", "result": "我们的方法在RWTH数据集上将当前最优准确率提升了5%，并且通过利用基于姿态的生成器模型（训练于庞大的HaGRID数据集）可以跨不同的手语数据集进行泛化。我们无需重新训练生成器即可与单源训练的分类器达到相当的性能。", "conclusion": "我们的技术解决了小规模和不平衡数据集的限制，提高了手语手势识别的准确度，并展示了跨数据集的泛化能力。这表明，通过合成数据增强可以有效改善手势识别模型的性能。"}}
{"id": "2507.17527", "categories": ["cs.CL", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.17527", "abs": "https://arxiv.org/abs/2507.17527", "authors": ["Shanbo Cheng", "Yu Bao", "Zhichao Huang", "Yu Lu", "Ningxin Peng", "Lu Xu", "Runsheng Yu", "Rong Cao", "Ting Han", "Zeyang Li", "Sitong Liu", "Shengtao Ma", "Shiguang Pan", "Jiongchen Xiao", "Nuo Xu", "Meng Yang", "Rong Ye", "Yiming Yu", "Ruofei Zhang", "Wanyi Zhang", "Wenhao Zhu", "Liehao Zou", "Lu Lu", "Yuxuan Wang", "Yonghui Wu"], "title": "Seed LiveInterpret 2.0: End-to-end Simultaneous Speech-to-speech Translation with Your Voice", "comment": "Seed-LiveInterpret 2.0 Technical Report", "summary": "Simultaneous Interpretation (SI) represents one of the most daunting\nfrontiers in the translation industry, with product-level automatic systems\nlong plagued by intractable challenges: subpar transcription and translation\nquality, lack of real-time speech generation, multi-speaker confusion, and\ntranslated speech inflation, especially in long-form discourses. In this study,\nwe introduce Seed-LiveInterpret 2.0, an end-to-end SI model that delivers\nhigh-fidelity, ultra-low-latency speech-to-speech generation with voice cloning\ncapabilities. As a fully operational product-level solution, Seed-LiveInterpret\n2.0 tackles these challenges head-on through our novel duplex speech-to-speech\nunderstanding-generating framework. Experimental results demonstrate that\nthrough large-scale pretraining and reinforcement learning, the model achieves\na significantly better balance between translation accuracy and latency,\nvalidated by human interpreters to exceed 70% correctness in complex scenarios.\nNotably, Seed-LiveInterpret 2.0 outperforms commercial SI solutions by\nsignificant margins in translation quality, while slashing the average latency\nof cloned speech from nearly 10 seconds to a near-real-time 3 seconds, which is\naround a near 70% reduction that drastically enhances practical usability.", "AI": {"tldr": "本文介绍了Seed-LiveInterpret 2.0系统，通过创新的双工语音理解和生成框架实现了同声传译的高质量和低延迟，超出商业解决方案显著。", "motivation": "目标是开发一种能够提供高质量且超低延迟语音到语音转换的同声传译系统，并通过语音克隆技术解决现有系统中存在的问题。", "method": "介绍了一种名为Seed-LiveInterpret 2.0的端到端实时同声传译模型，该模型通过创新的双工语音理解和生成框架解决了同声传译中常见的问题，包括次优的转换和翻译质量、缺乏实时语音生成、多方讲话者的混淆以及翻译中的语音膨胀。", "result": "实验证明，该模型通过大规模预训练和强化学习，成功实现了翻译准确性和延迟之间的更好平衡，据人类译员验证，在复杂场景下的正确率超过了70%。与现有的商业解决方案相比，Seed-LiveInterpret 2.0的翻译质量显著提高，同时将克隆语音的平均延迟从近10秒降低到了几乎实时的3秒，提高了近70%，极大地增强了实际使用性。", "conclusion": "通过引入Seed-LiveInterpret 2.0，本研究证明了在现有同声传译系统存在的挑战中取得显著进步的可能性，特别是在翻译质量和延迟方面。"}}
{"id": "2507.17038", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.17038", "abs": "https://arxiv.org/abs/2507.17038", "authors": ["Muhammad Kamran", "Mohammad Moein Sheikholeslami", "Andreas Wichmann", "Gunho Sohn"], "title": "Transformer Based Building Boundary Reconstruction using Attraction Field Maps", "comment": null, "summary": "In recent years, the number of remote satellites orbiting the Earth has grown\nsignificantly, streaming vast amounts of high-resolution visual data to support\ndiverse applications across civil, public, and military domains. Among these\napplications, the generation and updating of spatial maps of the built\nenvironment have become critical due to the extensive coverage and detailed\nimagery provided by satellites. However, reconstructing spatial maps from\nsatellite imagery is a complex computer vision task, requiring the creation of\nhigh-level object representations, such as primitives, to accurately capture\nthe built environment. While the past decade has witnessed remarkable\nadvancements in object detection and representation using visual data,\nprimitives-based object representation remains a persistent challenge in\ncomputer vision. Consequently, high-quality spatial maps often rely on\nlabor-intensive and manual processes. This paper introduces a novel deep\nlearning methodology leveraging Graph Convolutional Networks (GCNs) to address\nthese challenges in building footprint reconstruction. The proposed approach\nenhances performance by incorporating geometric regularity into building\nboundaries, integrating multi-scale and multi-resolution features, and\nembedding Attraction Field Maps into the network. These innovations provide a\nscalable and precise solution for automated building footprint extraction from\na single satellite image, paving the way for impactful applications in urban\nplanning, disaster management, and large-scale spatial analysis. Our model,\nDecoupled-PolyGCN, outperforms existing methods by 6% in AP and 10% in AR,\ndemonstrating its ability to deliver accurate and regularized building\nfootprints across diverse and challenging scenarios.", "AI": {"tldr": "本文介绍了一种新的深度学习方法——Decoupled-PolyGCN，用于从卫星图像中自动提取建筑物边界，其性能优于现有方法，精度提高了6%的平均精度（AP）和10%的平均召回率（AR）。", "motivation": "尽管在物体检测和表示方面取得了显著进展，但基于结构元素的对象表示仍然是计算机视觉的一个挑战。当前高质量的空间图往往依赖于耗时的手动过程，而本文提出的方法旨在解决这一问题。", "method": "本研究提出了一种基于图卷积网络（GCN）的新型深度学习方法，以解决从卫星图像中重建建筑物边界的问题。该方法通过集成多尺度和多分辨率特征，并在网络中嵌入吸引场图，利用几何规则化增强建筑物边界的精度。", "result": "实验结果显示，与现有方法相比，Decoupled-PolyGCN在平均精度（AP）和平均召回率（AR）上分别提高了6%和10%。", "conclusion": "Decoupled-PolyGCN方法在自动建筑物边界提取上展现出优越的精度和规律性，为城市规划、灾害管理和大规模空间分析提供了强有力的支持。"}}
{"id": "2507.17578", "categories": ["cs.CL", "I.2.7"], "pdf": "https://arxiv.org/pdf/2507.17578", "abs": "https://arxiv.org/abs/2507.17578", "authors": ["Brian DeRenzi", "Anna Dixon", "Mohamed Aymane Farhi", "Christian Resch"], "title": "Synthetic Voice Data for Automatic Speech Recognition in African Languages", "comment": "29 pages incl. appendix, 8 tables, 5 figures. Authors are listed in\n  alphabetical order", "summary": "Speech technology remains out of reach for most of the over 2300 languages in\nAfrica. We present the first systematic assessment of large-scale synthetic\nvoice corpora for African ASR. We apply a three-step process: LLM-driven text\ncreation, TTS voice synthesis, and ASR fine-tuning. Eight out of ten languages\nfor which we create synthetic text achieved readability scores above 5 out of\n7. We evaluated ASR improvement for three (Hausa, Dholuo, Chichewa) and created\nmore than 2,500 hours of synthetic voice data at below 1% of the cost of real\ndata. Fine-tuned Wav2Vec-BERT-2.0 models trained on 250h real and 250h\nsynthetic Hausa matched a 500h real-data-only baseline, while 579h real and\n450h to 993h synthetic data created the best performance. We also present\ngender-disaggregated ASR performance evaluation. For very low-resource\nlanguages, gains varied: Chichewa WER improved about 6.5% relative with a 1:2\nreal-to-synthetic ratio; a 1:1 ratio for Dholuo showed similar improvements on\nsome evaluation data, but not on others. Investigating intercoder reliability,\nASR errors and evaluation datasets revealed the need for more robust reviewer\nprotocols and more accurate evaluation data. All data and models are publicly\nreleased to invite further work to improve synthetic data for African\nlanguages.", "AI": {"tldr": "这项研究展示了非洲地区八种语言的合成语音语料库的创建方法，并通过结合真实和合成数据提升了这些语言的自动语音识别性能，所有数据和模型均公开。", "motivation": "这篇论文的动机是评估非洲语言大规模合成语音语料库的能力，目前这些地区大部分语言的语音技术尚未普及。", "method": "我们采用了三步流程来生成非洲语言的合成语音语料库：由LLM驱动的文本创建、TTS语音合成和针对ASR的微调。", "result": "对于被评估的三种语言（豪萨语、多鲁阿语、奇切瓦语），我们生成了超过2500小时的合成语音数据，其成本还不到真实数据成本的1%。Hausa语言的细调模型显示，合成数据与真实数据相结合可以达到甚至超越仅使用真实数据的效果。对于资源非常有限的语言，如奇切瓦语，合成数据能带来约6.5%的相对改善; 多鲁阿语在某些评估数据上的改进也非常相似。", "conclusion": "研究指出需要更稳健的审稿人协议和更准确的评估数据，并且，所有数据和模型都已公开发布，以促进后续关于非洲语言的合成数据研究工作。"}}
{"id": "2507.17047", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.17047", "abs": "https://arxiv.org/abs/2507.17047", "authors": ["Kuleen Sasse", "Efsun Sarioglu Kayi", "Arun Reddy"], "title": "Controllable Hybrid Captioner for Improved Long-form Video Understanding", "comment": null, "summary": "Video data, especially long-form video, is extremely dense and\nhigh-dimensional. Text-based summaries of video content offer a way to\nrepresent query-relevant content in a much more compact manner than raw video.\nIn addition, textual representations are easily ingested by state-of-the-art\nlarge language models (LLMs), which enable reasoning over video content to\nanswer complex natural language queries. To solve this issue, we rely on the\nprogressive construction of a text-based memory by a video captioner operating\non shorter chunks of the video, where spatio-temporal modeling is\ncomputationally feasible. We explore ways to improve the quality of the\nactivity log comprised solely of short video captions. Because the video\ncaptions tend to be focused on human actions, and questions may pertain to\nother information in the scene, we seek to enrich the memory with static scene\ndescriptions using Vision Language Models (VLMs). Our video understanding\nsystem relies on the LaViLa video captioner in combination with a LLM to answer\nquestions about videos. We first explored different ways of partitioning the\nvideo into meaningful segments such that the textual descriptions more\naccurately reflect the structure of the video content. Furthermore, we\nincorporated static scene descriptions into the captioning pipeline using LLaVA\nVLM, resulting in a more detailed and complete caption log and expanding the\nspace of questions that are answerable from the textual memory. Finally, we\nhave successfully fine-tuned the LaViLa video captioner to produce both action\nand scene captions, significantly improving the efficiency of the captioning\npipeline compared to using separate captioning models for the two tasks. Our\nmodel, controllable hybrid captioner, can alternate between different types of\ncaptions according to special input tokens that signals scene changes detected\nin the video.", "AI": {"tldr": "本文通过结合视频字幕生成和视觉语言模型来构建文本记忆，改进了视频内容的理解和查询能力。", "motivation": "鉴于视频数据尤其是长视频的高度密集和高维特性，本文旨在通过文本摘要方式提供一种更紧凑的查询相关视频内容表示方法。此外，文本表示便于现有大型语言模型（LLMs）处理，支持对视频内容进行复杂自然语言查询的推理。", "method": "本文提出了一种基于文本的记忆构建方法，结合视频字幕生成器和视觉语言模型（VLM），通过将视频分割成更小的片段以进行时空建模，并使用VLM来丰富静态场景描述。这种方法旨在改进单纯基于短视频片段的活动日志质量。同时，通过微调LaViLa视频字幕生成器，使其能够生成动作和场景描述，从而提高字幕生成流程的效率。", "result": "提出的可控制混合字幕生成器能够根据视频中的场景变化及时地交替生成不同类型的描述，提高了视频内容描述的准确性和丰富性，从而增强了视频理解系统的性能。", "conclusion": "本文证明了使用改进的视频字幕生成方法结合LLMs可以有效回答有关视频的问题，特别是在增加了静态场景描述后，显著扩展了可以从文本记忆中回答的问题范围。"}}
{"id": "2507.17618", "categories": ["cs.CL", "cs.PF"], "pdf": "https://arxiv.org/pdf/2507.17618", "abs": "https://arxiv.org/abs/2507.17618", "authors": ["Bowen Zheng", "Ming Ma", "Zhongqiao Lin", "Tianming Yang"], "title": "A Hybrid Early-Exit Algorithm for Large Language Models Based on Space Alignment Decoding (SPADE)", "comment": null, "summary": "Large language models are computationally expensive due to their deep\nstructures. Prior research has shown that intermediate layers contain\nsufficient information to generate accurate answers, leading to the development\nof early-exit algorithms that reduce inference costs by terminating computation\nat earlier layers. However, these methods often suffer from poor performance\ndue to misalignment between intermediate and output layer representations that\nlead to decoding inaccuracy. To address these challenges, we propose SPADE\n(SPace Alignment DEcoding), a novel decoding method that aligns intermediate\nlayer representations with the output layer by propagating a minimally reduced\nsequence consisting of only the start token and the answer token. We further\noptimize the early-exit decision-making process by training a linear\napproximation of SPADE that computes entropy-based confidence metrics. Putting\nthem together, we create a hybrid early-exit algorithm that monitors confidence\nlevels and stops inference at intermediate layers while using SPADE to generate\nhigh-quality outputs. This approach significantly reduces inference costs\nwithout compromising accuracy, offering a scalable and efficient solution for\ndeploying large language models in real-world applications.", "AI": {"tldr": "本文提出了一种新的解码方法SPADE，结合优化的早期退出决策，提升了解码准确性，降低了计算成本，适合大规模语言模型的部署。", "motivation": "现有的大型语言模型由于其深层次结构而计算成本高昂。虽然先前的研究表明中间层含有足够信息生成准确答案，并提出了减少计算成本的早期退出算法，但这些方法因中间层与输出层表示不对齐而导致解码不准确。", "method": "我们提出了SPADE（SPace Alignment DEcoding），一种新的解码方法，通过传播仅包含开始标记和答案标记的最简序列来对齐中间层与输出层表示。此外，我们通过训练SPADE的线性近似来优化早期退出决策过程，该近似使用基于熵的信心度量。", "result": "我们的方法显著减少了推理成本，同时不牺牲准确性，为在实际应用中部署大规模语言模型提供了可扩展和高效的解决方案。", "conclusion": "通过使用SPADE进行高质量输出生成，并通过监控信心水平在中间层停止推理，我们成功地解决了现有早期退出算法性能不佳的问题。"}}
{"id": "2507.17050", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.17050", "abs": "https://arxiv.org/abs/2507.17050", "authors": ["Tz-Ying Wu", "Tahani Trigui", "Sharath Nittur Sridhar", "Anand Bodas", "Subarna Tripathi"], "title": "Toward Scalable Video Narration: A Training-free Approach Using Multimodal Large Language Models", "comment": "Accepted to CVAM Workshop at ICCV 2025", "summary": "In this paper, we introduce VideoNarrator, a novel training-free pipeline\ndesigned to generate dense video captions that offer a structured snapshot of\nvideo content. These captions offer detailed narrations with precise\ntimestamps, capturing the nuances present in each segment of the video. Despite\nadvancements in multimodal large language models (MLLMs) for video\ncomprehension, these models often struggle with temporally aligned narrations\nand tend to hallucinate, particularly in unfamiliar scenarios. VideoNarrator\naddresses these challenges by leveraging a flexible pipeline where\noff-the-shelf MLLMs and visual-language models (VLMs) can function as caption\ngenerators, context providers, or caption verifiers. Our experimental results\ndemonstrate that the synergistic interaction of these components significantly\nenhances the quality and accuracy of video narrations, effectively reducing\nhallucinations and improving temporal alignment. This structured approach not\nonly enhances video understanding but also facilitates downstream tasks such as\nvideo summarization and video question answering, and can be potentially\nextended for advertising and marketing applications.", "AI": {"tldr": "", "motivation": "", "method": "", "result": "{\n  \"tldr\": \"VideoNarrator 是一个无需训练的视频描述生成管道，它致力于生成详尽且时间精确的视频描述以准确捕获视频内容。实验表明，它有效减少了描述中所产生的不准确性，并改善了时间上的同步性。\", \n  \"motivation\": \"现有的多模态大型语言模型在视频理解方面取得了进展，但在时间对齐的描述生成方面仍然存在困难，并且在不熟悉的场景中容易产生不准确的描述。为了克服这些问题，提出了VideoNarrator。\", \n  \"method\": \"VideoNarrator 使用了一个灵活的管道，其中包括现成的多模态语言模型和视觉语言模型，它们可以作为描述生成器、背景提供者或描述验证器发挥作用。\", \n  \"result\": \"实验结果表明管道中的组件协同工作有效增强了视频描述的质量和准确性，并且减少了不准确性的产生且提高了同步性。\", \n  \"conclusion\": \"该方法不仅提高了视频理解的质量，也促进了下游任务如视频摘要生成和视频问答的发展，且可能在广告和市场营销领域进行扩展。 \" \n}", "conclusion": ""}}
{"id": "2507.17634", "categories": ["cs.CL", "cs.LG", "I.2.7"], "pdf": "https://arxiv.org/pdf/2507.17634", "abs": "https://arxiv.org/abs/2507.17634", "authors": ["Changxin Tian", "Jiapeng Wang", "Qian Zhao", "Kunlong Chen", "Jia Liu", "Ziqi Liu", "Jiaxin Mao", "Wayne Xin Zhao", "Zhiqiang Zhang", "Jun Zhou"], "title": "WSM: Decay-Free Learning Rate Schedule via Checkpoint Merging for LLM Pre-training", "comment": null, "summary": "Recent advances in learning rate (LR) scheduling have demonstrated the\neffectiveness of decay-free approaches that eliminate the traditional decay\nphase while maintaining competitive performance. Model merging techniques have\nemerged as particularly promising solutions in this domain. We present\nWarmup-Stable and Merge (WSM), a general framework that establishes a formal\nconnection between learning rate decay and model merging. WSM provides a\nunified theoretical foundation for emulating various decay strategies-including\ncosine decay, linear decay and inverse square root decay-as principled model\naveraging schemes, while remaining fully compatible with diverse optimization\nmethods. Through extensive experiments, we identify merge duration-the training\nwindow for checkpoint aggregation-as the most critical factor influencing model\nperformance, surpassing the importance of both checkpoint interval and merge\nquantity. Our framework consistently outperforms the widely-adopted\nWarmup-Stable-Decay (WSD) approach across multiple benchmarks, achieving\nsignificant improvements of +3.5% on MATH, +2.9% on HumanEval, and +5.5% on\nMMLU-Pro. The performance advantages extend to supervised fine-tuning\nscenarios, highlighting WSM's potential for long-term model refinement.", "AI": {"tldr": "The paper introduces Warmup-Stable and Merge (WSM), a framework that connects learning rate decay with model merging, showing that merge duration is the key to improved model performance.", "motivation": "To provide a unified theoretical foundation for learning rate scheduling that can emulate various decay strategies while being compatible with diverse optimization methods.", "method": "WSM framework which merges the concept of learning rate decay with model averaging techniques.", "result": "WSM outperforms traditional approaches with significant improvements in multiple benchmark tests and highlights the importance of merge duration.", "conclusion": "WSM is a promising method that shows potential for enhancing model performance and could be valuable for long-term model refinement efforts."}}
{"id": "2507.17079", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.17079", "abs": "https://arxiv.org/abs/2507.17079", "authors": ["Md Meftahul Ferdaus", "Kendall N. Niles", "Joe Tom", "Mahdi Abdelguerfi", "Elias Ioup"], "title": "Few-Shot Learning in Video and 3D Object Detection: A Survey", "comment": "Under review in ACM Computing Surveys", "summary": "Few-shot learning (FSL) enables object detection models to recognize novel\nclasses given only a few annotated examples, thereby reducing expensive manual\ndata labeling. This survey examines recent FSL advances for video and 3D object\ndetection. For video, FSL is especially valuable since annotating objects\nacross frames is more laborious than for static images. By propagating\ninformation across frames, techniques like tube proposals and temporal matching\nnetworks can detect new classes from a couple examples, efficiently leveraging\nspatiotemporal structure. FSL for 3D detection from LiDAR or depth data faces\nchallenges like sparsity and lack of texture. Solutions integrate FSL with\nspecialized point cloud networks and losses tailored for class imbalance.\nFew-shot 3D detection enables practical autonomous driving deployment by\nminimizing costly 3D annotation needs. Core issues in both domains include\nbalancing generalization and overfitting, integrating prototype matching, and\nhandling data modality properties. In summary, FSL shows promise for reducing\nannotation requirements and enabling real-world video, 3D, and other\napplications by efficiently leveraging information across feature, temporal,\nand data modalities. By comprehensively surveying recent advancements, this\npaper illuminates FSL's potential to minimize supervision needs and enable\ndeployment across video, 3D, and other real-world applications.", "AI": {"tldr": "This paper reviews recent advancements in few-shot learning for video and 3D object detection, emphasizing efficient use of limited labeled data.", "motivation": "To address the challenge of high labeling costs by utilizing few-shot learning techniques in video and 3D detection, thereby minimizing the need for extensive annotated data.", "method": "The paper conducts a survey of recent techniques that leverage few-shot learning for object detection in dynamic scenes and 3D environments.", "result": "Few-shot learning shows significant potential in reducing the amount of labeled data required for model training, improving efficiency in both video and 3D detection scenarios.", "conclusion": "Through integrating few-shot learning approaches, it is possible to enhance the real-world applicability of object detection models in various scenarios, particularly where data annotation is laborious or costly."}}
{"id": "2507.17636", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.17636", "abs": "https://arxiv.org/abs/2507.17636", "authors": ["Victor Hartman", "Petter Törnberg"], "title": "Who Attacks, and Why? Using LLMs to Identify Negative Campaigning in 18M Tweets across 19 Countries", "comment": null, "summary": "Negative campaigning is a central feature of political competition, yet\nempirical research has been limited by the high cost and limited scalability of\nexisting classification methods. This study makes two key contributions. First,\nit introduces zero-shot Large Language Models (LLMs) as a novel approach for\ncross-lingual classification of negative campaigning. Using benchmark datasets\nin ten languages, we demonstrate that LLMs achieve performance on par with\nnative-speaking human coders and outperform conventional supervised machine\nlearning approaches. Second, we leverage this novel method to conduct the\nlargest cross-national study of negative campaigning to date, analyzing 18\nmillion tweets posted by parliamentarians in 19 European countries between 2017\nand 2022. The results reveal consistent cross-national patterns: governing\nparties are less likely to use negative messaging, while ideologically extreme\nand populist parties -- particularly those on the radical right -- engage in\nsignificantly higher levels of negativity. These findings advance our\nunderstanding of how party-level characteristics shape strategic communication\nin multiparty systems. More broadly, the study demonstrates the potential of\nLLMs to enable scalable, transparent, and replicable research in political\ncommunication across linguistic and cultural contexts.", "AI": {"tldr": "本研究利用大型语言模型对跨国政治推文负面竞选活动进行分类，展现了执政党和极激进右翼政党在使用负面信息上的差异及LLMs未来在政治数据分析方面所带来的潜力。", "motivation": "负面竞选活动是政治竞争中的核心特征，但是现有的分类方法成本高且扩展性有限，限制了实证研究的发展。该研究旨在克服这些限制，通过大规模的跨国家研究提供新的见解。", "method": "本研究引入零样本大型语言模型（LLMs）作为跨语言分类负面竞选活动的新型方法。通过包含十种语言的基准数据集，证明了LLMs的性能可与懂本地语言的人类编码者媲美，并优于传统的监督机器学习方法。此外，研究利用这一新方法，对2017年至2022年间19个欧洲国家议会成员发布的1800万条推文进行分析。", "result": "结果显示，执政党使用负面信息的可能性较低；而激进右翼和民粹主义政党表现出更高的负面信息比例。", "conclusion": "研究结果进一步深化了我们对政党品牌特性如何塑造多党制系统中战略沟通的理解。总体而言，该研究证明了LLMs在未来能够推动跨语言和文化背景下的政治沟通研究的发展。"}}
{"id": "2507.17083", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.17083", "abs": "https://arxiv.org/abs/2507.17083", "authors": ["Zaipeng Duan", "Chenxu Dang", "Xuzhong Hu", "Pei An", "Junfeng Ding", "Jie Zhan", "Yunbiao Xu", "Jie Ma"], "title": "SDGOCC: Semantic and Depth-Guided Bird's-Eye View Transformation for 3D Multimodal Occupancy Prediction", "comment": "accepted by CVPR2025", "summary": "Multimodal 3D occupancy prediction has garnered significant attention for its\npotential in autonomous driving. However, most existing approaches are\nsingle-modality: camera-based methods lack depth information, while LiDAR-based\nmethods struggle with occlusions. Current lightweight methods primarily rely on\nthe Lift-Splat-Shoot (LSS) pipeline, which suffers from inaccurate depth\nestimation and fails to fully exploit the geometric and semantic information of\n3D LiDAR points. Therefore, we propose a novel multimodal occupancy prediction\nnetwork called SDG-OCC, which incorporates a joint semantic and depth-guided\nview transformation coupled with a fusion-to-occupancy-driven active\ndistillation. The enhanced view transformation constructs accurate depth\ndistributions by integrating pixel semantics and co-point depth through\ndiffusion and bilinear discretization. The fusion-to-occupancy-driven active\ndistillation extracts rich semantic information from multimodal data and\nselectively transfers knowledge to image features based on LiDAR-identified\nregions. Finally, for optimal performance, we introduce SDG-Fusion, which uses\nfusion alone, and SDG-KL, which integrates both fusion and distillation for\nfaster inference. Our method achieves state-of-the-art (SOTA) performance with\nreal-time processing on the Occ3D-nuScenes dataset and shows comparable\nperformance on the more challenging SurroundOcc-nuScenes dataset, demonstrating\nits effectiveness and robustness. The code will be released at\nhttps://github.com/DzpLab/SDGOCC.", "AI": {"tldr": "This paper introduces SDG-OCC, a multimodal occupancy prediction model that improves 3D occupancy prediction for autonomous driving by integrating depth and semantic information from multimodal data, achieving real-time processing and state-of-the-art performance.", "motivation": "The motivation for this work is to overcome the limitations of existing single-modality occupancy prediction methods, which either lack depth information (camera-based) or struggle with occlusions (LiDAR-based). The goal is to improve the accuracy and robustness of 3D occupancy prediction for autonomous driving scenarios.", "method": "The proposed method, SDG-OCC, integrates a joint semantic and depth-guided view transformation with a fusion-to-occupancy-driven active distillation. It aims to construct accurate depth distributions and extract rich semantic information from multimodal data to enhance occupancy prediction.", "result": "The proposed method achieves state-of-the-art performance with real-time processing on the Occ3D-nuScenes dataset and shows comparable performance on the more challenging SurroundOcc-nuScenes dataset, indicating its effectiveness and robustness.", "conclusion": "The research concludes that the SDG-OCC network effectively enhances 3D occupancy prediction through multimodal data fusion and active distillation, surpassing existing methods' performance and providing a real-time solution. The code will also be released for further research and application."}}
{"id": "2507.17702", "categories": ["cs.CL", "I.2.7"], "pdf": "https://arxiv.org/pdf/2507.17702", "abs": "https://arxiv.org/abs/2507.17702", "authors": ["Changxin Tian", "Kunlong Chen", "Jia Liu", "Ziqi Liu", "Zhiqiang Zhang", "Jun Zhou"], "title": "Towards Greater Leverage: Scaling Laws for Efficient Mixture-of-Experts Language Models", "comment": null, "summary": "Mixture-of-Experts (MoE) has become a dominant architecture for scaling Large\nLanguage Models (LLMs) efficiently by decoupling total parameters from\ncomputational cost. However, this decoupling creates a critical challenge:\npredicting the model capacity of a given MoE configurations (e.g., expert\nactivation ratio and granularity) remains an unresolved problem. To address\nthis gap, we introduce Efficiency Leverage (EL), a metric quantifying the\ncomputational advantage of an MoE model over a dense equivalent. We conduct a\nlarge-scale empirical study, training over 300 models up to 28B parameters, to\nsystematically investigate the relationship between MoE architectural\nconfigurations and EL. Our findings reveal that EL is primarily driven by the\nexpert activation ratio and the total compute budget, both following\npredictable power laws, while expert granularity acts as a non-linear modulator\nwith a clear optimal range. We integrate these discoveries into a unified\nscaling law that accurately predicts the EL of an MoE architecture based on its\nconfiguration. To validate our derived scaling laws, we designed and trained\nLing-mini-beta, a pilot model for Ling-2.0 series with only 0.85B active\nparameters, alongside a 6.1B dense model for comparison. When trained on an\nidentical 1T high-quality token dataset, Ling-mini-beta matched the performance\nof the 6.1B dense model while consuming over 7x fewer computational resources,\nthereby confirming the accuracy of our scaling laws. This work provides a\nprincipled and empirically-grounded foundation for the scaling of efficient MoE\nmodels.", "AI": {"tldr": "研究通过引入EL度量标准和大规模实证研究，揭示了MoE模型配置与其计算优势之间的关系，并开发了一个准确预测EL的统一缩放法则，证实了法则的准确性。", "motivation": "MoE架构通过解耦参数规模和计算成本来高效扩展大型语言模型，然而，如何预测给定配置的MoE模型的能力仍然是一个问题。", "method": "引入了一个新的度量标准Efficiency Leverage (EL)，用于量化MoE模型相对于密集型等效模型的计算优势。通过大规模实证研究，系统地调查了几百个不同配置的MoE模型（参数规模高达28B）及其EL之间的关系。", "result": "研究结果表明，EL主要由专家激活比例和计算预算总量决定，这两者都遵循可预测的幂律关系，而专家粒度作为一个非线性调节因子，在一定范围内有明显的最优值。一个统一的缩放法则被整合，能够根据MoE架构配置准确预测其EL。通过对比训练量为0.85B的Ling-mini-beta和6.1B的密集型模型，验证了该缩放法则的准确性。", "conclusion": "这项工作提供了高效MoE模型扩展的原理基础和实证依据，有助于更好地理解和提升模型架构设计。"}}
{"id": "2507.17088", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.17088", "abs": "https://arxiv.org/abs/2507.17088", "authors": ["Arkajyoti Mitra", "Afia Anjum", "Paul Agbaje", "Mert Pesé", "Habeeb Olufowobi"], "title": "FedVLM: Scalable Personalized Vision-Language Models through Federated Learning", "comment": null, "summary": "Vision-language models (VLMs) demonstrate impressive zero-shot and few-shot\nlearning capabilities, making them essential for several downstream tasks.\nHowever, fine-tuning these models at scale remains challenging, particularly in\nfederated environments where data is decentralized and non-iid across clients.\nExisting parameter-efficient tuning methods like LoRA (Low-Rank Adaptation)\nreduce computational overhead but struggle with heterogeneous client data,\nleading to suboptimal generalization. To address these challenges, we propose\nFedVLM, a federated LoRA fine-tuning framework that enables decentralized\nadaptation of VLMs while preserving model privacy and reducing reliance on\ncentralized training. To further tackle data heterogeneity, we introduce\npersonalized LoRA (pLoRA), which dynamically adapts LoRA parameters to each\nclient's unique data distribution, significantly improving local adaptation\nwhile maintaining global model aggregation. Experiments on the RLAIF-V dataset\nshow that pLoRA improves client-specific performance by 24.5% over standard\nLoRA, demonstrating superior adaptation in non-iid settings. FedVLM provides a\nscalable and efficient solution for fine-tuning VLMs in federated settings,\nadvancing personalized adaptation in distributed learning scenarios.", "AI": {"tldr": "提出FedVLM和pLoRA新技术以解决视觉语言模型在异构数据和分布式环境中微调的挑战。实验表明新的方法显著提升了模型在非独立同分布环境中的性能。", "motivation": "视觉语言模型（VLMs）虽然具有出色的零样本和小样本学习能力，但在大规模微调特别是在分布式环境中面临着挑战。现有的参数高效微调方法如LoRA在处理异构客户端数据时效果不佳，导致泛化能力较差。因此需提出一种新的解决方案来解决这些问题。", "method": "我们提出了一种名为FedVLM的联邦LoRA微调框架，该框架允许在保持模型隐私的同时在去中心化的环境中适应视觉语言模型。此外，我们引入了个性化LoRA（pLoRA），以动态适应每个客户端的独特数据分布，从而在保持全局模型聚合的同时提高了本地适应性。", "result": "在RLAIF-V数据集上的实验表明，pLoRA将客户端特定性能提升了24.5%，展示了它在非独立同分布环境中的优越适应性。", "conclusion": "FedVLM提供了一种在联邦设置中微调视觉语言模型的可扩展且高效的解决方案，并推进了分布式学习场景中的个性化适应。"}}
{"id": "2507.17709", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.17709", "abs": "https://arxiv.org/abs/2507.17709", "authors": ["Parker Riley", "Siamak Shakeri", "Waleed Ammar", "Jonathan H. Clark"], "title": "TyDi QA-WANA: A Benchmark for Information-Seeking Question Answering in Languages of West Asia and North Africa", "comment": null, "summary": "We present TyDi QA-WANA, a question-answering dataset consisting of 28K\nexamples divided among 10 language varieties of western Asia and northern\nAfrica. The data collection process was designed to elicit information-seeking\nquestions, where the asker is genuinely curious to know the answer. Each\nquestion in paired with an entire article that may or may not contain the\nanswer; the relatively large size of the articles results in a task suitable\nfor evaluating models' abilities to utilize large text contexts in answering\nquestions. Furthermore, the data was collected directly in each language\nvariety, without the use of translation, in order to avoid issues of cultural\nrelevance. We present performance of two baseline models, and release our code\nand data to facilitate further improvement by the research community.", "AI": {"tldr": "TyDi QA-WANA是一个旨在评估模型在西亚和北非的10种语言变体中利用大文本上下文回答问题能力的问答数据集，包含28K个信息寻求类问题。", "motivation": "动机在于提供一个多语言问答数据集，用于评估模型在未经过翻译直接在各种语言变体中处理问题和回答的能力，并特别考虑了文化相关性问题。", "method": "TyDi QA-WANA是一个问答数据集，包含28K个示例，涵盖了西亚和北非的10种语言变体。这个数据集的收集过程旨在激发知识探索类型的问题，即提问者真正在乎问题的答案。每个问题都与一篇文章配对，这篇文章可能包含也可能不包含答案。为了评估模型利用大量文本上下文回答问题的能力，文章的篇幅较大。此外，数据直接在每种语言变体中收集，避免了文化相关性的问题。", "result": "文中呈现了两个基线模型的性能，并且公开了代码和数据，以便研究社区进一步改进。", "conclusion": "该研究创建了一个具有挑战性的多语言问答数据集，并向社区发布了相关数据和模型代码，以促进该领域的进一步研究和发展。"}}
{"id": "2507.17089", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.17089", "abs": "https://arxiv.org/abs/2507.17089", "authors": ["Shanshan Zhang", "Siyue Wang", "Tianshui Wen", "Qi Zhang", "Ziheng Zhou", "Lingxiang Zheng", "Yu Yang"], "title": "IONext: Unlocking the Next Era of Inertial Odometry", "comment": null, "summary": "Researchers have increasingly adopted Transformer-based models for inertial\nodometry. While Transformers excel at modeling long-range dependencies, their\nlimited sensitivity to local, fine-grained motion variations and lack of\ninherent inductive biases often hinder localization accuracy and\ngeneralization. Recent studies have shown that incorporating large-kernel\nconvolutions and Transformer-inspired architectural designs into CNN can\neffectively expand the receptive field, thereby improving global motion\nperception. Motivated by these insights, we propose a novel CNN-based module\ncalled the Dual-wing Adaptive Dynamic Mixer (DADM), which adaptively captures\nboth global motion patterns and local, fine-grained motion features from\ndynamic inputs. This module dynamically generates selective weights based on\nthe input, enabling efficient multi-scale feature aggregation. To further\nimprove temporal modeling, we introduce the Spatio-Temporal Gating Unit (STGU),\nwhich selectively extracts representative and task-relevant motion features in\nthe temporal domain. This unit addresses the limitations of temporal modeling\nobserved in existing CNN approaches. Built upon DADM and STGU, we present a new\nCNN-based inertial odometry backbone, named Next Era of Inertial Odometry\n(IONext). Extensive experiments on six public datasets demonstrate that IONext\nconsistently outperforms state-of-the-art (SOTA) Transformer- and CNN-based\nmethods. For instance, on the RNIN dataset, IONext reduces the average ATE by\n10% and the average RTE by 12% compared to the representative model iMOT.", "AI": {"tldr": "本文提出了一个名为IONext的CNN基础模型，通过引入DADM和STGU组件，能更好地捕捉全局和局部运动特征，从而改进了惯性里程计的性能，表现出超越现有方法的结果。", "motivation": "尽管Transformer模型在建模长距离依赖方面表现出色，但这类模型对局部细粒度运动变化的敏感性有限，缺乏先天的归纳偏置，从而影响了定位精度和泛化能力。因此，本文旨在通过一个新的CNN模块来克服这些问题。", "method": "本文提出了一个新的基于CNN的模块，称为双翼自适应动态混合器（DADM），能够自适应地捕获动态输入中的全局运动模式和局部细粒度的运动特征。该模块能够根据输入动态生成选择性权重，提高多尺度特征聚集的效率。此外，还引入了一个名为时空门控单元（STGU）的组件，改进了时间建模。基于DADM和STGU，构建了一个名为IONext的新CNN惯性里程计骨干模型。", "result": "实验结果表明，IONext在六个公共数据集上的性能始终优于最先进的Transformer和CNN方法。例如，在RNIN数据集上，IONext相比代表性模型iMOT将平均ATE减少了10%，平均RTE减少了12%。", "conclusion": "实验验证了本文提出的IONext模型在惯性里程计任务上具有卓越的性能，尤其是在局部和全局运动特征的捕捉方面，证明了其作为高级CNN方法的有效性和优越性。"}}
{"id": "2507.17717", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.17717", "abs": "https://arxiv.org/abs/2507.17717", "authors": ["Karen Zhou", "John Giorgi", "Pranav Mani", "Peng Xu", "Davis Liang", "Chenhao Tan"], "title": "From Feedback to Checklists: Grounded Evaluation of AI-Generated Clinical Notes", "comment": null, "summary": "AI-generated clinical notes are increasingly used in healthcare, but\nevaluating their quality remains a challenge due to high subjectivity and\nlimited scalability of expert review. Existing automated metrics often fail to\nalign with real-world physician preferences. To address this, we propose a\npipeline that systematically distills real user feedback into structured\nchecklists for note evaluation. These checklists are designed to be\ninterpretable, grounded in human feedback, and enforceable by LLM-based\nevaluators. Using deidentified data from over 21,000 clinical encounters,\nprepared in accordance with the HIPAA safe harbor standard, from a deployed AI\nmedical scribe system, we show that our feedback-derived checklist outperforms\nbaseline approaches in our offline evaluations in coverage, diversity, and\npredictive power for human ratings. Extensive experiments confirm the\nchecklist's robustness to quality-degrading perturbations, significant\nalignment with clinician preferences, and practical value as an evaluation\nmethodology. In offline research settings, the checklist can help identify\nnotes likely to fall below our chosen quality thresholds.", "AI": {"tldr": "本研究针对AI生成的临床笔记质量评估提出了基于系统化的用户反馈检查表方法，该方法能够更好地反映临床医生的真实偏好，并在多个方面优于现有的自动化评估方法。", "motivation": "现有的自动化指标往往未能与真正的临床医生偏好相一致，因此，研究目的是解决AI生成的临床记录的质量评价难题。", "method": "提出了一种将真实用户反馈系统地提炼成结构化检查表的流水线方法。该检查表旨在可解释、基于人类反馈、并通过基于大规模语言模型（LLM）的评估者来强制执行。", "result": "利用来自超过21,000次临床会诊的去标识化数据进行的离线评估表明，与基线方法相比，研究生成的反馈检查表在覆盖范围、多样性和预测力方面更加出色。", "conclusion": "实验表明，该检查表对质量退化扰动有较强的鲁棒性，与临床医生偏好高度一致，证明了作为评估方法的有效性。离线研究环境中，该检查表有助于识别质量可能低于设定阈值的笔记。"}}
{"id": "2507.17121", "categories": ["cs.CV", "cs.LG", "F.2.2; I.2.7"], "pdf": "https://arxiv.org/pdf/2507.17121", "abs": "https://arxiv.org/abs/2507.17121", "authors": ["Faisal Ahmed", "Mohammad Alfrad Nobel Bhuiyan"], "title": "Robust Five-Class and binary Diabetic Retinopathy Classification Using Transfer Learning and Data Augmentation", "comment": "9 pages, 1 Figure", "summary": "Diabetic retinopathy (DR) is a leading cause of vision loss worldwide, and\nearly diagnosis through automated retinal image analysis can significantly\nreduce the risk of blindness. This paper presents a robust deep learning\nframework for both binary and five-class DR classification, leveraging transfer\nlearning and extensive data augmentation to address the challenges of class\nimbalance and limited training data. We evaluate a range of pretrained\nconvolutional neural network architectures, including variants of ResNet and\nEfficientNet, on the APTOS 2019 dataset.\n  For binary classification, our proposed model achieves a state-of-the-art\naccuracy of 98.9%, with a precision of 98.6%, recall of 99.3%, F1-score of\n98.9%, and an AUC of 99.4%. In the more challenging five-class severity\nclassification task, our model obtains a competitive accuracy of 84.6% and an\nAUC of 94.1%, outperforming several existing approaches. Our findings also\ndemonstrate that EfficientNet-B0 and ResNet34 offer optimal trade-offs between\naccuracy and computational efficiency across both tasks.\n  These results underscore the effectiveness of combining class-balanced\naugmentation with transfer learning for high-performance DR diagnosis. The\nproposed framework provides a scalable and accurate solution for DR screening,\nwith potential for deployment in real-world clinical environments.", "AI": {"tldr": "论文提出了一种用于DR分类的深度学习框架，利用迁移学习和数据增强技术，实现了在APTOS 2019数据集上的二分类和五分类任务的高准确率，展示了在DR诊断中的实际应用潜力。", "motivation": "DR是全球视力丧失的主要原因，通过自动化视网膜图像分析进行早期诊断可以显著降低失明的风险。因此，研究高效准确的DR诊断方法是非常有意义的。", "method": "本论文提出了一种基于深度学习的框架，用于糖尿病视网膜病变（DR）的二分类和五分类，采用迁移学习和大量的数据增强来应对类别不平衡和训练数据有限的挑战。详细来说，他们评估了包括ResNet和EfficientNet在内的预训练卷积神经网络架构在APTOS 2019数据集上的表现。", "result": "在二分类任务中，该模型达到了98.9%的最高准确率，精确率为98.6%，召回率为99.3%，F1分数为98.9%，AUC为99.4%。在更具有挑战性的五分类严重程度分类任务中，模型获得了84.6%的准确率和94.1%的AUC，优于几种现有的方法。同时发现EfficientNet-B0和ResNet34在精确率和计算效率之间提供了最优的平衡。", "conclusion": "结合类别平衡增强和迁移学习的框架，展现了在DR诊断中的高效性能。该框架提供了一种可供扩展且准确的DR筛查方案，有可能在实际临床环境中部署。"}}
{"id": "2507.17718", "categories": ["cs.CL", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.17718", "abs": "https://arxiv.org/abs/2507.17718", "authors": ["Danny D. Leybzon", "Shreyas Tirumala", "Nishant Jain", "Summer Gillen", "Michael Jackson", "Cameron McPhee", "Jennifer Schmidt"], "title": "AI Telephone Surveying: Automating Quantitative Data Collection with an AI Interviewer", "comment": null, "summary": "With the rise of voice-enabled artificial intelligence (AI) systems,\nquantitative survey researchers have access to a new data-collection mode: AI\ntelephone surveying. By using AI to conduct phone interviews, researchers can\nscale quantitative studies while balancing the dual goals of human-like\ninteractivity and methodological rigor. Unlike earlier efforts that used\ninteractive voice response (IVR) technology to automate these surveys, voice AI\nenables a more natural and adaptive respondent experience as it is more robust\nto interruptions, corrections, and other idiosyncrasies of human speech.\n  We built and tested an AI system to conduct quantitative surveys based on\nlarge language models (LLM), automatic speech recognition (ASR), and speech\nsynthesis technologies. The system was specifically designed for quantitative\nresearch, and strictly adhered to research best practices like question order\nrandomization, answer order randomization, and exact wording.\n  To validate the system's effectiveness, we deployed it to conduct two pilot\nsurveys with the SSRS Opinion Panel and followed-up with a separate\nhuman-administered survey to assess respondent experiences. We measured three\nkey metrics: the survey completion rates, break-off rates, and respondent\nsatisfaction scores. Our results suggest that shorter instruments and more\nresponsive AI interviewers may contribute to improvements across all three\nmetrics studied.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2507.17149", "categories": ["cs.CV", "cs.AI", "cs.LG", "I.4.6"], "pdf": "https://arxiv.org/pdf/2507.17149", "abs": "https://arxiv.org/abs/2507.17149", "authors": ["Bo Fang", "Jianan Fan", "Dongnan Liu", "Hang Chang", "Gerald J. Shami", "Filip Braet", "Weidong Cai"], "title": "ScSAM: Debiasing Morphology and Distributional Variability in Subcellular Semantic Segmentation", "comment": "Accepted by 28th European Conference on Artificial Intelligence\n  (ECAI)", "summary": "The significant morphological and distributional variability among\nsubcellular components poses a long-standing challenge for learning-based\norganelle segmentation models, significantly increasing the risk of biased\nfeature learning. Existing methods often rely on single mapping relationships,\noverlooking feature diversity and thereby inducing biased training. Although\nthe Segment Anything Model (SAM) provides rich feature representations, its\napplication to subcellular scenarios is hindered by two key challenges: (1) The\nvariability in subcellular morphology and distribution creates gaps in the\nlabel space, leading the model to learn spurious or biased features. (2) SAM\nfocuses on global contextual understanding and often ignores fine-grained\nspatial details, making it challenging to capture subtle structural alterations\nand cope with skewed data distributions. To address these challenges, we\nintroduce ScSAM, a method that enhances feature robustness by fusing\npre-trained SAM with Masked Autoencoder (MAE)-guided cellular prior knowledge\nto alleviate training bias from data imbalance. Specifically, we design a\nfeature alignment and fusion module to align pre-trained embeddings to the same\nfeature space and efficiently combine different representations. Moreover, we\npresent a cosine similarity matrix-based class prompt encoder to activate\nclass-specific features to recognize subcellular categories. Extensive\nexperiments on diverse subcellular image datasets demonstrate that ScSAM\noutperforms state-of-the-art methods.", "AI": {"tldr": "提出ScSAM方法以克服亚细胞场景下的学习偏差和细粒度细节丢失，通过融合SAM和MAE知识，在各种亚细胞图像数据集中表现出优于现有最先进方法的性能。", "motivation": "解决亚细胞成分形态和分布的显著变异性所带来的挑战，这种变异性增加了基于学习的细胞器分割模型产生偏差特征学习的风险。", "method": "引入了ScSAM方法，通过将预训练的SAM与Masked Autoencoder (MAE)引导的细胞先验知识融合，以减轻由于数据不平衡带来的训练偏差。具体来说，设计了一个特征对齐和融合模块，将预训练嵌入对齐到同一个特征空间，并有效结合不同的表示形式。此外，提出了一种基于余弦相似度矩阵的分类提示编码器，激活类别特定特征以识别亚细胞类别。", "result": "在多种亚细胞图像数据集上的广泛实验表明，ScSAM优于现有最先进的方法。", "conclusion": "ScSAM通过融合SAM和MAE引导的知识，解决了SAM在亚细胞场景应用上的两个关键挑战：特征偏差学习和细粒度空间细节的忽略。这种方法在多个亚细胞图像数据集上表现出色，超越了当前最先进的方法。"}}
{"id": "2507.17728", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.17728", "abs": "https://arxiv.org/abs/2507.17728", "authors": ["Boxun Li", "Yadong Li", "Zhiyuan Li", "Congyi Liu", "Weilin Liu", "Guowei Niu", "Zheyue Tan", "Haiyang Xu", "Zhuyu Yao", "Tao Yuan", "Dong Zhou", "Yueqing Zhuang", "Bo Zhao", "Guohao Dai", "Yu Wang"], "title": "Megrez2 Technical Report", "comment": null, "summary": "We present Megrez2, a novel lightweight and high-performance language model\narchitecture optimized for device native deployment. Megrez2 introduces a novel\ncross-layer expert sharing mechanism, which significantly reduces total\nparameter count by reusing expert modules across adjacent transformer layers\nwhile maintaining most of the model's capacity. It also incorporates pre-gated\nrouting, enabling memory-efficient expert loading and faster inference. As the\nfirst instantiation of the Megrez2 architecture, we introduce the\nMegrez2-Preview model, which is pre-trained on a 5-trillion-token corpus and\nfurther enhanced through supervised fine-tuning and reinforcement learning with\nverifiable rewards. With only 3B activated and 7.5B stored parameters,\nMegrez2-Preview demonstrates competitive or superior performance compared to\nlarger models on a wide range of tasks, including language understanding,\ninstruction following, mathematical reasoning, and code generation. These\nresults highlight the effectiveness of the Megrez2 architecture to achieve a\nbalance between accuracy, efficiency, and deployability, making it a strong\ncandidate for real-world, resource-constrained applications.", "AI": {"tldr": "Megrez2-Preview：参数量少但性能优秀的轻量级语言模型，适用于资源受限的应用。", "motivation": "开发一种轻量级且高性能的语言模型架构，适用于设备端，能够在保持模型性能的同时降低资源需求。", "method": "引入跨层专家共享机制和预门控路由技术，减少参数量，提升推理速度并降低内存消耗。", "result": "Megrez2 是一种轻量且高性能的语言模型架构，专为设备本地部署优化。通过引入跨层专家共享机制和预门控路由，Megrez2-Preview 版本在参数量显著减少的同时保持了模型大部分能力，并展现了在多种任务上的竞争力或优越性能。", "conclusion": "Megrez2-Preview 在语言理解、指令跟随、数学推理和代码生成等任务中表现出色，证明了 Megrez2 架构在准确性、效率和部署性之间的良好平衡。"}}
{"id": "2507.17157", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.17157", "abs": "https://arxiv.org/abs/2507.17157", "authors": ["Ruodai Cui", "Lei Zhang"], "title": "UNICE: Training A Universal Image Contrast Enhancer", "comment": null, "summary": "Existing image contrast enhancement methods are typically designed for\nspecific tasks such as under-/over-exposure correction, low-light and backlit\nimage enhancement, etc. The learned models, however, exhibit poor\ngeneralization performance across different tasks, even across different\ndatasets of a specific task. It is important to explore whether we can learn a\nuniversal and generalized model for various contrast enhancement tasks. In this\nwork, we observe that the common key factor of these tasks lies in the need of\nexposure and contrast adjustment, which can be well-addressed if high-dynamic\nrange (HDR) inputs are available. We hence collect 46,928 HDR raw images from\npublic sources, and render 328,496 sRGB images to build multi-exposure\nsequences (MES) and the corresponding pseudo sRGB ground-truths via\nmulti-exposure fusion. Consequently, we train a network to generate an MES from\na single sRGB image, followed by training another network to fuse the generated\nMES into an enhanced image. Our proposed method, namely UNiversal Image\nContrast Enhancer (UNICE), is free of costly human labeling. However, it\ndemonstrates significantly stronger generalization performance than existing\nimage contrast enhancement methods across and within different tasks, even\noutperforming manually created ground-truths in multiple no-reference image\nquality metrics. The dataset, code and model are available at\nhttps://github.com/BeyondHeaven/UNICE.", "AI": {"tldr": "UNICE方法利用HDR图像生成多曝光序列并融合，实现了高效的图像对比度增强，展现出优于特定任务模型的泛化性能。", "motivation": "现有的图像对比度增强方法通常为特定任务设计，模型在不同任务或同一任务的不同数据集上的泛化能力较弱。研究的目的在于探索是否可以学习到一种普遍适用和富有泛化能力的对比度增强模型。", "method": "我们提出了名为UNICE的方法，通过收集46,928张HDR原始图像，并渲染出328,496张sRGB图像来构建多曝光序列（MES）以及相应的伪sRGB真实值，以此训练网络从一张sRGB图像生成MES，再融合成增强图像。", "result": "所提出的方法在不同的对比度增强任务中展示了比现有方法更强的泛化性能，甚至在无参考的图像质量指标中超过了手动创建的真实值。", "conclusion": "UNICE方法证明了通过学习一种普遍的对比度增强模型能够在不同任务间提供显著的性能提升，而无需耗费人力标注。"}}
{"id": "2507.17747", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.17747", "abs": "https://arxiv.org/abs/2507.17747", "authors": ["Linbo Cao", "Jinman Zhao"], "title": "Pretraining on the Test Set Is No Longer All You Need: A Debate-Driven Approach to QA Benchmarks", "comment": "22 pages, 7 figures. Accepted to COLM 2025. Code available at:\n  github.com/l6cao/Debate-Driven-Evaluation", "summary": "As frontier language models increasingly saturate standard QA benchmarks,\nconcerns about data contamination, memorization, and escalating dataset\ncreation costs persist. We propose a debate-driven evaluation paradigm that\ntransforms any existing QA dataset into structured adversarial debates--where\none model is given the official answer to defend, and another constructs and\ndefends an alternative answer--adjudicated by a judge model blind to the\ncorrect solution. By forcing multi-round argumentation, this approach\nsubstantially increases difficulty while penalizing shallow memorization, yet\nreuses QA items to reduce curation overhead. We make two main contributions:\n(1) an evaluation pipeline to systematically convert QA tasks into debate-based\nassessments, and (2) a public benchmark that demonstrates our paradigm's\neffectiveness on a subset of MMLU-Pro questions, complete with standardized\nprotocols and reference models. Empirical results validate the robustness of\nthe method and its effectiveness against data contamination--a Llama 3.1 model\nfine-tuned on test questions showed dramatic accuracy improvements (50% -> 82%)\nbut performed worse in debates. Results also show that even weaker judges can\nreliably differentiate stronger debaters, highlighting how debate-based\nevaluation can scale to future, more capable systems while maintaining a\nfraction of the cost of creating new benchmarks. Overall, our framework\nunderscores that \"pretraining on the test set is no longer all you need,\"\noffering a sustainable path for measuring the genuine reasoning ability of\nadvanced language models.", "AI": {"tldr": "本文提出了一种新的基于辩论的评估方法，通过让模型进行对抗性的多轮辩论来评估其性能，这种方法可以有效减少数据污染和记忆效应，并降低了数据集创建的成本。", "motivation": "随着前沿语言模型在标准问答基准上变得愈发普及，关于数据污染、记忆效应和不断上升的数据集成本的问题依然存在。本文试图通过新的评估范式解决这些问题，提出了一种更有效、低成本的评估方法。", "method": "本文提出了一种基于辩论的评估范式，将现有的问答数据集转换为结构化的对抗辩论，其中一端模型要辩护官方答案，另一端模型则构建并防守一个替代答案，这些辩论由一个对正确答案不知情的裁判模型判定。通过这种多轮论辩的方式，评估方法显著增加了难度，并且对浅层次记忆进行了惩罚，同时利用问答项目减少了策划成本。", "result": "实验结果证明了该方法的鲁棒性和其在抵御数据污染方面的有效性。即使对测试问题进行微调的Llama 3.1模型其准确性大幅提高，但辩论中的表现却更差。此外，即使是较弱的裁判也能可靠地区分较强的辩论者，证明了基于辩论的评估方法更适用于未来更强大的系统，同时其成本较低。", "conclusion": "本文提出的技术框架强调了“测试集上的预训练不再是衡量高级语言模型真正推理能力的唯一方法”，并且提供了一条可持续的道路来衡量先进语言模型的真正确推理能力。"}}
{"id": "2507.17158", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.17158", "abs": "https://arxiv.org/abs/2507.17158", "authors": ["Bharath Krishnamurthy", "Ajita Rattani"], "title": "DOOMGAN:High-Fidelity Dynamic Identity Obfuscation Ocular Generative Morphing", "comment": "Accepted to IJCB 2025 (IEEE/IAPR International Joint Conference on\n  Biometrics). 11 pages with references, 8-page main paper with 4 figures and 4\n  tables. Includes 6 pages of supplementary material with 3 additional figures\n  and 3 tables. Code is available at the official lab repository:\n  https://github.com/vcbsl/DOOMGAN and the author's repository:\n  https://github.com/Bharath-K3/DOOMGAN", "summary": "Ocular biometrics in the visible spectrum have emerged as a prominent\nmodality due to their high accuracy, resistance to spoofing, and non-invasive\nnature. However, morphing attacks, synthetic biometric traits created by\nblending features from multiple individuals, threaten biometric system\nintegrity. While extensively studied for near-infrared iris and face\nbiometrics, morphing in visible-spectrum ocular data remains underexplored.\nSimulating such attacks demands advanced generation models that handle\nuncontrolled conditions while preserving detailed ocular features like iris\nboundaries and periocular textures. To address this gap, we introduce DOOMGAN,\nthat encompasses landmark-driven encoding of visible ocular anatomy,\nattention-guided generation for realistic morph synthesis, and dynamic\nweighting of multi-faceted losses for optimized convergence. DOOMGAN achieves\nover 20% higher attack success rates than baseline methods under stringent\nthresholds, along with 20% better elliptical iris structure generation and 30%\nimproved gaze consistency. We also release the first comprehensive ocular\nmorphing dataset to support further research in this domain.", "AI": {"tldr": "本文针对可见光谱下的眼部生物识别中的形态攻击问题，提出DOOMGAN模型，并发布了首个眼部形态数据集。", "motivation": "形态攻击对生物识别系统的完整性构成威胁，尤其是对于可见光谱下的眼部生物特征，这一领域尚未得到充分研究。", "method": "我们提出了DOOMGAN模型，该模型通过使用基于地标点的可见光眼部解剖结构编码、注意力引导的生成方法以合成真实的形态特征，并采用多方面损失的动态加权优化收敛。", "result": "DOOMGAN在严格的阈值条件下，比基准方法的攻击成功率高出20%以上，椭圆形虹膜结构生成更优（提高了20%），以及更好的凝视一致性（提高了30%）。", "conclusion": "通过引入DOOMGAN和首个全面的眼部形态数据集，本研究填补了可见光谱眼部生物识别领域形态攻击研究的空白。"}}
{"id": "2507.17176", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.17176", "abs": "https://arxiv.org/abs/2507.17176", "authors": ["Li Pingzhen", "Xu Sheng", "Chen Jing", "Su Chengyue"], "title": "Multi-Scale PCB Defect Detection with YOLOv8 Network Improved via Pruning and Lightweight Network", "comment": null, "summary": "With the high density of printed circuit board (PCB) design and the high\nspeed of production, the traditional PCB defect detection model is difficult to\ntake into account the accuracy and computational cost, and cannot meet the\nrequirements of high accuracy and real-time detection of tiny defects.\nTherefore, in this paper, a multi-scale PCB defect detection method is improved\nwith YOLOv8 using a comprehensive strategy of tiny target sensitivity strategy,\nnetwork lightweighting and adaptive pruning, which is able to improve the\ndetection speed and accuracy by optimizing the backbone network, the neck\nnetwork and the detection head, the loss function and the adaptive pruning\nrate. Firstly, a Ghost-HGNetv2 structure with fewer parameters is used in the\nbackbone network, and multilevel features are used to extract image semantic\nfeatures to discover accurate defects. Secondly, we integrate C2f-Faster with\nsmall number of parameters in the neck section to enhance the ability of\nmulti-level feature fusion. Next, in the Head part, we design a new GCDetect\ndetection head, which allows the prediction of bounding boxes and categories to\nshare the weights of GroupConv, and uses a small number of grouping\nconvolutions to accomplish the regression and classification tasks, which\nsignificantly reduces the number of parameters while maintaining the accuracy\nof detection. We also design the Inner-MPDIoU boundary loss function to improve\nthe detection and localization of tiny targets. Finally, the model was pruned\nby an optimized adaptive pruning rate to further reduce the complexity of the\nmodel. Experimental results show that the model exhibits advantages in terms of\naccuracy and speed. On the publicly available PCB defect dataset, mAP0.5\nreaches 99.32% and mAP0.5:0.9 reaches 75.18%, which is 10.13% higher compared\nto YOLOv8n.", "AI": {"tldr": "本文提出了一种基于YOLOv8的改进多尺度PCB缺陷检测方法，通过优化网络结构和损失函数，实现了在高精度检测方面的提升，特别是在检测小缺陷方面具有明显优势。", "motivation": "传统的PCB缺陷检测模型难以兼顾准确性和计算成本，在高密度、高速生产的要求下无法满足高精度实时检测小缺陷的需求。", "method": "提出了一种基于YOLOv8的多尺度PCB缺陷检测方法，采用了小目标敏感策略、网络轻量化和自适应剪枝的综合策略，优化了骨干网络、颈部网络和检测头、损失函数以及自适应剪枝率。具体来说，在骨干网络中使用了参数较少的Ghost-HGNetv2结构，利用多级特征提取图像语义特征；在颈部部分集成了参数较少的C2f-Faster，增强多级特征融合；在检测头部分，设计了一种新的GCDetect检测头，利用组卷积共享权重，减少了参数量，同时保留了检测准确性，设计了内部-MPDIoU边界损失函数以改善小目标的检测和定位；最后通过优化的自适应剪枝率进一步减少了模型复杂度。", "result": "实验结果表明，该模型在精确度和速度方面表现出优势，在公开的PCB缺陷数据集上，mAP0.5达到了99.32%，mAP0.5:0.9达到了75.18%，比YOLOv8n高出10.13%。", "conclusion": "改进的多尺度PCB缺陷检测方法通过优化网络架构和损失函数设计，在保持高检测精度的同时，提高了检测速度，并成功应用于实际的PCB缺陷检测任务中。"}}
{"id": "2507.17182", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.17182", "abs": "https://arxiv.org/abs/2507.17182", "authors": ["Linghe Meng", "Jiarun Song"], "title": "Hierarchical Fusion and Joint Aggregation: A Multi-Level Feature Representation Method for AIGC Image Quality Assessment", "comment": null, "summary": "The quality assessment of AI-generated content (AIGC) faces multi-dimensional\nchallenges, that span from low-level visual perception to high-level semantic\nunderstanding. Existing methods generally rely on single-level visual features,\nlimiting their ability to capture complex distortions in AIGC images. To\naddress this limitation, a multi-level visual representation paradigm is\nproposed with three stages, namely multi-level feature extraction, hierarchical\nfusion, and joint aggregation. Based on this paradigm, two networks are\ndeveloped. Specifically, the Multi-Level Global-Local Fusion Network (MGLF-Net)\nis designed for the perceptual quality assessment, extracting complementary\nlocal and global features via dual CNN and Transformer visual backbones. The\nMulti-Level Prompt-Embedded Fusion Network (MPEF-Net) targets Text-to-Image\ncorrespondence by embedding prompt semantics into the visual feature fusion\nprocess at each feature level. The fused multi-level features are then\naggregated for final evaluation. Experiments on benchmarks demonstrate\noutstanding performance on both tasks, validating the effectiveness of the\nproposed multi-level visual assessment paradigm.", "AI": {"tldr": "论文提出了一个针对AI生成内容质量评估的多级视觉表示范式，开发了两种新网络，并在实验证明其有效性。", "motivation": "现有的方法通常依赖单级视觉特征，不能充分捕捉AIGC图像中的复杂失真。为解决这种局限性，提出了多级视觉表示范式。", "method": "提出了一种多级视觉表示范式，包含三个阶段：多级特征提取、层级融合和联合聚合。基于此范式，开发了两种网络：一种为多级全局局部融合网络（MGLF-Net），用于感知质量评估；另一种为多级提示嵌入融合网络（MPEF-Net），用于文本到图像的对应评估。该范式在实验中展示了出色的表现。", "result": "在基准测试上的实验表明，所开发的网络在感知质量和文本到图像的对应任务中均表现出色。", "conclusion": "实验证明该多级视觉评估范式在感知质量和文本到图像对应评定任务中都表现出色，证实了其有效性。"}}
{"id": "2507.17335", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.17335", "abs": "https://arxiv.org/abs/2507.17335", "authors": ["Guangzhu Xu", "Zhi Ke", "Pengcheng Zuo", "Bangjun Lei"], "title": "TransLPRNet: Lite Vision-Language Network for Single/Dual-line Chinese License Plate Recognition", "comment": null, "summary": "License plate recognition in open environments is widely applicable across\nvarious domains; however, the diversity of license plate types and imaging\nconditions presents significant challenges. To address the limitations\nencountered by CNN and CRNN-based approaches in license plate recognition, this\npaper proposes a unified solution that integrates a lightweight visual encoder\nwith a text decoder, within a pre-training framework tailored for single and\ndouble-line Chinese license plates. To mitigate the scarcity of double-line\nlicense plate datasets, we constructed a single/double-line license plate\ndataset by synthesizing images, applying texture mapping onto real scenes, and\nblending them with authentic license plate images. Furthermore, to enhance the\nsystem's recognition accuracy, we introduce a perspective correction network\n(PTN) that employs license plate corner coordinate regression as an implicit\nvariable, supervised by license plate view classification information. This\nnetwork offers improved stability, interpretability, and low annotation costs.\nThe proposed algorithm achieves an average recognition accuracy of 99.34% on\nthe corrected CCPD test set under coarse localization disturbance. When\nevaluated under fine localization disturbance, the accuracy further improves to\n99.58%. On the double-line license plate test set, it achieves an average\nrecognition accuracy of 98.70%, with processing speeds reaching up to 167\nframes per second, indicating strong practical applicability.", "AI": {"tldr": "本文提出了一种结合轻量级视觉编码器和文本解码器的方法来解决开放环境下车牌识别的挑战，实现了在多种条件下的高精度识别，并提高了处理速度。", "motivation": "旨在解决当前CNN和CRNN方法在车牌识别中遇到的局限性，特别是在开放环境下的多类型车牌和成像条件所带来的挑战。", "method": "本文提出了一种结合轻量级视觉编码器和文本解码器的统一解决方案，并在单行和双行中文车牌识别上进行了预训练。为了解决双行车牌数据集稀缺的问题，本文通过合成图像、纹理映射到真实场景并融合真实车牌图像的方式构建了一个单行/双行车牌数据集。此外，为了提高系统的识别精度，本文还引入了一个视角校正网络(PTN)，该网络利用车牌角点坐标回归作为隐变量，并使用车牌视角分类信息进行监督。", "result": "所提出的算法在粗略定位扰动下的CCPD测试集上达到了99.34%的平均识别精度，在精细定位扰动下进一步提升到了99.58%。在双行车牌测试集上，也达到了98.70%的平均识别精度，处理速度达到了每秒167帧。", "conclusion": "提出的解决方案不仅提高了车牌识别的稳定性、可解释性和降低了标注成本，而且在实际应用中表现出了强大的适用性。"}}
{"id": "2507.17185", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.17185", "abs": "https://arxiv.org/abs/2507.17185", "authors": ["M. A. Rasel", "Sameem Abdul Kareem", "Zhenli Kwan", "Nik Aimee Azizah Faheem", "Winn Hui Han", "Rebecca Kai Jan Choong", "Shin Shen Yong", "Unaizah Obaidellah"], "title": "Asymmetric Lesion Detection with Geometric Patterns and CNN-SVM Classification", "comment": "Accepted version. Published in Computers in Biology and Medicine,\n  Volume 179, 2024. DOI: 10.1016/j.compbiomed.2024.108851", "summary": "In dermoscopic images, which allow visualization of surface skin structures\nnot visible to the naked eye, lesion shape offers vital insights into skin\ndiseases. In clinically practiced methods, asymmetric lesion shape is one of\nthe criteria for diagnosing melanoma. Initially, we labeled data for a\nnon-annotated dataset with symmetrical information based on clinical\nassessments. Subsequently, we propose a supporting technique, a supervised\nlearning image processing algorithm, to analyze the geometrical pattern of\nlesion shape, aiding non-experts in understanding the criteria of an asymmetric\nlesion. We then utilize a pre-trained convolutional neural network (CNN) to\nextract shape, color, and texture features from dermoscopic images for training\na multiclass support vector machine (SVM) classifier, outperforming\nstate-of-the-art methods from the literature. In the geometry-based experiment,\nwe achieved a 99.00% detection rate for dermatological asymmetric lesions. In\nthe CNN-based experiment, the best performance is found with 94% Kappa Score,\n95% Macro F1-score, and 97% Weighted F1-score for classifying lesion shapes\n(Asymmetric, Half-Symmetric, and Symmetric).", "AI": {"tldr": "该研究利用监督学习和CNN技术对皮肤病变的形状特征进行分类分析，通过几何方法和CNN方法分别达到很高的检测率和分类准确率。", "motivation": "研究的动机是帮助非专家理解皮肤病变诊断中的重要标准——病变形状的不对称性，并且在分类性能上超越现有的方法。", "method": "研究的第一步是标注皮肤镜图像数据中的对称信息；然后提出了一个支持性技术，即监督学习图像处理算法，用于分析病变形状；最后利用预训练的CNN提取特征并用SVM分类器进行多类分类。", "result": "该研究基于皮肤镜图像，通过标注数据集中的对称性信息，并提出一种监督学习图像处理算法来分析病变形状的几何模式，辅助非专家理解不对称病变的诊断标准。同时，利用预训练的卷积神经网络(CNN)提取形状、颜色和纹理特征，训练多类支持向量机(SVM)分类器对皮肤病变形状进行分类。几何方法的不对称病变检测率为99.00%，CNN方法的最佳性能指标为94% Kappa值，95% Macro F1得分和97% Weighted F1得分。", "conclusion": "研究证实了提出的几何分析方法和基于CNN的多类分类器在皮肤病变形状分类中的高性能，为非专家理解和诊断提供了强有力的支持。"}}
{"id": "2507.17515", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.17515", "abs": "https://arxiv.org/abs/2507.17515", "authors": ["Songshuo Lu", "Hua Wang", "Zhi Chen", "Yaohua Tang"], "title": "URPO: A Unified Reward & Policy Optimization Framework for Large Language Models", "comment": null, "summary": "Large-scale alignment pipelines typically pair a policy model with a\nseparately trained reward model whose parameters remain frozen during\nreinforcement learning (RL). This separation creates a complex,\nresource-intensive pipeline and suffers from a performance ceiling due to a\nstatic reward signal. We propose a novel framework, Unified Reward & Policy\nOptimization (URPO), that unifies instruction-following (\"player\") and reward\nmodeling (\"referee\") within a single model and a single training phase. Our\nmethod recasts all alignment data-including preference pairs, verifiable\nreasoning, and open-ended instructions-into a unified generative format\noptimized by a single Group-Relative Policy Optimization (GRPO) loop. This\nenables the model to learn from ground-truth preferences and verifiable logic\nwhile simultaneously generating its own rewards for open-ended tasks.\nExperiments on the Qwen2.5-7B model demonstrate URPO's superiority. Our unified\nmodel significantly outperforms a strong baseline using a separate generative\nreward model, boosting the instruction-following score on AlpacaEval from 42.24\nto 44.84 and the composite reasoning average from 32.66 to 35.66. Furthermore,\nURPO cultivates a superior internal evaluator as a byproduct of training,\nachieving a RewardBench score of 85.15 and surpassing the dedicated reward\nmodel it replaces (83.55). By eliminating the need for a separate reward model\nand fostering a co-evolutionary dynamic between generation and evaluation, URPO\npresents a simpler, more efficient, and more effective path towards robustly\naligned language models.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2507.17192", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.17192", "abs": "https://arxiv.org/abs/2507.17192", "authors": ["Haiyu Wu", "Jaskirat Singh", "Sicong Tian", "Liang Zheng", "Kevin W. Bowyer"], "title": "Vec2Face+ for Face Dataset Generation", "comment": null, "summary": "When synthesizing identities as face recognition training data, it is\ngenerally believed that large inter-class separability and intra-class\nattribute variation are essential for synthesizing a quality dataset. % This\nbelief is generally correct, and this is what we aim for. However, when\nincreasing intra-class variation, existing methods overlook the necessity of\nmaintaining intra-class identity consistency. % To address this and generate\nhigh-quality face training data, we propose Vec2Face+, a generative model that\ncreates images directly from image features and allows for continuous and easy\ncontrol of face identities and attributes. Using Vec2Face+, we obtain datasets\nwith proper inter-class separability and intra-class variation and identity\nconsistency using three strategies: 1) we sample vectors sufficiently different\nfrom others to generate well-separated identities; 2) we propose an AttrOP\nalgorithm for increasing general attribute variations; 3) we propose LoRA-based\npose control for generating images with profile head poses, which is more\nefficient and identity-preserving than AttrOP. % Our system generates VFace10K,\na synthetic face dataset with 10K identities, which allows an FR model to\nachieve state-of-the-art accuracy on seven real-world test sets. Scaling the\nsize to 4M and 12M images, the corresponding VFace100K and VFace300K datasets\nyield higher accuracy than the real-world training dataset, CASIA-WebFace, on\nfive real-world test sets. This is the first time a synthetic dataset beats the\nCASIA-WebFace in average accuracy. In addition, we find that only 1 out of 11\nsynthetic datasets outperforms random guessing (\\emph{i.e., 50\\%}) in twin\nverification and that models trained with synthetic identities are more biased\nthan those trained with real identities. Both are important aspects for future\ninvestigation.", "AI": {"tldr": "提出Vec2Face+模型，通过三个策略生成具有适当类内一致性和类间可分性的高质量人脸合成数据集，并验证了合成数据集在人脸识别中的有效性。", "motivation": "解决现有方法在增加类内变异时忽视维护类内身份一致性的问题，生成高质量的人脸训练数据。", "method": "Vec2Face+模型直接从图像特征生成图像，使用三种策略来维持适当的身份一致性和类间可分性以及增加类内变异。", "result": "生成的VFace10K、VFace100K和VFace300K数据集在多个真实测试集上取得了更好的结果，证明了合成数据集的有效性。", "conclusion": "首次合成的数据集在平均准确率上超过了真实数据集CASIA-WebFace，但也指出合成身份训练的模型可能更加偏见。"}}
{"id": "2507.17588", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.17588", "abs": "https://arxiv.org/abs/2507.17588", "authors": ["Jie Wang", "Zhendong Yang", "Liansong Zong", "Xiaobo Zhang", "Dexian Wang", "Ji Zhang"], "title": "Dual-branch Prompting for Multimodal Machine Translation", "comment": null, "summary": "Multimodal Machine Translation (MMT) typically enhances text-only translation\nby incorporating aligned visual features. Despite the remarkable progress,\nstate-of-the-art MMT approaches often rely on paired image-text inputs at\ninference and are sensitive to irrelevant visual noise, which limits their\nrobustness and practical applicability. To address these issues, we propose\nD2P-MMT, a diffusion-based dual-branch prompting framework for robust\nvision-guided translation. Specifically, D2P-MMT requires only the source text\nand a reconstructed image generated by a pre-trained diffusion model, which\nnaturally filters out distracting visual details while preserving semantic\ncues. During training, the model jointly learns from both authentic and\nreconstructed images using a dual-branch prompting strategy, encouraging rich\ncross-modal interactions. To bridge the modality gap and mitigate\ntraining-inference discrepancies, we introduce a distributional alignment loss\nthat enforces consistency between the output distributions of the two branches.\nExtensive experiments on the Multi30K dataset demonstrate that D2P-MMT achieves\nsuperior translation performance compared to existing state-of-the-art\napproaches.", "AI": {"tldr": "提出D2P-MMT，一种基于扩散的双分支提示框架，用于提升视觉引导翻译的稳健性和性能", "motivation": "解决现有的多模态机器翻译方法对无关视觉噪音敏感和对齐图片文本输入依赖性问题，提升多模态机器翻译的鲁棒性和实际应用性", "method": "D2P-MMT, 一个基于扩散的双分支提示框架，旨在实现稳健的视觉引导翻译。该框架仅需源文本和由预训练扩散模型生成的重建图像即可工作，能够自然过滤掉分散注意力的视觉细节同时保留语义线索", "result": "在Multi30K数据集上的实验表明，D2P-MMT相比于现有的最先进技术实现了更好的翻译性能", "conclusion": "研究展示了D2P-MMT在保持翻译质量的同时减少了对无关视觉信息的依赖，证明其在多模态翻译中的有效性和应用潜力"}}
{"id": "2507.17202", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.17202", "abs": "https://arxiv.org/abs/2507.17202", "authors": ["Jooyeol Yun", "Heng Wang", "Yotaro Shimose", "Jaegul Choo", "Shingo Takamatsu"], "title": "DesignLab: Designing Slides Through Iterative Detection and Correction", "comment": "https://yeolj00.github.io/personal-projects/designlab", "summary": "Designing high-quality presentation slides can be challenging for non-experts\ndue to the complexity involved in navigating various design choices. Numerous\nautomated tools can suggest layouts and color schemes, yet often lack the\nability to refine their own output, which is a key aspect in real-world\nworkflows. We propose DesignLab, which separates the design process into two\nroles, the design reviewer, who identifies design-related issues, and the\ndesign contributor who corrects them. This decomposition enables an iterative\nloop where the reviewer continuously detects issues and the contributor\ncorrects them, allowing a draft to be further polished with each iteration,\nreaching qualities that were unattainable. We fine-tune large language models\nfor these roles and simulate intermediate drafts by introducing controlled\nperturbations, enabling the design reviewer learn design errors and the\ncontributor learn how to fix them. Our experiments show that DesignLab\noutperforms existing design-generation methods, including a commercial tool, by\nembracing the iterative nature of designing which can result in polished,\nprofessional slides.", "AI": {"tldr": "我们提出了一种名为 DesignLab 的方法，通过将设计过程分解为设计审查者和设计贡献者两个角色，以实现设计的迭代优化，这种方法优于现有的设计生成方法。", "motivation": "针对非设计专家在设计高质量演示文稿时面临的复杂设计选择，以及现有自动化工具无法有效优化其输出的问题，我们提出了一种新的解决方案。", "method": "我们将设计过程分解为设计审查者和设计贡献者两个角色，设计审查者识别设计相关问题，设计贡献者修正这些问题。我们对大型语言模型进行微调以实现这两个角色，并通过引入控制扰动来模拟中间草稿，使设计审查者学习设计错误，设计贡献者学习如何修复这些错误。", "result": "我们的实验表明，DesignLab 在生成设计方面优于现有方法，包括商业工具，这得益于其采纳设计的迭代性质，能够生成精致、专业的幻灯片。", "conclusion": "我们得出结论，通过将设计过程分解成两个角色并通过迭代解决设计问题，DesignLab 能生成更高质量的幻灯片，比现有工具更出色。"}}
{"id": "2507.17205", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.17205", "abs": "https://arxiv.org/abs/2507.17205", "authors": ["Linda Wei", "Chang Liu", "Wenran Zhang", "Zengji Zhang", "Shaoting Zhang", "Hongsheng Li"], "title": "VBCD: A Voxel-Based Framework for Personalized Dental Crown Design", "comment": null, "summary": "The design of restorative dental crowns from intraoral scans is\nlabor-intensive for dental technicians. To address this challenge, we propose a\nnovel voxel-based framework for automated dental crown design (VBCD). The VBCD\nframework generates an initial coarse dental crown from voxelized intraoral\nscans, followed by a fine-grained refiner incorporating distance-aware\nsupervision to improve accuracy and quality. During the training stage, we\nemploy the Curvature and Margin line Penalty Loss (CMPL) to enhance the\nalignment of the generated crown with the margin line. Additionally, a\npositional prompt based on the FDI tooth numbering system is introduced to\nfurther improve the accuracy of the generated dental crowns. Evaluation on a\nlarge-scale dataset of intraoral scans demonstrated that our approach\noutperforms existing methods, providing a robust solution for personalized\ndental crown design.", "AI": {"tldr": "本文提出了一种基于体素的全自动牙冠设计框架VBCD，该框架通过体素化的口腔扫描并利用距离感知监督和位置提示，提升了牙冠设计的准确性和效率，并且在大规模数据集上的表现优于现有方法。", "motivation": "现有的牙冠设计流程对于牙科技术人员来说既费时又费力。本研究旨在通过自动化设计来解决这一问题，以提高设计效率和准确性。", "method": "我们提出了一种基于体素的全自动牙冠设计框架VBCD，该框架通过体素化的口腔扫描生成粗略的牙冠，然后使用带有距离感知监督的细化器来提高精确度和质量。在训练阶段，使用曲率和边界线罚则损失（CMPL）来增强生成牙冠与边界线的对齐，并引入了基于FDI牙编号系统的定位提示以进一步提高生成牙冠的准确度。", "result": "在大规模口腔扫描数据集上的评估表明，我们的方法优于现有的方法，为个性化的牙冠设计提供了强大的解决方案。", "conclusion": "我们的方法通过综合使用VBCD框架、基于距离感知的细化技术和定位提示，显著提高了牙冠设计的准确性和效率，展示了一个全新的牙冠设计途径。"}}
