<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 39]
- [cs.CV](#cs.CV) [Total: 34]
- [eess.IV](#eess.IV) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Planted in Pretraining, Swayed by Finetuning: A Case Study on the Origins of Cognitive Biases in LLMs](https://arxiv.org/abs/2507.07186)
*Itay Itzhak,Yonatan Belinkov,Gabriel Stanovsky*

Main category: cs.CL

> 本文研究了大型语言模型中的认知偏见来源，通过实验揭示偏见主要由预训练决定，同时考虑训练随机性的影响。这些发现有助于未来减少偏见的策略开发。

<details>
  <summary>Details</summary>

**Motivation:** 此前的研究发现，这些偏见在不同的模型间有所不同，并且可以通过指令调优来放大。然而，尚不清楚这些偏见差别是源自预训练、微调，还是仅由于训练中的随机性。

**Method:** 本研究提出了一个两步的因果实验方法来解开这些因素。第一步，使用不同的随机种子多次微调模型，以研究训练随机性对30多种认知偏见的影响。第二步，引入“交叉调优”——交换模型之间的指令数据集，以隔离偏见的来源。这一交换使用的是导致不同偏见模式的数据集，直接测试偏见是否依赖于数据集。

**Result:** 研究结果表明，虽然训练的随机性引入了一些可变性，但偏见主要由预训练决定：具有相同预训练基础的模型表现出的偏见模式比仅有相同微调数据的模型更为相似。

**Conclusion:** 这一发现表明，理解微调模型中的偏见需要考虑其预训练起源，而不仅仅是微调效果。此视角可以引导未来的努力，发展评估和减少LLMs中偏见的原则策略。

**Abstract:** Large language models (LLMs) exhibit cognitive biases -- systematic
tendencies of irrational decision-making, similar to those seen in humans.
Prior work has found that these biases vary across models and can be amplified
by instruction tuning. However, it remains unclear if these differences in
biases stem from pretraining, finetuning, or even random noise due to training
stochasticity. We propose a two-step causal experimental approach to
disentangle these factors. First, we finetune models multiple times using
different random seeds to study how training randomness affects over $30$
cognitive biases. Second, we introduce \emph{cross-tuning} -- swapping
instruction datasets between models to isolate bias sources. This swap uses
datasets that led to different bias patterns, directly testing whether biases
are dataset-dependent. Our findings reveal that while training randomness
introduces some variability, biases are mainly shaped by pretraining: models
with the same pretrained backbone exhibit more similar bias patterns than those
sharing only finetuning data. These insights suggest that understanding biases
in finetuned models requires considering their pretraining origins beyond
finetuning effects. This perspective can guide future efforts to develop
principled strategies for evaluating and mitigating bias in LLMs.

</details>


### [2] [Prompt Perturbations Reveal Human-Like Biases in LLM Survey Responses](https://arxiv.org/abs/2507.07188)
*Jens Rupprecht,Georg Ahnert,Markus Strohmaier*

Main category: cs.CL

> 该研究显示，大型语言模型（LLMs）在用于社交科学调查时存在反应偏差，包括近期偏见，并且对于微调问题表述有敏感反应，提示了在使用LLMs生成合成数据时需要注意模型的反应稳健性。

<details>
  <summary>Details</summary>

**Motivation:** 该研究旨在探讨大型语言模型在规范调查情境中的反应稳健性，特别是它们对已知响应偏差的倾向，因为以前这方面的理解十分有限。

**Method:** 本研究测试了九种不同的大型语言模型（LLMs）在世界价值观调查（WVS）问题上的反应，通过改变问题表述和答案选项结构，应用了11种不同的干扰，并进行了超过167,000次模拟访谈。

**Result:** 研究揭示了模型对干扰的脆弱性以及所有测试模型中存在的一致性近期偏见，所有模型对语义变化和复合干扰仍然敏感。

**Conclusion:** 通过引入一系列干扰性测试，研究发现LLMs的部分人类调查响应偏差特征，强调了在使用LLMs生成合成调查数据时，妥善设计提示词和进行稳健性测试的重要性。

**Abstract:** Large Language Models (LLMs) are increasingly used as proxies for human
subjects in social science surveys, but their reliability and susceptibility to
known response biases are poorly understood. This paper investigates the
response robustness of LLMs in normative survey contexts -- we test nine
diverse LLMs on questions from the World Values Survey (WVS), applying a
comprehensive set of 11 perturbations to both question phrasing and answer
option structure, resulting in over 167,000 simulated interviews. In doing so,
we not only reveal LLMs' vulnerabilities to perturbations but also reveal that
all tested models exhibit a consistent \textit{recency bias} varying in
intensity, disproportionately favoring the last-presented answer option. While
larger models are generally more robust, all models remain sensitive to
semantic variations like paraphrasing and to combined perturbations. By
applying a set of perturbations, we reveal that LLMs partially align with
survey response biases identified in humans. This underscores the critical
importance of prompt design and robustness testing when using LLMs to generate
synthetic survey data.

</details>


### [3] [SynthTextEval: Synthetic Text Data Generation and Evaluation for High-Stakes Domains](https://arxiv.org/abs/2507.07229)
*Krithika Ramesh,Daniel Smolyak,Zihao Zhao,Nupoor Gandhi,Ritu Agarwal,Margrét Bjarnadóttir,Anjalie Field*

Main category: cs.CL

> 本文提出了SynthTextEval工具包，用于对合成文本进行多维度的评估，以提高合成文本的可行性和隐私保护。

<details>
  <summary>Details</summary>

**Motivation:** 大型语言模型输出的流畅性使得合成文本在诸如减少AI系统开发和部署过程中隐私侵犯风险等众多应用中变得可行，但实现这一潜力需要对合成数据进行全面评估。

**Method:** 我们提出了SynthTextEval，这是一个用于全面评估合成文本的工具包。该工具包可以评估合成数据在下游系统中的效用、系统的公平性、隐私泄露的风险、与原始文本的分布差异以及领域专家的定性反馈。

**Result:** SynthTextEval展示了其在医疗保健和法律这两个高风险领域的功能和有效性，通过整合和标准化评估指标来提高合成文本的可行性，进而改善AI开发中的隐私保护。

**Conclusion:** 通过使用SynthTextEval，我们能够改善合成文本的可行性和AI开发中的隐私保护。

**Abstract:** We present SynthTextEval, a toolkit for conducting comprehensive evaluations
of synthetic text. The fluency of large language model (LLM) outputs has made
synthetic text potentially viable for numerous applications, such as reducing
the risks of privacy violations in the development and deployment of AI systems
in high-stakes domains. Realizing this potential, however, requires principled
consistent evaluations of synthetic data across multiple dimensions: its
utility in downstream systems, the fairness of these systems, the risk of
privacy leakage, general distributional differences from the source text, and
qualitative feedback from domain experts. SynthTextEval allows users to conduct
evaluations along all of these dimensions over synthetic data that they upload
or generate using the toolkit's generation module. While our toolkit can be run
over any data, we highlight its functionality and effectiveness over datasets
from two high-stakes domains: healthcare and law. By consolidating and
standardizing evaluation metrics, we aim to improve the viability of synthetic
text, and in-turn, privacy-preservation in AI development.

</details>


### [4] [Medical Red Teaming Protocol of Language Models: On the Importance of User Perspectives in Healthcare Settings](https://arxiv.org/abs/2507.07248)
*Minseon Kim,Jean-Philippe Corbeil,Alessandro Sordoni,Francois Beaulieu,Paul Vozila*

Main category: cs.CL

> 本文介绍了一个专门针对医疗领域的大型语言模型安全评估协议，并从患者和临床医生的角度进行了详细的安全性量分析。通过构建PatientSafetyBench数据集并应用针对性的红队测试方法，研究了MediPhi模型的安全性。

<details>
  <summary>Details</summary>

**Motivation:** 大型语言模型在医疗领域的广泛应用提高了对其安全性的要求。现有的安全性评估大多关注于通用的安全基准，而忽视了医疗领域特有的风险和考量。

**Method:** 构建了一个包含466个样本的PatientSafetyBench数据集，覆盖5个关键的安全评估类别，从患者和临床医生的不同视角进行安全性评估。

**Result:** 通过红队测试方法对MediPhi模型进行评估，提出了医疗语言模型安全性评估的新标准，并从患者、临床医生和一般用户三个角度进行考量。

**Conclusion:** 这项工作首次定义了医疗领域大型语言模型的安全性评估标准，为医疗领域的安全部署奠定了基础。

**Abstract:** As the performance of large language models (LLMs) continues to advance,
their adoption is expanding across a wide range of domains, including the
medical field. The integration of LLMs into medical applications raises
critical safety concerns, particularly due to their use by users with diverse
roles, e.g. patients and clinicians, and the potential for model's outputs to
directly affect human health. Despite the domain-specific capabilities of
medical LLMs, prior safety evaluations have largely focused only on general
safety benchmarks. In this paper, we introduce a safety evaluation protocol
tailored to the medical domain in both patient user and clinician user
perspectives, alongside general safety assessments and quantitatively analyze
the safety of medical LLMs. We bridge a gap in the literature by building the
PatientSafetyBench containing 466 samples over 5 critical categories to measure
safety from the perspective of the patient. We apply our red-teaming protocols
on the MediPhi model collection as a case study. To our knowledge, this is the
first work to define safety evaluation criteria for medical LLMs through
targeted red-teaming taking three different points of view - patient,
clinician, and general user - establishing a foundation for safer deployment in
medical domains.

</details>


### [5] [The Impact of Background Speech on Interruption Detection in Collaborative Groups](https://arxiv.org/abs/2507.07280)
*Mariah Bradford,Nikhil Krishnaswamy,Nathaniel Blanchard*

Main category: cs.CL

> 研究了单对话和多组对话环境中的中断检测，开发了能够应对重叠语音情况的中断识别方法，揭示了协作小组互动中的语言学和韵律信息。

<details>
  <summary>Details</summary>

**Motivation:** 大多数中断检测研究是在单对话环境和相对纯净的音频背景下进行的，而实际的小组协作学习环境中存在重叠语音，需要有效的方法来识别中断。

**Method:** 通过对比单对话环境和多小组对话环境中的中断检测，开发了一种针对重叠语音情况的先进中断识别方法。

**Result:** 创建了能够应对重叠语音情况的先进中断识别方法，为进一步研究奠定了基础。

**Conclusion:** 该方法在包含多个小组的对话环境中可以有效识别中断，并揭示了中断在协作小组互动中的语言学和韵律信息。

**Abstract:** Interruption plays a crucial role in collaborative learning, shaping group
interactions and influencing knowledge construction. AI-driven support can
assist teachers in monitoring these interactions. However, most previous work
on interruption detection and interpretation has been conducted in
single-conversation environments with relatively clean audio. AI agents
deployed in classrooms for collaborative learning within small groups will need
to contend with multiple concurrent conversations -- in this context,
overlapping speech will be ubiquitous, and interruptions will need to be
identified in other ways. In this work, we analyze interruption detection in
single-conversation and multi-group dialogue settings. We then create a
state-of-the-art method for interruption identification that is robust to
overlapping speech, and thus could be deployed in classrooms. Further, our work
highlights meaningful linguistic and prosodic information about how
interruptions manifest in collaborative group interactions. Our investigation
also paves the way for future works to account for the influence of overlapping
speech from multiple groups when tracking group dialog.

</details>


### [6] [Multi-Agent Retrieval-Augmented Framework for Evidence-Based Counterspeech Against Health Misinformation](https://arxiv.org/abs/2507.07307)
*Anirban Saha Anik,Xiaoying Song,Elliott Wang,Bryan Wang,Bengisu Yarimbas,Lingzi Hong*

Main category: cs.CL

> 本文提出了一种多代理检索增强框架，用于应对健康领域的虚假信息，该框架比现有方法在多个关键指标上表现更优，并通过消融研究和人类评估验证了其有效性和必要性。

<details>
  <summary>Details</summary>

**Motivation:** 现有的研究依赖于有限的证据，并且对最终输出的控制较少。为了应对这些挑战，我们提出了这一新框架。

**Method:** 我们提出了一种多代理检索增强框架，用于对抗健康领域的虚假信息，该框架集成了多个大型语言模型以优化知识检索、证据增强和响应细化。这种方法结合了静态和动态证据，以确保生成的对抗言论是相关的、有根据的和最新的。

**Result:** 我们的方法在礼貌性、相关性、信息量和事实准确性方面超过了基线方法，证明了其在生成高质量对抗言论方面的有效性。我们还进行了消融研究以验证框架中每个组件的必要性，并通过人类评估表明细化过程显著提升了对抗言论的质量。

**Conclusion:** 我们的研究表明，多代理检索增强框架在生成高质量的对抗言论方面是有效的，特别是在礼貌性、相关性、信息量和事实准确性方面。该框架能够有效地整合和利用动态及静态证据，验证了其在处理健康领域的虚假信息中的潜力。human评估进一步证实，该框架对于生成高质量的对抗言论至关重要。

**Abstract:** Large language models (LLMs) incorporated with Retrieval-Augmented Generation
(RAG) have demonstrated powerful capabilities in generating counterspeech
against misinformation. However, current studies rely on limited evidence and
offer less control over final outputs. To address these challenges, we propose
a Multi-agent Retrieval-Augmented Framework to generate counterspeech against
health misinformation, incorporating multiple LLMs to optimize knowledge
retrieval, evidence enhancement, and response refinement. Our approach
integrates both static and dynamic evidence, ensuring that the generated
counterspeech is relevant, well-grounded, and up-to-date. Our method
outperforms baseline approaches in politeness, relevance, informativeness, and
factual accuracy, demonstrating its effectiveness in generating high-quality
counterspeech. To further validate our approach, we conduct ablation studies to
verify the necessity of each component in our framework. Furthermore, human
evaluations reveal that refinement significantly enhances counterspeech quality
and obtains human preference.

</details>


### [7] [GNN-CNN: An Efficient Hybrid Model of Convolutional and Graph Neural Networks for Text Representation](https://arxiv.org/abs/2507.07414)
*Fardin Rastakhiz*

Main category: cs.CL

> This paper presents a new model that integrates CNNs and GNNs with a real-time graph generation mechanism to efficiently process long texts. The model shows competitive performance in text classification tasks and is more efficient than currently state-of-the-art transformers.

<details>
  <summary>Details</summary>

**Motivation:** The motivation for this study arises from the need for more efficient and time-effective models capable of handling long texts. Current state-of-the-art models, like transformers, exhibit computational complexity that is unsuitable for processing extended documents.

**Method:** The paper introduces a new model architecture that combines Graph Neural Networks (GNNs) and Convolutional Neural Networks (CNNs) for processing long texts efficiently. This model uses real-time, end-to-end graph generation and utilizes character-level inputs without padding or truncation, thus achieving better performance and efficiency.

**Result:** The model captures local contextual patterns with CNNs and expands local receptive fields using lattice-based graph structures. It also employs small-world graphs to aggregate document-level information. Experimental results show that the proposed model is efficient and performs well in text classification tasks.

**Conclusion:** The new model architecture, combining GNNs and CNNs with real-time graph generation, offers a competitive alternative to the existing quadratic computational complexity models used for long text processing. It achieves high performance and efficiency.

**Abstract:** Time, cost, and energy efficiency are critical considerations in
Deep-Learning (DL), particularly when processing long texts. Transformers,
which represent the current state of the art, exhibit quadratic computational
complexity relative to input length, making them inefficient for extended
documents. This study introduces a novel model architecture that combines Graph
Neural Networks (GNNs) and Convolutional Neural Networks (CNNs), integrated
with a real-time, end-to-end graph generation mechanism. The model processes
compact batches of character-level inputs without requiring padding or
truncation. To enhance performance while maintaining high speed and efficiency,
the model incorporates information from Large Language Models (LLMs), such as
token embeddings and sentiment polarities, through efficient dictionary
lookups. It captures local contextual patterns using CNNs, expands local
receptive fields via lattice-based graph structures, and employs small-world
graphs to aggregate document-level information. The generated graphs exhibit
structural properties indicative of meaningful semantic organization, with an
average clustering coefficient of approximately 0.45 and an average shortest
path length ranging between 4 and 5. The model is evaluated across multiple
text classification tasks, including sentiment analysis and
news-categorization, and is compared against state-of-the-art models.
Experimental results confirm the proposed model's efficiency and competitive
performance.

</details>


### [8] [MedReadCtrl: Personalizing medical text generation with readability-controlled instruction learning](https://arxiv.org/abs/2507.07419)
*Hieu Tran,Zonghai Yao,Won Seok Jang,Sharmin Sultana,Allen Chang,Yuan Zhang,Hong Yu*

Main category: cs.CL

> 研究介绍了一种名为MedReadCtrl的可调整语言复杂度的框架，以提升人机沟通的有效性，尤其在医学领域。评估显示其在多个指标上优于GPT-4，专家也对其高度偏好。

<details>
  <summary>Details</summary>

**Motivation:** 在医疗保健领域部署生成式AI的关键挑战之一是有效的人机沟通，这要求内容必须既个性化又易于理解。

**Method:** 引入了MedReadCtrl，这是一种可控制可读性的指令微调框架，使大语言模型在不损失意义的前提下调整输出的复杂度。

**Result:** 评估了医疗和一般领域的九个数据集和三个任务，结果表明MedReadCtrl的可读性指令遵循错误显著低于GPT-4，并在未见过的临床任务上取得了显著提升。专家们也更偏好MedReadCtrl。

**Conclusion:** 这些成果反映出MedReadCtrl可以将临床内容重新结构化为易于理解的语言，同时保留医疗意图，为患者教育和扩大AI技术支持的医疗的公平访问提供了一个可扩展的解决方案。

**Abstract:** Generative AI has demonstrated strong potential in healthcare, from clinical
decision support to patient-facing chatbots that improve outcomes. A critical
challenge for deployment is effective human-AI communication, where content
must be both personalized and understandable. We introduce MedReadCtrl, a
readability-controlled instruction tuning framework that enables LLMs to adjust
output complexity without compromising meaning. Evaluations of nine datasets
and three tasks across medical and general domains show that MedReadCtrl
achieves significantly lower readability instruction-following errors than
GPT-4 (e.g., 1.39 vs. 1.59 on ReadMe, p<0.001) and delivers substantial gains
on unseen clinical tasks (e.g., +14.7 ROUGE-L, +6.18 SARI on MTSamples).
Experts consistently preferred MedReadCtrl (71.7% vs. 23.3%), especially at low
literacy levels. These gains reflect MedReadCtrl's ability to restructure
clinical content into accessible, readability-aligned language while preserving
medical intent, offering a scalable solution to support patient education and
expand equitable access to AI-enabled care.

</details>


### [9] [SynthEHR-Eviction: Enhancing Eviction SDoH Detection with LLM-Augmented Synthetic EHR Data](https://arxiv.org/abs/2507.07421)
*Zonghai Yao,Youxia Zhao,Avijit Mitra,David A. Levy,Emily Druhl,Jack Tsai,Hong Yu*

Main category: cs.CL

> 该研究提出SynthEHR-Eviction用于提取电子健康记录（EHRs）中的驱逐记录，实现高效的数据抽取和低成本部署，同时创建了大规模公共数据集，提高了驱逐信息及其他SDoH信息提取的准确性。

<details>
  <summary>Details</summary>

**Motivation:** 驱逐作为一种重要的健康社会决定因素（SDoH）至今研究不足。它与住房不稳定性、失业和精神健康问题相关联。驱逐记录虽然存在于未结构化的电子健康记录中，但在结构化字段中极少编码，限制了其后续应用。为了应对这些问题，该研究提出了SynthEHR-Eviction。

**Method:** 该研究使用了一种名为SynthEHR-Eviction的可扩展管道，结合了大语言模型（LLMs）、人类在回路中注释和自动化提示优化（APO），用于从临床记录中提取驱逐状态。该系统包括LLMs处理未结构化的电子健康记录中的驱逐信息，以及通过人机协作优化模型和标记的工作流程。

**Result:** 研究团队利用该管道创建了迄今为止最大的公共驱逐相关信息提取SDoH数据集，其中包括14个细粒度类别。经过微调的大型语言模型（例如Qwen2.5、LLaMA3）在人工验证的数据上实现了88.8%（驱逐）和90.3%（其他SDoH）的宏F1分数，优于GPT-4o-APO（87.8%，87.3%）、GPT-4o-mini-APO（69.1%，78.1%）和BioBERT（60.7%，68.3%）。

**Conclusion:** 该研究提出的SynthEHR-Eviction管道显著减少了标注工作量，加速了数据集的创建，并为驱逐信息的可扩展检测提供了可能，同时还可以泛化应用于其他信息提取任务。这种方法不仅提升了行业内的数据挖掘效率，也扩大了对SDoH关注的研究领域。

**Abstract:** Eviction is a significant yet understudied social determinants of health
(SDoH), linked to housing instability, unemployment, and mental health. While
eviction appears in unstructured electronic health records (EHRs), it is rarely
coded in structured fields, limiting downstream applications. We introduce
SynthEHR-Eviction, a scalable pipeline combining LLMs, human-in-the-loop
annotation, and automated prompt optimization (APO) to extract eviction
statuses from clinical notes. Using this pipeline, we created the largest
public eviction-related SDoH dataset to date, comprising 14 fine-grained
categories. Fine-tuned LLMs (e.g., Qwen2.5, LLaMA3) trained on
SynthEHR-Eviction achieved Macro-F1 scores of 88.8% (eviction) and 90.3% (other
SDoH) on human validated data, outperforming GPT-4o-APO (87.8%, 87.3%),
GPT-4o-mini-APO (69.1%, 78.1%), and BioBERT (60.7%, 68.3%), while enabling
cost-effective deployment across various model sizes. The pipeline reduces
annotation effort by over 80%, accelerates dataset creation, enables scalable
eviction detection, and generalizes to other information extraction tasks.

</details>


### [10] [Towards Interpretable Time Series Foundation Models](https://arxiv.org/abs/2507.07439)
*Matthieu Boileau,Philippe Helluy,Jeremy Pawlus,Svitlana Vyetrenko*

Main category: cs.CL

> 研究显示，通过微调小型语言模型，可以实现对时间序列数据的解释性理解，适合设备端或注重隐私的部署。

<details>
  <summary>Details</summary>

**Motivation:** 探索将时间序列推理能力蒸馏到小型指令调整语言模型中，以实现可解释的时间序列基础模型。

**Method:** 利用合成的均值回复时间序列数据集，通过一个大型多模态模型生成自然语言注释，并用这些注释监督紧凑型Qwen模型的微调。

**Result:** 研究表明，微调后的模型获得了有意义的解释能力，证明了将时间序列理解压缩到轻量级语言模型中的可行性。

**Conclusion:** 本研究为发展能够以自然语言解释时间模式的小型可解释模型提供了实际基础。

**Abstract:** In this paper, we investigate the distillation of time series reasoning
capabilities into small, instruction-tuned language models as a step toward
building interpretable time series foundation models. Leveraging a synthetic
dataset of mean-reverting time series with systematically varied trends and
noise levels, we generate natural language annotations using a large multimodal
model and use these to supervise the fine-tuning of compact Qwen models. We
introduce evaluation metrics that assess the quality of the distilled reasoning
- focusing on trend direction, noise intensity, and extremum localization - and
show that the post-trained models acquire meaningful interpretive capabilities.
Our results highlight the feasibility of compressing time series understanding
into lightweight, language-capable models suitable for on-device or
privacy-sensitive deployment. This work contributes a concrete foundation
toward developing small, interpretable models that explain temporal patterns in
natural language.

</details>


### [11] [SAND: Boosting LLM Agents with Self-Taught Action Deliberation](https://arxiv.org/abs/2507.07441)
*Yu Xia,Yiran Jenny Shen,Junda Wu,Tong Yu,Sungchul Kim,Ryan A. Rossi,Lina Yao,Julian McAuley*

Main category: cs.CL

> 该研究提出了一种新的SAND框架，通过自我教学行动审议，解决了大语言模型代理在行动选择中的次优决策问题，实现了显著性能提升。

<details>
  <summary>Details</summary>

**Motivation:** 大多数当前的方法关注于模仿特定的专家行为或促进被选择的推理思想和行动，而忽略了对备选行动的较为全面的探索。这样的限制导致LLM代理可能过于倾向于那些看似可能但实际上次优的行动。因此，提出一个新的框架来改善当前的策略显得十分必要。

**Method:** 大型语言模型（LLM）代理通常通过监督微调或成对滚动的偏好优化进行调整。为解决仅模仿特定专家行为或选择性促进推理思维而忽略子优化动作的问题，本文提出了一种自我教学行动审议（SAND）框架，使LLM代理能够显式地审议候选行动。为了解决在大规模行动空间和行动评估中的审议挑战，我们融入了自我一致性行动采样和执行引导的行动评估，以帮助合成逐步的行动审议思维。通过迭代方式使用审议轨迹对LLM代理进行微调，SAND方法在两个代表性交互代理任务中实现了平均20%的改进，超越了最先进的代理调整方法。

**Result:** 实验结果表明，与初始监督微调相比，SAND框架在两个代表性交互代理任务上获得了平均20%的性能提升，并优于最先进的代理调优方法。

**Conclusion:** SAND框架通过使LLM代理能显式审议候选行动，解决了选择次优行动的问题，并在测试任务中表现出优于现有方法的表现。

**Abstract:** Large Language Model (LLM) agents are commonly tuned with supervised
finetuning on ReAct-style expert trajectories or preference optimization over
pairwise rollouts. Most of these methods focus on imitating specific expert
behaviors or promoting chosen reasoning thoughts and actions over rejected
ones. However, without reasoning and comparing over alternatives actions, LLM
agents finetuned with these methods may over-commit towards seemingly plausible
but suboptimal actions due to limited action space exploration. To address
this, in this paper we propose Self-taught ActioN Deliberation (SAND)
framework, enabling LLM agents to explicitly deliberate over candidate actions
before committing to one. To tackle the challenges of when and what to
deliberate given large action space and step-level action evaluation, we
incorporate self-consistency action sampling and execution-guided action
critique to help synthesize step-wise action deliberation thoughts using the
base model of the LLM agent. In an iterative manner, the deliberation
trajectories are then used to finetune the LLM agent itself. Evaluating on two
representative interactive agent tasks, SAND achieves an average 20%
improvement over initial supervised finetuning and also outperforms
state-of-the-art agent tuning approaches.

</details>


### [12] [RLEP: Reinforcement Learning with Experience Replay for LLM Reasoning](https://arxiv.org/abs/2507.07451)
*Hongzhi Zhang,Jia Fu,Jingyuan Zhang,Kai Fu,Qi Wang,Fuzheng Zhang,Guorui Zhou*

Main category: cs.CL

> RLEP框架通过收集验证轨迹并回放这些轨迹，优化大型语言模型的强化学习，实现更快的收敛速度和更好的最终性能。

<details>
  <summary>Details</summary>

**Motivation:** 传统的强化学习方法对于大型语言模型来说能耗高且不稳定，本研究希望通过提出的新框架优化这一问题。

**Method:** 研究设计了RLEP框架，包括收集验证轨迹和回放这些轨迹的两个阶段，优化新生成的滚动数据和回放成功轨迹的混合方法。

**Result:** 该研究提出了RLEP框架，通过两个阶段收集验证轨迹并随后在其上进行回放，从而提高了强化学习中大型语言模型的速度和最终性能。RLEP通过回放高质量的轨迹，避免了无效探索，加快了模型的收敛速度，且最终表现超过了基线。实验表明，RLEP在不同数据集上均展示了更强的性能。

**Conclusion:** RLEP框架在保持并改善大型语言模型的实际性能的同时，提升了强化学习的效率和效果，对于研究和应用都有着重要意义。

**Abstract:** Reinforcement learning (RL) for large language models is an energy-intensive
endeavor: training can be unstable, and the policy may gradually drift away
from its pretrained weights. We present \emph{RLEP}\, -- \,Reinforcement
Learning with Experience rePlay\, -- \,a two-phase framework that first
collects verified trajectories and then replays them during subsequent
training. At every update step, the policy is optimized on mini-batches that
blend newly generated rollouts with these replayed successes. By replaying
high-quality examples, RLEP steers the model away from fruitless exploration,
focuses learning on promising reasoning paths, and delivers both faster
convergence and stronger final performance. On the Qwen2.5-Math-7B base model,
RLEP reaches baseline peak accuracy with substantially fewer updates and
ultimately surpasses it, improving accuracy on AIME-2024 from 38.2% to 39.9%,
on AIME-2025 from 19.8% to 22.3%, and on AMC-2023 from 77.0% to 82.2%. Our
code, datasets, and checkpoints are publicly available at
https://github.com/Kwai-Klear/RLEP to facilitate reproducibility and further
research.

</details>


### [13] [Machine Bullshit: Characterizing the Emergent Disregard for Truth in Large Language Models](https://arxiv.org/abs/2507.07484)
*Kaiqu Liang,Haimin Hu,Xuandong Zhao,Dawn Song,Thomas L. Griffiths,Jaime Fernández Fisac*

Main category: cs.CL

> 该论文提出machine bullshit的概念，引入Bullshit Index量化机器模型的bullshit，并发现模型的微调和推理时的chain-of-thought提示会加剧此类问题，指出了AI对齐过程中系统的挑战。

<details>
  <summary>Details</summary>

**Motivation:** 该论文提出了将machine bullshit作为研究LLM中更广泛的脱离真实性现象的概念框架，并探索其潜在机制，目的是为研究人员提供一种分析复杂问题的新方法。

**Method:** 该论文引入了“Bullshit Index”来量化大语言模型（LLM）对真理的漠视，并提出了一个分析四种质性形式的bullshit（空洞的修辞、佯装、含糊其辞和未经验证的声明）的补充分类法。同时，进行了Marketplace数据集、政治中立数据集和新的BullshitEval基准（2400种情境，涵盖100个AI助手）的实证评估，后者专门设计用于评估机器bullshit。

**Result:** 实证评估结果表明，利用来自人类反馈的强化学习进行模型微调显著加剧了bullshit，推理阶段的chain-of-thought提示显著放大了特定形式的bullshit，尤其是在政治情景下含糊其辞的表现最为突出。

**Conclusion:** 研究结果揭示了系统性的AI对齐挑战，并为进一步实现更真实的LLM行为提供了新的见解。

**Abstract:** Bullshit, as conceptualized by philosopher Harry Frankfurt, refers to
statements made without regard to their truth value. While previous work has
explored large language model (LLM) hallucination and sycophancy, we propose
machine bullshit as an overarching conceptual framework that can allow
researchers to characterize the broader phenomenon of emergent loss of
truthfulness in LLMs and shed light on its underlying mechanisms. We introduce
the Bullshit Index, a novel metric quantifying LLMs' indifference to truth, and
propose a complementary taxonomy analyzing four qualitative forms of bullshit:
empty rhetoric, paltering, weasel words, and unverified claims. We conduct
empirical evaluations on the Marketplace dataset, the Political Neutrality
dataset, and our new BullshitEval benchmark (2,400 scenarios spanning 100 AI
assistants) explicitly designed to evaluate machine bullshit. Our results
demonstrate that model fine-tuning with reinforcement learning from human
feedback (RLHF) significantly exacerbates bullshit and inference-time
chain-of-thought (CoT) prompting notably amplify specific bullshit forms,
particularly empty rhetoric and paltering. We also observe prevalent machine
bullshit in political contexts, with weasel words as the dominant strategy. Our
findings highlight systematic challenges in AI alignment and provide new
insights toward more truthful LLM behavior.

</details>


### [14] [PLAN-TUNING: Post-Training Language Models to Learn Step-by-Step Planning for Complex Problem Solving](https://arxiv.org/abs/2507.07495)
*Mihir Parmar,Palash Goyal,Xin Liu,Yiwen Song,Mingyang Ling,Chitta Baral,Hamid Palangi,Tomas Pfister*

Main category: cs.CL

> PLAN-TUNING 是一种后训练框架，它将大型语言模型（LLM）的合成任务分解（即“规划轨迹”）提炼出来，然后通过监督学习和强化学习目标微调较小的模型，以模仿这些规划过程并提高复杂推理能力。实验结果表明，这种方法在多个基准测试中表现出色，展示了比强基线高出约7%的性能，并具有更好的泛化能力。

<details>
  <summary>Details</summary>

**Motivation:** 虽然将复杂问题分解为简单子任务对于提高大型语言模型的性能有很大帮助，但却很少有研究利用这种规划结构来提升小型的开源语言模型的性能。因此，提出 PLAN-TUNING 来填补这一领域的空白。

**Method:** PLAN-TUNING 方法包括两大部分：一是从大规模语言模型中提炼出合成任务分解（规划轨迹），二是通过设计促进模仿这些规划过程的监督学习和强化学习目标来微调小型模型。

**Result:** 在 GSM8k 和 MATH 基准测试中，经 PLAN-TUNING 微调的模型比强基线高出约7%的性能；在 OlympiadBench 和 AIME 2024 测试集上，性能提升分别为约10%和12%，验证了 PLAN-TUNING 对于改进小型语言模型的特定任务性能是有效的。

**Conclusion:** 通过详细的分析显示，规划轨迹对提升复杂推理能力有益，并且 PLAN-TUNING 是一种有效策略，能够更好地解决复杂的问题，特别是在提高小型语言模型的特定任务性能方面取得了显著效果。

**Abstract:** Recently, decomposing complex problems into simple subtasks--a crucial part
of human-like natural planning--to solve the given problem has significantly
boosted the performance of large language models (LLMs). However, leveraging
such planning structures during post-training to boost the performance of
smaller open-source LLMs remains underexplored. Motivated by this, we introduce
PLAN-TUNING, a unified post-training framework that (i) distills synthetic task
decompositions (termed "planning trajectories") from large-scale LLMs and (ii)
fine-tunes smaller models via supervised and reinforcement-learning objectives
designed to mimic these planning processes to improve complex reasoning. On
GSM8k and the MATH benchmarks, plan-tuned models outperform strong baselines by
an average $\sim7\%$. Furthermore, plan-tuned models show better generalization
capabilities on out-of-domain datasets, with average $\sim10\%$ and $\sim12\%$
performance improvements on OlympiadBench and AIME 2024, respectively. Our
detailed analysis demonstrates how planning trajectories improves complex
reasoning capabilities, showing that PLAN-TUNING is an effective strategy for
improving task-specific performance of smaller LLMs.

</details>


### [15] [Teaching LLM to Reason: Reinforcement Learning from Algorithmic Problems without Code](https://arxiv.org/abs/2507.07498)
*Keqin Bao,Nuo Chen,Xiaoyuan Li,Binyuan Hui,Bowen Yu,Fuli Feng,Junyang Lin,Xiangnan He,Dayiheng Liu*

Main category: cs.CL

> 论文提出了 TeaR 方法，通过数据整理和强化学习提高 LLM 的推理能力，实验结果显示显著的性能提升，特别是在某些模型上的大幅性能改进。

<details>
  <summary>Details</summary>

**Motivation:** 论文动机在于改进大型语言模型（LLM）的推理能力。通常，模型在模拟代码执行时容易过拟合到算法模式，而不是核心推理结构。

**Method:** TeaR 提出了一种通过仔细的数据整理和强化学习来指导模型发现与代码相关的任务中的最优推理路径的方法，从而提高模型的总体推理能力。

**Result:** 实验结果表明，在跨越数学、知识、代码和逻辑推理17个基准测试中，使用不同大小基础模型和长CoT蒸馏模型的测试中，TeaR 方法表现出了显著的性能提升，如在 Qwen2.5-7B 上提升了 35.9%，在 R1-Distilled-7B 上提升了 5.9%。

**Conclusion:** TeaR 方法通过优化数据整理和应用强化学习指导模型发现最优推理路径，显著提高了大型语言模型在多种推理任务上的表现。

**Abstract:** Enhancing reasoning capabilities remains a central focus in the LLM reasearch
community. A promising direction involves requiring models to simulate code
execution step-by-step to derive outputs for given inputs. However, as code is
often designed for large-scale systems, direct application leads to
over-reliance on complex data structures and algorithms, even for simple cases,
resulting in overfitting to algorithmic patterns rather than core reasoning
structures. To address this, we propose TeaR, which aims at teaching LLMs to
reason better. TeaR leverages careful data curation and reinforcement learning
to guide models in discovering optimal reasoning paths through code-related
tasks, thereby improving general reasoning abilities. We conduct extensive
experiments using two base models and three long-CoT distillation models, with
model sizes ranging from 1.5 billion to 32 billion parameters, and across 17
benchmarks spanning Math, Knowledge, Code, and Logical Reasoning. The results
consistently show significant performance improvements. Notably, TeaR achieves
a 35.9% improvement on Qwen2.5-7B and 5.9% on R1-Distilled-7B.

</details>


### [16] [Extracting ORR Catalyst Information for Fuel Cell from Scientific Literature](https://arxiv.org/abs/2507.07499)
*Hein Htet,Amgad Ahmed Ali Ibrahim,Yutaka Sasaki,Ryoji Asahi*

Main category: cs.CL

> 本研究提出了一种基于多个BERT变体的NER和RE方法来从科学文献中提取ORR催化剂的信息，实验结果表明领域特定的BERT模型在提取精度上表现更为优异。

<details>
  <summary>Details</summary>

**Motivation:** 氧还原反应（ORR）催化剂对提高燃料电池效率至关重要，而从大量的科学文献中提取结构化的ORR催化剂信息依然是一项重大挑战，这是材料科学研究的关键焦点。

**Method:** 提出了一种基于DyGIE++的命名实体识别(NER)和关系抽取(RE)方法，利用多种预训练的BERT变体（包括MatSciBERT和PubMedBERT）从科学文献中提取ORR催化剂相关的信息，这些信息被整合到燃料电池语料库（FC-CoMIcs）中用于材料信息学研究。

**Result:** 实验评估表明，微调后的PubMedBERT模型实现了最高的NER F1分数82.19%，而MatSciBERT模型则在RE上获得了最佳的F1分数66.10%。

**Conclusion:** 结果表明，领域特定的BERT模型在ORR催化剂提取方面优于一般科学模型，如BlueBERT，这展示了它们在大规模自动化文献分析中的潜力。

**Abstract:** The oxygen reduction reaction (ORR) catalyst plays a critical role in
enhancing fuel cell efficiency, making it a key focus in material science
research. However, extracting structured information about ORR catalysts from
vast scientific literature remains a significant challenge due to the
complexity and diversity of textual data. In this study, we propose a named
entity recognition (NER) and relation extraction (RE) approach using DyGIE++
with multiple pre-trained BERT variants, including MatSciBERT and PubMedBERT,
to extract ORR catalyst-related information from the scientific literature,
which is compiled into a fuel cell corpus for materials informatics
(FC-CoMIcs). A comprehensive dataset was constructed manually by identifying 12
critical entities and two relationship types between pairs of the entities. Our
methodology involves data annotation, integration, and fine-tuning of
transformer-based models to enhance information extraction accuracy. We assess
the impact of different BERT variants on extraction performance and investigate
the effects of annotation consistency. Experimental evaluations demonstrate
that the fine-tuned PubMedBERT model achieves the highest NER F1-score of
82.19% and the MatSciBERT model attains the best RE F1-score of 66.10%.
Furthermore, the comparison with human annotators highlights the reliability of
fine-tuned models for ORR catalyst extraction, demonstrating their potential
for scalable and automated literature analysis. The results indicate that
domain-specific BERT models outperform general scientific models like BlueBERT
for ORR catalyst extraction.

</details>


### [17] [Hallucination Stations: On Some Basic Limitations of Transformer-Based Language Models](https://arxiv.org/abs/2507.07505)
*Varin Sikka,Vishal Sikka*

Main category: cs.CL

> 该论文探讨了基于变压器的语言模型（LLM）在执行任务及验证任务准确性方面的计算复杂性限制。结果表明，LLM无法处理超过一定复杂度的计算和代理任务，并且也无法验证这些复杂任务的准确性。

<details>
  <summary>Details</summary>

**Motivation:** 随着变压器式语言模型在AI应用中的普及，研究其能力局限，尤其是生成虚假、错误信息的问题变得尤为重要。尤其是当使用LLM创建自主或半自主代理以执行各种任务时，了解哪些任务LLM能处理、哪些不能变得越来越关键。

**Method:** 文章从LLM推理的计算复杂度角度出发，探讨了LLM处理任务及验证准确性时的局限性。

**Result:** 研究表明，LLM无法执行超过一定复杂度的计算和代理任务，同样，也无法验证这些复杂任务的准确性。

**Conclusion:** 通过提供具体例子和讨论工作后果，文章指出了了解基于变压器的语言模型执行复杂任务和验证准确性能力局限的重要性。

**Abstract:** With widespread adoption of transformer-based language models in AI, there is
significant interest in the limits of LLMs capabilities, specifically so-called
hallucinations, occurrences in which LLMs provide spurious, factually incorrect
or nonsensical information when prompted on certain subjects. Furthermore,
there is growing interest in agentic uses of LLMs - that is, using LLMs to
create agents that act autonomously or semi-autonomously to carry out various
tasks, including tasks with applications in the real world. This makes it
important to understand the types of tasks LLMs can and cannot perform. We
explore this topic from the perspective of the computational complexity of LLM
inference. We show that LLMs are incapable of carrying out computational and
agentic tasks beyond a certain complexity, and further that LLMs are incapable
of verifying the accuracy of tasks beyond a certain complexity. We present
examples of both, then discuss some consequences of this work.

</details>


### [18] [Toward Real-World Chinese Psychological Support Dialogues: CPsDD Dataset and a Co-Evolving Multi-Agent System](https://arxiv.org/abs/2507.07509)
*Yuanchen Shi,Longyin Zhang,Fang Kong*

Main category: cs.CL

> Error

<details>
  <summary>Details</summary>

**Motivation:** Error

**Method:** Error

**Result:** Error

**Conclusion:** Error

**Abstract:** The growing need for psychological support due to increasing pressures has
exposed the scarcity of relevant datasets, particularly in non-English
languages. To address this, we propose a framework that leverages limited
real-world data and expert knowledge to fine-tune two large language models:
Dialog Generator and Dialog Modifier. The Generator creates large-scale
psychological counseling dialogues based on predefined paths, which guide
system response strategies and user interactions, forming the basis for
effective support. The Modifier refines these dialogues to align with
real-world data quality. Through both automated and manual review, we construct
the Chinese Psychological support Dialogue Dataset (CPsDD), containing 68K
dialogues across 13 groups, 16 psychological problems, 13 causes, and 12
support focuses. Additionally, we introduce the Comprehensive Agent Dialogue
Support System (CADSS), where a Profiler analyzes user characteristics, a
Summarizer condenses dialogue history, a Planner selects strategies, and a
Supporter generates empathetic responses. The experimental results of the
Strategy Prediction and Emotional Support Conversation (ESC) tasks demonstrate
that CADSS achieves state-of-the-art performance on both CPsDD and ESConv
datasets.

</details>


### [19] [Triadic Multi-party Voice Activity Projection for Turn-taking in Spoken Dialogue Systems](https://arxiv.org/abs/2507.07518)
*Mikey Elmers,Koji Inoue,Divesh Lala,Tatsuya Kawahara*

Main category: cs.CL

> 本研究首次将VAP技术应用于三人间对话场景以预测语音活动，实验表明此方法优于基线模型，但对话类型影响准确性。

<details>
  <summary>Details</summary>

**Motivation:** 传统的语音对话研究大多集中在二元设置上，本研究旨在通过应用语音活动投影技术来扩展到三人间的对话场景，以预测即将进行的轮流说话。

**Method:** 本研究首次将语音活动投影（VAP）技术应用于三人间的多角色对话场景中，通过仅使用声学数据来预测未来的说话活动。

**Result:** 研究结果表明，在三人间对话场景中训练的VAP模型在所有模型上均优于基线模型，但对话类型影响了准确性。

**Conclusion:** 这项研究证明了VAP技术可用于三人间对话场景中的轮流说话预测，未来的工作将把这种三人间VAP轮流说话模型整合进语音对话系统中。

**Abstract:** Turn-taking is a fundamental component of spoken dialogue, however
conventional studies mostly involve dyadic settings. This work focuses on
applying voice activity projection (VAP) to predict upcoming turn-taking in
triadic multi-party scenarios. The goal of VAP models is to predict the future
voice activity for each speaker utilizing only acoustic data. This is the first
study to extend VAP into triadic conversation. We trained multiple models on a
Japanese triadic dataset where participants discussed a variety of topics. We
found that the VAP trained on triadic conversation outperformed the baseline
for all models but that the type of conversation affected the accuracy. This
study establishes that VAP can be used for turn-taking in triadic dialogue
scenarios. Future work will incorporate this triadic VAP turn-taking model into
spoken dialogue systems.

</details>


### [20] [CEA-LIST at CheckThat! 2025: Evaluating LLMs as Detectors of Bias and Opinion in Text](https://arxiv.org/abs/2507.07539)
*Akram Elbouanani,Evan Dufraisse,Aboubacar Tuo,Adrian Popescu*

Main category: cs.CL

> 本研究展示了大型语言模型在少量示例提示的辅助下，能够有效地进行多语言主观性检测，并在CheckThat! 2025评估活动中多种语言任务中取得优异成绩。

<details>
  <summary>Details</summary>

**Motivation:** 动机在于探索在多语言环境下使用少量示例提示的大型语言模型进行主观性检测的竞争力。

**Method:** 使用大量语言模型（LLMs）和少量示例提示的方法进行多语言主观性检测。参与了CheckThat! 2025评估活动的任务1：主观性评估。通过精心设计的提示，展示了大型语言模型能够匹敌或超越经过微调的小型语言模型（SLMs），特别是在面对嘈杂或质量较低的数据时。在尝试了高级提示工程技术后，如辩论型LLMs和各种示例选择策略，发现并没有显著提升超过精心设计的标准少量示例提示的效果。

**Result:** 我们的系统在CheckThat! 2025主观性检测任务中多种语言的表现均为顶级，其中包括阿拉伯语和波兰语的第一名，以及意大利语、英语、德语和多语言项目的前四名。在阿拉伯语数据集上，该方法表现出特别的鲁棒性，可能是因为更能适应标注不一致的情况。

**Conclusion:** 这些发现强调了基于LLM的少量示例学习方法在多语言情感任务中的有效性与适应性，提供了一种强大的替代方案，特别是当标注数据稀缺或不一致时。

**Abstract:** This paper presents a competitive approach to multilingual subjectivity
detection using large language models (LLMs) with few-shot prompting. We
participated in Task 1: Subjectivity of the CheckThat! 2025 evaluation
campaign. We show that LLMs, when paired with carefully designed prompts, can
match or outperform fine-tuned smaller language models (SLMs), particularly in
noisy or low-quality data settings. Despite experimenting with advanced prompt
engineering techniques, such as debating LLMs and various example selection
strategies, we found limited benefit beyond well-crafted standard few-shot
prompts. Our system achieved top rankings across multiple languages in the
CheckThat! 2025 subjectivity detection task, including first place in Arabic
and Polish, and top-four finishes in Italian, English, German, and multilingual
tracks. Notably, our method proved especially robust on the Arabic dataset,
likely due to its resilience to annotation inconsistencies. These findings
highlight the effectiveness and adaptability of LLM-based few-shot learning for
multilingual sentiment tasks, offering a strong alternative to traditional
fine-tuning, particularly when labeled data is scarce or inconsistent.

</details>


### [21] [The Cross-Lingual Cost: Retrieval Biases in RAG over Arabic-English Corpora](https://arxiv.org/abs/2507.07543)
*Chen Amiraz,Yaroslav Fyodorov,Elad Haramaty,Zohar Karnin,Liane Lewin-Eytan*

Main category: cs.CL

> 针对阿拉伯语-英语的跨语言RAG，使用企业数据集进行特定领域的研究发现，检索是跨语言特定场景中的关键瓶颈，提出通过强制从两种语言等比例检索文档来改善性能。

<details>
  <summary>Details</summary>

**Motivation:** 先前的工作主要集中于生成方面，并使用主要来自维基百科的通用领域基准。在这种情况下，由于语言不平衡、与预训练数据的重叠以及内容记忆等因素，检索挑战往往被掩盖。为了解决这一问题，本研究提出了一种新的方法。

**Method:** 研究采用来自实际企业数据集的基准，在特定领域背景下研究阿拉伯语-英语的跨语言检索增强生成（RAG）。基准涵盖了用户查询和辅助文档语言的所有组合，这些组合独立且均匀随机抽取。这种设置使得可以系统地研究多语言检索行为。

**Result:** 研究表明，当用户查询与辅助文档语言不同时，跨语言特定领域的检索和生成性能显著下降。研究还提出了通过强制从两种语言中以相等概率检索文档的新策略，以解决这一问题，从而显著提高了跨语言和整体性能。

**Conclusion:** 这一发现强调了在实际的RAG应用程序中改善多语言检索方面具有重要机会。

**Abstract:** Cross-lingual retrieval-augmented generation (RAG) is a critical capability
for retrieving and generating answers across languages. Prior work in this
context has mostly focused on generation and relied on benchmarks derived from
open-domain sources, most notably Wikipedia. In such settings, retrieval
challenges often remain hidden due to language imbalances, overlap with
pretraining data, and memorized content. To address this gap, we study
Arabic-English RAG in a domain-specific setting using benchmarks derived from
real-world corporate datasets. Our benchmarks include all combinations of
languages for the user query and the supporting document, drawn independently
and uniformly at random. This enables a systematic study of multilingual
retrieval behavior.
  Our findings reveal that retrieval is a critical bottleneck in cross-lingual
domain-specific scenarios, with significant performance drops occurring when
the user query and supporting document languages differ. A key insight is that
these failures stem primarily from the retriever's difficulty in ranking
documents across languages. Finally, we propose a simple retrieval strategy
that addresses this source of failure by enforcing equal retrieval from both
languages, resulting in substantial improvements in cross-lingual and overall
performance. These results highlight meaningful opportunities for improving
multilingual retrieval, particularly in practical, real-world RAG applications.

</details>


### [22] [The Synergy Dilemma of Long-CoT SFT and RL: Investigating Post-Training Techniques for Reasoning VLMs](https://arxiv.org/abs/2507.07562)
*Jierun Chen,Tiezheng Yu,Haoli Bai,Lewei Yao,Jiannan Wu,Kaican Li,Fei Mi,Chaofan Tao,Lei Zhu,Manyi Zhang,Xiaohui Li,Lu Hou,Lifeng Shang,Qun Liu*

Main category: cs.CL

> 研究了长链式细思监督微调与强化学习在视觉语言模型中的作用及其结合存在的局限性。

<details>
  <summary>Details</summary>

**Motivation:** 尽管这些方法在仅包含语言的模型中展现出协同效果，但它们在视觉语言模型（VLM）中的综合有效性尚不明确。

**Method:** 通过系统性的研究探讨了长链式细思（CoT）监督微调（SFT）与强化学习（RL）在多种多模态推理基准上的独特作用及其相互作用。

**Result:** 发现SFT通过深入、结构化的推理提升对难题的回答性能，但带来冗长的回复并降低对简单问题的回答性能。而RL促进了可泛化、简洁的回答风格，但在最难的问题上改进不如SFT明显。结合两种技术并未产生叠加的效果，反而在精度、推理风格和回复长度等方面产生了权衡。

**Conclusion:** 这种“协同困境”揭示了需要开发更无缝且自适应的方法，以充分利用联合后续训练技术为推理VLM解锁全部潜力。

**Abstract:** Large vision-language models (VLMs) increasingly adopt post-training
techniques such as long chain-of-thought (CoT) supervised fine-tuning (SFT) and
reinforcement learning (RL) to elicit sophisticated reasoning. While these
methods exhibit synergy in language-only models, their joint effectiveness in
VLMs remains uncertain. We present a systematic investigation into the distinct
roles and interplay of long-CoT SFT and RL across multiple multimodal reasoning
benchmarks. We find that SFT improves performance on difficult questions by
in-depth, structured reasoning, but introduces verbosity and degrades
performance on simpler ones. In contrast, RL promotes generalization and
brevity, yielding consistent improvements across all difficulty levels, though
the improvements on the hardest questions are less prominent compared to SFT.
Surprisingly, combining them through two-staged, interleaved, or progressive
training strategies, as well as data mixing and model merging, all fails to
produce additive benefits, instead leading to trade-offs in accuracy, reasoning
style, and response length. This ``synergy dilemma'' highlights the need for
more seamless and adaptive approaches to unlock the full potential of combined
post-training techniques for reasoning VLMs.

</details>


### [23] [Single-to-mix Modality Alignment with Multimodal Large Language Model for Document Image Machine Translation](https://arxiv.org/abs/2507.07572)
*Yupu Liang,Yaping Zhang,Zhiyang Zhang,Yang Zhao,Lu Xiang,Chengqing Zong,Yu Zhou*

Main category: cs.CL

> M4Doc, a novel DIMT framework, uses alignment with MLLMs to achieve better translation quality and cross-domain generalization.

<details>
  <summary>Details</summary>

**Motivation:** To improve the generalization of DIMT models with limited training data and to balance the complex interaction between visual and textual elements.

**Method:** M4Doc aligns an image-only encoder with the multimodal representations of an MLLM pre-trained on large-scale document image datasets to learn visual-textual correlations.

**Result:** Experiments show significant improvements in translation quality and robustness, particularly in cross-domain generalization and handling difficult document images.

**Conclusion:** The M4Doc approach facilitates a lightweight model to achieve high translation quality while maintaining computational efficiency.

**Abstract:** Document Image Machine Translation (DIMT) aims to translate text within
document images, facing generalization challenges due to limited training data
and the complex interplay between visual and textual information. To address
these challenges, we introduce M4Doc, a novel single-to-mix modality alignment
framework leveraging Multimodal Large Language Models (MLLMs). M4Doc aligns an
image-only encoder with the multimodal representations of an MLLM, pre-trained
on large-scale document image datasets. This alignment enables a lightweight
DIMT model to learn crucial visual-textual correlations during training. During
inference, M4Doc bypasses the MLLM, maintaining computational efficiency while
benefiting from its multimodal knowledge. Comprehensive experiments demonstrate
substantial improvements in translation quality, especially in cross-domain
generalization and challenging document image scenarios.

</details>


### [24] [Bayesian Discrete Diffusion Beats Autoregressive Perplexity](https://arxiv.org/abs/2507.07586)
*Cooper Doyle*

Main category: cs.CL

> The paper uncovers a Bayesian aspect of discrete-diffusion language models and uses this to develop a lightweight ensemble method for better token probabilities and uncertainty estimates.

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to improve the performance of language models by revealing their Bayesian core and using this insight to enhance inference-time processing.

**Method:** We introduce a lightweight inference-time ensemble method that averages K mask-and-denoise passes to obtain posterior-aware token probabilities and uncertainty estimates, leveraging the hidden Bayesian core of discrete-diffusion language models.

**Result:** The method achieves a test perplexity of 8.8 with K=8 on WikiText-2, which is a significant improvement over GPT-2 Small, despite using a model of comparable size.

**Conclusion:** The paper demonstrates that by leveraging a Bayesian perspective, one can improve the performance of discrete-diffusion language models with minimal additional computational cost.

**Abstract:** We reveal a hidden Bayesian core of discrete-diffusion language models by
showing that the expected denoiser output under the forward masking
distribution recovers the exact posterior over clean tokens. Under minimal
assumptions, Monte Carlo marginalization over K independent corruptions
converges to this posterior at rate O(1/sqrt(K)), yielding a simple proof of
consistency and finite-sample error bounds. Building on this insight, we
introduce a lightweight inference-time ensemble that averages K
mask-and-denoise passes to obtain posterior-aware token probabilities and
uncertainty estimates at no extra training cost. On WikiText-2, our method
achieves test perplexity 8.8 with K=8, versus 20.3 for GPT-2 Small, despite
using a model of comparable size. Code is available at
https://github.com/mercury0100/bayesradd.

</details>


### [25] [Exploring the Limits of Model Compression in LLMs: A Knowledge Distillation Study on QA Tasks](https://arxiv.org/abs/2507.07630)
*Joyeeta Datta,Niclas Doll,Qusai Ramadan,Zeyd Boukhers*

Main category: cs.CL

> Error

<details>
  <summary>Details</summary>

**Motivation:** Error

**Method:** Error

**Result:** Error

**Conclusion:** Error

**Abstract:** Large Language Models (LLMs) have demonstrated outstanding performance across
a range of NLP tasks, however, their computational demands hinder their
deployment in real-world, resource-constrained environments. This work
investigates the extent to which LLMs can be compressed using Knowledge
Distillation (KD) while maintaining strong performance on Question Answering
(QA) tasks. We evaluate student models distilled from the Pythia and Qwen2.5
families on two QA benchmarks, SQuAD and MLQA, under zero-shot and one-shot
prompting conditions. Results show that student models retain over 90% of their
teacher models' performance while reducing parameter counts by up to 57.1%.
Furthermore, one-shot prompting yields additional performance gains over
zero-shot setups for both model families. These findings underscore the
trade-off between model efficiency and task performance, demonstrating that KD,
combined with minimal prompting, can yield compact yet capable QA systems
suitable for resource-constrained applications.

</details>


### [26] [FrugalRAG: Learning to retrieve and reason for multi-hop QA](https://arxiv.org/abs/2507.07634)
*Abhinav Java,Srivathsan Koundinyan,Nagarajan Natarajan,Amit Sharma*

Main category: cs.CL

> The study suggests that significant enhancement in RAG metrics can be achieved with a minimal amount of additional training data and improved prompts, challenging the notion that large-scale fine-tuning is necessary.

<details>
  <summary>Details</summary>

**Motivation:** The goal is to find a more efficient way of improving RAG metrics such as accuracy while reducing the number of retrieval searches required for a satisfactory answer.

**Method:** We explore the efficiency of retrieval-augmented generation (RAG) for answering complex questions by comparing the effects of large-scale fine-tuning and improved prompting techniques. This is done without relying on extensive retraining as other works suggest, using benchmarks like HotPotQA to assess performance.

**Result:** The research demonstrates that a standard ReAct pipeline with enhanced prompts can match or surpass state-of-the-art RAG performance, with fewer retrieval searches and a reduced training dataset.

**Conclusion:** Efficiency in RAG can be improved significantly by focusing on prompting techniques rather than solely on large-scale training, thus suggesting alternative paths for future development in question answering systems.

**Abstract:** We consider the problem of answering complex questions, given access to a
large unstructured document corpus. The de facto approach to solving the
problem is to leverage language models that (iteratively) retrieve and reason
through the retrieved documents, until the model has sufficient information to
generate an answer. Attempts at improving this approach focus on
retrieval-augmented generation (RAG) metrics such as accuracy and recall and
can be categorized into two types: (a) fine-tuning on large question answering
(QA) datasets augmented with chain-of-thought traces, and (b) leveraging
RL-based fine-tuning techniques that rely on question-document relevance
signals. However, efficiency in the number of retrieval searches is an equally
important metric, which has received less attention. In this work, we show
that: (1) Large-scale fine-tuning is not needed to improve RAG metrics,
contrary to popular claims in recent literature. Specifically, a standard ReAct
pipeline with improved prompts can outperform state-of-the-art methods on
benchmarks such as HotPotQA. (2) Supervised and RL-based fine-tuning can help
RAG from the perspective of frugality, i.e., the latency due to number of
searches at inference time. For example, we show that we can achieve
competitive RAG metrics at nearly half the cost (in terms of number of
searches) on popular RAG benchmarks, using the same base model, and at a small
training cost (1000 examples).

</details>


### [27] [Lost in Pronunciation: Detecting Chinese Offensive Language Disguised by Phonetic Cloaking Replacement](https://arxiv.org/abs/2507.07640)
*Haotan Guo,Jianfei He,Jiayuan Ma,Hongbin Na,Zimu Wang,Haiyang Zhang,Qi Chen,Wei Wang,Zijing Shi,Tao Shen,Ling Chen*

Main category: cs.CL

> 研究定义并分类了中文中的音韵伪装替换（PCR）现象，使用一个自然产生的数据集评估了现有模型的表现，发现它们存在严重不足，并提出了一种基于拼音的提示策略来提高检测效果。

<details>
  <summary>Details</summary>

**Motivation:** 研究者观察到，现有的评测大多依赖于基于规则的、合成扰动，忽视了真实的用户创造力，这成为了中文内容审核的主要障碍。本研究旨在解决这一问题，并提出了一个对现有检测器限制的现实基准。

**Method:** 研究者们通过创建一个四分类的表面形式分类法，并收集了一个包含500个自然发生的音韵伪装攻击帖子的数据集来定义音韵伪装替换（PCR）现象。他们还评估了最先进的语言模型对此数据集的性能，并探讨了一种基于拼音的提示策略。

**Result:** 研究表明，即使是最好的语言模型在检测这种攻击时的F1分数也只有0.672，而零样本链式思考提示的效果更差。研究发现，基于拼音的提示策略可以显著提高检测准确性。

**Conclusion:** 该研究提供了中文PCR的首个全面分类法、一个揭示现有检测器限制的现实基准，以及一种轻量级的缓解技术，从而推动了对稳健毒性检测的研究。

**Abstract:** Phonetic Cloaking Replacement (PCR), defined as the deliberate use of
homophonic or near-homophonic variants to hide toxic intent, has become a major
obstacle to Chinese content moderation. While this problem is well-recognized,
existing evaluations predominantly rely on rule-based, synthetic perturbations
that ignore the creativity of real users. We organize PCR into a four-way
surface-form taxonomy and compile \ours, a dataset of 500 naturally occurring,
phonetically cloaked offensive posts gathered from the RedNote platform.
Benchmarking state-of-the-art LLMs on this dataset exposes a serious weakness:
the best model reaches only an F1-score of 0.672, and zero-shot
chain-of-thought prompting pushes performance even lower. Guided by error
analysis, we revisit a Pinyin-based prompting strategy that earlier studies
judged ineffective and show that it recovers much of the lost accuracy. This
study offers the first comprehensive taxonomy of Chinese PCR, a realistic
benchmark that reveals current detectors' limits, and a lightweight mitigation
technique that advances research on robust toxicity detection.

</details>


### [28] [An Automated Length-Aware Quality Metric for Summarization](https://arxiv.org/abs/2507.07653)
*Andrew D. Foland*

Main category: cs.CL

> 提出NOIR指标，基于语义保留和长度压缩来评估摘要质量，与人工感知相关，可应用于各种摘要任务。

<details>
  <summary>Details</summary>

**Motivation:** 旨在提供一种不需要依赖耗时的人工参考摘要而自动评估摘要质量的方法，适应各种摘要任务。

**Method:** 提出了一种名为NOrmed Index of Retention (NOIR)的指标，该指标基于语义意义的保持和摘要长度的压缩来定量评估任意文本的摘要质量。

**Result:** 实验表明NOIR能够有效捕捉生成器的token长度/语义保留权衡，并且与人类对摘要质量的感知相关。

**Conclusion:** NOIR可以作为一种自动工具，用于评估并改进摘要算法、摘要提示和人工生成的摘要。

**Abstract:** This paper proposes NOrmed Index of Retention (NOIR), a quantitative
objective metric for evaluating summarization quality of arbitrary texts that
relies on both the retention of semantic meaning and the summary length
compression. This gives a measure of how well the recall-compression tradeoff
is managed, the most important skill in summarization. Experiments demonstrate
that NOIR effectively captures the token-length / semantic retention tradeoff
of a summarizer and correlates to human perception of sumarization quality.
Using a language model-embedding to measure semantic similarity, it provides an
automated alternative for assessing summarization quality without relying on
time-consuming human-generated reference summaries. The proposed metric can be
applied to various summarization tasks, offering an automated tool for
evaluating and improving summarization algorithms, summarization prompts, and
synthetically-generated summaries.

</details>


### [29] [SAS: Simulated Attention Score](https://arxiv.org/abs/2507.07694)
*Chuanyang Zheng,Jiankai Sun,Yihang Gao,Yuehao Wang,Peihao Wang,Jing Xiong,Liliang Ren,Hao Cheng,Janardhan Kulkarni,Yelong Shen,Atlas Wang,Mac Schwager,Anderson Schneider,Xiaodong Liu,Jianfeng Gao*

Main category: cs.CL

> 论文提出了一种新颖的注意力机制方法SAS，能够在不显著增加参数数量的情况下模拟更多的注意力头和增加隐藏特征维度，实验表明该方法在不同的数据集和任务上都获得了显著的性能提升。

<details>
  <summary>Details</summary>

**Motivation:** 通过观察多头注意力机制（MHA）发现，随着注意力头数量的增加，其性能得到提升，前提是每个头的隐藏大小足够大。因此，增加头数量和隐藏大小可以带来显著性能提升而不增加模型参数数量。基于此，提出了SAS方法。

**Method:** 文中提出了一种名为模拟注意分数（SAS）的方法，该方法通过将低维头表示投影到高维空间来保持紧凑的模型大小同时模拟更多的注意头和每个头的隐藏特征维度。此外，该方法还扩展到键和查询嵌入的特征维度模拟，以增强表达能力。为了控制参数成本，文中还提出了参数高效注意力聚合（PEAA）的方法。

**Result:** 在各种数据集和任务上的综合实验表明，提出的SAS方法在不同的注意力变体中实现了显著的改进。

**Conclusion:** 通过对多头注意力机制的研究，提出了一种模拟注意分数（SAS）的方法，该方法在保持模型大小不变的情况下，增强了模型的注意力能力和表达能力，从而在不增加模型参数数量的前提下实现了性能的提升。

**Abstract:** The attention mechanism is a core component of the Transformer architecture.
Various methods have been developed to compute attention scores, including
multi-head attention (MHA), multi-query attention, group-query attention and so
on. We further analyze the MHA and observe that its performance improves as the
number of attention heads increases, provided the hidden size per head remains
sufficiently large. Therefore, increasing both the head count and hidden size
per head with minimal parameter overhead can lead to significant performance
gains at a low cost. Motivated by this insight, we introduce Simulated
Attention Score (SAS), which maintains a compact model size while simulating a
larger number of attention heads and hidden feature dimension per head. This is
achieved by projecting a low-dimensional head representation into a
higher-dimensional space, effectively increasing attention capacity without
increasing parameter count. Beyond the head representations, we further extend
the simulation approach to feature dimension of the key and query embeddings,
enhancing expressiveness by mimicking the behavior of a larger model while
preserving the original model size. To control the parameter cost, we also
propose Parameter-Efficient Attention Aggregation (PEAA). Comprehensive
experiments on a variety of datasets and tasks demonstrate the effectiveness of
the proposed SAS method, achieving significant improvements over different
attention variants.

</details>


### [30] [KeyKnowledgeRAG (K^2RAG): An Enhanced RAG method for improved LLM question-answering capabilities](https://arxiv.org/abs/2507.07695)
*Hruday Markondapatnaikuni,Basem Suleiman,Abdelkarim Erradi,Shijing Chen*

Main category: cs.CL

> K2RAG是一种新型框架，克服了RAG的局限性，提高检索质量和系统效率，同时提高了精度和效率，并表现出更好的可扩展性。

<details>
  <summary>Details</summary>

**Motivation:** 尽管已经开发了多种减少训练时间的方法，但随着LLMs的规模和复杂性增加，它们在重新训练时仍然非常耗费资源。因此，需要一种新的知识扩展方法。

**Method:** K2RAG整合了稠密向量搜索、稀疏向量搜索、知识图谱和文本总结，以提高检索质量和系统效率，包括训练数据的预处理步骤，以显著减少训练时间。

**Result:** K2RAG在MultiHopRAG数据集上的表现优于常见的RAG实现，达到最高的答案相似度平均值0.57和第三分位数0.82。

**Conclusion:** K2RAG不仅在答案准确性和执行速度上有所改进，同时显著减少了所需的VRAM，提高了系统效率和数据处理能力。

**Abstract:** Fine-tuning is an immensely resource-intensive process when retraining Large
Language Models (LLMs) to incorporate a larger body of knowledge. Although many
fine-tuning techniques have been developed to reduce the time and computational
cost involved, the challenge persists as LLMs continue to grow in size and
complexity. To address this, a new approach to knowledge expansion in LLMs is
needed. Retrieval-Augmented Generation (RAG) offers one such alternative by
storing external knowledge in a database and retrieving relevant chunks to
support question answering. However, naive implementations of RAG face
significant limitations in scalability and answer accuracy. This paper
introduces KeyKnowledgeRAG (K2RAG), a novel framework designed to overcome
these limitations. Inspired by the divide-and-conquer paradigm, K2RAG
integrates dense and sparse vector search, knowledge graphs, and text
summarization to improve retrieval quality and system efficiency. The framework
also includes a preprocessing step that summarizes the training data,
significantly reducing the training time. K2RAG was evaluated using the
MultiHopRAG dataset, where the proposed pipeline was trained on the document
corpus and tested on a separate evaluation set. Results demonstrated notable
improvements over common naive RAG implementations. K2RAG achieved the highest
mean answer similarity score of 0.57, and reached the highest third quartile
(Q3) similarity of 0.82, indicating better alignment with ground-truth answers.
In addition to improved accuracy, the framework proved highly efficient. The
summarization step reduced the average training time of individual components
by 93%, and execution speed was up to 40% faster than traditional knowledge
graph-based RAG systems. K2RAG also demonstrated superior scalability,
requiring three times less VRAM than several naive RAG implementations tested
in this study.

</details>


### [31] [Rethinking the Privacy of Text Embeddings: A Reproducibility Study of "Text Embeddings Reveal (Almost) As Much As Text"](https://arxiv.org/abs/2507.07700)
*Dominykas Seputis,Yongkang Li,Karsten Langerak,Serghei Mihailov*

Main category: cs.CL

> 该研究通过验证Vec2Text框架和扩展实验，探讨了文本嵌入的隐私风险，指出虽然Vec2Text在理想条件下能有效重构原始文本，但存在一些弱点，并提出高斯噪声和量化技术作为可能的隐私防御手段。

<details>
  <summary>Details</summary>

**Motivation:** 动机是验证Vec2Text的研究结果及其隐私保护能力，考虑到高维嵌入空间通常不直观且不透明。

**Method:** 研究方法包括重复Vec2Text的实验，以及通过参数敏感性分析、敏感输入重构测试和嵌入量化实验来扩展研究。

**Result:** 结果显示Vec2Text在理想条件下能有效重构包括密码在内的文本序列，但它受到输入序列长度等限制。

**Conclusion:** 结论强调在使用文本嵌入时需要谨慎，并强调需要进一步研究以构建更强大的NLP系统隐私防御机制。

**Abstract:** Text embeddings are fundamental to many natural language processing (NLP)
tasks, extensively applied in domains such as recommendation systems and
information retrieval (IR). Traditionally, transmitting embeddings instead of
raw text has been seen as privacy-preserving. However, recent methods such as
Vec2Text challenge this assumption by demonstrating that controlled decoding
can successfully reconstruct original texts from black-box embeddings. The
unexpectedly strong results reported by Vec2Text motivated us to conduct
further verification, particularly considering the typically non-intuitive and
opaque structure of high-dimensional embedding spaces. In this work, we
reproduce the Vec2Text framework and evaluate it from two perspectives: (1)
validating the original claims, and (2) extending the study through targeted
experiments. First, we successfully replicate the original key results in both
in-domain and out-of-domain settings, with only minor discrepancies arising due
to missing artifacts, such as model checkpoints and dataset splits.
Furthermore, we extend the study by conducting a parameter sensitivity
analysis, evaluating the feasibility of reconstructing sensitive inputs (e.g.,
passwords), and exploring embedding quantization as a lightweight privacy
defense. Our results show that Vec2Text is effective under ideal conditions,
capable of reconstructing even password-like sequences that lack clear
semantics. However, we identify key limitations, including its sensitivity to
input sequence length. We also find that Gaussian noise and quantization
techniques can mitigate the privacy risks posed by Vec2Text, with quantization
offering a simpler and more widely applicable solution. Our findings emphasize
the need for caution in using text embeddings and highlight the importance of
further research into robust defense mechanisms for NLP systems.

</details>


### [32] [Not All Preferences are What You Need for Post-Training: Selective Alignment Strategy for Preference Optimization](https://arxiv.org/abs/2507.07725)
*Zhijin Dong*

Main category: cs.CL

> 本论文介绍了一种选择性对齐策略，专注于高影响力的标记，以提高对齐的保真度并减少计算负担。实验表明，这个Selective-DPO方法优于现有的方法。

<details>
  <summary>Details</summary>

**Motivation:** 训练后期模型的对齐是一个关键挑战，因为并非所有标记对模型性能都有同等贡献。

**Method:** 此论文提出了一种选择性对齐策略，该策略优先考虑偏好对中高影响的标记，并利用当前策略和参考模型之间的标记级对数概率差异。通过聚焦这些信息量大的标记，该方法减少了计算开销并增强了对齐的保真度。

**Result:** 在诸如Arena-Hard和MT-Bench的基准测试上进行的全面实验验证了Selective-DPO方法优于标准DPO和基于蒸馏的基线。

**Conclusion:** 研究发现强调了标记级优化和参考模型选择在推进LLM偏好对齐中的重要性。

**Abstract:** Post-training alignment of large language models (LLMs) is a critical
challenge, as not all tokens contribute equally to model performance. This
paper introduces a selective alignment strategy that prioritizes high-impact
tokens within preference pairs, leveraging token-level log-probability
differences between the current policy and a reference model. By focusing on
these informative tokens, our approach reduces computational overhead and
enhances alignment fidelity. We further explore the role of reference model
quality, demonstrating that stronger reference models significantly improve
token selection accuracy and overall optimization effectiveness. Comprehensive
experiments on benchmarks such as Arena-Hard and MT-Bench validate the
superiority of our Selective-DPO method over standard DPO and
distillation-based baselines. Our findings highlight the importance of
token-level optimization and reference model selection in advancing preference
alignment for LLMs. The code is available at
https://github.com/Dongzhijin/SDPO.

</details>


### [33] [Code-Switching in End-to-End Automatic Speech Recognition: A Systematic Literature Review](https://arxiv.org/abs/2507.07741)
*Maha Tufail Agro,Atharva Kulkarni,Karima Kadaoui,Zeerak Talat,Hanan Aldarmaki*

Main category: cs.CL

> 本文提供了一个关于代码切换在端到端语音识别模型中系统文献回顾的摘要，分析了涉及的语言、数据集、度量方法、模型选择和性能，讨论了在端到端代码切换语音识别中的挑战。

<details>
  <summary>Details</summary>

**Motivation:** 受到自动语音识别（ASR）研究兴趣的增长，以及在经常发生代码切换（CS）的语言中工作的不断增多的驱动，作者开展了这项研究。

**Method:** 收集并手动标注发表在同行评审会议上的论文，文档中包括考虑的语言、数据集、度量方法、模型选择和性能。

**Result:** 提供了关于当前研究工作和可用资源以及未来研究机会和缺口的见解。

**Conclusion:** 这项分析揭示了在端到端代码切换语音识别研究领域的现状和未来的发展方向。

**Abstract:** Motivated by a growing research interest into automatic speech recognition
(ASR), and the growing body of work for languages in which code-switching (CS)
often occurs, we present a systematic literature review of code-switching in
end-to-end ASR models. We collect and manually annotate papers published in
peer reviewed venues. We document the languages considered, datasets, metrics,
model choices, and performance, and present a discussion of challenges in
end-to-end ASR for code-switching. Our analysis thus provides insights on
current research efforts and available resources as well as opportunities and
gaps to guide future research.

</details>


### [34] [When Large Language Models Meet Law: Dual-Lens Taxonomy, Technical Advances, and Ethical Governance](https://arxiv.org/abs/2507.07748)
*Peizhang Shao,Linrui Xu,Jinxi Wang,Wei Zhou,Xingyu Wu*

Main category: cs.CL

> 该论文为大语言模型在法律领域的应用提供了一个综合性的回顾和前景展望，同时也指出了采用LLM所面临的挑战，包括幻觉问题、可解释性缺陷、司法区域适应性和伦理不对称问题。最终，该著作提供了一份技术路线图和概念框架，为研究者和从业者导航法律人工智能的算法未来奠定了坚实的基础。

<details>
  <summary>Details</summary>

**Motivation:** 该论文首次全面回顾了大语言模型（LLM）在法律领域的应用，并提出了一种创新的双重视角分类法，将法律推理框架和专业术语结合起来，系统性地统一了历史研究和最新成果。

**Method:** 通过利用基于Transformer的LLM，这些模型展示了如情境推理和生成性论证等新兴能力，克服了传统限制，动态捕捉法律语义并统一证据推理。

**Result:** 文中记录了任务泛化、推理形式化、流程集成以及解决文本处理、知识整合和评估严谨性方面核心挑战的显著进步，通过诸如稀疏注意力机制和专家混合架构等技术创新来实现。

**Conclusion:** 该篇综述提出了一个将法律角色映射到NLP子任务并计算实现Toulmin论证框架的新分类法，从而系统化地推动了在推理、检索、预测和争端解决方面的进步。指出了关键前沿领域，包括低资源系统、多模态证据整合和动态反驳处理。

**Abstract:** This paper establishes the first comprehensive review of Large Language
Models (LLMs) applied within the legal domain. It pioneers an innovative dual
lens taxonomy that integrates legal reasoning frameworks and professional
ontologies to systematically unify historical research and contemporary
breakthroughs. Transformer-based LLMs, which exhibit emergent capabilities such
as contextual reasoning and generative argumentation, surmount traditional
limitations by dynamically capturing legal semantics and unifying evidence
reasoning. Significant progress is documented in task generalization, reasoning
formalization, workflow integration, and addressing core challenges in text
processing, knowledge integration, and evaluation rigor via technical
innovations like sparse attention mechanisms and mixture-of-experts
architectures. However, widespread adoption of LLM introduces critical
challenges: hallucination, explainability deficits, jurisdictional adaptation
difficulties, and ethical asymmetry. This review proposes a novel taxonomy that
maps legal roles to NLP subtasks and computationally implements the Toulmin
argumentation framework, thus systematizing advances in reasoning, retrieval,
prediction, and dispute resolution. It identifies key frontiers including
low-resource systems, multimodal evidence integration, and dynamic rebuttal
handling. Ultimately, this work provides both a technical roadmap for
researchers and a conceptual framework for practitioners navigating the
algorithmic future, laying a robust foundation for the next era of legal
artificial intelligence. We have created a GitHub repository to index the
relevant papers: https://github.com/Kilimajaro/LLMs_Meet_Law.

</details>


### [35] [StreamUni: Achieving Streaming Speech Translation with a Unified Large Speech-Language Model](https://arxiv.org/abs/2507.07803)
*Shoutao Guo,Xiang Li,Shaolei Zhang,Mengge Liu,Wei Chen,Yang Feng*

Main category: cs.CL

> 本文提出了一种新的流式语音翻译方法StreamUni，利用统一的大型语音语言模型来改进现有方法中存在的限制，实验显示其性能优越。

<details>
  <summary>Details</summary>

**Motivation:** 文章旨在提出一种新的方法来解决现有流式语音翻译方法在实时性和翻译质量之间的平衡问题，特别是由于语音分段限制了翻译模型的策略决策和生成能力。

**Method:** StreamUni采用统一的大型语音语言模型(LSLM)，通过语音链条思维(CoT)引导多阶段输出来实现流式语音翻译(StreamST)。这种方法解决了传统的SimulST模型在学习有效策略方面的难题，避免了对外部分词模型的依赖，从而减少了限制并提高了翻译质量。

**Result:** 实验表明，这种方法在流式语音翻译任务上达到了最先进的性能。

**Conclusion:** 通过引入StreamUni和相应的流水线CoT训练方法，研究在没有大量特定策略训练的情况下实现了高效的低延迟策略决策和生成能力，证明了其方法的有效性和先进性。

**Abstract:** Streaming speech translation (StreamST) requires determining appropriate
timing, known as policy, to generate translations while continuously receiving
source speech inputs, balancing low latency with high translation quality.
However, existing StreamST methods typically operate on sentence-level speech
segments, referred to as simultaneous speech translation (SimulST). In
practice, they require collaboration with segmentation models to accomplish
StreamST, where the truncated speech segments constrain SimulST models to make
policy decisions and generate translations based on limited contextual
information. Moreover, SimulST models struggle to learn effective policies due
to the complexity of speech inputs and cross-lingual generation. To address
these challenges, we propose StreamUni, which achieves StreamST through a
unified Large Speech-Language Model (LSLM). Specifically, StreamUni
incorporates speech Chain-of-Thought (CoT) in guiding the LSLM to generate
multi-stage outputs. Leveraging these multi-stage outputs, StreamUni
simultaneously accomplishes speech segmentation, policy decision, and
translation generation, completing StreamST without requiring massive
policy-specific training. Additionally, we propose a streaming CoT training
method that enhances low-latency policy decisions and generation capabilities
using limited CoT data. Experiments demonstrate that our approach achieves
state-of-the-art performance on StreamST tasks.

</details>


### [36] [Bridging Logic and Learning: Decoding Temporal Logic Embeddings via Transformers](https://arxiv.org/abs/2507.07808)
*Sara Candussio,Gaia Saveri,Gabriele Sarti,Luca Bortolussi*

Main category: cs.CL

> 研究提出一种使用Transformer模型逆向转换STL公式嵌入的方法，以实现从连续表示到具体逻辑规范的转换，适用于要求挖掘任务，尤其在信号时间属性描述方面。

<details>
  <summary>Details</summary>

**Motivation:** 使连续表示转化为具体的逻辑公式，以便在公式语义空间中进行连续学习和优化。

**Method:** 使用基于Transformer的仅解码器模型来逆向转换信号时态逻辑（STL）公式语义嵌入，通过从STL语法构造的小词汇表生成有效公式。

**Result:** 模型经过仅1个轮次训练后即能生成有效公式，大约10轮次后能掌握逻辑语义，且解码出的公式往往更简洁并尽量保持与参考公式语义等价。

**Conclusion:** 该模型在不同复杂度训练公式水平上的测试展示了其从嵌入中捕获语义信息并推广到新分布的能力，证明了在直接解决基于轨迹的分类任务的逻辑要求挖掘任务上的有效性。

**Abstract:** Continuous representations of logic formulae allow us to integrate symbolic
knowledge into data-driven learning algorithms. If such embeddings are
semantically consistent, i.e. if similar specifications are mapped into nearby
vectors, they enable continuous learning and optimization directly in the
semantic space of formulae. However, to translate the optimal continuous
representation into a concrete requirement, such embeddings must be invertible.
We tackle this issue by training a Transformer-based decoder-only model to
invert semantic embeddings of Signal Temporal Logic (STL) formulae. STL is a
powerful formalism that allows us to describe properties of signals varying
over time in an expressive yet concise way. By constructing a small vocabulary
from STL syntax, we demonstrate that our proposed model is able to generate
valid formulae after only 1 epoch and to generalize to the semantics of the
logic in about 10 epochs. Additionally, the model is able to decode a given
embedding into formulae that are often simpler in terms of length and nesting
while remaining semantically close (or equivalent) to gold references. We show
the effectiveness of our methodology across various levels of training formulae
complexity to assess the impact of training data on the model's ability to
effectively capture the semantic information contained in the embeddings and
generalize out-of-distribution. Finally, we deploy our model for solving a
requirement mining task, i.e. inferring STL specifications that solve a
classification task on trajectories, performing the optimization directly in
the semantic space.

</details>


### [37] [Understanding and Controlling Repetition Neurons and Induction Heads in In-Context Learning](https://arxiv.org/abs/2507.07810)
*Nhi Hoai Doan,Tatsuya Hiraoka,Kentaro Inui*

Main category: cs.CL

> 本文通过技能神经元来探讨大型语言模型识别重复输入模式的能力与其在上下文学习中的表现之间的关系，并提出减少重复输出的方法。

<details>
  <summary>Details</summary>

**Motivation:** 本研究的动机是探索并理解大型语言模型识别重复模式的能力如何影响其在上下文学习任务中的表现，并试图找到减少重复输出同时保持强大上下文学习能力的方法。

**Method:** 研究着重于通过技能神经元，特别是重复神经元，来分析大型语言模型(LLMs)识别重复输入模式的能力与其在上下文学习(ICL)中的表现之间的关系。不同于以前集中关注注意力头的工作，本研究通过实验展示了这些神经元对ICL性能的影响取决于它们所在的层数深度。

**Result:** 研究结果显示，重复神经元对ICL性能的影响与其所在神经网络层的深度有关。通过比较重复神经元和归纳头的影响，提出了一些减少重复输出而维持强大ICL能力的策略。

**Conclusion:** 研究结论指出，重复神经元在不同深度会影响模型的上下文学习性能。通过精确控制重复神经元的激活状态，可以在减少模型重复输出的同时保持或提高其上下文学习能力。

**Abstract:** This paper investigates the relationship between large language models'
(LLMs) ability to recognize repetitive input patterns and their performance on
in-context learning (ICL). In contrast to prior work that has primarily focused
on attention heads, we examine this relationship from the perspective of skill
neurons, specifically repetition neurons. Our experiments reveal that the
impact of these neurons on ICL performance varies depending on the depth of the
layer in which they reside. By comparing the effects of repetition neurons and
induction heads, we further identify strategies for reducing repetitive outputs
while maintaining strong ICL capabilities.

</details>


### [38] [On the Effect of Instruction Tuning Loss on Generalization](https://arxiv.org/abs/2507.07817)
*Anwoy Chatterjee,H S V N S Kowndinya Renduchintala,Sumit Bhatia,Tanmoy Chakraborty*

Main category: cs.CL

> 文章提出了一种新的指令调整方法WIT，该方法通过适当地调整提示和响应令牌在损失函数中的权重来改进模型性能和鲁棒性。实验结果表明，WIT比传统的方法更优。

<details>
  <summary>Details</summary>

**Motivation:** 现有的指令调整方法通常忽略了对损失函数的优化，特别是仅针对响应令牌计算损失的方法的有效性。研究动机在于探讨通过重新考虑损失函数的设计来提升模型性能和鲁棒性的可能性。

**Method:** 研究中系统性地调查了对提示令牌和响应令牌在指令调整损失中施加不同权重的影响，并提出了一种新的方法——加权指令调整（Weighted Instruction Tuning, WIT）。通过实验测试了该方法在不同规模和家族的语言模型、不同大小的细调数据集和五个不同的评估基准上的效果。

**Result:** 研究分析了当前指令调整（Instruction Tuning）中损失函数的优化问题，提出加权指令调整（Weighted Instruction Tuning, WIT）方法，通过实验验证了传统的仅对响应令牌计算损失的方法存在性能不足和对输入提示变化鲁棒性有限的问题。实验表明，对提示令牌和响应令牌赋予适当的权重可以提高模型的性能和鲁棒性。

**Conclusion:** 研究表明标准的指令调整损失往往会导致次优的性能和对输入提示变化鲁棒性不足。通过对提示令牌赋予较低到中等的权重，同时对响应令牌赋予中等到较高的权重，可以得到性能最佳的模型。这表明重新设计指令调整的损失函数是必要的，并为开发更强大、更具通用性的语言模型提供了有用的看法。

**Abstract:** Instruction Tuning has emerged as a pivotal post-training paradigm that
enables pre-trained language models to better follow user instructions. Despite
its significance, little attention has been given to optimizing the loss
function used. A fundamental, yet often overlooked, question is whether the
conventional auto-regressive objective - where loss is computed only on
response tokens, excluding prompt tokens - is truly optimal for instruction
tuning. In this work, we systematically investigate the impact of
differentially weighting prompt and response tokens in instruction tuning loss,
and propose Weighted Instruction Tuning (WIT) as a better alternative to
conventional instruction tuning. Through extensive experiments on five language
models of different families and scale, three finetuning datasets of different
sizes, and five diverse evaluation benchmarks, we show that the standard
instruction tuning loss often yields suboptimal performance and limited
robustness to input prompt variations. We find that a low-to-moderate weight
for prompt tokens coupled with a moderate-to-high weight for response tokens
yields the best-performing models across settings and also serve as better
starting points for the subsequent preference alignment training. These
findings highlight the need to reconsider instruction tuning loss and offer
actionable insights for developing more robust and generalizable models. Our
code is open-sourced at https://github.com/kowndinya-renduchintala/WIT.

</details>


### [39] [Conditional Unigram Tokenization with Parallel Data](https://arxiv.org/abs/2507.07824)
*Gianluca Vico,Jindřinch Libovický*

Main category: cs.CL

> 本文介绍了一种条件单词素化的创新技术，该技术在一定条件下显现出了改善语言模型性能的潜力，但在机器翻译上的表现则没有显著改进。

<details>
  <summary>Details</summary>

**Motivation:** 我们引入一种新的条件单词素化方法，目的是探索如何增强跨语言任务中的语义对齐效果。

**Method:** 我们提出了一种新的方法——条件单词素化，这种方法通过利用平行数据中源语言词素来调整目标语言词素的概率，从而扩展了单词素化。给定一个固定的源语言词素器，我们的方法学习一个目标语言词素器，旨在最大化跨语言语义对齐。

**Result:** 我们在四个不同语言系列和资源水平的语言对上评估了我们的词素化方法，观察了单词词素器的内在性质及其在机器翻译和语言模型中的下游性能。虽然我们的条件单词词素器的统计性质与标准单词素器相当，但结果是混合的：我们没有观察到机器翻译质量的提高，但在语言模型中发现了持续的困惑度降低。我们假设条件概率估计与词汇表大小呈二次关系扩大了数据效率瓶颈。

**Conclusion:** 研究结果表明，利用条件单词词素化进行跨语言任务并非总是理想的解决方案，应当考虑其他参数化方式来提高实践中的效率。

**Abstract:** We introduce conditional unigram tokenization, a novel approach that extends
unigram tokenization by conditioning target token probabilities on
source-language tokens from parallel data. Given a fixed source tokenizer, our
method learns a target tokenizer that maximizes cross-lingual semantic
alignment. We evaluate our tokenizer on four language pairs across different
families and resource levels, examining intrinsic properties and downstream
performance on machine translation and language modeling. While our conditional
tokenizer maintains comparable statistical properties to standard unigram
tokenizers, results are mixed: we observe no improvements in machine
translation quality, but find consistent perplexity reductions in language
modeling. We hypothesize that quadratic scaling of conditional probability
estimation with respect to the vocabulary size creates a data efficiency
bottleneck. Our findings suggest that alternative parameterizations may be
necessary for practical cross-lingual tokenization.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [40] [Multi-level Mixture of Experts for Multimodal Entity Linking](https://arxiv.org/abs/2507.07108)
*Zhiwei Hu,Víctor Gutiérrez-Basulto,Zhiliang Xiang,Ru Li,Jeff Z. Pan*

Main category: cs.CV

> 本文提出了一种多级专家混合模型(MMoE)，用于解决多模态实体链接中的指代模糊和动态选择模态内容的问题。

<details>
  <summary>Details</summary>

**Motivation:** 现有的多模态实体链接方法缺乏处理指代模糊和区分模态信息重要性的能力，本文旨在通过MMoE模型来改进这些问题。

**Method:** MMoE模型包含四个部分：描述感知的提及增强模块、多模态特征提取模块、多层次专家混合模块，通过开关混合专家机制动态地选择信息区域的特征。

**Result:** 实验表明MMoE模型在多模态实体链接任务上表现优于现有方法。

**Conclusion:** MMoE模型通过多级专家混合机制和描述感知的提及增强模块提高了多模态实体链接的性能。

**Abstract:** Multimodal Entity Linking (MEL) aims to link ambiguous mentions within
multimodal contexts to associated entities in a multimodal knowledge base.
Existing approaches to MEL introduce multimodal interaction and fusion
mechanisms to bridge the modality gap and enable multi-grained semantic
matching. However, they do not address two important problems: (i) mention
ambiguity, i.e., the lack of semantic content caused by the brevity and
omission of key information in the mention's textual context; (ii) dynamic
selection of modal content, i.e., to dynamically distinguish the importance of
different parts of modal information. To mitigate these issues, we propose a
Multi-level Mixture of Experts (MMoE) model for MEL. MMoE has four components:
(i) the description-aware mention enhancement module leverages large language
models to identify the WikiData descriptions that best match a mention,
considering the mention's textual context; (ii) the multimodal feature
extraction module adopts multimodal feature encoders to obtain textual and
visual embeddings for both mentions and entities; (iii)-(iv) the intra-level
mixture of experts and inter-level mixture of experts modules apply a switch
mixture of experts mechanism to dynamically and adaptively select features from
relevant regions of information. Extensive experiments demonstrate the
outstanding performance of MMoE compared to the state-of-the-art. MMoE's code
is available at: https://github.com/zhiweihu1103/MEL-MMoE.

</details>


### [41] [CoPT: Unsupervised Domain Adaptive Segmentation using Domain-Agnostic Text Embeddings](https://arxiv.org/abs/2507.07125)
*Cristina Mata,Kanchana Ranasinghe,Michael S. Ryoo*

Main category: cs.CV

> markdown

<details>
  <summary>Details</summary>

**Motivation:** markdown

**Method:** markdown

**Result:** {
  "tldr": "本文提出了一种基于协方差的像素文本损失(CoPT)，利用领域无关的文本嵌入来学习图像分割编码器中的领域不变特征，从而改进无监督领域适应(UDA)的性能，特别是在语义分割任务中。实验结果表明，使用CoPT训练的模型在四个基准测试中达到了UDA语义分割的新技术水平。",
  "motivation": "无监督领域适应方法在语义分割任务中的进步有限，主要是因为没有充分利用文本描述的领域无关特性。通过结合大模型生成的源领域和目标领域的描述，以及CLIP模型生成的文本嵌入，本文旨在提高UDA在语义分割中的性能。",
  "method": "本文提出了一种新的方法，即基于协方差的像素文本损失(CoPT)。该方法利用领域无关的文本嵌入来学习领域不变的特征。文本嵌入是通过一个大语言模型(LLM)生成的源领域和目标领域的描述转换的，这些描述通过一个冻结的CLIP模型进行处理和结合。",
  "result": "通过在四个基准测试上的实验，本文方法表现出了在无监督领域适应(UDA)的语义分割中的新技术水平，证明了所提出的方法的有效性和优越性。",
  "conclusion": "实验表明，基于协方差的像素文本损失(CoPT)对于提高无监督领域适应在语义分割任务中的性能是非常有效的，具体结果在多个数据集上得到了验证。
}

**Conclusion:** markdown

**Abstract:** Unsupervised domain adaptation (UDA) involves learning class semantics from
labeled data within a source domain that generalize to an unseen target domain.
UDA methods are particularly impactful for semantic segmentation, where
annotations are more difficult to collect than in image classification. Despite
recent advances in large-scale vision-language representation learning, UDA
methods for segmentation have not taken advantage of the domain-agnostic
properties of text. To address this, we present a novel Covariance-based
Pixel-Text loss, CoPT, that uses domain-agnostic text embeddings to learn
domain-invariant features in an image segmentation encoder. The text embeddings
are generated through our LLM Domain Template process, where an LLM is used to
generate source and target domain descriptions that are fed to a frozen CLIP
model and combined. In experiments on four benchmarks we show that a model
trained using CoPT achieves the new state of the art performance on UDA for
segmentation. The code can be found at https://github.com/cfmata/CoPT.

</details>


### [42] [Image Can Bring Your Memory Back: A Novel Multi-Modal Guided Attack against Image Generation Model Unlearning](https://arxiv.org/abs/2507.07139)
*Renyang Liu,Guanlin Li,Tianwei Zhang,See-Kiong Ng*

Main category: cs.CV

> The paper introduces Recall, an adversarial framework that targets vulnerabilities in unlearning mechanisms of image generation models by using optimized adversarial image prompts.

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to address the ethical and societal risks associated with image generation models by evaluating and improving the effectiveness of machine unlearning techniques, particularly against multi-modal adversarial inputs.

**Method:** Recall exploits the multi-modal conditioning of diffusion models by optimizing adversarial image prompts guided by a relevant reference image, contrasting with existing methods that mainly use adversarial text prompts.

**Result:** Experiment results demonstrate Recall's superior performance in degrading unlearned image generation models across several dimensions, indicating critical vulnerabilities in current unlearning strategies.

**Conclusion:** The findings highlight the need for more robust unlearning mechanisms within image generation models to ensure their safe application and reliability.

**Abstract:** Recent advances in image generation models (IGMs), particularly
diffusion-based architectures such as Stable Diffusion (SD), have markedly
enhanced the quality and diversity of AI-generated visual content. However,
their generative capability has also raised significant ethical, legal, and
societal concerns, including the potential to produce harmful, misleading, or
copyright-infringing content. To mitigate these concerns, machine unlearning
(MU) emerges as a promising solution by selectively removing undesirable
concepts from pretrained models. Nevertheless, the robustness and effectiveness
of existing unlearning techniques remain largely unexplored, particularly in
the presence of multi-modal adversarial inputs.
  To bridge this gap, we propose Recall, a novel adversarial framework
explicitly designed to compromise the robustness of unlearned IGMs. Unlike
existing approaches that predominantly rely on adversarial text prompts, Recall
exploits the intrinsic multi-modal conditioning capabilities of diffusion
models by efficiently optimizing adversarial image prompts with guidance from a
single semantically relevant reference image. Extensive experiments across ten
state-of-the-art unlearning methods and diverse tasks show that Recall
consistently outperforms existing baselines in terms of adversarial
effectiveness, computational efficiency, and semantic fidelity with the
original textual prompt. These findings reveal critical vulnerabilities in
current unlearning mechanisms and underscore the need for more robust solutions
to ensure the safety and reliability of generative models. Code and data are
publicly available at \textcolor{blue}{https://github.com/ryliu68/RECALL}.

</details>


### [43] [Explainable Artificial Intelligence in Biomedical Image Analysis: A Comprehensive Survey](https://arxiv.org/abs/2507.07148)
*Getamesay Haile Dagnaw,Yanming Zhu,Muhammad Hassan Maqsood,Wencheng Yang,Xingshuai Dong,Xuefei Yin,Alan Wee-Chung Liew*

Main category: cs.CV

> 本文系统地分类并分析了适用于生物医学图像分析的XAI方法，提出了针对不同成像模态的分类方法，探讨了多模态学习和视觉语言模型在该领域的重要性，并讨论了持续的挑战和未来的研究方向。

<details>
  <summary>Details</summary>

**Motivation:** 本文旨在填补现有XAI技术综述中忽略的模态感知视角、多模态和视觉-语言范式的最近进展，以及提供有限的实际指导的空白。

**Method:** 本文通过系统地分类XAI方法，分析了这些方法的基本原理、优势和局限性，特别是在生物医学成像背景下的应用。此外，本文提出了以模态为中心的分类方法，将XAI方法与其特定的成像类型相结合，并特别关注多模态学习和视觉语言模型在可解释的生物医学AI中的作用，这是以前研究中较少探讨的主题。

**Result:** 通过对XAI方法的全面结构化综合，提出了一个以模态为中心的分类法，总结了广泛使用的评估指标和开源框架，讨论了现有挑战及未来方向。

**Conclusion:** 本文提供了一个及时且深入的基础，有助于推动生物医学图像分析中解释性深度学习的发展。

**Abstract:** Explainable artificial intelligence (XAI) has become increasingly important
in biomedical image analysis to promote transparency, trust, and clinical
adoption of DL models. While several surveys have reviewed XAI techniques, they
often lack a modality-aware perspective, overlook recent advances in multimodal
and vision-language paradigms, and provide limited practical guidance. This
survey addresses this gap through a comprehensive and structured synthesis of
XAI methods tailored to biomedical image analysis.We systematically categorize
XAI methods, analyzing their underlying principles, strengths, and limitations
within biomedical contexts. A modality-centered taxonomy is proposed to align
XAI methods with specific imaging types, highlighting the distinct
interpretability challenges across modalities. We further examine the emerging
role of multimodal learning and vision-language models in explainable
biomedical AI, a topic largely underexplored in previous work. Our
contributions also include a summary of widely used evaluation metrics and
open-source frameworks, along with a critical discussion of persistent
challenges and future directions. This survey offers a timely and in-depth
foundation for advancing interpretable DL in biomedical image analysis.

</details>


### [44] [Robust Multimodal Large Language Models Against Modality Conflict](https://arxiv.org/abs/2507.07151)
*Zongmeng Zhang,Wengang Zhou,Jie Zhao,Houqiang Li*

Main category: cs.CV

> 研究揭示了多模态大型语言模型中的模态冲突现象，提出了缓解措施，并通过实验验证了这些方法的有效性。

<details>
  <summary>Details</summary>

**Motivation:** 研究多模态大型语言模型在现实世界场景中由于模态冲突导致的幻象现象，探究输入来自不同模态的固有冲突如何使得模型陷入困境从而直接导致幻觉。

**Method:** 提出了基于提示工程、监督微调和强化学习的三种方法来缓解由模态冲突引起的幻象。

**Result:** 实验显示强化学习方法在缓解模态冲突条件下幻象方面表现最好，而监督微调方法表现出稳定且有潜力的表现。

**Conclusion:** 该研究为理解多模态大型语言模型中的模态冲突现象提供了新的视角，并为提高这些模型的鲁棒性提出了见解。

**Abstract:** Despite the impressive capabilities of multimodal large language models
(MLLMs) in vision-language tasks, they are prone to hallucinations in
real-world scenarios. This paper investigates the hallucination phenomenon in
MLLMs from the perspective of modality conflict. Unlike existing works focusing
on the conflicts between model responses and inputs, we study the inherent
conflicts in inputs from different modalities that place MLLMs in a dilemma and
directly lead to hallucinations. We formally define the modality conflict and
construct a dataset named Multimodal Modality Conflict (MMMC) to simulate this
phenomenon in vision-language tasks. Three methods based on prompt engineering,
supervised fine-tuning, and reinforcement learning are proposed to alleviate
the hallucination caused by modality conflict. Extensive experiments are
conducted on the MMMC dataset to analyze the merits and demerits of these
methods. Our results show that the reinforcement learning method achieves the
best performance in mitigating the hallucination under modality conflict, while
the supervised fine-tuning method shows promising and stable performance. Our
work sheds light on the unnoticed modality conflict that leads to
hallucinations and provides more insights into the robustness of MLLMs.

</details>


### [45] [Aerial Maritime Vessel Detection and Identification](https://arxiv.org/abs/2507.07153)
*Antonella Barisic Kulas,Frano Petric,Stjepan Bogdan*

Main category: cs.CV

> The paper discusses an autonomous maritime surveillance system using UAVs with on-board vision for target vessel identification in GNSS-denied environments. It uses YOLOv8 for vessel detection, followed by feature matching and hue histogram analysis for target identification, and evaluates the system's performance under various perspectives during the MBZIRC2023 competition.

<details>
  <summary>Details</summary>

**Motivation:** The motivation of the paper is to provide an autonomous solution for surveillance and target identification in areas where GPS signals are unreliable or unavailable, which is crucial for several applications including search and rescue and threat detection.

**Method:** The paper utilizes the YOLOv8 object detection model for identifying all vessels and uses additional techniques like feature matching and hue histogram analysis to confirm the target vessel. A geometric approach is then used to localize the target.

**Result:** The method is evaluated through real-world experiments during the MBZIRC2023 competition, demonstrating its capabilities in a fully autonomous system. The impact of perspective on detection accuracy and localization precision is also analyzed.

**Conclusion:** The proposed method effectively accomplishes target vessel identification and localization in GNSS-denied environments, showing promising results from its implementation in autonomous UAV systems tested in real-world conditions.

**Abstract:** Autonomous maritime surveillance and target vessel identification in
environments where Global Navigation Satellite Systems (GNSS) are not available
is critical for a number of applications such as search and rescue and threat
detection. When the target vessel is only described by visual cues and its last
known position is not available, unmanned aerial vehicles (UAVs) must rely
solely on on-board vision to scan a large search area under strict
computational constraints. To address this challenge, we leverage the YOLOv8
object detection model to detect all vessels in the field of view. We then
apply feature matching and hue histogram distance analysis to determine whether
any detected vessel corresponds to the target. When found, we localize the
target using simple geometric principles. We demonstrate the proposed method in
real-world experiments during the MBZIRC2023 competition, integrated into a
fully autonomous system with GNSS-denied navigation. We also evaluate the
impact of perspective on detection accuracy and localization precision and
compare it with the oracle approach.

</details>


### [46] [CL-Polyp: A Contrastive Learning-Enhanced Network for Accurate Polyp Segmentation](https://arxiv.org/abs/2507.07154)
*Desheng Li,Chaoliang Liu,Zhiyong Xiao*

Main category: cs.CV

> 本文提出了CL-Polyp，一种基于对比学习增强的息肉分割网络，通过对比正负样本对来提升编码器提取判别特征的能力，同时引入了MASPP和CA模块来改善多尺度特征融合和边界重建。实验表明，CL-Polyp在五个基准数据集上均优于现有方法。

<details>
  <summary>Details</summary>

**Motivation:** 现有的息肉分割方法依赖于辅助任务并需要额外标签，这限制了其在不同任务间的广泛应用性。

**Method:** 本文提出了CL-Polyp，利用对比学习来增强特征提取，引入了MASPP和CA模块，分别用于多尺度特征融合和边界重建。

**Result:** 在五个基准数据集（Kvasir-SEG, CVC-ClinicDB, CVC-ColonDB, CVC-300, ETIS）上的实验表明，CL-Polyp优于现有的方法，具体而言，在Kvasir-SEG和CVC-ClinicDB数据集上，IoU分别提高了0.011和0.020。

**Conclusion:** CL-Polyp作为对比学习增强的息肉分割模型，通过改进特征提取、多尺度特征融合和边界重建，提升了息肉分割的性能，对临床息肉分割任务有效。

**Abstract:** Accurate segmentation of polyps from colonoscopy images is crucial for the
early diagnosis and treatment of colorectal cancer. Most existing deep
learning-based polyp segmentation methods adopt an Encoder-Decoder
architecture, and some utilize multi-task frameworks that incorporate auxiliary
tasks such as classification to enhance segmentation performance. However,
these approaches often require additional labeled data and rely on task
similarity, which can limit their generalizability. To address these
challenges, we propose CL-Polyp, a contrastive learning-enhanced polyp
segmentation network. Our method leverages contrastive learning to improve the
encoder's ability to extract discriminative features by contrasting positive
and negative sample pairs derived from polyp images. This self-supervised
strategy enhances visual representation without requiring additional
annotations. In addition, we introduce two lightweight and effective modules:
the Modified Atrous Spatial Pyramid Pooling (MASPP) module for better
multi-scale feature fusion, and the Channel Concatenate and Element Add (CA)
module to fuse low-level and upsampled features for improved boundary
reconstruction. Extensive experiments on five benchmark datasets-Kvasir-SEG,
CVC-ClinicDB, CVC-ColonDB, CVC-300, and ETIS-demonstrate that CL-Polyp
consistently outperforms state-of-the-art methods. Specifically, it improves
the IoU metric by 0.011 and 0.020 on the Kvasir-SEG and CVC-ClinicDB datasets,
respectively, validating its effectiveness in clinical polyp segmentation
tasks.

</details>


### [47] [Interpretable EEG-to-Image Generation with Semantic Prompts](https://arxiv.org/abs/2507.07157)
*Arshak Rezvani,Ali Akbari,Kosar Sanjar Arani,Maryam Mirian,Emad Arasteh,Martin J. McKeown*

Main category: cs.CV

> 本文提出了一种使用文本中介框架进行认知对齐的视觉解码方法，该方法通过对EEG信号与多层级语义字幕的对齐实现了从EEG到图像的先进生成。

<details>
  <summary>Details</summary>

**Motivation:** 虽然EEG在时间和可访问性方面有优势，但其空间细节的局限性阻碍了图像重建。本文的方法通过绕过直接的EEG到图像生成，改进了从EEG信号中解码视觉体验的效果。

**Method:** 使用大型语言模型生成多层级语义字幕（从对象层面到抽象主题）来与EEG信号对齐，通过对比学习将脑活动映射到这些字幕，利用transformer为基础的EEG编码器。在推理过程中，通过投射头检索到的字幕嵌入条件化预训练的潜在扩散模型来生成图像。

**Result:** 该文本中介框架在EEGCVPR数据集上实现了最先进的视觉解码，揭示了不同的语义层级在感知图像中的重要性，并且通过显着性图和t-SNE投影展示了头皮上的语义地形。

**Conclusion:** 研究表明结构化的语义中介能更有效地实现从EEG到视觉的解码，揭示了神经认知路径的解释性对齐。

**Abstract:** Decoding visual experience from brain signals offers exciting possibilities
for neuroscience and interpretable AI. While EEG is accessible and temporally
precise, its limitations in spatial detail hinder image reconstruction. Our
model bypasses direct EEG-to-image generation by aligning EEG signals with
multilevel semantic captions -- ranging from object-level to abstract themes --
generated by a large language model. A transformer-based EEG encoder maps brain
activity to these captions through contrastive learning. During inference,
caption embeddings retrieved via projection heads condition a pretrained latent
diffusion model for image generation. This text-mediated framework yields
state-of-the-art visual decoding on the EEGCVPR dataset, with interpretable
alignment to known neurocognitive pathways. Dominant EEG-caption associations
reflected the importance of different semantic levels extracted from perceived
images. Saliency maps and t-SNE projections reveal semantic topography across
the scalp. Our model demonstrates how structured semantic mediation enables
cognitively aligned visual decoding from EEG.

</details>


### [48] [A Survey on Long-Video Storytelling Generation: Architectures, Consistency, and Cinematic Quality](https://arxiv.org/abs/2507.07202)
*Mohamed Elmoghany,Ryan Rossi,Seunghyun Yoon,Subhojyoti Mukherjee,Eslam Bakr,Puneet Mathur,Gang Wu,Viet Dac Lai,Nedim Lipka,Ruiyi Zhang,Varun Manjunatha,Chien Nguyen,Daksh Dangi,Abel Salinas,Mohammad Taesiri,Hongjie Chen,Xiaolei Huang,Joe Barrow,Nesreen Ahmed,Hoda Eldardiry,Namyong Park,Yu Wang,Jaemin Cho,Anh Totti Nguyen,Zhengzhong Tu,Thien Nguyen,Dinesh Manocha,Mohamed Elhoseiny,Franck Dernoncourt*

Main category: cs.CV

> 本文探讨了视频生成的关键架构和训练策略，并对现有方法进行了分类，以提高生成长视频的质量，解决帧冗余和时间多样性的挑战。

<details>
  <summary>Details</summary>

**Motivation:** 尽管视频生成模型取得了显著进展，但现有最先进的方法只能生成持续5-16秒的视频，而且长时间视频难以保持角色外观和场景布局的一致性，特别是多角色长视频更难以保持角色一致性和动作连贯性。此外，虽然一些方法能生成长达150秒的视频，但它们常常存在帧冗余和低时间多样性的问题。本文试图通过研究解决这些问题。

**Method:** 本文通过对32篇视频生成论文的全面研究，旨在识别出能够生成高质量长视频的关键架构组件和训练策略。此外，本文还构建了一个新的分类法，并给出了比较表格，对现有方法按其架构设计和性能特点进行分类。

**Result:** 无具体结果，因摘要仅描述了研究动机、方法和目标，并未展示具体研究结果。

**Conclusion:** 通过构建全面的论文分类法和比较表格，本文为未来长视频生成研究提供了宝贵的参考框架，指出了关键的架构组件和训练策略。

**Abstract:** Despite the significant progress that has been made in video generative
models, existing state-of-the-art methods can only produce videos lasting 5-16
seconds, often labeled "long-form videos". Furthermore, videos exceeding 16
seconds struggle to maintain consistent character appearances and scene layouts
throughout the narrative. In particular, multi-subject long videos still fail
to preserve character consistency and motion coherence. While some methods can
generate videos up to 150 seconds long, they often suffer from frame redundancy
and low temporal diversity. Recent work has attempted to produce long-form
videos featuring multiple characters, narrative coherence, and high-fidelity
detail. We comprehensively studied 32 papers on video generation to identify
key architectural components and training strategies that consistently yield
these qualities. We also construct a comprehensive novel taxonomy of existing
methods and present comparative tables that categorize papers by their
architectural designs and performance characteristics.

</details>


### [49] [Colors See Colors Ignore: Clothes Changing ReID with Color Disentanglement](https://arxiv.org/abs/2507.07230)
*Priyank Pathak,Yogesh S. Rawat*

Main category: cs.CV

> 提出一种以颜色为代理的服装变化重识别方法CSCI，该方法没有依赖额外的模型或标注，通过RGB信息直接从原始数据中分离出颜色和身份特征，显著提升了ReID性能。

<details>
  <summary>Details</summary>

**Motivation:** 解决现有的服装变化重识别方法依赖额外模型或标注，资源消耗大的问题。探索颜色作为对抗重识别模型外观偏差的轻量级、无标注代理。

**Method:** Colors See, Colors Ignore (CSCI), 使用RGB信息直接从原始图像或视频帧中捕获颜色相关外观偏差，同时隔离与身份相关的重识别特征。为防止特征空间中颜色和身份特征之间的信息泄露，引入了S2A自注意力机制。

**Result:** 在四个服装变化重识别数据集上进行了广泛的实验，证明了CSCI的有效性，相比于基准，在图像基重识别上提高了LTCC数据集Top-1 2.9%，PRCC数据集Top-1 5.0%，在视频基重识别上提高了CCVID数据集1.0%，MeVID数据集2.5%，且无需依赖额外的监督。

**Conclusion:** 证明了颜色作为当显式服装标签不可用时的有效代理，并展示了颜色作为一种成本效益高的解决方案，在处理CC-ReID中的外观偏差方面的潜力。

**Abstract:** Clothes-Changing Re-Identification (CC-ReID) aims to recognize individuals
across different locations and times, irrespective of clothing. Existing
methods often rely on additional models or annotations to learn robust,
clothing-invariant features, making them resource-intensive. In contrast, we
explore the use of color - specifically foreground and background colors - as a
lightweight, annotation-free proxy for mitigating appearance bias in ReID
models. We propose Colors See, Colors Ignore (CSCI), an RGB-only method that
leverages color information directly from raw images or video frames. CSCI
efficiently captures color-related appearance bias ('Color See') while
disentangling it from identity-relevant ReID features ('Color Ignore'). To
achieve this, we introduce S2A self-attention, a novel self-attention to
prevent information leak between color and identity cues within the feature
space. Our analysis shows a strong correspondence between learned color
embeddings and clothing attributes, validating color as an effective proxy when
explicit clothing labels are unavailable. We demonstrate the effectiveness of
CSCI on both image and video ReID with extensive experiments on four CC-ReID
datasets. We improve the baseline by Top-1 2.9% on LTCC and 5.0% on PRCC for
image-based ReID, and 1.0% on CCVID and 2.5% on MeVID for video-based ReID
without relying on additional supervision. Our results highlight the potential
of color as a cost-effective solution for addressing appearance bias in
CC-ReID. Github: https://github.com/ppriyank/ICCV-CSCI-Person-ReID.

</details>


### [50] [Automated Video Segmentation Machine Learning Pipeline](https://arxiv.org/abs/2507.07242)
*Johannes Merz,Lucien Fostier*

Main category: cs.CV

> 论文描述了一种自动化的视频分割管道，涉及到机器学习、容器化部署和时间一致的实例遮罩，显著提升了VFX生产的效率。

<details>
  <summary>Details</summary>

**Motivation:** 视觉效果（VFX）制作过程中，通常会遇到生成蒙版过程缓慢且资源消耗大的问题。本论文就是为了解决这些问题提出的。

**Method:** 自动化的视频分割管道采用机器学习技术来创建时间一致的实例蒙版。具体使用了三个关键步骤：（1）通过文本提示灵活的对象检测，（2）每帧图像的精确分割，（3）确保时间稳定性的鲁棒视频跟踪。部署使用了容器化技术，并采用了结构化的输出格式。

**Result:** 该管道被艺术家迅速采纳，显著减少了人工工作量，加速了初步合成的创建，并提供了全面的分割数据，从而提高了整体VFX生产的效率。

**Conclusion:** 提出的方法有效地解决了VFX制作中面临的慢速和耗资源的问题，通过自动化的视频分割管道，提高了生产和制作效率。

**Abstract:** Visual effects (VFX) production often struggles with slow, resource-intensive
mask generation. This paper presents an automated video segmentation pipeline
that creates temporally consistent instance masks. It employs machine learning
for: (1) flexible object detection via text prompts, (2) refined per-frame
image segmentation and (3) robust video tracking to ensure temporal stability.
Deployed using containerization and leveraging a structured output format, the
pipeline was quickly adopted by our artists. It significantly reduces manual
effort, speeds up the creation of preliminary composites, and provides
comprehensive segmentation data, thereby enhancing overall VFX production
efficiency.

</details>


### [51] [DisenQ: Disentangling Q-Former for Activity-Biometrics](https://arxiv.org/abs/2507.07262)
*Shehreen Azad,Yogesh S Rawat*

Main category: cs.CV

> 本文提出DisenQ框架，通过结构化的语言指导来解缠特征，实现了更准确的活动生物特征识别，并在三个视频基准测试中达到了最先进的性能。

<details>
  <summary>Details</summary>

**Motivation:** 本文旨在解决活动生物特征识别问题，即在不同活动下识别个体，这比传统的个人识别更加复杂，因为身份线索与运动动力学和外观变化交织在一起。

**Method:** 本文提出了一种多模态语言引导框架，使用结构化的文本监督代替依赖额外的视觉数据。核心是一个名为DisenQ（解缠Q-Former）的统一查询变压器，它通过结构化的语言指导来解缠生物特征、运动和非生物特征特征，确保身份线索独立于外观和运动变化，从而防止误识别。

**Result:** 实验结果表明，该框架在三个基于活动的视频基准测试中获得了最先进的性能，并且在传统视频识别基准测试中展示了良好的泛化能力。

**Conclusion:** 该研究证明了所提出的框架在活动生物特征识别任务中的有效性，并展示了其实现身份线索独立于外观和运动变化的能力。

**Abstract:** In this work, we address activity-biometrics, which involves identifying
individuals across diverse set of activities. Unlike traditional person
identification, this setting introduces additional challenges as identity cues
become entangled with motion dynamics and appearance variations, making
biometrics feature learning more complex. While additional visual data like
pose and/or silhouette help, they often struggle from extraction inaccuracies.
To overcome this, we propose a multimodal language-guided framework that
replaces reliance on additional visual data with structured textual
supervision. At its core, we introduce \textbf{DisenQ} (\textbf{Disen}tangling
\textbf{Q}-Former), a unified querying transformer that disentangles
biometrics, motion, and non-biometrics features by leveraging structured
language guidance. This ensures identity cues remain independent of appearance
and motion variations, preventing misidentifications. We evaluate our approach
on three activity-based video benchmarks, achieving state-of-the-art
performance. Additionally, we demonstrate strong generalization to complex
real-world scenario with competitive performance on a traditional video-based
identification benchmark, showing the effectiveness of our framework.

</details>


### [52] [LinguaMark: Do Multimodal Models Speak Fairly? A Benchmark-Based Evaluation](https://arxiv.org/abs/2507.07274)
*Ananya Raval,Aravind Narayanan,Vahid Reza Khazaie,Shaina Raza*

Main category: cs.CV

> 研究提出了LinguaMark基准，用于评估大模型在多语言视觉问答任务上的表现，发现闭源模型性能最高，开源模型也有较强的推广能力。

<details>
  <summary>Details</summary>

**Motivation:** 尽管大型多模态模型（LMMs）通常是在大量的图像-文本数据上训练的，但它们在语言覆盖范围上往往有限，导致在不同语言间的输出存在偏见和不公平。之前的工作探索了多模态评估，但很少重视评估多语言能力。

**Method:** 本研究介绍了LinguaMark基准测试，旨在评估最先进的大模型在多语言视觉问答任务上的表现。该数据集包含6875个图像-文本对，涵盖了11种语言和五个社会属性。使用三个关键指标（偏见、答案相关性和忠实性）评估模型表现。

**Result:** 研究发现闭源模型（GPT-4o和Gemini2.5）通常在整体性能上表现最佳，开源模型（Gemma3，Qwen2.5）在社会属性上也表现得很有竞争力，特别是Qwen2.5在多语言上有较强的泛化能力。

**Conclusion:** 发布了基准测试和评估代码，鼓励可重复性和进一步研究。

**Abstract:** Large Multimodal Models (LMMs) are typically trained on vast corpora of
image-text data but are often limited in linguistic coverage, leading to biased
and unfair outputs across languages. While prior work has explored multimodal
evaluation, less emphasis has been placed on assessing multilingual
capabilities. In this work, we introduce LinguaMark, a benchmark designed to
evaluate state-of-the-art LMMs on a multilingual Visual Question Answering
(VQA) task. Our dataset comprises 6,875 image-text pairs spanning 11 languages
and five social attributes. We evaluate models using three key metrics: Bias,
Answer Relevancy, and Faithfulness. Our findings reveal that closed-source
models generally achieve the highest overall performance. Both closed-source
(GPT-4o and Gemini2.5) and open-source models (Gemma3, Qwen2.5) perform
competitively across social attributes, and Qwen2.5 demonstrates strong
generalization across multiple languages. We release our benchmark and
evaluation code to encourage reproducibility and further research.

</details>


### [53] [MagiC: Evaluating Multimodal Cognition Toward Grounded Visual Reasoning](https://arxiv.org/abs/2507.07297)
*Chengfei Wu,Ronald Seoh,Bingxuan Li,Liqiang Zhang,Fengrong Han,Dan Goldwasser*

Main category: cs.CV

> MagiC是一个新的评估基准，用于评估视觉语言模型的视觉推理水平，揭示了这些模型在推理准确性、定位和自我纠正能力上的限制。

<details>
  <summary>Details</summary>

**Motivation:** 动机在于评估当前视觉语言模型是否真正进行了基于视觉证据的推理，而非依赖浅层模式和数据偏差。

**Method:** 通过介绍MagiC这一全面评估基准，来测试视觉语言模型的视觉推理能力。该基准包含约5,500个来自强模型输出的弱监督问答示例和900个人类策划的例子，其中包含答案、理由和边界框标注。

**Result:** 评估了15个视觉语言模型在最终答案准确性、推理有效性、定位准确性以及自我纠正能力上的表现。

**Conclusion:** 引入了新的评估指标，如MagiScore和StepSense，揭示了现有方法在基于视觉推理上的关键限制和机会。

**Abstract:** Recent advances in large vision-language models have led to impressive
performance in visual question answering and multimodal reasoning. However, it
remains unclear whether these models genuinely perform grounded visual
reasoning or rely on superficial patterns and dataset biases. In this work, we
introduce MagiC, a comprehensive benchmark designed to evaluate grounded
multimodal cognition, assessing not only answer accuracy but also the quality
of step-by-step reasoning and its alignment with relevant visual evidence. Our
benchmark includes approximately 5,500 weakly supervised QA examples generated
from strong model outputs and 900 human-curated examples with fine-grained
annotations, including answers, rationales, and bounding box groundings. We
evaluate 15 vision-language models ranging from 7B to 70B parameters across
four dimensions: final answer correctness, reasoning validity, grounding
fidelity, and self-correction ability. MagiC further includes diagnostic
settings to probe model robustness under adversarial visual cues and assess
their capacity for introspective error correction. We introduce new metrics
such as MagiScore and StepSense, and provide comprehensive analyses that reveal
key limitations and opportunities in current approaches to grounded visual
reasoning.

</details>


### [54] [ADIEE: Automatic Dataset Creation and Scorer for Instruction-Guided Image Editing Evaluation](https://arxiv.org/abs/2507.07317)
*Sherry X. Chen,Yi Wei,Luowei Zhou,Suren Kumar*

Main category: cs.CV

> 提出ADIEE，一种用于指导图像编辑评价的自动化数据集创建方法，生成大样本数据集训练LLaVA-NeXT-8B模型，该评分器在各种基准测试中超越现有模型，显著提高了相应的评分相关性和准确率。

<details>
  <summary>Details</summary>

**Motivation:** 为解决图像编辑指导中自动评估的有效性问题，现有的开源VLM模型难以对齐，专有模型透明度低且成本效益差，缺乏可训练数据集。

**Method:** 引入ADIEE，一种自动数据集创建方法，用于训练图像编辑评价评分模型。用超过10万条样本训练LLaVA-NeXT-8B模型，并修改它从自定义标记中解码出数值分数。

**Result:** 评分器超越了所有开源VLMs和Gemini-Pro 1.5，与人类评估评分的相关性提高了17.24%，在GenAI-Bench和AURORA-Bench基准测试中提高了4.03%和4.75%的配对比较准确率。

**Conclusion:** 该评分器在AURORA-Bench和GenAI-Bench基准测试中的分数与人类评价的相关性提高了17.24%和7.21%，并提高了MagicBrush模型在ImagenHub上的平均评估分数8.98%。

**Abstract:** Recent advances in instruction-guided image editing underscore the need for
effective automated evaluation. While Vision-Language Models (VLMs) have been
explored as judges, open-source models struggle with alignment, and proprietary
models lack transparency and cost efficiency. Additionally, no public training
datasets exist to fine-tune open-source VLMs, only small benchmarks with
diverse evaluation schemes. To address this, we introduce ADIEE, an automated
dataset creation approach which is then used to train a scoring model for
instruction-guided image editing evaluation. We generate a large-scale dataset
with over 100K samples and use it to fine-tune a LLaVA-NeXT-8B model modified
to decode a numeric score from a custom token. The resulting scorer outperforms
all open-source VLMs and Gemini-Pro 1.5 across all benchmarks, achieving a
0.0696 (+17.24%) gain in score correlation with human ratings on AURORA-Bench,
and improving pair-wise comparison accuracy by 4.03% (+7.21%) on GenAI-Bench
and 4.75% (+9.35%) on AURORA-Bench, respectively, compared to the
state-of-the-art. The scorer can act as a reward model, enabling automated best
edit selection and model fine-tuning. Notably, the proposed scorer can boost
MagicBrush model's average evaluation score on ImagenHub from 5.90 to 6.43
(+8.98%).

</details>


### [55] [Scalable and Realistic Virtual Try-on Application for Foundation Makeup with Kubelka-Munk Theory](https://arxiv.org/abs/2507.07333)
*Hui Pang,Sunil Hadap,Violetta Shevchenko,Rahul Suresh,Amin Banitalebi-Dehkordi*

Main category: cs.CV

> 研究开发了一种新的方法来提高AR底妆虚拟试用应用的图像合成速度，同时保持颜色融合的真实感，并验证了其有效性。

<details>
  <summary>Details</summary>

**Motivation:** 鉴于AR技术在美容行业中的应用，尤其是在底妆虚拟试用中的应用，本研究旨在解决快速、准确地合成底妆与肤色色调融合的技术难题，同时确保方法的可扩展性。

**Method:** 提出了一种近似Kubelka-Munk理论的新方法，用于加快图像合成速度，同时保持底妆与肤色的自然融合。此外，我们构建了一个仅依赖电商平台产品信息的实际底妆虚拟试用的端到端框架。

**Result:** 通过使用真实妆容图像进行了验证，该框架在底妆虚拟试用方面优于其他技术。

**Conclusion:** 本研究提出的方法和框架，为美妆行业提供了更快速、更真实的底妆虚拟试用解决方案。

**Abstract:** Augmented reality is revolutionizing beauty industry with virtual try-on
(VTO) applications, which empowers users to try a wide variety of products
using their phones without the hassle of physically putting on real products. A
critical technical challenge in foundation VTO applications is the accurate
synthesis of foundation-skin tone color blending while maintaining the
scalability of the method across diverse product ranges. In this work, we
propose a novel method to approximate well-established Kubelka-Munk (KM) theory
for faster image synthesis while preserving foundation-skin tone color blending
realism. Additionally, we build a scalable end-to-end framework for realistic
foundation makeup VTO solely depending on the product information available on
e-commerce sites. We validate our method using real-world makeup images,
demonstrating that our framework outperforms other techniques.

</details>


### [56] [Entity Re-identification in Visual Storytelling via Contrastive Reinforcement Learning](https://arxiv.org/abs/2507.07340)
*Daniel A. P. Oliveira,David Martins de Matos*

Main category: cs.CV

> 本研究通过对比强化学习方法解决视觉叙事系统中实体根植和重新识别的挑战，提升了代词根植准确性和故事构建的连贯性。

<details>
  <summary>Details</summary>

**Motivation:** 视觉叙事系统，尤其是在大型视觉语言模型中，难以在不同帧间维护角色和对象的身份，这导致了不一致的引用和实体引用幻觉。

**Method:** 提出了一种对比强化学习方法，该方法训练模型区分连贯的图像序列和故事与不相关的图像，并扩展了Story Reasoning数据集，加入了合成的负面例子来教授适当的实体连接行为。采用直接偏好优化，奖励函数由两个部分组成，一个促进现实故事中实体的根植和重新识别，另一个则在合成上下文中对错误的实体连接进行惩罚。

**Result:** 通过这种对比框架，微调了Qwen Storyteller（基于Qwen2.5-VL 7B）。评估显示在实体根植mAP从0.27提升到0.31（+14.8%），F1从0.35提升到0.41（+17.1%）。代词根植准确性在所有代词类型中均有提升，除了“its”之外。跨帧角色和对象的持久性在所有帧数中均有增加，特别是在5帧或更多的实体从29.3%提升到33.3%（+13.7%）。结构良好的故事（包括连贯的思想链和根植的故事）从79.1%提升到97.5%（+23.3%）。

**Conclusion:** 研究结果表明，通过引入合成负面例子和双组件奖励函数来改进训练框架后，模型在实体根植、代词根植和故事构建中的性能显著提升。

**Abstract:** Visual storytelling systems, particularly large vision-language models,
struggle to maintain character and object identity across frames,
  often failing to recognize when entities in different images represent the
same individuals or objects,
  leading to inconsistent references and referential hallucinations.
  This occurs because models lack explicit training on when to establish entity
connections across frames.
  We propose a contrastive reinforcement learning approach that trains models
to discriminate between coherent image sequences
  and stories from unrelated images.
  We extend the Story Reasoning dataset with synthetic negative examples to
teach appropriate entity connection behavior.
  We employ Direct Preference Optimization with a dual-component reward
function that promotes grounding and re-identification of entities
  in real stories while penalizing incorrect entity connections in synthetic
contexts.
  Using this contrastive framework, we fine-tune Qwen Storyteller (based on
Qwen2.5-VL 7B).
  Evaluation shows improvements in grounding mAP from 0.27 to 0.31 (+14.8%), F1
from 0.35 to 0.41 (+17.1%).
  Pronoun grounding accuracy improved across all pronoun types except ``its'',
  and cross-frame character and object persistence increased
  across all frame counts, with entities appearing in 5 or more frames
advancing from 29.3% to 33.3% (+13.7%).
  Well-structured stories, containing the chain-of-thought and grounded story,
increased from 79.1% to 97.5% (+23.3%).

</details>


### [57] [PacGDC: Label-Efficient Generalizable Depth Completion with Projection Ambiguity and Consistency](https://arxiv.org/abs/2507.07374)
*Haotian Wang,Aoran Xiao,Xiaoqin Zhang,Meng Yang,Shijian Lu*

Main category: cs.CV

> 本文介绍了PacGDC，一种能有效提升数据多样性并减少标注工作的深度补全技术，通过尺度操纵及多种合成方法，实现了在不同场景下的优秀泛化效果。

<details>
  <summary>Details</summary>

**Motivation:** 针对通用深度补全模型通常需要大量带有深度标签的数据集且这些数据集的收集过程费时费力的问题，本研究提出了一种标签高效的技术来增强数据多样性并减少标注工作量。

**Method:** 通过利用场景中物体形状和位置在2D到3D投影过程中的内在不确定性和一致性，PacGDC能合成同一视觉场景下的多种伪几何数据，大大丰富了可用的几何数据。此外，通过多个深度基础模型进行尺度操纵，结合插值和重新定位策略以及未标注图像，来进一步提高几何多样性。

**Result:** 实验结果表明，PacGDC在多个基准测试中展现出良好的泛化能力，在不同的场景语义/尺度以及深度稀疏/模式下均表现出色，不论是零样本还是少样本设置。

**Conclusion:** PacGDC作为一种标签高效的深度补全技术，通过引入尺度操纵策略以及多种数据合成方法，能在减少标注成本的同时提升模型的泛化性能。

**Abstract:** Generalizable depth completion enables the acquisition of dense metric depth
maps for unseen environments, offering robust perception capabilities for
various downstream tasks. However, training such models typically requires
large-scale datasets with metric depth labels, which are often labor-intensive
to collect. This paper presents PacGDC, a label-efficient technique that
enhances data diversity with minimal annotation effort for generalizable depth
completion. PacGDC builds on novel insights into inherent ambiguities and
consistencies in object shapes and positions during 2D-to-3D projection,
allowing the synthesis of numerous pseudo geometries for the same visual scene.
This process greatly broadens available geometries by manipulating scene scales
of the corresponding depth maps. To leverage this property, we propose a new
data synthesis pipeline that uses multiple depth foundation models as scale
manipulators. These models robustly provide pseudo depth labels with varied
scene scales, affecting both local objects and global layouts, while ensuring
projection consistency that supports generalization. To further diversify
geometries, we incorporate interpolation and relocation strategies, as well as
unlabeled images, extending the data coverage beyond the individual use of
foundation models. Extensive experiments show that PacGDC achieves remarkable
generalizability across multiple benchmarks, excelling in diverse scene
semantics/scales and depth sparsity/patterns under both zero-shot and few-shot
settings. Code: https://github.com/Wang-xjtu/PacGDC.

</details>


### [58] [Adaptive Particle-Based Shape Modeling for Anatomical Surface Correspondence](https://arxiv.org/abs/2507.07379)
*Hong Xu,Shireen Y. Elhabian*

Main category: cs.CV

> 本文提出了两种机制以增强粒子配置对表面的适应性，并在保持一致性的同时提升局部几何特征的适应能力。

<details>
  <summary>Details</summary>

**Motivation:** 现有的基于粒子的形状建模方法虽然在捕捉解剖结构的复杂几何特征上取得了进展，但仍缺乏自适应性，即自动调整粒子配置以适应每个表面局部几何特征的能力，这对准确表示复杂的解剖变异至关重要。

**Method:** 引入了两种机制来增强表面适应性同时保持粒子配置的一致性：(1) 一种新颖的邻域对应损失，以实现高适应性；(2) 一种测地线对应算法，用于在优化过程中强制执行测地线邻域一致性。

**Result:** 在具有挑战性的数据集上评估了所提方法的效果和可扩展性，详细分析了适应性-对应权衡，并与现有方法在表面表示准确性和对应度量上进行了基准测试。

**Conclusion:** 本文的方法对复杂的表面表示准确性以及对应度量进行了基准测试，展示了其在适应性-对应权衡中的出色表现。

**Abstract:** Particle-based shape modeling (PSM) is a family of approaches that
automatically quantifies shape variability across anatomical cohorts by
positioning particles (pseudo landmarks) on shape surfaces in a consistent
configuration. Recent advances incorporate implicit radial basis function
representations as self-supervised signals to better capture the complex
geometric properties of anatomical structures. However, these methods still
lack self-adaptivity -- that is, the ability to automatically adjust particle
configurations to local geometric features of each surface, which is essential
for accurately representing complex anatomical variability. This paper
introduces two mechanisms to increase surface adaptivity while maintaining
consistent particle configurations: (1) a novel neighborhood correspondence
loss to enable high adaptivity and (2) a geodesic correspondence algorithm that
regularizes optimization to enforce geodesic neighborhood consistency. We
evaluate the efficacy and scalability of our approach on challenging datasets,
providing a detailed analysis of the adaptivity-correspondence trade-off and
benchmarking against existing methods on surface representation accuracy and
correspondence metrics.

</details>


### [59] [Multi-Scale Attention and Gated Shifting for Fine-Grained Event Spotting in Videos](https://arxiv.org/abs/2507.07381)
*Hao Xu,Arbind Agrahari Baniya,Sam Wells,Mohamed Reda Bouadjenek,Richard Dazeley,Sunil Aryal*

Main category: cs.CV

> 本文提出了一种多尺度注意力门移模块（MSAGSM）用于增强运动视频中细粒度动作识别的精确事件捕捉，引入了首个乒乓球精确事件捕捉基准（Table Tennis Australia，TTA数据集），在多个基准测试中表现优异，提高了性能并降低了计算成本。

<details>
  <summary>Details</summary>

**Motivation:** 现有的精确事件捕捉（PES）模型在处理运动视频中的细粒度动作识别时，使用轻量级时间模块（例如GSM或GSF）来增强2D卷积神经网络特征提取器。然而，这些模块在时间接收域和空间适应性方面有限。为了解决这些问题，我们提出了MSAGSM。

**Method:** 我们提出了一种多尺度注意力门移模块（MSAGSM），该模块通过引入多尺度时间膨胀和多头空间注意力来增强现有门移模块，以更有效地建模短期和长期的时间依赖性并聚焦于显著区域。MSAGSM是一个轻量级的可插拔模块，可以与各种2D骨干网络轻松集成。

**Result:** 我们引入了首个乒乓球精确事件捕捉基准表（Table Tennis Australia，TTA），该数据集包含超过4800个精细标注的事件。在五个PES基准上的广泛实验表明，MSAGSM在性能提升上表现一致，并且只需极小的计算成本，刷新了最新的技术水平。

**Conclusion:** MSAGSM通过多尺度时间膨胀和空间注意力机制提高了时间依赖性的建模效率，与多种2D骨干网络兼容，展示了其在改善PES性能上的应用潜力和优势。

**Abstract:** Precise Event Spotting (PES) in sports videos requires frame-level
recognition of fine-grained actions from single-camera footage. Existing PES
models typically incorporate lightweight temporal modules such as Gate Shift
Module (GSM) or Gate Shift Fuse (GSF) to enrich 2D CNN feature extractors with
temporal context. However, these modules are limited in both temporal receptive
field and spatial adaptability. We propose a Multi-Scale Attention Gate Shift
Module (MSAGSM) that enhances GSM with multi-scale temporal dilations and
multi-head spatial attention, enabling efficient modeling of both short- and
long-term dependencies while focusing on salient regions. MSAGSM is a
lightweight plug-and-play module that can be easily integrated with various 2D
backbones. To further advance the field, we introduce the Table Tennis
Australia (TTA) dataset-the first PES benchmark for table tennis-containing
over 4800 precisely annotated events. Extensive experiments across five PES
benchmarks demonstrate that MSAGSM consistently improves performance with
minimal overhead, setting new state-of-the-art results.

</details>


### [60] [KeyRe-ID: Keypoint-Guided Person Re-Identification using Part-Aware Representation in Videos](https://arxiv.org/abs/2507.07393)
*Jinseong Kim,Junghoon Song,Gyeongseon Baek,Byeongjoon Noh*

Main category: cs.CV

> 提出了KeyRe-ID，一个基于关键点的视频人物重识别框架，在多个基准测试中显示了优异性能。

<details>
  <summary>Details</summary>

**Motivation:** 视频人物重识别中，现有方法在处理复杂的遮挡、视角变化和光照条件下表现不佳，因此提出了结合关键点和Transformer技术的框架以改善时空表示学习。

**Method:** 我们提出了KeyRe-ID，一个基于关键点引导的视频人物重识别框架，包括全局分支和局部分支，利用人体关键点进行增强的时空表示学习。全局分支通过基于Transformer的时间聚合来捕捉整体身份语义，而局部分支则基于关键点动态分割身体区域，生成细粒度、部分感知的特征。

**Result:** 在MARS和iLIDS-VID基准测试上的广泛实验展示了该方法的先进性能。在MARS上达到了91.73% mAP和97.32% Rank-1准确率，在iLIDS-VID上达到了96.00% Rank-1和100.0% Rank-5准确率。

**Conclusion:** 实验显示了KeyRe-ID框架在处理视频人物重识别任务中的有效性，尤其是通过关键点和Transformer来增强时空特征。该方法实现了在MARS和iLIDS-VID基准上的最优性能。

**Abstract:** We propose \textbf{KeyRe-ID}, a keypoint-guided video-based person
re-identification framework consisting of global and local branches that
leverage human keypoints for enhanced spatiotemporal representation learning.
The global branch captures holistic identity semantics through
Transformer-based temporal aggregation, while the local branch dynamically
segments body regions based on keypoints to generate fine-grained, part-aware
features. Extensive experiments on MARS and iLIDS-VID benchmarks demonstrate
state-of-the-art performance, achieving 91.73\% mAP and 97.32\% Rank-1 accuracy
on MARS, and 96.00\% Rank-1 and 100.0\% Rank-5 accuracy on iLIDS-VID. The code
for this work will be publicly available on GitHub upon publication.

</details>


### [61] [Behave Your Motion: Habit-preserved Cross-category Animal Motion Transfer](https://arxiv.org/abs/2507.07394)
*Zhimin Zhang,Bi'an Du,Caoyuan Ma,Zheng Wang,Wei Hu*

Main category: cs.CV

> Error

<details>
  <summary>Details</summary>

**Motivation:** Error

**Method:** Error

**Result:** Error

**Conclusion:** Error

**Abstract:** Animal motion embodies species-specific behavioral habits, making the
transfer of motion across categories a critical yet complex task for
applications in animation and virtual reality. Existing motion transfer
methods, primarily focused on human motion, emphasize skeletal alignment
(motion retargeting) or stylistic consistency (motion style transfer), often
neglecting the preservation of distinct habitual behaviors in animals. To
bridge this gap, we propose a novel habit-preserved motion transfer framework
for cross-category animal motion. Built upon a generative framework, our model
introduces a habit-preservation module with category-specific habit encoder,
allowing it to learn motion priors that capture distinctive habitual
characteristics. Furthermore, we integrate a large language model (LLM) to
facilitate the motion transfer to previously unobserved species. To evaluate
the effectiveness of our approach, we introduce the DeformingThings4D-skl
dataset, a quadruped dataset with skeletal bindings, and conduct extensive
experiments and quantitative analyses, which validate the superiority of our
proposed model.

</details>


### [62] [Seg-Wild: Interactive Segmentation based on 3D Gaussian Splatting for Unconstrained Image Collections](https://arxiv.org/abs/2507.07395)
*Yongtang Bao,Chengjie Tang,Yuze Wang,Haojie Li*

Main category: cs.CV

> Error

<details>
  <summary>Details</summary>

**Motivation:** Error

**Method:** Error

**Result:** Error

**Conclusion:** Error

**Abstract:** Reconstructing and segmenting scenes from unconstrained photo collections
obtained from the Internet is a novel but challenging task. Unconstrained photo
collections are easier to get than well-captured photo collections. These
unconstrained images suffer from inconsistent lighting and transient
occlusions, which makes segmentation challenging. Previous segmentation methods
cannot address transient occlusions or accurately restore the scene's lighting
conditions. Therefore, we propose Seg-Wild, an interactive segmentation method
based on 3D Gaussian Splatting for unconstrained image collections, suitable
for in-the-wild scenes. We integrate multi-dimensional feature embeddings for
each 3D Gaussian and calculate the feature similarity between the feature
embeddings and the segmentation target to achieve interactive segmentation in
the 3D scene. Additionally, we introduce the Spiky 3D Gaussian Cutter (SGC) to
smooth abnormal 3D Gaussians. We project the 3D Gaussians onto a 2D plane and
calculate the ratio of 3D Gaussians that need to be cut using the SAM mask. We
also designed a benchmark to evaluate segmentation quality in in-the-wild
scenes. Experimental results demonstrate that compared to previous methods,
Seg-Wild achieves better segmentation results and reconstruction quality. Our
code will be available at https://github.com/Sugar0725/Seg-Wild.

</details>


### [63] [EscherNet++: Simultaneous Amodal Completion and Scalable View Synthesis through Masked Fine-Tuning and Enhanced Feed-Forward 3D Reconstruction](https://arxiv.org/abs/2507.07410)
*Xinan Zhang,Muhammad Zubair Irshad,Anthony Yezzi,Yi-Chang Tsai,Zsolt Kira*

Main category: cs.CV

> 本文提出了一种名为EscherNet++的新模型，该模型能够在零样本方式下合成新颖视角的物体。

<details>
  <summary>Details</summary>

**Motivation:** 已有方法通过多个阶段和复杂管道来先排除图像的缺失部分，然后进行新颖视角合成，忽视了跨视角依赖问题，且需要额外的存储和计算。

**Method:** 该研究采用了带mask的微调方法，包括输入级和特征级mask，使模型具备端到端合成新颖视角和进行无模态补全的能力。

**Result:** 该方法在遮挡任务中，PSNR提升了3.9，Volume IoU提升了0.28，在10输入设置下达到最先进的结果，并且能够泛化到现实世界的遮挡重建。

**Conclusion:** EscherNet++展示了在零样本方式下合成新颖视角物体的强大能力，并且通过减少95%的重建时间提高了效率。

**Abstract:** We propose EscherNet++, a masked fine-tuned diffusion model that can
synthesize novel views of objects in a zero-shot manner with amodal completion
ability. Existing approaches utilize multiple stages and complex pipelines to
first hallucinate missing parts of the image and then perform novel view
synthesis, which fail to consider cross-view dependencies and require redundant
storage and computing for separate stages. Instead, we apply masked fine-tuning
including input-level and feature-level masking to enable an end-to-end model
with the improved ability to synthesize novel views and conduct amodal
completion. In addition, we empirically integrate our model with other
feed-forward image-to-mesh models without extra training and achieve
competitive results with reconstruction time decreased by 95%, thanks to its
ability to synthesize arbitrary query views. Our method's scalable nature
further enhances fast 3D reconstruction. Despite fine-tuning on a smaller
dataset and batch size, our method achieves state-of-the-art results, improving
PSNR by 3.9 and Volume IoU by 0.28 on occluded tasks in 10-input settings,
while also generalizing to real-world occluded reconstruction.

</details>


### [64] [EPIC: Efficient Prompt Interaction for Text-Image Classification](https://arxiv.org/abs/2507.07415)
*Xinyao Yu,Hao Sun,Zeyu Ling,Ziwei Niu,Zhenjia Bai,Rui Qin,Yen-Wei Chen,Lanfen Lin*

Main category: cs.CV

> 提出了一种新的高效提示方式，用于文本图像分类任务，与其它微调策略相比，该方法计算资源消耗更少，参数量更少，同时在某些数据集上表现更优。

<details>
  <summary>Details</summary>

**Motivation:** 大型预训练多模态模型在多模态任务上取得了显著成功，但其量级的增长导致了微调这些模型的计算成本增加。因此，研究了基于提示的交互策略，以更高效地对齐模态。

**Method:** 提出了名为Efficient Prompt Interaction for text-image Classification（EPIC）的新型高效提示方式。通过在中间层利用时间提示，集成基于相似度的提示交互来优化不同模态的信息交换，减少计算资源消耗和可训练参数。

**Result:** 相较于其它微调策略，该方法减少了计算资源消费和可训练参数，并在UPMC-Food101和SNLI-VE数据集上表现出色，同时在MM-IMDB数据集上表现出相当的性能。

**Conclusion:** 该方法通过集成基于相似度的提示交互，有效减少了计算资源消耗和参数量，同时改善了在特定数据集上的性能表现，为解决大型预训练多模态模型在微调时的计算成本提供了有效方案。

**Abstract:** In recent years, large-scale pre-trained multimodal models (LMMs) generally
emerge to integrate the vision and language modalities, achieving considerable
success in multimodal tasks, such as text-image classification. The growing
size of LMMs, however, results in a significant computational cost for
fine-tuning these models for downstream tasks. Hence, prompt-based interaction
strategy is studied to align modalities more efficiently. In this context, we
propose a novel efficient prompt-based multimodal interaction strategy, namely
Efficient Prompt Interaction for text-image Classification (EPIC).
Specifically, we utilize temporal prompts on intermediate layers, and integrate
different modalities with similarity-based prompt interaction, to leverage
sufficient information exchange between modalities. Utilizing this approach,
our method achieves reduced computational resource consumption and fewer
trainable parameters (about 1\% of the foundation model) compared to other
fine-tuning strategies. Furthermore, it demonstrates superior performance on
the UPMC-Food101 and SNLI-VE datasets, while achieving comparable performance
on the MM-IMDB dataset.

</details>


### [65] [Corvid: Improving Multimodal Large Language Models Towards Chain-of-Thought Reasoning](https://arxiv.org/abs/2507.07424)
*Jingjing Jiang,Chao Ma,Xurui Song,Hanwang Zhang,Jun Luo*

Main category: cs.CV

> 本文介绍了Corvid，一种具备增强链式推理能力的多模态大型语言模型。实验结果表明其在数学推理和科学问题解决上表现优于同类模型。

<details>
  <summary>Details</summary>

**Motivation:** 多模态大型语言模型在多模态感知和理解方面表现出色，但在复杂和结构化推理任务上存在明显局限性，特别是在需要深入推理的决策和问题解决任务中。本研究旨在解决这一问题，提高模型的推理能力。

**Method:** Corvid采用混合视觉编码器和精心设计的跨模态对齐连接器GateMixer来提高链式思考（CoT）推理能力。通过使用MCoT-Instruct-287K这一高质量的多模态CoT指令跟随数据集，并采用两阶段的CoT格式训练方法进行微调，逐步提升了其逐步推理能力。此外，Corvid还采用了一个有效的推理时间扩展策略来解决过度推理和推理不足的问题。

**Result:** 实验表明，Corvid在数学推理和科学问题解决方面表现出色，超过了现有参数规模相似的多模态大型语言模型。

**Conclusion:** Corvid通过改进架构设计和训练策略，在提高模型推理能力方面取得了显著的进展，特别是在数学推理和科学问题解决任务上具有显著优势。

**Abstract:** Recent advancements in multimodal large language models (MLLMs) have
demonstrated exceptional performance in multimodal perception and
understanding. However, leading open-source MLLMs exhibit significant
limitations in complex and structured reasoning, particularly in tasks
requiring deep reasoning for decision-making and problem-solving. In this work,
we present Corvid, an MLLM with enhanced chain-of-thought (CoT) reasoning
capabilities. Architecturally, Corvid incorporates a hybrid vision encoder for
informative visual representation and a meticulously designed connector
(GateMixer) to facilitate cross-modal alignment. To enhance Corvid's CoT
reasoning capabilities, we introduce MCoT-Instruct-287K, a high-quality
multimodal CoT instruction-following dataset, refined and standardized from
diverse public reasoning sources. Leveraging this dataset, we fine-tune Corvid
with a two-stage CoT-formatted training approach to progressively enhance its
step-by-step reasoning abilities. Furthermore, we propose an effective
inference-time scaling strategy that enables Corvid to mitigate over-reasoning
and under-reasoning through self-verification. Extensive experiments
demonstrate that Corvid outperforms existing o1-like MLLMs and state-of-the-art
MLLMs with similar parameter scales, with notable strengths in mathematical
reasoning and science problem-solving. Project page:
https://mm-vl.github.io/corvid.

</details>


### [66] [Towards High-Resolution 3D Anomaly Detection: A Scalable Dataset and Real-Time Framework for Subtle Industrial Defects](https://arxiv.org/abs/2507.07435)
*Yuqi Cheng,Yihan Sun,Hui Zhang,Weiming Shen,Yunkang Cao*

Main category: cs.CV

> 本文为工业点云分析中检测细微异常引入了高分辨率的3D异常检测数据集MiniShift，并提出了一个高效的检测框架Simple3D，该框架在速度和精度上超越了现有方法。

<details>
  <summary>Details</summary>

**Motivation:** 当前的基准数据集主要用于低分辨率的输入，而这并不适用于需要高分辨率数据的工业点云异常检测。因此，需要一种新的高分辨率数据集和检测框架来解决这个问题。

**Method:** 我们提出了一种可扩展的生成高分辨率3D数据的管道，以及一个名为MiniShift的新数据集。同时，我们设计了一个简单的框架Simple3D，该框架结合了多尺度邻域描述符（MSND）和局部特征空间聚合（LFSA），以高效的方式捕捉几何细节，实现实时检测。

**Result:** 实验结果显示，Simple3D框架在MiniShift数据集和现有的基准数据集上都表现出了优于现有方法的准确性和速度。

**Conclusion:** 这表明高分辨率数据和有效的特征聚合对于提升3D异常检测的实际应用具有关键性的作用。

**Abstract:** In industrial point cloud analysis, detecting subtle anomalies demands
high-resolution spatial data, yet prevailing benchmarks emphasize
low-resolution inputs. To address this disparity, we propose a scalable
pipeline for generating realistic and subtle 3D anomalies. Employing this
pipeline, we developed MiniShift, the inaugural high-resolution 3D anomaly
detection dataset, encompassing 2,577 point clouds, each with 500,000 points
and anomalies occupying less than 1\% of the total. We further introduce
Simple3D, an efficient framework integrating Multi-scale Neighborhood
Descriptors (MSND) and Local Feature Spatial Aggregation (LFSA) to capture
intricate geometric details with minimal computational overhead, achieving
real-time inference exceeding 20 fps. Extensive evaluations on MiniShift and
established benchmarks demonstrate that Simple3D surpasses state-of-the-art
methods in both accuracy and speed, highlighting the pivotal role of
high-resolution data and effective feature aggregation in advancing practical
3D anomaly detection.

</details>


### [67] [Dual Semantic-Aware Network for Noise Suppressed Ultrasound Video Segmentation](https://arxiv.org/abs/2507.07443)
*Ling Zhou,Runtian Yuan,Yi Liu,Yuejie Zhang,Rui Feng,Shang Gao*

Main category: cs.CV

> 研究提出了一种称为DSANet的新框架，用于增强超声视频分割的噪声鲁棒性，展示了在精度和速度上的显著优势。

<details>
  <summary>Details</summary>

**Motivation:** 超声成像作为一种无创诊断工具，因其固有的特点会引入大量噪声，这对自动化病变或器官分割带来了挑战。为了应对这些挑战而提出该框架。

**Method:** 提出了一种名为Dual Semantic-Aware Network (DSANet) 的新框架来增强超声视频分割的噪声鲁棒性。该框架引入了Adjacent-Frame Semantic-Aware (AFSA) 模块和Local-and-Global Semantic-Aware (LGSA) 模块，通过引导相邻帧之间的特征融合来减轻随机噪声的影响，同时保持对空间细节和时间上下文的关注。

**Result:** 在四个基准数据集上的广泛评估表明，DSANet 在分割精度上显著优于现有方法。此外，该模型的推理速度也显著高于基于视频的方法，甚至超过了某些基于图像的模型。

**Conclusion:** 通过集成局部和全局特征的多级语义表征，DSANet 能够有效应对噪声干扰，实现更高的分割精度和更快的推理速度。

**Abstract:** Ultrasound imaging is a prevalent diagnostic tool known for its simplicity
and non-invasiveness. However, its inherent characteristics often introduce
substantial noise, posing considerable challenges for automated lesion or organ
segmentation in ultrasound video sequences. To address these limitations, we
propose the Dual Semantic-Aware Network (DSANet), a novel framework designed to
enhance noise robustness in ultrasound video segmentation by fostering mutual
semantic awareness between local and global features. Specifically, we
introduce an Adjacent-Frame Semantic-Aware (AFSA) module, which constructs a
channel-wise similarity matrix to guide feature fusion across adjacent frames,
effectively mitigating the impact of random noise without relying on
pixel-level relationships. Additionally, we propose a Local-and-Global
Semantic-Aware (LGSA) module that reorganizes and fuses temporal unconditional
local features, which capture spatial details independently at each frame, with
conditional global features that incorporate temporal context from adjacent
frames. This integration facilitates multi-level semantic representation,
significantly improving the model's resilience to noise interference. Extensive
evaluations on four benchmark datasets demonstrate that DSANet substantially
outperforms state-of-the-art methods in segmentation accuracy. Moreover, since
our model avoids pixel-level feature dependencies, it achieves significantly
higher inference FPS than video-based methods, and even surpasses some
image-based models. Code can be found in
\href{https://github.com/ZhouL2001/DSANet}{DSANet}

</details>


### [68] [Bluish Veil Detection and Lesion Classification using Custom Deep Learnable Layers with Explainable Artificial Intelligence (XAI)](https://arxiv.org/abs/2507.07453)
*M. A. Rasel,Sameem Abdul Kareem,Zhenli Kwan,Shin Shen Yong,Unaizah Obaidellah*

Main category: cs.CV

> 研究提出了一种基于颜色阈值技术将非标注皮肤病变数据集转换为标注数据集的方法，并设计了一种深度卷积神经网络（DCNN），用于识别皮肤病变中的蓝白膜（BWV），其在多个数据集上的测试准确率均超过85%，优于现有模型。

<details>
  <summary>Details</summary>

**Motivation:** 鉴于蓝白膜在诊断黑色素瘤中的重要性及当前对其在皮肤病学图像中检测研究的不足，该研究旨在开发更优的BWV检测方法以提高早期诊断效率。

**Method:** 研究使用颜色阈值技术将非标注皮肤病变数据集转换为标注数据集，并设计一种使用自定义层替代标准激活函数层的深度卷积神经网络（DCNN）。

**Result:** DCNN模型在增强后的不同数据集上达到较高的测试准确率，包括PH2数据集上的85.71%，ISIC归档数据集上的95.00%，结合增强的PH2+ISIC归档数据集上的95.05%，以及Derm7pt数据集上的90.00%。

**Conclusion:** 该研究提出的DCNN模型结合可解释人工智能算法在皮肤病变中识别BWV方面表现出色，比现有模型更加优越，为早期黑色素瘤诊断提供了有力工具。

**Abstract:** Melanoma, one of the deadliest types of skin cancer, accounts for thousands
of fatalities globally. The bluish, blue-whitish, or blue-white veil (BWV) is a
critical feature for diagnosing melanoma, yet research into detecting BWV in
dermatological images is limited. This study utilizes a non-annotated skin
lesion dataset, which is converted into an annotated dataset using a proposed
imaging algorithm based on color threshold techniques on lesion patches and
color palettes. A Deep Convolutional Neural Network (DCNN) is designed and
trained separately on three individual and combined dermoscopic datasets, using
custom layers instead of standard activation function layers. The model is
developed to categorize skin lesions based on the presence of BWV. The proposed
DCNN demonstrates superior performance compared to conventional BWV detection
models across different datasets. The model achieves a testing accuracy of
85.71% on the augmented PH2 dataset, 95.00% on the augmented ISIC archive
dataset, 95.05% on the combined augmented (PH2+ISIC archive) dataset, and
90.00% on the Derm7pt dataset. An explainable artificial intelligence (XAI)
algorithm is subsequently applied to interpret the DCNN's decision-making
process regarding BWV detection. The proposed approach, coupled with XAI,
significantly improves the detection of BWV in skin lesions, outperforming
existing models and providing a robust tool for early melanoma diagnosis.

</details>


### [69] [Objectomaly: Objectness-Aware Refinement for OoD Segmentation with Structural Consistency and Boundary Precision](https://arxiv.org/abs/2507.07460)
*Jeonghoon Song,Sunghun Kim,Jaegyun Im,Byeongjoon Noh*

Main category: cs.CV

> Objectomaly 是一种对象意识增强的框架，处理 OoD 分割问题，通过三阶段的框架，在多个基准测试中显示了优越的性能和鲁棒性。

<details>
  <summary>Details</summary>

**Motivation:** 该论文的动机在于解决现有基于掩码的方法在 OoD 分割中的边界不精确、对象内的异常评分不一致以及来自背景噪声的假阳性等问题，尤其是在自动驾驶等领域中需要高度安全性的应用。

**Method:** Objectomaly 方法包含三个阶段：1) 使用现有的 OoD 背骨网络进行粗略异常评分 (Coarse Anomaly Scoring, CAS)，2) 利用 SAM 生成的实例掩码进行对象级分数校准 (Objectness-Aware Score Calibration, OASC)，3) 使用拉普拉斯滤波和高斯平滑进行轮廓精修 (Meticulous Boundary Precision, MBP).

**Result:** Objectomaly 在几个关键的 OoD 分割基准上实现了最先进的性能，包括 SMIYC AnomalyTrack/ObstacleTrack 和 RoadAnomaly，提升了像素级（AuPRC 最高达到 96.99，FPR95 最低达到 0.07）和组件级（F1 分数最高达到 83.44）的评估指标。

**Conclusion:** 通过消融研究和在真实驾驶视频上的定性结果，证明了 Objectomaly 方法的鲁棒性和通用性。

**Abstract:** Out-of-Distribution (OoD) segmentation is critical for safety-sensitive
applications like autonomous driving. However, existing mask-based methods
often suffer from boundary imprecision, inconsistent anomaly scores within
objects, and false positives from background noise. We propose
\textbf{\textit{Objectomaly}}, an objectness-aware refinement framework that
incorporates object-level priors. Objectomaly consists of three stages: (1)
Coarse Anomaly Scoring (CAS) using an existing OoD backbone, (2)
Objectness-Aware Score Calibration (OASC) leveraging SAM-generated instance
masks for object-level score normalization, and (3) Meticulous Boundary
Precision (MBP) applying Laplacian filtering and Gaussian smoothing for contour
refinement. Objectomaly achieves state-of-the-art performance on key OoD
segmentation benchmarks, including SMIYC AnomalyTrack/ObstacleTrack and
RoadAnomaly, improving both pixel-level (AuPRC up to 96.99, FPR$_{95}$ down to
0.07) and component-level (F1$-$score up to 83.44) metrics. Ablation studies
and qualitative results on real-world driving videos further validate the
robustness and generalizability of our method. Code will be released upon
publication.

</details>


### [70] [Degradation-Agnostic Statistical Facial Feature Transformation for Blind Face Restoration in Adverse Weather Conditions](https://arxiv.org/abs/2507.07464)
*Chang-Hwan Son*

Main category: cs.CV

> 论文提出了一种基于GAN的盲面部图像恢复框架，包括局部统计面部特征变换模块和降质不可知特征嵌入模块，有效提高了恶劣天气下的面部识别准确度。

<details>
  <summary>Details</summary>

**Motivation:** 随着智能CCTV系统的普及，对恶劣天气条件下性能优化的面部识别系统的需求日益增长。现有模型在应对天气引起的降质方面存在局限性，导致面部纹理和结构的扭曲。

**Method:** 提出了一个结合局部统计面部特征变换(SFFT)和降质不可知特征嵌入(DAFE)模块的新型GAN框架，SFFT模块通过低质量（LQ）和高质量（HQ）面部区域之间的局部统计分布对齐改进面部结构和颜色保真度；DAFE模块通过LQ和HQ编码器表示的对齐提高恶劣天气下的面部特征提取效果。

**Result:** 实验结果表明，提出的降质不可知SFFT模型在抑制纹理扭曲方面和准确重建面部结构方面优于现有的GAN和扩散模型，特别是极大改善了恶劣天气下的面部图像恢复效果。

**Conclusion:** 所提技术和模块在恶劣天气场景中增强结构性保真度和感知质量方面得到了验证，表明了方法的有效性。

**Abstract:** With the increasing deployment of intelligent CCTV systems in outdoor
environments, there is a growing demand for face recognition systems optimized
for challenging weather conditions. Adverse weather significantly degrades
image quality, which in turn reduces recognition accuracy. Although recent face
image restoration (FIR) models based on generative adversarial networks (GANs)
and diffusion models have shown progress, their performance remains limited due
to the lack of dedicated modules that explicitly address weather-induced
degradations. This leads to distorted facial textures and structures. To
address these limitations, we propose a novel GAN-based blind FIR framework
that integrates two key components: local Statistical Facial Feature
Transformation (SFFT) and Degradation-Agnostic Feature Embedding (DAFE). The
local SFFT module enhances facial structure and color fidelity by aligning the
local statistical distributions of low-quality (LQ) facial regions with those
of high-quality (HQ) counterparts. Complementarily, the DAFE module enables
robust statistical facial feature extraction under adverse weather conditions
by aligning LQ and HQ encoder representations, thereby making the restoration
process adaptive to severe weather-induced degradations. Experimental results
demonstrate that the proposed degradation-agnostic SFFT model outperforms
existing state-of-the-art FIR methods based on GAN and diffusion models,
particularly in suppressing texture distortions and accurately reconstructing
facial structures. Furthermore, both the SFFT and DAFE modules are empirically
validated in enhancing structural fidelity and perceptual quality in face
restoration under challenging weather scenarios.

</details>


### [71] [Temporal Unlearnable Examples: Preventing Personal Video Data from Unauthorized Exploitation by Object Tracking](https://arxiv.org/abs/2507.07483)
*Qiangqiang Wu,Yi Yu,Chenqi Kong,Ziquan Liu,Jia Wan,Haoliang Li,Alex C. Kot,Antoni B. Chan*

Main category: cs.CV

> This paper presents a new method for protecting personal video data privacy in visual object tracking by generating Temporal Unlearnable Examples (TUEs) and applying a temporal contrastive loss to improve privacy protection effectiveness.

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the overlooked data privacy concerns in the use of personal videos for training visual object tracking models, focusing on the inadequacy of existing methods that primarily target image-based tasks.

**Method:** This paper proposes a novel generative framework for generating Temporal Unlearnable Examples (TUEs) to protect personal video data privacy in visual object tracking tasks, complemented by a temporal contrastive loss to enhance the effectiveness of these TUEs.

**Result:** The experiments show that the proposed approach achieves state-of-the-art performance in protecting video data privacy with strong transferability across visual object tracking models, datasets, and tasks.

**Conclusion:** The introduction of TUEs and a temporal contrastive loss significantly advances the protection of video data privacy in visual object tracking, overcoming the limitations of existing methods.

**Abstract:** With the rise of social media, vast amounts of user-uploaded videos (e.g.,
YouTube) are utilized as training data for Visual Object Tracking (VOT).
However, the VOT community has largely overlooked video data-privacy issues, as
many private videos have been collected and used for training commercial models
without authorization. To alleviate these issues, this paper presents the first
investigation on preventing personal video data from unauthorized exploitation
by deep trackers. Existing methods for preventing unauthorized data use
primarily focus on image-based tasks (e.g., image classification), directly
applying them to videos reveals several limitations, including inefficiency,
limited effectiveness, and poor generalizability. To address these issues, we
propose a novel generative framework for generating Temporal Unlearnable
Examples (TUEs), and whose efficient computation makes it scalable for usage on
large-scale video datasets. The trackers trained w/ TUEs heavily rely on
unlearnable noises for temporal matching, ignoring the original data structure
and thus ensuring training video data-privacy. To enhance the effectiveness of
TUEs, we introduce a temporal contrastive loss, which further corrupts the
learning of existing trackers when using our TUEs for training. Extensive
experiments demonstrate that our approach achieves state-of-the-art performance
in video data-privacy protection, with strong transferability across VOT
models, datasets, and temporal matching tasks.

</details>


### [72] [Driving by Hybrid Navigation: An Online HD-SD Map Association Framework and Benchmark for Autonomous Vehicles](https://arxiv.org/abs/2507.07487)
*Jiaxu Wan,Xu Wang,Mengwei Xie,Xinyuan Chang,Xinran Liu,Zheng Pan,Mu Xu,Ding Yuan*

Main category: cs.CV

> 在线地图关联（OMA）是首个用于混合导航在线地图关联的基准测试，旨在提升自动驾驶车辆的路径规划能力。该框架包含48万条道路与26万条车道路径，提出了一种新的基于路径和空间注意力机制的方法，名为地图关联转换器（MAT）。此方法能够支持几何和拓扑关系的理解。代码和数据集可在指定GitHub仓库获取。

<details>
  <summary>Details</summary>

**Motivation:** 观察到近期研究专注于生成在线高精度地图，但忽略了与标准地图的关联对混合导航的实际应用挑战。因此，引入了在线地图关联基准，以解决自动驾驶车辆在导航能力上的局限。

**Method:** Structure

**Result:** {
  "tldr": "在线地图关联（OMA）是首个用于混合导航在线地图关联的基准测试，旨在提升自动驾驶车辆的路径规划能力。该框架包含48万条道路与26万条车道路径，提出了一种新的基于路径和空间注意力机制的方法，名为地图关联转换器（MAT）。此方法能够支持几何和拓扑关系的理解。代码和数据集可在指定GitHub仓库获取。", 
  "motivation": "观察到近期研究专注于生成在线高精度地图，但忽略了与标准地图的关联对混合导航的实际应用挑战。因此，引入了在线地图关联基准，以解决自动驾驶车辆在导航能力上的局限。", 
  "method": "引入了名为地图关联转换器的新框架，采用了路径感知注意力和空间注意力机制来增强模型对地图几何和拓扑关系的理解能力。", 
  "conclusion": "该项目提供了一个详实的基准测试（OMA）和新的基准框架（MAT），有助于推动自动驾驶车辆地图融合及规划技术的发展。", 
  "result": "OMA提供了丰富的道路与路径数据，并通过相应的评估指标测试模型性能。MAT框架展现出路径关联的理解能力，为混合导航提供了关键技术基础。"}
}

**Conclusion:** 该项目提供了一个详实的基准测试（OMA）和新的基准框架（MAT），有助于推动自动驾驶车辆地图融合及规划技术的发展。

**Abstract:** Autonomous vehicles rely on global standard-definition (SD) maps for
road-level route planning and online local high-definition (HD) maps for
lane-level navigation. However, recent work concentrates on construct online HD
maps, often overlooking the association of global SD maps with online HD maps
for hybrid navigation, making challenges in utilizing online HD maps in the
real world. Observing the lack of the capability of autonomous vehicles in
navigation, we introduce \textbf{O}nline \textbf{M}ap \textbf{A}ssociation, the
first benchmark for the association of hybrid navigation-oriented online maps,
which enhances the planning capabilities of autonomous vehicles. Based on
existing datasets, the OMA contains 480k of roads and 260k of lane paths and
provides the corresponding metrics to evaluate the performance of the model.
Additionally, we propose a novel framework, named Map Association Transformer,
as the baseline method, using path-aware attention and spatial attention
mechanisms to enable the understanding of geometric and topological
correspondences. The code and dataset can be accessed at
https://github.com/WallelWan/OMA-MAT.

</details>


### [73] [Divergence Minimization Preference Optimization for Diffusion Model Alignment](https://arxiv.org/abs/2507.07510)
*Binxu Li,Minkai Xu,Meihua Dang,Stefano Ermon*

Main category: cs.CV

> 本文提出了DMPO方法，通过最小化逆KL散度进行扩散模型对齐，有效超越现有技术。

<details>
  <summary>Details</summary>

**Motivation:** 受到最近语言模型进展的启发，我们希望通过与人类偏好一致来进一步改进模型。通过从散度最小化的角度研究对齐，我们发现现有的偏好优化方法通常陷入次优的均值寻找优化中。

**Method:** 我们的方法名为Divergence Minimization Preference Optimization (DMPO)，这是一种新颖且原理明确的方法，通过最小化逆KL散度来对齐扩散模型，其优化方向与原始RL渐近相同。

**Result:** 我们的实验结果全面证实了DMPO在人类评价和自动指标两方面的有效性，特别是在PickScore评估数据集上，DMPO比现有扩散模型基线高出至少64.6%，展示了其将生成行为与所需输出对齐的优势。

**Conclusion:** 总体而言，DMPO为扩散模型的偏好对齐解锁了稳健而优雅的路径，既具有坚实的理论基础，又提供了优异的实际表现。

**Abstract:** Diffusion models have achieved remarkable success in generating realistic and
versatile images from text prompts. Inspired by the recent advancements of
language models, there is an increasing interest in further improving the
models by aligning with human preferences. However, we investigate alignment
from a divergence minimization perspective and reveal that existing preference
optimization methods are typically trapped in suboptimal mean-seeking
optimization. In this paper, we introduce Divergence Minimization Preference
Optimization (DMPO), a novel and principled method for aligning diffusion
models by minimizing reverse KL divergence, which asymptotically enjoys the
same optimization direction as original RL. We provide rigorous analysis to
justify the effectiveness of DMPO and conduct comprehensive experiments to
validate its empirical strength across both human evaluations and automatic
metrics. Our extensive results show that diffusion models fine-tuned with DMPO
can consistently outperform or match existing techniques, specifically
outperforming all existing diffusion alignment baselines by at least 64.6% in
PickScore across all evaluation datasets, demonstrating the method's
superiority in aligning generative behavior with desired outputs. Overall, DMPO
unlocks a robust and elegant pathway for preference alignment, bridging
principled theory with practical performance in diffusion models.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [74] [Semi-supervised learning and integration of multi-sequence MR-images for carotid vessel wall and plaque segmentation](https://arxiv.org/abs/2507.07496)
*Marie-Christine Pali,Christina Schwaiger,Malik Galijasevic,Valentin K. Ladenhauf,Stephanie Mangesius,Elke R. Gizewski*

Main category: eess.IV

> 本文提出了一种半监督深度学习方法用于颈动脉及其斑块的精准分割，采用多层级多序列的U-Net架构，并通过不同融合点的选择提高了MRI数据分割的准确性。实验结果显示了方法的有效性。

<details>
  <summary>Details</summary>

**Motivation:** 研究的动机在于分析颈动脉斑块并通过多序列MRI数据进行准确的分割，这对于评估动脉粥样硬化和缺血性中风的风险至关重要。然而，复杂的斑块形态和欠缺的标注数据带来了挑战。

**Method:** 该研究提出了一种半监督深度学习方法，用于整合多序列MRI数据以进行颈动脉壁及其斑块的分割。该算法由两个网络组成：粗略定位模型和精细分割模型。粗略定位模型用于基于先前的知识识别感兴趣区域，而精细分割模型则用于精确划定血管壁和斑块。研究还探讨了不同融合策略，并提出了一种多层级多序列的U-Net架构版本。为了解决标注数据有限和颈动脉MRI复杂度的问题，研究提出了一个半监督方法，通过在不同的输入变换下保持一致性来提高准确性。

**Result:** 方法在52名患有动脉粥样硬化的患者的数据集上进行了评估，每个患者都有五种MRI序列影像。实验结果显示了该方法的有效性和在U-Net架构中融合点选择的重要性。研究还包含了基于专家的模型性能评估。

**Conclusion:** 研究结论表明，融合策略和半监督学习对于数据有限条件下的颈动脉分割具有潜在的提升作用。

**Abstract:** The analysis of carotid arteries, particularly plaques, in multi-sequence
Magnetic Resonance Imaging (MRI) data is crucial for assessing the risk of
atherosclerosis and ischemic stroke. In order to evaluate metrics and radiomic
features, quantifying the state of atherosclerosis, accurate segmentation is
important. However, the complex morphology of plaques and the scarcity of
labeled data poses significant challenges. In this work, we address these
problems and propose a semi-supervised deep learning-based approach designed to
effectively integrate multi-sequence MRI data for the segmentation of carotid
artery vessel wall and plaque. The proposed algorithm consists of two networks:
a coarse localization model identifies the region of interest guided by some
prior knowledge on the position and number of carotid arteries, followed by a
fine segmentation model for precise delineation of vessel walls and plaques. To
effectively integrate complementary information across different MRI sequences,
we investigate different fusion strategies and introduce a multi-level
multi-sequence version of U-Net architecture. To address the challenges of
limited labeled data and the complexity of carotid artery MRI, we propose a
semi-supervised approach that enforces consistency under various input
transformations. Our approach is evaluated on 52 patients with
arteriosclerosis, each with five MRI sequences. Comprehensive experiments
demonstrate the effectiveness of our approach and emphasize the role of fusion
point selection in U-Net-based architectures. To validate the accuracy of our
results, we also include an expert-based assessment of model performance. Our
findings highlight the potential of fusion strategies and semi-supervised
learning for improving carotid artery segmentation in data-limited MRI
applications.

</details>
