<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 7]
- [cs.CV](#cs.CV) [Total: 11]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Subjective Evaluation Profile Analysis of Science Fiction Short Stories and its Critical-Theoretical Significance](https://arxiv.org/abs/2507.11582)
*Kazuyoshi Otsuka*

Main category: cs.CL

> 本研究通过将大语言模型（LLMs）视为“主观文学评论家”，探索其在文学评估中的审美偏好和评价模式。通过对十篇日本科幻短篇小说的英译版本进行六种最先进的LLMs评估，揭示了它们评价的一致性（α值从1.00到0.35不等）和五种不同的评价模式。这表明LLMs可能具有类似人类批评学派的个体评价特征，而不是作为中立的评估工具。

<details>
  <summary>Details</summary>

**Motivation:** 研究动机是为了探索LLMs在文学评价任务中的主观性，特别是它们的表现形式和背后的逻辑，通过这种方式来更好地理解这些模型内部可能固有的价值体系。

**Method:** 研究采用十篇日本科幻短篇小说的英译版本，由六种最先进的LLMs在七次独立的评估会话中进行评分。使用主成分分析和聚类技术分析评价结果的一致性和差异。

**Result:** 结果发现LLMs的评估存在显著的一致性差异（α值范围1.00到0.35，五种不同的评价模式）。TF-IDF分析显示每个模型的评价词汇有所不同。

**Conclusion:** 结论表明，LLMs可能具有类似人类批评学派的个体评价特性，这并非中立基准评估工具，而是受其训练方式（如强化学习反馈）影响的主观评价系统。

**Abstract:** This study positions large language models (LLMs) as "subjective literary
critics" to explore aesthetic preferences and evaluation patterns in literary
assessment. Ten Japanese science fiction short stories were translated into
English and evaluated by six state-of-the-art LLMs across seven independent
sessions. Principal component analysis and clustering techniques revealed
significant variations in evaluation consistency ({\alpha} ranging from 1.00 to
0.35) and five distinct evaluation patterns. Additionally, evaluation variance
across stories differed by up to 4.5-fold, with TF-IDF analysis confirming
distinctive evaluation vocabularies for each model. Our seven-session
within-day protocol using an original Science Fiction corpus strategically
minimizes external biases, allowing us to observe implicit value systems shaped
by RLHF and their influence on literary judgment. These findings suggest that
LLMs may possess individual evaluation characteristics similar to human
critical schools, rather than functioning as neutral benchmarkers.

</details>


### [2] [MapIQ: Benchmarking Multimodal Large Language Models for Map Question Answering](https://arxiv.org/abs/2507.11625)
*Varun Srivastava,Fan Lei,Srija Mukhopadhyay,Vivek Gupta,Ross Maciejewski*

Main category: cs.CL

> 研究介绍了一个新的数据集MapIQ，用来评估多模态大语言模型在不同地图类型上的视觉问答性能，同时探讨了改变地图设计对模型性能的影响。

<details>
  <summary>Details</summary>

**Motivation:** 当前的视觉问答研究主要集中在等值地图上，这仅涵盖了有限的主题类别和可视分析任务。为了填补这些空白，引入了一个新的数据集和评估方法，以便于更全面地评估模型的性能。

**Method:** 通过引入MapIQ数据集，该数据集包含14,706个问题答案对，涵盖三种地图类型：等值地图、变形地图和比例符号地图，跨越六个不同的主题。同时，通过改变地图设计（例如，改变颜色方案，修改图例设计，去除地图元素）来评估多种多模态大语言模型（MLLMs）的性能，并将其与人类基准进行比较。

**Result:** 通过对比MLLMs模型在六种视觉分析任务上的表现，提供了关于模型鲁棒性、敏感性和对内部地理知识依赖性的见解。

**Conclusion:** 研究表明通过MapIQ可以有效地评估多模态大语言模型在视觉问答上的性能，并且指出未来可以通过优化地图设计来提高模型的表现。

**Abstract:** Recent advancements in multimodal large language models (MLLMs) have driven
researchers to explore how well these models read data visualizations, e.g.,
bar charts, scatter plots. More recently, attention has shifted to visual
question answering with maps (Map-VQA). However, Map-VQA research has primarily
focused on choropleth maps, which cover only a limited range of thematic
categories and visual analytical tasks. To address these gaps, we introduce
MapIQ, a benchmark dataset comprising 14,706 question-answer pairs across three
map types: choropleth maps, cartograms, and proportional symbol maps spanning
topics from six distinct themes (e.g., housing, crime). We evaluate multiple
MLLMs using six visual analytical tasks, comparing their performance against
one another and a human baseline. An additional experiment examining the impact
of map design changes (e.g., altered color schemes, modified legend designs,
and removal of map elements) provides insights into the robustness and
sensitivity of MLLMs, their reliance on internal geographic knowledge, and
potential avenues for improving Map-VQA performance.

</details>


### [3] [Cross-lingual Few-shot Learning for Persian Sentiment Analysis with Incremental Adaptation](https://arxiv.org/abs/2507.11634)
*Farideh Majidi,Ziaeddin Beheshtifard*

Main category: cs.CL

> The study fine-tunes multilingual pre-trained models using few-shot and incremental learning to conduct sentiment analysis in Persian, achieving 96% accuracy with mDeBERTa and XLM-RoBERTa.

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to develop a model that can perform sentiment analysis in Persian with a limited amount of data, leveraging knowledge from high-resource languages.

**Method:** The paper uses few-shot and incremental learning methods to fine-tune three pre-trained multilingual models (XLM-RoBERTa, mDeBERTa, DistilBERT) on small samples of Persian datasets from various sources like X, Instagram, Digikala, Snappfood, and Taaghche.

**Result:** The mDeBERTa and XLM-RoBERTa models achieved 96% accuracy in Persian sentiment analysis.

**Conclusion:** Combining few-shot learning and incremental learning with multilingual pre-trained models is effective for performing sentiment analysis in Persian with limited data.

**Abstract:** This research examines cross-lingual sentiment analysis using few-shot
learning and incremental learning methods in Persian. The main objective is to
develop a model capable of performing sentiment analysis in Persian using
limited data, while getting prior knowledge from high-resource languages. To
achieve this, three pre-trained multilingual models (XLM-RoBERTa, mDeBERTa, and
DistilBERT) were employed, which were fine-tuned using few-shot and incremental
learning approaches on small samples of Persian data from diverse sources,
including X, Instagram, Digikala, Snappfood, and Taaghche. This variety enabled
the models to learn from a broad range of contexts. Experimental results show
that the mDeBERTa and XLM-RoBERTa achieved high performances, reaching 96%
accuracy on Persian sentiment analysis. These findings highlight the
effectiveness of combining few-shot learning and incremental learning with
multilingual pre-trained models.

</details>


### [4] [Partitioner Guided Modal Learning Framework](https://arxiv.org/abs/2507.11661)
*Guimin Hu,Yi Xin,Lijie Hu,Zhihong Zhu,Hasti Seifi*

Main category: cs.CL

> Error

<details>
  <summary>Details</summary>

**Motivation:** Error

**Method:** Error

**Result:** Error

**Conclusion:** Error

**Abstract:** Multimodal learning benefits from multiple modal information, and each
learned modal representations can be divided into uni-modal that can be learned
from uni-modal training and paired-modal features that can be learned from
cross-modal interaction. Building on this perspective, we propose a
partitioner-guided modal learning framework, PgM, which consists of the modal
partitioner, uni-modal learner, paired-modal learner, and uni-paired modal
decoder. Modal partitioner segments the learned modal representation into
uni-modal and paired-modal features. Modal learner incorporates two dedicated
components for uni-modal and paired-modal learning. Uni-paired modal decoder
reconstructs modal representation based on uni-modal and paired-modal features.
PgM offers three key benefits: 1) thorough learning of uni-modal and
paired-modal features, 2) flexible distribution adjustment for uni-modal and
paired-modal representations to suit diverse downstream tasks, and 3) different
learning rates across modalities and partitions. Extensive experiments
demonstrate the effectiveness of PgM across four multimodal tasks and further
highlight its transferability to existing models. Additionally, we visualize
the distribution of uni-modal and paired-modal features across modalities and
tasks, offering insights into their respective contributions.

</details>


### [5] [ExpliCIT-QA: Explainable Code-Based Image Table Question Answering](https://arxiv.org/abs/2507.11694)
*Maximiliano Hormazábal Lagos,Álvaro Bueno Sáez,Pedro Alonso Doval,Jorge Alcalde Vesteiro,Héctor Cerezo-Costas*

Main category: cs.CL

> 研究提出了一种名为ExpliCIT-QA的系统，该系统通过可解释的步骤处理表格图像，并在最终回答问题时提供透明度和可审核性。

<details>
  <summary>Details</summary>

**Motivation:** 本研究旨在解决表格视觉问题回答（TableVQA）系统中解释性差距的问题，特别是为金融和医疗保健等关键领域的应用提供支持。

**Method:** ExpliCIT-QA系统采用模块化设计，包括5个主要模块：多模态表格理解、基于语言的推理、自动代码生成、代码执行和自然语言解释。系统旨在提高透明度和可审核性，所有中间输出、解析的表格、推理步骤、生成的代码和最终答案都可以供检查。

**Result:** 研究在TableVQA-Bench基准测试中评估了ExpliCIT-QA系统，展示了在解释性和透明度方面的改进。

**Conclusion:** 通过这些改进，该系统为金融和医疗保健等对结果审核至关重要的领域打开了应用程序的大门。

**Abstract:** We present ExpliCIT-QA, a system that extends our previous MRT approach for
tabular question answering into a multimodal pipeline capable of handling
complex table images and providing explainable answers. ExpliCIT-QA follows a
modular design, consisting of: (1) Multimodal Table Understanding, which uses a
Chain-of-Thought approach to extract and transform content from table images;
(2) Language-based Reasoning, where a step-by-step explanation in natural
language is generated to solve the problem; (3) Automatic Code Generation,
where Python/Pandas scripts are created based on the reasoning steps, with
feedback for handling errors; (4) Code Execution to compute the final answer;
and (5) Natural Language Explanation that describes how the answer was
computed. The system is built for transparency and auditability: all
intermediate outputs, parsed tables, reasoning steps, generated code, and final
answers are available for inspection. This strategy works towards closing the
explainability gap in end-to-end TableVQA systems. We evaluated ExpliCIT-QA on
the TableVQA-Bench benchmark, comparing it with existing baselines. We
demonstrated improvements in interpretability and transparency, which open the
door for applications in sensitive domains like finance and healthcare where
auditing results are critical.

</details>


### [6] [CRABS: A syntactic-semantic pincer strategy for bounding LLM interpretation of Python notebooks](https://arxiv.org/abs/2507.11742)
*Meng Li,Timothy M. McPhillips,Dingmin Wang,Shin-Rong Tsai,Bertram Ludäscher*

Main category: cs.CL

> 本文提出了一种称为CRABS的方法，用于理解Python笔记本，它结合了浅层语法分析和大型语言模型（LLM），以消除理解过程中的歧义。实验表明该方法在理解笔记信息流和单元执行依赖关系方面非常有效。

<details>
  <summary>Details</summary>

**Motivation:** 理解Python笔记本中的信息流和操作对于评估、复用和适应新的任务是至关重要的。然而，通过重新执行来理解笔记本并不总是可行的，因为它涉及到解决数据和软件依赖关系的难题。尽管大型语言模型（LLMs）在理解代码方面表现出色，但它们仍然在解析一些现实中的笔记本时存在幻觉和长上下文的问题。

**Method:** 本文提出了一种称为CRABS的方法，该方法结合了浅层语法分析和大型语言模型（LLM），以理解Python笔记本中的信息流和单元执行依赖关系。具体来说，它使用抽象语法树（AST）来捕捉单元间输入输出的下限和上限估计，并通过单元逐个零样本学习解决剩余的歧义。

**Result:** 作者通过一个包含50个具有代表性的、高评价的Kaggle笔记本的注释数据集进行了评估。实验表明，LLM正确解决了1425个歧义中的1397个（98%），CRABS在50个笔记本上的平均F1分为98%，用于识别单元间信息流，而识别传递的单元执行依赖关系的平均F1分为99%。

**Conclusion:** 研究结果显示，结合语法分析和大型语言模型的方法有效解决了笔记本理解中的许多挑战，尤其是在处理代码解析时的幻觉和长上下文难题。CRABS方法在识别单元间信息流和传递执行依赖关系时表现出高准确度。这为更好地评估、复用和适应Python笔记本提供了途径。

**Abstract:** Recognizing the information flows and operations comprising data science and
machine learning Python notebooks is critical for evaluating, reusing, and
adapting notebooks for new tasks. Investigating a notebook via re-execution
often is impractical due to the challenges of resolving data and software
dependencies. While Large Language Models (LLMs) pre-trained on large codebases
have demonstrated effectiveness in understanding code without running it, we
observe that they fail to understand some realistic notebooks due to
hallucinations and long-context challenges. To address these issues, we propose
a notebook understanding task yielding an information flow graph and
corresponding cell execution dependency graph for a notebook, and demonstrate
the effectiveness of a pincer strategy that uses limited syntactic analysis to
assist full comprehension of the notebook using an LLM. Our Capture and Resolve
Assisted Bounding Strategy (CRABS) employs shallow syntactic parsing and
analysis of the abstract syntax tree (AST) to capture the correct
interpretation of a notebook between lower and upper estimates of the
inter-cell I/O sets, then uses an LLM to resolve remaining ambiguities via
cell-by-cell zero-shot learning, thereby identifying the true data inputs and
outputs of each cell. We evaluate and demonstrate the effectiveness of our
approach using an annotated dataset of 50 representative, highly up-voted
Kaggle notebooks that together represent 3454 actual cell inputs and outputs.
The LLM correctly resolves 1397 of 1425 (98%) ambiguities left by analyzing the
syntactic structure of these notebooks. Across 50 notebooks, CRABS achieves
average F1 scores of 98% identifying cell-to-cell information flows and 99%
identifying transitive cell execution dependencies.

</details>


### [7] [AI Wizards at CheckThat! 2025: Enhancing Transformer-Based Embeddings with Sentiment for Subjectivity Detection in News Articles](https://arxiv.org/abs/2507.11764)
*Matteo Fasulo,Luca Babboni,Luca Tedeschini*

Main category: cs.CL

> 本文介绍了AI Wizards团队在CLEF 2025 CheckThat! 实验室任务1中的方法，通过情感特征融合改进了基于transformer的分类器，从而显著提高了主观性检测的性能。

<details>
  <summary>Details</summary>

**Motivation:** 研究动机是为了提高在CLEF 2025 CheckThat! 实验室任务1中的主观性检测的性能，任务包括在单语、多语和零样本设置下对新闻文章中的句子进行主观/客观分类。

**Method:** 本文提出了一种基于transformer的分类器增强策略，通过将由辅助模型生成的.sentiment分数与句子表示结合，改进了标准的微调方法。实验中使用了mDeBERTaV3-base, ModernBERT-base（英语）和Llama3.2-1B模型。为了应对跨语言普遍存在的话题不平衡问题，采用了在开发集上优化的决策阈值校准策略。

**Result:** 实验结果显示，通过情感特征融合可以显著提高性能，特别是在提高主观性F1分数方面。该框架在多个语言中取得了高的排名，尤其是在希腊语中获得了1st（Macro F1 = 0.51）。

**Conclusion:** 通过情感特征融合改进的transformer基分类器可以显著提升主观性检测的性能，特别是在处理多语言和零样本情况下，能够应对话题不平衡问题，提高F1分数。

**Abstract:** This paper presents AI Wizards' participation in the CLEF 2025 CheckThat! Lab
Task 1: Subjectivity Detection in News Articles, classifying sentences as
subjective/objective in monolingual, multilingual, and zero-shot settings.
Training/development datasets were provided for Arabic, German, English,
Italian, and Bulgarian; final evaluation included additional unseen languages
(e.g., Greek, Romanian, Polish, Ukrainian) to assess generalization. Our
primary strategy enhanced transformer-based classifiers by integrating
sentiment scores, derived from an auxiliary model, with sentence
representations, aiming to improve upon standard fine-tuning. We explored this
sentiment-augmented architecture with mDeBERTaV3-base, ModernBERT-base
(English), and Llama3.2-1B. To address class imbalance, prevalent across
languages, we employed decision threshold calibration optimized on the
development set. Our experiments show sentiment feature integration
significantly boosts performance, especially subjective F1 score. This
framework led to high rankings, notably 1st for Greek (Macro F1 = 0.51).

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [8] [An Memory-Efficient Framework for Deformable Transformer with Neural Architecture Search](https://arxiv.org/abs/2507.11549)
*Wendong Mao,Mingfan Zhao,Jianfeng Guan,Qiwei Dong,Zhongfeng Wang*

Main category: cs.CV

> A new slicing strategy in combination with a NAS-based approach is introduced to optimize DAT for hardware efficiency without compromising accuracy, verified through algorithm and hardware experiments.

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges of efficient hardware deployment of deformable attention transformers (DAT), which include significant hardware overhead or compromise in model accuracy due to irregular memory access patterns from their data-dependent sampling mechanism.

**Method:** First, a neural architecture search (NAS)-based method with a new slicing strategy is proposed to automatically divide the input feature into uniform patches during the inference process, avoiding memory conflicts without modifying model architecture. Secondly, an FPGA-based verification system is designed to test the performance of this framework on edge-side hardware.

**Result:** Algorithm experiments on the ImageNet-1K dataset show only a 0.2% accuracy drop compared to the baseline DAT, and hardware experiments on Xilinx FPGA demonstrate that the proposed method reduces DRAM access times to 18% compared with existing DAT acceleration methods.

**Conclusion:** The proposed hardware-friendly optimization framework for DAT effectively balances hardware efficiency and model accuracy, making it a promising solution for deploying DAT on edge devices.

**Abstract:** Deformable Attention Transformers (DAT) have shown remarkable performance in
computer vision tasks by adaptively focusing on informative image regions.
However, their data-dependent sampling mechanism introduces irregular memory
access patterns, posing significant challenges for efficient hardware
deployment. Existing acceleration methods either incur high hardware overhead
or compromise model accuracy. To address these issues, this paper proposes a
hardware-friendly optimization framework for DAT. First, a neural architecture
search (NAS)-based method with a new slicing strategy is proposed to
automatically divide the input feature into uniform patches during the
inference process, avoiding memory conflicts without modifying model
architecture. The method explores the optimal slice configuration by jointly
optimizing hardware cost and inference accuracy. Secondly, an FPGA-based
verification system is designed to test the performance of this framework on
edge-side hardware. Algorithm experiments on the ImageNet-1K dataset
demonstrate that our hardware-friendly framework can maintain have only 0.2%
accuracy drop compared to the baseline DAT. Hardware experiments on Xilinx FPGA
show the proposed method reduces DRAM access times to 18% compared with
existing DAT acceleration methods.

</details>


### [9] [Deformable Dynamic Convolution for Accurate yet Efficient Spatio-Temporal Traffic Prediction](https://arxiv.org/abs/2507.11550)
*Hyeonseok Jin,Geonmin Kim,Kyungbaek Kim*

Main category: cs.CV

> DDCN提出了一种基于动态可变形卷积的新型交通预测模型，能够有效处理时空异质性以及大规模数据的可扩展性问题，实验结果表明其具有很高的预测精度和效率。

<details>
  <summary>Details</summary>

**Motivation:** 提出DDCN是为了克服传统的时空交通预测方法在捕获时空异质性和模型复杂性方面的局限，尤其是针对大规模数据的可扩展性问题。

**Method:** DDCN提出了一种基于动态可变形卷积的方法，以应对时空异质性和非欧氏空间结构建模的挑战。该方法基于卷积神经网络，并且将Transformer风格的CNN分解为编码器-解码器结构，通过应用可变形偏移滤波器来增强重要特征。解码器由前馈模块组成，用于补充编码器的输出。

**Result:** DDCN在四个真实世界的数据集上进行了全面实验，展现了具有竞争力的表现，强调了CNN方法在时空交通预测中的潜力和有效性。

**Conclusion:** DDCN作为一种新颖的结构，能够实现既精确又高效的交通预测，并为基于卷积的时空预测方法展现了新的可能性。

**Abstract:** Spatio-temporal traffic prediction plays a key role in intelligent
transportation systems by enabling accurate prediction in complex urban areas.
Although not only accuracy but also efficiency for scalability is important,
some previous methods struggle to capture heterogeneity such as varying traffic
patterns across regions and time periods. Moreover, Graph Neural Networks
(GNNs), which are the mainstream of traffic prediction, not only require
predefined adjacency matrix, but also limit scalability to large-scale data
containing many nodes due to their inherent complexity. To overcome these
limitations, we propose Deformable Dynamic Convolution Network (DDCN) for
accurate yet efficient traffic prediction. Traditional Convolutional Neural
Networks (CNNs) are limited in modeling non-Euclidean spatial structures and
spatio-temporal heterogeneity, DDCN overcomes these challenges by dynamically
applying deformable filters based on offset. Specifically, DDCN decomposes
transformer-style CNN to encoder-decoder structure, and applies proposed
approaches to the spatial and spatio-temporal attention blocks of the encoder
to emphasize important features. The decoder, composed of feed-forward module,
complements the output of the encoder. This novel structure make DDCN can
perform accurate yet efficient traffic prediction. In comprehensive experiments
on four real-world datasets, DDCN achieves competitive performance, emphasizing
the potential and effectiveness of CNN-based approaches for spatio-temporal
traffic prediction.

</details>


### [10] [Inversion-DPO: Precise and Efficient Post-Training for Diffusion Models](https://arxiv.org/abs/2507.11554)
*Zejian Li,Yize Li,Chenye Meng,Zhongni Liu,Yang Ling,Shengyuan Zhang,Guang Yang,Changyuan Yang,Zhiyuan Yang,Lingyun Sun*

Main category: cs.CV

> 本文提出了Inversion-DPO，一种新的对齐框架，它使用DDIM逆式对Direct Preference Optimization (DPO)进行了重新表述，避免了对奖励模型的需要。这提高了精度和效率，特别是在由11,140张具有复杂结构标注图像的实验数据集上验证了其在组成图像生成的复杂任务中的优势。

<details>
  <summary>Details</summary>

**Motivation:** 尽管近年来扩散模型的进展是由对齐方法推动的，这些方法能够更好地使模型符合人类偏好，但是这些方法通常需要大量计算资源训练基础模型和奖励模型，导致计算资源消耗高、模型准确性和训练效率低下。为了解决这些问题，本文提出了Inversion-DPO。

**Method:** 我们提出了Inversion-DPO，一种新的对齐框架，通过将Direct Preference Optimization (DPO)与DDIM逆式结合，避免了奖励模型的训练。该方法在DMs中的Diffusion-DPO中通过从胜者和输者样本到噪声的确定性逆式进行难以处理的后验采样，从而形成一个新的后训练范式。这种方法消除了对辅助奖励模型或不准确近似的需要，显著提高了训练的精度和效率。

**Result:** 我们将Inversion-DPO应用于文本到图像生成的基本任务和组成图像生成的复杂任务。大量的实验表明，与现有的后训练方法相比，Inversion-DPO实现了显著的性能提升，并强调了训练生成模型生成高保真组合一致性图像的能力。

**Conclusion:** Inversion-DPO探索了快速、高精度对齐扩散模型的新途径，推动了它们在复杂现实生成任务中的应用。

**Abstract:** Recent advancements in diffusion models (DMs) have been propelled by
alignment methods that post-train models to better conform to human
preferences. However, these approaches typically require computation-intensive
training of a base model and a reward model, which not only incurs substantial
computational overhead but may also compromise model accuracy and training
efficiency. To address these limitations, we propose Inversion-DPO, a novel
alignment framework that circumvents reward modeling by reformulating Direct
Preference Optimization (DPO) with DDIM inversion for DMs. Our method conducts
intractable posterior sampling in Diffusion-DPO with the deterministic
inversion from winning and losing samples to noise and thus derive a new
post-training paradigm. This paradigm eliminates the need for auxiliary reward
models or inaccurate appromixation, significantly enhancing both precision and
efficiency of training. We apply Inversion-DPO to a basic task of text-to-image
generation and a challenging task of compositional image generation. Extensive
experiments show substantial performance improvements achieved by Inversion-DPO
compared to existing post-training methods and highlight the ability of the
trained generative models to generate high-fidelity compositionally coherent
images. For the post-training of compostitional image geneation, we curate a
paired dataset consisting of 11,140 images with complex structural annotations
and comprehensive scores, designed to enhance the compositional capabilities of
generative models. Inversion-DPO explores a new avenue for efficient,
high-precision alignment in diffusion models, advancing their applicability to
complex realistic generation tasks. Our code is available at
https://github.com/MIGHTYEZ/Inversion-DPO

</details>


### [11] [Reprogramming Vision Foundation Models for Spatio-Temporal Forecasting](https://arxiv.org/abs/2507.11558)
*Changlu Chen,Yanbin Liu,Chaoxi Niu,Ling Chen,Tianqing Zhu*

Main category: cs.CV

> 本文提出了ST-VFM框架，重新编程视觉基础模型以进行时空预测，通过双分支结构和两个专用的再编程阶段解决时空预测的挑战，实验表明其优越性。

<details>
  <summary>Details</summary>

**Motivation:** 尽管大型语言模型在时间序列预测方面有所探索，但它们主要捕捉一维的序列依赖关系，难以建模预测时空数据所需的丰富时空相关性，因此本文提出ST-VFM框架以解决此问题。

**Method:** ST-VFM采用双分支架构，整合原始时空输入与辅助的时空流输入，其中流编码轻量级的时间差异信号作为动态空间线索。为有效处理双分支输入，ST-VFM引入两个专有的重新编程阶段：预VFMs重新编程阶段通过应用带有时间感知令牌适配器来嵌入时间上下文，对齐两个分支以满足VFMs兼容的特征空间；以及后VFMs重新编程阶段，引入双边交叉提示协调模块，通过提示条件动态交互分支，增强联合表示学习而无需修改固定的VFMs主干。

**Result:** 在十个时空数据集上的广泛实验表明，ST-VFM在保持时空预测的准确性的同时，还能在多种VFMs的基础上超越现有的最先进基线方法。

**Conclusion:** ST-VFM框架证明了作为通用时空预测方法的有效性和鲁棒性，展示了在多种VFMs后端及其消融研究中的优越性能，确立了其作为强大通用时空预测框架的地位。

**Abstract:** Foundation models have achieved remarkable success in natural language
processing and computer vision, demonstrating strong capabilities in modeling
complex patterns. While recent efforts have explored adapting large language
models (LLMs) for time-series forecasting, LLMs primarily capture
one-dimensional sequential dependencies and struggle to model the richer
spatio-temporal (ST) correlations essential for accurate ST forecasting. In
this paper, we present \textbf{ST-VFM}, a novel framework that systematically
reprograms Vision Foundation Models (VFMs) for general-purpose spatio-temporal
forecasting. While VFMs offer powerful spatial priors, two key challenges arise
when applying them to ST tasks: (1) the lack of inherent temporal modeling
capacity and (2) the modality gap between visual and ST data. To address these,
ST-VFM adopts a \emph{dual-branch architecture} that integrates raw ST inputs
with auxiliary ST flow inputs, where the flow encodes lightweight temporal
difference signals interpretable as dynamic spatial cues. To effectively
process these dual-branch inputs, ST-VFM introduces two dedicated reprogramming
stages. The \emph{pre-VFM reprogramming} stage applies a Temporal-Aware Token
Adapter to embed temporal context and align both branches into VFM-compatible
feature spaces. The \emph{post-VFM reprogramming} stage introduces a Bilateral
Cross-Prompt Coordination module, enabling dynamic interaction between branches
through prompt-based conditioning, thus enriching joint representation learning
without modifying the frozen VFM backbone. Extensive experiments on ten
spatio-temporal datasets show that ST-VFM outperforms state-of-the-art
baselines, demonstrating effectiveness and robustness across VFM backbones
(e.g., DINO, CLIP, DEIT) and ablation studies, establishing it as a strong
general framework for spatio-temporal forecasting.

</details>


### [12] [Expert Operational GANS: Towards Real-Color Underwater Image Restoration](https://arxiv.org/abs/2507.11562)
*Ozer Can Devecioglu,Serkan Kiranyaz,Mehmet Yamac,Moncef Gabbouj*

Main category: cs.CV

> The paper introduces xOp-GAN, a GAN model using multiple experts for underwater image restoration, leading to improved restoration quality.

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to improve the quality of underwater images by overcoming the limitations of traditional single deep regressor networks and conventional GAN-based approaches, which are ineffective in capturing a broad range of underwater image degradations.

**Method:** The paper proposes xOp-GAN, a novel GAN model with multiple expert generator networks trained on specific subsets of image quality, aiming to address the limitation of a single generator network in dealing with diverse visual degradations in underwater images.

**Result:** Experimental results show that xOp-GAN outperforms existing single-regressor methods on the LSUI dataset with PSNR levels up to 25.16 dB.

**Conclusion:** xOp-GAN, with its multiple expert generator networks and discriminator selection, is a more effective model for underwater image restoration, achieving better performance than previous single-regressor models.

**Abstract:** The wide range of deformation artifacts that arise from complex light
propagation, scattering, and depth-dependent attenuation makes the underwater
image restoration to remain a challenging problem. Like other single deep
regressor networks, conventional GAN-based restoration methods struggle to
perform well across this heterogeneous domain, since a single generator network
is typically insufficient to capture the full range of visual degradations. In
order to overcome this limitation, we propose xOp-GAN, a novel GAN model with
several expert generator networks, each trained solely on a particular subset
with a certain image quality. Thus, each generator can learn to maximize its
restoration performance for a particular quality range. Once a xOp-GAN is
trained, each generator can restore the input image and the best restored image
can then be selected by the discriminator based on its perceptual confidence
score. As a result, xOP-GAN is the first GAN model with multiple generators
where the discriminator is being used during the inference of the regression
task. Experimental results on benchmark Large Scale Underwater Image (LSUI)
dataset demonstrates that xOp-GAN achieves PSNR levels up to 25.16 dB,
surpassing all single-regressor models by a large margin even, with reduced
complexity.

</details>


### [13] [Data-Driven Meta-Analysis and Public-Dataset Evaluation for Sensor-Based Gait Age Estimation](https://arxiv.org/abs/2507.11571)
*Varun Velankar*

Main category: cs.CV

> 本研究通过综合元分析和大规模实验，对步态年龄估计技术进行了评估，揭示了不同方法的表现，并提出实际指南以降低现实场景中的年龄估计误差。

<details>
  <summary>Details</summary>

**Motivation:** 步态年龄估算在医疗保健、安全和人机交互领域具有重要应用，本研究旨在建立坚实的性能基线和实用指南，以在现实场景中减少步态年龄估算误差。

**Method:** 本研究结合了广泛的元分析和大规模实验，分析了五十九项研究，涉及超过七万五千名受试者通过视频、可穿戴设备和雷达传感器记录的数据。通过分析，探讨了卷积神经网络、惯性传感器模型以及多传感器融合方法在年龄估算上的表现，并且对 OU-ISIR 大规模数据集中六万三千八百四十六个步态周期进行了分析，以量化年龄与步长、步速、步频、步距变异性和关节角度熵之间的相关性，并通过 Grad-CAM 解析了 ResNet34 模型关注的步态特征区域。最后，在 VersatileGait 数据库的一百万样本子集中，比较了支持向量机、决策树、随机森林、多层感知器和卷积神经网络的表现，评估模型的准确性和处理速度。

**Result:** 研究结果显示，卷积神经网络在年龄估算上的平均误差约为4.2年，惯性传感器模型约为4.5年，而多传感器融合方法的误差可低至3.4年，但在实验室数据和现实世界数据之间存在显著差异。年龄与步长、步速等五个关键指标间的相关系数至少为0.27。实验表明，深化神经网络可以达到96%的准确性，每个样本处理时间不到0.1秒。

**Conclusion:** 通过广泛的分析和实验，确立了步态年龄估算技术的性能基线，并提供了具体的实践指导，以期在未来研究中进一步减少步态年龄估算的误差。

**Abstract:** Estimating a person's age from their gait has important applications in
healthcare, security and human-computer interaction. In this work, we review
fifty-nine studies involving over seventy-five thousand subjects recorded with
video, wearable and radar sensors. We observe that convolutional neural
networks produce an average error of about 4.2 years, inertial-sensor models
about 4.5 years and multi-sensor fusion as low as 3.4 years, with notable
differences between lab and real-world data. We then analyse sixty-three
thousand eight hundred forty-six gait cycles from the OU-ISIR Large-Population
dataset to quantify correlations between age and five key metrics: stride
length, walking speed, step cadence, step-time variability and joint-angle
entropy, with correlation coefficients of at least 0.27. Next, we fine-tune a
ResNet34 model and apply Grad-CAM to reveal that the network attends to the
knee and pelvic regions, consistent with known age-related gait changes.
Finally, on a one hundred thousand sample subset of the VersatileGait database,
we compare support vector machines, decision trees, random forests, multilayer
perceptrons and convolutional neural networks, finding that deep networks
achieve up to 96 percent accuracy while processing each sample in under 0.1
seconds. By combining a broad meta-analysis with new large-scale experiments
and interpretable visualizations, we establish solid performance baselines and
practical guidelines for reducing gait-age error below three years in
real-world scenarios.

</details>


### [14] [What cat is that? A re-id model for feral cats](https://arxiv.org/abs/2507.11575)
*Victor Caquilpan*

Main category: cs.CV

> 研究通过修改PPGNet模型为PPGNet-Cat来识别野猫个体，使用对比学习方法使其在野猫重识别中表现出色（mAP 0.86，rank-1准确率0.95），展示了其在生态系统保护中的应用潜力。

<details>
  <summary>Details</summary>

**Motivation:** 野猫对澳大利亚野生动物造成显著损害，使它们成为世界上最危险的入侵物种之一。因此，密切监控这些野猫对于减少其影响是至关重要的工作。

**Method:** 本研究主要采用了对部分姿态引导网络(PPGNet)进行修改的方法，使其适应于野猫个体识别的应用，生成了PPGNet-Cat模型。同时，研究还采用了对比学习方法如ArcFace损失函数进行了多种实验。

**Result:** 主要结果显示，PPGNet-Cat模型在野猫个体识别中表现出色，以平均精度均值(mAP)0.86和第一排名准确率0.95的高绩效，确立了该模型在重识别领域的竞争力。

**Conclusion:** 研究结果表明，通过修改和优化PPGNet模型为PPGNet-Cat，可以有效提高野猫个体识别的精度和效率，有助于更好地监控野猫活动，减少其对生态系统的影响。

**Abstract:** Feral cats exert a substantial and detrimental impact on Australian wildlife,
placing them among the most dangerous invasive species worldwide. Therefore,
closely monitoring these cats is essential labour in minimising their effects.
In this context, the potential application of Re-Identification (re-ID) emerges
to enhance monitoring activities for these animals, utilising images captured
by camera traps. This project explores different CV approaches to create a
re-ID model able to identify individual feral cats in the wild. The main
approach consists of modifying a part-pose guided network (PPGNet) model,
initially used in the re-ID of Amur tigers, to be applicable for feral cats.
This adaptation, resulting in PPGNet-Cat, which incorporates specific
modifications to suit the characteristics of feral cats images. Additionally,
various experiments were conducted, particularly exploring contrastive learning
approaches such as ArcFace loss. The main results indicate that PPGNet-Cat
excels in identifying feral cats, achieving high performance with a mean
Average Precision (mAP) of 0.86 and a rank-1 accuracy of 0.95. These outcomes
establish PPGNet-Cat as a competitive model within the realm of re-ID.

</details>


### [15] [SketchDNN: Joint Continuous-Discrete Diffusion for CAD Sketch Generation](https://arxiv.org/abs/2507.11579)
*Sathvik Chereddy,John Femiani*

Main category: cs.CV

> 本文提出了SketchDNN模型，利用高斯-Softmax扩散过程，解决了CAD草图生成中的异质性和置换不变性问题，提高了生成质量，达到了新的最先进水平。

<details>
  <summary>Details</summary>

**Motivation:** 这种建模方式解决两个关键挑战：CAD草图中图元参数化的异质性和图元的置换不变性。

**Method:** 我们提出了SketchDNN，这是一种通过统一的连续-离散扩散过程来共同建模连续参数和离散类别标签的生成模型。核心创新在于使用高斯-Softmax扩散，其中通过高斯噪声扰动的 logits 经过softmax转换后投影到概率单纯形，从而为离散变量提供混合类别标签。

**Result:** 我们的方法在生成质量上有了显著提高，将Frechet Inception Distance（FID）从16.04降到7.80，将负对数似然（NLL）从84.8降低到81.33，在SketchGraphs数据集上建立了新的生成CAD草图的最先进水平。

**Conclusion:** 通过这种方法，我们验证了高斯-Softmax扩散过程在同时处理离散和连续参数时的有效性，并展示了它在CAD草图生成中的优越性能。

**Abstract:** We present SketchDNN, a generative model for synthesizing CAD sketches that
jointly models both continuous parameters and discrete class labels through a
unified continuous-discrete diffusion process. Our core innovation is
Gaussian-Softmax diffusion, where logits perturbed with Gaussian noise are
projected onto the probability simplex via a softmax transformation,
facilitating blended class labels for discrete variables. This formulation
addresses 2 key challenges, namely, the heterogeneity of primitive
parameterizations and the permutation invariance of primitives in CAD sketches.
Our approach significantly improves generation quality, reducing Fr\'echet
Inception Distance (FID) from 16.04 to 7.80 and negative log-likelihood (NLL)
from 84.8 to 81.33, establishing a new state-of-the-art in CAD sketch
generation on the SketchGraphs dataset.

</details>


### [16] [Interpretable Prediction of Lymph Node Metastasis in Rectal Cancer MRI Using Variational Autoencoders](https://arxiv.org/abs/2507.11638)
*Benjamin Keel,Aaron Quyn,David Jayne,Maryam Mohsin,Samuel D. Relton*

Main category: cs.CV

> 本文研究使用变分自编码器（VAE）替代大预训练的卷积神经网络（CNN），改进了MRI图像中淋巴结转移（LNM）的诊断准确性，达到了最先进的性能。

<details>
  <summary>Details</summary>

**Motivation:** 由于基于淋巴结（LN）大小、形状和纹理特征的放射学标准的诊断准确性有限，本文旨在改进现有模型的诊断性能。VAE的使用目的在于，其生成模型旨在重建图像，从而直接编码数据中的视觉特征和有意义的模式，从而得到一个更可解释的、解缠的、结构化的潜在空间。

**Method:** 本文使用变分自编码器（VAE）替换现有的大型预训练卷积神经网络（CNN），作为MRI图像中的淋巴结特征提取模型，旨在提高直肠癌淋巴结转移（LNM）的诊断准确性。

**Result:** 提出的VAE-MLP模型在使用168名未经新辅助治疗患者的内部MRI数据集中，达到了最先进的性能，其交叉验证指标为AUC 0.86 +/- 0.05，灵敏度0.79 +/- 0.06，特异性0.85 +/- 0.05。

**Conclusion:** 实验表明，VAE模型在淋巴结转移过程中能有效提高诊断准确性，结构化和可解释的潜在空间有助于理解数据中的复杂模式。

**Abstract:** Effective treatment for rectal cancer relies on accurate lymph node
metastasis (LNM) staging. However, radiological criteria based on lymph node
(LN) size, shape and texture morphology have limited diagnostic accuracy. In
this work, we investigate applying a Variational Autoencoder (VAE) as a feature
encoder model to replace the large pre-trained Convolutional Neural Network
(CNN) used in existing approaches. The motivation for using a VAE is that the
generative model aims to reconstruct the images, so it directly encodes visual
features and meaningful patterns across the data. This leads to a disentangled
and structured latent space which can be more interpretable than a CNN. Models
are deployed on an in-house MRI dataset with 168 patients who did not undergo
neo-adjuvant treatment. The post-operative pathological N stage was used as the
ground truth to evaluate model predictions. Our proposed model 'VAE-MLP'
achieved state-of-the-art performance on the MRI dataset, with cross-validated
metrics of AUC 0.86 +/- 0.05, Sensitivity 0.79 +/- 0.06, and Specificity 0.85
+/- 0.05. Code is available at:
https://github.com/benkeel/Lymph_Node_Classification_MIUA.

</details>


### [17] [Posture-Driven Action Intent Inference for Playing style and Fatigue Assessment](https://arxiv.org/abs/2507.11642)
*Abhishek Jaiswal,Nisheeth Srivastava*

Main category: cs.CV

> A posture-based technique achieves high accuracy in inferring shot intent in cricket, suggesting broad applications in human behavior analysis across multiple domains.

<details>
  <summary>Details</summary>

**Motivation:** The research is motivated by the potential of posture-based mental state inference for diagnosing fatigue, preventing injury, and enhancing performance in various fields, while addressing challenges of acquiring and managing human subject data.

**Method:** The study proposes a posture-based method for inferring human intent from activity videos in the context of cricket, aiming to differentiate between aggressive and defensive shot intent using motion analysis.

**Result:** The method achieves over 75% F1 score and over 80% AUC-ROC in identifying aggressive or defensive shot intent in cricket players based on posture analysis.

**Conclusion:** The research highlights that posture can reveal strong signals for intent inference, even with data noise. It further demonstrates the feasibility of using existing data statistics as weak supervision to validate findings and overcomes limitations of data labeling in sports analytics and potentially in the analysis of human behavior in other fields.

**Abstract:** Posture-based mental state inference has significant potential in diagnosing
fatigue, preventing injury, and enhancing performance across various domains.
Such tools must be research-validated with large datasets before being
translated into practice. Unfortunately, such vision diagnosis faces serious
challenges due to the sensitivity of human subject data. To address this, we
identify sports settings as a viable alternative for accumulating data from
human subjects experiencing diverse emotional states. We test our hypothesis in
the game of cricket and present a posture-based solution to identify human
intent from activity videos. Our method achieves over 75\% F1 score and over
80\% AUC-ROC in discriminating aggressive and defensive shot intent through
motion analysis. These findings indicate that posture leaks out strong signals
for intent inference, even with inherent noise in the data pipeline.
Furthermore, we utilize existing data statistics as weak supervision to
validate our findings, offering a potential solution for overcoming data
labelling limitations. This research contributes to generalizable techniques
for sports analytics and also opens possibilities for applying human behavior
analysis across various fields.

</details>


### [18] [VISTA: Monocular Segmentation-Based Mapping for Appearance and View-Invariant Global Localization](https://arxiv.org/abs/2507.11653)
*Hannah Shafferman,Annika Thomas,Jouko Kinnari,Michael Ricard,Jose Nino,Jonathan How*

Main category: cs.CV

> 提出VISTA框架，用于解决非结构化环境中的全局定位问题，特别是在面对视角和季节变化时提高定位准确性，并减少了存储需求。

<details>
  <summary>Details</summary>

**Motivation:** 解决由于视角变化、季节变化、空间别名和遮挡等引起的外观变化，这些是传统地点识别方法的失败模式，特别是在非结构化环境中进行自主导航时的全局定位问题。

**Method:** VISTA（视图不变分割追踪框架）结合基于对象的分割和追踪前端流程，以及利用环境地图几何一致性的子地图对应搜索后端流程。

**Result:** 在季节性和倾斜角度的航空数据集上进行评估，VISTA取得了比基线方法高69%的召回率提升，且只占最节省内存基线方法0.6%的存储空间。

**Conclusion:** VISTA提出了一种新颖的、开集的、单目全局定位框架，适用于实时处理资源受限的平台，能够在不同视角和季节变化下保持一致的定位效果。

**Abstract:** Global localization is critical for autonomous navigation, particularly in
scenarios where an agent must localize within a map generated in a different
session or by another agent, as agents often have no prior knowledge about the
correlation between reference frames. However, this task remains challenging in
unstructured environments due to appearance changes induced by viewpoint
variation, seasonal changes, spatial aliasing, and occlusions -- known failure
modes for traditional place recognition methods. To address these challenges,
we propose VISTA (View-Invariant Segmentation-Based Tracking for Frame
Alignment), a novel open-set, monocular global localization framework that
combines: 1) a front-end, object-based, segmentation and tracking pipeline,
followed by 2) a submap correspondence search, which exploits geometric
consistencies between environment maps to align vehicle reference frames. VISTA
enables consistent localization across diverse camera viewpoints and seasonal
changes, without requiring any domain-specific training or finetuning. We
evaluate VISTA on seasonal and oblique-angle aerial datasets, achieving up to a
69% improvement in recall over baseline methods. Furthermore, we maintain a
compact object-based map that is only 0.6% the size of the most
memory-conservative baseline, making our approach capable of real-time
implementation on resource-constrained platforms.

</details>
