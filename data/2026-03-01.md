<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 7]
- [cs.CV](#cs.CV) [Total: 3]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Decoder-based Sense Knowledge Distillation](https://arxiv.org/abs/2602.22351)
*Qitong Wang,Mohammed J. Zaki,Georgios Kollias,Vasileios Kalantzis*

Main category: cs.CL

> 本文提出了一种新颖的框架DSKD，用以解决在解码器式语言模型中加入词汇知识的挑战，并通过实验验证了其有效性。

<details>
  <summary>Details</summary>

**Motivation:** 虽然大语言模型能捕捉丰富的语义信息，但在结构化的词汇知识（如词汇意义和关系）方面常有不足。以往研究表明，在编码模型的知识蒸馏中加入词典能够提高效果，但在解码器作为生成模型的应用上还存在挑战。

**Method:** 本文提出了Decoder-based Sense Knowledge Distillation (DSKD)框架，该框架能够在不需字典查询的情况下在解码器式LLM的训练中集成词汇资源。

**Result:** 广泛的实验结果表明，DSKD显著提升了解码器的知识蒸馏性能，使得生成模型能够继承结构化的语义，同时保持高效的训练。

**Conclusion:** 本研究证明了DSKD框架在解码器式LLM的训练中的有效性，成功继承了结构化的词汇知识，同时保持了模型训练的效率。

**Abstract:** Large language models (LLMs) learn contextual embeddings that capture rich semantic information, yet they often overlook structured lexical knowledge such as word senses and relationships. Prior work has shown that incorporating sense dictionaries can improve knowledge distillation for encoder models, but their application to decoder as generative models remains challenging. In this paper, we introduce Decoder-based Sense Knowledge Distillation (DSKD), a framework that integrates lexical resources into the training of decoder-style LLMs without requiring dictionary lookup at inference time. Extensive experiments on diverse benchmarks demonstrate that DSKD significantly enhances knowledge distillation performance for decoders, enabling generative models to inherit structured semantics while maintaining efficient training.

</details>


### [2] [Scaling In, Not Up? Testing Thick Citation Context Analysis with GPT-5 and Fragile Prompts](https://arxiv.org/abs/2602.22359)
*Arno Simons*

Main category: cs.CL

> 该论文测试了大型语言模型（LLMs）在解释性引用上下文分析中的表现，通过调整提示结构和框架，GPT-5模型能够重构出一系列可能的解读，但提示的选择影响了解读的方向和词汇。

<details>
  <summary>Details</summary>

**Motivation:** 本研究的动机是检验大型语言模型在从单个复杂案例的文本深度阅读中进行解释性引用上下文分析（CCA）的能力，而不是通过扩展类型标签以概要方式扩大研究范围。

**Method:** 该论文采用了一种双阶段的GPT-5流水线方法来测试大型语言模型（LLMs）是否能支持解释性引用上下文分析（CCA）。具体来说，它通过调整提示结构和框架在2x3设计中研究提示敏感性分析。研究使用了Chubin和Moitra（1975年）的脚注6和Gilbert（1977年）的重构作为探测，并采取了引用文本的表面分类和期望通过，随后进行跨文档的解释性重构。

**Result:** 通过90次重构，该模型产生了450种不同的假设。细致的阅读和归纳编码识别出了21个反复出现的解释性步骤。线性概率模型估计提示选择如何改变这些步骤的频率和词汇库。模型表明，在重构时，引文被归类为“辅助性”，但框架和示例可以重新分配注意力和词汇，有时会导向过度解读。

**Conclusion:** 研究结果展示了使用大型语言模型作为指导型共同分析师进行可检视和可讨论的解释性引用上下文分析（CCA）的可能性和风险，并且提示结构和框架的选择系统性地影响了模型选择的合理解释和词汇。

**Abstract:** This paper tests whether large language models (LLMs) can support interpretative citation context analysis (CCA) by scaling in thick, text-grounded readings of a single hard case rather than scaling up typological labels. It foregrounds prompt-sensitivity analysis as a methodological issue by varying prompt scaffolding and framing in a balanced 2x3 design. Using footnote 6 in Chubin and Moitra (1975) and Gilbert's (1977) reconstruction as a probe, I implement a two-stage GPT-5 pipeline: a citation-text-only surface classification and expectation pass, followed by cross-document interpretative reconstruction using the citing and cited full texts. Across 90 reconstructions, the model produces 450 distinct hypotheses. Close reading and inductive coding identify 21 recurring interpretative moves, and linear probability models estimate how prompt choices shift their frequencies and lexical repertoire. GPT-5's surface pass is highly stable, consistently classifying the citation as "supplementary". In reconstruction, the model generates a structured space of plausible alternatives, but scaffolding and examples redistribute attention and vocabulary, sometimes toward strained readings. Relative to Gilbert, GPT-5 detects the same textual hinges yet more often resolves them as lineage and positioning than as admonishment. The study outlines opportunities and risks of using LLMs as guided co-analysts for inspectable, contestable interpretative CCA, and it shows that prompt scaffolding and framing systematically tilt which plausible readings and vocabularies the model foregrounds.

</details>


### [3] [Detecting Hate and Inflammatory Content in Bengali Memes: A New Multimodal Dataset and Co-Attention Framework](https://arxiv.org/abs/2602.22391)
*Rakib Ullah,Mominul islam,Md Sanjid Hossain,Md Ismail Hossain*

Main category: cs.CL

> 该研究提出了一种名为Bn-HIB的新数据集，其中包括3247个手工标注的孟加拉语表情包，分为了良性、仇恨言论和煽动性内容三类，并提出了MCFM模型来准确分类这些内容，实验表明该模型在Bn-HIB数据集上的表现优于已有模型。

<details>
  <summary>Details</summary>

**Motivation:** 互联网表情包已成为社交媒体的主要表达形式，尤其是在孟语社区。尽管表情包常常是幽默的，但它们也可能被用来传播针对个人和群体的有害和煽动性内容。由于其讽刺性、微妙性和文化特定性，检测这类内容极其具有挑战性，对于孟加拉语等资源贫乏的语言来说这个问题尤为严重。

**Method:** 介绍了一种名为MCFM（多模态共注意融合模型）的架构，该模型能够同时分析表情包的视觉和文本元素，通过共注意机制来识别并融合来自两种模态的关键特征，以实现更准确的分类。

**Result:** 实验结果表明，MCFM在Bn-HIB数据集上显著优于几种最先进的模型，证明了其在这一复杂任务中的有效性。

**Conclusion:** 该研究是第一个在孟加拉语表情包中区分煽动性内容与直接仇恨言论的研究。提出的MCFM模型通过共注意机制从视觉和文本元素中提取关键信息，进行有效的分类。

**Abstract:** Internet memes have become a dominant form of expression on social media, including within the Bengali-speaking community. While often humorous, memes can also be exploited to spread offensive, harmful, and inflammatory content targeting individuals and groups. Detecting this type of content is excep- tionally challenging due to its satirical, subtle, and culturally specific nature. This problem is magnified for low-resource lan- guages like Bengali, as existing research predominantly focuses on high-resource languages. To address this critical research gap, we introduce Bn-HIB (Bangla Hate Inflammatory Benign), a novel dataset containing 3,247 manually annotated Bengali memes categorized as Benign, Hate, or Inflammatory. Significantly, Bn- HIB is the first dataset to distinguish inflammatory content from direct hate speech in Bengali memes. Furthermore, we propose the MCFM (Multi-Modal Co-Attention Fusion Model), a simple yet effective architecture that mutually analyzes both the visual and textual elements of a meme. MCFM employs a co-attention mechanism to identify and fuse the most critical features from each modality, leading to a more accurate classification. Our experiments show that MCFM significantly outperforms several state-of-the-art models on the Bn-HIB dataset, demonstrating its effectiveness in this nuanced task.Warning: This work contains material that may be disturbing to some audience members. Viewer discretion is advised.

</details>


### [4] [SAFARI: A Community-Engaged Approach and Dataset of Stereotype Resources in the Sub-Saharan African Context](https://arxiv.org/abs/2602.22404)
*Aishwarya Verma,Laud Ammah,Olivia Nercy Ndlovu Lucas,Andrew Zaldivar,Vinodkumar Prabhakaran,Sunipa Dev*

Main category: cs.CL

> 该研究开发了一个多语言的刻板印象资源，涵盖了四个撒哈拉以南非洲国家：加纳、肯尼亚、尼日利亚和南非。

<details>
  <summary>Details</summary>

**Motivation:** 当前的刻板印象资料库在全球范围内的覆盖率不足，迫切需要优先进行有针对性的扩展，而不是仅仅增加数据量。

**Method:** 通过使用社会文化定位和社区参与的方法，包括以母语进行的电话调查，建立了一种可重现的方法，这种方法对地区的复杂语言多样性和口语传统敏感。

**Result:** 最终建立了包含15种本土语言的3,206种刻板印象和英语的3,534种刻板印象的数据集。

**Conclusion:** 本研究确立了一种敏感于地区复杂语言多样性及传统口语的可重现的方法，并在广泛的民族和人口背景中进行样本平衡，以确保广泛的覆盖范围。

**Abstract:** Stereotype repositories are critical to assess generative AI model safety, but currently lack adequate global coverage. It is imperative to prioritize targeted expansion, strategically addressing existing deficits, over merely increasing data volume. This work introduces a multilingual stereotype resource covering four sub-Saharan African countries that are severely underrepresented in NLP resources: Ghana, Kenya, Nigeria, and South Africa. By utilizing socioculturally-situated, community-engaged methods, including telephonic surveys moderated in native languages, we establish a reproducible methodology that is sensitive to the region's complex linguistic diversity and traditional orality. By deliberately balancing the sample across diverse ethnic and demographic backgrounds, we ensure broad coverage, resulting in a dataset of 3,534 stereotypes in English and 3,206 stereotypes across 15 native languages.

</details>


### [5] [Causality $\neq$ Invariance: Function and Concept Vectors in LLMs](https://arxiv.org/abs/2602.22424)
*Gustaw Opiełka,Hannes Rosenbusch,Claire E. Stevenson*

Main category: cs.CL

> 本文研究了LLMs是否独立于输入格式表示概念，发现虽然功能向量（FVs）在相同概念下的不同输入格式下几乎正交，但新提出的概念向量（CVs）则跨不同输入格式和语言展示了更好的泛化能力。

<details>
  <summary>Details</summary>

**Motivation:** 文章旨在探讨大型语言模型（LLMs）是否以抽象的形式表示概念，即独立于输入格式。为了解决这个问题，作者回顾了功能向量（FVs），一种在上下文学习任务中因果驱动性能的紧凑表示方法。

**Method:** 本文提出了一种新的向量表示方法——概念向量（CVs），它能够更稳定地表示概念。CVs通过注意力头的输出组成，这些头是通过表示相似性分析（RSA）选择的，基于它们是否能跨输入格式一致地编码概念。

**Result:** 研究发现，功能向量（FVs）在不同输入格式下的几乎正交，即使它们指向相同的概念。然而，概念向量（CVs）在不同的输入格式和语言上具有更好的泛化能力。

**Conclusion:** 研究结果显示，LLMs确实包含抽象的概念表示，但与驱动上下文学习性能的表示不同。

**Abstract:** Do large language models (LLMs) represent concepts abstractly, i.e., independent of input format? We revisit Function Vectors (FVs), compact representations of in-context learning (ICL) tasks that causally drive task performance. Across multiple LLMs, we show that FVs are not fully invariant: FVs are nearly orthogonal when extracted from different input formats (e.g., open-ended vs. multiple-choice), even if both target the same concept. We identify Concept Vectors (CVs), which carry more stable concept representations. Like FVs, CVs are composed of attention head outputs; however, unlike FVs, the constituent heads are selected using Representational Similarity Analysis (RSA) based on whether they encode concepts consistently across input formats. While these heads emerge in similar layers to FV-related heads, the two sets are largely distinct, suggesting different underlying mechanisms. Steering experiments reveal that FVs excel in-distribution, when extraction and application formats match (e.g., both open-ended in English), while CVs generalize better out-of-distribution across both question types (open-ended vs. multiple-choice) and languages. Our results show that LLMs do contain abstract concept representations, but these differ from those that drive ICL performance.

</details>


### [6] [A Fusion of context-aware based BanglaBERT and Two-Layer Stacked LSTM Framework for Multi-Label Cyberbullying Detection](https://arxiv.org/abs/2602.22449)
*Mirza Raquib,Asif Pervez Polok,Kedar Nath Biswas,Rahat Uddin Azad,Saydul Akbar Murad,Nick Rahimi*

Main category: cs.CL

> The paper proposes a fusion model combining BanglaBERT-Large and LSTM for multilabel cyberbullying detection in Bangla, addressing the limitations of single-label classification and language-specific challenges.

<details>
  <summary>Details</summary>

**Motivation:** To effectively detect cyberbullying that involves multiple forms of abuse in comments, especially in low-resource languages like Bangla, where robust models are few.

**Method:** A model that combines BanglaBERT-Large for contextual understanding with a two-layer stacked LSTM for temporal flow is proposed. The model is fine-tuned on a multilabel Bangla cyberbullying dataset.

**Result:** The model's performance is evaluated using various metrics including accuracy, precision, recall, F1-score, Hamming loss, Cohen's kappa, and AUC-ROC. 5-fold cross-validation is used to assess the model's generalization.

**Conclusion:** The proposed fusion architecture demonstrates a viable approach for multilabel cyberbullying detection, balancing semantic and contextual understanding with temporal dependencies, particularly in low-resource languages like Bangla.

**Abstract:** Cyberbullying has become a serious and growing concern in todays virtual world. When left unnoticed, it can have adverse consequences for social and mental health. Researchers have explored various types of cyberbullying, but most approaches use single-label classification, assuming that each comment contains only one type of abuse. In reality, a single comment may include overlapping forms such as threats, hate speech, and harassment. Therefore, multilabel detection is both realistic and essential. However, multilabel cyberbullying detection has received limited attention, especially in low-resource languages like Bangla, where robust pre-trained models are scarce. Developing a generalized model with moderate accuracy remains challenging. Transformers offer strong contextual understanding but may miss sequential dependencies, while LSTM models capture temporal flow but lack semantic depth. To address these limitations, we propose a fusion architecture that combines BanglaBERT-Large with a two-layer stacked LSTM. We analyze their behavior to jointly model context and sequence. The model is fine-tuned and evaluated on a publicly available multilabel Bangla cyberbullying dataset covering cyberbully, sexual harassment, threat, and spam. We apply different sampling strategies to address class imbalance. Evaluation uses multiple metrics, including accuracy, precision, recall, F1-score, Hamming loss, Cohens kappa, and AUC-ROC. We employ 5-fold cross-validation to assess the generalization of the architecture.

</details>


### [7] [Bridging Latent Reasoning and Target-Language Generation via Retrieval-Transition Heads](https://arxiv.org/abs/2602.22453)
*Shaswat Patel,Vishvesh Trivedi,Yue Han,Yihuai Hong,Eunsol Choi*

Main category: cs.CL

> 本文研究了多语言环境中的注意力头，特别是发现跨语言转换头（RTH）对链式思维推理的重要性，并在多个基准测试和模型中验证了这些发现。

<details>
  <summary>Details</summary>

**Motivation:** 最近的研究将Transformer中的注意力头分为检索头，本论文动机是在多语言环境下进一步研究这一发现，特别是在跨语言设置中的应用。

**Method:** 通过研究多语言环境中的检索头，本文识别出跨语言转换头（RTH），这些头控制着特定目标语言的输出转换。实验表明，RTH与检索头不同，并且对多语言大语言模型中的链式思维推理更加重要。

**Result:** 在四个多语言基准测试（MMLU-ProX、MGSM、MLQA和XQuaD）和两个模型家族（Qwen-2.5和Llama-3.1）中，屏蔽RTH比屏蔽检索头导致更大的性能下降。

**Conclusion:** 本文的工作通过隔离负责映射到目标语言的注意力头，推进了对多语言大语言模型的理解。

**Abstract:** Recent work has identified a subset of attention heads in Transformer as retrieval heads, which are responsible for retrieving information from the context. In this work, we first investigate retrieval heads in multilingual contexts. In multilingual language models, we find that retrieval heads are often shared across multiple languages. Expanding the study to cross-lingual setting, we identify Retrieval-Transition heads(RTH), which govern the transition to specific target-language output. Our experiments reveal that RTHs are distinct from retrieval heads and more vital for Chain-of-Thought reasoning in multilingual LLMs. Across four multilingual benchmarks (MMLU-ProX, MGSM, MLQA, and XQuaD) and two model families (Qwen-2.5 and Llama-3.1), we demonstrate that masking RTH induces bigger performance drop than masking Retrieval Heads (RH). Our work advances understanding of multilingual LMs by isolating the attention heads responsible for mapping to target languages.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [8] [Enabling clinical use of foundation models in histopathology](https://arxiv.org/abs/2602.22347)
*Audun L. Henriksen,Ole-Johan Skrede,Lisa van der Schee,Enric Domingo,Sepp De Raedt,Ilyá Kostolomov,Jennifer Hay,Karolina Cyll,Wanja Kildal,Joakim Kalsnes,Robert W. Williams,Manohar Pradhan,John Arne Nesheim,Hanne A. Askautrud,Maria X. Isaksen,Karmele Saez de Gordoa,Miriam Cuatrecasas,Joanne Edwards,TransSCOT group,Arild Nesbakken,Neil A. Shepherd,Ian Tomlinson,Daniel-Christoph Wagner,Rachel S. Kerr,Tarjei Sveinsgjerd Hveem,Knut Liestøl,Yoshiaki Nakamura,Marco Novelli,Masaaki Miyo,Sebastian Foersch,David N. Church,Miangela M. Lacle,David J. Kerr,Andreas Kleppe*

Main category: cs.CV

> 研究显示在计算病理学的基础模型训练中引入鲁棒性损失，可以改善模型对技术变化的鲁棒性，并提高预测准确性。

<details>
  <summary>Details</summary>

**Motivation:** 当前的模型在捕捉生物相关的特征时，也会捕获预分析和扫描仪特定的变化，这些变化可能会使从基础模型特征训练的任务特定模型的预测产生偏差，因此希望通过引入鲁棒性损失来提高模型的鲁棒性。

**Method:** 本研究通过在下游任务特定模型训练中引入新颖的鲁棒性损失，减少对技术变量的敏感性。

**Result:** 通过对27,042张WSIs（全玻片图像）的数据集进行实验，发现不仅鲁棒性有了显著的提高，同时由于更专注于生物相关的特征，预测准确性也提高了。

**Conclusion:** 该方法成功解决了计算病理学中基础模型的鲁棒性问题，无需重新训练基础模型，就可以适用于临床实践中真正数据的鲁棒计算病理学模型的开发。

**Abstract:** Foundation models in histopathology are expected to facilitate the development of high-performing and generalisable deep learning systems. However, current models capture not only biologically relevant features, but also pre-analytic and scanner-specific variation that bias the predictions of task-specific models trained from the foundation model features. Here we show that introducing novel robustness losses during training of downstream task-specific models reduces sensitivity to technical variability. A purpose-designed comprehensive experimentation setup with 27,042 WSIs from 6155 patients is used to train thousands of models from the features of eight popular foundation models for computational pathology. In addition to a substantial improvement in robustness, we observe that prediction accuracy improves by focusing on biologically relevant features. Our approach successfully mitigates robustness issues of foundation models for computational pathology without retraining the foundation models themselves, enabling development of robust computational pathology models applicable to real-world data in routine clinical practice.

</details>


### [9] [Optimizing Neural Network Architecture for Medical Image Segmentation Using Monte Carlo Tree Search](https://arxiv.org/abs/2602.22361)
*Liping Meng,Fan Nie,Yunyun Zhang,Chao Han*

Main category: cs.CV

> MNAS-Unet, combining MCTS and NAS, improves medical image segmentation efficiency and accuracy, decreasing resource usage and search time compared to NAS-Unet.

<details>
  <summary>Details</summary>

**Motivation:** The motivation for this paper is to develop a more efficient and accurate medical image segmentation framework that can reduce the resource requirements and search time compared to the existing methods.

**Method:** This paper proposes MNAS-Unet, a medical image segmentation framework using Monte Carlo Tree Search (MCTS) and Neural Architecture Search (NAS) to dynamically explore efficient network architectures and optimize structural units for faster and more precise model adjustments.

**Result:** MNAS-Unet outperforms NAS-Unet and other state-of-the-art models in segmentation accuracy across multiple medical image datasets while reducing the search budget and resource requirements.

**Conclusion:** The study concludes that MNAS-Unet can improve search efficiency while maintaining competitive segmentation accuracy, thus enhancing practical applicability under constrained resources.

**Abstract:** This paper proposes a novel medical image segmentation framework, MNAS-Unet, which combines Monte Carlo Tree Search (MCTS) and Neural Architecture Search (NAS). MNAS-Unet dynamically explores promising network architectures through MCTS, significantly enhancing the efficiency and accuracy of architecture search. It also optimizes the DownSC and UpSC unit structures, enabling fast and precise model adjustments. Experimental results demonstrate that MNAS-Unet outperforms NAS-Unet and other state-of-the-art models in segmentation accuracy on several medical image datasets, including PROMISE12, Ultrasound Nerve, and CHAOS. Furthermore, compared with NAS-Unet, MNAS-Unet reduces the architecture search budget by 54% (early stopping at 139 epochs versus 300 epochs under the same search setting), while achieving a lightweight model with only 0.6M parameters and lower GPU memory consumption, which further improves its practical applicability. These results suggest that MNAS-Unet can improve search efficiency while maintaining competitive segmentation accuracy under practical resource constraints.

</details>


### [10] [AeroDGS: Physically Consistent Dynamic Gaussian Splatting for Single-Sequence Aerial 4D Reconstruction](https://arxiv.org/abs/2602.22376)
*Hanyang Liu,Rongjun Qin*

Main category: cs.CV

> The paper presents AeroDGS, a physics-guided framework for 4D reconstruction in monocular aerial videos, which outperforms existing methods by addressing depth ambiguity and unstable motion estimation.

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to address the challenges of depth ambiguity and unstable motion estimation in monocular aerial 4D scene reconstruction, which arise due to single-view capture, wide spatial range, and dynamic objects with large motion disparities.

**Method:** The paper introduces AeroDGS, a physics-guided 4D Gaussian splatting framework for monocular UAV videos. It consists of a Monocular Geometry Lifting module to reconstruct static and dynamic geometry, and a Physics-Guided Optimization module that refines the reconstruction using differentiable physics priors.

**Result:** Experiments on both synthetic and real UAV scenes show that AeroDGS outperforms existing methods, achieving better reconstruction fidelity in dynamic aerial environments.

**Conclusion:** The conclusion is that AeroDGS effectively improves the robustness of dynamic aerial reconstruction by incorporating physical priors into the optimization process, leading to superior results over state-of-the-art techniques.

**Abstract:** Recent advances in 4D scene reconstruction have significantly improved dynamic modeling across various domains. However, existing approaches remain limited under aerial conditions with single-view capture, wide spatial range, and dynamic objects of limited spatial footprint and large motion disparity. These challenges cause severe depth ambiguity and unstable motion estimation, making monocular aerial reconstruction inherently ill-posed. To this end, we present AeroDGS, a physics-guided 4D Gaussian splatting framework for monocular UAV videos. AeroDGS introduces a Monocular Geometry Lifting module that reconstructs reliable static and dynamic geometry from a single aerial sequence, providing a robust basis for dynamic estimation. To further resolve monocular ambiguity, we propose a Physics-Guided Optimization module that incorporates differentiable ground-support, upright-stability, and trajectory-smoothness priors, transforming ambiguous image cues into physically consistent motion. The framework jointly refines static backgrounds and dynamic entities with stable geometry and coherent temporal evolution. We additionally build a real-world UAV dataset that spans various altitudes and motion conditions to evaluate dynamic aerial reconstruction. Experiments on synthetic and real UAV scenes demonstrate that AeroDGS outperforms state-of-the-art methods, achieving superior reconstruction fidelity in dynamic aerial environments.

</details>
