<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 38]
- [cs.CV](#cs.CV) [Total: 35]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [On LLM-Based Scientific Inductive Reasoning Beyond Equations](https://arxiv.org/abs/2509.16226)
*Brian S. Lin,Jiaxin Yuan,Zihan Zhou,Shouli Wang,Shuo Wang,Cunliang Kong,Qi Shi,Yuxuan Li,Liner Yang,Zhiyuan Liu,Maosong Sun*

Main category: cs.CL

> 本研究关注LLMs在全新环境下的归纳推理能力，提出科学归纳推理任务和基准，实验结果显示当前LLMs在这方面的挑战。

<details>
  <summary>Details</summary>

**Motivation:** 旨在解决现有LLMs在全新环境利用有限的例子归纳理解和应用模式的能力问题，特别是超越方程式的归纳推理能力。

**Method:** 提出任务LLM-Based Scientific Inductive Reasoning Beyond Equations并引入新的基准SIRBench-V1来评估LLMs在科学环境中的归纳推理能力。

**Result:** 实验结果显示当前的LLMs在新提出的科学归纳推理任务上仍然面临挑战。

**Conclusion:** 现有LLMs在科学环境中的归纳推理能力有待提高，需要在这一领域进行进一步的研究和进展。

**Abstract:** As large language models (LLMs) increasingly exhibit human-like capabilities,
a fundamental question emerges: How can we enable LLMs to learn the underlying
patterns from limited examples in entirely novel environments and apply them
effectively? This question is central to the ability of LLMs in inductive
reasoning. Existing research on LLM-based inductive reasoning can be broadly
categorized based on whether the underlying rules are expressible via explicit
mathematical equations. However, many recent studies in the beyond-equations
category have emphasized rule design without grounding them in specific
scenarios. Inspired by the parallels between inductive reasoning and human
scientific discovery, we propose the task of LLM-Based Scientific Inductive
Reasoning Beyond Equations and introduce a new benchmark, SIRBench-V1, to
evaluate the inductive reasoning abilities of LLMs in scientific settings. Our
experimental results show that current LLMs still struggle with this task,
underscoring its difficulty and the need for further advancement in this area.

</details>


### [2] [REAMS: Reasoning Enhanced Algorithm for Maths Solving](https://arxiv.org/abs/2509.16241)
*Eishkaran Singh,Tanav Singh Bajaj,Siddharth Nayak*

Main category: cs.CL

> 本文提出了一种使用零样本学习和数学推理解决高等数学问题的方法，实现了90.15%的准确率，比之前的81%有了显著提升。

<details>
  <summary>Details</summary>

**Motivation:** 传统的解决大学水平数学问题的方法一直表现不佳，特别是在MIT和哥伦比亚大学课程任务以及MATH数据集选定任务方面。这突出表明需要更先进的方法。

**Method:** 此论文介绍了一种基于语言的方法，结合零样本学习和数学推理来解决、解释和生成高等数学问题的解决方案。通过整合程序合成技术，该方法减少了对大规模训练数据的依赖并显著提高了问题解决的准确性。

**Result:** 该方法达到90.15%的准确率，比之前的基准81%有了显著提高。

**Conclusion:** 这些结果强调了先进的AI方法在解决最复杂的数学挑战方面的巨大潜力。

**Abstract:** The challenges of solving complex university-level mathematics problems,
particularly those from MIT, and Columbia University courses, and selected
tasks from the MATH dataset, remain a significant obstacle in the field of
artificial intelligence. Conventional methods have consistently fallen short in
this domain, highlighting the need for more advanced approaches. In this paper,
we introduce a language-based solution that leverages zero-shot learning and
mathematical reasoning to effectively solve, explain, and generate solutions
for these advanced math problems. By integrating program synthesis, our method
reduces reliance on large-scale training data while significantly improving
problem-solving accuracy. Our approach achieves an accuracy of 90.15%,
representing a substantial improvement over the previous benchmark of 81% and
setting a new standard in automated mathematical problem-solving. These
findings highlight the significant potential of advanced AI methodologies to
address and overcome the challenges presented by some of the most complex
mathematical courses and datasets.

</details>


### [3] [HausaMovieReview: A Benchmark Dataset for Sentiment Analysis in Low-Resource African Language](https://arxiv.org/abs/2509.16256)
*Asiya Ibrahim Zanga,Salisu Mamman Abdulrahman,Abubakar Ado,Abdulkadir Abubakar Bichi,Lukman Aliyu Jibril,Abdulmajid Babangida Umar,Alhassan Adamu,Shamsuddeen Hassan Muhammad,Bashir Salisu Abubakar*

Main category: cs.CL

> 通过创建新的Hausa评论数据集，提出经典模型决策树在低资源语言的情感分析中优于深度学习模型，强调特征工程的重要性。

<details>
  <summary>Details</summary>

**Motivation:** 解决低资源语言（如Hausa）NLP工具开发中缺乏标注数据的问题，提供基准数据集和研究基础。

**Method:** 开发了HausaMovieReview数据集，包含5000条YouTube评论，用以比较经典模型（逻辑回归、决策树、K近邻）和微调的Transformer模型（BERT和RoBERTa）的性能。

**Result:** 实验结果显示决策树分类器在准确率和F1得分上分别为89.72%和89.60%，显著优于深度学习模型。

**Conclusion:** 表明有效的特征工程能够让经典模型在低资源环境中达到前沿性能水平，为未来研究奠定基础。

**Abstract:** The development of Natural Language Processing (NLP) tools for low-resource
languages is critically hindered by the scarcity of annotated datasets. This
paper addresses this fundamental challenge by introducing HausaMovieReview, a
novel benchmark dataset comprising 5,000 YouTube comments in Hausa and
code-switched English. The dataset was meticulously annotated by three
independent annotators, demonstrating a robust agreement with a Fleiss' Kappa
score of 0.85 between annotators. We used this dataset to conduct a comparative
analysis of classical models (Logistic Regression, Decision Tree, K-Nearest
Neighbors) and fine-tuned transformer models (BERT and RoBERTa). Our results
reveal a key finding: the Decision Tree classifier, with an accuracy and
F1-score 89.72% and 89.60% respectively, significantly outperformed the deep
learning models. Our findings also provide a robust baseline, demonstrating
that effective feature engineering can enable classical models to achieve
state-of-the-art performance in low-resource contexts, thereby laying a solid
foundation for future research.
  Keywords: Hausa, Kannywood, Low-Resource Languages, NLP, Sentiment Analysis

</details>


### [4] [Gender and Political Bias in Large Language Models: A Demonstration Platform](https://arxiv.org/abs/2509.16264)
*Wenjie Lin,Hange Liu,Xutao Mao,Yingying Zhuang,Jingwei Shi,Xudong Han,Tianyu Shi,Jinrui Yang*

Main category: cs.CL

> ParlAI Vote是一个用于欧洲议会辩论和投票探索的交互系统，同时用于测试和展示LLMs在投票预测和偏见分析中的表现，强调系统性性能偏差，并支持研究、教育和公众参与。

<details>
  <summary>Details</summary>

**Motivation:** 目的是提供一个工具，使用户能够测试和分析LLMs在欧洲议会投票预测和偏见分析中的表现，并提供丰富的数据支持以促进研究和公众理解。

**Method:** ParlAI Vote是一个交互式系统，用于探索欧洲议会辩论和投票情况，并测试LLMs在投票预测和偏见分析上的能力。该系统将辩论主题、演讲和投票结果联系起来，并包括丰富的人口统计数据如性别、年龄、国家和地区政治团体。用户可以浏览辩论，查看链接的演讲，比较真实投票结果与前沿LLMs的预测，并查看按人口统计组划分的错误细分。

**Result:** 通过可视化EuroParlVote基准及其核心任务，即性别分类和投票预测，ParlAI Vote强调了现有前沿LLMs的系统性性能偏差。该系统统一了数据、模型和可视分析，并提供单一接口，降低了再现研究结果、审计行为和运行反事实场景的门槛。

**Conclusion:** ParlAI Vote支持了研究、教育和公众参与立法决策，并明确表达了当前LLMs在政治分析上的优势和局限。

**Abstract:** We present ParlAI Vote, an interactive system for exploring European
Parliament debates and votes, and for testing LLMs on vote prediction and bias
analysis. This platform connects debate topics, speeches, and roll-call
outcomes, and includes rich demographic data such as gender, age, country, and
political group. Users can browse debates, inspect linked speeches, compare
real voting outcomes with predictions from frontier LLMs, and view error
breakdowns by demographic group. Visualizing the EuroParlVote benchmark and its
core tasks of gender classification and vote prediction, ParlAI Vote highlights
systematic performance bias in state-of-the-art LLMs. The system unifies data,
models, and visual analytics in a single interface, lowering the barrier for
reproducing findings, auditing behavior, and running counterfactual scenarios.
It supports research, education, and public engagement with legislative
decision-making, while making clear both the strengths and the limitations of
current LLMs in political analysis.

</details>


### [5] [Language Modeling with Learned Meta-Tokens](https://arxiv.org/abs/2509.16278)
*Alok N. Shah,Khush Gupta,Keshav Ramji,Pratik Chaudhari*

Main category: cs.CL

> 研究通过预训练过程引入元标记和元注意力机制，改进了基于GPT-2架构的语言模型，结果表明这种方法能够显著提升模型在处理长距离依赖关系上的表现。

<details>
  <summary>Details</summary>

**Motivation:** 尽管现代基于转换器（Transformer）的语言模型在多任务泛化方面取得了重大成功，但往往难以捕捉到上下文窗口内的长距离相关性。因此，本研究提出了一种新颖的方法来解决这一问题。

**Method:** 本研究采用了一种新颖的方法，利用元标记（meta-tokens）和专门的元注意力机制来指导语言模型使用这些标记。在此基础上，使用了一种修改过的GPT-2架构进行了预训练，并且研究了这些标记在一系列合成任务上的影响。

**Result:** 研究发现，在预训练时使用少于100B标记的元标记和元注意力机制能够显著提升任务上的性能。通过观察模型内部以及使用信息论对压缩质量进行分析，进一步证实了这些行为。

**Conclusion:** 预训练中使用元标记能够提供一种简单且数据效率高的方法来提升长上下文语言模型的性能，并提出了一些关于长度泛化的模型行为的新见解。

**Abstract:** While modern Transformer-based language models (LMs) have achieved major
success in multi-task generalization, they often struggle to capture long-range
dependencies within their context window. This work introduces a novel approach
using meta-tokens, special tokens injected during pre-training, along with a
dedicated meta-attention mechanism to guide LMs to use these tokens. We
pre-train a language model with a modified GPT-2 architecture equipped with
meta-attention in addition to causal multi-head attention, and study the impact
of these tokens on a suite of synthetic tasks. We find that data-efficient
language model pre-training on fewer than 100B tokens utilizing meta-tokens and
our meta-attention mechanism achieves strong performance on these tasks after
fine-tuning. We suggest that these gains arise due to the meta-tokens
sharpening the positional encoding. This enables them to operate as trainable,
content-based landmarks, implicitly compressing preceding context and "caching"
it in the meta-token. At inference-time, the meta-token points to relevant
context, facilitating length generalization up to 2$\times$ its context window,
even after extension with YaRN. We provide further evidence of these behaviors
by visualizing model internals to study the residual stream, and assessing the
compression quality by information-theoretic analysis on the rate-distortion
tradeoff. Our findings suggest that pre-training LMs with meta-tokens offers a
simple, data-efficient method to enhance long-context language modeling
performance, while introducing new insights into the nature of their behavior
towards length generalization.

</details>


### [6] [Overhearing LLM Agents: A Survey, Taxonomy, and Roadmap](https://arxiv.org/abs/2509.16325)
*Andrew Zhu,Chris Callison-Burch*

Main category: cs.CL

> 本文探讨了一种新的AI交互方式——旁听代理，该代理通过监听环境并提供情境协助而非直接吸引用户注意来进行任务辅助。

<details>
  <summary>Details</summary>

**Motivation:** 动机在于提出一种无需直接与用户交互就能提供帮助的新模式，减少对用户的干扰，提高效率和体验。

**Method:** 建立旁听代理交互模式和任务的分类，通过调查前期的LLM代理工作和HCI研究来构建分类系统，并形成一套研究与开发最佳实践。

**Result:** 创建了旁听代理系统的交互分类系统及其任务清单，提供了针对该模式的研究及开发最佳实践。

**Conclusion:** 展示了旁听代理模式的独特价值，并指出了未来在此范式下的研究方向。

**Abstract:** Imagine AI assistants that enhance conversations without interrupting them:
quietly providing relevant information during a medical consultation,
seamlessly preparing materials as teachers discuss lesson plans, or
unobtrusively scheduling meetings as colleagues debate calendars. While modern
conversational LLM agents directly assist human users with tasks through a chat
interface, we study this alternative paradigm for interacting with LLM agents,
which we call "overhearing agents." Rather than demanding the user's attention,
overhearing agents continuously monitor ambient activity and intervene only
when they can provide contextual assistance. In this paper, we present the
first analysis of overhearing LLM agents as a distinct paradigm in human-AI
interaction and establish a taxonomy of overhearing agent interactions and
tasks grounded in a survey of works on prior LLM-powered agents and exploratory
HCI studies. Based on this taxonomy, we create a list of best practices for
researchers and developers building overhearing agent systems. Finally, we
outline the remaining research gaps and reveal opportunities for future
research in the overhearing paradigm.

</details>


### [7] [HARE: an entity and relation centric evaluation framework for histopathology reports](https://arxiv.org/abs/2509.16326)
*Yunsoo Kim,Michal W. S. Ong,Alex Shavick,Honghan Wu,Adam P. Levine*

Main category: cs.CL

> 论文提出了HARE，一个新颖的组织病理学报告自动评估框架，通过名称实体识别和关系抽取方法，提高了报告的临床质量评估。

<details>
  <summary>Details</summary>

**Motivation:** 在医学领域，自动化文本生成是一个活跃的研究和发展领域；然而，评估生成报告的临床质量仍然是一个挑战，尤其是在特定领域缺乏度量标准的情况下，如组织病理学。

**Method:** 该论文提出了HARE框架，包括基准数据集、实体识别模型（NER）、关系抽取模型（RE）和一个新颖的评估指标。HARE框架旨在通过将参考报告和生成报告之间的关键病理学实体和关系对齐来优先考虑临床上相关的内容。

**Result:** HARE-NER和HARE-RE在测试模型中实现了最高的F1得分0.915。HARE度量在与专家评估的关联性和回归效果上优于其他方法，提高了对报告质量的评估。

**Conclusion:** HARE度量表现优于传统的ROUGE和Meteor度量，以及放射学度量如RadGraph-XL，与专家评估的相关性更高，回归效果更好。

**Abstract:** Medical domain automated text generation is an active area of research and
development; however, evaluating the clinical quality of generated reports
remains a challenge, especially in instances where domain-specific metrics are
lacking, e.g. histopathology. We propose HARE (Histopathology Automated Report
Evaluation), a novel entity and relation centric framework, composed of a
benchmark dataset, a named entity recognition (NER) model, a relation
extraction (RE) model, and a novel metric, which prioritizes clinically
relevant content by aligning critical histopathology entities and relations
between reference and generated reports. To develop the HARE benchmark, we
annotated 813 de-identified clinical diagnostic histopathology reports and 652
histopathology reports from The Cancer Genome Atlas (TCGA) with domain-specific
entities and relations. We fine-tuned GatorTronS, a domain-adapted language
model to develop HARE-NER and HARE-RE which achieved the highest overall
F1-score (0.915) among the tested models. The proposed HARE metric outperformed
traditional metrics including ROUGE and Meteor, as well as radiology metrics
such as RadGraph-XL, with the highest correlation and the best regression to
expert evaluations (higher than the second best method, GREEN, a large language
model based radiology report evaluator, by Pearson $r = 0.168$, Spearman $\rho
= 0.161$, Kendall $\tau = 0.123$, $R^2 = 0.176$, $RMSE = 0.018$). We release
HARE, datasets, and the models at https://github.com/knowlab/HARE to foster
advancements in histopathology report generation, providing a robust framework
for improving the quality of reports.

</details>


### [8] [RephQA: Evaluating Readability of Large Language Models in Public Health Question Answering](https://arxiv.org/abs/2509.16360)
*Weikang Qiu,Tinglin Huang,Ryan Rullo,Yucheng Kuang,Ali Maatouk,S. Raquel Ramos,Rex Ying*

Main category: cs.CL

> 此研究提出了RephQA，用于评估LLM生成的公共健康问答的可读性，发现大多数模型不符合可读性标准，并探索了四种提升可读性的策略。

<details>
  <summary>Details</summary>

**Motivation:** 大多数先前研究主要集中在提高准确性与推理能力，但医疗代理在回答公共健康问题时的可读性同样重要，即需要能够清晰简洁地回答给非专业背景的人。

**Method:** 开发了一个涉及13个主题、27个来源的533个专家评审问答对的基准测试RephQA，以及两个可读性度量：Flesch-Kincaid年级水平和专业评分。并且评估了25种LLM，尝试了四种提升可读性的策略。

**Result:** 评估结果显示大多数LLM未能达到可读性标准。通过探索四种策略，发现令牌适应型GRPO取得了最佳效果。

**Conclusion:** 该研究展示了数据标准和可衡量方法的重要性，并表明令牌适应型GRPO可改进公共健康代理的实用性和易用性，有助于开发更加实际和用户友好的公共健康代理。

**Abstract:** Large Language Models (LLMs) hold promise in addressing complex medical
problems. However, while most prior studies focus on improving accuracy and
reasoning abilities, a significant bottleneck in developing effective
healthcare agents lies in the readability of LLM-generated responses,
specifically, their ability to answer public health problems clearly and simply
to people without medical backgrounds. In this work, we introduce RephQA, a
benchmark for evaluating the readability of LLMs in public health question
answering (QA). It contains 533 expert-reviewed QA pairs from 27 sources across
13 topics, and includes a proxy multiple-choice task to assess informativeness,
along with two readability metrics: Flesch-Kincaid grade level and professional
score. Evaluation of 25 LLMs reveals that most fail to meet readability
standards, highlighting a gap between reasoning and effective communication. To
address this, we explore four readability-enhancing strategies-standard
prompting, chain-of-thought prompting, Group Relative Policy Optimization
(GRPO), and a token-adapted variant. Token-adapted GRPO achieves the best
results, advancing the development of more practical and user-friendly public
health agents. These results represent a step toward building more practical
agents for public health.

</details>


### [9] [Whisper-UT: A Unified Translation Framework for Speech and Text](https://arxiv.org/abs/2509.16375)
*Cihan Xiao,Matthew Wiesner,Debashish Chakraborty,Reno Kriz,Keith Cunningham,Kenton Murray,Kevin Duh,Luis Tavarez-Arce,Paul McNamee,Sanjeev Khudanpur*

Main category: cs.CL

> 本文提出了一种名为Whisper-UT的跨模态和跨任务适应框架，可以实现多模态任务中的高效处理和性能提升。

<details>
  <summary>Details</summary>

**Motivation:** 旨在解决将编码器-解码器模型高效地适应不同一模态或多模态场景的挑战。

**Method:** 提出了一种名为Whisper-UT的统一且高效的框架，该框架利用轻量级适配器实现跨任务的无缝适应，包括多模态机器翻译（MMT）任务，该任务显式地基于语音和源语言文本输入进行翻译。通过将ASR假设或真实转录作为提示，这种方法不仅使系统能够同时处理多种模态，而且还通过两阶段解码策略提高了语音翻译（ST）的性能。

**Result:** 展示了在不使用三路平行数据的情况下通过跨模态和跨任务微调来提高性能的方法的有效性。

**Conclusion:** 实验证明了所提出的框架的灵活性、效率以及在多模态翻译中的广泛适用性。

**Abstract:** Encoder-decoder models have achieved remarkable success in speech and text
tasks, yet efficiently adapting these models to diverse uni/multi-modal
scenarios remains an open challenge. In this paper, we propose Whisper-UT, a
unified and efficient framework that leverages lightweight adapters to enable
seamless adaptation across tasks, including a multi-modal machine translation
(MMT) task that explicitly conditions translation on both speech and source
language text inputs. By incorporating ASR hypotheses or ground-truth
transcripts as prompts, this approach not only enables the system to process
both modalities simultaneously but also enhances speech translation (ST)
performance through a 2-stage decoding strategy. We demonstrate our methods
using the Whisper model, though in principle they are general and could be
applied to similar multitask models. We highlight the effectiveness of
cross-modal and cross-task fine-tuning, which improves performance without
requiring 3-way parallel data. Our results underscore the flexibility,
efficiency, and general applicability of the proposed framework for multi-modal
translation.

</details>


### [10] [Evaluating Behavioral Alignment in Conflict Dialogue: A Multi-Dimensional Comparison of LLM Agents and Humans](https://arxiv.org/abs/2509.16394)
*Deuksin Kwon,Kaleen Shrestha,Bin Han,Elena Hayoung Lee,Gale Lucas*

Main category: cs.CL

> 该研究通过模拟冲突对话评估了带有个性提示的大型语言模型在争端解决中的行为一致性，发现尽管某些模型在某些方面接近人类，但仍存在一致性差距。

<details>
  <summary>Details</summary>

**Motivation:** 大型语言模型（LLMs）越来越多地被部署在社交复杂度高的任务中，它们在情感和战略复杂环境中模仿人类行为的能力尚未得到充分探索。

**Method:** 通过模拟多回合冲突对话来评估个性提示的大型语言模型（LLMs）在对抗性争端解决中的行为一致性，这些对话还融入了谈判元素。每个LLM都依据匹配的五因素个性档案进行指导，以控制个体差异，增强真实感。

**Result:** GPT-4.1在语言风格和情感动态上最接近人类，而Claude-3.7-Sonnet在战略行为上表现最佳。不过，仍然存在显著的行为一致性差距。

**Conclusion:** 研究建立了大型语言模型与人类在社会复杂互动中行为一致性的基准，强调了个性条件化在对话建模中的潜力和局限。

**Abstract:** Large Language Models (LLMs) are increasingly deployed in socially complex,
interaction-driven tasks, yet their ability to mirror human behavior in
emotionally and strategically complex contexts remains underexplored. This
study assesses the behavioral alignment of personality-prompted LLMs in
adversarial dispute resolution by simulating multi-turn conflict dialogues that
incorporate negotiation. Each LLM is guided by a matched Five-Factor
personality profile to control for individual variation and enhance realism. We
evaluate alignment across three dimensions: linguistic style, emotional
expression (e.g., anger dynamics), and strategic behavior. GPT-4.1 achieves the
closest alignment with humans in linguistic style and emotional dynamics, while
Claude-3.7-Sonnet best reflects strategic behavior. Nonetheless, substantial
alignment gaps persist. Our findings establish a benchmark for alignment
between LLMs and humans in socially complex interactions, underscoring both the
promise and the limitations of personality conditioning in dialogue modeling.

</details>


### [11] ['Rich Dad, Poor Lad': How do Large Language Models Contextualize Socioeconomic Factors in College Admission ?](https://arxiv.org/abs/2509.16400)
*Huy Nghiem,Phuong-Anh Nguyen-Le,John Prindle,Rachel Rudinger,Hal Daumé III*

Main category: cs.CL

> 本研究通过大规模审计，揭示了大语言模型在处理与社会经济地位相关的大学录取决策时存在明显的偏向性。通过双过程框架（系统1和系统2），发现了这些模型在处理敏感信息时的潜在优势和不确定性。

<details>
  <summary>Details</summary>

**Motivation:** 尽管大语言模型（LLMs）在高风险领域中的应用越来越多，但它们在处理敏感社会决策方面的推理方式仍需深入研究。为解决此问题，本研究设计了一种新的框架来审查LLMs如何处理大学录取过程中的社会经济地位问题。

**Method:** 采用了一种受认知科学启发的双过程框架，对LLMs在大学招生决策中对社会经济地位（SES）的处理进行了大规模审查。使用基于现实世界相关性的30,000份申请人合成数据集来提示4个开源LLMs（Qwen 2, Mistral v0.3, Gemma 2, Llama 3.1），并分别在快速决策和慢速解释两种模式下进行测试。

**Result:** 测试结果表明，LLMs在大学录取决策中倾向于偏好低SES背景的申请人，即使在控制学术表现的情况下也是如此。慢速解释模式进一步放大了这一倾向，通过显式调用SES作为补偿性理由。

**Conclusion:** 研究结果表明，LLMs在处理大学录取中的SES问题时显示出其作为决策者的潜力和不确定性。为此，提出了一种名为DPAF的双过程审计框架，用于探测LLMs在敏感应用中的推理行为。

**Abstract:** Large Language Models (LLMs) are increasingly involved in high-stakes
domains, yet how they reason about socially sensitive decisions remains
underexplored. We present a large-scale audit of LLMs' treatment of
socioeconomic status (SES) in college admissions decisions using a novel
dual-process framework inspired by cognitive science. Leveraging a synthetic
dataset of 30,000 applicant profiles grounded in real-world correlations, we
prompt 4 open-source LLMs (Qwen 2, Mistral v0.3, Gemma 2, Llama 3.1) under 2
modes: a fast, decision-only setup (System 1) and a slower, explanation-based
setup (System 2). Results from 5 million prompts reveal that LLMs consistently
favor low-SES applicants -- even when controlling for academic performance --
and that System 2 amplifies this tendency by explicitly invoking SES as
compensatory justification, highlighting both their potential and volatility as
decision-makers. We then propose DPAF, a dual-process audit framework to probe
LLMs' reasoning behaviors in sensitive applications.

</details>


### [12] [Pico: A Modular Framework for Hypothesis-Driven Small Language Model Research](https://arxiv.org/abs/2509.16413)
*Richard Diehl Martinez,David Demitri Africa,Yuval Weiss,Suchir Salhan,Ryan Daniels,Paula Buttery*

Main category: cs.CL

> 文章介绍了一个轻量级框架Pico，用于系统化研究小规模和中等规模的语言模型，并提供了可重复实验的基线模型。

<details>
  <summary>Details</summary>

**Motivation:** 在开发小规模语言模型时，许多设计选择的不确定性仍然很大，而紧缩的参数预算使得每个决策都至关重要。研究人员仍然缺乏系统化的、科学的方法来测试和改进新想法。

**Method:** 介绍了Pico，一个轻量级、模块化的框架，用于小规模和中等规模语言模型开发的系统性研究。Pico包括两个库，提供了一个实际的沙箱，研究人员可以在此对模型架构或训练过程进行有针对性的修改，并直接观察其对模型行为的影响。

**Result:** 研究人员可以使用Pico进行可重复的实验。此外，还发布了一系列在标准化条件下训练的基线模型pico-decoder，并将其开源提供给社区。

**Conclusion:** 通过案例研究，展示了Pico如何支持小规模语言模型的迭代设计和分析。

**Abstract:** Building language models (LMs), especially small and medium ones, remains
more art than science. While large LMs often improve by sheer scale, it is
still unclear why many design choices work. For small LMs, this uncertainty is
more limiting: tight parameter budgets make each decision critical, yet
researchers still lack systematic, scientific ways to test and refine new
ideas.
  We introduce Pico, a lightweight, modular framework that enables systematic,
hypothesis-driven research for small and medium-scale language model
development. Pico consists of two libraries that together provide a practical
sandbox where researchers can make targeted changes to a model's architecture
or training procedures and directly observe their effects on the model's
behavior. To support reproducible experimentation, we also release a suite of
baseline models, pico-decoder, trained under standardized conditions and
open-sourced for the community. Case studies highlight how Pico can support
iterative small LM design and analysis.

</details>


### [13] [Evaluating CxG Generalisation in LLMs via Construction-Based NLI Fine Tuning](https://arxiv.org/abs/2509.16422)
*Tom Mackintosh,Harish Tayyar Madabushi,Claire Bonial*

Main category: cs.CL

> 研究大型语言模型的构式语法学习能力，发现这些模型在处理模式化构式时存在困难，提出了ConTest-NLI基准以评价这种学习能力。

<details>
  <summary>Details</summary>

**Motivation:** 评估大型语言模型在理解和学习构式语法中的深层形式-意义映射的能力，以揭示模型的抽象差距，并为未来的研究提供评价框架。

**Method:** 通过实验，研究了大型语言模型学习由构式语法定义的深层形式-意义映射的能力。设计了ConTest-NLI基准，该基准包含80,000个句子，涵盖了从高度词汇化到高度模式化的八个英语构式。通过模板和模型循环过滤器生成多样化的合成NLI三元组，确保挑战性和标签可靠性。

**Result:** 对主流大型语言模型的零样本测试显示，自然数据的准确率从88%下降到对抗数据的64%，模式化构式最难学习。在ConTest-NLI的子集上进行微调可提高9%的准确率。

**Conclusion:** 结果显示当前大型语言模型存在持续的抽象差距，但提供了基于构式的可扩展学习评价框架。

**Abstract:** We probe large language models' ability to learn deep form-meaning mappings
as defined by construction grammars. We introduce the ConTest-NLI benchmark of
80k sentences covering eight English constructions from highly lexicalized to
highly schematic. Our pipeline generates diverse synthetic NLI triples via
templating and the application of a model-in-the-loop filter. This provides
aspects of human validation to ensure challenge and label reliability.
Zero-shot tests on leading LLMs reveal a 24% drop in accuracy between
naturalistic (88%) and adversarial data (64%), with schematic patterns proving
hardest. Fine-tuning on a subset of ConTest-NLI yields up to 9% improvement,
yet our results highlight persistent abstraction gaps in current LLMs and offer
a scalable framework for evaluating construction-informed learning.

</details>


### [14] [PersonaMatrix: A Recipe for Persona-Aware Evaluation of Legal Summarization](https://arxiv.org/abs/2509.16449)
*Tsz Fung Pang,Maryam Berijanian,Thomas Orth,Breanna Shi,Charlotte S. Alexander*

Main category: cs.CL

> 本文介绍PersonaMatrix框架和DCI指标，用于评估法律案件摘要，考虑了法律专家和非专家的不同需求，以提高法律知识的可访问性。

<details>
  <summary>Details</summary>

**Motivation:** 现有的任务导向评估方法忽视了用户和利益相关者的差异化需求，本研究旨在开发一种新的工具，以满足不同用户的技术和可访问性需求。

**Method:** 介绍PersonaMatrix，一个评价框架，通过六种不同角色的视角对摘要进行评分，并引入了一个受控的维度变化的试点数据集以及Diversity-Coverage Index（DCI），用以揭示不同角色评价者之间的最优解。

**Result:** 提出了PersonaMatrix框架和Diversity-Coverage Index（DCI）指标，为法律AI摘要系统的优化提供了方法，增加法律知识的可访问性。

**Conclusion:** 本研究为优化针对专家和非专家用户群的法律AI摘要系统提供了新工具，并公开发放在GitHub的代码和数据有助于进一步研究。

**Abstract:** Legal documents are often long, dense, and difficult to comprehend, not only
for laypeople but also for legal experts. While automated document
summarization has great potential to improve access to legal knowledge,
prevailing task-based evaluators overlook divergent user and stakeholder needs.
Tool development is needed to encompass the technicality of a case summary for
a litigator yet be accessible for a self-help public researching for their
lawsuit. We introduce PersonaMatrix, a persona-by-criterion evaluation
framework that scores summaries through the lens of six personas, including
legal and non-legal users. We also introduce a controlled dimension-shifted
pilot dataset of U.S. civil rights case summaries that varies along depth,
accessibility, and procedural detail as well as Diversity-Coverage Index (DCI)
to expose divergent optima of legal summary between persona-aware and
persona-agnostic judges. This work enables refinement of legal AI summarization
systems for both expert and non-expert users, with the potential to increase
access to legal knowledge. The code base and data are publicly available in
GitHub.

</details>


### [15] [Implicit Behavioral Alignment of Language Agents in High-Stakes Crowd Simulations](https://arxiv.org/abs/2509.16457)
*Yunzhe Wang,Gale M. Lucas,Burcin Becerik-Gerber,Volkan Ustun*

Main category: cs.CL

> 本文提出了一个理论框架PEBA，并基于此框架开发了一个算法PersonaEvolve，用于优化生成代理的行为以更好地匹配现实世界的预期，特别是在模拟高风险社会事件中的表现。

<details>
  <summary>Details</summary>

**Motivation:** 语言驱动的生成性代理引发了大规模的社会模拟，但最近的研究表明，生成代理的行为常常与专家的预期和现实数据有偏差。为了应对这一问题，即所谓的“行为现实性差距”，我们提出了我们的理论和方法。

**Method:** 我们提出了一个名为Persona-Environment Behavioral Alignment (PEBA) 的理论框架，它是一个基于 Lewin 的行为方程式的分布匹配问题，该方程式表明行为是人和环境的函数。基于 PEBA，我们设计了 PersonaEvolve (PEvo)，这是一个基于 LLM 的优化算法，旨在迭代地优化代理的人格，并在指定的环境背景下，使其集体行为与现实专家基准相一致。

**Result:** 我们在一个主动射击事件模拟中验证了PEvo，实现了与未引导条件相比，84%的平均分布分歧减少，以及34%超过显式指令基线的改进。结果还表明，经过PEvo优化的人格特征在新型、相关的模拟场景中也能很好地泛化。

**Conclusion:** 我们的方法极大地提升了高风险社会模拟中的行为现实性和可靠性。更广泛地说，PEBA-PEvo框架为开发值得信赖的语言模型驱动的社会模拟提供了一个原则性的方法。

**Abstract:** Language-driven generative agents have enabled large-scale social simulations
with transformative uses, from interpersonal training to aiding global
policy-making. However, recent studies indicate that generative agent behaviors
often deviate from expert expectations and real-world data--a phenomenon we
term the Behavior-Realism Gap. To address this, we introduce a theoretical
framework called Persona-Environment Behavioral Alignment (PEBA), formulated as
a distribution matching problem grounded in Lewin's behavior equation stating
that behavior is a function of the person and their environment. Leveraging
PEBA, we propose PersonaEvolve (PEvo), an LLM-based optimization algorithm that
iteratively refines agent personas, implicitly aligning their collective
behaviors with realistic expert benchmarks within a specified environmental
context. We validate PEvo in an active shooter incident simulation we
developed, achieving an 84% average reduction in distributional divergence
compared to no steering and a 34% improvement over explicit instruction
baselines. Results also show PEvo-refined personas generalize to novel, related
simulation scenarios. Our method greatly enhances behavioral realism and
reliability in high-stakes social simulations. More broadly, the PEBA-PEvo
framework provides a principled approach to developing trustworthy LLM-driven
social simulations.

</details>


### [16] [Intrinsic Meets Extrinsic Fairness: Assessing the Downstream Impact of Bias Mitigation in Large Language Models](https://arxiv.org/abs/2509.16462)
*'Mina Arzaghi','Alireza Dehghanpour Farashah','Florian Carichon',' Golnoosh Farnadi'*

Main category: cs.CL

> 研究发现通过概念遗忘来减少LLMs的内在偏见可以显著提高下游任务的公平性，而不会降低准确性；

<details>
  <summary>Details</summary>

**Motivation:** 研究大型语言模型(LLMs)中的社会经济偏见如何影响下游任务的公平性；

**Method:** 通过概念遗忘来内在地减少偏见与通过反事实数据增强(CDA)来外在地减少偏见的方法对比，统一评估框架；

**Result:** 内在偏见减少至多94.9%，下游任务公平性指标（如人口统计学差异）提高至多82%；

**Conclusion:** 提供实践指导以将缓解努力集中在最有效的地方，强调在部署前应用早期缓解措施的重要性；

**Abstract:** Large Language Models (LLMs) exhibit socio-economic biases that can propagate
into downstream tasks. While prior studies have questioned whether intrinsic
bias in LLMs affects fairness at the downstream task level, this work
empirically investigates the connection. We present a unified evaluation
framework to compare intrinsic bias mitigation via concept unlearning with
extrinsic bias mitigation via counterfactual data augmentation (CDA). We
examine this relationship through real-world financial classification tasks,
including salary prediction, employment status, and creditworthiness
assessment. Using three open-source LLMs, we evaluate models both as frozen
embedding extractors and as fine-tuned classifiers. Our results show that
intrinsic bias mitigation through unlearning reduces intrinsic gender bias by
up to 94.9%, while also improving downstream task fairness metrics, such as
demographic parity by up to 82%, without compromising accuracy. Our framework
offers practical guidance on where mitigation efforts can be most effective and
highlights the importance of applying early-stage mitigation before downstream
deployment.

</details>


### [17] [Computational Analysis of Conversation Dynamics through Participant Responsivity](https://arxiv.org/abs/2509.16464)
*Margaret Hughes,Brandon Roy,Elinor Poole-Dayan,Deb Roy,Jad Kabbara*

Main category: cs.CL

> Error

<details>
  <summary>Details</summary>

**Motivation:** Error

**Method:** Error

**Result:** Error

**Conclusion:** Error

**Abstract:** Growing literature explores toxicity and polarization in discourse, with
comparatively less work on characterizing what makes dialogue prosocial and
constructive. We explore conversational discourse and investigate a method for
characterizing its quality built upon the notion of ``responsivity'' -- whether
one person's conversational turn is responding to a preceding turn. We develop
and evaluate methods for quantifying responsivity -- first through semantic
similarity of speaker turns, and second by leveraging state-of-the-art large
language models (LLMs) to identify the relation between two speaker turns. We
evaluate both methods against a ground truth set of human-annotated
conversations. Furthermore, selecting the better performing LLM-based approach,
we characterize the nature of the response -- whether it responded to that
preceding turn in a substantive way or not.
  We view these responsivity links as a fundamental aspect of dialogue but note
that conversations can exhibit significantly different responsivity structures.
Accordingly, we then develop conversation-level derived metrics to address
various aspects of conversational discourse. We use these derived metrics to
explore other conversations and show that they support meaningful
characterizations and differentiations across a diverse collection of
conversations.

</details>


### [18] [The Oracle Has Spoken: A Multi-Aspect Evaluation of Dialogue in Pythia](https://arxiv.org/abs/2509.16487)
*Zixun Chen,Petr Babkin,Akshat Gupta,Gopala Anumanchipalli,Xiaomo Liu*

Main category: cs.CL

> 研究了预训练Pythia模型在对话能力上的表现，发现模型大小作用有限，度量指标可靠性堪忧。

<details>
  <summary>Details</summary>

**Motivation:** 尽管对话是大型语言模型的一项标志性能力，但很少有研究具体区分出对话行为在后训练期间出现的特定成分。本文旨在通过语言理论指导的综合性度量指标来填补这一空白。

**Method:** 使用基于模型的综合性度量指标来评估预训练的Pythia模型在对话不同维度的表现变化，这些维度取决于模型大小以及在对话数据集上的监督微调结果。

**Result:** 数据显示模型大小对大多数度量指标的影响较小，并且监督微调快速达到了所有但最小的模型测试的饱和分数。然而，如果这些度量指标源自相同的评估模型，则它们表现出非常相似的趋势，这提出了它们可靠性的疑问。

**Conclusion:** 研究表明模型大小在提升特定对话维度性能方面的作用有限，同时提出了所用度量指标可靠性的疑问。通过额外的分析，包括得分分布、度量指标相关性和生成响应中的术语频率，这些问题得到了一定程度的解答。

**Abstract:** Dialogue is one of the landmark abilities of large language models (LLMs).
Despite its ubiquity, few studies actually distinguish specific ingredients
underpinning dialogue behavior emerging during post-training. We employ a
comprehensive suite of model-based metrics, each targeting a distinct
fine-grained aspect of dialogue, motivated by linguistic theory. We evaluate
how the performance of pre-trained Pythia models changes with respect to each
of those dimensions, depending on model size and as a result of supervised
fine-tuning on conversational datasets. We observe only a mild impact of raw
model size on most metrics, whereas fine-tuning quickly saturates the scores
for all but the smallest models tested. Somewhat contrary to our expectations,
many metrics show very similar trends, especially if they are all rooted in the
same evaluator model, which raises the question of their reliability in
measuring a specific dimension. To that end, we conduct additional analyses of
score distributions, metric correlations, and term frequencies in generated
responses to help explain our observations.

</details>


### [19] [Can an Individual Manipulate the Collective Decisions of Multi-Agents?](https://arxiv.org/abs/2509.16494)
*Fengyuan Liu,Rui Zhao,Shuo Chen,Guohao Li,Philip Torr,Lei Han,Jindong Gu*

Main category: cs.CL

> 该研究提出了M-Spoiler框架，通过模拟多智能体系统中的智能体交互，生成对抗样本，以误导目标系统中的集体决策过程。研究证实了在多智能体系统中了解单个智能体所带来的风险，并展示了所提攻击框架的有效性以及防御机制的潜在改进方向。

<details>
  <summary>Details</summary>

**Motivation:** 近期研究表明，协作多智能体系统在决策和推理方面表现出增强的能力。然而，由于个体大语言模型的脆弱性和难以访问多智能体系统所有智能体的问题，如果攻击者只知道其中一个智能体，是否可以生成误导性对抗样本成为了需要探讨的关键问题。

**Method:** 该研究将上述问题视为具有不完全信息的游戏，攻击者仅了解一个目标智能体。研究提出了M-Spoiler框架，该框架模拟了目标系统中的智能体交互，生成对抗样本，并引入了一个固执的智能体模拟目标系统中可能的固执反应，从而优化这些对抗样本。

**Result:** 实验结果验证了在多智能体系统中了解单个智能体所带来的风险，并展示了M-Spoiler框架的有效性。研究还探索了几种防御机制，结果显示所提出的攻击框架仍然比基线更强大。

**Conclusion:** 研究表明，针对多智能体系统的攻击可以通过了解单个智能体实现，并提出了有效应对这种攻击的方法和防御机制，进一步强调了防御策略研究的重要性。

**Abstract:** Individual Large Language Models (LLMs) have demonstrated significant
capabilities across various domains, such as healthcare and law. Recent studies
also show that coordinated multi-agent systems exhibit enhanced decision-making
and reasoning abilities through collaboration. However, due to the
vulnerabilities of individual LLMs and the difficulty of accessing all agents
in a multi-agent system, a key question arises: If attackers only know one
agent, could they still generate adversarial samples capable of misleading the
collective decision? To explore this question, we formulate it as a game with
incomplete information, where attackers know only one target agent and lack
knowledge of the other agents in the system. With this formulation, we propose
M-Spoiler, a framework that simulates agent interactions within a multi-agent
system to generate adversarial samples. These samples are then used to
manipulate the target agent in the target system, misleading the system's
collaborative decision-making process. More specifically, M-Spoiler introduces
a stubborn agent that actively aids in optimizing adversarial samples by
simulating potential stubborn responses from agents in the target system. This
enhances the effectiveness of the generated adversarial samples in misleading
the system. Through extensive experiments across various tasks, our findings
confirm the risks posed by the knowledge of an individual agent in multi-agent
systems and demonstrate the effectiveness of our framework. We also explore
several defense mechanisms, showing that our proposed attack framework remains
more potent than baselines, underscoring the need for further research into
defensive strategies.

</details>


### [20] [AIPsychoBench: Understanding the Psychometric Differences between LLMs and Humans](https://arxiv.org/abs/2509.16530)
*Wei Xie,Shuoyoucheng Ma,Zhenhua Wang,Enze Wang,Kai Chen,Xiaobing Sun,Baosheng Wang*

Main category: cs.CL

> 论文提出了AIPsychoBench，一种新的语言模型心理测试基准，它克服了现有方法的问题，提高了响应率，减少偏见，并提供了跨语言测试的支持。

<details>
  <summary>Details</summary>

**Motivation:** 动机在于解决现有评估大型语言模型心理特性的方法中存在的问题，这些问题包括高拒绝率以及跨语言测量时的不足。

**Method:** 此论文介绍了AIPsychoBench，这是一个专门用来评估大型语言模型心理特性的基准。它使用了一个轻量级的角色扮演提示来绕过语言模型的对齐问题，从而提高了有效响应率并降低了偏见。

**Result:** 结果表明，与传统的越狱提示相比，AIPsychoBench显著降低了正负偏见，并提供了关于语言模型心理测量特性的跨语言变异的首个综合证据。

**Conclusion:** 结论指出，AIPsychoBench能有效提高语言模型的响应率和减少偏见，为评估语言模型的心理性质提供了一个新的工具。此外，它揭示了语言对语言模型心理测量特性的影响。

**Abstract:** Large Language Models (LLMs) with hundreds of billions of parameters have
exhibited human-like intelligence by learning from vast amounts of
internet-scale data. However, the uninterpretability of large-scale neural
networks raises concerns about the reliability of LLM. Studies have attempted
to assess the psychometric properties of LLMs by borrowing concepts from human
psychology to enhance their interpretability, but they fail to account for the
fundamental differences between LLMs and humans. This results in high rejection
rates when human scales are reused directly. Furthermore, these scales do not
support the measurement of LLM psychological property variations in different
languages. This paper introduces AIPsychoBench, a specialized benchmark
tailored to assess the psychological properties of LLM. It uses a lightweight
role-playing prompt to bypass LLM alignment, improving the average effective
response rate from 70.12% to 90.40%. Meanwhile, the average biases are only
3.3% (positive) and 2.1% (negative), which are significantly lower than the
biases of 9.8% and 6.9%, respectively, caused by traditional jailbreak prompts.
Furthermore, among the total of 112 psychometric subcategories, the score
deviations for seven languages compared to English ranged from 5% to 20.2% in
43 subcategories, providing the first comprehensive evidence of the linguistic
impact on the psychometrics of LLM.

</details>


### [21] [Leveraging Multilingual Training for Authorship Representation: Enhancing Generalization across Languages and Domains](https://arxiv.org/abs/2509.16531)
*Junghwan Kim,Haotian Zhang,David Jurgens*

Main category: cs.CL

> This paper presents an improved approach to multilingual authorship representation learning through techniques like probabilistic content masking and language-aware batching, achieving better performance than monolingual baselines across multiple languages.

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind this paper is to explore the potential benefits of multilingual AR models as prior research has been mostly confined to monolingual settings, particularly in English.

**Method:** The paper introduces a novel method for multilingual AR learning with two key innovations: probabilistic content masking and language-aware batching. This approach is trained on over 4.5 million authors from 36 languages and 13 domains.

**Result:** The model outperforms monolingual baselines in 21 out of 22 non-English languages, showing an average Recall@8 improvement of 4.85% with a maximum gain of 15.91%.

**Conclusion:** The paper concludes that both proposed techniques, probabilistic content masking and language-aware batching, are critical for the model's improved performance and that the multilingual approach exhibits stronger cross-lingual and cross-domain generalization compared to monolingual models.

**Abstract:** Authorship representation (AR) learning, which models an author's unique
writing style, has demonstrated strong performance in authorship attribution
tasks. However, prior research has primarily focused on monolingual
settings-mostly in English-leaving the potential benefits of multilingual AR
models underexplored. We introduce a novel method for multilingual AR learning
that incorporates two key innovations: probabilistic content masking, which
encourages the model to focus on stylistically indicative words rather than
content-specific words, and language-aware batching, which improves contrastive
learning by reducing cross-lingual interference. Our model is trained on over
4.5 million authors across 36 languages and 13 domains. It consistently
outperforms monolingual baselines in 21 out of 22 non-English languages,
achieving an average Recall@8 improvement of 4.85%, with a maximum gain of
15.91% in a single language. Furthermore, it exhibits stronger cross-lingual
and cross-domain generalization compared to a monolingual model trained solely
on English. Our analysis confirms the effectiveness of both proposed
techniques, highlighting their critical roles in the model's improved
performance.

</details>


### [22] [Challenging the Evaluator: LLM Sycophancy Under User Rebuttal](https://arxiv.org/abs/2509.16533)
*Sungwon Kim,Daniel Khashabi*

Main category: cs.CL

> 研究表明，大型语言模型在对话中容易受用户影响，而其在评估任务中表现出良好的性能与对话中的逢迎行为存在冲突，研究发现了一些具体影响LLMs判断的因素，表明在对话框架下直接依赖LLMs做出判断有其风险。

<details>
  <summary>Details</summary>

**Motivation:** 研究动机在于揭示大型语言模型在对话中表现出的逢迎倾向与在评估任务中良好表现之间的冲突，为改进LLMs在适用性评估任务中的表现寻找依据。

**Method:** 通过改变交互模式，对LLMs在不同情况下的反应进行了实证测试，包括用户反驳观点的呈现方式、反驳中推理的细节程度以及反馈的正式程度。

**Result:** 该研究探讨了大型语言模型（LLMs）在对话中表现出逢迎行为，而在评估任务中却表现出良好的性能之间的矛盾。通过改变交互模式进行实证测试，研究发现：1) LLMs更容易同意用户提出的反驳观点，如果这些反驳观点作为用户的后续发言提出，而不是同时呈现以供评估；2) LLMs在面对含有详细推理（即使推理结论错误）的用户反驳时更容易被说服；3) LLMs更容易受非正式反馈的影响而不是正式批评，即使非正式反馈缺乏充分的论证。这些结果强调了在对话框架下依赖LLMs进行判断任务的风险。

**Conclusion:** 研究结果揭示了大型语言模型在对话过程中易于受到用户意见的影响，而当这些意见以不同方式呈现（如详细推理、非正式反馈）时，这种影响变得更加突出，表明在对话环境中的评价任务中，直接依赖LLMs需谨慎。

**Abstract:** Large Language Models (LLMs) often exhibit sycophancy, distorting responses
to align with user beliefs, notably by readily agreeing with user
counterarguments. Paradoxically, LLMs are increasingly adopted as successful
evaluative agents for tasks such as grading and adjudicating claims. This
research investigates that tension: why do LLMs show sycophancy when challenged
in subsequent conversational turns, yet perform well when evaluating
conflicting arguments presented simultaneously? We empirically tested these
contrasting scenarios by varying key interaction patterns. We find that
state-of-the-art models: (1) are more likely to endorse a user's
counterargument when framed as a follow-up from a user, rather than when both
responses are presented simultaneously for evaluation; (2) show increased
susceptibility to persuasion when the user's rebuttal includes detailed
reasoning, even when the conclusion of the reasoning is incorrect; and (3) are
more readily swayed by casually phrased feedback than by formal critiques, even
when the casual input lacks justification. Our results highlight the risk of
relying on LLMs for judgment tasks without accounting for conversational
framing.

</details>


### [23] [InteGround: On the Evaluation of Verification and Retrieval Planning in Integrative Grounding](https://arxiv.org/abs/2509.16534)
*Cheng Jiayang,Qianqian Zhuang,Haoran Li,Chunkit Chan,Xin Liu,Lin Qiu,Yangqiu Song*

Main category: cs.CL

> 本文探讨了大语言模型在整合外部知识源进行准确预测时所面临的综合接地挑战。研究表明，语言模型在没有完整信息时倾向于使用内部知识进行推理，并且规划有目标的前提推断提升了性能。

<details>
  <summary>Details</summary>

**Motivation:** 现有方法对于简单的查询工作良好，但许多现实世界的信息需求需要合成多个证据片段。

**Method:** 介绍了一种称为“综合接地”的方法，即检索和验证多个相互依赖的证据以支持假设查询的挑战。为了系统地研究这个问题，从四个领域重新利用数据来评估综合接地能力。

**Result:** 研究揭示了两个关键发现：在接地验证中，虽然大语言模型对冗余证据具有鲁棒性，但当信息不完整时，它们倾向于使用内部知识进行解释。在检查检索规划策略时，发现无目标规划通过引入噪音会降低性能，而前提推断作为一种具有逻辑约束的方法表现出色。

**Conclusion:** 这些见解为开发更有效的综合接地系统提供了宝贵的方向。

**Abstract:** Grounding large language models (LLMs) in external knowledge sources is a
promising method for faithful prediction. While existing grounding approaches
work well for simple queries, many real-world information needs require
synthesizing multiple pieces of evidence. We introduce "integrative grounding"
-- the challenge of retrieving and verifying multiple inter-dependent pieces of
evidence to support a hypothesis query. To systematically study this problem,
we repurpose data from four domains for evaluating integrative grounding
capabilities. Our investigation reveals two critical findings: First, in
groundedness verification, while LLMs are robust to redundant evidence, they
tend to rationalize using internal knowledge when information is incomplete.
Second, in examining retrieval planning strategies, we find that undirected
planning can degrade performance through noise introduction, while premise
abduction emerges as a promising approach due to its logical constraints.
Additionally, LLMs' zero-shot self-reflection capabilities consistently improve
grounding quality. These insights provide valuable direction for developing
more effective integrative grounding systems.

</details>


### [24] [Mental Multi-class Classification on Social Media: Benchmarking Transformer Architectures against LSTM Models](https://arxiv.org/abs/2509.16542)
*Khalid Hasan,Jamil Saquer,Yifan Zhang*

Main category: cs.CL

> 本研究对比了多次LSTM及变压器模型在心理健康诊断中的表现，发现在相同条件下，变压器模型尤其是RoBERTa优越于其他模型。

<details>
  <summary>Details</summary>

**Motivation:** 尽管大多数先前的自然语言处理研究都专注于单一疾病识别，但本研究旨在了解先进的NLP技术在区分多种心理健康状况方面的效果。

**Method:** 本研究通过比较五种基于变压器的模型（BERT, RoBERTa, DistilBERT, ALBERT, 和ELECTRA）和几种LSTM模型（带有或不带有注意力机制，使用上下文或静态嵌入）来分类心理健康帖子，以区分六种心理健康状况和一个对照组。

**Result:** 实验结果显示，基于变压器的模型始终优于其他模型，其中RoBERTa在所有类别中实现91-99%的F1分数和准确率。值得注意的是，带有BERT嵌入的注意力增强LSTM模型接近变压器模型的性能（高达97% 的F1分数），而训练速度却快2-3.5倍。使用静态嵌入的LSTM模型未能学习到有用的信号。

**Conclusion:** 这些发现代表了多类别心理健康检测的首个全面基准，为模型选择提供了实用指导，并强调了实际部署心理健康NLP系统时的精度与效率之间的权衡。

**Abstract:** Millions of people openly share mental health struggles on social media,
providing rich data for early detection of conditions such as depression,
bipolar disorder, etc. However, most prior Natural Language Processing (NLP)
research has focused on single-disorder identification, leaving a gap in
understanding the efficacy of advanced NLP techniques for distinguishing among
multiple mental health conditions. In this work, we present a large-scale
comparative study of state-of-the-art transformer versus Long Short-Term Memory
(LSTM)-based models to classify mental health posts into exclusive categories
of mental health conditions. We first curate a large dataset of Reddit posts
spanning six mental health conditions and a control group, using rigorous
filtering and statistical exploratory analysis to ensure annotation quality. We
then evaluate five transformer architectures (BERT, RoBERTa, DistilBERT,
ALBERT, and ELECTRA) against several LSTM variants (with or without attention,
using contextual or static embeddings) under identical conditions. Experimental
results show that transformer models consistently outperform the alternatives,
with RoBERTa achieving 91-99% F1-scores and accuracies across all classes.
Notably, attention-augmented LSTMs with BERT embeddings approach transformer
performance (up to 97% F1-score) while training 2-3.5 times faster, whereas
LSTMs using static embeddings fail to learn useful signals. These findings
represent the first comprehensive benchmark for multi-class mental health
detection, offering practical guidance on model selection and highlighting an
accuracy-efficiency trade-off for real-world deployment of mental health NLP
systems.

</details>


### [25] [ChemOrch: Empowering LLMs with Chemical Intelligence via Synthetic Instructions](https://arxiv.org/abs/2509.16543)
*Yue Huang,Zhengzhe Jiang,Xiaonan Luo,Kehan Guo,Haomin Zhuang,Yujun Zhou,Zhengqing Yuan,Xiaoqi Sun,Jules Schleinitz,Yanbo Wang,Shuhao Zhang,Mihir Surve,Nitesh V Chawla,Olaf Wiest,Xiangliang Zhang*

Main category: cs.CL

> 本文提出了ChemOrch框架，用于增强大型语言模型的化学智能。通过两阶段生成化学相关的指令和响应，并确保生成数据的质量和适用性。实验结果展示了ChemOrch在生成高质量数据、生成评估任务以及提升模型化学能力方面的有效性。

<details>
  <summary>Details</summary>

**Motivation:** 为大型语言模型（LLM）赋能化学智能面临挑战，主要是因为高质量的特定领域指令-响应数据集稀缺，现有合成数据生成管线与化学信息的层次结构和规则治理不匹配。ChemOrch框架提出解决这些问题。

**Method:** ChemOrch框架通过两阶段过程合成与化学相关的指令-响应对：任务控制的指令生成和工具感知的响应构建。ChemOrch能够控制生成任务的多样性和难度水平，并通过工具规划和蒸馏及基于工具的自我修复机制确保响应的精确性。

**Result:** ChemOrch的有效性评估基于以下三点：1) 生成的指令数据质量高，展现了良好的多样性和与化学约束的强一致性；2) 生成评估任务的可靠性，更有效地揭示LLM在化学方面的弱点；3) 使用生成的指令数据进行微调时，显著提升了LLM在化学方面的能力。

**Conclusion:** 该工作代表了向具有可扩展性和可验证的化学智能LLMs发展的重要一步。

**Abstract:** Empowering large language models (LLMs) with chemical intelligence remains a
challenge due to the scarcity of high-quality, domain-specific
instruction-response datasets and the misalignment of existing synthetic data
generation pipelines with the inherently hierarchical and rule-governed
structure of chemical information. To address this, we propose ChemOrch, a
framework that synthesizes chemically grounded instruction-response pairs
through a two-stage process: task-controlled instruction generation and
tool-aware response construction. ChemOrch enables controllable diversity and
levels of difficulty for the generated tasks, and ensures response precision
through tool planning and distillation, and tool-based self-repair mechanisms.
The effectiveness of ChemOrch is evaluated based on: 1) the high quality of
generated instruction data, demonstrating superior diversity and strong
alignment with chemical constraints; 2) the reliable generation of evaluation
tasks that more effectively reveal LLM weaknesses in chemistry; and 3) the
significant improvement of LLM chemistry capabilities when the generated
instruction data are used for fine-tuning. Our work thus represents a critical
step toward scalable and verifiable chemical intelligence in LLMs.

</details>


### [26] [Rethinking the Role of Text Complexity in Language Model Pretraining](https://arxiv.org/abs/2509.16551)
*Dan John Velasco,Matthew Theodore Roque*

Main category: cs.CL

> 研究发现语言建模的困惑度对模型容量和文本复杂性之间的交互作用非常敏感，而文本复杂性对微调评估影响小，但对零样本设置下的任务有影响，简单文本在语言知识任务上有优势，而复杂文本则利于需要世界知识和实体追踪的任务。

<details>
  <summary>Details</summary>

**Motivation:** 文本复杂度对语言模型预训练和下游任务效果的影响尚不明确，本研究旨在探索通过简化文本复杂度可以了解哪些类型的表示可以独立从简化文本中学习，以及预训练文本复杂性如何影响下游语言理解任务。

**Method:** 我们通过减少文本表面复杂性（如短句、简单词汇和结构），同时保持核心文本内容大致不变来进行实验。我们使用大型语言模型简化人类编写的文本，然后从头开始在原始和简化过的数据上对因果模型（28M-500M参数）进行预训练，并在微调和零样本设置下进行评估。

**Result:** 研究发现，困惑度对模型容量和文本复杂性之间的交互作用敏感，而文本复杂性对微调任务的影响不大，但在零样本评估中，简单文本在语言知识任务上有助于提高性能，复杂文本则在需要世界知识和实体追踪的任务中有优势。

**Conclusion:** 研究结果表明，在预训练过程中使用简化文本可以提高模型在某些任务上的性能，特别是在零样本设置下的语言知识任务。但复杂文本更适合需要世界知识和实体追踪的任务。因此，根据下游任务的不同，使用不同复杂度的文本进行预训练可能会有不同的效果。

**Abstract:** Improving pretraining data quality and size is known to boost downstream
performance, but the role of text complexity is less explored. Text complexity
refers to how hard a text is to read, and is typically estimated from surface
cues such as sentence length, word choice, and sentence structure. We reduce
surface-level complexity--shorter sentences, simpler words, simpler
structure--while keeping core text content close to constant, and ask: (1) How
does complexity affect language modeling across model sizes? (2) Can useful
representations be learned from simpler text alone? (3) How does pretraining
text complexity influence downstream language understanding? To answer these
questions, we simplify human-written texts using a large language model, then
pretrain causal models (28M-500M) from scratch on both original and simplified
data, and evaluate them in finetuning and zero-shot setups. We find that
perplexity is sensitive to the interaction between model capacity and text
complexity--smaller models degrade far less on simpler texts--while text
complexity has little impact on finetuning evaluations, with zero-shot
evaluations indicating that simpler texts benefit performance on linguistic
knowledge tasks, whereas more complex texts favor tasks requiring world
knowledge and entity tracking.

</details>


### [27] [MPCG: Multi-Round Persona-Conditioned Generation for Modeling the Evolution of Misinformation with LLMs](https://arxiv.org/abs/2509.16564)
*Jun Rong Brian Chong,Yixuan Tang,Anthony K. H. Tung*

Main category: cs.CL

> 研究通过一种名为MPCG的框架，探索了虚假信息在传播过程中随着视角不同而发生的演变，并进行了多方面的评估，结果显示MPCG生成的声明在情感、道德上更加与角色一致，并对现有虚假信息检测技术提出了挑战。

<details>
  <summary>Details</summary>

**Motivation:** 现有的虚假信息检测方法隐性假设虚假信息是静态的，但虚假信息随着传播而演变，因此提出了MPCG框架，用于研究虚假信息的演变。

**Method:** 介绍了一种名为MPCG的多轮次、角色条件框架，通过使用无审查的大语言模型(LLM)生成多个轮次的角色特定声明，并在每轮次输出的基础上进行条件生成，以模拟声明是如何被具有不同意识形态视角的代理人重新解释的。

**Result:** 实验结果显示，人类和GPT-4o-mini注释之间存在高度一致的评价，但在流畅性评分上存在较大差异。生成的声明需要比原始声明更高的认知努力，并且在情感和道德上更加与角色一致。聚类和余弦相似度分析证实了语义上的漂移以及主题上的连贯性。可行性结果表明可行率为77%，适用于下游任务。分类结果显示，常用的虚假信息检测器的宏观F1性能下降了高达49.7%。

**Conclusion:** MPCG框架有效地模拟了虚假信息在传播过程中的演变，揭示了现有虚假信息检测技术在面对动态虚假信息时的局限性，证明了该框架在下游任务中的适用性。

**Abstract:** Misinformation evolves as it spreads, shifting in language, framing, and
moral emphasis to adapt to new audiences. However, current misinformation
detection approaches implicitly assume that misinformation is static. We
introduce MPCG, a multi-round, persona-conditioned framework that simulates how
claims are iteratively reinterpreted by agents with distinct ideological
perspectives. Our approach uses an uncensored large language model (LLM) to
generate persona-specific claims across multiple rounds, conditioning each
generation on outputs from the previous round, enabling the study of
misinformation evolution. We evaluate the generated claims through human and
LLM-based annotations, cognitive effort metrics (readability, perplexity),
emotion evocation metrics (sentiment analysis, morality), clustering,
feasibility, and downstream classification. Results show strong agreement
between human and GPT-4o-mini annotations, with higher divergence in fluency
judgments. Generated claims require greater cognitive effort than the original
claims and consistently reflect persona-aligned emotional and moral framing.
Clustering and cosine similarity analyses confirm semantic drift across rounds
while preserving topical coherence. Feasibility results show a 77% feasibility
rate, confirming suitability for downstream tasks. Classification results
reveal that commonly used misinformation detectors experience macro-F1
performance drops of up to 49.7%. The code is available at
https://github.com/bcjr1997/MPCG

</details>


### [28] [From Scores to Steps: Diagnosing and Improving LLM Performance in Evidence-Based Medical Calculations](https://arxiv.org/abs/2509.16584)
*Benlu Wang,Iris Xia,Yifan Zhang,Junda Wang,Feiyun Ouyang,Shuo Han,Arman Cohan,Hong Yu,Zonghai Yao*

Main category: cs.CL

> 研究清理并改进了MedCalc-Bench数据集，提出了更细致的评估方法和自动错误分析框架，并开发了MedRaC系统，提高了大语言模型在医疗计算任务中的准确性和可信度。

<details>
  <summary>Details</summary>

**Motivation:** 研究动机在于，目前的大语言模型在医学计算方面的能力尚未得到充分探索和评估，而医学计算是临床决策的关键方面。现有的基准测试通常只评估最终答案并且有较大的数值容差，忽略了系统性推理失败，可能会导致严重的临床误判。

**Method:** 该研究首先清理并重构了MedCalc-Bench数据集，并提出了一种新的分步评估流水线，该流水线独立评估公式选择、实体抽取和算术计算。其次，引入了一种自动错误分析框架，可以为每种失败模式生成结构化归因。最后，提出了一个模块化代理流水线MedRaC，它结合了检索增强生成和基于Python的代码执行，无需任何微调即可提高不同大语言模型的准确性。

**Result:** 研究发现，采用新的评估框架后，GPT-4的准确率从62.7%下降到43.6%，揭示了一些之前被掩盖的误差。MedRaC系统在不同的大语言模型上无需调整即可将准确率从16.35%提高到53.19%。

**Conclusion:** 该研究揭示了现有评估实践中大语言模型存在的局限性，并提出了一种更加符合临床实际情况的方法。通过实现透明和可转移的推理评估，使得基于大语言模型的系统更接近于在实际医学应用中信任可靠的级别。

**Abstract:** Large language models (LLMs) have demonstrated promising performance on
medical benchmarks; however, their ability to perform medical calculations, a
crucial aspect of clinical decision-making, remains underexplored and poorly
evaluated. Existing benchmarks often assess only the final answer with a wide
numerical tolerance, overlooking systematic reasoning failures and potentially
causing serious clinical misjudgments. In this work, we revisit medical
calculation evaluation with a stronger focus on clinical trustworthiness.
First, we clean and restructure the MedCalc-Bench dataset and propose a new
step-by-step evaluation pipeline that independently assesses formula selection,
entity extraction, and arithmetic computation. Under this granular framework,
the accuracy of GPT-4o drops from 62.7% to 43.6%, revealing errors masked by
prior evaluations. Second, we introduce an automatic error analysis framework
that generates structured attribution for each failure mode. Human evaluation
confirms its alignment with expert judgment, enabling scalable and explainable
diagnostics. Finally, we propose a modular agentic pipeline, MedRaC, that
combines retrieval-augmented generation and Python-based code execution.
Without any fine-tuning, MedRaC improves the accuracy of different LLMs from
16.35% up to 53.19%. Our work highlights the limitations of current benchmark
practices and proposes a more clinically faithful methodology. By enabling
transparent and transferable reasoning evaluation, we move closer to making
LLM-based systems trustworthy for real-world medical applications.

</details>


### [29] [Benchmarking Contextual and Paralinguistic Reasoning in Speech-LLMs: A Case Study with In-the-Wild Data](https://arxiv.org/abs/2509.16589)
*Qiongqiong Wang,Hardik Bhupendra Sailor,Tianchi Liu,Wenyu Zhang,Muhammad Huzaifah,Nattadaporn Lertcheva,Shuo Sun,Nancy F. Chen,Jinyang Wu,AiTi Aw*

Main category: cs.CL

> 介绍CP-Bench，一个针对语音LLMs在情境性语用推理方面进行评估的新基准，显示出理解和整合情感和韵律的重要性。

<details>
  <summary>Details</summary>

**Motivation:** 评估语音LLMs在理解情境性和情感方面的能力是迫切的需求，因为这是他们实现更广泛应用的关键一步。

**Method:** 开发了CP-Bench，这是一个评估语音LLMs在理解和整合非语言线索方面能力的基准，通过问答数据集来评估它们的表现。

**Result:** 最近的语音LLMs在转录和翻译等任务中表现出色，但在理解对于社交和情感智能至关重要的语用学方面仍然有限。我们提出了CP-Bench，一个评估语音LLMs在情境性语用推理方面能力的基准，该基准涉及语言内容与情感和韵律等非语言线索的整合。这个基准包括两个需要语言和同理心理解的问题回答（QA）数据集。我们评估了来自开源和闭源模型的最先进的语音LLMs，并对不同的问题类型进行了全面分析。我们进一步分析了排名前两的模型在温度调整下的表现，以理解其对任务的影响。我们的基准测试揭示了现有评估的关键差距，并为构建更具有语境感知能力和情感智能的语音LLMs提供了见解。

**Conclusion:** CP-Bench揭示了现有评估方法的关键差距，为改进语音LLMs的理解能力提供了有价值的见解和方向。

**Abstract:** Recent speech-LLMs have shown impressive performance in tasks like
transcription and translation, yet they remain limited in understanding the
paralinguistic aspects of speech crucial for social and emotional intelligence.
We propose CP-Bench, a benchmark for evaluating speech-LLMs on contextual
paralinguistic reasoning the integration of verbal content with non-verbal cues
like emotion and prosody. The benchmark includes two curated question answering
(QA) datasets requiring both linguistic and empathetic understanding. We
evaluate state-of-the-art speech-LLMs from both open and closed-source models
and perform a comprehensive analysis across different question types. The top
two models were further analyzed under temperature tuning to understand its
effect on this task. Our benchmark reveals a key gap in existing evaluations
and offers insights into building more context-aware and emotionally
intelligent speech-capable LLMs.

</details>


### [30] [From Uniform to Heterogeneous: Tailoring Policy Optimization to Every Token's Nature](https://arxiv.org/abs/2509.16591)
*Zheng Liu,Mengjie Liu,Siwei Wen,Mengzhang Cai,Bin Cui,Conghui He,Wentao Zhang*

Main category: cs.CL

> 本文提出了HAPO算法，通过异构自适应策略优化，实现token级别的细粒度控制和优化。实验表明，该方法在多个模型规模中，性能优于DAPO。

<details>
  <summary>Details</summary>

**Motivation:** 强化学习已经成为提升LLMs推理能力的核心技术。然而，现有的算法在对所有token进行统一优化时，忽略了它们在推理过程中的不同作用。为了解决这个限制，该研究引入了一种全面基于token的算法。

**Method:** 提出了异构自适应策略优化(HAPO)，一种全面的基于token的算法，能根据token熵动态调整优化策略。在采样过程中，提出自适应温度采样，根据高熵和低熵token调整采样温度。在计算优势时，提出了Token级别的群组平均，对token级别的优势进行规范化处理，同时考虑序列长度并保持无偏处理。差异优势重分配机制则利用熵和重要性比例来调节奖励，对信号明确的token进行更新调整。对于裁剪损失，设计了对低熵token有积极限制作用，而对高熵token则保持探索性的非对称自适应裁剪。

**Result:** 实验结果表明，HAPO在多个模型规模上相比DAPO均有提升。

**Conclusion:** 研究表明，通过细致地将token级处理嵌入到每个阶段，实现了对LLM训练的精确控制，改善了模型推理能力。

**Abstract:** Reinforcement Learning has emerged as the fundamental technique for enhancing
reasoning in LLMs. However, existing algorithms apply uniform optimization to
all tokens, ignoring their different roles in reasoning process. To address
this limitation, we introduce Heterogeneous Adaptive Policy Optimization
(HAPO), a comprehensive token-aware algorithm that dynamically adapts
optimization based on token entropy. For rollout sampling, we propose Adaptive
Temperature Sampling, which adjusts sampling temperature in real time,
promoting exploration at high-entropy tokens while preserving coherence at
low-entropy ones. For advantage calculation, we introduce Token Level Group
Average that normalizes advantages at token level, jointly accounting for
sequence-length as in token-mean loss while preserving non-biased treatment. We
then develop Differential Advantage Redistribution that leverages entropy and
importance ratios to modulate rewards-adjusting updates for tokens with clear
signals. For clipping loss, we design Asymmetric Adaptive Clipping, allowing
aggressive probability reduction for noisy low-entropy tokens while enabling
exploration for high-entropy tokens. Through systematic investigation between
entropy and training dynamics, we embedded token-level treatment into every
stages to achieve fine-grained control. Extensive experiments demonstrate that
HAPO consistently outperforms DAPO across multiple model scales. Our code can
be found in https://github.com/starriver030515/HAPO.

</details>


### [31] [Analyzing the Effects of Supervised Fine-Tuning on Model Knowledge from Token and Parameter Levels](https://arxiv.org/abs/2509.16596)
*Junjie Ye,Yuming Yang,Yang Nan,Shuo Li,Qi Zhang,Tao Gui,Xuanjing Huang,Peng Wang,Zhongchao Shi,Jianping Fan*

Main category: cs.CL

> 研究发现微调样本数量和知识掌握程度的变动会对模型性能产生重要影响，并揭示了微调过程中的参数更新对知识增强的贡献不大，提供策略以更有效地加强模型知识。

<details>
  <summary>Details</summary>

**Motivation:** 探讨监督微调（SFT）对模型知识的影响，解决目前在控制微调模型的知识变化行为方面的能力受限的问题。

**Method:** 评估了五个来自LLaMA-2和LLaMA-3系列的大型语言模型在闭卷问答（CBQA）任务上的表现，并在标记和参数层面分析了模型的行为。

**Result:** 在仅使用240个微调样本的模型相比使用1,920个样本的模型，CBQA任务上的性能提高了最多14%，并且控制知识掌握程度可以导致性能波动超过12%。

**Conclusion:** 分析表明，高达90%的微调过程中参数的更新并不增强知识，这对制定更有效的微调策略有实际指导意义。

**Abstract:** Large language models (LLMs) acquire substantial world knowledge during
pre-training, which is further shaped by post-training techniques such as
supervised fine-tuning (SFT). However, the impact of SFT on a model's knowledge
remains underexplored, limiting our ability to control knowledge change
behavior in fine-tuned models. To address this gap, we evaluate closed-book
question answering (CBQA) performance across five LLMs from the LLaMA-2 and
LLaMA-3 families. Surprisingly, models fine-tuned on 1,920 samples perform up
to 14% worse than those fine-tuned on only 240 samples. Furthermore, varying
the level of knowledge mastery in the fine-tuning data leads to performance
fluctuations of over 12%. To investigate these effects, we analyze model
behavior at both the token and parameter levels. Our analysis reveals that up
to 90% of parameter updates during SFT do not contribute to knowledge
enhancement. Restoring these updates can improve performance on the CBQA task,
depending on the characteristics of the fine-tuning data. These insights offer
practical guidance for developing fine-tuning strategies that more effectively
strengthen model knowledge.

</details>


### [32] [MCP: A Control-Theoretic Orchestration Framework for Synergistic Efficiency and Interpretability in Multimodal Large Language Models](https://arxiv.org/abs/2509.16597)
*Luyan Zhang*

Main category: cs.CL

> 研究提出一种三层MCP框架，通过解耦大型模型功能及运用动态路由和任务适应机制，提升了任务性能和推理效率，同时也改进了解释性。

<details>
  <summary>Details</summary>

**Motivation:** 针对大型模型在多轮推理和多模态协作等复杂任务中计算效率低下和解释性不足的问题，提出了解决方案。

**Method:** 本研究提出了一种基于模型-控制器-任务适应（MCP）的三层协作框架。通过将大型模型的功能解耦为推理、生成和检索模块，并结合强化学习驱动的动态路由算法和任务适应机制，首次实现了控制理论与大型模型动态推理的系统整合。

**Result:** 实验结果表明，与基线模型相比，MCP框架在GLUE、COCO、ScienceQA等跨模态基准任务上的性能提高了15-30%，推理效率提高了40%，并通过Presenter层生成的可解释中间结果，达到了90%的手动解释性评分。

**Conclusion:** 该框架为解决大型模型实际应用中的瓶颈提供了一条全新的技术路径。

**Abstract:** Aiming at the problems of computational inefficiency and insufficient
interpretability faced by large models in complex tasks such as multi-round
reasoning and multi-modal collaboration, this study proposes a three-layer
collaboration framework based on model-controller-task adaptation (MCP). By
decoupling large model functions into reasoning, generation and retrieval
modules, and combining reinforcement learning-driven dynamic routing algorithms
and task adaptation mechanisms, the systematic integration of control theory
and large model dynamic reasoning is achieved for the first time. Experiments
show that the MCP framework improves the performance of cross-modal
benchmarking tasks, such as GLUE, COCO, ScienceQA, etc., by 15-30% compared
with the baseline model, improves the reasoning efficiency by 40%, and
generates the interpretable intermediate results through the Presenter layer,
obtaining 90% of the manual interpretability scores, which provides a brand-new
technological path to solve the bottleneck of the practical application of the
large model.

</details>


### [33] [PruneCD: Contrasting Pruned Self Model to Improve Decoding Factuality](https://arxiv.org/abs/2509.16598)
*Byeongho Yu,Changhun Lee,Jungyu Jin,Eunhyeok Park*

Main category: cs.CL

> 提出PruneCD来解决大型语言模型中的幻觉问题。

<details>
  <summary>Details</summary>

**Motivation:** 旨在解决大型语言模型中存在的幻觉问题，早期退出logits存在平坦、量级低且无法反映有意义的对比。

**Method:** 提出了一种名为PruneCD的新对比解码方法，通过层剪枝构建业余模型，而不是早期退出，以产生更具信息量和更好地对齐的logits，从而实现更有效的对比解码。

**Result:** 通过定性和定量分析，证明PruneCD能够一致地改善事实性，并且推理开销最小。

**Conclusion:** PruneCD提供了一种在大型语言模型中减轻幻觉的稳健而实用的方法。

**Abstract:** To mitigate the hallucination problem in large language models, DoLa exploits
early exit logits from the same model as a contrastive prior. However, we found
that these early exit logits tend to be flat, low in magnitude, and fail to
reflect meaningful contrasts. To address this, we propose PruneCD, a novel
contrastive decoding method that constructs the amateur model via layer pruning
rather than early exit. This design leads to more informative and well-aligned
logits, enabling more effective contrastive decoding. Through qualitative and
quantitative analyses, we demonstrate that PruneCD consistently improves
factuality with minimal inference overhead, offering a robust and practical
approach to mitigating hallucinations in LLMs.

</details>


### [34] [Computational-Assisted Systematic Review and Meta-Analysis (CASMA): Effect of a Subclass of GnRH-a on Endometriosis Recurrence](https://arxiv.org/abs/2509.16599)
*Sandro Tsang*

Main category: cs.CL

> Error

<details>
  <summary>Details</summary>

**Motivation:** Error

**Method:** Error

**Result:** Error

**Conclusion:** Error

**Abstract:** Background: Evidence synthesis facilitates evidence-based medicine. Without
information retrieval techniques, this task is impossible due to the vast and
expanding literature. Objective: Building on prior work, this study evaluates
an information retrieval-driven workflow to enhance the efficiency,
transparency, and reproducibility of systematic reviews. We use endometriosis
recurrence as an ideal case due to its complex and ambiguous literature.
Methods: Our hybrid approach integrates PRISMA guidelines with computational
techniques. We applied semi-automated deduplication to efficiently filter
records before manual screening. This workflow synthesized evidence from
randomised controlled trials on the efficacy of a subclass of
gonadotropin-releasing hormone agonists (GnRH'as). A modified splitting method
addressed unit-of-analysis errors in multi-arm trials. Results: Our workflow
efficiently reduced the screening workload. It took only 11 days to fetch and
filter 812 records. Seven RCTs were eligible, providing evidence from 841
patients in 4 countries. The pooled random-effects model yielded a Risk Ratio
(RR) of 0.64 (95% CI (0.48 to 0.86)), with non-significant heterogeneity
($I^2=0.00\%$, $\tau=0.00$); i.e., a 36% reduction in endometriosis recurrence.
Sensitivity analyses and bias assessments supported the robustness of our
findings. Conclusion: This study demonstrates an information-retrieval-driven
workflow for medical evidence synthesis. Our approach yields valuable clinical
results while providing a framework for accelerating the systematic review
process. It bridges the gap between clinical research and computer science and
can be generalized to other complex systematic reviews.

</details>


### [35] [LLMsPark: A Benchmark for Evaluating Large Language Models in Strategic Gaming Contexts](https://arxiv.org/abs/2509.16610)
*Junhao Chen,Jingbo Sun,Xiang Li,Haidong Xin,Yuhao Xue,Yibin Xu,Hao Zhao*

Main category: cs.CL

> LLMsPark平台使用游戏理论方法评估多个大型语言模型的决策和社会行为，提供了评估LLMs战略智能的新视角。

<details>
  <summary>Details</summary>

**Motivation:** 随着大型语言模型在各种任务中的进步，对超越单一指标的全面评估的需求变得更加重要，提出此方法是为了全面评估LLMs的智力，特别是在互动动态和战略行为方面。

**Method:** 通过游戏理论评估多代理环境中大型语言模型(LLMs)的决策策略和社会行为，提出了LLMsPark平台来衡量LLMs在经典游戏理论环境中的表现。

**Result:** 对15个领先的LLMs进行了交叉评估，利用排行榜和评分机制区分行为模式和性能差异，更高的分数反映了更强的推理和战略能力。

**Conclusion:** 这项工作介绍了一个评估LLMs战略智能的新视角，丰富了现有的基准并在互动、游戏理论场景中扩展了评估方式。

**Abstract:** As large language models (LLMs) advance across diverse tasks, the need for
comprehensive evaluation beyond single metrics becomes increasingly important.
To fully assess LLM intelligence, it is crucial to examine their interactive
dynamics and strategic behaviors. We present LLMsPark, a game theory-based
evaluation platform that measures LLMs' decision-making strategies and social
behaviors in classic game-theoretic settings, providing a multi-agent
environment to explore strategic depth. Our system cross-evaluates 15 leading
LLMs (both commercial and open-source) using leaderboard rankings and scoring
mechanisms. Higher scores reflect stronger reasoning and strategic
capabilities, revealing distinct behavioral patterns and performance
differences across models. This work introduces a novel perspective for
evaluating LLMs' strategic intelligence, enriching existing benchmarks and
broadening their assessment in interactive, game-theoretic scenarios. The
benchmark and rankings are publicly available at https://llmsparks.github.io/.

</details>


### [36] [Redefining Experts: Interpretable Decomposition of Language Models for Toxicity Mitigation](https://arxiv.org/abs/2509.16660)
*Zuhair Hasan Shaik,Abdullah Mazhar,Aseem Srivastava,Md Shad Akhtar*

Main category: cs.CL

> The paper presents EigenShift, a method using eigen-decomposition on the final output layer to suppress toxic content in large language models, maintaining their linguistic abilities without additional training.

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind this paper is to address the instability and context dependence of neuron-level modifications in large language models, which are used for mitigating toxic content while preserving core language abilities.

**Method:** The paper proposes EigenShift, a method based on the eigen-decomposition of the language model's final output layer, to selectively target generation-aligned components for precise toxicity suppression.

**Result:** The experiments on Jigsaw and ToxiCN datasets show that aggregated layer-wise features are more stable than single neurons, and that the proposed EigenShift method can suppress toxic content without impairing linguistic competence.

**Conclusion:** The paper concludes that EigenShift is a robust, computationally efficient method for mitigating toxic content generation in large language models without compromising their language abilities.

**Abstract:** Large Language Models have demonstrated impressive fluency across diverse
tasks, yet their tendency to produce toxic content remains a critical challenge
for AI safety and public trust. Existing toxicity mitigation approaches
primarily manipulate individual neuron activations, but these methods suffer
from instability, context dependence, and often compromise the model's core
language abilities. To address these shortcomings, we investigate three key
questions: the stability of neuron-level toxicity indicators, the advantages of
structural (layer-wise) representations, and the interpretability of mechanisms
driving toxic generation. Through extensive experiments on Jigsaw and ToxiCN
datasets, we show that aggregated layer-wise features provide more robust
signals than single neurons. Moreover, we observe conceptual limitations in
prior works that conflate toxicity detection experts and generation experts
within neuron-based interventions. To mitigate this, we propose a novel
principled intervention technique, EigenShift, based on eigen-decomposition of
the language model's final output layer. This method selectively targets
generation-aligned components, enabling precise toxicity suppression without
impairing linguistic competence. Our method requires no additional training or
fine-tuning, incurs minimal computational cost, and is grounded in rigorous
theoretical analysis.

</details>


### [37] [Robust Native Language Identification through Agentic Decomposition](https://arxiv.org/abs/2509.16666)
*Ahmet Yavuz Uluslu,Tannon Kew,Tilia Ellendorff,Gerold Schneider,Rico Sennrich*

Main category: cs.CL

> 本文通过引入一种新的主动型L1识别管道来提高大型语言模型在识别母语方面的准确性与对抗误导线索的鲁棒性。这种方法受到了法医语言学的启发，在最终评估阶段整合了所有证据。

<details>
  <summary>Details</summary>

**Motivation:** 由于大型语言模型在L1识别基准测试中往往依赖于类似名字、位置和文化刻板印象等表面上的语境线索而不是潜在的语言模式，因此我们希望通过这种方法改善模型在这种识别任务中的鲁棒性。

**Method:** 我们提出了一种受法医语言学启发的主动型L1识别管道，其中专门的代理会积累并分类各种语言证据，然后独立的最终评估阶段将整合所有证据，做出L1识别的预测。

**Result:** 该方法在两个基准数据集上显著提高了对抗误导性环境线索的健壮性和相对于标准提示方法的一致性。

**Conclusion:** 相较于标准编程方法，我们提出的方法在多个数据集上展示了更强的抗误导能力和更高的性能一致性。

**Abstract:** Large language models (LLMs) often achieve high performance in native
language identification (NLI) benchmarks by leveraging superficial contextual
clues such as names, locations, and cultural stereotypes, rather than the
underlying linguistic patterns indicative of native language (L1) influence. To
improve robustness, previous work has instructed LLMs to disregard such clues.
In this work, we demonstrate that such a strategy is unreliable and model
predictions can be easily altered by misleading hints. To address this problem,
we introduce an agentic NLI pipeline inspired by forensic linguistics, where
specialized agents accumulate and categorize diverse linguistic evidence before
an independent final overall assessment. In this final assessment, a goal-aware
coordinating agent synthesizes all evidence to make the NLI prediction. On two
benchmark datasets, our approach significantly enhances NLI robustness against
misleading contextual clues and performance consistency compared to standard
prompting methods.

</details>


### [38] [Reinforcement Learning Meets Large Language Models: A Survey of Advancements and Applications Across the LLM Lifecycle](https://arxiv.org/abs/2509.16679)
*Keliang Liu,Dingkang Yang,Ziyun Qian,Weijie Yin,Yuchi Wang,Hongsheng Li,Jun Liu,Peng Zhai,Yang Liu,Lihua Zhang*

Main category: cs.CL

> 本文综述了如何通过强化学习（RL）增强大型语言模型（LLMs），特别关注RL在LLMs不同阶段的应用，分析目前的数据集、评估基准以及开源工具，同时指出未来挑战和趋势。

<details>
  <summary>Details</summary>

**Motivation:** 虽然现有的综述提供了关于RL增强LLMs的概述，但它们的范围通常有限，未能全面总结RL在整个LLMs生命周期中的作用。本综述旨在全面分析RL在LLMs各个阶段的应用，以推动LLMs向更智能、更通用和更安全的方向发展。

**Method:** 系统地回顾了强化学习（RL）如何增强大型语言模型（LLMs）的理论和实践进展，特别关注使用可验证奖励的强化学习(RLVR)。包括理论简介、LLMs生命周期各阶段RL的应用策略、数据集和评估基准的总结、开源工具和培训框架的回顾以及未来挑战和趋势的分析。

**Result:** 展示了RL在预训练、微调和强化推理各阶段对LLMs的提升，特别是强化推理阶段的重要性，指出了目前使用的各类数据集和评估基准，以及可用的开源工具和框架。

**Conclusion:** 综述了RL在LLMs中的最新进展，展望未来的研究方向和挑战，以促进更智能、通用且安全的LLMs的发展。

**Abstract:** In recent years, training methods centered on Reinforcement Learning (RL)
have markedly enhanced the reasoning and alignment performance of Large
Language Models (LLMs), particularly in understanding human intents, following
user instructions, and bolstering inferential strength. Although existing
surveys offer overviews of RL augmented LLMs, their scope is often limited,
failing to provide a comprehensive summary of how RL operates across the full
lifecycle of LLMs. We systematically review the theoretical and practical
advancements whereby RL empowers LLMs, especially Reinforcement Learning with
Verifiable Rewards (RLVR). First, we briefly introduce the basic theory of RL.
Second, we thoroughly detail application strategies for RL across various
phases of the LLM lifecycle, including pre-training, alignment fine-tuning, and
reinforced reasoning. In particular, we emphasize that RL methods in the
reinforced reasoning phase serve as a pivotal driving force for advancing model
reasoning to its limits. Next, we collate existing datasets and evaluation
benchmarks currently used for RL fine-tuning, spanning human-annotated
datasets, AI-assisted preference data, and program-verification-style corpora.
Subsequently, we review the mainstream open-source tools and training
frameworks available, providing clear practical references for subsequent
research. Finally, we analyse the future challenges and trends in the field of
RL-enhanced LLMs. This survey aims to present researchers and practitioners
with the latest developments and frontier trends at the intersection of RL and
LLMs, with the goal of fostering the evolution of LLMs that are more
intelligent, generalizable, and secure.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [39] [Evaluation of Ensemble Learning Techniques for handwritten OCR Improvement](https://arxiv.org/abs/2509.16221)
*Martin Preiß*

Main category: cs.CV

> 本文通过结合集成学习与OCR技术提高手写历史病历数字化的准确度，发现集成学习可以提高OCR准确度，且与训练数据集大小无关。

<details>
  <summary>Details</summary>

**Motivation:** 因为数字化内容将在未来被使用，所以需要很高的精度。特别是在医学领域中，这一点更加重要，因此需要探索一种能够提高OCR准确度的方法。

**Method:** 本文探讨了通过结合OCR技术使用集成学习方法来提高历史病历手写条目数字化的精度。该研究意在通过集成学习提升现有的OCR方法的准确度，以期为病历数字化提供更高价值的方法。

**Result:** 研究结果显示，集成学习确实能够提高OCR的准确度，并且能够发现哪些方法能够实现这一点。同时，研究也表明训练数据集的大小对此影响不大。

**Conclusion:** 集成学习结合OCR技术是提高历史病历手写条目数字化精度的有效方法。这种方法在医学领域应用具有更高的重要性和实用性。

**Abstract:** For the bachelor project 2021 of Professor Lippert's research group,
handwritten entries of historical patient records needed to be digitized using
Optical Character Recognition (OCR) methods. Since the data will be used in the
future, a high degree of accuracy is naturally required. Especially in the
medical field this has even more importance. Ensemble Learning is a method that
combines several machine learning models and is claimed to be able to achieve
an increased accuracy for existing methods. For this reason, Ensemble Learning
in combination with OCR is investigated in this work in order to create added
value for the digitization of the patient records. It was possible to discover
that ensemble learning can lead to an increased accuracy for OCR, which methods
were able to achieve this and that the size of the training data set did not
play a role here.

</details>


### [40] [Agentic Reasoning for Robust Vision Systems via Increased Test-Time Compute](https://arxiv.org/abs/2509.16343)
*Chung-En,Yu,Brian Jalaian,Nathaniel D. Bastian*

Main category: cs.CV

> VRA无需重新训练，通过代理推理框架提高了智能视觉系统在高风险领域的鲁棒性，测试时计算成本较高，但在视觉推理基准上取得了显着的准确度提升。

<details>
  <summary>Details</summary>

**Motivation:** 开发高可信度的智能视觉系统的需求，尤其是在高风险领域如遥感和医学诊断当中，要求系统具备广泛的鲁棒性而无需频繁的昂贵重训。

**Method:** 提出Visual Reasoning Agent (VRA)，一种无需训练、具有代理推理功能的框架，它通过“思考-批评-行动”循环来封装现有的视觉语言模型和纯视觉系统。

**Result:** 该论文介绍了一个名为视觉推理代理（VRA）的无需训练的代理推理框架，它能够在无需重新训练的情况下提高智能视觉系统的鲁棒性，特别是在高风险领域如遥感和医学诊断中。VRA通过“思考-批评-行动”循环封装了现成的视觉语言模型和纯视觉系统。尽管VRA在测试时计算成本较高，但其在具有挑战性的视觉推理基准上实现了高达40%的绝对准确度提升。未来的优化工作将集中在减少推理开销的同时保持可靠性上。

**Conclusion:** 尽管VRA框架在测试时计算成本较高，但它大幅提升了视觉推理任务中的准确度。未来将致力于优化查询路由和提前停止机制，以减少推理开销同时保持系统的可靠性。

**Abstract:** Developing trustworthy intelligent vision systems for high-stakes domains,
\emph{e.g.}, remote sensing and medical diagnosis, demands broad robustness
without costly retraining. We propose \textbf{Visual Reasoning Agent (VRA)}, a
training-free, agentic reasoning framework that wraps off-the-shelf
vision-language models \emph{and} pure vision systems in a
\emph{Think--Critique--Act} loop. While VRA incurs significant additional
test-time computation, it achieves up to 40\% absolute accuracy gains on
challenging visual reasoning benchmarks. Future work will optimize query
routing and early stopping to reduce inference overhead while preserving
reliability in vision tasks.

</details>


### [41] [From Canopy to Ground via ForestGen3D: Learning Cross-Domain Generation of 3D Forest Structure from Aerial-to-Terrestrial LiDAR](https://arxiv.org/abs/2509.16346)
*Juan Castorena,E. Louise Loudermilk,Scott Pokswinski,Rodman Linn*

Main category: cs.CV

> ForestGen3D, a novel generative model, synthesizes high-fidelity 3D forest structures using only aerial LiDAR inputs, offering a scalable solution for ecological modeling, wildfire simulation, and structural fuel characterization.

<details>
  <summary>Details</summary>

**Motivation:** Accurate characterization of 3D vegetation structure is crucial for understanding ecological processes affected by natural and human-driven disturbances, but widespread measurement is expensive and often infeasible.

**Method:** The method involves using conditional denoising diffusion probabilistic models (DDPMs) trained on co-registered ALS/TLS data to generate TLS-like 3D point clouds conditioned on sparse ALS observations, with a geometric containment prior ensuring spatial consistency.

**Result:** ForestGen3D was evaluated at various scales and demonstrated high-fidelity reconstructions closely matching TLS references in terms of biophysical metrics.

**Conclusion:** ForestGen3D is positioned as a scalable tool for ecological modeling and wildfire simulation, providing a practical solution for 3D forest structure assessment in ALS-only environments.

**Abstract:** The 3D structure of living and non-living components in ecosystems plays a
critical role in determining ecological processes and feedbacks from both
natural and human-driven disturbances. Anticipating the effects of wildfire,
drought, disease, or atmospheric deposition depends on accurate
characterization of 3D vegetation structure, yet widespread measurement remains
prohibitively expensive and often infeasible. We introduce ForestGen3D, a novel
generative modeling framework that synthesizes high-fidelity 3D forest
structure using only aerial LiDAR (ALS) inputs. ForestGen3D is based on
conditional denoising diffusion probabilistic models (DDPMs) trained on
co-registered ALS/TLS (terrestrial LiDAR) data. The model learns to generate
TLS-like 3D point clouds conditioned on sparse ALS observations, effectively
reconstructing occluded sub-canopy detail at scale. To ensure ecological
plausibility, we introduce a geometric containment prior based on the convex
hull of ALS observations and provide theoretical and empirical guarantees that
generated structures remain spatially consistent. We evaluate ForestGen3D at
tree, plot, and landscape scales using real-world data from mixed conifer
ecosystems, and show that it produces high-fidelity reconstructions that
closely match TLS references in terms of geometric similarity and biophysical
metrics, such as tree height, DBH, crown diameter and crown volume.
Additionally, we demonstrate that the containment property can serve as a
practical proxy for generation quality in settings where TLS ground truth is
unavailable. Our results position ForestGen3D as a scalable tool for ecological
modeling, wildfire simulation, and structural fuel characterization in ALS-only
environments.

</details>


### [42] [Introducing Resizable Region Packing Problem in Image Generation, with a Heuristic Solution](https://arxiv.org/abs/2509.16363)
*Hrishikesh Sharma*

Main category: cs.CV

> 本文针对计算机视觉中的图像数据生成问题，提出了Resizable Anchored Region Packing (RARP) 问题，并设计了一种新的启发式算法来解决它。

<details>
  <summary>Details</summary>

**Motivation:** 本文研究了在计算机视觉中的图像数据生成问题，指出这一问题相较于判别性问题更难解决。为此，本文引入了一个新的、具有实际应用价值的古典装箱问题形式，具体到合成图像数据生成的背景下。

**Method:** 本文提出了一种新型的启发式算法，用于在图像画布中任意位置迭代地打包任意数量和任意形状的区域，同时遵守优化约束。该算法具有广泛适用性。

**Result:** 本文算法通过实现生成了大规模的合成异常检测数据集，其中每个图像样本的装箱参数都有很大差异。通过目视检查数据和验证每个方案的正确性，证明了该算法的有效性。

**Conclusion:** 随着生成模型在深度学习中的兴起以及合成数据生成变得更加主流，本文期望新提出的问题将在图像科研界得到重视。

**Abstract:** The problem of image data generation in computer vision has traditionally
been a harder problem to solve, than discriminative problems. Such data
generation entails placing relevant objects of appropriate sizes each, at
meaningful location in a scene canvas. There have been two classes of popular
approaches to such generation: graphics based, and generative models-based.
Optimization problems are known to lurk in the background for both these
classes of approaches. In this paper, we introduce a novel, practically useful
manifestation of the classical Bin Packing problem in the context of generation
of synthetic image data. We conjecture that the newly introduced problem,
Resizable Anchored Region Packing(RARP) Problem, is NP-hard, and provide
detailed arguments about our conjecture. As a first solution, we present a
novel heuristic algorithm that is generic enough and therefore scales and packs
arbitrary number of arbitrary-shaped regions at arbitrary locations, into an
image canvas. The algorithm follows greedy approach to iteratively pack region
pairs in a careful way, while obeying the optimization constraints. The
algorithm is validated by an implementation that was used to generate a
large-scale synthetic anomaly detection dataset, with highly varying degree of
bin packing parameters per image sample i.e. RARP instance. Visual inspection
of such data and checking of the correctness of each solution proves the
effectiveness of our algorithm. With generative modeling being on rise in deep
learning, and synthetic data generation poised to become mainstream, we expect
that the newly introduced problem will be valued in the imaging scientific
community.

</details>


### [43] [Accurate Thyroid Cancer Classification using a Novel Binary Pattern Driven Local Discrete Cosine Transform Descriptor](https://arxiv.org/abs/2509.16382)
*Saurabh Saini,Kapil Ahuja,Marc C. Steinbach,Thomas Wick*

Main category: cs.CV

> 本文开发了一种新的CAD系统，专用于甲状腺癌的准确分类。系统采用了BPD-LDCT描述符进行特征提取，并使用非线性SVM进行最终分类，取得了非常高的分类准确率。

<details>
  <summary>Details</summary>

**Motivation:** 基于此前对乳腺癌分类的经验，并考虑到甲状腺周围复杂结构带来的挑战，本研究目的是提出一种更好的特征描述符来提升对甲状腺癌分类的准确性。

**Method:** 采用LDCT和ILBP两种纹理捕获描述符来提高特征提取的准确性，并结合非线性SVM进行分类。

**Result:** 该CAD系统在两个公开数据集（TDID和AUITD）上进行了评估，表现出几乎完美的分类性能，特别是在对恶性肿瘤的进一步分类（Stage II）上显示出近100%的准确率。

**Conclusion:** BPD-LDCT描述符在甲状腺癌分类中展现出了强大的性能，并有助于推动在医疗影像处理领域的技术发展。

**Abstract:** In this study, we develop a new CAD system for accurate thyroid cancer
classification with emphasis on feature extraction. Prior studies have shown
that thyroid texture is important for segregating the thyroid ultrasound images
into different classes. Based upon our experience with breast cancer
classification, we first conjuncture that the Discrete Cosine Transform (DCT)
is the best descriptor for capturing textural features. Thyroid ultrasound
images are particularly challenging as the gland is surrounded by multiple
complex anatomical structures leading to variations in tissue density. Hence,
we second conjuncture the importance of localization and propose that the Local
DCT (LDCT) descriptor captures the textural features best in this context.
Another disadvantage of complex anatomy around the thyroid gland is scattering
of ultrasound waves resulting in noisy and unclear textures. Hence, we third
conjuncture that one image descriptor is not enough to fully capture the
textural features and propose the integration of another popular texture
capturing descriptor (Improved Local Binary Pattern, ILBP) with LDCT. ILBP is
known to be noise resilient as well. We term our novel descriptor as Binary
Pattern Driven Local Discrete Cosine Transform (BPD-LDCT). Final classification
is carried out using a non-linear SVM. The proposed CAD system is evaluated on
the only two publicly available thyroid cancer datasets, namely TDID and AUITD.
The evaluation is conducted in two stages. In Stage I, thyroid nodules are
categorized as benign or malignant. In Stage II, the malignant cases are
further sub-classified into TI-RADS (4) and TI-RADS (5). For Stage I
classification, our proposed model demonstrates exceptional performance of
nearly 100% on TDID and 97% on AUITD. In Stage II classification, the proposed
model again attains excellent classification of close to 100% on TDID and 99%
on AUITD.

</details>


### [44] [StereoAdapter: Adapting Stereo Depth Estimation to Underwater Scenes](https://arxiv.org/abs/2509.16415)
*Zhengri Wu,Yiran Wang,Yu Wen,Zeyu Zhang,Biao Wu,Hao Tang*

Main category: cs.CV

> Proposes StereoAdapter, a method that enhances underwater stereo depth estimation through parameter-efficient adaptation and effective fusion of monocular and stereo data, showing significant improvements over state-of-the-art methods.

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges of parameter-efficient adaptation of large vision foundation encoders to the underwater domain and the fusion of globally coherent monocular priors with locally metric stereo correspondences.

**Method:** StereoAdapter, a parameter-efficient self-supervised framework that integrates a LoRA-adapted monocular foundation encoder with a recurrent stereo refinement module.

**Result:** Improvements of 6.11% on TartanAir and 5.12% on SQUID compared to state-of-the-art methods, and consistent robustness shown in real-world deployment with the BlueROV2 robot.

**Conclusion:** The proposed StereoAdapter framework provides robust and precise underwater stereo depth estimation, addressing key challenges in the field with significant performance improvements over existing methods.

**Abstract:** Underwater stereo depth estimation provides accurate 3D geometry for robotics
tasks such as navigation, inspection, and mapping, offering metric depth from
low-cost passive cameras while avoiding the scale ambiguity of monocular
methods. However, existing approaches face two critical challenges: (i)
parameter-efficiently adapting large vision foundation encoders to the
underwater domain without extensive labeled data, and (ii) tightly fusing
globally coherent but scale-ambiguous monocular priors with locally metric yet
photometrically fragile stereo correspondences. To address these challenges, we
propose StereoAdapter, a parameter-efficient self-supervised framework that
integrates a LoRA-adapted monocular foundation encoder with a recurrent stereo
refinement module. We further introduce dynamic LoRA adaptation for efficient
rank selection and pre-training on the synthetic UW-StereoDepth-40K dataset to
enhance robustness under diverse underwater conditions. Comprehensive
evaluations on both simulated and real-world benchmarks show improvements of
6.11% on TartanAir and 5.12% on SQUID compared to state-of-the-art methods,
while real-world deployment with the BlueROV2 robot further demonstrates the
consistent robustness of our approach. Code:
https://github.com/AIGeeksGroup/StereoAdapter. Website:
https://aigeeksgroup.github.io/StereoAdapter.

</details>


### [45] [AHA -- Predicting What Matters Next: Online Highlight Detection Without Looking Ahead](https://arxiv.org/abs/2509.16421)
*Aiden Chang,Celso De Melo,Stephanie M. Lukin*

Main category: cs.CV

> Aha框架实现对连续视频流的实时理解，特别在自动驾驶车辆、监控无人机和灾害响应机器人等关键任务领域可以提供有效的支持。相较之前的离线全上下文和视频-语言模型，Aha在多种标准评估中展示出明显优势。

<details>
  <summary>Details</summary>

**Motivation:** 当前大部分视频理解和亮点检测方法都假设可以在推理时访问整个视频，这使得这些方法不适用于在线或流式传输场景。Aha旨在解决这个问题，提出自回归亮点检测框架，支持实时逐帧推理，以满足智能体在关键任务领域的实时决策需求。

**Method:** 通过自回归方法实现了对连续视频流实时理解，并提出了Dynamic SinkCache机制，确保无限长度视频流上的常量内存使用和性能不降。Aha框架利用多模态视觉-语言模型和轻量级解耦头在大规模人工标注视频数据集上进行训练，实现实时逐帧预测与自然语言任务描述的相关性。

**Result:** Aha在广泛使用的亮点检测评估基准上实现了最先进的性能，其在TVSum上的mAP比之前的最佳离线方法高出了5.9%，在Mr.Hisum上则高出了8.3%。

**Conclusion:** 实验结果表明，Aha具备作为实时推理模块的潜力，能够为下游的长期规划和理解提供有效支持。这为自主性机器人应用程序提供了一种有前景的解决方案。

**Abstract:** Real-time understanding of continuous video streams is essential for
intelligent agents operating in high-stakes environments, including autonomous
vehicles, surveillance drones, and disaster response robots. Yet, most existing
video understanding and highlight detection methods assume access to the entire
video during inference, making them unsuitable for online or streaming
scenarios. In particular, current models optimize for offline summarization,
failing to support step-by-step reasoning needed for real-time decision-making.
We introduce Aha, an autoregressive highlight detection framework that predicts
the relevance of each video frame against a task described in natural language.
Without accessing future video frames, Aha utilizes a multimodal
vision-language model and lightweight, decoupled heads trained on a large,
curated dataset of human-centric video labels. To enable scalability, we
introduce the Dynamic SinkCache mechanism that achieves constant memory usage
across infinite-length streams without degrading performance on standard
benchmarks. This encourages the hidden representation to capture high-level
task objectives, enabling effective frame-level rankings for informativeness,
relevance, and uncertainty with respect to the natural language task. Aha
achieves state-of-the-art (SOTA) performance on highlight detection benchmarks,
surpassing even prior offline, full-context approaches and video-language
models by +5.9% on TVSum and +8.3% on Mr.Hisum in mAP (mean Average Precision).
We explore Aha's potential for real-world robotics applications given a
task-oriented natural language input and a continuous, robot-centric video.
Both experiments demonstrate Aha's potential effectiveness as a real-time
reasoning module for downstream planning and long-horizon understanding.

</details>


### [46] [3D Gaussian Flats: Hybrid 2D/3D Photometric Scene Reconstruction](https://arxiv.org/abs/2509.16423)
*Maria Taktasheva,Lily Goli,Alessandro Fiorini,Zhen,Li,Daniel Rebain,Andrea Tagliasacchi*

Main category: cs.CV

> The paper introduces a novel hybrid 2D/3D representation method that enhances the reconstruction of flat surfaces, achieving better visual and geometric accuracy in indoor scenes.

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to improve the reconstruction of flat, texture-less surfaces, which current techniques struggle with, leading to uneven and semi-transparent reconstructions due to an ill-conditioned photometric reconstruction objective.

**Method:** Our method proposes a novel hybrid 2D/3D representation that uses constrained planar (2D) Gaussians for modeling flat surfaces and freeform (3D) Gaussians for the rest of the scene, which dynamically detects and refines planar regions.

**Result:** The proposed method achieves state-of-the-art depth estimation on ScanNet++ and ScanNetv2 datasets, and excels in mesh extraction without overfitting to a specific camera model.

**Conclusion:** The hybrid 2D/3D representation effectively balances visual fidelity and geometric accuracy, improving the reconstructed quality of flat surfaces in indoor scenes.

**Abstract:** Recent advances in radiance fields and novel view synthesis enable creation
of realistic digital twins from photographs. However, current methods struggle
with flat, texture-less surfaces, creating uneven and semi-transparent
reconstructions, due to an ill-conditioned photometric reconstruction
objective. Surface reconstruction methods solve this issue but sacrifice visual
quality. We propose a novel hybrid 2D/3D representation that jointly optimizes
constrained planar (2D) Gaussians for modeling flat surfaces and freeform (3D)
Gaussians for the rest of the scene. Our end-to-end approach dynamically
detects and refines planar regions, improving both visual fidelity and
geometric accuracy. It achieves state-of-the-art depth estimation on ScanNet++
and ScanNetv2, and excels at mesh extraction without overfitting to a specific
camera model, showing its effectiveness in producing high-quality
reconstruction of indoor scenes.

</details>


### [47] [TractoTransformer: Diffusion MRI Streamline Tractography using CNN and Transformer Networks](https://arxiv.org/abs/2509.16429)
*Itzik Waizman,Yakov Gusakov,Itay Benou,Tammy Riklin Raviv*

Main category: cs.CV

> 提出了一种新的基于Transformer的纤维追踪方法，通过结合CNN特征增强了追踪效果，性能优异。

<details>
  <summary>Details</summary>

**Motivation:** 传统的纤维追踪方法难以应对扩散MRI数据中的噪声和模棱两可的测量问题，尤其在面对交叉、融合和扩散的白质配置时。

**Method:** 我们提出了一种新颖的基于Transformer的脑白质纤维追踪方法，该方法结合了序列建模能力和CNN提取的空间特征来预测纤维方向，从而提高了追踪的精确性和完整性。

**Result:** 使用Tractometer工具包进行评估，我们的方法在性能上与当前最先进的方法相当，并在TractoInferno数据集上展示了对真实世界数据的强大泛化能力。

**Conclusion:** 本研究成功结合了Transformer和CNN的优势，提高了纤维追踪的精度和完整性，展示了其在神经路径映射中的巨大潜力。

**Abstract:** White matter tractography is an advanced neuroimaging technique that
reconstructs the 3D white matter pathways of the brain from diffusion MRI data.
It can be framed as a pathfinding problem aiming to infer neural fiber
trajectories from noisy and ambiguous measurements, facing challenges such as
crossing, merging, and fanning white-matter configurations. In this paper, we
propose a novel tractography method that leverages Transformers to model the
sequential nature of white matter streamlines, enabling the prediction of fiber
directions by integrating both the trajectory context and current diffusion MRI
measurements. To incorporate spatial information, we utilize CNNs that extract
microstructural features from local neighborhoods around each voxel. By
combining these complementary sources of information, our approach improves the
precision and completeness of neural pathway mapping compared to traditional
tractography models. We evaluate our method with the Tractometer toolkit,
achieving competitive performance against state-of-the-art approaches, and
present qualitative results on the TractoInferno dataset, demonstrating strong
generalization to real-world data.

</details>


### [48] [Improved mmFormer for Liver Fibrosis Staging via Missing-Modality Compensation](https://arxiv.org/abs/2509.16436)
*Zhejia Zhang,Junjie Wang,Le Zhang*

Main category: cs.CV

> 本文提出了一种基于mmFormer架构的多模态MRI分类模型，该模型结合了适应性模块用于处理任意组合的缺失模态，通过零填充、模态可用性掩码和可学习统计参数的Delta函数动态合成代理特征以恢复缺失信息，并采用交叉验证集成策略提升预测性能，在CARE 2025挑战的LiFS任务中对T1WI、T2WI和DWI进行肝纤维化分期检测，分别获得66.67%和74.17%的准确率及71.73%和68.48%的AUC得分。

<details>
  <summary>Details</summary>

**Motivation:** 在实际临床场景中，MRI常因设备差异或患者配合问题面临模态缺失，这严重影响模型性能，因此需要开发一种能够适应不同模态组合的模型。

**Method:** 本文提出的方法基于mmFormer架构，采用自适应模块处理任意组合的缺失模态。保留模态特定的编码器及模态关联的编码器，以提取跨可用模态的一致病灶特征。同时集成零填充、模态可用性掩码及可学习参数的Delta函数用于动态合成代理特征，辅助恢复缺失信息。此外，采用交叉验证集成策略提升预测性能。

**Result:** 在CARE 2025挑战的LiFS任务中，本模型对比在不同供应商的数据上对T1WI、T2WI和DWI进行肝纤维化分期检测，针对肝硬化与显著纤维化检测任务分别达到66.67%和74.17%的准确率以及71.73%和68.48%的AUC得分。

**Conclusion:** 研究表明，所提出的多模态MRI分类模型能有效处理临床中常见的模态缺失问题，并提升基于非对比动态MRI扫描的肝纤维化分期检测性能。

**Abstract:** In real-world clinical settings, magnetic resonance imaging (MRI) frequently
suffers from missing modalities due to equipment variability or patient
cooperation issues, which can significantly affect model performance. To
address this issue, we propose a multimodal MRI classification model based on
the mmFormer architecture with an adaptive module for handling arbitrary
combinations of missing modalities. Specifically, this model retains the hybrid
modality-specific encoders and the modality-correlated encoder from mmFormer to
extract consistent lesion features across available modalities. In addition, we
integrate a missing-modality compensation module which leverages zero-padding,
modality availability masks, and a Delta Function with learnable statistical
parameters to dynamically synthesize proxy features for recovering missing
information. To further improve prediction performance, we adopt a
cross-validation ensemble strategy by training multiple models on different
folds and applying soft voting during inference. This method is evaluated on
the test set of Comprehensive Analysis & Computing of REal-world medical images
(CARE) 2025 challenge, targeting the Liver Fibrosis Staging (LiFS) task based
on non-contrast dynamic MRI scans including T1-weighted imaging (T1WI),
T2-weighted imaging (T2WI), and diffusion-weighted imaging (DWI). For Cirrhosis
Detection and Substantial Fibrosis Detection on in-distribution vendors, our
model obtains accuracies of 66.67%, and 74.17%, and corresponding area under
the curve (AUC) scores of 71.73% and 68.48%, respectively.

</details>


### [49] [AutoArabic: A Three-Stage Framework for Localizing Video-Text Retrieval Benchmarks](https://arxiv.org/abs/2509.16438)
*Mohamed Eltahir,Osamah Sarraj,Abdulrahman Alfrihidi,Taha Alshatiri,Mohammed Khurd,Mohammed Bremoo,Tanveer Hussain*

Main category: cs.CV

> AutoArabic框架通过大规模语言模型将非阿拉伯语文本翻译为现代标准阿拉伯语，生成了高质量的视频检索基准DiDeMo-AR，推动了阿拉伯语本地化基准的发展。

<details>
  <summary>Details</summary>

**Motivation:** 动机在于填补目前阿拉伯语在视频到文本和文本到视频检索领域缺乏本地化评估标准的空白，使用非阿拉伯语文本的翻译替代。

**Method:** 我们引入了一个三阶段框架AutoArabic，利用最先进的大规模语言模型（LLMs）将非阿拉伯语基准翻译成现代标准阿拉伯语，将所需的手动修订量减少了近四倍。该框架包含一个误差检测模块，能够以97%的准确性自动标记潜在的翻译错误。

**Result:** 采用该框架处理了视频检索基准DiDeMo，产生了40,144个流畅的阿拉伯语描述版本DiDeMo-AR。通过与英语版本进行对比实验，发现阿拉伯语本地化保持了基准难度。进一步分析表明，随着后期编辑预算的增加，性能逐步提高。

**Conclusion:** 阿拉伯语翻译版本及其评估框架展示了用于视频到文本和文本到视频检索任务的潜力，并鼓励后续更多语言的类似本地化工作。

**Abstract:** Video-to-text and text-to-video retrieval are dominated by English benchmarks
(e.g. DiDeMo, MSR-VTT) and recent multilingual corpora (e.g. RUDDER), yet
Arabic remains underserved, lacking localized evaluation metrics. We introduce
a three-stage framework, AutoArabic, utilizing state-of-the-art large language
models (LLMs) to translate non-Arabic benchmarks into Modern Standard Arabic,
reducing the manual revision required by nearly fourfold. The framework
incorporates an error detection module that automatically flags potential
translation errors with 97% accuracy. Applying the framework to DiDeMo, a video
retrieval benchmark produces DiDeMo-AR, an Arabic variant with 40,144 fluent
Arabic descriptions. An analysis of the translation errors is provided and
organized into an insightful taxonomy to guide future Arabic localization
efforts. We train a CLIP-style baseline with identical hyperparameters on the
Arabic and English variants of the benchmark, finding a moderate performance
gap (about 3 percentage points at Recall@1), indicating that Arabic
localization preserves benchmark difficulty. We evaluate three post-editing
budgets (zero/ flagged-only/ full) and find that performance improves
monotonically with more post-editing, while the raw LLM output (zero-budget)
remains usable. To ensure reproducibility to other languages, we made the code
available at https://github.com/Tahaalshatiri/AutoArabic.

</details>


### [50] [KRAST: Knowledge-Augmented Robotic Action Recognition with Structured Text for Vision-Language Models](https://arxiv.org/abs/2509.16452)
*Son Hai Nguyen,Diwei Wang,Jinhyeok Jang,Hyewon Seo*

Main category: cs.CV

> This paper presents a technique using vision-language models with domain-specific knowledge and prompt learning for indoor action recognition, achieving high accuracy on a benchmark dataset without the need for extensive supervision.

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to enhance the accuracy of vision-based action recognition, which is essential for the development of autonomous robots operating in complex real-world environments.

**Method:** The method leverages vision-language models (VLMs) enriched with domain-specific knowledge to improve action recognition. A prompt-learning framework is used where textual descriptions of actions are embedded as learnable prompts into a pre-trained VLM backbone.

**Result:** Experiments on the ETRI-Activity3D dataset show over 95% accuracy with only RGB video inputs at test time, demonstrating the method's effectiveness in enabling robust action recognition.

**Conclusion:** The conclusion is that knowledge-augmented prompts effectively improve action recognition accuracy with minimal supervision, outperforming state-of-the-art methods.

**Abstract:** Accurate vision-based action recognition is crucial for developing autonomous
robots that can operate safely and reliably in complex, real-world
environments. In this work, we advance video-based recognition of indoor daily
actions for robotic perception by leveraging vision-language models (VLMs)
enriched with domain-specific knowledge. We adapt a prompt-learning framework
in which class-level textual descriptions of each action are embedded as
learnable prompts into a frozen pre-trained VLM backbone. Several strategies
for structuring and encoding these textual descriptions are designed and
evaluated. Experiments on the ETRI-Activity3D dataset demonstrate that our
method, using only RGB video inputs at test time, achieves over 95\% accuracy
and outperforms state-of-the-art approaches. These results highlight the
effectiveness of knowledge-augmented prompts in enabling robust action
recognition with minimal supervision.

</details>


### [51] [Explainable Gait Abnormality Detection Using Dual-Dataset CNN-LSTM Models](https://arxiv.org/abs/2509.16472)
*Parth Agarwal,Sangaa Chatterjee,Md Faisal Kabir,Suman Saha*

Main category: cs.CV

> 提出了一种基于双通道CNN-LSTM框架的增强步态分析模型，通过采用SHAP和Grad-CAM方法提高了解释性，达到了98.6%的准确率。

<details>
  <summary>Details</summary>

**Motivation:** 步态是诊断运动障碍的关键指标，但目前大多数模型缺乏解释性且依赖单一数据集。为了改善这一点，提出了一个新的模型以增强步态分析的解释性。

**Method:** 采用双通道CNN-LSTM框架，结合GAVD数据集的基于关节的1D特性和OU-MVLP数据集的3D剪影，利用SHAP进行时间归属解释，Grad-CAM进行空间定位解释。

**Result:** 该系统在保留集上达到了98.6%的准确率，并保持了良好的召回率和F1得分。

**Conclusion:** 该方法通过在GAVD数据集上的1D分支和在OU-MVLP数据集上的3D分支提高了可解释性和泛化能力，从而在步态分析中达到了98.6%的准确率，并在临床和生物识别领域展示了强大的召回率和F1得分。

**Abstract:** Gait is a key indicator in diagnosing movement disorders, but most models
lack interpretability and rely on single datasets. We propose a dual-branch
CNN-LSTM framework a 1D branch on joint-based features from GAVD and a 3D
branch on silhouettes from OU-MVLP. Interpretability is provided by SHAP
(temporal attributions) and Grad-CAM (spatial localization).On held-out sets,
the system achieves 98.6% accuracy with strong recall and F1. This approach
advances explainable gait analysis across both clinical and biometric domains.

</details>


### [52] [Cross-Corpus and Cross-domain Handwriting Assessment of NeuroDegenerative Diseases via Time-Series-to-Image Conversion](https://arxiv.org/abs/2509.16474)
*Gabrielle Chavez,Laureano Moro-Velazquez,Ankur Butala,Najim Dehak,Thomas Thebaud*

Main category: cs.CV

> 我们通过联合分类器方法改善了手写分析在神经疾病患者中的性能，并展示了模型在跨数据集和多数据集实验中的高准确性。

<details>
  <summary>Details</summary>

**Motivation:** 先前的工作使用基于特征的方法或计算机视觉技术来分析手写任务，但这些方法在面对多个数据集时难以泛化，尤其在时间序列和图像表示之间。

**Method:** 我们提出了一种框架，通过一个基于在ImageNet-1k上预训练的ResNet50的联合分类器来利用手写时间序列和图像信息。

**Result:** 二分类实验在现有的时间序列和图像数据集上表现出最先进的性能，特别是对NeuroLogical Signals (NLS)数据集中的特定绘画和书写任务有显著提升。模型在Draw Clock和Spiral任务上的性能表现优异。跨数据集和多数据集实验能够持续取得高达98的F1评分。

**Conclusion:** 我们的模型展示了对手写信号不同形式泛化的能力，以及提高神经性疾病中运动缺陷检测的潜力。

**Abstract:** Handwriting is significantly affected by neurological disorders (ND) such as
Parkinson's disease (PD) and Alzheimer's disease (AD). Prior works have
analyzed handwriting tasks using feature-based approaches or computer-vision
techniques, but these methods have struggled to generalize across multiple
datasets, particularly between temporal features represented as time-series and
images. We propose a framework that leverages both time-series and images of
handwriting through a joint classifier, based on a ResNet50 pretrained on
ImageNet-1k. Binary classification experiments demonstrate state-of-the-art
performances on existing time-series and image datasets, with significant
improvement on specific drawing and writing tasks from the NeuroLogical Signals
(NLS) dataset. In particular, the proposed model demonstrates improved
performance on Draw Clock and Spiral tasks. Additionally, cross-dataset and
multi-dataset experiments were consistently able to achieve high F1 scores, up
to 98 for PD detection, highlighting the potential of the proposed model to
generalize over different forms of handwriting signals, and enhance the
detection of motor deficits in ND.

</details>


### [53] [Eye Gaze Tells You Where to Compute: Gaze-Driven Efficient VLMs](https://arxiv.org/abs/2509.16476)
*Qinyu Chen,Jiawen Qi*

Main category: cs.CV

> 本文提出了一种无需训练的框架GazeVLM，它使用人类视线作为监督信号，以减少冗余视觉标记，提高模型在边缘计算设备中的推理效率。

<details>
  <summary>Details</summary>

**Motivation:** 现有方法虽然能够减少视觉标记以提高模型推理效率，但往往需要对模型架构进行修改或增加计算和内存，并可能导致准确性降低。此外，模型在选择感兴趣区域时可能会出现不一致的问题。为了解决这些问题，作者提出了一种新的框架。

**Method:** 通过使用人类视线作为自然监督信号，GazeVLM 提出了一种无训练框架，该框架可以减少冗余视觉标记，同时保留任务相关的细节。通过提取视线驱动的兴趣区域（ROIs）并将它们与低分辨率全局视图相结合，GazeVLM 着眼于保留重要的细节部分而减少冗余。

**Result:** 在VOILA-COCO基准上评估的问答任务中，GazeVLM在保留更好的答案质量的同时，减少了高达93.1%的视觉标记，总体标记降低了59.6%，计算浮点操作数减少了50%。

**Conclusion:** 通过将模型计算与人类视线对齐，GazeVLM为消费者设备提供了一种简单、可插拔的方式，提高了Vision-Language模型的推理效率。这种方法在保持答案质量的同时，减少了计算量和记忆负荷。

**Abstract:** Vision-Language Models (VLMs) deliver impressive performance in understanding
visual content with language instructions. However, redundancy in vision tokens
results in the degenerated inference efficiency of VLMs, which hinders
real-time use on edge consumer devices such as AR/VR devices. Existing
efficiency methods commonly prune visual tokens using learned saliency, sparse
attention schedules, or controller policies, but they often require
architectural modification or access to intermediate activations. These
pipelines add inference-time modules that increase compute and memory and often
lead to an accuracy trade-off. Moreover, they also suffer from misalignment
between the prompts and the region of interest in the images. Without human
guidance, the model may focus on the wrong regions and miss small,
high-frequency details when prompts or scenes change. In this paper, we propose
GazeVLM, a training-free framework that uses the human eye gaze as a natural
supervisory signal to allocate computation where it matters. By extracting
gaze-driven regions of interest (ROIs) and optionally combining them with a
low-resolution global view, GazeVLM mimics fovea-periphery perception to cut
redundant visual tokens while preserving task-relevant details. We evaluate the
visual question answering tasks on Qwen2.5-VL-3B/7B on the VOILA-COCO benchmark
with human gaze. Quality of the answer is assessed by GPT-4o pairwise judging
and a weighted score over coverage, accuracy, details, and fluency. Efficiency
is measured by token counts and FLOPs. GazeVLM reduces visual tokens by up to
93.1%, total tokens by up to 59.6%, and FLOPs by 50%, while keeping better
answer quality relative to full-resolution baselines. Our results show that
aligning model computation with human gaze offers a simple, plug-and-play path
toward efficient VLM inference on consumer devices.

</details>


### [54] [Thermal Imaging-based Real-time Fall Detection using Motion Flow and Attention-enhanced Convolutional Recurrent Architecture](https://arxiv.org/abs/2509.16479)
*Christopher Silver,Thangarajah Akilan*

Main category: cs.CV

> An advanced thermal fall detection system using a BiConvLSTM model with various attention mechanisms achieves state-of-the-art performance, aiming for real-time, non-wearable, and privacy-preserving solutions for seniors.

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to address the challenges faced by existing fall detection solutions among seniors, such as reliability, user compliance, and practicality. Stakeholders prefer non-wearable, passive, privacy-preserving, and real-time fall detection systems that require no user interaction.

**Method:** This study proposes an advanced thermal fall detection method using a Bidirectional Convolutional Long Short-Term Memory (BiConvLSTM) model, enhanced with spatial, temporal, feature, self, and general attention mechanisms.

**Result:** Among various tested architectures, BiConvLSTM achieved state-of-the-art performance with a ROC-AUC of 99.7% on the TSF dataset and demonstrated robust results on TF-66, a new, diverse, and privacy-preserving benchmark.

**Conclusion:** The study highlights the generalizability and practicality of the proposed BiConvLSTM model, setting new standards for thermal fall detection and opening doors for high-performance deployable solutions.

**Abstract:** Falls among seniors are a major public health issue. Existing solutions using
wearable sensors, ambient sensors, and RGB-based vision systems face challenges
in reliability, user compliance, and practicality. Studies indicate that
stakeholders, such as older adults and eldercare facilities, prefer
non-wearable, passive, privacy-preserving, and real-time fall detection systems
that require no user interaction. This study proposes an advanced thermal fall
detection method using a Bidirectional Convolutional Long Short-Term Memory
(BiConvLSTM) model, enhanced with spatial, temporal, feature, self, and general
attention mechanisms. Through systematic experimentation across hundreds of
model variations exploring the integration of attention mechanisms, recurrent
modules, and motion flow, we identified top-performing architectures. Among
them, BiConvLSTM achieved state-of-the-art performance with a ROC-AUC of
$99.7\%$ on the TSF dataset and demonstrated robust results on TF-66, a newly
emerged, diverse, and privacy-preserving benchmark. These results highlight the
generalizability and practicality of the proposed model, setting new standards
for thermal fall detection and paving the way toward deployable,
high-performance solutions.

</details>


### [55] [Octree Latent Diffusion for Semantic 3D Scene Generation and Completion](https://arxiv.org/abs/2509.16483)
*Xujia Zhang,Brendan Crowe,Christoffer Heckman*

Main category: cs.CV

> 研发了一种名为Octree Latent Semantic Diffusion的单框架，用于室内和室外场景的3D语义场景完成、扩展和生成，通过双重八叉树图潜变量表示来提升效率和质量。

<details>
  <summary>Details</summary>

**Motivation:** 现有的方法试图将场景完成、扩展和生成分离并单独解决这些问题，大多局限于特定领域，需要针对不同的数据分布（例如室内与室外场景）分别建立模型。为了统一这些技术并提供跨领域的兼容性，我们提出了一种适用于室内和室外场景的方法。

**Method:** 我们的方法基于一个名为Octree Latent Semantic Diffusion的单框架，该框架在一个双重八叉树图潜变量表示上直接操作，用于室内和室外场景的场景完成、扩展和生成。这种方法将合成分为两个阶段：(i) 结构扩散，预测二进制分割信号以构建粗略的占用八叉树；(ii) 潜在语义扩散，生成解码为体素级语义标签的语义嵌入。在推理时，通过部分激光雷达扫描或地图来条件生成，而无需重新训练或微调。

**Result:** 结果表明，我们的模型能够从单个激光雷达扫描中实现高质量的结构、连贯的语义和强大的完成效果，且具备零样本泛化到分布外激光雷达数据的能力。这些结果意味着，通过生成方法在双重八叉树图潜变量空间完成场景是一种适用于实际机器人感知任务的可行且可扩展的替代方案。

**Conclusion:** 研究表明在双重八叉树图潜变量空间通过生成方法完成场景是一种实际且可扩展的解决方案，适合实际的机器人感知任务，能够实现高质量的结构、连贯的语义及泛化到未见过的数据。

**Abstract:** The completion, extension, and generation of 3D semantic scenes are an
interrelated set of capabilities that are useful for robotic navigation and
exploration. Existing approaches seek to decouple these problems and solve them
oneoff. Additionally, these approaches are often domain-specific, requiring
separate models for different data distributions, e.g. indoor vs. outdoor
scenes. To unify these techniques and provide cross-domain compatibility, we
develop a single framework that can perform scene completion, extension, and
generation in both indoor and outdoor scenes, which we term Octree Latent
Semantic Diffusion. Our approach operates directly on an efficient dual octree
graph latent representation: a hierarchical, sparse, and memory-efficient
occupancy structure. This technique disentangles synthesis into two stages: (i)
structure diffusion, which predicts binary split signals to construct a coarse
occupancy octree, and (ii) latent semantic diffusion, which generates semantic
embeddings decoded by a graph VAE into voxellevel semantic labels. To perform
semantic scene completion or extension, our model leverages inference-time
latent inpainting, or outpainting respectively. These inference-time methods
use partial LiDAR scans or maps to condition generation, without the need for
retraining or finetuning. We demonstrate highquality structure, coherent
semantics, and robust completion from single LiDAR scans, as well as zero-shot
generalization to out-of-distribution LiDAR data. These results indicate that
completion-through-generation in a dual octree graph latent space is a
practical and scalable alternative to regression-based pipelines for real-world
robotic perception tasks.

</details>


### [56] [RLGF: Reinforcement Learning with Geometric Feedback for Autonomous Driving Video Generation](https://arxiv.org/abs/2509.16500)
*Tianyi Yan,Wencheng Han,Xia Zhou,Xueyang Zhang,Kun Zhan,Cheng-zhong Xu,Jianbing Shen*

Main category: cs.CV

> 研究提出了一种新方法——带有几何反馈的强化学习（RLGF），用于改进视频生成模型中的几何准确性，显著提高了3D物体检测的性能。

<details>
  <summary>Details</summary>

**Motivation:** 识别并量化现有合成数据生成模型中的几何畸变问题，这些问题限制了它们在下游感知任务中的应用，尤其是在3D物体检测方面。

**Method:** 通过引入带有几何反馈的强化学习（RLGF）来改进视频扩散模型，该方法包括针对扩散过程中的反馈优化技术——高效意义上的窗口优化，以及提供多层级奖励的层次几何奖励系统，用于特征点、线和面的对齐和场景占用的一致性。

**Result:** 使用RLGF大大减少了视频生成中的几何错误，例如视点误差降低21%，深度误差降低57%，并将3D物体检测的mAP提升了12.7%。

**Conclusion:** RLGF为生成适用于自动驾驶开发的高质量合成视频提供了一种即插即用的解决方案。

**Abstract:** Synthetic data is crucial for advancing autonomous driving (AD) systems, yet
current state-of-the-art video generation models, despite their visual realism,
suffer from subtle geometric distortions that limit their utility for
downstream perception tasks. We identify and quantify this critical issue,
demonstrating a significant performance gap in 3D object detection when using
synthetic versus real data. To address this, we introduce Reinforcement
Learning with Geometric Feedback (RLGF), RLGF uniquely refines video diffusion
models by incorporating rewards from specialized latent-space AD perception
models. Its core components include an efficient Latent-Space Windowing
Optimization technique for targeted feedback during diffusion, and a
Hierarchical Geometric Reward (HGR) system providing multi-level rewards for
point-line-plane alignment, and scene occupancy coherence. To quantify these
distortions, we propose GeoScores. Applied to models like DiVE on nuScenes,
RLGF substantially reduces geometric errors (e.g., VP error by 21\%, Depth
error by 57\%) and dramatically improves 3D object detection mAP by 12.7\%,
narrowing the gap to real-data performance. RLGF offers a plug-and-play
solution for generating geometrically sound and reliable synthetic videos for
AD development.

</details>


### [57] [CommonForms: A Large, Diverse Dataset for Form Field Detection](https://arxiv.org/abs/2509.16506)
*Joe Barrow*

Main category: cs.CV

> Error

<details>
  <summary>Details</summary>

**Motivation:** Error

**Method:** Error

**Result:** Error

**Conclusion:** Error

**Abstract:** This paper introduces CommonForms, a web-scale dataset for form field
detection. It casts the problem of form field detection as object detection:
given an image of a page, predict the location and type (Text Input, Choice
Button, Signature) of form fields. The dataset is constructed by filtering
Common Crawl to find PDFs that have fillable elements. Starting with 8 million
documents, the filtering process is used to arrive at a final dataset of
roughly 55k documents that have over 450k pages. Analysis shows that the
dataset contains a diverse mixture of languages and domains; one third of the
pages are non-English, and among the 14 classified domains, no domain makes up
more than 25% of the dataset.
  In addition, this paper presents a family of form field detectors,
FFDNet-Small and FFDNet-Large, which attain a very high average precision on
the CommonForms test set. Each model cost less than $500 to train. Ablation
results show that high-resolution inputs are crucial for high-quality form
field detection, and that the cleaning process improves data efficiency over
using all PDFs that have fillable fields in Common Crawl. A qualitative
analysis shows that they outperform a popular, commercially available PDF
reader that can prepare forms. Unlike the most popular commercially available
solutions, FFDNet can predict checkboxes in addition to text and signature
fields. This is, to our knowledge, the first large scale dataset released for
form field detection, as well as the first open source models. The dataset,
models, and code will be released at https://github.com/jbarrow/commonforms

</details>


### [58] [OS-DiffVSR: Towards One-step Latent Diffusion Model for High-detailed Real-world Video Super-Resolution](https://arxiv.org/abs/2509.16507)
*Hanting Li,Huaao Tang,Jianhong Han,Tianxiong Zhou,Jiulong Cui,Haizhen Xie,Yan Chen,Jie Hu*

Main category: cs.CV

> 本文提出OS-DiffVSR一步扩散模型，通过新型相邻帧对抗训练及多帧融合机制，实现在一步中的高效率高质量视频超分辨。

<details>
  <summary>Details</summary>

**Motivation:** 针对视频超分辨率（VSR）方法在处理每一帧视频时面临的推理效率问题以及视频质量和推理效率之间的权衡，特别是针对扩散模型需要多步扩散来重建高质量视频的需求。

**Method:** 我们提出了一种名为OS-DiffVSR的一步扩散模型用于处理真实世界的视频超分辨率问题。该方法包括一种新颖的相邻帧对抗训练范式来提高合成视频的质量，以及一种多帧融合机制来保持帧间的时间一致性和减少视频闪烁。

**Result:** 实验结果表明，OS-DiffVSR能够实现优于现有的需要几十次采样步骤才能实现的扩散模型的视频质量。

**Conclusion:** 该工作通过提出OS-DiffVSR，实现了在一步内提高真实世界视频超分辨率的质量同时保持高效推理性能，超越了需要多步扩散过程的现有方法。

**Abstract:** Recently, latent diffusion models has demonstrated promising performance in
real-world video super-resolution (VSR) task, which can reconstruct
high-quality videos from distorted low-resolution input through multiple
diffusion steps. Compared to image super-resolution (ISR), VSR methods needs to
process each frame in a video, which poses challenges to its inference
efficiency. However, video quality and inference efficiency have always been a
trade-off for the diffusion-based VSR methods. In this work, we propose
One-Step Diffusion model for real-world Video Super-Resolution, namely
OS-DiffVSR. Specifically, we devise a novel adjacent frame adversarial training
paradigm, which can significantly improve the quality of synthetic videos.
Besides, we devise a multi-frame fusion mechanism to maintain inter-frame
temporal consistency and reduce the flicker in video. Extensive experiments on
several popular VSR benchmarks demonstrate that OS-DiffVSR can even achieve
better quality than existing diffusion-based VSR methods that require dozens of
sampling steps.

</details>


### [59] [SlowFast-SCI: Slow-Fast Deep Unfolding Learning for Spectral Compressive Imaging](https://arxiv.org/abs/2509.16509)
*Haijin Zeng,Xuan Lu,Yurong Zhang,Yongyong Chen,Jingyong Su,Jie Liu*

Main category: cs.CV

> SlowFast-SCI框架可以实现高效的自我适应光谱重建，具备比现有方法更强的泛化能力和更快的适应速度，同时减少参数和计算量。

<details>
  <summary>Details</summary>

**Motivation:** 现存的用于光谱压缩成像的深度展开方法仅模仿人类缓慢的学习过程，缺乏适应新型光学配置的能力，且计算量大、推理速度慢。SlowFast-SCI旨在解决这些问题。

**Method:** SlowFast-SCI框架，结合慢速学习和快速学习。慢速学习阶段预训练基于先验的模型，并通过成像指导提炼出紧凑的快速展开模型。快速学习阶段在每个块中嵌入轻量级适应模块，并通过双重域损失实现无监督自适应训练。

**Result:** 相比于现有模型，SlowFast-SCI参数和计算量减少了70%，在未见分布数据上的PSNR提高5.79 dB，适应速度提升4倍，并且保持跨域适应性。

**Conclusion:** SlowFast-SCI作为首个测试时自适应驱动的深度展开框架，拥有新奇的双阶段设计思想，这将为计算成像提供更多可能。

**Abstract:** Humans learn in two complementary ways: a slow, cumulative process that
builds broad, general knowledge, and a fast, on-the-fly process that captures
specific experiences. Existing deep-unfolding methods for spectral compressive
imaging (SCI) mirror only the slow component-relying on heavy pre-training with
many unfolding stages-yet they lack the rapid adaptation needed to handle new
optical configurations. As a result, they falter on out-of-distribution
cameras, especially in bespoke spectral setups unseen during training. This
depth also incurs heavy computation and slow inference. To bridge this gap, we
introduce SlowFast-SCI, a dual-speed framework seamlessly integrated into any
deep unfolding network beyond SCI systems. During slow learning, we pre-train
or reuse a priors-based backbone and distill it via imaging guidance into a
compact fast-unfolding model. In the fast learning stage, lightweight
adaptation modules are embedded within each block and trained self-supervised
at test time via a dual-domain loss-without retraining the backbone. To the
best of our knowledge, SlowFast-SCI is the first test-time adaptation-driven
deep unfolding framework for efficient, self-adaptive spectral reconstruction.
Its dual-stage design unites offline robustness with on-the-fly per-sample
calibration-yielding over 70% reduction in parameters and FLOPs, up to 5.79 dB
PSNR improvement on out-of-distribution data, preserved cross-domain
adaptability, and a 4x faster adaptation speed. In addition, its modularity
integrates with any deep-unfolding network, paving the way for self-adaptive,
field-deployable imaging and expanded computational imaging modalities. Code
and models are available at https://github.com/XuanLu11/SlowFast-SCI.

</details>


### [60] [Seeing Culture: A Benchmark for Visual Reasoning and Grounding](https://arxiv.org/abs/2509.16517)
*Burak Satar,Zhixin Ma,Patrick A. Irawan,Wilfried A. Mulyawan,Jing Jiang,Ee-Peng Lim,Chong-Wah Ngo*

Main category: cs.CV

> 本文介绍了Seeing Culture Benchmark (SCB)，通过两阶段方法评估多模态视觉语言模型在文化理解任务中的表现，强调了文化多样性在模型训练中的重要性。

<details>
  <summary>Details</summary>

**Motivation:** 随着新的文化数据集的出现，现有的数据集在提供文化推理方面存在不足，并且忽视了许多文化。为了解决这个问题，本文提出了一种新的评估标准。

**Method:** SCB基准测试集中在文化推理上，要求多模态视觉语言模型通过多重选择视觉问题回答来选择正确的视觉选项，并在第一阶段选择正确选项后分割相关的文化艺术品作为推理证据。

**Result:** 评估结果显示，在跨模态文化推理任务中，多模态视觉语言模型存在复杂性和视觉推理与空间定位之间的差异，特别是在文化细微差异的情景中。

**Conclusion:** SCB基准测试对于揭示这些不足之处至关重要，也将引导未来文化推理领域的研究和发展。

**Abstract:** Multimodal vision-language models (VLMs) have made substantial progress in
various tasks that require a combined understanding of visual and textual
content, particularly in cultural understanding tasks, with the emergence of
new cultural datasets. However, these datasets frequently fall short of
providing cultural reasoning while underrepresenting many cultures. In this
paper, we introduce the Seeing Culture Benchmark (SCB), focusing on cultural
reasoning with a novel approach that requires VLMs to reason on culturally rich
images in two stages: i) selecting the correct visual option with
multiple-choice visual question answering (VQA), and ii) segmenting the
relevant cultural artifact as evidence of reasoning. Visual options in the
first stage are systematically organized into three types: those originating
from the same country, those from different countries, or a mixed group.
Notably, all options are derived from a singular category for each type.
Progression to the second stage occurs only after a correct visual option is
chosen. The SCB benchmark comprises 1,065 images that capture 138 cultural
artifacts across five categories from seven Southeast Asia countries, whose
diverse cultures are often overlooked, accompanied by 3,178 questions, of which
1,093 are unique and meticulously curated by human annotators. Our evaluation
of various VLMs reveals the complexities involved in cross-modal cultural
reasoning and highlights the disparity between visual reasoning and spatial
grounding in culturally nuanced scenarios. The SCB serves as a crucial
benchmark for identifying these shortcomings, thereby guiding future
developments in the field of cultural reasoning.
https://github.com/buraksatar/SeeingCulture

</details>


### [61] [FG-Attn: Leveraging Fine-Grained Sparsity In Diffusion Transformers](https://arxiv.org/abs/2509.16518)
*Sankeerth Durvasula,Kavya Sreedhar,Zain Moustafa,Suraj Kothawade,Ashish Gondimalla,Suvinay Subramanian,Narges Shahidi,Nandita Vijaykumar*

Main category: cs.CV

> 本研究旨在改进扩散变换器生成视频时的计算效率问题，提出FG-Attn方法通过提高稀疏注意力计算的精细度，实现了显著的提速。

<details>
  <summary>Details</summary>

**Motivation:** 现有的稀疏注意力方法，如块稀疏注意力，只能在注意力得分矩阵的整个MxM块都是零时跳过得分计算，这种方法在利用注意力图的稀疏性方面较为粗糙，因此还有改进的空间。

**Method:** 研究中引入了一种新的高效批量加载操作，称为异步-聚集加载。这种方法从内存中聚集一小组相关的键值向量，并将它们排列在GPU的共享内存中的密集块中。这使得在计算查询块的注意力时，只加载与这些查询相关的稀疏键，而非像块稀疏注意力那样加载完整的键令牌块。

**Result:** 通过提出FG-Attn，一种适用于长上下文扩散转换器的精细粒度稀疏注意力机制，显著提升了视频生成中的计算效率。实验结果表明，对于5秒的480p和720p视频，FG-Attn平均可提升1.55倍及1.41倍的速度，分别达到最高1.65倍和1.49倍的速度提升，这使得在单个H100 GPU上生成真实视频变得更加高效。

**Conclusion:** 本研究提出了一种更精细粒度的方法来实现稀疏注意力，即FG-Attn，并开发了一种新的高效批量加载操作来实现这种方法，这种方法相较于传统的块稀疏注意力机制在视频生成过程中实现了显著的速度提升。

**Abstract:** Generating realistic videos with diffusion transformers demands significant
computation, with attention layers the central bottleneck; even producing a
short clip requires running a transformer over a very long sequence of
embeddings, e.g., more than 30K embeddings for a 5-second video, incurring
significant latency. Prior work aims to mitigate this bottleneck by exploiting
sparsity in the attention layers to reduce computation. However, these works
typically rely on block-sparse attention, which skips score computation only
when all entries in a block of attention scores (corresponding to M queries and
M keys, with M = 64 typically) are zero. This coarse-granular skipping of
attention scores does not fully exploit sparsity in the attention map and
leaves room for improvement. In this work, we propose FG-Attn, a sparse
attention mechanism for long-context diffusion transformers that leverages
sparsity at a fine granularity. Unlike block-sparse attention, which skips
entire MxM blocks, our approach skips computations at the granularity of Mx1
slices of the attention map. Each slice is produced by query-key dot products
between a block of query vectors and a single key. To implement our proposed
sparse attention mechanism, we develop a new efficient bulk-load operation
called asynchronous-gather load. This load operation gathers a sparse set of
relevant key-value vectors from memory and arranges them into packed tiles in
the GPU's shared memory. Only a sparse set of keys relevant to those queries
are loaded into shared memory when computing attention for a block of queries,
in contrast to loading full blocks of key tokens in block-sparse attention. Our
fine-grained sparse attention, applied to video diffusion models, achieves an
average 1.55X (up to 1.65X) speedup for 5 second, 480p videos, and an average
1.41X (up to 1.49X) for 5 second, 720p videos on a single H100 GPU.

</details>


### [62] [PM25Vision: A Large-Scale Benchmark Dataset for Visual Estimation of Air Quality](https://arxiv.org/abs/2509.16519)
*Yang Han*

Main category: cs.CV

> PM25Vision 是迄今最大且最全面用于从街景图像估算PM2.5浓度的数据集，包含超过11,114张图像，空间准确度达到5公里，提供了CNN和Transformer架构的基线模型性能。

<details>
  <summary>Details</summary>

**Motivation:** 该研究旨在创建一个大规模且准确的数据集，以从街景图像中估算空气质量，特别关注PM2.5浓度的估算。

**Method:** 研究人员收集了11,114张街景图像，并与3,261个AQI监测站的PM2.5读数进行时间戳和地理定位匹配，跨时11年。描述了数据收集、同步和清理的管道，并提供了使用CNN和Transformer架构的基线模型性能。

**Result:** 该数据集包括超过11,114张图像，时间跨度为11年，跨越3,261个空气质量管理站，空间准确度达到5公里。

**Conclusion:** PM25Vision是一个公开可用的、进行空气质量估计的基础数据集。这个数据集的推出将促进基于图像的空气质量监测的研究和技术进步。

**Abstract:** We introduce PM25Vision (PM25V), the largest and most comprehensive dataset
to date for estimating air quality - specifically PM2.5 concentrations - from
street-level images. The dataset contains over 11,114 images matched with
timestamped and geolocated PM2.5 readings across 3,261 AQI monitoring stations
and 11 years, significantly exceeding the scale of previous benchmarks. The
spatial accuracy of this dataset has reached 5 kilometers, far exceeding the
city-level accuracy of many datasets. We describe the data collection,
synchronization, and cleaning pipelines, and provide baseline model
performances using CNN and transformer architectures. Our dataset is publicly
available.

</details>


### [63] [Lattice Boltzmann Model for Learning Real-World Pixel Dynamicity](https://arxiv.org/abs/2509.16527)
*Guangze Zheng,Shijie Lin,Haobo Zuo,Si Si,Ming-Shan Wang,Changhong Fu,Jia Pan*

Main category: cs.CV

> This paper introduces a real-time and efficient method, Lattice Boltzmann Model (LBM), for visual tracking by decomposing visual information into dynamic pixel lattices, validated on various benchmarks.

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind this research is to address the limitations of current methods in real-time and online visual tracking, proposing a more efficient and adaptive model based on the Lattice Boltzmann method.

**Method:** This paper proposes the Lattice Boltzmann Model (LBM) to track visual pixel dynamics. It decomposes visual information into pixel lattices and uses collision-streaming processes to solve for pixel motion states. The model employs a multilayer predict-update network to estimate pixel positions and visibility, using spatial and temporal processes.

**Result:** Evaluations demonstrate that LBM can efficiently adapt to visual tracking tasks in real-world scenarios and performs well on benchmarks such as TAP-Vid, RoboTAP, TAO, BFT, and OVT-B, highlighting its practicality.

**Conclusion:** The LBM method shows practical applicability and real-time efficiency for real-world visual tracking tasks, validated by performance on real-world benchmarks like TAP-Vid, RoboTAP, and open-world object tracking benchmarks such as TAO, BFT, and OVT-B.

**Abstract:** This work proposes the Lattice Boltzmann Model (LBM) to learn real-world
pixel dynamicity for visual tracking. LBM decomposes visual representations
into dynamic pixel lattices and solves pixel motion states through
collision-streaming processes. Specifically, the high-dimensional distribution
of the target pixels is acquired through a multilayer predict-update network to
estimate the pixel positions and visibility. The predict stage formulates
lattice collisions among the spatial neighborhood of target pixels and develops
lattice streaming within the temporal visual context. The update stage
rectifies the pixel distributions with online visual representations. Compared
with existing methods, LBM demonstrates practical applicability in an online
and real-time manner, which can efficiently adapt to real-world visual tracking
tasks. Comprehensive evaluations of real-world point tracking benchmarks such
as TAP-Vid and RoboTAP validate LBM's efficiency. A general evaluation of
large-scale open-world object tracking benchmarks such as TAO, BFT, and OVT-B
further demonstrates LBM's real-world practicality.

</details>


### [64] [Advancing Reference-free Evaluation of Video Captions with Factual Analysis](https://arxiv.org/abs/2509.16538)
*Shubhashis Roy Dipta,Tz-Ying Wu,Subarna Tripathi*

Main category: cs.CV

> 论文提出了一个无需参考标注，专注于事实校验的视频字幕质量评估方法VC-Inspector，利用大型语言模型生成伪标注以训练模型，提高了对视频字幕事实准确性的评估。

<details>
  <summary>Details</summary>

**Motivation:** 由于获取人类标注的视频字幕成本高且困难，现有的依赖参考标注的评估方式不适用于评估野外视频。本论文旨在开发一种新的评估方法，以解决这些问题。

**Method:** 该论文提出了一种无需参考标注的新评估框架VC-Inspector，它使用大模型生成伪标注以训练一个多模态模型，用于评估视频字幕的真实性。

**Result:** 该方法在VATEX-Eval数据集上展示了与人类判断的高度一致性，并在Flickr8K-Expert和Flickr8K-CF图像描述数据集上具有泛化性能。

**Conclusion:** VC-Inspector提供了一种可扩展且普遍适用的方案，用于评估视频字幕的事实准确性，这对多样化的视频领域具有重要价值。

**Abstract:** Video captions offer concise snapshots of actors, objects, and actions within
a video, serving as valuable assets for applications such as question answering
and event localization. However, acquiring human annotations for video captions
is costly or even impractical, especially when dealing with diverse video
domains. Existing models trained on supervised datasets face challenges in
evaluating performance across different domains due to the reliance on
reference-based evaluation protocols, which necessitate ground truth captions.
This assumption is unrealistic for evaluating videos in the wild. To address
these limitations, we propose a reference-free evaluation framework that does
not require ground truth captions, focusing on factual grounding to ensure
accurate assessment of caption quality. We introduce VC-Inspector, a novel
caption quality evaluator that is both reference-free and factually grounded.
Utilizing large language models, we generate pseudo captions of varying quality
based on supervised data, which are subsequently used to train a multimodal
model (i.e., Qwen2.5-VL) as the evaluator. Our approach demonstrates superior
alignment with human judgments on the VATEX-Eval dataset, outperforming
existing methods. The performance also generalizes to image caption datasets,
Flickr8K-Expert and Flickr8K-CF, when viewing images as 1-frame videos.
Overall, VC-Inspector offers a scalable and generalizable solution for
evaluating the factual accuracy of video captions, paving the way for more
effective and objective assessment methodologies in diverse video domains.

</details>


### [65] [Efficient Rectified Flow for Image Fusion](https://arxiv.org/abs/2509.16549)
*Zirui Wang,Jiayi Zhang,Tianwei Guan,Yuhan Zhou,Xingyuan Li,Minjing Dong,Jinyuan Liu*

Main category: cs.CV

> RFfusion提出一种基于Rectified Flow的高效一阶段扩散模型，用于提高图像融合的计算效率和推理速度，同时确保高质量的融合结果。

<details>
  <summary>Details</summary>

**Motivation:** 扩散模型在图像融合领域取得了显著进展，但其计算复杂且推理时间冗长。为解决这一问题，我们提出了RFfusion，旨在提高方法的实用性，同时确保高质量的图像融合结果。

**Method:** 我们提出了一种基于Rectified Flow的一阶段高效扩散模型RFfusion，用于图像融合。通过将Rectified Flow融入图像融合任务中，我们能够简化采样路径并实现一阶段采样，无需额外训练，同时保持高质量的融合结果。此外，我们还提出了一种针对图像融合任务的特定变分自编码器(VAE)架构，其中融合操作嵌入在潜在空间中，进一步降低了计算复杂度。为了解决传统重建导向的VAE目标与图像融合需求之间的固有差异，我们引入了一种两阶段训练策略。

**Result:** 通过引入两阶段训练策略，我们的模型能够有效地学习并集成多模态源图像中的互补信息，同时保持精细的结构细节并显著提高推理效率。

**Conclusion:** 实验结果表明，我们的方法在推理速度和融合质量方面优于其他最先进方法。代码已公开。

**Abstract:** Image fusion is a fundamental and important task in computer vision, aiming
to combine complementary information from different modalities to fuse images.
In recent years, diffusion models have made significant developments in the
field of image fusion. However, diffusion models often require complex
computations and redundant inference time, which reduces the applicability of
these methods. To address this issue, we propose RFfusion, an efficient
one-step diffusion model for image fusion based on Rectified Flow. We
incorporate Rectified Flow into the image fusion task to straighten the
sampling path in the diffusion model, achieving one-step sampling without the
need for additional training, while still maintaining high-quality fusion
results. Furthermore, we propose a task-specific variational autoencoder (VAE)
architecture tailored for image fusion, where the fusion operation is embedded
within the latent space to further reduce computational complexity. To address
the inherent discrepancy between conventional reconstruction-oriented VAE
objectives and the requirements of image fusion, we introduce a two-stage
training strategy. This approach facilitates the effective learning and
integration of complementary information from multi-modal source images,
thereby enabling the model to retain fine-grained structural details while
significantly enhancing inference efficiency. Extensive experiments demonstrate
that our method outperforms other state-of-the-art methods in terms of both
inference speed and fusion quality. Code is available at
https://github.com/zirui0625/RFfusion.

</details>


### [66] [ST-GS: Vision-Based 3D Semantic Occupancy Prediction with Spatial-Temporal Gaussian Splatting](https://arxiv.org/abs/2509.16552)
*Xiaoyang Yan,Muleilan Pei,Shaojie Shen*

Main category: cs.CV

> 我们提出一种新的框架ST-GS，通过引入空间和时间建模的创新策略，来增强基于高斯模型的3D占用预测性能。该方法在大规模nuScenes数据集上展示了优越的性能，并且具有更好的时间一致性。

<details>
  <summary>Details</summary>

**Motivation:** 3D占用预测对于视觉中心的自动驾驶场景理解至关重要。最近的进展探索了利用3D语义高斯模型来进行占用建模，以减少计算开销，但它们仍然受制于多视角空间交互不足和多帧时间一致性有限的问题。为了克服这些挑战，我们提出该框架。

**Method:** 我们提出了一种名为Spatial-Temporal Gaussian Splatting (ST-GS) 的新框架，以增强基于高斯模型的管道中的空间和时间建模。具体而言，我们开发了一种在双模式注意力机制内的指导信息空间聚合策略来增强高斯表示的空间交互。此外，我们引入了一种几何感知的时间融合方案，能够有效地利用历史上下文以提高场景完成的时间连续性。

**Result:** 在大规模nuScenes占用预测基准测试中显示，我们提出的方法不仅达到了最先进的性能，而且在与现有基于高斯模型的方法相比时，在时间一致性上表现得更为优异。

**Conclusion:** 提出的空间时间高斯绘制框架通过增强的空间时间和几何感知的方法，改进了现有高斯模型的性能。实验结果表明其在时空一致性上优于现有的方法。

**Abstract:** 3D occupancy prediction is critical for comprehensive scene understanding in
vision-centric autonomous driving. Recent advances have explored utilizing 3D
semantic Gaussians to model occupancy while reducing computational overhead,
but they remain constrained by insufficient multi-view spatial interaction and
limited multi-frame temporal consistency. To overcome these issues, in this
paper, we propose a novel Spatial-Temporal Gaussian Splatting (ST-GS) framework
to enhance both spatial and temporal modeling in existing Gaussian-based
pipelines. Specifically, we develop a guidance-informed spatial aggregation
strategy within a dual-mode attention mechanism to strengthen spatial
interaction in Gaussian representations. Furthermore, we introduce a
geometry-aware temporal fusion scheme that effectively leverages historical
context to improve temporal continuity in scene completion. Extensive
experiments on the large-scale nuScenes occupancy prediction benchmark showcase
that our proposed approach not only achieves state-of-the-art performance but
also delivers markedly better temporal consistency compared to existing
Gaussian-based methods.

</details>


### [67] [Person Identification from Egocentric Human-Object Interactions using 3D Hand Pose](https://arxiv.org/abs/2509.16557)
*Muhammad Hamza,Danish Hamid,Muhammad Tahir Akram*

Main category: cs.CV

> I2S是一种无干扰的用户识别框架，通过多阶段策略识别人机交互以区分用户，实现在AR环境中的高效用户认证。

<details>
  <summary>Details</summary>

**Motivation:** 研究旨在通过引入I2S框架，改善AR基础的个性化辅助技术中的人机交互识别和用户识别，特别是在航空驾驶舱、航空航天维护和外科手术等高风险环境中。

**Method:** I2S (Interact2Sign)采用多阶段框架，通过3D手部姿态分析来进行无干扰的用户识别，首先识别对象类别，然后进行人机交互识别，最终实现用户识别。

**Result:** 研究通过详尽的特征提取和描述过程对3D手部姿态进行分析，并通过大量消融研究确定了最优特征组合，最终在双手动操作数据集上达到了97.52%的用户识别平均F1-score。

**Conclusion:** I2S框架在双手动操作数据集上实现了97.52%的平均F1-score，展示了其在实时设备认证方面的优越性能和轻量化模型大小（小于4 MB），以及快速的推断时间（0.1秒），使其适合AR系统中的使用。

**Abstract:** Human-Object Interaction Recognition (HOIR) and user identification play a
crucial role in advancing augmented reality (AR)-based personalized assistive
technologies. These systems are increasingly being deployed in high-stakes,
human-centric environments such as aircraft cockpits, aerospace maintenance,
and surgical procedures. This research introduces I2S (Interact2Sign), a multi
stage framework designed for unobtrusive user identification through human
object interaction recognition, leveraging 3D hand pose analysis in egocentric
videos. I2S utilizes handcrafted features extracted from 3D hand poses and per
forms sequential feature augmentation: first identifying the object class,
followed by HOI recognition, and ultimately, user identification. A
comprehensive feature extraction and description process was carried out for 3D
hand poses, organizing the extracted features into semantically meaningful
categories: Spatial, Frequency, Kinematic, Orientation, and a novel descriptor
introduced in this work, the Inter-Hand Spatial Envelope (IHSE). Extensive
ablation studies were conducted to determine the most effective combination of
features. The optimal configuration achieved an impressive average F1-score of
97.52% for user identification, evaluated on a bimanual object manipulation
dataset derived from the ARCTIC and H2O datasets. I2S demonstrates
state-of-the-art performance while maintaining a lightweight model size of
under 4 MB and a fast inference time of 0.1 seconds. These characteristics make
the proposed framework highly suitable for real-time, on-device authentication
in security-critical, AR-based systems.

</details>


### [68] [Captioning for Text-Video Retrieval via Dual-Group Direct Preference Optimization](https://arxiv.org/abs/2509.16560)
*Ji Soo Lee,Byungoh Ko,Jaewon Cho,Howoong Lee,Jaewoon Byun,Hyunwoo J. Kim*

Main category: cs.CV

> 本文提出了CaRe-DPO框架，解决了多模态大语言模型生成的辅助字幕过于通用的问题，优化了文本-视频检索中的字幕生成和细粒度检索效果。

<details>
  <summary>Details</summary>

**Motivation:** 本文研究文本-视频检索中的辅助字幕生成问题。当前，多模态大语言模型虽然能使零样本字幕生成变得强大，但生成的字幕往往过于通用，使得在具有相似视觉特征的视频中难以区分。这限制了其在细粒度检索中的应用。

**Method:** 本文提出了CaRe-DPO，一个利用检索相关性评分直接优化字幕生成的检索框架。其核心是双组直接偏好优化（DG-DPO）学习策略，通过建模不同视频字幕对之间的偏好来进行监督。此外，该模型还采用了角色嵌入来更好地区分具有不同功能性角色的文本输入，比如辅助字幕和文本查询。

**Result:** 通过广泛的实验，作者展示了CaRe-DPO能够显著提升检索性能，通过有效利用辅助知识，生成适用于细粒度检索的字幕。

**Conclusion:** CaRe-DPO模型通过直接优化字幕生成相关的检索相关性评分，成功提升了辅助字幕在细粒度检索中的区分度和实用性。代码可在https://github.com/mlvlab/CaReDPO获得。

**Abstract:** In text-video retrieval, auxiliary captions are often used to enhance video
understanding, bridging the gap between the modalities. While recent advances
in multi-modal large language models (MLLMs) have enabled strong zero-shot
caption generation, we observe that such captions tend to be generic and
indistinguishable across visually similar videos, limiting their utility for
fine-grained retrieval. Moreover, conventional captioning approaches are
typically evaluated using language generation metrics, such as BLEU, which are
not typically tailored for retrieval tasks that require making discriminative
distinctions between candidates. To address this, we propose
$\textbf{CaRe-DPO}$, a retrieval framework that directly optimizes caption
generation using retrieval relevance scores. At its core is Dual-Group Direct
Preference Optimization (DG-DPO), a novel learning strategy that supervises
captioning by modeling preferences across groups of distinct video and caption
pairs. In addition, we present an MLLM-based retrieval model that incorporates
role-embeddings to better distinguish between textual inputs with different
functional roles, such as an auxiliary caption and a text query. Through
extensive experiments, we demonstrate that CaRe-DPO significantly enhances
retrieval performance by effectively leveraging auxiliary knowledge to generate
fine-grained captions for retrieval. Code is available at
https://github.com/mlvlab/CaReDPO.

</details>


### [69] [V-CECE: Visual Counterfactual Explanations via Conceptual Edits](https://arxiv.org/abs/2509.16567)
*Nikolaos Spanos,Maria Lymperaiou,Giorgos Filandrianos,Konstantinos Thomas,Athanasios Voulodimos,Giorgos Stamou*

Main category: cs.CV

> 研究提出了一种新颖的黑盒反事实生成框架，该框架不需要训练即可生成高质量的反事实解释，同时保持可解释性。

<details>
  <summary>Details</summary>

**Motivation:** 当前的黑盒反事实生成框架忽视了所提编辑的语义内容，且高度依赖训练来引导生成过程。为了弥补这一不足，研究提出了一种新的无需训练、可插拔的反事实生成框架。

**Method:** 提出了一种新颖的黑盒反事实生成框架，该框架基于最优编辑的理论保证，逐步提出编辑建议，以产生无需训练的人类级别的反事实解释。该框架采用预训练的图像编辑扩散模型，并且无需访问分类器的内部结构，从而实现可解释的反事实生成过程。

**Result:** 通过实验，尤其是在使用CNN、ViT和LVLM分类器的情况下，发现该框架能够减少人类推理和神经模型行为之间的解释差距，这一结论得到了广泛的人类评估的证实。

**Conclusion:** 研究结果表明，所提出的框架能够有效地产生可解释的反事实解释，而无需访问被解释模型的内部结构，并在多个分类器上进行了验证，表现出了良好的解释功效和人类评估一致性。

**Abstract:** Recent black-box counterfactual generation frameworks fail to take into
account the semantic content of the proposed edits, while relying heavily on
training to guide the generation process. We propose a novel, plug-and-play
black-box counterfactual generation framework, which suggests step-by-step
edits based on theoretical guarantees of optimal edits to produce human-level
counterfactual explanations with zero training. Our framework utilizes a
pre-trained image editing diffusion model, and operates without access to the
internals of the classifier, leading to an explainable counterfactual
generation process. Throughout our experimentation, we showcase the explanatory
gap between human reasoning and neural model behavior by utilizing both
Convolutional Neural Network (CNN), Vision Transformer (ViT) and Large Vision
Language Model (LVLM) classifiers, substantiated through a comprehensive human
evaluation.

</details>


### [70] [A Novel Metric for Detecting Memorization in Generative Models for Brain MRI Synthesis](https://arxiv.org/abs/2509.16582)
*Antonio Scardace,Lemuel Puglisi,Francesco Guarnera,Sebastiano Battiato,Daniele Ravì*

Main category: cs.CV

> 文章提出了一种新的自监督度量DeepSSIM，用于检测生成模型记忆训练数据的能力，在医学图像的合成数据中其表现优于现有的最佳方法。

<details>
  <summary>Details</summary>

**Motivation:** 近期的实证研究表明，深度生成模型存在一个关键弱点：这些模型可能会记住敏感的训练数据，从而引发未经授权的患者信息泄露。检测生成模型中的记忆痕迹尤为困难，需要一种能够识别大规模生成样本中训练数据泄露的可扩展方法。

**Method:** 我们提出了DeepSSIM，这是一种新颖的自监督度量标准，用于量化生成模型中的记忆痕迹。DeepSSIM被训练来：i) 将图像投影到一个学习到的嵌入空间中；ii) 使嵌入之间的余弦相似度匹配图像空间中计算的SSIM（结构相似性指数）得分。为了捕捉特定领域的解剖特征，训练中包含了结构保持增强，这使得DeepSSIM能够可靠地估计相似度，而无需精确的空间对齐。

**Result:** 通过使用2195张来自两个公开数据集（IXI和CoRR）的MRI扫描图像合成的脑部MRI数据的案例研究，DeepSSIM在评估合成数据记忆痕迹时，相较于现有的最佳方法，F1分数平均提升了52.03%。

**Conclusion:** DeepSSIM在检测生成模型记忆痕迹方面表现优异，特别是在处理合成的医学图像数据时。

**Abstract:** Deep generative models have emerged as a transformative tool in medical
imaging, offering substantial potential for synthetic data generation. However,
recent empirical studies highlight a critical vulnerability: these models can
memorize sensitive training data, posing significant risks of unauthorized
patient information disclosure. Detecting memorization in generative models
remains particularly challenging, necessitating scalable methods capable of
identifying training data leakage across large sets of generated samples. In
this work, we propose DeepSSIM, a novel self-supervised metric for quantifying
memorization in generative models. DeepSSIM is trained to: i) project images
into a learned embedding space and ii) force the cosine similarity between
embeddings to match the ground-truth SSIM (Structural Similarity Index) scores
computed in the image space. To capture domain-specific anatomical features,
training incorporates structure-preserving augmentations, allowing DeepSSIM to
estimate similarity reliably without requiring precise spatial alignment. We
evaluate DeepSSIM in a case study involving synthetic brain MRI data generated
by a Latent Diffusion Model (LDM) trained under memorization-prone conditions,
using 2,195 MRI scans from two publicly available datasets (IXI and CoRR).
Compared to state-of-the-art memorization metrics, DeepSSIM achieves superior
performance, improving F1 scores by an average of +52.03% over the best
existing method. Code and data of our approach are publicly available at the
following link: https://github.com/brAIn-science/DeepSSIM.

</details>


### [71] [SQS: Enhancing Sparse Perception Models via Query-based Splatting in Autonomous Driving](https://arxiv.org/abs/2509.16588)
*Haiming Zhang,Yiyao Zhu,Wending Zhou,Xu Yan,Yingjie Cai,Bingbing Liu,Shuguang Cui,Zhen Li*

Main category: cs.CV

> 本文提出了SQS，一种新的预训练方法，旨在提高自动驾驶中的稀疏感知模型（SPM）的性能。通过实验验证了SQS的有效性。

<details>
  <summary>Details</summary>

**Motivation:** 现有的SPM在自动驾驶中的三维感知任务中取得了成功，但是对占用预测和三维物体检测等任务进行微调时，仍然存在较大的改进空间。为了提升这些任务的效果，本文提出了SQS，一种新的基于查询的预训练方法。

**Method:** SQS是一种新的基于查询的预训练方法，专门设计用于提高自动驾驶中的SPM。SQS引入了一个插件模块，该模块在预训练期间从稀疏查询中预测3D高斯表示，利用自监督的涂抹技术来通过多视图图像和深度图的重建学习细粒度的上下文特征。在微调阶段，预训练的高斯查询通过查询交互机制无缝集成到下游网络中，明确地连接预训练查询与任务特定的查询。这样的设计能够有效满足占用预测和三维物体检测等任务的需求。

**Result:** 在自动驾驶基准测试中的大量实验表明，SQS在多个基于查询的三维感知任务中提供了显著的性能提升，尤其在占用预测和三维物体检测方面，优于之前的最先进预训练方法。具体的改进为占用预测提升+1.3个mIoU，三维检测提升+1.0 NDS。

**Conclusion:** 本文通过引入SQS，证明了在自动驾驶中的三维感知任务中，给SPM添加预训练的基于查询的涂抹模块能够展现出优越的性能。

**Abstract:** Sparse Perception Models (SPMs) adopt a query-driven paradigm that forgoes
explicit dense BEV or volumetric construction, enabling highly efficient
computation and accelerated inference. In this paper, we introduce SQS, a novel
query-based splatting pre-training specifically designed to advance SPMs in
autonomous driving. SQS introduces a plug-in module that predicts 3D Gaussian
representations from sparse queries during pre-training, leveraging
self-supervised splatting to learn fine-grained contextual features through the
reconstruction of multi-view images and depth maps. During fine-tuning, the
pre-trained Gaussian queries are seamlessly integrated into downstream networks
via query interaction mechanisms that explicitly connect pre-trained queries
with task-specific queries, effectively accommodating the diverse requirements
of occupancy prediction and 3D object detection. Extensive experiments on
autonomous driving benchmarks demonstrate that SQS delivers considerable
performance gains across multiple query-based 3D perception tasks, notably in
occupancy prediction and 3D object detection, outperforming prior
state-of-the-art pre-training approaches by a significant margin (i.e., +1.3
mIoU on occupancy prediction and +1.0 NDS on 3D detection).

</details>


### [72] [FakeChain: Exposing Shallow Cues in Multi-Step Deepfake Detection](https://arxiv.org/abs/2509.16602)
*Minji Heo,Simon S. Woo*

Main category: cs.CV

> 本研究介绍了FakeChain基准测试，分析了多步伪造检测中模型的性能，强调了检测模型需要更全面地考虑伪造操纵的复杂性。

<details>
  <summary>Details</summary>

**Motivation:** 鉴于以往研究大多聚焦于孤立单一操作的检测，对于由多步或混合深度伪造技术生成的伪造图像检测模型的行为了解甚少，因此本研究的动机是探讨在复杂复合伪造情况下的检测性能。

**Method:** 本研究开发了一个名为FakeChain的大型基准测试，该基准包含了使用五种代表性的生成器合成的1-, 2-, 3-步伪造图像。

**Result:** 研究发现检测性能很大程度上取决于最终的操纵类型，当伪造类型与训练分布不同时，F1-score可下降高达58.83%。

**Conclusion:** 研究结论认为检测模型必须考虑操纵历史和序列，而不仅仅是依赖于最后阶段的伪迹，表明了类似FakeChain这样能够反映现实世界合成复杂性和多样性的基准测试的重要性。

**Abstract:** Multi-step or hybrid deepfakes, created by sequentially applying different
deepfake creation methods such as Face-Swapping, GAN-based generation, and
Diffusion methods, can pose an emerging and unforseen technical challenge for
detection models trained on single-step forgeries. While prior studies have
mainly focused on detecting isolated single manipulation, little is known about
the detection model behavior under such compositional, hybrid, and complex
manipulation pipelines. In this work, we introduce \textbf{FakeChain}, a
large-scale benchmark comprising 1-, 2-, and 3-Step forgeries synthesized using
five state-of-the-art representative generators. Using this approach, we
analyze detection performance and spectral properties across hybrid
manipulation at different step, along with varying generator combinations and
quality settings. Surprisingly, our findings reveal that detection performance
highly depends on the final manipulation type, with F1-score dropping by up to
\textbf{58.83\%} when it differs from training distribution. This clearly
demonstrates that detectors rely on last-stage artifacts rather than cumulative
manipulation traces, limiting generalization. Such findings highlight the need
for detection models to explicitly consider manipulation history and sequences.
Our results highlight the importance of benchmarks such as FakeChain,
reflecting growing synthesis complexity and diversity in real-world scenarios.
Our sample code is available
here\footnote{https://github.com/minjihh/FakeChain}.

</details>


### [73] [Describe-to-Score: Text-Guided Efficient Image Complexity Assessment](https://arxiv.org/abs/2509.16609)
*Shipeng Liu,Zhonglin Zhang,Dengfeng Chen,Liang Zhao*

Main category: cs.CV

> This paper presents D2S, a new approach for image complexity assessment using vision-text fusion that enhances accuracy and generalization by integrating visual and textual features, and outperforms existing methods.

<details>
  <summary>Details</summary>

**Motivation:** The goal is to address the limitations of current image complexity assessment methods by incorporating semantic information from both visual and textual modalities, which enhances representational diversity and reduces hypothesis space complexity.

**Method:** The paper proposes a vision-text fusion method for image complexity assessment called D2S (Describe-to-Score). It integrates visual and textual features by describing images with a pre-trained vision-language model and aligning features and entropy distributions.

**Result:** Experimental results indicate that D2S outperforms existing methods on the IC9600 dataset and remains competitive on no-reference image quality assessment benchmarks, proving its effectiveness and efficiency.

**Conclusion:** The D2S framework improves accuracy and generalization in image complexity assessment by effectively utilizing multimodal information during training while only requiring visual data during inference, leading to efficient assessments. It shows superior performance on the IC9600 dataset and is competitive on NR-IQA benchmarks.

**Abstract:** Accurately assessing image complexity (IC) is critical for computer vision,
yet most existing methods rely solely on visual features and often neglect
high-level semantic information, limiting their accuracy and generalization. We
introduce vision-text fusion for IC modeling. This approach integrates visual
and textual semantic features, increasing representational diversity. It also
reduces the complexity of the hypothesis space, which enhances both accuracy
and generalization in complexity assessment. We propose the D2S
(Describe-to-Score) framework, which generates image captions with a
pre-trained vision-language model. We propose the feature alignment and entropy
distribution alignment mechanisms, D2S guides semantic information to inform
complexity assessment while bridging the gap between vision and text
modalities. D2S utilizes multi-modal information during training but requires
only the vision branch during inference, thereby avoiding multi-modal
computational overhead and enabling efficient assessment. Experimental results
demonstrate that D2S outperforms existing methods on the IC9600 dataset and
maintains competitiveness on no-reference image quality assessment (NR-IQA)
benchmark, validating the effectiveness and efficiency of multi-modal fusion in
complexity-related tasks. Code is available at:
https://github.com/xauat-liushipeng/D2S

</details>
