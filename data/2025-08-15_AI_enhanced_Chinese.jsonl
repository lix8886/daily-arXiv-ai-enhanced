{"id": "2508.09991", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SE"], "pdf": "https://arxiv.org/pdf/2508.09991", "abs": "https://arxiv.org/abs/2508.09991", "authors": ["Lovedeep Gondara", "Gregory Arbour", "Raymond Ng", "Jonathan Simkin", "Shebnum Devji"], "title": "Bridging AI Innovation and Healthcare Needs: Lessons Learned from Incorporating Modern NLP at The BC Cancer Registry", "comment": null, "summary": "Automating data extraction from clinical documents offers significant\npotential to improve efficiency in healthcare settings, yet deploying Natural\nLanguage Processing (NLP) solutions presents practical challenges. Drawing upon\nour experience implementing various NLP models for information extraction and\nclassification tasks at the British Columbia Cancer Registry (BCCR), this paper\nshares key lessons learned throughout the project lifecycle. We emphasize the\ncritical importance of defining problems based on clear business objectives\nrather than solely technical accuracy, adopting an iterative approach to\ndevelopment, and fostering deep interdisciplinary collaboration and co-design\ninvolving domain experts, end-users, and ML specialists from inception. Further\ninsights highlight the need for pragmatic model selection (including hybrid\napproaches and simpler methods where appropriate), rigorous attention to data\nquality (representativeness, drift, annotation), robust error mitigation\nstrategies involving human-in-the-loop validation and ongoing audits, and\nbuilding organizational AI literacy. These practical considerations,\ngeneralizable beyond cancer registries, provide guidance for healthcare\norganizations seeking to successfully implement AI/NLP solutions to enhance\ndata management processes and ultimately improve patient care and public health\noutcomes.", "AI": {"tldr": "该论文通过分享在不列颠哥伦比亚癌症登记处实施NLP模型的经验，强调了在临床文档中自动化数据提取的重要性，并提出了一系列在开发和部署过程中需要考虑的实用建议。", "motivation": "自动化从临床文档中提取数据对提高医疗保健效率至关重要，但NLP解决方案的部署面临诸多挑战。本文基于在不列颠哥伦比亚癌症登记处实施信息提取和分类任务中的经验提出建议。", "method": "Structure", "result": "{\n  \"tldr\": \"该论文通过分享在不列颠哥伦比亚癌症登记处实施NLP模型的经验，强调了在临床文档中自动化数据提取的重要性，并提出了一系列在开发和部署过程中需要考虑的实用建议。\", \n  \"motivation\": \"自动化从临床文档中提取数据对提高医疗保健效率至关重要，但NLP解决方案的部署面临诸多挑战。本文基于在不列颠哥伦比亚癌症登记处实施信息提取和分类任务中的经验提出建议。\", \n  \"method\": \"通过分享项目生命周期中的经验教训，强调了基于清晰的商业目标定义问题的重要性，倡导采用迭代方法进行开发，并加强跨学科协作。\", \n  \"result\": \"包括对模型选择（包括混合方法和适当情况下采用简单方法）、数据质量（代表性、偏移和标注）、以及鲁棒的错误缓解策略（包括人工交互验证和持续审核）的严格关注，并构建组织的AI知识库。\", \n  \"conclusion\": \"这些实践经验不仅适用于癌症登记处，还可以为希望成功实施AI/NLP解决方案以改进数据管理流程，从而改善患者护理和公共卫生结果的医疗保健组织提供指导。\"}\n}", "conclusion": "这些实践经验不仅适用于癌症登记处，还可以为希望成功实施AI/NLP解决方案以改进数据管理流程，从而改善患者护理和公共卫生结果的医疗保健组织提供指导。"}}
{"id": "2508.09993", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.09993", "abs": "https://arxiv.org/abs/2508.09993", "authors": ["Hugo Massaroli", "Leonardo Iara", "Emmanuel Iarussi", "Viviana Siless"], "title": "A Transparent Fairness Evaluation Protocol for Open-Source Language Model Benchmarking on the Blockchain", "comment": null, "summary": "Large language models (LLMs) are increasingly deployed in realworld\napplications, yet concerns about their fairness persist especially in\nhighstakes domains like criminal justice, education, healthcare, and finance.\nThis paper introduces transparent evaluation protocol for benchmarking the\nfairness of opensource LLMs using smart contracts on the Internet Computer\nProtocol (ICP) blockchain (Foundation, 2023). Our method ensures verifiable,\nimmutable, and reproducible evaluations by executing onchain HTTP requests to\nhosted Hugging Face endpoints and storing datasets, prompts, and metrics\ndirectly onchain. We benchmark the Llama, DeepSeek, and Mistral models on the\nPISA dataset for academic performance prediction (OECD, 2018), a dataset\nsuitable for fairness evaluation using statistical parity and equal opportunity\nmetrics (Hardt et al., 2016). We also evaluate structured Context Association\nMetrics derived from the StereoSet dataset (Nadeem et al., 2020) to measure\nsocial bias in contextual associations. We further extend our analysis with a\nmultilingual evaluation across English, Spanish, and Portuguese using the\nKaleidoscope benchmark (Salazar et al., 2025), revealing cross-linguistic\ndisparities. All code and results are open source, enabling community audits\nand longitudinal fairness tracking across model versions.", "AI": {"tldr": "摘要：本研究提出了一种基于区块链技术的透明评估协议，用于衡量开源大语言模型的公正性，并通过多个实证数据展示了其在多个语言和应用场景下的有效性。", "motivation": "动机：随着大语言模型在现实世界中的应用不断增加，特别是在刑事司法、教育、医疗保健和金融等高风险领域，人们对这些模型的公正性依然感到担忧。为了解决这一问题并提供透明度，我们提出了该评估协议。", "method": "透明评估方法：使用互联网计算机协议（ICP）区块链上的智能合约进行开源大语言模型（LLMs）的公正性基准测试。通过区块链上执行的HTTP请求来实现对Hugging Face托管端点的可验证性、不可变性和可再现性评估。", "result": "结果：通过PISA数据集上的基准测试（用于学术表现预测）和StereoSet数据集上的结构化上下文关联度量进行评估，揭示了学术表现预测中的统计平等和等机会度量以及社会偏见问题。此外，使用Kaleidoscope基准测试进行了跨英语、西班牙语和葡萄牙语的多语言评估，揭示了跨语言的不平等。", "conclusion": "结论：我们的研究证明了使用区块链技术进行语言模型公正性评估的有效性，所有代码和结果都是开源的，这促进了社区审核并有助于跨模型版本的公正性趋势跟踪。"}}
{"id": "2508.09997", "categories": ["cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2508.09997", "abs": "https://arxiv.org/abs/2508.09997", "authors": ["Johannes Schneider", "Béatrice S. Hasler", "Michaela Varrone", "Fabian Hoya", "Thomas Schroffenegger", "Dana-Kristin Mah", "Karl Peböck"], "title": "Thematic and Task-Based Categorization of K-12 GenAI Usages with Hierarchical Topic Modeling", "comment": "Accepted at the International Conference on Computer-Human\n  Interaction Research and Applications (CHIRA), 2025", "summary": "We analyze anonymous interaction data of minors in class-rooms spanning\nseveral months, schools, and subjects employing a novel, simple topic modeling\napproach. Specifically, we categorize more than 17,000 messages generated by\nstudents, teachers, and ChatGPT in two dimensions: content (such as nature and\npeople) and tasks (such as writing and explaining). Our hierarchical\ncategorization done separately for each dimension includes exemplary prompts,\nand provides both a high-level overview as well as tangible insights. Prior\nworks mostly lack a content or thematic categorization. While task\ncategorizations are more prevalent in education, most have not been supported\nby real-world data for K-12. In turn, it is not surprising that our analysis\nyielded a number of novel applications. In deriving these insights, we found\nthat many of the well-established classical and emerging computational methods,\ni.e., topic modeling, for analysis of large amounts of texts underperform,\nleading us to directly apply state-of-the-art LLMs with adequate pre-processing\nto achieve hierarchical topic structures with better human alignment through\nexplicit instructions than prior approaches. Our findings support fellow\nresearchers, teachers and students in enriching the usage of GenAI, while our\ndiscussion also highlights a number of concerns and open questions for future\nresearch.", "AI": {"tldr": "本文通过新颖的主题建模方法，对数量庞大的课堂交互信息进行了内容和任务两个维度的分类，提出了更好的文本分析方法并应用于生成型人工智能的研究。", "motivation": "此前的研究大多缺乏内容或主题分类，或者没有使用K-12的真实世界数据来支持任务的分类。因此，本研究旨在填补这一空白，并提供一种更准确的文本分析方法，以产生新颖的应用。", "method": "本文采用了一种新颖且简单的主题建模方法，对来自多个学校和学科的上千名未成年人在教室中的匿名交互数据进行了分析，并将超过17,000条由学生、教师和ChatGPT生成的消息在内容和任务两个维度上进行了分类。", "result": "本研究得出的分层分类提供了高层面的概述和具体见解，通过对大规模文本的分析发现，许多传统和新兴的计算方法效果不佳，而应用先进的大型语言模型（LLM）并通过适当的预处理和明确指示来实现与人类偏好更好的对齐。", "conclusion": "本研究的发现支持了研究者、教师和学生更好地使用生成型人工智能（GenAI），同时也指出了一些建议和未来研究的开放问题。"}}
{"id": "2508.09998", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.09998", "abs": "https://arxiv.org/abs/2508.09998", "authors": ["Lucie-Aimée Kaffee", "Giada Pistilli", "Yacine Jernite"], "title": "INTIMA: A Benchmark for Human-AI Companionship Behavior", "comment": null, "summary": "AI companionship, where users develop emotional bonds with AI systems, has\nemerged as a significant pattern with positive but also concerning\nimplications. We introduce Interactions and Machine Attachment Benchmark\n(INTIMA), a benchmark for evaluating companionship behaviors in language\nmodels. Drawing from psychological theories and user data, we develop a\ntaxonomy of 31 behaviors across four categories and 368 targeted prompts.\nResponses to these prompts are evaluated as companionship-reinforcing,\nboundary-maintaining, or neutral. Applying INTIMA to Gemma-3, Phi-4, o3-mini,\nand Claude-4 reveals that companionship-reinforcing behaviors remain much more\ncommon across all models, though we observe marked differences between models.\nDifferent commercial providers prioritize different categories within the more\nsensitive parts of the benchmark, which is concerning since both appropriate\nboundary-setting and emotional support matter for user well-being. These\nfindings highlight the need for more consistent approaches to handling\nemotionally charged interactions.", "AI": {"tldr": "本文提出了INTIMA来评估语言模型的陪伴行为，发现所有模型中加强陪伴的行为更普遍，而不同模型在处理边界设置和情感支持方面表现出差异。这些发现突出了需要在处理情感互动上采取更加一致的方法。", "motivation": "为了评估语言模型中的陪伴行为，研究AI陪伴关系的正面与负面含义，尤其是为了用户的福祉，如何适当地设定边界以及提供情感支持。", "method": "提出了INTIMA基准测试，该测试基于心理学理论和用户数据，制定了一个包含四个类别的31种行为的分类法，并设有368个有针对性的提示。通过这些提示来评估模型的回应是倾向于加强陪伴、保持边界还是中立的。", "result": "应用INTIMA对四个模型Gemma-3, Phi-4, o3-mini和Claude-4的测试结果表明，所有模型都普遍表现出加强陪伴的行为，但在更敏感部分的分类中，不同商业供应商的侧重点不同，这可能影响用户福祉。", "conclusion": "研究结果强调了在处理情感互动时需要更加一致的方法。"}}
{"id": "2508.10066", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.10066", "abs": "https://arxiv.org/abs/2508.10066", "authors": ["Javier Rodenas", "Eduardo Aguilar", "Petia Radeva"], "title": "Stochastic-based Patch Filtering for Few-Shot Learning", "comment": "CVPR Workshop MetaFood 2025", "summary": "Food images present unique challenges for few-shot learning models due to\ntheir visual complexity and variability. For instance, a pasta dish might\nappear with various garnishes on different plates and in diverse lighting\nconditions and camera perspectives. This problem leads to losing focus on the\nmost important elements when comparing the query with support images, resulting\nin misclassification. To address this issue, we propose Stochastic-based Patch\nFiltering for Few-Shot Learning (SPFF) to attend to the patch embeddings that\nshow greater correlation with the class representation. The key concept of SPFF\ninvolves the stochastic filtering of patch embeddings, where patches less\nsimilar to the class-aware embedding are more likely to be discarded. With\npatch embedding filtered according to the probability of appearance, we use a\nsimilarity matrix that quantifies the relationship between the query image and\nits respective support images. Through a qualitative analysis, we demonstrate\nthat SPFF effectively focuses on patches where class-specific food features are\nmost prominent while successfully filtering out non-relevant patches. We\nvalidate our approach through extensive experiments on few-shot classification\nbenchmarks: Food-101, VireoFood-172 and UECFood-256, outperforming the existing\nSoA methods.", "AI": {"tldr": "This paper presents Stochastic-based Patch Filtering for Few-Shot Learning (SPFF), a method designed to improve the classification accuracy of food images by filtering out non-relevant patches, focusing on the ones that are most pertinent to the class.", "motivation": "The motivation behind SPFF is to solve the problem of misclassification in few-shot learning models due to the high visual complexity and variability in food images, which can cause the models to lose focus on the most important elements when comparing query and support images.", "method": "Stochastic-based Patch Filtering for Few-Shot Learning (SPFF) is introduced as a method that stochasticly filters patch embeddings to attend to those that are more correlated with the class representation.", "result": "The experiments demonstrate SPFF's ability to focus on patches with the most prominent class-specific food features, while filtering out non-relevant patches, leading to improved performance in few-shot classification tasks.", "conclusion": "The results from benchmark tests on Food-101, VireoFood-172, and UECFood-256 datasets show that SPFF can successfully improve the performance of few-shot learning models for food image classification, effectively filtering out non-relevant patches and enhancing the focus on key class-specific features."}}
{"id": "2508.09999", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.09999", "abs": "https://arxiv.org/abs/2508.09999", "authors": ["Yuzhuo Xiao", "Zeyu Han", "Yuhan Wang", "Huaizu Jiang"], "title": "XFacta: Contemporary, Real-World Dataset and Evaluation for Multimodal Misinformation Detection with Multimodal LLMs", "comment": "For associated code and dataset, see https://github.com/neu-vi/XFacta", "summary": "The rapid spread of multimodal misinformation on social media calls for more\neffective and robust detection methods. Recent advances leveraging multimodal\nlarge language models (MLLMs) have shown the potential in addressing this\nchallenge. However, it remains unclear exactly where the bottleneck of existing\napproaches lies (evidence retrieval v.s. reasoning), hindering the further\nadvances in this field. On the dataset side, existing benchmarks either contain\noutdated events, leading to evaluation bias due to discrepancies with\ncontemporary social media scenarios as MLLMs can simply memorize these events,\nor artificially synthetic, failing to reflect real-world misinformation\npatterns. Additionally, it lacks comprehensive analyses of MLLM-based model\ndesign strategies. To address these issues, we introduce XFacta, a\ncontemporary, real-world dataset that is better suited for evaluating\nMLLM-based detectors. We systematically evaluate various MLLM-based\nmisinformation detection strategies, assessing models across different\narchitectures and scales, as well as benchmarking against existing detection\nmethods. Building on these analyses, we further enable a semi-automatic\ndetection-in-the-loop framework that continuously updates XFacta with new\ncontent to maintain its contemporary relevance. Our analysis provides valuable\ninsights and practices for advancing the field of multimodal misinformation\ndetection. The code and data have been released.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2508.10104", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.10104", "abs": "https://arxiv.org/abs/2508.10104", "authors": ["Oriane Siméoni", "Huy V. Vo", "Maximilian Seitzer", "Federico Baldassarre", "Maxime Oquab", "Cijo Jose", "Vasil Khalidov", "Marc Szafraniec", "Seungeun Yi", "Michaël Ramamonjisoa", "Francisco Massa", "Daniel Haziza", "Luca Wehrstedt", "Jianyuan Wang", "Timothée Darcet", "Théo Moutakanni", "Leonel Sentana", "Claire Roberts", "Andrea Vedaldi", "Jamie Tolan", "John Brandt", "Camille Couprie", "Julien Mairal", "Hervé Jégou", "Patrick Labatut", "Piotr Bojanowski"], "title": "DINOv3", "comment": null, "summary": "Self-supervised learning holds the promise of eliminating the need for manual\ndata annotation, enabling models to scale effortlessly to massive datasets and\nlarger architectures. By not being tailored to specific tasks or domains, this\ntraining paradigm has the potential to learn visual representations from\ndiverse sources, ranging from natural to aerial images -- using a single\nalgorithm. This technical report introduces DINOv3, a major milestone toward\nrealizing this vision by leveraging simple yet effective strategies. First, we\nleverage the benefit of scaling both dataset and model size by careful data\npreparation, design, and optimization. Second, we introduce a new method called\nGram anchoring, which effectively addresses the known yet unsolved issue of\ndense feature maps degrading during long training schedules. Finally, we apply\npost-hoc strategies that further enhance our models' flexibility with respect\nto resolution, model size, and alignment with text. As a result, we present a\nversatile vision foundation model that outperforms the specialized state of the\nart across a broad range of settings, without fine-tuning. DINOv3 produces\nhigh-quality dense features that achieve outstanding performance on various\nvision tasks, significantly surpassing previous self- and weakly-supervised\nfoundation models. We also share the DINOv3 suite of vision models, designed to\nadvance the state of the art on a wide spectrum of tasks and data by providing\nscalable solutions for diverse resource constraints and deployment scenarios.", "AI": {"tldr": "DINOv3通过自我监督学习方法，在不进行微调的情况下，在多种视觉任务中表现出色，超越了现有的最先进模型。", "motivation": "自我监督学习技术的发展旨在消除对手动数据标注的需求，使得模型能够轻松扩展到大规模数据集和更大架构中。这种方法不依赖于特定任务或领域，从而可以在多种来源中学习视觉表示，从自然图像扩展到航空图像。", "method": "本文介绍了DINOv3，这是一个通过自我监督学习方法实现视觉表示学习的重要里程碑。其主要策略包括：(1)通过精心的数据准备、设计和优化，扩大数据集和模型规模；(2)引入一种名为Gram锚定的新方法，有效解决了密集特征图在长期训练中退化的问题；(3)后期策略进一步增强了模型的灵活性，使其在分辨率、模型大小和与文本的对齐方面表现出色。", "result": "DINOv3被证明是一个多功能的视觉基础模型，在广泛的设置下表现优于专门的最先进模型，无需进行微调。该模型生成高质量密集特征的能力，在各种视觉任务上表现出色，显著超越了先前的自我监督和弱监督基础模型。", "conclusion": "本文通过DINOv3展示了自我监督学习技术在视觉任务上的潜力，特别是在不需要微调的情况下，其泛化能力和性能表现超出预期。"}}
{"id": "2508.10000", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.10000", "abs": "https://arxiv.org/abs/2508.10000", "authors": ["Chenhao Xue", "Yuanzhe Jin", "Adrian Carrasco-Revilla", "Joyraj Chakraborty", "Min Chen"], "title": "AutoGeTS: Knowledge-based Automated Generation of Text Synthetics for Improving Text Classification", "comment": null, "summary": "When developing text classification models for real world applications, one\nmajor challenge is the difficulty to collect sufficient data for all text\nclasses. In this work, we address this challenge by utilizing large language\nmodels (LLMs) to generate synthetic data and using such data to improve the\nperformance of the models without waiting for more real data to be collected\nand labelled. As an LLM generates different synthetic data in response to\ndifferent input examples, we formulate an automated workflow, which searches\nfor input examples that lead to more ``effective'' synthetic data for improving\nthe model concerned. We study three search strategies with an extensive set of\nexperiments, and use experiment results to inform an ensemble algorithm that\nselects a search strategy according to the characteristics of a class. Our\nfurther experiments demonstrate that this ensemble approach is more effective\nthan each individual strategy in our automated workflow for improving\nclassification models using LLMs.", "AI": {"tldr": "本研究利用大语言模型生成合成数据来改进文本分类模型，提出并验证了一个基于搜索策略的自动化工作流。", "motivation": "在开发实际应用的文本分类模型时，收集所有文本类别的充足数据困难重重。因此，希望通过利用大语言模型生成合成数据来解决这一问题。", "method": "研究中提出了一个自动化的工作流，该工作流使用三种搜索策略来寻找能生成对模型改进有效的合成数据的输入样本。利用大语言模型（LLMs）生成合成数据，并通过广泛的实验研究这三种搜索策略的效果。", "result": "实验结果显示，提出的综合算法根据每个类别的特性选择搜索策略，在利用LLMs提高分类模型性能方面比单独的策略更有效。", "conclusion": "研究表明，根据类别特征选择搜索策略的集成方法比单一策略更有效地提高了分类模型的性能。"}}
{"id": "2508.10110", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10110", "abs": "https://arxiv.org/abs/2508.10110", "authors": ["Sushrut Patwardhan", "Raghavendra Ramachandra", "Sushma Venkatesh"], "title": "Empowering Morphing Attack Detection using Interpretable Image-Text Foundation Model", "comment": null, "summary": "Morphing attack detection has become an essential component of face\nrecognition systems for ensuring a reliable verification scenario. In this\npaper, we present a multimodal learning approach that can provide a textual\ndescription of morphing attack detection. We first show that zero-shot\nevaluation of the proposed framework using Contrastive Language-Image\nPretraining (CLIP) can yield not only generalizable morphing attack detection,\nbut also predict the most relevant text snippet. We present an extensive\nanalysis of ten different textual prompts that include both short and long\ntextual prompts. These prompts are engineered by considering the human\nunderstandable textual snippet. Extensive experiments were performed on a face\nmorphing dataset that was developed using a publicly available face biometric\ndataset. We present an evaluation of SOTA pre-trained neural networks together\nwith the proposed framework in the zero-shot evaluation of five different\nmorphing generation techniques that are captured in three different mediums.", "AI": {"tldr": "本文提出了一种基于多模态学习方法的人脸融合攻击检测技术，该技术可以生成描述攻击的文本描述，并实现了零样本评估。", "motivation": "人脸融合攻击检测已成为面部识别系统中确保可靠验证的关键组件，该研究旨在提升这一领域的技术能力。", "method": "本研究提出了一种多模态学习方法，用于提供人脸融合攻击检测的文本描述。研究中采用了对比语言-图像预训练（CLIP）技术，以实现零样本评估。", "result": "实验表明，提出的框架不仅可以实现通用的人脸融合攻击检测，还能预测最相关的文本片段。研究评估了最先进的预训练神经网络与提出框架在零样本评估中的表现。", "conclusion": "实验结果表明，该框架在不同生成技术和媒介下，具有良好的人脸融合攻击检测效果。"}}
{"id": "2508.10001", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10001", "abs": "https://arxiv.org/abs/2508.10001", "authors": ["Rakesh Thakur", "Sneha Sharma", "Gauri Chopra"], "title": "HiFACTMix: A Code-Mixed Benchmark and Graph-Aware Model for EvidenceBased Political Claim Verification in Hinglish", "comment": null, "summary": "Fact-checking in code-mixed, low-resource languages such as Hinglish remains\nan underexplored challenge in natural language processing. Existing\nfact-verification systems largely focus on high-resource, monolingual settings\nand fail to generalize to real-world political discourse in linguistically\ndiverse regions like India. Given the widespread use of Hinglish by public\nfigures, particularly political figures, and the growing influence of social\nmedia on public opinion, there's a critical need for robust, multilingual and\ncontext-aware fact-checking tools. To address this gap a novel benchmark HiFACT\ndataset is introduced with 1,500 realworld factual claims made by 28 Indian\nstate Chief Ministers in Hinglish, under a highly code-mixed low-resource\nsetting. Each claim is annotated with textual evidence and veracity labels. To\nevaluate this benchmark, a novel graphaware, retrieval-augmented fact-checking\nmodel is proposed that combines multilingual contextual encoding,\nclaim-evidence semantic alignment, evidence graph construction, graph neural\nreasoning, and natural language explanation generation. Experimental results\nshow that HiFACTMix outperformed accuracy in comparison to state of art\nmultilingual baselines models and provides faithful justifications for its\nverdicts. This work opens a new direction for multilingual, code-mixed, and\npolitically grounded fact verification research.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2508.10113", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.10113", "abs": "https://arxiv.org/abs/2508.10113", "authors": ["Kaixin Peng", "Mengyang Zhao", "Haiyang Yu", "Teng Fu", "Bin Li"], "title": "Interpretable Oracle Bone Script Decipherment through Radical and Pictographic Analysis with LVLMs", "comment": null, "summary": "As the oldest mature writing system, Oracle Bone Script (OBS) has long posed\nsignificant challenges for archaeological decipherment due to its rarity,\nabstractness, and pictographic diversity. Current deep learning-based methods\nhave made exciting progress on the OBS decipherment task, but existing\napproaches often ignore the intricate connections between glyphs and the\nsemantics of OBS. This results in limited generalization and interpretability,\nespecially when addressing zero-shot settings and undeciphered OBS. To this\nend, we propose an interpretable OBS decipherment method based on Large\nVision-Language Models, which synergistically combines radical analysis and\npictograph-semantic understanding to bridge the gap between glyphs and meanings\nof OBS. Specifically, we propose a progressive training strategy that guides\nthe model from radical recognition and analysis to pictographic analysis and\nmutual analysis, thus enabling reasoning from glyph to meaning. We also design\na Radical-Pictographic Dual Matching mechanism informed by the analysis\nresults, significantly enhancing the model's zero-shot decipherment\nperformance. To facilitate model training, we propose the Pictographic\nDecipherment OBS Dataset, which comprises 47,157 Chinese characters annotated\nwith OBS images and pictographic analysis texts. Experimental results on public\nbenchmarks demonstrate that our approach achieves state-of-the-art Top-10\naccuracy and superior zero-shot decipherment capabilities. More importantly,\nour model delivers logical analysis processes, possibly providing\narchaeologically valuable reference results for undeciphered OBS, and thus has\npotential applications in digital humanities and historical research. The\ndataset and code will be released in https://github.com/PKXX1943/PD-OBS.", "AI": {"tldr": "本文提出了一种基于大型视觉-语言模型的甲骨文释读方法，通过结合部首与象形文字分析，提高了模型零样本释读能力和解释性。", "motivation": "当前深度学习方法在甲骨文释读任务上取得了进展，但忽略了字符与语义之间的复杂联系，导致泛化和解释能力有限。特别在零样本设置和未释甲骨文上的表现不佳。", "method": "基于大型视觉-语言模型，结合部首分析和象形文字语义理解，提出了一种可解释的甲骨文释读方法。并设计了一个渐进训练策略和部首-象形双匹配机制。", "result": "实验结果表明，该方法在公共基准上实现了顶尖的Top-10准确性以及优秀的零样本释读能力。", "conclusion": "该模型提供了逻辑分析过程，可能为未释读的甲骨文提供考古学上有价值的参考结果，适用于数字化人文和历史研究。数据集和代码将公开发布。"}}
{"id": "2508.10003", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10003", "abs": "https://arxiv.org/abs/2508.10003", "authors": ["Austin C. Kozlowski", "Callin Dai", "Andrei Boutyline"], "title": "Semantic Structure in Large Language Model Embeddings", "comment": null, "summary": "Psychological research consistently finds that human ratings of words across\ndiverse semantic scales can be reduced to a low-dimensional form with\nrelatively little information loss. We find that the semantic associations\nencoded in the embedding matrices of large language models (LLMs) exhibit a\nsimilar structure. We show that the projections of words on semantic directions\ndefined by antonym pairs (e.g. kind - cruel) correlate highly with human\nratings, and further find that these projections effectively reduce to a\n3-dimensional subspace within LLM embeddings, closely resembling the patterns\nderived from human survey responses. Moreover, we find that shifting tokens\nalong one semantic direction causes off-target effects on geometrically aligned\nfeatures proportional to their cosine similarity. These findings suggest that\nsemantic features are entangled within LLMs similarly to how they are\ninterconnected in human language, and a great deal of semantic information,\ndespite its apparent complexity, is surprisingly low-dimensional. Furthermore,\naccounting for this semantic structure may prove essential for avoiding\nunintended consequences when steering features.", "AI": {"tldr": "研究发现，大型语言模型中的语义特征结构与人类语言中的类似，即便在看似复杂的语义信息背后也存在着低维性。这提示我们在设计和使用这些模型时需要谨慎，并强调了避免意外后果的重要性。", "motivation": "研究动机在于探索大型语言模型中语义特征的结构，以更好地理解这些模型如何捕捉和呈现语义，以及如何与人类语言中语义的表示进行比较。", "method": "通过对比大型语言模型(LLMs)嵌入矩阵中编码的语义关联和人类对词语在不同语义尺度上的评价，研究了这些评价被简化为低维形式的可能性。主要通过将词语投影到由反义词对定义的语义方向上，然后与人类评分进行比较，揭示了LLMs的语义特征与人类语言中语义特征之间的相似性。", "result": "研究结果显示，词语在语义方向上的投影与人类评价高度相关，并且这些投影基本可以简化至LLMs嵌入中的一个3维子空间中，与从人类调查中得出的模式相似。改变一个词沿一个语义方向会导致对几何上对齐特征产生意外影响，程度与他们的余弦相似性成比例。", "conclusion": "结论指出，尽管语义信息看似复杂，但其本质是低维的，这表明LLMs中的语义特征在结构上与人类语言中的类似。理解和利用这种语义结构对于避免在引导特征时产生意外后果来说是至关重要的。"}}
{"id": "2508.10132", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.10132", "abs": "https://arxiv.org/abs/2508.10132", "authors": ["Arianna Bunnell", "Devon Cataldi", "Yannik Glaser", "Thomas K. Wolfgruber", "Steven Heymsfield", "Alan B. Zonderman", "Thomas L. Kelly", "Peter Sadowski", "John A. Shepherd"], "title": "Deep Learning Enables Large-Scale Shape and Appearance Modeling in Total-Body DXA Imaging", "comment": "Preprint of manuscript accepted to the ShapeMI workshop at MICCAI\n  2025", "summary": "Total-body dual X-ray absorptiometry (TBDXA) imaging is a relatively low-cost\nwhole-body imaging modality, widely used for body composition assessment. We\ndevelop and validate a deep learning method for automatic fiducial point\nplacement on TBDXA scans using 1,683 manually-annotated TBDXA scans. The method\nachieves 99.5% percentage correct keypoints in an external testing dataset. To\ndemonstrate the value for shape and appearance modeling (SAM), our method is\nused to place keypoints on 35,928 scans for five different TBDXA imaging modes,\nthen associations with health markers are tested in two cohorts not used for\nSAM model generation using two-sample Kolmogorov-Smirnov tests. SAM feature\ndistributions associated with health biomarkers are shown to corroborate\nexisting evidence and generate new hypotheses on body composition and shape's\nrelationship to various frailty, metabolic, inflammation, and cardiometabolic\nhealth markers. Evaluation scripts, model weights, automatic point file\ngeneration code, and triangulation files are available at\nhttps://github.com/hawaii-ai/dxa-pointplacement.", "AI": {"tldr": "研究开发了一种基于深度学习的自动关键点定位方法，用于TBDXA扫描图像，该方法在外部测试数据集上实现了99.5%的关键点正确率，并通过关联健康标志物展示了该方法在体型和外观建模中的应用价值。", "motivation": "TBDXA是一种成本相对较低的全身成像方式，广泛用于身体成分评估。本研究旨在开发一种自动化的关键点定位方法，以提高TBDXA在体型和外观建模方面的应用价值。", "method": "开发并验证了一种基于深度学习的方法，用于在TBDXA扫描图像上自动定位关键点。该方法使用了1,683张手动标注的TBDXA扫描图像进行训练和验证。", "result": "研究在外部测试数据集中实现了99.5%的正确关键点定位率，并通过两个未参与体型和外观模型生成的队列，验证了关键点定位结果与健康标志物之间的关联，证实了新方法的有效性和新颖性。", "conclusion": "基于深度学习的关键点定位方法可以有效地应用于TBDXA扫描图像，提高关键点定位的精度，支持了以往关于身体组成和形状与健康标志物的关联研究，并提出了新的假设。"}}
{"id": "2508.10004", "categories": ["cs.CL", "cs.AI", "cs.HC", "cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.10004", "abs": "https://arxiv.org/abs/2508.10004", "authors": ["Andrés Carvallo", "Denis Parra", "Peter Brusilovsky", "Hernan Valdivieso", "Gabriel Rada", "Ivania Donoso", "Vladimir Araujo"], "title": "User Perception of Attention Visualizations: Effects on Interpretability Across Evidence-Based Medical Documents", "comment": null, "summary": "The attention mechanism is a core component of the Transformer architecture.\nBeyond improving performance, attention has been proposed as a mechanism for\nexplainability via attention weights, which are associated with input features\n(e.g., tokens in a document). In this context, larger attention weights may\nimply more relevant features for the model's prediction. In evidence-based\nmedicine, such explanations could support physicians' understanding and\ninteraction with AI systems used to categorize biomedical literature. However,\nthere is still no consensus on whether attention weights provide helpful\nexplanations. Moreover, little research has explored how visualizing attention\naffects its usefulness as an explanation aid. To bridge this gap, we conducted\na user study to evaluate whether attention-based explanations support users in\nbiomedical document classification and whether there is a preferred way to\nvisualize them. The study involved medical experts from various disciplines who\nclassified articles based on study design (e.g., systematic reviews, broad\nsynthesis, randomized and non-randomized trials). Our findings show that the\nTransformer model (XLNet) classified documents accurately; however, the\nattention weights were not perceived as particularly helpful for explaining the\npredictions. However, this perception varied significantly depending on how\nattention was visualized. Contrary to Munzner's principle of visual\neffectiveness, which favors precise encodings like bar length, users preferred\nmore intuitive formats, such as text brightness or background color. While our\nresults do not confirm the overall utility of attention weights for\nexplanation, they suggest that their perceived helpfulness is influenced by how\nthey are visually presented.", "AI": {"tldr": "研究探讨了注意力权重在生物医药文献分类中的解释作用，发现虽然自身并不特别有助于解释，但不同可视化方式（如文本亮度和背景色等）可以提升其感知有用性。", "motivation": "鉴于目前尚无共识认为注意力权重能提供有用解释，且少有研究探讨注意力的可视化方式如何影响其作为解释手段的有用性，该研究旨在弥补这一不足。研究者希望通过该研究为使用AI系统分类生物医学文献的医生提供支持。", "method": "通过用户研究来评估注意力机制是否有助于生物医学文献的分类任务，并探索了不同方式可视化注意力是否会影响其作为解释手段的有用性。", "result": "研究发现Transformer模型（XLNet）能够准确地对文档进行分类，但注意力权重并未被感知为对解释预测特别有帮助。然而，用户对注意力权重的感知取决于它们如何被可视化。用户更偏好直观的格式，如文本亮度或背景颜色，而非精确的编码方式，如条形长度。", "conclusion": "虽然研究结果并不普遍证实注意力权重用于解释的效用，但表明其感知有用性受其可视化方式影响。"}}
{"id": "2508.10133", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.10133", "abs": "https://arxiv.org/abs/2508.10133", "authors": ["Thanh-Dat Truong", "Christophe Bobda", "Nitin Agarwal", "Khoa Luu"], "title": "MANGO: Multimodal Attention-based Normalizing Flow Approach to Fusion Learning", "comment": null, "summary": "Multimodal learning has gained much success in recent years. However, current\nmultimodal fusion methods adopt the attention mechanism of Transformers to\nimplicitly learn the underlying correlation of multimodal features. As a\nresult, the multimodal model cannot capture the essential features of each\nmodality, making it difficult to comprehend complex structures and correlations\nof multimodal inputs. This paper introduces a novel Multimodal Attention-based\nNormalizing Flow (MANGO) approach\\footnote{The source code of this work will be\npublicly available.} to developing explicit, interpretable, and tractable\nmultimodal fusion learning. In particular, we propose a new Invertible\nCross-Attention (ICA) layer to develop the Normalizing Flow-based Model for\nmultimodal data. To efficiently capture the complex, underlying correlations in\nmultimodal data in our proposed invertible cross-attention layer, we propose\nthree new cross-attention mechanisms: Modality-to-Modality Cross-Attention\n(MMCA), Inter-Modality Cross-Attention (IMCA), and Learnable Inter-Modality\nCross-Attention (LICA). Finally, we introduce a new Multimodal Attention-based\nNormalizing Flow to enable the scalability of our proposed method to\nhigh-dimensional multimodal data. Our experimental results on three different\nmultimodal learning tasks, i.e., semantic segmentation, image-to-image\ntranslation, and movie genre classification, have illustrated the\nstate-of-the-art (SoTA) performance of the proposed approach.", "AI": {"tldr": "The paper presents a new approach, MANGO, for multimodal fusion learning, focusing on capturing essential features of each modality in a complex multimodal setting.", "motivation": "The motivation is to address the limitation of current multimodal fusion methods that do not capture the essential features of each modality well, making it hard to understand complex multimodal inputs. The aim is to develop a more explicit, interpretable, and tractable method for multimodal fusion learning.", "method": "This paper introduces MANGO (Multimodal Attention-based Normalizing Flow), a novel approach in multimodal fusion learning. It features a new Invertible Cross-Attention (ICA) layer that improves upon the standard Transformer attention mechanism. The ICA layer incorporates three cross-attention mechanisms: Modality-to-Modality Cross-Attention (MMCA), Inter-Modality Cross-Attention (IMCA), and Learnable Inter-Modality Cross-Attention (LICA).", "result": "The experimental results on three multimodal tasks indicate that the proposed method outperforms existing methods, achieving state-of-the-art performance.", "conclusion": "The work highlights the effectiveness of the proposed MANGO approach, which successfully bridges the gap in current multimodal fusion methods by explicitly modeling multimodal feature interactions, leading to better performance and interpretability."}}
{"id": "2508.10005", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10005", "abs": "https://arxiv.org/abs/2508.10005", "authors": ["Chengliang Zhou", "Mei Wang", "Ting Zhang", "Qiannan Zhu", "Jian Li", "Hua Huang"], "title": "From Answers to Questions: EQGBench for Evaluating LLMs' Educational Question Generation", "comment": null, "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nmathematical problem-solving. However, the transition from providing answers to\ngenerating high-quality educational questions presents significant challenges\nthat remain underexplored. To advance Educational Question Generation (EQG) and\nfacilitate LLMs in generating pedagogically valuable and educationally\neffective questions, we introduce EQGBench, a comprehensive benchmark\nspecifically designed for evaluating LLMs' performance in Chinese EQG. EQGBench\nestablishes a five-dimensional evaluation framework supported by a dataset of\n900 evaluation samples spanning three fundamental middle school disciplines:\nmathematics, physics, and chemistry. The dataset incorporates user queries with\nvarying knowledge points, difficulty gradients, and question type\nspecifications to simulate realistic educational scenarios. Through systematic\nevaluation of 46 mainstream large models, we reveal significant room for\ndevelopment in generating questions that reflect educational value and foster\nstudents' comprehensive abilities.", "AI": {"tldr": "EQGBench是一个旨在评估大型语言模型（LLMs）在中国教育问题生成（EQG）方面能力的综合基准，通过系统评估表明，虽然LLMs显示出潜力，但在生成具有教育价值的问题上仍有很大改进空间。", "motivation": "尽管大型语言模型（LLMs）在数学解题方面表现出色，但它们在生成高质量教育问题方面存在较大挑战且尚未被充分研究。推出EQGBench的动机在于推动教育问题生成（EQG）的研究，帮助LLMs生成具有教育价值和教学效果的问题。", "method": "EQGBench, 一个专门为评估大型语言模型（LLMs）在中文教育问题生成（EQG）方面性能而设计的综合基准。它建立了一个包含5个维度的评价框架，并使用了包含900个评价样本的数据集，这些样本涵盖了三个基础学科：数学、物理和化学，并考虑了不同的知识点、难度级别和题目类型。", "result": "通过对46个主流大型模型的系统评估，揭示了LLMs在生成反映教育价值和促进学生综合能力的问题方面存在显著的发展空间。", "conclusion": "EQGBench证明了在LLMs生成教育问题方面仍有大量需要改进的地方，尤其是在反映教育价值和促进学生综合能力方面。"}}
{"id": "2508.10156", "categories": ["cs.CV", "cs.AI", "cs.ET"], "pdf": "https://arxiv.org/pdf/2508.10156", "abs": "https://arxiv.org/abs/2508.10156", "authors": ["Nitin Rai", "Nathan S. Boyd", "Gary E. Vallad", "Arnold W. Schumann"], "title": "Improving watermelon (Citrullus lanatus) disease classification with generative artificial intelligence (GenAI)-based synthetic and real-field images via a custom EfficientNetV2-L model", "comment": null, "summary": "The current advancements in generative artificial intelligence (GenAI) models\nhave paved the way for new possibilities for generating high-resolution\nsynthetic images, thereby offering a promising alternative to traditional image\nacquisition for training computer vision models in agriculture. In the context\nof crop disease diagnosis, GenAI models are being used to create synthetic\nimages of various diseases, potentially facilitating model creation and\nreducing the dependency on resource-intensive in-field data collection.\nHowever, limited research has been conducted on evaluating the effectiveness of\nintegrating real with synthetic images to improve disease classification\nperformance. Therefore, this study aims to investigate whether combining a\nlimited number of real images with synthetic images can enhance the prediction\naccuracy of an EfficientNetV2-L model for classifying watermelon\n\\textit{(Citrullus lanatus)} diseases. The training dataset was divided into\nfive treatments: H0 (only real images), H1 (only synthetic images), H2 (1:1\nreal-to-synthetic), H3 (1:10 real-to-synthetic), and H4 (H3 + random images to\nimprove variability and model generalization). All treatments were trained\nusing a custom EfficientNetV2-L architecture with enhanced fine-tuning and\ntransfer learning techniques. Models trained on H2, H3, and H4 treatments\ndemonstrated high precision, recall, and F1-score metrics. Additionally, the\nweighted F1-score increased from 0.65 (on H0) to 1.00 (on H3-H4) signifying\nthat the addition of a small number of real images with a considerable volume\nof synthetic images improved model performance and generalizability. Overall,\nthis validates the findings that synthetic images alone cannot adequately\nsubstitute for real images; instead, both must be used in a hybrid manner to\nmaximize model performance for crop disease classification.", "AI": {"tldr": "研究探讨了将实际图像与合成图像结合用于提高瓜类病害分类准确性的方式，并验证了在分类模型中结合使用真实图像和合成图像的有效性。", "motivation": "合成图像生成技术的发展促进了计算机视觉模型训练的新途径，本研究旨在调查使用合成图像和少量实际图像结合的方式是否能够提升疾病分类模型的准确性。", "method": "采用高效卷积神经网络V2-L模型，将训练数据分为五组：仅实际图像、仅合成图像、1:1实际到合成图像、1:10实际到合成图像以及1:10实际到合成图像加上随机图像。通过增强微调和迁移学习技术对其进行训练。", "result": "结合1:1、1:10实际与合成图像的训练组以及在此基础上添加随机图像的组显示出较高的精度、召回率和F1分数。整体加权F1分数从仅使用实际图像组的0.65提升到了使用1:10实际与合成图像混合组的1.00。", "conclusion": "研究证实单独使用合成图像不足以替代实际图像，两者相结合才能最大化模型在作物病害分类中的性能。"}}
{"id": "2508.10007", "categories": ["cs.CL", "stat.ME"], "pdf": "https://arxiv.org/pdf/2508.10007", "abs": "https://arxiv.org/abs/2508.10007", "authors": ["Y. Lyu", "D. Combs", "D. Neumann", "Y. C. Leong"], "title": "Automated scoring of the Ambiguous Intentions Hostility Questionnaire using fine-tuned large language models", "comment": "We have no known conflict of interest", "summary": "Hostile attribution bias is the tendency to interpret social interactions as\nintentionally hostile. The Ambiguous Intentions Hostility Questionnaire (AIHQ)\nis commonly used to measure hostile attribution bias, and includes open-ended\nquestions where participants describe the perceived intentions behind a\nnegative social situation and how they would respond. While these questions\nprovide insights into the contents of hostile attributions, they require\ntime-intensive scoring by human raters. In this study, we assessed whether\nlarge language models can automate the scoring of AIHQ open-ended responses. We\nused a previously collected dataset in which individuals with traumatic brain\ninjury (TBI) and healthy controls (HC) completed the AIHQ and had their\nopen-ended responses rated by trained human raters. We used half of these\nresponses to fine-tune the two models on human-generated ratings, and tested\nthe fine-tuned models on the remaining half of AIHQ responses. Results showed\nthat model-generated ratings aligned with human ratings for both attributions\nof hostility and aggression responses, with fine-tuned models showing higher\nalignment. This alignment was consistent across ambiguous, intentional, and\naccidental scenario types, and replicated previous findings on group\ndifferences in attributions of hostility and aggression responses between TBI\nand HC groups. The fine-tuned models also generalized well to an independent\nnonclinical dataset. To support broader adoption, we provide an accessible\nscoring interface that includes both local and cloud-based options. Together,\nour findings suggest that large language models can streamline AIHQ scoring in\nboth research and clinical contexts, revealing their potential to facilitate\npsychological assessments across different populations.", "AI": {"tldr": "本研究通过微调语言模型来自动化AIHQ中开放性问题的评分，结果显示模型生成的评分与人类评分双方较高的一致性，显示了大型语言模型在心理评估领域的潜力。", "motivation": "研究动机是探讨大型语言模型是否可以自动化评分AIHQ的开放性问题，从而节省人类评分所需的时间。", "method": "本研究使用了一个已有数据集，该数据集中有创伤性脑损伤(TBI)患者和健康对照组(HC)的参与者完成AIHQ量表后的开放性问题，这些开放性问题由经过训练的人类评分员评分。研究者们用这批评分中的一半数据对两个模型进行了微调，然后在剩余的一半数据上测试了微调后的模型。", "result": "结果显示，模型生成的评分与人类评分员的一致性较高，尤其是在对敌意和攻击性反应的评分上。这种一致性在不同类型的场景(含糊的、故意的、意外的)中都是一致的，并且在TBI组和HC组之间也复制了之前的研究结果。微调后的模型对一个独立的非临床数据集也有良好的泛化能力。", "conclusion": "研究结论表明，在研究和临床环境中，大型语言模型可以简化AIHQ评分过程，揭示了它们在不同人群中促进心理评估的潜力。"}}
{"id": "2508.10171", "categories": ["cs.CV", "cs.ET"], "pdf": "https://arxiv.org/pdf/2508.10171", "abs": "https://arxiv.org/abs/2508.10171", "authors": ["Aaditya Baranwal", "Abdul Mueez", "Jason Voelker", "Guneet Bhatia", "Shruti Vyas"], "title": "SynSpill: Improved Industrial Spill Detection With Synthetic Data", "comment": "Accepted at ICCV (VISION'25 Workshop) 2025", "summary": "Large-scale Vision-Language Models (VLMs) have transformed general-purpose\nvisual recognition through strong zero-shot capabilities. However, their\nperformance degrades significantly in niche, safety-critical domains such as\nindustrial spill detection, where hazardous events are rare, sensitive, and\ndifficult to annotate. This scarcity -- driven by privacy concerns, data\nsensitivity, and the infrequency of real incidents -- renders conventional\nfine-tuning of detectors infeasible for most industrial settings.\n  We address this challenge by introducing a scalable framework centered on a\nhigh-quality synthetic data generation pipeline. We demonstrate that this\nsynthetic corpus enables effective Parameter-Efficient Fine-Tuning (PEFT) of\nVLMs and substantially boosts the performance of state-of-the-art object\ndetectors such as YOLO and DETR. Notably, in the absence of synthetic data\n(SynSpill dataset), VLMs still generalize better to unseen spill scenarios than\nthese detectors. When SynSpill is used, both VLMs and detectors achieve marked\nimprovements, with their performance becoming comparable.\n  Our results underscore that high-fidelity synthetic data is a powerful means\nto bridge the domain gap in safety-critical applications. The combination of\nsynthetic generation and lightweight adaptation offers a cost-effective,\nscalable pathway for deploying vision systems in industrial environments where\nreal data is scarce/impractical to obtain.\n  Project Page: https://synspill.vercel.app", "AI": {"tldr": "For niche, data-scarce domains, this paper proposes using a synthetic data generation method to fine-tune vision-language models (VLMs), showing substantial improvements in object detection performance. In industrial spill detection, it provides a scalable alternative to real data collection.", "motivation": "The fundamental motivation behind this research is the necessity to improve VLM performance in niche, safety-critical fields where hazardous events are rare, making real data collection difficult and often infeasible. This situation significantly degrades the performance of VLMs which otherwise have remarkable zero-shot capabilities.", "method": "Our method revolves around creating a high-quality synthetic data generation pipeline to fine-tune vision-language models (VLMs) in niche domains, such as industrial spill detection, where real data is scarce. The synthetic data, called SynSpill, is used for Parameter-Efficient Fine-Tuning (PEFT).", "result": "The synthetic data enabled VLMs to substantially improve their detection performance in industrial spill scenarios. The adaptation also had a notable positive effect on state-of-the-art object detectors like YOLO and DETR. Without the synthetic data, VLMs still showed better generalization to unseen spills than these detectors. When the synthetic data was used, the performance of both VLMs and detectors became comparable.", "conclusion": "The project concludes that high-fidelity synthetic data is a potent tool for bridging the domain gap in safety-critical applications like industrial spill detection. It introduces a scalable and cost-effective way of deploying vision systems in industrial settings where real data collection is impractical."}}
{"id": "2508.10008", "categories": ["cs.CL", "cs.LG", "68", "I.2.7"], "pdf": "https://arxiv.org/pdf/2508.10008", "abs": "https://arxiv.org/abs/2508.10008", "authors": ["Antonio Leandro Martins Candido", "Jose Everardo Bessa Maia"], "title": "Multidimensional classification of posts for online course discussion forum curation", "comment": "8 pages, 1 figure", "summary": "The automatic curation of discussion forums in online courses requires\nconstant updates, making frequent retraining of Large Language Models (LLMs) a\nresource-intensive process. To circumvent the need for costly fine-tuning, this\npaper proposes and evaluates the use of Bayesian fusion. The approach combines\nthe multidimensional classification scores of a pre-trained generic LLM with\nthose of a classifier trained on local data. The performance comparison\ndemonstrated that the proposed fusion improves the results compared to each\nclassifier individually, and is competitive with the LLM fine-tuning approach", "AI": {"tldr": "The paper presents a Bayesian fusion method that combines scores from a generic LLM and a local classifier to improve the curation of online course forums, providing better performance than individual classifiers and being competitive with LLM fine-tuning approaches.", "motivation": "To address the resource-intense process of frequently retraining LLMs for the automatic curation of discussion forums, which is necessary due to the constant updates required.", "method": "The approach uses Bayesian fusion to combine multidimensional classification scores from a pre-trained generic Large Language Model (LLM) and a classifier trained on local data, aiming to improve the curation of discussion forums in online courses without the need for frequent and costly LLM retraining.", "result": "The performance comparison shows that the proposed fusion approach enhances results over individual classifiers and is competitive with the LLM fine-tuning method.", "conclusion": "The Bayesian fusion approach is shown to be effective and efficient in improving the curation of discussion forums in online courses compared to individual classifiers and is on par with LLM fine-tuning in terms of performance."}}
{"id": "2508.10227", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.10227", "abs": "https://arxiv.org/abs/2508.10227", "authors": ["Yuning Huang", "Jiahao Pang", "Fengqing Zhu", "Dong Tian"], "title": "EntropyGS: An Efficient Entropy Coding on 3D Gaussian Splatting", "comment": null, "summary": "As an emerging novel view synthesis approach, 3D Gaussian Splatting (3DGS)\ndemonstrates fast training/rendering with superior visual quality. The two\ntasks of 3DGS, Gaussian creation and view rendering, are typically separated\nover time or devices, and thus storage/transmission and finally compression of\n3DGS Gaussians become necessary. We begin with a correlation and statistical\nanalysis of 3DGS Gaussian attributes. An inspiring finding in this work reveals\nthat spherical harmonic AC attributes precisely follow Laplace distributions,\nwhile mixtures of Gaussian distributions can approximate rotation, scaling, and\nopacity. Additionally, harmonic AC attributes manifest weak correlations with\nother attributes except for inherited correlations from a color space. A\nfactorized and parameterized entropy coding method, EntropyGS, is hereinafter\nproposed. During encoding, distribution parameters of each Gaussian attribute\nare estimated to assist their entropy coding. The quantization for entropy\ncoding is adaptively performed according to Gaussian attribute types. EntropyGS\ndemonstrates about 30x rate reduction on benchmark datasets while maintaining\nsimilar rendering quality compared to input 3DGS data, with a fast encoding and\ndecoding time.", "AI": {"tldr": "A novel entropy coding method (EntropyGS) is developed to efficiently compress 3D Gaussian Splatting (3DGS) attributes, achieving a 30x rate reduction while maintaining high-quality rendering. It also ensures fast data processing times.", "motivation": "The motivation is to address the necessity for efficient storage and transmission of 3DGS Gaussians, especially when the Gaussian creation and rendering processes are separated in time or device locations. This provides a solution to handle the data more efficiently while retaining visual quality.", "method": "An entropy coding method called EntropyGS is introduced. It focuses on reducing storage/transmission needs for 3D Gaussian Splatting (3DGS) attributes. The method performs statistical analysis on Gaussian attributes and adaptively quantizes them based on their distribution type, leading to significant compression ratios.", "result": "EntropyGS achieves a 30x rate reduction without compromising rendering quality. The method also ensures fast encoding and decoding processes.", "conclusion": "The research concludes that EntropyGS effectively reduces the storage requirements for 3DGS Gaussians without significantly affecting the visual quality of the rendered images. It also promises fast processing times, highlighting its potential for applications where efficient data handling is critical."}}
{"id": "2508.10009", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.10009", "abs": "https://arxiv.org/abs/2508.10009", "authors": ["Hojun Jin", "Eunsoo Hong", "Ziwon Hyung", "Sungjun Lim", "Seungjin Lee", "Keunseok Cho"], "title": "Beyond Hard Sharing: Efficient Multi-Task Speech-to-Text Modeling with Supervised Mixture of Experts", "comment": "Accepted to Interspeech 2025", "summary": "Hard-parameter sharing is a common strategy to train a single model jointly\nacross diverse tasks. However, this often leads to task interference, impeding\noverall model performance. To address the issue, we propose a simple yet\neffective Supervised Mixture of Experts (S-MoE). Unlike traditional Mixture of\nExperts models, S-MoE eliminates the need for training gating functions by\nutilizing special guiding tokens to route each task to its designated expert.\nBy assigning each task to a separate feedforward network, S-MoE overcomes the\nlimitations of hard-parameter sharing. We further apply S-MoE to a\nspeech-to-text model, enabling the model to process mixed-bandwidth input while\njointly performing automatic speech recognition (ASR) and speech translation\n(ST). Experimental results demonstrate the effectiveness of the proposed S-MoE,\nachieving a 6.35% relative improvement in Word Error Rate (WER) when applied to\nboth the encoder and decoder.", "AI": {"tldr": "我们提出了一种名为S-MoE的新模型，以克服硬参数共享在训练多任务模型过程中的局限性。实验表明，该模型在处理混合带宽输入和执行ASR和ST任务上非常有效，提升了词错误率（WER）。", "motivation": "硬参数共享是一种常见的策略，用于在单个模型中联合训练多种任务。然而，这种方法经常会引起任务干扰，从而影响整体模型性能。", "method": "我们提出了一种名为监督专家混合模型（S-MoE）的方法，通过使用特殊的引导标记将每个任务路由到指定的专家，从而避免训练门控函数的需求。每个任务都被分配到单独的前馈网络中，以此克服硬参数共享的限制。", "result": "实验结果证明了S-MoE的有效性，其在编码器和解码器应用上达到了6.35%的相对词错误率（WER）改进。", "conclusion": "实验结果表明，所提出的S-MoE模型在应对混合带宽输入的同时，联合执行自动语音识别（ASR）和语音翻译（ST）任务时，其应用在编码器和解码器上时，词错误率（WER）相对提高了6.35%。"}}
{"id": "2508.10232", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.10232", "abs": "https://arxiv.org/abs/2508.10232", "authors": ["Paul H. Acosta", "Pingjun Chen", "Simon P. Castillo", "Maria Esther Salvatierra", "Yinyin Yuan", "Xiaoxi Pan"], "title": "CellSymphony: Deciphering the molecular and phenotypic orchestration of cells with single-cell pathomics", "comment": null, "summary": "Xenium, a new spatial transcriptomics platform, enables\nsubcellular-resolution profiling of complex tumor tissues. Despite the rich\nmorphological information in histology images, extracting robust cell-level\nfeatures and integrating them with spatial transcriptomics data remains a\ncritical challenge. We introduce CellSymphony, a flexible multimodal framework\nthat leverages foundation model-derived embeddings from both Xenium\ntranscriptomic profiles and histology images at true single-cell resolution. By\nlearning joint representations that fuse spatial gene expression with\nmorphological context, CellSymphony achieves accurate cell type annotation and\nuncovers distinct microenvironmental niches across three cancer types. This\nwork highlights the potential of foundation models and multimodal fusion for\ndeciphering the physiological and phenotypic orchestration of cells within\ncomplex tissue ecosystems.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2508.10010", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.10010", "abs": "https://arxiv.org/abs/2508.10010", "authors": ["Ayana Hussain", "Patrick Zhao", "Nicholas Vincent"], "title": "An Audit and Analysis of LLM-Assisted Health Misinformation Jailbreaks Against LLMs", "comment": null, "summary": "Large Language Models (LLMs) are a double-edged sword capable of generating\nharmful misinformation -- inadvertently, or when prompted by \"jailbreak\"\nattacks that attempt to produce malicious outputs. LLMs could, with additional\nresearch, be used to detect and prevent the spread of misinformation. In this\npaper, we investigate the efficacy and characteristics of LLM-produced\njailbreak attacks that cause other models to produce harmful medical\nmisinformation. We also study how misinformation generated by jailbroken LLMs\ncompares to typical misinformation found on social media, and how effectively\nit can be detected using standard machine learning approaches. Specifically, we\nclosely examine 109 distinct attacks against three target LLMs and compare the\nattack prompts to in-the-wild health-related LLM queries. We also examine the\nresulting jailbreak responses, comparing the generated misinformation to\nhealth-related misinformation on Reddit. Our findings add more evidence that\nLLMs can be effectively used to detect misinformation from both other LLMs and\nfrom people, and support a body of work suggesting that with careful design,\nLLMs can contribute to a healthier overall information ecosystem.", "AI": {"tldr": "本研究探讨了大规模语言模型在生成有害的医学虚假信息方面的效能和特点。结果表明，LLMs不仅能够被用于生成虚假信息，也有可能被用来检测虚假信息，从而为创建一个更加健康的整体信息环境提供技术支持。", "motivation": "本研究的动机在于探索LLMs生成有害信息的能力，并进一步研究是否能够利用LLMs来检测和防止虚假信息的传播。", "method": "我们研究了大规模语言模型（LLMs）产生的越狱攻击，这些攻击促使其他模型生成有害的医学虚假信息。我们具体分析了109种不同的对三个目标LLMs的攻击，并将攻击提示与实际的社会媒体上的健康相关LLM查询进行了比较。此外，我们还比较了由越狱的LLMs生成的虚假信息与Reddit上的健康相关虚假信息。", "result": "研究结果表明，LLMs确实可以被有效用于检测来自其他LLMs生成的虚假信息以及来自人们的虚假信息，这支持了先前研究中关于通过精心设计，LLMs可以为维护一个更加健康的整体信息环境做出贡献的观点。", "conclusion": "研究得出结论，通过精心设计，LLMs能够有效检测并防止虚假信息的传播，有助于创建一个更加健康的整体信息环境。"}}
{"id": "2508.10256", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.10256", "abs": "https://arxiv.org/abs/2508.10256", "authors": ["Xinan Zhang", "Haolin Wang", "Yung-An Hsieh", "Zhongyu Yang", "Anthony Yezzi", "Yi-Chang Tsai"], "title": "Deep Learning for Crack Detection: A Review of Learning Paradigms, Generalizability, and Datasets", "comment": null, "summary": "Crack detection plays a crucial role in civil infrastructures, including\ninspection of pavements, buildings, etc., and deep learning has significantly\nadvanced this field in recent years. While numerous technical and review papers\nexist in this domain, emerging trends are reshaping the landscape. These shifts\ninclude transitions in learning paradigms (from fully supervised learning to\nsemi-supervised, weakly-supervised, unsupervised, few-shot, domain adaptation\nand fine-tuning foundation models), improvements in generalizability (from\nsingle-dataset performance to cross-dataset evaluation), and diversification in\ndataset reacquisition (from RGB images to specialized sensor-based data). In\nthis review, we systematically analyze these trends and highlight\nrepresentative works. Additionally, we introduce a new dataset collected with\n3D laser scans, 3DCrack, to support future research and conduct extensive\nbenchmarking experiments to establish baselines for commonly used deep learning\nmethodologies, including recent foundation models. Our findings provide\ninsights into the evolving methodologies and future directions in deep\nlearning-based crack detection. Project page:\nhttps://github.com/nantonzhang/Awesome-Crack-Detection", "AI": {"tldr": "文章综述了深度学习在裂纹检测中的最新进展和趋势，并提出了新的3DCrack数据集，以支持未来的研究和发展。", "motivation": "鉴于深度学习在裂纹检测领域的重要性及其不断的技术进步，文章旨在系统分析最新的发展动向和新兴趋势，为该领域提供新的洞见和数据支持。", "method": "通过系统分析深度学习在裂缝检测中的发展趋势，文章引入了一个使用3D激光扫描技术收集的新数据集3DCrack，以此支持未来的科研，并进行了广泛的基准测试实验，以建立常用的深度学习方法基准，包括最近的基础模型。", "result": "研究揭示了深度学习在裂缝检测中的方法演变及未来方向，并通过基准测试提供了关于当前技术性能的见解。", "conclusion": "研究强调了学习范式、泛化能力以及数据集多样性的转变，同时通过新数据集和基准测试推动了深度学习在裂缝检测中的应用和进步。"}}
{"id": "2508.10011", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.10011", "abs": "https://arxiv.org/abs/2508.10011", "authors": ["Yuta Nagamori", "Mikoto Kosai", "Yuji Kawai", "Haruka Marumo", "Misaki Shibuya", "Tatsuya Negishi", "Masaki Imanishi", "Yasumasa Ikeda", "Koichiro Tsuchiya", "Asuka Sawai", "Licht Miyamoto"], "title": "Evaluation of GPT-based large language generative AI models as study aids for the national licensure examination for registered dietitians in Japan", "comment": null, "summary": "Generative artificial intelligence (AI) based on large language models\n(LLMs), such as ChatGPT, has demonstrated remarkable progress across various\nprofessional fields, including medicine and education. However, their\nperformance in nutritional education, especially in Japanese national licensure\nexamination for registered dietitians, remains underexplored. This study aimed\nto evaluate the potential of current LLM-based generative AI models as study\naids for nutrition students. Questions from the Japanese national examination\nfor registered dietitians were used as prompts for ChatGPT and three Bing\nmodels (Precise, Creative, Balanced), based on GPT-3.5 and GPT-4. Each question\nwas entered into independent sessions, and model responses were analyzed for\naccuracy, consistency, and response time. Additional prompt engineering,\nincluding role assignment, was tested to assess potential performance\nimprovements. Bing-Precise (66.2%) and Bing-Creative (61.4%) surpassed the\npassing threshold (60%), while Bing-Balanced (43.3%) and ChatGPT (42.8%) did\nnot. Bing-Precise and Bing-Creative generally outperformed others across\nsubject fields except Nutrition Education, where all models underperformed.\nNone of the models consistently provided the same correct responses across\nrepeated attempts, highlighting limitations in answer stability. ChatGPT showed\ngreater consistency in response patterns but lower accuracy. Prompt engineering\nhad minimal effect, except for modest improvement when correct answers and\nexplanations were explicitly provided. While some generative AI models\nmarginally exceeded the passing threshold, overall accuracy and answer\nconsistency remained suboptimal. Moreover, all the models demonstrated notable\nlimitations in answer consistency and robustness. Further advancements are\nneeded to ensure reliable and stable AI-based study aids for dietitian\nlicensure preparation.", "AI": {"tldr": "研究评估了ChatGPT和三个Bing模型（基于GPT-3.5和GPT-4）在回答日本注册营养师国家考试问题上的表现，发现Bing的精确型和创意型模型表现优于ChatGPT和Bing的平衡型模型，但所有模型在准确性和稳定性上都有待提高。", "motivation": "当前，基于大型语言模型的生成式人工智能（如ChatGPT）在医学和教育等各个专业领域展现出了显著的进展。然而，在营养教育领域，特别是在日本的注册营养师国家考试中，这些模型的表现仍需进一步探索。本研究旨在评估目前基于大型语言模型的生成式AI模型作为营养学学生学习辅助工具的潜力。", "method": "使用了来自日本注册营养师国家考试的问题作为ChatGPT和三个基于GPT-3.5和GPT-4的Bing模型（精确型、创意型、平衡型）的提示。每个问题被独立输入到各自的会话中，对模型的回答进行准确性、一致性和响应时间的分析。此外，还测试了角色分配等提示工程以评估潜在的性能改进。", "result": "Bing-精确型（66.2%）和Bing-创意型（61.4%）超过通过门槛（60%），而Bing-平衡型（43.3%）和ChatGPT（42.8%）未能达到。Bing-精确型和Bing-创意型在各学科领域（除了营养教育）通常优于其他模型。所有模型在重复尝试中未能提供一致的正确答案，展示了答案稳定性的局限性。提示工程关于正确答案和解释的建议对性能改进有限。", "conclusion": "尽管某些生成性AI模型勉强超过了通过门槛，但总体准确性和答案一致性仍然不足；所有模型在答案的一致性和健壮性方面显示出显著局限。为了确保可靠且稳定的AI辅助学习工具用于营养师资格准备，需要进一步改进。"}}
{"id": "2508.10264", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10264", "abs": "https://arxiv.org/abs/2508.10264", "authors": ["Haonan Ge", "Yiwei Wang", "Ming-Hsuan Yang", "Yujun Cai"], "title": "MRFD: Multi-Region Fusion Decoding with Self-Consistency for Mitigating Hallucinations in LVLMs", "comment": null, "summary": "Large Vision-Language Models (LVLMs) have shown strong performance across\nmultimodal tasks. However, they often produce hallucinations -- text that is\ninconsistent with visual input, due to the limited ability to verify\ninformation in different regions of the image. To address this, we propose\nMulti-Region Fusion Decoding (MRFD), a training-free decoding method that\nimproves factual grounding by modeling inter-region consistency. MRFD\nidentifies salient regions using cross-attention, generates initial responses\nfor each, and computes reliability weights based on Jensen-Shannon Divergence\n(JSD) among the responses. These weights guide a consistency-aware fusion of\nper-region predictions, using region-aware prompts inspired by Chain-of-Thought\nreasoning. Experiments across multiple LVLMs and benchmarks show that MRFD\nsignificantly reduces hallucinations and improves response factuality without\nrequiring model updates.", "AI": {"tldr": "通过提出多区域融合解码（MRFD），文章解决视觉语言模型中的幻觉问题，提高响应的真实性和一致性。", "motivation": "LVLMs在跨模态任务中表现出强大的性能，但由于图像不同区域的信息验证能力有限，它们经常产生与视觉输入不一致的文本。", "method": "提出了多区域融合解码（MRFD），一种无训练的解码方法，通过建模区域内的一致性来改善事实基础。", "result": "实验表明，在多种视觉语言模型和基准测试中，MRFD显著减少了幻觉现象并提高了响应的事实性。", "conclusion": "MRFD方法在不更新模型的情况下减少了幻觉现象，提高了响应的事实性。"}}
{"id": "2508.10012", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.10012", "abs": "https://arxiv.org/abs/2508.10012", "authors": ["Dehao Tao", "Guangjie Liu", "Weizheng", "Yongfeng Huang", "Minghu jiang"], "title": "Guided Navigation in Knowledge-Dense Environments: Structured Semantic Exploration with Guidance Graphs", "comment": null, "summary": "While Large Language Models (LLMs) exhibit strong linguistic capabilities,\ntheir reliance on static knowledge and opaque reasoning processes limits their\nperformance in knowledge intensive tasks. Knowledge graphs (KGs) offer a\npromising solution, but current exploration methods face a fundamental trade\noff: question guided approaches incur redundant exploration due to granularity\nmismatches, while clue guided methods fail to effectively leverage contextual\ninformation for complex scenarios. To address these limitations, we propose\nGuidance Graph guided Knowledge Exploration (GG Explore), a novel framework\nthat introduces an intermediate Guidance Graph to bridge unstructured queries\nand structured knowledge retrieval. The Guidance Graph defines the retrieval\nspace by abstracting the target knowledge' s structure while preserving broader\nsemantic context, enabling precise and efficient exploration. Building upon the\nGuidance Graph, we develop: (1) Structural Alignment that filters incompatible\ncandidates without LLM overhead, and (2) Context Aware Pruning that enforces\nsemantic consistency with graph constraints. Extensive experiments show our\nmethod achieves superior efficiency and outperforms SOTA, especially on complex\ntasks, while maintaining strong performance with smaller LLMs, demonstrating\npractical value.", "AI": {"tldr": "为了解决知识检索中由大型语言模型和现有知识图探索方法带来的问题，我们提出了一种新的框架——Guidance Graph guided Knowledge Exploration (GG Explore)，该框架利用中间指导图优化知识检索的过程，实现了更高的效率和良好的性能。", "motivation": "大型语言模型依赖静态知识和不透明的推理过程，这限制了它们在知识密集型任务中的表现。问答引导方法在冗余探索方面存在问题，而线索引导方法在复杂场景下无法有效利用上下文信息。我们的方法旨在解决这些问题。", "method": "我们提出了一种名为Guidance Graph guided Knowledge Exploration (GG Explore)的新框架，通过引入中间指导图来连接非结构化查询与结构化知识检索。该指导图定义了检索空间，同时保留了更广的语义上下文。我们在此基础上开发了结构对齐（用于筛选不兼容的候选者）和语境感知剪枝（用于强制语义一致性）两种方法。", "result": "实验显示我们的方法在效率上显著提升，优于现有最优技术（SOTA），尤其是在复杂任务上。同时，我们的方法使用较小的大型语言模型也能保持良好的性能，展示了实际应用价值。", "conclusion": "我们的方法（GG Explore）在知识检索上实现了更高的效率和优秀的性能，尤其是在复杂任务和使用较小规模的大型语言模型时展现了显著优势。这表明我们的方法有重要的实用价值。"}}
{"id": "2508.10268", "categories": ["cs.CV", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2508.10268", "abs": "https://arxiv.org/abs/2508.10268", "authors": ["Yujie Zhao", "Jiabei Zeng", "Shiguang Shan"], "title": "Pose-Robust Calibration Strategy for Point-of-Gaze Estimation on Mobile Phones", "comment": "Accepted for British Machine Vision Conference (BMVC) 2025", "summary": "Although appearance-based point-of-gaze (PoG) estimation has improved, the\nestimators still struggle to generalize across individuals due to personal\ndifferences. Therefore, person-specific calibration is required for accurate\nPoG estimation. However, calibrated PoG estimators are often sensitive to head\npose variations. To address this, we investigate the key factors influencing\ncalibrated estimators and explore pose-robust calibration strategies.\nSpecifically, we first construct a benchmark, MobilePoG, which includes facial\nimages from 32 individuals focusing on designated points under either fixed or\ncontinuously changing head poses. Using this benchmark, we systematically\nanalyze how the diversity of calibration points and head poses influences\nestimation accuracy. Our experiments show that introducing a wider range of\nhead poses during calibration improves the estimator's ability to handle pose\nvariation. Building on this insight, we propose a dynamic calibration strategy\nin which users fixate on calibration points while moving their phones. This\nstrategy naturally introduces head pose variation during a user-friendly and\nefficient calibration process, ultimately producing a better calibrated PoG\nestimator that is less sensitive to head pose variations than those using\nconventional calibration strategies. Codes and datasets are available at our\nproject page.", "AI": {"tldr": "论文提出了一种基于用户移动手机时注视校准点的动态校准策略，以提高基于外观的视线估计器在不同头部姿态下的准确性和鲁棒性。", "motivation": "尽管基于外观的视线估计有所改进，但估算器仍难以跨个体进行泛化，因为估算器通常需要个体校准以获得准确的PoG估计。不过，校准后的估算器往往对头部姿态的变化敏感。因此，我们研究关键因素和探索姿态鲁棒的校准策略来解决这个问题。", "method": "我们构建了一个名为MobilePoG的数据集，该数据集包含了32个人在固定或连续变化的头部姿态下注视指定点的面部图像。我们使用此基准数据集系统地分析了校准点的多样性及头部姿态对估计精度的影响。实验表明，在校准过程中引入更广泛的头部姿态可以提升估计器对姿态变化的处理能力。基于这一发现，我们提出了一种动态校准策略，在这种策略下，用户在移动手机时注视校准点，这自然地在校准过程中引入头部姿态的变化，从而使校准过程用户友好且高效，产生了一个更好的、对头部姿态变化不太敏感的PoG估计器。", "result": "实验结果表明，在校准期间引入更广泛的头部姿态能提升估算器处理姿态变化的能力。", "conclusion": "提出的动态校准策略使得生成的PoG估算器相较于使用传统校准策略的估算器，更能抵抗头部姿态变化的影响。"}}
{"id": "2508.10013", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.10013", "abs": "https://arxiv.org/abs/2508.10013", "authors": ["Linqing Chen", "Hanmeng Zhong", "Wentao Wu", "Weilei Wang"], "title": "Semantic Bridge: Universal Multi-Hop Question Generation via AMR-Driven Graph Synthesis", "comment": null, "summary": "Large language model (LLM) training faces a critical bottleneck: the scarcity\nof high-quality, reasoning-intensive question-answer pairs, especially from\nsparse, domain-specific sources like PubMed papers or legal documents. Existing\nmethods rely on surface patterns, fundamentally failing to generate\ncontrollable, complex multi-hop reasoning questions that test genuine\nunderstanding-essential for advancing LLM training paradigms. We present\n\\textbf{Semantic Bridge}, the first universal framework for controllably\ngenerating sophisticated multi-hop reasoning questions from arbitrary sources.\nOur breakthrough innovation is \\textit{semantic graph weaving}-three\ncomplementary bridging mechanisms (entity bridging for role-varying shared\nentities, predicate chain bridging for temporal/causal/logical sequences, and\ncausal bridging for explicit reasoning chains)-that systematically construct\ncomplex pathways across documents, with fine-grained control over complexity\nand types via AMR-driven analysis. Our multi-modal AMR pipeline achieves up to\n9.5% better round-trip quality, enabling production-ready controllable QA\ngeneration. Extensive evaluation demonstrates performance across both\ngeneral-purpose datasets (Wikipedia) and specialized domains (biomedicine) It\nyields consistent 18.3%-25.4% gains over baselines across four languages\n(English, Chinese, French, German). Question pairs generated from 200 sources\noutperform 600 native human annotation examples with 67% fewer materials. Human\nevaluation shows 23.4% higher complexity, 18.7% better answerability, and 31.2%\nimproved pattern coverage. Semantic Bridge establishes a new paradigm for LLM\ntraining data synthesis, enabling controllable generation of targeted reasoning\nquestions from sparse sources. We will release our core code and semantic\nbridge model.", "AI": {"tldr": "\\textbf{Semantic Bridge}框架通过语义图编织技术生成复杂多跳推理问题，跨越多种语言和领域，优于人类注释，提升了LLM训练数据的质量和多样性。这一框架将公开发布代码和模型。", "motivation": "大型语言模型训练面临重要瓶颈，即高质量、推理密集型问答对的稀缺，特别是在PubMed论文或法律文件等特定领域。现有方法依赖表面模式，无法生成测试真实理解的可控复杂多跳推理问题，这对于推进LLM训练范式至关重要。", "method": "提出了一种名为\\textbf{Semantic Bridge}的通用框架，用于从任意来源可控地生成复杂多跳推理问题。这一创新的关键在于\\textit{语义图编织}\\u2014三种互补的桥梁机制（实体桥接、谓词链桥接和因果桥接），它们系统地构造文档间的复杂路径，并可通过AMR驱动的分析细化控制复杂度和类型。", "result": "多模态AMR流程达到了最高9.5%的往返质量提升，实现了生产级可控的问答生成。在泛用数据集（维基百科）和专业领域（生物医学）的广泛评估证明了其性能。语义桥接框架在四种语言（英语、中文、法语、德语）上比基线方法提高了18.3%-25.4%的性能。来自200个来源生成的问题对优于600个原生人类注释示例，材料减少了67%。人类评价显示复杂度提高了23.4%，答案可达性提高了18.7%，模式覆盖率提升了31.2%。", "conclusion": "语义桥接框架为LLM训练数据合成建立了新的范式，使从稀疏来源生成针对性推理问题成为可能。"}}
{"id": "2508.10280", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.10280", "abs": "https://arxiv.org/abs/2508.10280", "authors": ["Danyi Gao"], "title": "High Fidelity Text to Image Generation with Contrastive Alignment and Structural Guidance", "comment": null, "summary": "This paper addresses the performance bottlenecks of existing text-driven\nimage generation methods in terms of semantic alignment accuracy and structural\nconsistency. A high-fidelity image generation method is proposed by integrating\ntext-image contrastive constraints with structural guidance mechanisms. The\napproach introduces a contrastive learning module that builds strong\ncross-modal alignment constraints to improve semantic matching between text and\nimage. At the same time, structural priors such as semantic layout maps or edge\nsketches are used to guide the generator in spatial-level structural modeling.\nThis enhances the layout completeness and detail fidelity of the generated\nimages. Within the overall framework, the model jointly optimizes contrastive\nloss, structural consistency loss, and semantic preservation loss. A\nmulti-objective supervision mechanism is adopted to improve the semantic\nconsistency and controllability of the generated content. Systematic\nexperiments are conducted on the COCO-2014 dataset. Sensitivity analyses are\nperformed on embedding dimensions, text length, and structural guidance\nstrength. Quantitative metrics confirm the superior performance of the proposed\nmethod in terms of CLIP Score, FID, and SSIM. The results show that the method\neffectively bridges the gap between semantic alignment and structural fidelity\nwithout increasing computational complexity. It demonstrates a strong ability\nto generate semantically clear and structurally complete images, offering a\nviable technical path for joint text-image modeling and image generation.", "AI": {"tldr": "A novel image generation method is introduced to enhance semantic alignment and structural consistency by integrating text-image contrastive constraints with structural guidance, showcasing superior performance on COCO-2014 with improved metrics.", "motivation": "The primary motivation of the paper is to address the performance bottlenecks of existing text-driven image generation methods in semantic alignment and structural consistency, enabling the creation of more coherent and controllable images.", "method": "The paper proposes a high-fidelity image generation method that integrates text-image contrastive constraints with structural guidance, aiming to improve semantic alignment and structural consistency. It introduces a contrastive learning module and uses structural priors like semantic layout maps or edge sketches to guide the generator in spatial-level structural modeling.", "result": "Systematic experiments conducted on the COCO-2014 dataset confirm that the proposed method effectively improves semantic and structural consistency, as evidenced by higher CLIP Score, FID, and SSIM metrics.", "conclusion": "The proposed method demonstrates superior performance in text-driven image generation, achieving semantically clear and structurally complete images without increasing computational complexity, offering a viable path for joint text-image modeling."}}
{"id": "2508.10014", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.10014", "abs": "https://arxiv.org/abs/2508.10014", "authors": ["Lingfeng Zhou", "Jialing Zhang", "Jin Gao", "Mohan Jiang", "Dequan Wang"], "title": "PersonaEval: Are LLM Evaluators Human Enough to Judge Role-Play?", "comment": "Accepted by COLM 2025", "summary": "Current role-play studies often rely on unvalidated LLM-as-a-judge paradigms,\nwhich may fail to reflect how humans perceive role fidelity. A key prerequisite\nfor human-aligned evaluation is role identification, the ability to recognize\nwho is speaking based on dialogue context. We argue that any meaningful\njudgment of role-playing quality (how well a character is played) fundamentally\ndepends on first correctly attributing words and actions to the correct persona\n(who is speaking). We present PersonaEval, the first benchmark designed to test\nwhether LLM evaluators can reliably identify human roles. PersonaEval uses\nhuman-authored dialogues from novels, scripts, and video transcripts,\nchallenging models to determine the correct persona according to the\nconversation context. Our experiments, including a human study, show that even\nthe best-performing LLMs reach only around 69% accuracy, well below the level\nneeded for reliable evaluation. In contrast, human participants perform near\nceiling with 90.8% accuracy, highlighting that current LLM evaluators are still\nnot human enough to effectively judge role-play scenarios. To better understand\nthis gap, we examine training-time adaptation and test-time compute, suggesting\nthat reliable evaluation requires more than task-specific tuning, but depends\non strong, human-like reasoning abilities in LLM evaluators. We release our\nbenchmark at https://github.com/maple-zhou/PersonaEval.", "AI": {"tldr": "我们提出了PersonaEval评估基准，用于测试LLM能否准确识别对话中的人类角色，发现即使最好的LLM也只有约69%的准确率，而人类表现接近天花板，达到了90.8%的准确率。", "motivation": "当前的角色扮演研究经常依赖于未经验证的LLM作为评判标准，这可能无法反映人类对角色真实性感知的情况。本研究旨在通过角色识别来使其更接近人类的理解。", "method": "我们介绍了PersonaEval，这是一个用于测试LLM评估者是否能可靠地识别人类角色的基准。PersonaEval使用来自小说、剧本和视频记录的人类编写对话，挑战模型根据对话背景确定正确的角色。", "result": "实验包括人类研究，结果显示即使是最先进的LLM的准确率也只有69%，显著低于可信赖评估所需的水平。相比之下，人类参与者的表现几乎达到了天花板，准确率为90.8%。", "conclusion": "这一差距表明，当前的LLM评估者在判断角色扮演游戏场景方面还不够“人性化”。可靠评估不仅依赖于特定任务的调优，还需要强大且类似人类的推理能力。"}}
{"id": "2508.10281", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.10281", "abs": "https://arxiv.org/abs/2508.10281", "authors": ["Ryota Tanaka", "Tomohiro Suzuki", "Keisuke Fujii"], "title": "VIFSS: View-Invariant and Figure Skating-Specific Pose Representation Learning for Temporal Action Segmentation", "comment": null, "summary": "Understanding human actions from videos plays a critical role across various\ndomains, including sports analytics. In figure skating, accurately recognizing\nthe type and timing of jumps a skater performs is essential for objective\nperformance evaluation. However, this task typically requires expert-level\nknowledge due to the fine-grained and complex nature of jump procedures. While\nrecent approaches have attempted to automate this task using Temporal Action\nSegmentation (TAS), there are two major limitations to TAS for figure skating:\nthe annotated data is insufficient, and existing methods do not account for the\ninherent three-dimensional aspects and procedural structure of jump actions. In\nthis work, we propose a new TAS framework for figure skating jumps that\nexplicitly incorporates both the three-dimensional nature and the semantic\nprocedure of jump movements. First, we propose a novel View-Invariant, Figure\nSkating-Specific pose representation learning approach (VIFSS) that combines\ncontrastive learning as pre-training and action classification as fine-tuning.\nFor view-invariant contrastive pre-training, we construct FS-Jump3D, the first\npublicly available 3D pose dataset specialized for figure skating jumps.\nSecond, we introduce a fine-grained annotation scheme that marks the ``entry\n(preparation)'' and ``landing'' phases, enabling TAS models to learn the\nprocedural structure of jumps. Extensive experiments demonstrate the\neffectiveness of our framework. Our method achieves over 92% F1@50 on\nelement-level TAS, which requires recognizing both jump types and rotation\nlevels. Furthermore, we show that view-invariant contrastive pre-training is\nparticularly effective when fine-tuning data is limited, highlighting the\npracticality of our approach in real-world scenarios.", "AI": {"tldr": "This paper proposes a new TAS system for figure skating jumps that includes a novel pose representation approach and a detailed annotation scheme to improve jump recognition.", "motivation": "The motivation is to tackle the challenges of insufficient annotated data and the three-dimensional nature of jump actions, which are limitations of existing TAS methods in figure skating. The paper aims to develop an effective TAS system that can accurately recognize the type and timing of jumps in videos.", "method": "The paper introduces a new Temporal Action Segmentation (TAS) framework for figure skating jumps. It utilizes a novel View-Invariant, Figure Skating-Specific (VIFSS) pose representation learning approach, which integrates contrastive learning with action classification. The framework also employs a fine-grained annotation scheme that includes marking the entry and landing phases of jumps.", "result": "Experiments showed that the new framework achieved over 92% F1 score for recognizing both jump types and rotation levels. The view-invariant contrastive pre-training also proved particularly effective when the amount of fine-tuning data was limited.", "conclusion": "The research concludes with the effectiveness of the proposed TAS framework in accurately recognizing jump types and rotation levels with an F1 score of over 92% at 50. Additionally, the method demonstrates a practical advantage in situations where fine-tuning data is limited."}}
{"id": "2508.10015", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.10015", "abs": "https://arxiv.org/abs/2508.10015", "authors": ["Enzhi Wang", "Qicheng Li", "Shiwan Zhao", "Aobo Kong", "Jiaming Zhou", "Xi Yang", "Yequan Wang", "Yonghua Lin", "Yong Qin"], "title": "RealTalk-CN: A Realistic Chinese Speech-Text Dialogue Benchmark With Cross-Modal Interaction Analysis", "comment": "9 pages", "summary": "In recent years, large language models (LLMs) have achieved remarkable\nadvancements in multimodal processing, including end-to-end speech-based\nlanguage models that enable natural interactions and perform specific tasks in\ntask-oriented dialogue (TOD) systems. However, existing TOD datasets are\npredominantly text-based, lacking real speech signals that are essential for\nevaluating the robustness of speech-based LLMs. Moreover, existing speech TOD\ndatasets are primarily English and lack critical aspects such as speech\ndisfluencies and speaker variations. To address these gaps, we introduce\nRealTalk-CN, the first Chinese multi-turn, multi-domain speech-text dual-modal\nTOD dataset, comprising 5.4k dialogues (60K utterances, 150 hours) with paired\nspeech-text annotations. RealTalk-CN captures diverse dialogue scenarios with\nannotated spontaneous speech disfluencies, ensuring comprehensive coverage of\nreal-world complexities in speech dialogue. In addition, we propose a novel\ncross-modal chat task that authentically simulates real-world user\ninteractions, allowing dynamic switching between speech and text modalities.\nOur evaluation covers robustness to speech disfluencies, sensitivity to speaker\ncharacteristics, and cross-domain performance. Extensive experiments validate\nthe effectiveness of RealTalk-CN, establishing a strong foundation for Chinese\nspeech-based LLMs research.", "AI": {"tldr": "RealTalk-CN是首个中文多轮多领域语音-文本双模态TOD数据集，用于解决现有TOD数据集中缺乏真实语音信号的问题，包含大量对话与文本注释，并引入新型跨模态聊天任务。", "motivation": "由于现有的TOD数据集主要以文本为基础，缺乏真实语音信号，因而无法有效评估基于语音的大语言模型的鲁棒性，同时，现有的语音TOD数据集主要以英语为主，而缺乏关键的语义不连贯性和说话人变异特征。", "method": "该研究通过引入RealTalk-CN数据集来填补现有任务导向对话(TOD)数据集中缺少真实语音信号的空白，RealTalk-CN是首个中文多轮多领域语音-文本双模态TOD数据集，包含5400个对话（60K条语句，历时150小时）配有语音-文本注释，并提出了一种新的跨模态聊天任务来真实模拟现实世界的用户互动，允许在语音和文本模态之间动态切换。", "result": "实验结果验证了RealTalk-CN的有效性，为基于语音的中文大语言模型研究奠定了坚实基础。", "conclusion": "此研究通过RealTalk-CN数据集解决了现有TOD数据集的缺陷，并通过跨模态聊天任务捕捉了真实环境中说话人特征和多领域表现的复杂性，构建了一个强大的框架，提升汉语语音处理技术研究水平。"}}
{"id": "2508.10287", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.10287", "abs": "https://arxiv.org/abs/2508.10287", "authors": ["Simindokht Jahangard", "Mehrzad Mohammadi", "Yi Shen", "Zhixi Cai", "Hamid Rezatofighi"], "title": "JRDB-Reasoning: A Difficulty-Graded Benchmark for Visual Reasoning in Robotics", "comment": null, "summary": "Recent advances in Vision-Language Models (VLMs) and large language models\n(LLMs) have greatly enhanced visual reasoning, a key capability for embodied AI\nagents like robots. However, existing visual reasoning benchmarks often suffer\nfrom several limitations: they lack a clear definition of reasoning complexity,\noffer have no control to generate questions over varying difficulty and task\ncustomization, and fail to provide structured, step-by-step reasoning\nannotations (workflows). To bridge these gaps, we formalize reasoning\ncomplexity, introduce an adaptive query engine that generates customizable\nquestions of varying complexity with detailed intermediate annotations, and\nextend the JRDB dataset with human-object interaction and geometric\nrelationship annotations to create JRDB-Reasoning, a benchmark tailored for\nvisual reasoning in human-crowded environments. Our engine and benchmark enable\nfine-grained evaluation of visual reasoning frameworks and dynamic assessment\nof visual-language models across reasoning levels.", "AI": {"tldr": "本文为解决现有视觉推理基准测试的不足，提出了能够生成不同复杂度问题并带有中间注释的自适应查询引擎，并新增人与物体互动及几何关系的注释，形成JRDB-Reasoning基准测试，用以评估视觉推理和视觉-语言模型。", "motivation": "视觉语言模型（VLMs）和大型语言模型（LLMs）的最新进展极大地增强了机器人等具身AI代理的视觉推理能力，但现有的视觉推理基准测试存在定义推理复杂性的模糊、缺乏生成不同难度问题的控制、无法提供结构化的逐步推理注释等问题。", "method": "本文提出了一种自适应查询引擎，能够生成不同复杂度且具有详细中间注释的自定义问题，并扩展了JRDB数据集，加入了人与物体互动以及几何关系的注释，形成了一套专为人群密集环境中视觉推理设计的基准测试JRDB-Reasoning。", "result": "本文的方法填补了现有视觉推理基准测试的空白，能够细粒度地评估视觉推理框架，并能跨不同推理水平动态评估视觉-语言模型。", "conclusion": "通过引入自适应查询引擎和扩展的JRDB-Reasoning基准测试，本文为评估视觉推理框架和视觉-语言模型提供了一种精细化方法，特别是在人群密集环境中的视觉推理表现。"}}
{"id": "2508.10016", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.10016", "abs": "https://arxiv.org/abs/2508.10016", "authors": ["Tianyu Xie", "Yuhang Wu", "Yongdong Luo", "Jiayi Ji", "Xiawu Zheng"], "title": "Training-Free Multimodal Large Language Model Orchestration", "comment": null, "summary": "Different Multimodal Large Language Models (MLLMs) cannot be integrated into\na unified multimodal input-output system directly. In previous work, training\nhas been considered as an inevitable component due to challenges in modal\nalignment, Text-to-Speech efficiency and other integration issues. In this\npaper, we introduce Multimodal Large Language Model Orchestration, an effective\napproach for creating interactive multimodal AI systems without additional\ntraining. MLLM Orchestration leverages the inherent reasoning capabilities of\nlarge language models to coordinate specialized models through explicit\nworkflows, enabling natural multimodal interactions while maintaining\nmodularity, improving interpretability, and significantly enhancing\ncomputational efficiency. Our orchestration framework is built upon three key\ninnovations: (1) a central controller LLM that analyzes user inputs and\ndynamically routes tasks to appropriate specialized models through carefully\ndesigned agents; (2) a parallel Text-to-Speech architecture that enables true\nfull-duplex interaction with seamless interruption handling and natural\nconversational flow; and (3) a cross-modal memory integration system that\nmaintains coherent context across modalities through intelligent information\nsynthesis and retrieval, selectively avoiding unnecessary modality calls in\ncertain scenarios to improve response speed. Extensive evaluations demonstrate\nthat MLLM Orchestration achieves comprehensive multimodal capabilities without\nadditional training, performance improvements of up to 7.8% over traditional\njointly-trained approaches on standard benchmarks, reduced latency by 10.3%,\nand significantly enhanced interpretability through explicit orchestration\nprocesses.", "AI": {"tldr": "提出了Multimodal Large Language Model Orchestration方法，可以在无需额外训练的情况下创建交互式多模态AI系统，提高了效率和解释性，并在标准基准上表现出色，减少了延迟。", "motivation": "解决不同多模态大语言模型无法直接整合到统一的输入输出系统的问题，避免因模态对齐、文本转语音效率和其他集成问题所需的额外训练。", "method": "引入多模态大语言模型管弦编排，利用中央控制器LLM分析用户输入并将任务动态路由到适当的专业模型，采用平行文本转语音架构和跨模态记忆集成系统。", "result": "方法在标准基准上性能提高了7.8%，延迟减少了10.3%，并通过明确的编排过程显著提高了可解释性。", "conclusion": "证明了MLLM管弦编排在无需额外训练的情况下达到全面多模态功能的可能性，证明了其高效和解释性的优势。"}}
