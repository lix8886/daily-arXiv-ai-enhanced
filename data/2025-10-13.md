<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 7]
- [cs.CV](#cs.CV) [Total: 10]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Enhancing Biomedical Named Entity Recognition using GLiNER-BioMed with Targeted Dictionary-Based Post-processing for BioASQ 2025 task 6](https://arxiv.org/abs/2510.08588)
*Ritesh Mehta*

Main category: cs.CL

> 研究提出了针对GLiNER-BioMed模型的后处理策略，在开发集上表现改善，但在测试集上未能保持效果，强调了不过度拟合开发数据的重要性。

<details>
  <summary>Details</summary>

**Motivation:** 由于生物医学命名实体识别（BioNER）在从科学文献中提取信息方面至关重要，但面临着区分基因和化学物质等相似实体类型等挑战，因此提出此项研究。

**Method:** 研究评估了GLiNER-BioMed模型在BioASQ数据集上的性能，并提出了一种基于目标字典的后处理策略来解决常见的错误分类问题。

**Result:** 后处理方法在开发数据集上显著提高了微F1分数，从基线的0.79增加到0.83，但在盲测数据集中，后处理模型的微F1分数下降到0.77，对比基线无明显改善。

**Conclusion:** 这一工作突显了基于字典的精调对于预训练BioNER模型的巨大潜力，但同时也强调了避免过度拟合到开发数据并保证模型稳健泛化至现实世界应用的重要性。

**Abstract:** Biomedical Named Entity Recognition (BioNER), task6 in BioASQ (A challenge in
large-scale biomedical semantic indexing and question answering), is crucial
for extracting information from scientific literature but faces hurdles such as
distinguishing between similar entity types like genes and chemicals. This
study evaluates the GLiNER-BioMed model on a BioASQ dataset and introduces a
targeted dictionary-based post-processing strategy to address common
misclassifications. While this post-processing approach demonstrated notable
improvement on our development set, increasing the micro F1-score from a
baseline of 0.79 to 0.83, this enhancement did not generalize to the blind test
set, where the post-processed model achieved a micro F1-score of 0.77 compared
to the baselines 0.79. We also discuss insights gained from exploring
alternative methodologies, including Conditional Random Fields. This work
highlights the potential of dictionary-based refinement for pre-trained BioNER
models but underscores the critical challenge of overfitting to development
data and the necessity of ensuring robust generalization for real-world
applicability.

</details>


### [2] [Less Diverse, Less Safe: The Indirect But Pervasive Risk of Test-Time Scaling in Large Language Models](https://arxiv.org/abs/2510.08592)
*Shahriar Kabir Nahin,Hadi Askari,Muhao Chen,Anshuman Chhabra*

Main category: cs.CL

> Error

<details>
  <summary>Details</summary>

**Motivation:** Error

**Method:** Error

**Result:** Error

**Conclusion:** Error

**Abstract:** Test-Time Scaling (TTS) improves LLM reasoning by exploring multiple
candidate responses and then operating over this set to find the best output. A
tacit premise behind TTS is that sufficiently diverse candidate pools enhance
reliability. In this work, we show that this assumption in TTS introduces a
previously unrecognized failure mode. When candidate diversity is curtailed,
even by a modest amount, TTS becomes much more likely to produce unsafe
outputs. We present a reference-guided diversity reduction protocol (RefDiv)
that serves as a diagnostic attack to stress test TTS pipelines. Through
extensive experiments across four open-source models (Qwen3, Mistral, Llama3.1,
Gemma3) and two widely used TTS strategies (Monte Carlo Tree Search and
Best-of-N), constraining diversity consistently signifies the rate at which TTS
produces unsafe results. The effect is often stronger than that produced by
prompts directly with high adversarial intent scores. This observed phenomenon
also transfers across TTS strategies and to closed-source models (e.g. OpenAI
o3 and Gemini-2.5-Pro), thus indicating that this is a general and extant
property of TTS rather than a model-specific artifact. Additionally, we find
that numerous widely used safety guardrail classifiers (e.g. Llama-Guard and
OpenAI Moderation API), are unable to flag the adversarial input prompts
generated by RefDiv, demonstrating that existing defenses offer limited
protection against this diversity-driven failure mode. Through this work, we
hope to motivate future research on designing robust TTS strategies that are
both effective and secure against diversity-targeted stress tests as
illustrated by RefDiv.

</details>


### [3] [Hierarchical Self-Supervised Representation Learning for Depression Detection from Speech](https://arxiv.org/abs/2510.08593)
*Yuxin Li,Eng Siong Chng,Cuntai Guan*

Main category: cs.CL

> HAREN-CTC, a novel speech-based depression detection system using multi-layer SSL, cross-attention, and CTC loss, achieves state-of-the-art results on standard datasets.

<details>
  <summary>Details</summary>

**Motivation:** The motivation stems from the limitations of existing speech-based depression detection methods, which are restricted by difficulty in extracting meaningful features and capturing sparse depressive cues over time.

**Method:** The paper introduces HAREN-CTC, an architecture that integrates multi-layer SSL features via cross-attention in a multitask framework, along with CTC loss for sparse temporal supervision. It has two main components: a Hierarchical Adaptive Clustering module and a Cross-Modal Fusion module.

**Result:** In evaluations, HAREN-CTC achieved the best macro F1-scores of 0.81 on DAIC-WOZ and 0.82 on MODMA, outperforming past approaches in both data splits and five-fold cross-validation settings.

**Conclusion:** The research concludes on the effectiveness of their proposed HAREN-CTC model in overcoming the limitations of current methods, demonstrating superior performance in detecting depression through speech analysis.

**Abstract:** Speech-based depression detection (SDD) is a promising, non-invasive
alternative to traditional clinical assessments. However, it remains limited by
the difficulty of extracting meaningful features and capturing sparse,
heterogeneous depressive cues over time. Pretrained self-supervised learning
(SSL) models such as WavLM provide rich, multi-layer speech representations,
yet most existing SDD methods rely only on the final layer or search for a
single best-performing one. These approaches often overfit to specific datasets
and fail to leverage the full hierarchical structure needed to detect subtle
and persistent depression signals.
  To address this challenge, we propose HAREN-CTC, a novel architecture that
integrates multi-layer SSL features using cross-attention within a multitask
learning framework, combined with Connectionist Temporal Classification loss to
handle sparse temporal supervision. HAREN-CTC comprises two key modules: a
Hierarchical Adaptive Clustering module that reorganizes SSL features into
complementary embeddings, and a Cross-Modal Fusion module that models
inter-layer dependencies through cross-attention. The CTC objective enables
alignment-aware training, allowing the model to track irregular temporal
patterns of depressive speech cues.
  We evaluate HAREN-CTC under both an upper-bound setting with standard data
splits and a generalization setting using five-fold cross-validation. The model
achieves state-of-the-art macro F1-scores of 0.81 on DAIC-WOZ and 0.82 on
MODMA, outperforming prior methods across both evaluation scenarios.

</details>


### [4] [Systematic Diagnosis of Brittle Reasoning in Large Language Models](https://arxiv.org/abs/2510.08595)
*V. S. Raghu Parupudi*

Main category: cs.CL

> 研究提出一种框架评估AI模型在数学推理上的实际能力，发现模型在特定的推理模式上表现出非人类般的脆弱性，并提供了开发更可靠AI应用的指引。

<details>
  <summary>Details</summary>

**Motivation:** 研究人工智能中机器学习模型理解和处理数学问题的能力，特别是想要了解模型在不同数学推理类型上的表现和弱点。

**Method:** 本研究提出了一种新的数学推理衡量框架，该框架超越了标准基准，能够诊断具体的失败点。首先，通过gpt-3.5-turbo对GSM8K数据集进行结构化的、逐步推理的生成。然后，使用更强大的分析模型gpt-4o-mini对错误进行分类，并对每个推理句子进行无监督聚类分析，以识别出现的“推理模式”。

**Result:** 分析揭示了一种非人类般的认知脆弱性：模型在顺序计算等程序模式上表现出近乎完美的准确率，但在需要组合推理和限制的问题上则表现得很差。

**Conclusion:** 本工作提供了一种更细致的方法来评估数学理解能力，为开发新的能力和未来的可靠应用提供了精准的路线图。

**Abstract:** A central question in artificial intelligence is the extent to which machine
learning models comprehend mathematics. To address this, we propose a novel
framework for measuring mathematical reasoning that moves beyond standard
benchmarks to diagnose specific failure points. Our method first generates
structured, step-by-step reasoning from gpt-3.5-turbo on the GSM8K dataset. We
then use a more capable analyst model, gpt-4o-mini, to categorize errors and,
crucially, perform an unsupervised clustering of every reasoning sentence to
identify emergent "reasoning modes." This analysis reveals a cognitive profile
with a stark, nonhuman-like brittleness: while the model achieves near-perfect
accuracy on procedural modes like sequential calculation, its performance on
modes requiring combinatorial reasoning with restrictions plummets. By
identifying and quantifying the reliability of these distinct reasoning skills,
our work provides a more granular method to evaluate mathematical comprehension
and offers a precise roadmap for developing new capabilities and more reliable
future applications.

</details>


### [5] [Confidence, Not Perplexity: A Better Metric for the Creative Era of LLMs](https://arxiv.org/abs/2510.08596)
*V. S. Raghu Parupudi*

Main category: cs.CL

> 研究提出了一种新的评价指标——置信度分数(CS)，它能更客观地评估生成文本的创造性，特别是在鉴别新奇响应方面优于现有指标。

<details>
  <summary>Details</summary>

**Motivation:** 现有的无参考评价指标，如自困惑度(self-perplexity)，存在对创新文本生成的偏差。研究动机在于寻找一种更为公正的评价方式。

**Method:** 提出了一种基于模型输出概率分布的置信度分数(Confidence Score, CS)，作为评估生成文本的替代指标。

**Result:** 实验结果显示，相较于传统的基于流畅度的评价指标，CS在99个创意提示中能更频繁地偏爱新颖的响应（19% vs 0%），且该差异具有统计学意义（95%置信区间：[11.1%，27.3%]）。CS还可以有效区分不同难度的任务。

**Conclusion:** CS解决了传统评估指标的创新偏见问题，同时保留了评估的主要优点，为现代语言模型提供了更平衡的评估。

**Abstract:** Reference-free metrics like self-perplexity are strongly biased against
creative text generation. We propose the Confidence Score (CS), derived from a
model's output probability distribution, as a less biased alternative.
Experiments on gpt-4o-mini show that while fluency-based metrics prefer novel
responses in 0\% of cases on 99 creative prompts, our CS does so 19% of the
time, a statistically significant difference (95% CI for difference: [11.1%,
27.3%]). We also show that CS effectively distinguishes between easy, medium,
and hard tasks, confirmed by non-overlapping confidence intervals. The
Confidence Score thus mitigates the creativity bias of traditional metrics
while retaining their core evaluative strengths, offering a more balanced
assessment for modern LLMs.

</details>


### [6] [Recover-LoRA: Data-Free Accuracy Recovery of Degraded Language Models via Low-Rank Adaptation](https://arxiv.org/abs/2510.08600)
*Devleena Das,Rajeev Patwari,Ashish Sirasao*

Main category: cs.CL

> 提出了Recover-LoRA方法，通过合成数据和logit蒸馏恢复小语言模型的性能。

<details>
  <summary>Details</summary>

**Motivation:** 论文旨在解决由模型序列化不当等来源引起的模型权重功能退化的问题，而不仅仅是集中在健壮的量化技术上来恢复部署性能。

**Method:** Recover-LoRA是使用合成数据和logit蒸馏来在选择的层上学习LoRA适配器，从而将退化的模型与其全精度模型对齐的轻量级且数据集无关的方法。

**Result:** 实验证明Recover-LoRA在多项评估数据集上对不同的小型语言模型恢复了模型准确度，提高了5-17%。

**Conclusion:** 研究表明，Recover-LoRA在多头注意力(MHA)和分组查询注意力(GQA)的小型语言模型(SLMs)中恢复了5-17%的模型准确性。

**Abstract:** Inference optimizations such as quantization, pruning, format and datatype
conversion, model export, and serialization can lead to functional degradations
in language model task performance. While most efforts on performance recovery
for deployment focus on robust quantization techniques, we focus on recovering
model accuracies from any sources that degrade model weights, such as improper
model serialization. In this work, we propose Recover-LoRA, a lightweight and
dataset agnostic method to recover accuracy in degraded models. Recover-LoRA
uses synthetic data and logit distillation to learn LoRA adapters on selective
layers that facilitate aligning the degraded model to its full precision model.
We investigate the utility of Recover-LoRA across a diverse set of small
language models (SLMs), including models with varying attention architectures,
multi-head attention (MHA) and group-query attention (GQA), as well as several
evaluation datasets. Our results show that Recover-LoRA recovers model
accuracies by 5-17% on MHA and GQA SLMs.

</details>


### [7] [Mnemosyne: An Unsupervised, Human-Inspired Long-Term Memory Architecture for Edge-Based LLMs](https://arxiv.org/abs/2510.08601)
*Aneesh Jonelagadda,Christina Hahn,Haoze Zheng,Salvatore Penachio*

Main category: cs.CL

> The paper introduces Mnemosyne, an unsupervised, human-inspired long-term memory architecture for edge-based language models, showing superior performance in longitudinal healthcare dialogues compared to existing methods.

<details>
  <summary>Details</summary>

**Motivation:** Current LLM memory systems, which rely on brute-force context expansion or static retrieval pipelines, fail on edge-constrained devices. There's a need for a more efficient and effective long-term memory solution for edge-based language models.

**Method:** Our approach uses graph-structured storage, modular substance and redundancy filters, memory committing and pruning mechanisms, and probabilistic recall with temporal decay and refresh processes modeled after human memory.

**Result:** In experiments with longitudinal healthcare dialogues, Mnemosyne demonstrates the highest win rate of 65.8% in blind human evaluations of realism and long-term memory capability, outperforming the baseline RAG win rate of 31.1%. It also achieves the highest LoCoMo benchmark scores in temporal reasoning and single-hop retrieval.

**Conclusion:** The research demonstrates that the introduction of Mnemosyne significantly enhances factual recall, temporal reasoning, and naturalness of user-facing responses in longitudinal healthcare assistants, which may be feasible with an unsupervised memory architecture that's compatible with edge devices.

**Abstract:** Long-term memory is essential for natural, realistic dialogue. However,
current large language model (LLM) memory systems rely on either brute-force
context expansion or static retrieval pipelines that fail on edge-constrained
devices. We introduce Mnemosyne, an unsupervised, human-inspired long-term
memory architecture designed for edge-based LLMs. Our approach uses
graph-structured storage, modular substance and redundancy filters, memory
committing and pruning mechanisms, and probabilistic recall with temporal decay
and refresh processes modeled after human memory. Mnemosyne also introduces a
concentrated "core summary" efficiently derived from a fixed-length subset of
the memory graph to capture the user's personality and other domain-specific
long-term details such as, using healthcare application as an example,
post-recovery ambitions and attitude towards care. Unlike existing
retrieval-augmented methods, Mnemosyne is designed for use in longitudinal
healthcare assistants, where repetitive and semantically similar but temporally
distinct conversations are limited by naive retrieval. In experiments with
longitudinal healthcare dialogues, Mnemosyne demonstrates the highest win rate
of 65.8% in blind human evaluations of realism and long-term memory capability
compared to a baseline RAG win rate of 31.1%. Mnemosyne also achieves current
highest LoCoMo benchmark scores in temporal reasoning and single-hop retrieval
compared to other same-backboned techniques. Further, the average overall score
of 54.6% was second highest across all methods, beating commonly used Mem0 and
OpenAI baselines among others. This demonstrates that improved factual recall,
enhanced temporal reasoning, and much more natural user-facing responses can be
feasible with an edge-compatible and easily transferable unsupervised memory
architecture.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [8] [Beyond CNNs: Efficient Fine-Tuning of Multi-Modal LLMs for Object Detection on Low-Data Regimes](https://arxiv.org/abs/2510.08589)
*Nirmal Elamon,Rouzbeh Davoudi*

Main category: cs.CV

> 研究展示了通过少量数据微调多模态LLM可显著提升图像中人工文本叠加检测的精度，并展示了这种方法的数据效率和适应性。

<details>
  <summary>Details</summary>

**Motivation:** 探索语言引导模型在需要少量监督的精确视觉理解任务中的应用，并展示多模态LLM的潜力。

**Method:** 比较了微调的传统CNN、零样本预训练多模态LLM和微调的多模态LLM在图像中人工文本叠加检测任务上的表现。

**Result:** 在少量数据（少于1000张图像）微调下，LLM的精度提升了多达36%，与需要大量数据的传统CNN基线相比不相上下或超越。

**Conclusion:** 多模态LLM方法在真实世界对象检测任务中展现出了高度的数据效率和适应性，为在资源匮乏的视觉环境中应用多模态变换器提供了实用指南。

**Abstract:** The field of object detection and understanding is rapidly evolving, driven
by advances in both traditional CNN-based models and emerging multi-modal large
language models (LLMs). While CNNs like ResNet and YOLO remain highly effective
for image-based tasks, recent transformer-based LLMs introduce new capabilities
such as dynamic context reasoning, language-guided prompts, and holistic scene
understanding. However, when used out-of-the-box, the full potential of LLMs
remains underexploited, often resulting in suboptimal performance on
specialized visual tasks. In this work, we conduct a comprehensive comparison
of fine-tuned traditional CNNs, zero-shot pre-trained multi-modal LLMs, and
fine-tuned multi-modal LLMs on the challenging task of artificial text overlay
detection in images. A key contribution of our study is demonstrating that LLMs
can be effectively fine-tuned on very limited data (fewer than 1,000 images) to
achieve up to 36% accuracy improvement, matching or surpassing CNN-based
baselines that typically require orders of magnitude more data. By exploring
how language-guided models can be adapted for precise visual understanding with
minimal supervision, our work contributes to the broader effort of bridging
vision and language, offering novel insights into efficient cross-modal
learning strategies. These findings highlight the adaptability and data
efficiency of LLM-based approaches for real-world object detection tasks and
provide actionable guidance for applying multi-modal transformers in
low-resource visual environments. To support continued progress in this area,
we have made the code used to fine-tune the models available in our GitHub,
enabling future improvements and reuse in related applications.

</details>


### [9] [Reproducible Evaluation of Data Augmentation and Loss Functions for Brain Tumor Segmentation](https://arxiv.org/abs/2510.08617)
*Saumya B*

Main category: cs.CV

> 本文通过使用焦点损失(focal loss)和基本数据扩增策略对U-Net在脑肿瘤MRI图像分割性能进行评估，对焦点损失参数进行调优，并评估了三种数据增广技术的影响。实验结果表明U-Net配合焦点损失达到了90%的精度，与最先进的结果相当。

<details>
  <summary>Details</summary>

**Motivation:** 由于类别不平衡和模型泛化能力有限等挑战，脑肿瘤分割的研究遇到了阻碍。本文试图通过可重复的方式评估U-Net模型在使用焦点损失和数据扩增策略时的性能，为未来的脑肿瘤分割研究提供一个透明的研究基准。

**Method:** 本文使用U-Net进行脑肿瘤分割，结合使用焦点损失和数据扩增方法如水平翻转、旋转和缩放。实验在公开的MRI数据集上进行。

**Result:** 实验结果表明，使用焦点损失的U-Net模型达到了90%的精度，这一结果与当前最先进的研究水平相当。

**Conclusion:** 本文通过透明且可重复的方式建立了U-Net在脑肿瘤分割研究中的基准，尤其在数据扩增策略和损失函数设计方面，为未来的研究提供了指导意义。

**Abstract:** Brain tumor segmentation is crucial for diagnosis and treatment planning, yet
challenges such as class imbalance and limited model generalization continue to
hinder progress. This work presents a reproducible evaluation of U-Net
segmentation performance on brain tumor MRI using focal loss and basic data
augmentation strategies. Experiments were conducted on a publicly available MRI
dataset, focusing on focal loss parameter tuning and assessing the impact of
three data augmentation techniques: horizontal flip, rotation, and scaling. The
U-Net with focal loss achieved a precision of 90%, comparable to
state-of-the-art results. By making all code and results publicly available,
this study establishes a transparent, reproducible baseline to guide future
research on augmentation strategies and loss function design in brain tumor
segmentation.

</details>


### [10] [Adjusting Initial Noise to Mitigate Memorization in Text-to-Image Diffusion Models](https://arxiv.org/abs/2510.08625)
*Hyeonggeun Han,Sehwan Kim,Hyungjun Joo,Sangwoo Hong,Jungwoo Lee*

Main category: cs.CV

> 研究发现通过调整初始噪声可以使文本转图像扩散模型早期脱离吸引盆地，从而减少记忆训练数据，并保持图像与文本的一致性。

<details>
  <summary>Details</summary>

**Motivation:** 文本到图像扩散模型经常复制训练数据，这引发对隐私和版权的担忧。最近的研究将其归因于吸引盆地，在这个区域内，无类别引导将去噪轨迹导向记忆输出。延迟应用无类别引导会导致图像与输入提示不匹配，因此需要找到一种促进快速脱离盆地的方法，以便稍后应用无类别引导。

**Method:** 本研究提出了两种调整初始噪声的方法，旨在帮助扩散模型更快地脱离吸引盆地，从而减少对训练数据的记忆效应。这两种方法可以集体或个别地调整初始噪声，以便找到鼓励早期脱离盆地的样本。

**Result:** 研究发现，不同的初始噪声样本会导致不同的脱离时间，并提出的方法显著减少了记忆效应，同时保持了图像与文本的对齐。

**Conclusion:** 通过调整初始噪声来促进扩散过程中的早期脱离盆地，可以有效地减少文本到图像扩散模型的记忆效应，同时保持生成图像与输入文本的一致性。

**Abstract:** Despite their impressive generative capabilities, text-to-image diffusion
models often memorize and replicate training data, prompting serious concerns
over privacy and copyright. Recent work has attributed this memorization to an
attraction basin-a region where applying classifier-free guidance (CFG) steers
the denoising trajectory toward memorized outputs-and has proposed deferring
CFG application until the denoising trajectory escapes this basin. However,
such delays often result in non-memorized images that are poorly aligned with
the input prompts, highlighting the need to promote earlier escape so that CFG
can be applied sooner in the denoising process. In this work, we show that the
initial noise sample plays a crucial role in determining when this escape
occurs. We empirically observe that different initial samples lead to varying
escape times. Building on this insight, we propose two mitigation strategies
that adjust the initial noise-either collectively or individually-to find and
utilize initial samples that encourage earlier basin escape. These approaches
significantly reduce memorization while preserving image-text alignment.

</details>


### [11] [The Digital Mirror: Gender Bias and Occupational Stereotypes in AI-Generated Images](https://arxiv.org/abs/2510.08628)
*Siiri Leppälampi,Sonja M. Hyrynsalmi,Erno Vanhala*

Main category: cs.CV

> 研究比较了DALL-E 3和Ideogram两种AI工具在职业场景中的表现偏差情况，发现它们都存在不同程度地强化性别刻板印象的风险，研究提出了有关如何增加性别多样性的建议。

<details>
  <summary>Details</summary>

**Motivation:** 此研究旨在弥补当前关于AI生成视觉内容研究中的一个空白，即忽视了表现偏差问题。研究关注于职业场景中的AI生成图片，并特别讨论了AI生成图像中关于老化和情绪的问题。随着AI图像工具的广泛使用，研究强调了必须解决和缓解这些有害的性别偏见，以确保在媒体和专业环境中实现多样化的表现。

**Method:** 研究通过比较两种AI图像生成工具DALL-E 3和Ideogram，测试了AI生成图片中表现偏差的存在。研究者们生成了超过750张职业场景的AI图像并进行了主题分析。

**Result:** 主题分析显示，DALL-E 3和Ideogram都在不同程度上强化了传统性别刻板印象。

**Conclusion:** 研究结果强调了AI可视化工具可能持续强化狭窄表现的风险。在讨论部分，研究者提出了建议，以帮助从业者、个人和研究者在生成具有可见性别的图像时增加代表性。

**Abstract:** Generative AI offers vast opportunities for creating visualisations, such as
graphics, videos, and images. However, recent studies around AI-generated
visualisations have primarily focused on the creation process and image
quality, overlooking representational biases. This study addresses this gap by
testing representation biases in AI-generated pictures in an occupational
setting and evaluating how two AI image generator tools, DALL-E 3 and Ideogram,
compare. Additionally, the study discusses topics such as ageing and emotions
in AI-generated images. As AI image tools are becoming more widely used,
addressing and mitigating harmful gender biases becomes essential to ensure
diverse representation in media and professional settings. In this study, over
750 AI-generated images of occupations were prompted. The thematic analysis
results revealed that both DALL-E 3 and Ideogram reinforce traditional gender
stereotypes in AI-generated images, although to varying degrees. These findings
emphasise that AI visualisation tools risk reinforcing narrow representations.
In our discussion section, we propose suggestions for practitioners,
individuals and researchers to increase representation when generating images
with visible genders.

</details>


### [12] [Dynamic Mixture-of-Experts for Visual Autoregressive Model](https://arxiv.org/abs/2510.08629)
*Jort Vincenti,Metod Jazbec,Guoxuan Xia*

Main category: cs.CV

> 通过集成动态专家混合路由机制到视觉自回归（VAR）模型，我们通过权衡计算量与图像生成质量，实现了更高的效率和相近的图像质量。

<details>
  <summary>Details</summary>

**Motivation:** 虽然视觉自回归模型提供了高效的图像生成，但是它们因在增加分辨率时重复调用变换器而存在计算冗余的问题。为此，我们提出了一个解决方案来减少这种冗余。

**Method:** 我们引入了一种集成到视觉自回归模型（VAR）中的动态专家混合路由机制。该新架构通过尺度感知阈值策略在计算量与生成质量之间进行权衡，这种策略根据token复杂度和分辨率来平衡专家选择，且不需要额外训练。

**Result:** 最终，我们实现了20%的FLOPs减少，推理速度提高了11%，并且达到了与密集基线相同水平的图像质量。

**Conclusion:** 通过引入尺度感知阈值策略，我们有效减少计算冗余，加速了图像生成过程，同时保持了高图像质量标准，展示了该方法在提升图像生成效率方面的潜力。

**Abstract:** Visual Autoregressive Models (VAR) offer efficient and high-quality image
generation but suffer from computational redundancy due to repeated Transformer
calls at increasing resolutions. We introduce a dynamic Mixture-of-Experts
router integrated into VAR. The new architecture allows to trade compute for
quality through scale-aware thresholding. This thresholding strategy balances
expert selection based on token complexity and resolution, without requiring
additional training. As a result, we achieve 20% fewer FLOPs, 11% faster
inference and match the image quality achieved by the dense baseline.

</details>


### [13] [Out-of-Distribution Detection in LiDAR Semantic Segmentation Using Epistemic Uncertainty from Hierarchical GMMs](https://arxiv.org/abs/2510.08631)
*Hanieh Shojaei Miandashti,Claus Brenner*

Main category: cs.CV

> 本文提出了一种新的无监督的OOD检测方法，该方法能够明确地区分模型不确定性和数据不确定性，而无需额外的数据集或训练阶段，显著提升在SemanticKITTI数据集上的检测性能。

<details>
  <summary>Details</summary>

**Motivation:** 该论文的动机是解决现有的无监督检测方法通常将模型不确定性和数据不确定性混合，导致对歧义的仪器内区域误分类为OOD问题。此外，本研究旨在不依赖辅助数据集的前提下，提升模型检测OOD的性能。

**Method:** 我们的方法是使用从深度神经网络特征空间中的高斯混合模型（GMM）参数的层次贝叶斯建模中得出的模型不确定性来进行无监督的OOD检测。与依赖于预测熵的方法不同，我们的方法能够区分模型不确定性（贝叶斯不确定性）和数据不确定性（数据散度），这样可以更准确地识别出分布式外的数据。

**Result:** 与先前工作中使用的预测熵方法相比，我们的方法在SemanticKITTI数据集上实现了18%的AUROC改进，22%的AUPRC增加，以及36%的FPR95减少（从76%降至40%）。

**Conclusion:** 结论是，该方法通过区分模型不确定性和数据不确定性，在未见过的数据检测上取得了显著的性能提升，显示出在自动驾驶等场景中的应用潜力。

**Abstract:** In addition to accurate scene understanding through precise semantic
segmentation of LiDAR point clouds, detecting out-of-distribution (OOD)
objects, instances not encountered during training, is essential to prevent the
incorrect assignment of unknown objects to known classes. While supervised OOD
detection methods depend on auxiliary OOD datasets, unsupervised methods avoid
this requirement but typically rely on predictive entropy, the entropy of the
predictive distribution obtained by averaging over an ensemble or multiple
posterior weight samples. However, these methods often conflate epistemic
(model) and aleatoric (data) uncertainties, misclassifying ambiguous in
distribution regions as OOD. To address this issue, we present an unsupervised
OOD detection approach that employs epistemic uncertainty derived from
hierarchical Bayesian modeling of Gaussian Mixture Model (GMM) parameters in
the feature space of a deep neural network. Without requiring auxiliary data or
additional training stages, our approach outperforms existing uncertainty-based
methods on the SemanticKITTI dataset, achieving an 18\% improvement in AUROC,
22\% increase in AUPRC, and 36\% reduction in FPR95 (from 76\% to 40\%),
compared to the predictive entropy approach used in prior works.

</details>


### [14] [Hi-OSCAR: Hierarchical Open-set Classifier for Human Activity Recognition](https://arxiv.org/abs/2510.08635)
*Conor McCarthy,Loes Quirijnen,Jan Peter van Zandwijk,Zeno Geradts,Marcel Worring*

Main category: cs.CV

> 提出了一个层级开放设置分类器（Hi-OSCAR）以识别已知活动并拒绝未知活动，并构建了一个新的数据集NFI_FARED来支持这项研究。

<details>
  <summary>Details</summary>

**Motivation:** 解决日常生活活动与训练集中标注的传感器数据活动之间的不可逾越差距，同时考虑到HAR（人类活动识别）中各类活动之间的相似性。

**Method:** 通过构建活动类别的层级结构来处理已知和未知活动的识别问题，并提出Hi-OSCAR（层级开放设置分类器用于活动识别）以实现状态-of-the-art精确识别已知活动，同时拒绝未知活动。

**Result:** 达到了已知活动识别的状态-of-the-art准确率，同时可以将未知活动定位到最接近的内部节点。

**Conclusion:** Hi-OSCAR不仅实现了开放集合分类，而且还通过收集的新数据集（NFI_FARED）推动了开放设置HAR研究的未来。

**Abstract:** Within Human Activity Recognition (HAR), there is an insurmountable gap
between the range of activities performed in life and those that can be
captured in an annotated sensor dataset used in training. Failure to properly
handle unseen activities seriously undermines any HAR classifier's reliability.
Additionally within HAR, not all classes are equally dissimilar, some
significantly overlap or encompass other sub-activities. Based on these
observations, we arrange activity classes into a structured hierarchy. From
there, we propose Hi-OSCAR: a Hierarchical Open-set Classifier for Activity
Recognition, that can identify known activities at state-of-the-art accuracy
while simultaneously rejecting unknown activities. This not only enables
open-set classification, but also allows for unknown classes to be localized to
the nearest internal node, providing insight beyond a binary "known/unknown"
classification. To facilitate this and future open-set HAR research, we
collected a new dataset: NFI_FARED. NFI_FARED contains data from multiple
subjects performing nineteen activities from a range of contexts, including
daily living, commuting, and rapid movements, which is fully public and
available for download.

</details>


### [15] [Detection of high-frequency oscillations using time-frequency analysis](https://arxiv.org/abs/2510.08637)
*Mostafa Mohammadpour,Mehdi Zekriyapanah Gashti,Yusif S. Gasimov*

Main category: cs.CV

> A novel method for detecting HFOs (High-frequency oscillations) using unsupervised clustering and S-transform shows high sensitivity, precision, and F-score, correlating positively with surgical outcomes in epilepsy patients.

<details>
  <summary>Details</summary>

**Motivation:** HFOs are critical biomarkers for the epileptogenic zone but are challenging to detect. This research aims to develop an automated method for detecting HFOs in different frequency bands for clinical use.

**Method:** We validated our novel HFO detection method using both controlled datasets and patient data. The method uses unsupervised clustering with S-transform to distinguish HFOs from other activities.

**Result:** Our method achieved high sensitivity (97.67%), precision (98.57%), and an F-score of 97.78%. In epilepsy patients, a strong correlation (ratio of 0.73) with surgical outcomes was found.

**Conclusion:** The study confirms that removing HFOs, particularly fast ripples, correlates with seizure freedom, validating HFOs as biomarkers for epileptogenicity.

**Abstract:** High-frequency oscillations (HFOs) are a new biomarker for identifying the
epileptogenic zone. Mapping HFO-generating regions can improve the precision of
resection sites in patients with refractory epilepsy. However, detecting HFOs
remains challenging, and their clinical features are not yet fully defined.
Visual identification of HFOs is time-consuming, labor-intensive, and
subjective. As a result, developing automated methods to detect HFOs is
critical for research and clinical use. In this study, we developed a novel
method for detecting HFOs in the ripple and fast ripple frequency bands (80-500
Hz). We validated it using both controlled datasets and data from epilepsy
patients. Our method employs an unsupervised clustering technique to categorize
events extracted from the time-frequency domain using the S-transform. The
proposed detector differentiates HFOs events from spikes, background activity,
and artifacts. Compared to existing detectors, our method achieved a
sensitivity of 97.67%, a precision of 98.57%, and an F-score of 97.78% on the
controlled dataset. In epilepsy patients, our results showed a stronger
correlation with surgical outcomes, with a ratio of 0.73 between HFOs rates in
resected versus non-resected contacts. The study confirmed previous findings
that HFOs are promising biomarkers of epileptogenicity in epileptic patients.
Removing HFOs, especially fast ripple, leads to seizure freedom, while
remaining HFOs lead to seizure recurrence.

</details>


### [16] [Into the Rabbit Hull: From Task-Relevant Concepts in DINO to Minkowski Geometry](https://arxiv.org/abs/2510.08638)
*Thomas Fel,Binxu Wang,Michael A. Lepori,Matthew Kowal,Andrew Lee,Randall Balestriero,Sonia Joseph,Ekdeep S. Lubana,Talia Konkle,Demba Ba,Martin Wattenberg*

Main category: cs.CV

> 本文通过对DINOv2模型部署不同下游任务时感知内容的探究，深入分析了模型内部的线性表示假设（LRH）机制，并提出了一种新的表示假设（MRH），展示了视觉变换器模型中词典表示的特征和结构。

<details>
  <summary>Details</summary>

**Motivation:** 本文旨在揭示DINOv2这样的视觉模型所感知内容的本质，尤其是在不同任务下的表现。通过探究模型内部的概念表示方式，希望能理解视觉模型如何处理和解释视觉信息。

**Method:** 本文采用了线性表示假设（LRH）作为工作基线，并使用SAEs（稀疏自动编码器）实现这一假设，创建了一个包含32,000个单元的词典以作为研究的可解释性基础。研究分为三个部分，首先是分析不同下游任务如何从学到的词典中调用概念，其次是分析词典的概念结构和统计特性，最后是提出一种新的表示假设（Minkowski Representation Hypothesis, MRH）并探讨其实证特征和对未来理解视觉变换器表示的意义。

**Result:** 研究揭示了代表性的功能特殊化，以及在逻辑结构上有别于先前的线性稀疏表示的观点，呈现出一种新的表示结构，证实词典中元素在局部具有连接性且部分表示是密集化的，而不是严格的稀疏化。

**Conclusion:** 本文提出了Minkowski表示假设（MRH）来更精确地描述DINOv2的词典表示方式，这表明模型的表示形式是由各种原型的凸组合构成，这种结构基于概念空间和模型中多头注意力机制产生凸组合和定义由原型界定的区域的思想。

**Abstract:** DINOv2 is routinely deployed to recognize objects, scenes, and actions; yet
the nature of what it perceives remains unknown. As a working baseline, we
adopt the Linear Representation Hypothesis (LRH) and operationalize it using
SAEs, producing a 32,000-unit dictionary that serves as the interpretability
backbone of our study, which unfolds in three parts.
  In the first part, we analyze how different downstream tasks recruit concepts
from our learned dictionary, revealing functional specialization:
classification exploits "Elsewhere" concepts that fire everywhere except on
target objects, implementing learned negations; segmentation relies on boundary
detectors forming coherent subspaces; depth estimation draws on three distinct
monocular depth cues matching visual neuroscience principles.
  Following these functional results, we analyze the geometry and statistics of
the concepts learned by the SAE. We found that representations are partly dense
rather than strictly sparse. The dictionary evolves toward greater coherence
and departs from maximally orthogonal ideals (Grassmannian frames). Within an
image, tokens occupy a low dimensional, locally connected set persisting after
removing position. These signs suggest representations are organized beyond
linear sparsity alone.
  Synthesizing these observations, we propose a refined view: tokens are formed
by combining convex mixtures of archetypes (e.g., a rabbit among animals, brown
among colors, fluffy among textures). This structure is grounded in Gardenfors'
conceptual spaces and in the model's mechanism as multi-head attention produces
sums of convex mixtures, defining regions bounded by archetypes. We introduce
the Minkowski Representation Hypothesis (MRH) and examine its empirical
signatures and implications for interpreting vision-transformer
representations.

</details>


### [17] [PhyDAE: Physics-Guided Degradation-Adaptive Experts for All-in-One Remote Sensing Image Restoration](https://arxiv.org/abs/2510.08653)
*Zhe Dong,Yuzhe Sun,Haochen Jiang,Tianzhu Liu,Yanfeng Gu*

Main category: cs.CV

> 提出了一种称为Physics-Guided Degradation-Adaptive Experts (PhyDAE) 的方法，通过两阶段级联架构，能够精准识别和处理多种异构退化因素，并通过引入物理感知专家模块和温度控制稀疏激活策略，提高了计算效率同时保持了成像物理一致性。实验表明，PhyDAE 在四个恢复任务上表现出色，同时参数量和计算复杂度显著降低。

<details>
  <summary>Details</summary>

**Motivation:** 现有的全功能恢复方法过于依赖隐式特征表示，缺乏对退化物理的显式建模，难以准确处理复杂的异质退化因素。

**Method:** PhyDAE 采用两阶段级联架构，将退化信息从隐式特征转变为显式决策信号。该模型通过引入Residual Manifold Projector (RMP) 和Frequency-Aware Degradation Decomposer (FADD)，从流形几何和频率两个角度分析退化特征。同时采用物理感知专家模块和温度控制稀疏激活策略来增强计算效率，保证成像物理一致性。

**Result:** 在三个基准数据集 (MD-RSID, MD-RRSHID, 和MDRS-Landsat) 的广泛实验表明，PhyDAE 在所有四个恢复任务上表现出色，参数量和计算复杂度显著降低。

**Conclusion:** PhyDAE 在图像恢复质量上表现出色，并且在参数量和计算复杂度方面优于主流方法，实现了性能和效率的最优平衡。

**Abstract:** Remote sensing images inevitably suffer from various degradation factors
during acquisition, including atmospheric interference, sensor limitations, and
imaging conditions. These complex and heterogeneous degradations pose severe
challenges to image quality and downstream interpretation tasks. Addressing
limitations of existing all-in-one restoration methods that overly rely on
implicit feature representations and lack explicit modeling of degradation
physics, this paper proposes Physics-Guided Degradation-Adaptive Experts
(PhyDAE). The method employs a two-stage cascaded architecture transforming
degradation information from implicit features into explicit decision signals,
enabling precise identification and differentiated processing of multiple
heterogeneous degradations including haze, noise, blur, and low-light
conditions. The model incorporates progressive degradation mining and
exploitation mechanisms, where the Residual Manifold Projector (RMP) and
Frequency-Aware Degradation Decomposer (FADD) comprehensively analyze
degradation characteristics from manifold geometry and frequency perspectives.
Physics-aware expert modules and temperature-controlled sparse activation
strategies are introduced to enhance computational efficiency while ensuring
imaging physics consistency. Extensive experiments on three benchmark datasets
(MD-RSID, MD-RRSHID, and MDRS-Landsat) demonstrate that PhyDAE achieves
superior performance across all four restoration tasks, comprehensively
outperforming state-of-the-art methods. Notably, PhyDAE substantially improves
restoration quality while achieving significant reductions in parameter count
and computational complexity, resulting in remarkable efficiency gains compared
to mainstream approaches and achieving optimal balance between performance and
efficiency. Code is available at https://github.com/HIT-SIRS/PhyDAE.

</details>
