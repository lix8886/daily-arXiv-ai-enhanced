{"id": "2506.20747", "categories": ["cs.CL", "68T50, 68T37", "I.2.7"], "pdf": "https://arxiv.org/pdf/2506.20747", "abs": "https://arxiv.org/abs/2506.20747", "authors": ["Chen Shen", "Sajjadur Rahman", "Estevam Hruschka"], "title": "Towards Probabilistic Question Answering Over Tabular Data", "comment": null, "summary": "Current approaches for question answering (QA) over tabular data, such as\nNL2SQL systems, perform well for factual questions where answers are directly\nretrieved from tables. However, they fall short on probabilistic questions\nrequiring reasoning under uncertainty. In this paper, we introduce a new\nbenchmark LUCARIO and a framework for probabilistic QA over large tabular data.\nOur method induces Bayesian Networks from tables, translates natural language\nqueries into probabilistic queries, and uses large language models (LLMs) to\ngenerate final answers. Empirical results demonstrate significant improvements\nover baselines, highlighting the benefits of hybrid symbolic-neural reasoning.", "AI": {"tldr": "The paper introduces LUCARIO, a new benchmark and framework for probabilistic question answering over large tabular data, demonstrating significant improvements over existing methods by utilizing a hybrid symbolic-neural approach.", "motivation": "Current QA systems perform well for factual questions but lack the capability to handle probabilistic questions requiring reasoning under uncertainty.", "method": "Our method induces Bayesian Networks from tables, translates natural language queries into probabilistic queries, and uses large language models to generate final answers.", "result": "Empirical results demonstrate significant improvements over baselines.", "conclusion": "The study highlights the benefits of hybrid symbolic-neural reasoning for probabilistic question answering over large tabular data."}}
{"id": "2506.20793", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.20793", "abs": "https://arxiv.org/abs/2506.20793", "authors": ["Victor Ojewale", "Inioluwa Deborah Raji", "Suresh Venkatasubramanian"], "title": "Multi-lingual Functional Evaluation for Large Language Models", "comment": null, "summary": "Multi-lingual competence in large language models is often evaluated via\nstatic data benchmarks such as Belebele, M-MMLU and M-GSM. However, these\nevaluations often fail to provide an adequate understanding of the practical\nperformance and robustness of models across multi-lingual settings. In\nresponse, we create multi-lingual functional benchmarks -- Cross-Lingual Grade\nSchool Math Symbolic (CL-GSM Symbolic) and Cross-Lingual Instruction-Following\nEval (CL-IFEval)-- by translating existing functional benchmark templates from\nEnglish to five additional languages that span the range of resources available\nfor NLP: French, Spanish, Hindi, Arabic and Yoruba. Our results reveal that\nsome static multi-lingual benchmarks capture functional performance much more\nclosely than others (i.e. across models, there is a 24%, 17% and 18% decrease\nin performance between M-GSM and CL-GSM Symbolic in English, French and Spanish\nrespectively; similarly there's a 15 - 24% performance drop across languages\nbetween Belebele and CL-IFEval, and only a 0.5% to 3% performance drop between\nM-MMLU and CL-IFEval). Similarly, we find that model robustness across\nlanguages varies significantly, with certain languages (eg. Arabic, English)\nbeing the most consistently well performing across evaluation iterations.", "AI": {"tldr": "本研究通过创建多语言功能基准来评估大型语言模型的实用性与鲁棒性，发现与其他现有基准相比，新的基准更能准确反映模型在多种语言环境下的真实表现。", "motivation": "现有的多语言评估方式常常未能提供对模型跨多种语言环境中实际表现和鲁棒性的充分理解。", "method": "通过将现有的功能基准模板从英语翻译成法语、西班牙语、印地语、阿拉伯语和约鲁巴语，创建了多语言功能基准——跨语言小学数学符号（CL-GSM Symbolic）和跨语言指令跟随评估（CL-IFEval）来回应静态数据评估在多语言环境下的表现理解不足的问题。", "result": "研究结果显示，一些静态多语言基准捕获功能表现比其他基准更接近。在CL-GSM Symbolic中，英文、法文和西班牙文的表现分别下降了24%、17%和18%；而在CL-IFEval中，性能下降范围介于15%到24%之间。不同语言之间的模型鲁棒性表现出显著差异。", "conclusion": "本研究证明了多语言功能基准对于评估大型语言模型跨多种语言的表现和鲁棒性的价值，同时指出了各类基准在不同语言表现上的差异性。"}}
{"id": "2506.20803", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.20803", "abs": "https://arxiv.org/abs/2506.20803", "authors": ["Chenglei Si", "Tatsunori Hashimoto", "Diyi Yang"], "title": "The Ideation-Execution Gap: Execution Outcomes of LLM-Generated versus Human Research Ideas", "comment": "main paper is 14 pages", "summary": "Large Language Models (LLMs) have shown promise in accelerating the\nscientific research pipeline. A key capability for this process is the ability\nto generate novel research ideas, and prior studies have found settings in\nwhich LLM-generated research ideas were judged as more novel than human-expert\nideas. However, a good idea should not simply appear to be novel, it should\nalso result in better research after being executed. To test whether\nAI-generated ideas lead to better research outcomes, we conduct an execution\nstudy by recruiting 43 expert researchers to execute randomly-assigned ideas,\neither written by experts or generated by an LLM. Each expert spent over 100\nhours implementing the idea and wrote a 4-page short paper to document the\nexperiments. All the executed projects are then reviewed blindly by expert NLP\nresearchers. Comparing the review scores of the same ideas before and after\nexecution, the scores of the LLM-generated ideas decrease significantly more\nthan expert-written ideas on all evaluation metrics (novelty, excitement,\neffectiveness, and overall; p < 0.05), closing the gap between LLM and human\nideas observed at the ideation stage. When comparing the aggregated review\nscores from the execution study, we even observe that for many metrics there is\na flip in rankings where human ideas score higher than LLM ideas. This\nideation-execution gap highlights the limitations of current LLMs in generating\ntruly effective research ideas and the challenge of evaluating research ideas\nin the absence of execution outcomes.", "AI": {"tldr": "本研究通过实验证实，尽管人工智能生成的研究想法在创意阶段表现新颖，但在执行阶段其质量下降显著，低于人类生成的想法。这揭示了大型语言模型生成有效研究想法的局限性。", "motivation": "尽管有研究表明，大型语言模型生成的研究想法在新颖性上可能超过人类专家的想法，但高质量的想法不仅应当新颖，还应在执行后带来更好的研究结果。因此，本研究旨在检验由人工智能生成的想法是否能提升研究的成效。", "method": "通过招募43位专家研究人员执行随机分配的研究想法（这些想法是由专家编写或由大型语言模型生成的），来测试人工智能生成的想法是否能带来更好的研究结果。每位专家投入超过100小时实施想法，并撰写一篇4页的短文记录实验过程。所有执行的项目由专家自然语言处理研究员进行匿名评审。", "result": "执行后，大型语言模型生成的想法的评审分数（新颖性、兴奋性、有效性、总体）显著下降，而且下降幅度高于专家编写的想法，这意味着人工智能生成的想法在执行后其质量不及预期。在许多评审指标中，人类生成的想法得分甚至超过了人工智能生成的想法。", "conclusion": "这种想法生成与执行之间的差异突显了当前大型语言模型在生成真正有效的研究想法方面的局限性，也暴露了在没有执行结果的情况下评估研究想法的挑战。"}}
{"id": "2506.20821", "categories": ["cs.CL", "cs.AI", "cs.CE", "68T50, 68T07 (Primary) 68P20, 91G15, 91G70, 68U10 (Secondary)", "I.2.7; I.2.10; H.3.3; H.2.8; I.5.4; J.1"], "pdf": "https://arxiv.org/pdf/2506.20821", "abs": "https://arxiv.org/abs/2506.20821", "authors": ["Chinmay Gondhalekar", "Urjitkumar Patel", "Fang-Chun Yeh"], "title": "MultiFinRAG: An Optimized Multimodal Retrieval-Augmented Generation (RAG) Framework for Financial Question Answering", "comment": "Preprint Copy", "summary": "Financial documents--such as 10-Ks, 10-Qs, and investor presentations--span\nhundreds of pages and combine diverse modalities, including dense narrative\ntext, structured tables, and complex figures. Answering questions over such\ncontent often requires joint reasoning across modalities, which strains\ntraditional large language models (LLMs) and retrieval-augmented generation\n(RAG) pipelines due to token limitations, layout loss, and fragmented\ncross-modal context. We introduce MultiFinRAG, a retrieval-augmented generation\nframework purpose-built for financial QA. MultiFinRAG first performs multimodal\nextraction by grouping table and figure images into batches and sending them to\na lightweight, quantized open-source multimodal LLM, which produces both\nstructured JSON outputs and concise textual summaries. These outputs, along\nwith narrative text, are embedded and indexed with modality-aware similarity\nthresholds for precise retrieval. A tiered fallback strategy then dynamically\nescalates from text-only to text+table+image contexts when necessary, enabling\ncross-modal reasoning while reducing irrelevant context. Despite running on\ncommodity hardware, MultiFinRAG achieves 19 percentage points higher accuracy\nthan ChatGPT-4o (free-tier) on complex financial QA tasks involving text,\ntables, images, and combined multimodal reasoning.", "AI": {"tldr": "提出了MultiFinRAG，一种专门为财务问答任务设计的检索增强生成框架。该方法通过分层次递归策略，实现了在普通硬件上跨模态推理的高效准确问答，表现出比ChatGPT-4o更高的准确率。", "motivation": "该研究的动机是由于财务文件（如年度报告10-Ks、季度报告10-Qs和投资者演示）通常包含大量文本、表格和图像等多种模态，并且对这些内容进行答案提取往往需要跨模态推理，这超出了传统大型语言模型和检索增强生成管道的能力，因为后者会受到token限制、布局损失和跨模态上下文碎片化的困扰。", "method": "MultiFinRAG方法首先进行多模态提取，将表格和图像分成批次并发送给一个轻量级的开源多模态大型语言模型，该模型生成结构化的JSON输出和简洁的文本摘要。这些输出连同叙事文本一起嵌入并根据模态感知相似度阈值进行索引，以便精确检索。该方法采用分层递归策略，根据需要从文本到文本+表格+图像上下文进行动态升级，实现在减少无关上下文的同时的跨模态推理能力。", "result": "在涉及文本、表格、图像和结合多模态推理的复杂财务问题回答任务中，尽管在普通硬件上运行，MultiFinRAG仍比ChatGPT-4o（免费级别）高出19个百分点的准确率。", "conclusion": "研究结果表明，MultiFinRAG作为一种专门针对财务领域的问答任务构建的检索增强生成框架，能够克服传统大型语言模型和检索增强生成方法的限制，通过分层递归策略实现在普通硬件上进行有效的跨模态推理，同时提高了问题回答准确性。"}}
{"id": "2506.20741", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.20741", "abs": "https://arxiv.org/abs/2506.20741", "authors": ["Qin Ren", "Yifan Wang", "Ruogu Fang", "Haibin Ling", "Chenyu You"], "title": "OTSurv: A Novel Multiple Instance Learning Framework for Survival Prediction with Heterogeneity-aware Optimal Transport", "comment": null, "summary": "Survival prediction using whole slide images (WSIs) can be formulated as a\nmultiple instance learning (MIL) problem. However, existing MIL methods often\nfail to explicitly capture pathological heterogeneity within WSIs, both\nglobally -- through long-tailed morphological distributions, and locally\nthrough -- tile-level prediction uncertainty. Optimal transport (OT) provides a\nprincipled way of modeling such heterogeneity by incorporating marginal\ndistribution constraints. Building on this insight, we propose OTSurv, a novel\nMIL framework from an optimal transport perspective. Specifically, OTSurv\nformulates survival predictions as a heterogeneity-aware OT problem with two\nconstraints: (1) global long-tail constraint that models prior morphological\ndistributions to avert both mode collapse and excessive uniformity by\nregulating transport mass allocation, and (2) local uncertainty-aware\nconstraint that prioritizes high-confidence patches while suppressing noise by\nprogressively raising the total transport mass. We then recast the initial OT\nproblem, augmented by these constraints, into an unbalanced OT formulation that\ncan be solved with an efficient, hardware-friendly matrix scaling algorithm.\nEmpirically, OTSurv sets new state-of-the-art results across six popular\nbenchmarks, achieving an absolute 3.6% improvement in average C-index. In\naddition, OTSurv achieves statistical significance in log-rank tests and offers\nhigh interpretability, making it a powerful tool for survival prediction in\ndigital pathology. Our codes are available at\nhttps://github.com/Y-Research-SBU/OTSurv.", "AI": {"tldr": "提出了一种用于使用全幻灯片图像（WSIs）进行生存预测的最优传输（OT）方法OTSurv，该方法可以全局和局部的层面捕捉病理异质性，在生存预测中取得了新的最先进结果。", "motivation": "传统的方法在处理全幻灯片图像（WSIs）中的生存预测时无法充分考虑病理异质性。这种异质性体现在全局的长尾形态分布和局部的瓦片级预测不确定性上。而最优传输理论提供了一个通过边际分布约束来建模异质性的方法，因此作者提出了OTSurv模型。", "method": "OTSurv模型利用最优传输理论，以异质性意识的最优传输问题来制定生存预测，包括两个约束条件：一个全局长尾约束条件和一个地方不确定性意识约束条件。全局长尾约束条件可以调节运输质量分配；地方不确定性意识约束条件可以通过逐步增加运输质量来抑制噪声，优先考虑高可信度的补丁。", "result": "实验结果显示，OTSurv模型在六个流行基准上实现了新的最先进的结果，平均C指标上的绝对改善率为3.6%。此外，OTSurv还在对数秩检验中达到了统计显著性，并提供了高可解释性。", "conclusion": "OTSurv通过将最优运输理论应用于多实例学习框架，成功地捕捉到病理异质性，并在生存预测中取得了优异的性能。"}}
{"id": "2506.20822", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.20822", "abs": "https://arxiv.org/abs/2506.20822", "authors": ["Quintin Myers", "Yanjun Gao"], "title": "Uncovering Hidden Violent Tendencies in LLMs: A Demographic Analysis via Behavioral Vignettes", "comment": "Under review", "summary": "Large language models (LLMs) are increasingly proposed for detecting and\nresponding to violent content online, yet their ability to reason about morally\nambiguous, real-world scenarios remains underexamined. We present the first\nstudy to evaluate LLMs using a validated social science instrument designed to\nmeasure human response to everyday conflict, namely the Violent Behavior\nVignette Questionnaire (VBVQ). To assess potential bias, we introduce\npersona-based prompting that varies race, age, and geographic identity within\nthe United States. Six LLMs developed across different geopolitical and\norganizational contexts are evaluated under a unified zero-shot setting. Our\nstudy reveals two key findings: (1) LLMs surface-level text generation often\ndiverges from their internal preference for violent responses; (2) their\nviolent tendencies vary across demographics, frequently contradicting\nestablished findings in criminology, social science, and psychology.", "AI": {"tldr": "研究评估了大型语言模型对暴力内容反应的倾向，发现这些模型的表面反应与其内部倾向不一致，并且模型显示的暴力倾向与个人的社会人口学特征有关。", "motivation": "随着大型语言模型越来越多地被提议用于在线检测和响应暴力内容，但这些模型在处理道德上模棱两可的现实世界场景时的能力仍然没有得到充分研究。因此，本研究旨在填补这一空白。", "method": "研究通过使用《暴力行为小品问卷》（VBVQ）这一经过验证的社会科学研究工具来评估大型语言模型（LLMs）对现实世界冲突事件的反应能力。为了评估潜在的偏见，研究引入了基于人物设定的提示，该设定变化了美国境内的种族、年龄、地理认同等因素。评估了六个在不同地缘政治组织背景下开发的LLMs在统一的零样本情况下对暴力内容的检测与响应能力。", "result": "研究揭示了两个关键发现：(1) LLMs的表面文本生成经常与其对暴力反应的内在偏好相矛盾；(2) 它们的暴力倾向在不同的社会人群中有所差异，通常与犯罪学、社会科学和心理学中已确立的发现不符。", "conclusion": "本研究发现大型语言模型在生成文本时表现出的暴力倾向和其内在的暴力倾向之间存在差异，且这种倾向性具有社会人口学差异，这与现有社会科学领域发现有悖。"}}
{"id": "2506.20756", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.20756", "abs": "https://arxiv.org/abs/2506.20756", "authors": ["Haodong Li", "Chen Wang", "Jiahui Lei", "Kostas Daniilidis", "Lingjie Liu"], "title": "StereoDiff: Stereo-Diffusion Synergy for Video Depth Estimation", "comment": "Work done in Nov. 2024. Project page: https://stereodiff.github.io/", "summary": "Recent video depth estimation methods achieve great performance by following\nthe paradigm of image depth estimation, i.e., typically fine-tuning pre-trained\nvideo diffusion models with massive data. However, we argue that video depth\nestimation is not a naive extension of image depth estimation. The temporal\nconsistency requirements for dynamic and static regions in videos are\nfundamentally different. Consistent video depth in static regions, typically\nbackgrounds, can be more effectively achieved via stereo matching across all\nframes, which provides much stronger global 3D cues. While the consistency for\ndynamic regions still should be learned from large-scale video depth data to\nensure smooth transitions, due to the violation of triangulation constraints.\nBased on these insights, we introduce StereoDiff, a two-stage video depth\nestimator that synergizes stereo matching for mainly the static areas with\nvideo depth diffusion for maintaining consistent depth transitions in dynamic\nareas. We mathematically demonstrate how stereo matching and video depth\ndiffusion offer complementary strengths through frequency domain analysis,\nhighlighting the effectiveness of their synergy in capturing the advantages of\nboth. Experimental results on zero-shot, real-world, dynamic video depth\nbenchmarks, both indoor and outdoor, demonstrate StereoDiff's SoTA performance,\nshowcasing its superior consistency and accuracy in video depth estimation.", "AI": {"tldr": "StereoDiff, a novel two-stage video depth estimation model, blends stereo matching and video depth diffusion techniques to achieve superior performance on dynamic video benchmarks by addressing the unique needs of static and dynamic regions within videos.", "motivation": "The motivation stems from the observation that current video depth estimation methods, which adapt image depth estimation paradigms, fail to account for the unique temporal consistency requirements of static and dynamic regions in videos.", "method": "The method introduces StereoDiff, a two-stage video depth estimation model that combines stereo matching for static areas and video depth diffusion for dynamic areas, based on the observation that video depth estimation requires different approaches for static and dynamic regions.", "result": "Experimental results show that StereoDiff outperforms existing methods on zero-shot, real-world dynamic video depth benchmarks, both indoor and outdoor, in terms of video depth estimation consistency and accuracy.", "conclusion": "The conclusion is that the synergy between stereo matching for static areas and video depth diffusion for dynamic areas not only addresses the shortcomings of naive extensions of image depth estimation but also provides a more effective approach for video depth estimation."}}
{"id": "2506.20876", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.20876", "abs": "https://arxiv.org/abs/2506.20876", "authors": ["Sebastian Joseph", "Lily Chen", "Barry Wei", "Michael Mackert", "Iain J. Marshall", "Paul Pu Liang", "Ramez Kouzy", "Byron C. Wallace", "Junyi Jessy Li"], "title": "Decide less, communicate more: On the construct validity of end-to-end fact-checking in medicine", "comment": null, "summary": "Technological progress has led to concrete advancements in tasks that were\nregarded as challenging, such as automatic fact-checking. Interest in adopting\nthese systems for public health and medicine has grown due to the high-stakes\nnature of medical decisions and challenges in critically appraising a vast and\ndiverse medical literature. Evidence-based medicine connects to every\nindividual, and yet the nature of it is highly technical, rendering the medical\nliteracy of majority users inadequate to sufficiently navigate the domain. Such\nproblems with medical communication ripens the ground for end-to-end\nfact-checking agents: check a claim against current medical literature and\nreturn with an evidence-backed verdict. And yet, such systems remain largely\nunused. To understand this, we present the first study examining how clinical\nexperts verify real claims from social media by synthesizing medical evidence.\nIn searching for this upper-bound, we reveal fundamental challenges in\nend-to-end fact-checking when applied to medicine: Difficulties connecting\nclaims in the wild to scientific evidence in the form of clinical trials;\nambiguities in underspecified claims mixed with mismatched intentions; and\ninherently subjective veracity labels. We argue that fact-checking should be\napproached and evaluated as an interactive communication problem, rather than\nan end-to-end process.", "AI": {"tldr": "研究探讨了在医学领域应用端到端事实核查系统时遇到的基本挑战，包括连接声明与科学证据的难度和声明的不确定性。", "motivation": "随着技术的发展，自动事实核查在医学领域内的应用引起关注。然而，此类系统的实际应用并不广泛，本研究旨在探索其背后的原因。", "method": "本研究通过分析临床专家如何核实社交媒体上的真实声明，从而综合医学证据，探讨了在医学领域应用端到端事实核查的基本挑战。", "result": "研究表明，在医学应用中的端到端事实核查面临着将野外声明与临床试验联系起来的困难，以及意图不匹配和声明的模糊性等挑战。", "conclusion": "事实核查应当被视作一种交互沟通问题而非端到端过程进行处理和评估。"}}
{"id": "2506.20757", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2506.20757", "abs": "https://arxiv.org/abs/2506.20757", "authors": ["Zhiyuan Wu", "Yongqiang Zhao", "Shan Luo"], "title": "ConViTac: Aligning Visual-Tactile Fusion with Contrastive Representations", "comment": null, "summary": "Vision and touch are two fundamental sensory modalities for robots, offering\ncomplementary information that enhances perception and manipulation tasks.\nPrevious research has attempted to jointly learn visual-tactile representations\nto extract more meaningful information. However, these approaches often rely on\ndirect combination, such as feature addition and concatenation, for modality\nfusion, which tend to result in poor feature integration. In this paper, we\npropose ConViTac, a visual-tactile representation learning network designed to\nenhance the alignment of features during fusion using contrastive\nrepresentations. Our key contribution is a Contrastive Embedding Conditioning\n(CEC) mechanism that leverages a contrastive encoder pretrained through\nself-supervised contrastive learning to project visual and tactile inputs into\nunified latent embeddings. These embeddings are used to couple visual-tactile\nfeature fusion through cross-modal attention, aiming at aligning the unified\nrepresentations and enhancing performance on downstream tasks. We conduct\nextensive experiments to demonstrate the superiority of ConViTac in real world\nover current state-of-the-art methods and the effectiveness of our proposed CEC\nmechanism, which improves accuracy by up to 12.0% in material classification\nand grasping prediction tasks.", "AI": {"tldr": "提出了一种名为ConViTac的视觉-触觉网络，利用对比嵌入条件(CEC)机制来改善特征融合，从而提高下游任务性能。", "motivation": "视觉和触觉是机器人感知和操作任务中的重要感官模态，提供互补信息。以往的研究试图通过直接组合（如特征加法和拼接）来融合模态，但这导致特征整合效果差。", "method": "提出了一种名为ConViTac的视觉-触觉表示学习网络，旨在通过对比表示来增强融合期间的特征对齐。关键贡献是一种对比嵌入条件（CEC）机制，该机制利用通过自监督对比学习预训练的对比编码器将视觉和触觉输入投影到统一的潜在嵌入中。通过跨模态注意力，这些嵌入用于结合视觉-触觉特征融合，以对齐统一表示并提高下游任务的表现。", "result": "实验表明ConViTac在现实世界中优于当前最先进的方法，CEC机制在材料分类和抓握预测任务中提高了12.0%的精度。", "conclusion": "ConViTac网络和CEC机制在提升视觉-触觉融合方面表现出优越性。"}}
{"id": "2506.20917", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.20917", "abs": "https://arxiv.org/abs/2506.20917", "authors": ["Zhengyan Shi"], "title": "Optimising Language Models for Downstream Tasks: A Post-Training Perspective", "comment": "PhD Thesis", "summary": "Language models (LMs) have demonstrated remarkable capabilities in NLP, yet\nadapting them efficiently and robustly to specific tasks remains challenging.\nAs their scale and complexity grow, fine-tuning LMs on labelled data often\nunderutilizes available unlabelled data, leads to overfitting on small\ntask-specific sets, and imposes significant computational costs. These\nlimitations hamper their application to the open-ended landscape of real-world\nlanguage tasks.\n  This thesis proposes a series of methods to better adapt LMs to downstream\napplications. First, we explore strategies for extracting task-relevant\nknowledge from unlabelled data, introducing a novel continued pre-training\ntechnique that outperforms state-of-the-art semi-supervised approaches. Next,\nwe present a parameter-efficient fine-tuning method that substantially reduces\nmemory and compute costs while maintaining competitive performance. We also\nintroduce improved supervised fine-tuning methods that enable LMs to better\nfollow instructions, especially when labelled data is scarce, enhancing their\nperformance across a range of NLP tasks, including open-ended generation.\nFinally, we develop new evaluation methods and benchmarks, such as multi-hop\nspatial reasoning tasks, to assess LM capabilities and adaptation more\ncomprehensively.\n  Through extensive empirical studies across diverse NLP tasks, our results\ndemonstrate that these approaches substantially improve LM robustness,\nefficiency, and generalization, making them more adaptable to a broad range of\napplications. These advances mark a significant step towards more robust and\nefficient LMs, bringing us closer to the goal of artificial general\nintelligence.", "AI": {"tldr": "论文提出了一系列方法以更有效地将语言模型（LM）适应于下游应用，包括从无标签数据中提取任务相关信息的新技术，计算成本低、性能优越的参数高效微调方法，以及评估和基准测试的新方法。结果表明，这些方法显著地提高了LM的鲁棒性、效率和泛化能力。", "motivation": "随着语言模型规模和复杂性的增长，它们在特定任务上进行有效和稳固地微调仍然面临挑战，如：对标注数据的低估、小样本任务中过拟合以及计算成本高等问题，这些限制阻碍了语言模型在真实世界语言任务中的应用。", "method": "论文引入了从无标签数据中提取任务相关信息的新技术，开发了计算成本低廉、性能优越的参数高效微调技术，改进了监督微调方法，使得语言模型能够在标注数据稀缺的情况下更好地遵循指令，以及开发了新的评估方法和基准测试，如多层次空间推理任务。", "result": "广泛的经验研究表明，这些方法显著提高了语言模型在各种NLP任务上的鲁棒性、效率和泛化能力，使得它们在不同应用场景中更加灵活适应。", "conclusion": "这些进展标志着向更为稳固和高效的语言模型迈进的重要一步，并接近了人工智能的目标。"}}
{"id": "2506.20786", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.20786", "abs": "https://arxiv.org/abs/2506.20786", "authors": ["Connor Ludwig", "Khashayar Namdar", "Farzad Khalvati"], "title": "AI-Driven MRI-based Brain Tumour Segmentation Benchmarking", "comment": null, "summary": "Medical image segmentation has greatly aided medical diagnosis, with U-Net\nbased architectures and nnU-Net providing state-of-the-art performance. There\nhave been numerous general promptable models and medical variations introduced\nin recent years, but there is currently a lack of evaluation and comparison of\nthese models across a variety of prompt qualities on a common medical dataset.\nThis research uses Segment Anything Model (SAM), Segment Anything Model 2 (SAM\n2), MedSAM, SAM-Med-3D, and nnU-Net to obtain zero-shot inference on the BraTS\n2023 adult glioma and pediatrics dataset across multiple prompt qualities for\nboth points and bounding boxes. Several of these models exhibit promising Dice\nscores, particularly SAM and SAM 2 achieving scores of up to 0.894 and 0.893,\nrespectively when given extremely accurate bounding box prompts which exceeds\nnnU-Net's segmentation performance. However, nnU-Net remains the dominant\nmedical image segmentation network due to the impracticality of providing\nhighly accurate prompts to the models. The model and prompt evaluation, as well\nas the comparison, are extended through fine-tuning SAM, SAM 2, MedSAM, and\nSAM-Med-3D on the pediatrics dataset. The improvements in point prompt\nperformance after fine-tuning are substantial and show promise for future\ninvestigation, but are unable to achieve better segmentation than bounding\nboxes or nnU-Net.", "AI": {"tldr": "研究通过在BraTS 2023成人胶质瘤和儿科数据集上使用多种模型的零样本推理，评估和比较了不同种类的提示质量对分割结果的影响。虽然SAM和SAM 2在给定极高精度的边界框提示时可达到超过nnU-Net的分割性能，但由于提供高精度提示在实践中不切实际，nnU-Net仍然是医学图像分割领域的主流模型。", "motivation": "研究目的是填补关于各种医学图像分割模型评估和比较的空白，特别是针对不同质量提示的应用效果。", "method": "该研究通过零样本推理和微调的方式，评估了包括SAM, SAM 2, MedSAM, SAM-Med-3D 和 nnU-Net在内的模型在BraTS 2023数据集上的性能，具体采用了点提示和边界框提示。", "result": "结果显示，SAM 和 SAM 2 在给定高度准确的边界框提示时，达到了非常高的Dice得分(0.894和0.893)，超过了nnU-Net的分割性能。但是，通过微调，SAM, SAM 2, MedSAM 和 SAM-Med-3D在儿科数据集上的性能提升，虽然在点提示上有很大进展，但仍然比不上边界框提示或nnU-Net的性能。", "conclusion": "尽管一些具有提示功能的新模型在特定条件下表现优异，但给予高精度提示的实际困难决定了nnU-Net依然是医学图像分割最好的模型。微调虽然对新模型在点提示上的性能有显著提升，但尚未超越nnU-Net。"}}
{"id": "2506.20920", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.20920", "abs": "https://arxiv.org/abs/2506.20920", "authors": ["Guilherme Penedo", "Hynek Kydlíček", "Vinko Sabolčec", "Bettina Messmer", "Negar Foroutan", "Amir Hossein Kargaran", "Colin Raffel", "Martin Jaggi", "Leandro Von Werra", "Thomas Wolf"], "title": "FineWeb2: One Pipeline to Scale Them All -- Adapting Pre-Training Data Processing to Every Language", "comment": null, "summary": "Pre-training state-of-the-art large language models (LLMs) requires vast\namounts of clean and diverse text data. While the open development of large\nhigh-quality English pre-training datasets has seen substantial recent\nprogress, training performant multilingual LLMs remains a challenge, in large\npart due to the inherent difficulty of tailoring filtering and deduplication\npipelines to a large number of languages. In this work, we introduce a new\npre-training dataset curation pipeline based on FineWeb that can be\nautomatically adapted to support any language. We extensively ablate our\npipeline design choices on a set of nine diverse languages, guided by a set of\nmeaningful and informative evaluation tasks that were chosen through a novel\nselection process based on measurable criteria. Ultimately, we show that our\npipeline can be used to create non-English corpora that produce more performant\nmodels than prior datasets. We additionally introduce a straightforward and\nprincipled approach to rebalance datasets that takes into consideration both\nduplication count and quality, providing an additional performance uplift.\nFinally, we scale our pipeline to over 1000 languages using almost 100 Common\nCrawl snapshots to produce FineWeb2, a new 20 terabyte (5 billion document)\nmultilingual dataset which we release along with our pipeline, training, and\nevaluation codebases.", "AI": {"tldr": "提出一种新的自动适应多语言的预训练数据集策划管线，并创建了一个20TB的多语言数据集(FineWeb2)。", "motivation": "由于难以将过滤和重复数据删除管线调整到大量语言中，训练高性能的多语言LLM仍面临挑战。", "method": "介绍了一种新的基于FineWeb的预训练数据集策划管线，该管线可自动适应任何语言。同时提出了一种简单而严谨的方法来调和数据集，该方法考虑了重复度和质量。", "result": "该管线可以用来创建非英语语料库，其生成的模型比之前的数据集性能更佳。最终管线扩展到超过1000种语言，生成了一个新的20TB多语言数据集(FineWeb2)。", "conclusion": "证明了新的策划管线能够生成性能更好的多语言LLM。"}}
{"id": "2506.20795", "categories": ["cs.CV", "cs.HC", "cs.RO", "I.2.10; I.2.9; I.5.4; I.4.8; I.4.9; H.1.2"], "pdf": "https://arxiv.org/pdf/2506.20795", "abs": "https://arxiv.org/abs/2506.20795", "authors": ["Stephanie Käs", "Anton Burenko", "Louis Markert", "Onur Alp Culha", "Dennis Mack", "Timm Linder", "Bastian Leibe"], "title": "How do Foundation Models Compare to Skeleton-Based Approaches for Gesture Recognition in Human-Robot Interaction?", "comment": null, "summary": "Gestures enable non-verbal human-robot communication, especially in noisy\nenvironments like agile production. Traditional deep learning-based gesture\nrecognition relies on task-specific architectures using images, videos, or\nskeletal pose estimates as input. Meanwhile, Vision Foundation Models (VFMs)\nand Vision Language Models (VLMs) with their strong generalization abilities\noffer potential to reduce system complexity by replacing dedicated\ntask-specific modules. This study investigates adapting such models for\ndynamic, full-body gesture recognition, comparing V-JEPA (a state-of-the-art\nVFM), Gemini Flash 2.0 (a multimodal VLM), and HD-GCN (a top-performing\nskeleton-based approach). We introduce NUGGET, a dataset tailored for\nhuman-robot communication in intralogistics environments, to evaluate the\ndifferent gesture recognition approaches. In our experiments, HD-GCN achieves\nbest performance, but V-JEPA comes close with a simple, task-specific\nclassification head - thus paving a possible way towards reducing system\ncomplexity, by using it as a shared multi-task model. In contrast, Gemini\nstruggles to differentiate gestures based solely on textual descriptions in the\nzero-shot setting, highlighting the need of further research on suitable input\nrepresentations for gestures.", "AI": {"tldr": "研究比较了基于VFMs、VLMs和骨架的手势识别方法，结果表明HD-GCN表现最佳，但V-JEPA次之，并展示了减少系统复杂性潜力。", "motivation": "传统的基于深度学习的手势识别依赖于使用图像、视频或骨骼姿态估计作为输入的任务特定架构，而视觉基础模型和视觉语言模型强大的泛化能力能简化系统。", "method": "此研究探讨了视觉基础模型（VFMs）和视觉语言模型（VLMs）在动态全身手势识别中的应用，并将V-JEPA（一种先进的VFM）、Gemini Flash 2.0（一种多模式VLM）和HD-GCN（一种顶尖的基于骨架的方法）进行了比较。", "result": "在评估不同手势识别方法的实验中，使用NUGGET数据集，HD-GCN的性能最优，而V-JEPA紧随其后，并展现了作为共享多任务模型的潜力。Gemini在仅使用文本描述时难以区分手势，说明输入表征仍需进一步研究。", "conclusion": "HD-GCN在动态全身手势识别中表现出色，但V-JEPA也表现了简化系统复杂性的潜力，揭示了手势识别输入表征的进一步研究需求。"}}
{"id": "2506.20923", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.20923", "abs": "https://arxiv.org/abs/2506.20923", "authors": ["Xinping Zhao", "Xinshuo Hu", "Zifei Shan", "Shouzheng Huang", "Yao Zhou", "Zetian Sun", "Zhenyu Liu", "Dongfang Li", "Xinyuan Wei", "Qian Chen", "Youcheng Pan", "Yang Xiang", "Meishan Zhang", "Haofen Wang", "Jun Yu", "Baotian Hu", "Min Zhang"], "title": "KaLM-Embedding-V2: Superior Training Techniques and Data Inspire A Versatile Embedding Model", "comment": "Technical Report; 26 pages 12 tables 1 figure. arXiv admin note:\n  substantial text overlap with arXiv:2501.01028", "summary": "In this paper, we propose KaLM-Embedding-V2, a versatile and compact\nembedding model, which achieves impressive performance in general-purpose text\nembedding tasks by leveraging superior training techniques and data. Our key\ninnovations include: (1) To better align the architecture with representation\nlearning, we remove the causal attention mask and adopt a fully bidirectional\ntransformer with simple yet effective mean-pooling to produce fixed-length\nembeddings; (2) We employ a multi-stage training pipeline: (i) pre-training on\nlarge-scale weakly supervised open-source corpora; (ii) fine-tuning on\nhigh-quality retrieval and non-retrieval datasets; and (iii) model-soup\nparameter averaging for robust generalization. Besides, we introduce a\nfocal-style reweighting mechanism that concentrates learning on difficult\nsamples and an online hard-negative mixing strategy to continuously enrich hard\nnegatives without expensive offline mining; (3) We collect over 20 categories\nof data for pre-training and 100 categories of data for fine-tuning, to boost\nboth the performance and generalization of the embedding model. Extensive\nevaluations on the Massive Text Embedding Benchmark (MTEB) Chinese and English\nshow that our model significantly outperforms others of comparable size, and\ncompetes with 3x, 14x, 18x, and 26x larger embedding models, setting a new\nstandard for a versatile and compact embedding model with less than 1B\nparameters.", "AI": {"tldr": "本文提出了KaLM-Embedding-V2模型，该模型在各种文本嵌入任务中表现出色，特点是多功能和参数量少于10亿。", "motivation": "我们旨在创造一种多功能且紧凑的文本嵌入模型，它能在各种文本嵌入任务中保持高性能，同时使用较少的参数（小于10亿）。", "method": "我们提出了KaLM-Embedding-V2，一个多功能且紧凑的嵌入模型，通过利用先进的训练技术和数据在通用文本嵌入任务中表现出色。我们主要的创新包括：(1) 为了更好地使架构与表示学习保持一致，我们移除了因果注意力掩码，采用了一种全双向Transformer，并通过简单的均值池化来生成固定长度的嵌入；(2) 我们采用了多阶段训练流程：(i) 在大规模弱监督的开源语料库上进行预训练；(ii) 在高质量检索和非检索数据集上进行微调；(iii) 模型soup参数平均以增强鲁棒性。此外，我们引入了一种焦点样式的重新加权机制，集中学习困难的样本，并采用了在线难负样本混合策略，以持续丰富难负样本而不需昂贵的离线挖掘；(3) 我们收集了超过20类预训练数据和100类微调数据，以提升嵌入模型的性能和泛化能力。", "result": "在大规模文本嵌入基准（MTEB）的中文和英文评价中，我们的模型在与同等规模的模型相比时表现出显著优势，并与3倍、14倍、18倍和26倍大大小小的嵌入模型相比，取得了竞争甚至超越的效果。", "conclusion": "我们的创新方法和广泛的训练数据使得KaLM-Embedding-V2模型在通用文本嵌入任务中表现出色，尤其是在参数量较小的情况下。"}}
{"id": "2506.20832", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.20832", "abs": "https://arxiv.org/abs/2506.20832", "authors": ["Cansu Korkmaz", "Ahmet Murat Tekalp", "Zafer Dogan"], "title": "Leveraging Vision-Language Models to Select Trustworthy Super-Resolution Samples Generated by Diffusion Models", "comment": "14 pages, 9 figures, 5 tables, accepted to IEEE Transactions on\n  Circuits and Systems for Video Technology", "summary": "Super-resolution (SR) is an ill-posed inverse problem with many feasible\nsolutions consistent with a given low-resolution image. On one hand, regressive\nSR models aim to balance fidelity and perceptual quality to yield a single\nsolution, but this trade-off often introduces artifacts that create ambiguity\nin information-critical applications such as recognizing digits or letters. On\nthe other hand, diffusion models generate a diverse set of SR images, but\nselecting the most trustworthy solution from this set remains a challenge. This\npaper introduces a robust, automated framework for identifying the most\ntrustworthy SR sample from a diffusion-generated set by leveraging the semantic\nreasoning capabilities of vision-language models (VLMs). Specifically, VLMs\nsuch as BLIP-2, GPT-4o, and their variants are prompted with structured queries\nto assess semantic correctness, visual quality, and artifact presence. The\ntop-ranked SR candidates are then ensembled to yield a single trustworthy\noutput in a cost-effective manner. To rigorously assess the validity of\nVLM-selected samples, we propose a novel Trustworthiness Score (TWS) a hybrid\nmetric that quantifies SR reliability based on three complementary components:\nsemantic similarity via CLIP embeddings, structural integrity using SSIM on\nedge maps, and artifact sensitivity through multi-level wavelet decomposition.\nWe empirically show that TWS correlates strongly with human preference in both\nambiguous and natural images, and that VLM-guided selections consistently yield\nhigh TWS values. Compared to conventional metrics like PSNR, LPIPS, which fail\nto reflect information fidelity, our approach offers a principled, scalable,\nand generalizable solution for navigating the uncertainty of the diffusion SR\nspace. By aligning outputs with human expectations and semantic correctness,\nthis work sets a new benchmark for trustworthiness in generative SR.", "AI": {"tldr": "研究提出利用VLM评估语义正确性、视觉质量和伪影存在情况，从扩散模型生成的超分辨率图像中确定最值得信赖的画面，提出了一个新的可信度评分（TWS），展示了该方法与人类偏好有强相关性，并优于传统指标如PSNR和LPIPS。", "motivation": "由于回归SR模型引入了可能在信息关键应用中造成歧义的伪影，扩散模型虽然可以生成多样性丰富的SR图像，但是挑选出最值得信赖的解决方案仍然是一个挑战。该研究的动机就是提出一个自动化的框架解决这个问题。", "method": "该研究开发了一个利用视觉语言模型（VLM）的鲁棒、自动框架，以确定从扩散模型生成的一组超分辨率（SR）图像中最值得信赖的画面。具体来说，使用如BLIP-2、GPT-4o等VLM处理结构化查询，评估语义正确性、视觉质量及伪影存在情况。通过综合排名最高的SR候选图像，最终得到单一的值得信赖的输出，且成本效益高。", "result": "研究通过实验证明了TWS与人类偏好在含糊不清及自然图像中有强相关性，并证明了VLM引导的选择可以持续产生高的TWS值，从而表明该方法在超分辨率生成中的优越性及可信度。", "conclusion": "研究结果表明所提的可信度评分（TWS）在含糊不清和自然图像中与人类偏好有强相关性，并且VLM引导选择一直产生高的TWS值。该工作通过与人类期望及语义正确性对齐，为生成SR的可信度设定了新的基准。"}}
{"id": "2506.20989", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.20989", "abs": "https://arxiv.org/abs/2506.20989", "authors": ["Eric Zhang", "Leshem Choshen", "Jacob Andreas"], "title": "Can Gradient Descent Simulate Prompting?", "comment": "14 pages, 2 figures", "summary": "There are two primary ways of incorporating new information into a language\nmodel (LM): changing its prompt or changing its parameters, e.g. via\nfine-tuning. Parameter updates incur no long-term storage cost for model\nchanges. However, for many model updates, prompting is significantly more\neffective: prompted models can generalize robustly from single examples and\ndraw logical inferences that do not occur under standard fine-tuning. Can\nmodels be modified so that fine-tuning does emulate prompting? This paper\ndescribes a method for meta-training LMs such that gradient updates emulate the\neffects of conditioning on new information. Our approach uses tools from\ngradient-based meta-learning but uses an LM's own prompted predictions as\ntargets, eliminating the need for ground-truth labels. Subsequent gradient\ndescent training recovers some (and occasionally all) of prompted model\nperformance -- showing improvement on the ``reversal curse'' tasks, and\nanswering questions about text passages after a single gradient update. These\nresults suggest that, with appropriate initialization, gradient descent can be\nsurprisingly expressive. Our results suggest new avenues for long-context\nmodeling and offer insight into the generalization capabilities of\ngradient-based learning.", "AI": {"tldr": "研究提出了一种元训练语言模型的方法，使得梯度更新可以模仿提示的效果，展示了在适当初始化情况下，梯度下降的高效性。", "motivation": "研究动机在于探究是否可以通过调整模型，使得微调模仿提示的效果，解决模型更新时长期内存成本以及微调不如提示有效的问题。", "method": "本研究提出了一种元训练语言模型的方法，该方法利用基于梯度的元学习工具，但使用语言模型自身被提示后的预测作为目标，从而消除了对地面真实标签的需求。通过这种方式，后续的梯度下降训练可以在某种程度上恢复被提示模型的表现。", "result": "实验结果显示，适当初始化后，梯度下降具有较强的表达能力，甚至在单次梯度更新后，可以改善“反转诅咒”任务的表现，并回答关于文本段落的问题。", "conclusion": "研究提供了长上下文建模的新途径，并为基于梯度的学习概括能力提供了见解。"}}
{"id": "2506.20841", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.20841", "abs": "https://arxiv.org/abs/2506.20841", "authors": ["Ha Min Son", "Shahbaz Rezaei", "Xin Liu"], "title": "FixCLR: Negative-Class Contrastive Learning for Semi-Supervised Domain Generalization", "comment": null, "summary": "Semi-supervised domain generalization (SSDG) aims to solve the problem of\ngeneralizing to out-of-distribution data when only a few labels are available.\nDue to label scarcity, applying domain generalization methods often\nunderperform. Consequently, existing SSDG methods combine semi-supervised\nlearning methods with various regularization terms. However, these methods do\nnot explicitly regularize to learn domains invariant representations across all\ndomains, which is a key goal for domain generalization. To address this, we\nintroduce FixCLR. Inspired by success in self-supervised learning, we change\ntwo crucial components to adapt contrastive learning for explicit domain\ninvariance regularization: utilization of class information from pseudo-labels\nand using only a repelling term. FixCLR can also be added on top of most\nexisting SSDG and semi-supervised methods for complementary performance\nimprovements. Our research includes extensive experiments that have not been\npreviously explored in SSDG studies. These experiments include benchmarking\ndifferent improvements to semi-supervised methods, evaluating the performance\nof pretrained versus non-pretrained models, and testing on datasets with many\ndomains. Overall, FixCLR proves to be an effective SSDG method, especially when\ncombined with other semi-supervised methods.", "AI": {"tldr": "提出FixCLR方法，有效解决标签稀缺情况下的领域泛化问题，适用于多种SSDG和半监督方法的性能提升。", "motivation": "解决标签稀缺情况下泛化到分布外数据的问题，现有SSDG方法未能显式地正则化以学习跨所有领域的领域不变表示，这是领域泛化的关键目标。", "method": "FixCLR，通过借鉴自监督学习的成功经验，调整了对比学习的两个关键组成部分，以实现显式的领域不变正则化：利用伪标签中的类别信息以及仅使用排斥项。", "result": "进行了广泛实验，包括基准测试不同对半监督方法的改进、评估预训练模型与非预训练模型的性能以及测试具有多个领域的数据集。", "conclusion": "FixCLR证明是一种有效的SSDG方法，尤其当与其它半监督方法结合使用时。"}}
{"id": "2506.20993", "categories": ["cs.CL", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2506.20993", "abs": "https://arxiv.org/abs/2506.20993", "authors": ["Adithya Chittem", "Aishna Shrivastava", "Sai Tarun Pendela", "Jagat Sesh Challa", "Dhruv Kumar"], "title": "SAC: A Framework for Measuring and Inducing Personality Traits in LLMs with Dynamic Intensity Control", "comment": "Under review", "summary": "Large language models (LLMs) have gained significant traction across a wide\nrange of fields in recent years. There is also a growing expectation for them\nto display human-like personalities during interactions. To meet this\nexpectation, numerous studies have proposed methods for modelling LLM\npersonalities through psychometric evaluations. However, most existing models\nface two major limitations: they rely on the Big Five (OCEAN) framework, which\nonly provides coarse personality dimensions, and they lack mechanisms for\ncontrolling trait intensity. In this paper, we address this gap by extending\nthe Machine Personality Inventory (MPI), which originally used the Big Five\nmodel, to incorporate the 16 Personality Factor (16PF) model, allowing\nexpressive control over sixteen distinct traits. We also developed a structured\nframework known as Specific Attribute Control (SAC) for evaluating and\ndynamically inducing trait intensity in LLMs. Our method introduces\nadjective-based semantic anchoring to guide trait intensity expression and\nleverages behavioural questions across five intensity factors:\n\\textit{Frequency}, \\textit{Depth}, \\textit{Threshold}, \\textit{Effort}, and\n\\textit{Willingness}. Through experimentation, we find that modelling intensity\nas a continuous spectrum yields substantially more consistent and controllable\npersonality expression compared to binary trait toggling. Moreover, we observe\nthat changes in target trait intensity systematically influence closely related\ntraits in psychologically coherent directions, suggesting that LLMs internalize\nmulti-dimensional personality structures rather than treating traits in\nisolation. Our work opens new pathways for controlled and nuanced human-machine\ninteractions in domains such as healthcare, education, and interviewing\nprocesses, bringing us one step closer to truly human-like social machines.", "AI": {"tldr": "本研究通过采用16PF模型和开发SAC框架解决了LLMs性格建模的两大限制，实现了对16种不同性格特质的细腻控，并表明多维性格结构的存在，推动了人机交互的发展。", "motivation": "研究动机是解决现有LLM性格建模的两个主要限制：依赖提供粗略性格维度的五大性格模型（OCEAN框架），以及缺乏控制性格强度的机制。", "method": "本研究扩展了机器性格量表(MPI)，从原本使用的五大性格特质模型改用16PF模型，以实现对16种不同特质的表达控制。同时，开发了一个结构化的框架，称为特定属性控制(SAC)，用于评估和动态诱导LLM的性格强度。方法引入了基于形容词的语义锚定来指导性格强度的表达，并结合了五个强度因素（频率、深度、阈值、努力、意愿）的行为问题。", "result": "实验表明，将强度建模为连续谱比二进制特质切换能产生更为一致和可控制的性格表达。此外，目标特质强度的变化系统地影响了与之紧密相关的特质，心理上是连贯的，表明LLMs内化了多维性格结构，而不仅仅是孤立地处理特质。", "conclusion": "该研究为控制和细腻的人机交互开辟了新的途径，尤其是在医疗保健、教育和面试等领域，使我们更接近真正类人的社交机器。"}}
{"id": "2506.20850", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.20850", "abs": "https://arxiv.org/abs/2506.20850", "authors": ["Yuting He", "Shuo Li"], "title": "Vector Contrastive Learning For Pixel-Wise Pretraining In Medical Vision", "comment": "Accepted by ICCV 2025", "summary": "Contrastive learning (CL) has become a cornerstone of self-supervised\npretraining (SSP) in foundation models, however, extending CL to pixel-wise\nrepresentation, crucial for medical vision, remains an open problem. Standard\nCL formulates SSP as a binary optimization problem (binary CL) where the\nexcessive pursuit of feature dispersion leads to an over-dispersion problem,\nbreaking pixel-wise feature correlation thus disrupting the intra-class\ndistribution. Our vector CL reformulates CL as a vector regression problem,\nenabling dispersion quantification in pixel-wise pretraining via modeling\nfeature distances in regressing displacement vectors. To implement this novel\nparadigm, we propose the COntrast in VEctor Regression (COVER) framework. COVER\nestablishes an extendable vector-based self-learning, enforces a consistent\noptimization flow from vector regression to distance modeling, and leverages a\nvector pyramid architecture for granularity adaptation, thus preserving\npixel-wise feature correlations in SSP. Extensive experiments across 8 tasks,\nspanning 2 dimensions and 4 modalities, show that COVER significantly improves\npixel-wise SSP, advancing generalizable medical visual foundation models.", "AI": {"tldr": "我们提出了一种矢量对比学习方法，解决了像素级表征的分散问题，通过新的COntrast in VEctor Regression (COVER)框架，在多个医疗视觉任务中表现出了显著的改进。", "motivation": "对比学习已成为了基础模型自监督预训练的基石，但在医疗视觉中至关重要的像素级表征上，对比学习的延伸至今仍是一个未解决的问题。标准对比学习格式将自监督预训练当作一个二元优化问题，对特征分散的过度追求导致了像素级特征相关性的破坏。", "method": "我们的方法称为COntrast in VEctor Regression (COVER)框架，将对比学习重新定义为向量回归问题，通过回归位移向量来量化特征距离，从而保留像素级特征的相关性。COVER框架采用了可扩展的向量学习方式，一致的从向量回归到距离建模的优化流程，以及用于粒度适应的向量金字塔架构。", "result": "实验结果涵盖了8个任务，包括2个维度和4种模态，显示COVER显著提高了像素级的自监督预训练，推动了可泛化的医疗视觉基础模型的发展。", "conclusion": "COVER框架成功地解决了标准对比学习在像素级表征上的局限性，对于医疗图像中有意义的像素间相关性提供了更好的保存，从而提升了预训练模型的泛化能力。"}}
{"id": "2506.21031", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.21031", "abs": "https://arxiv.org/abs/2506.21031", "authors": ["Jatin Gupta", "Akhil Sharma", "Saransh Singhania", "Mohammad Adnan", "Sakshi Deo", "Ali Imam Abidi", "Keshav Gupta"], "title": "Large Language Models Acing Chartered Accountancy", "comment": "Accepted for publication at MoStart 2025: International Conference on\n  Digital Transformation in Education and Applications of Artificial\n  Intelligence, Bosnia and Herzegovina, 2025", "summary": "Advanced intelligent systems, particularly Large Language Models (LLMs), are\nsignificantly reshaping financial practices through advancements in Natural\nLanguage Processing (NLP). However, the extent to which these models\neffectively capture and apply domain-specific financial knowledge remains\nuncertain. Addressing a critical gap in the expansive Indian financial context,\nthis paper introduces CA-Ben, a Chartered Accountancy benchmark specifically\ndesigned to evaluate the financial, legal, and quantitative reasoning\ncapabilities of LLMs. CA-Ben comprises structured question-answer datasets\nderived from the rigorous examinations conducted by the Institute of Chartered\nAccountants of India (ICAI), spanning foundational, intermediate, and advanced\nCA curriculum stages. Six prominent LLMs i.e. GPT 4o, LLAMA 3.3 70B, LLAMA 3.1\n405B, MISTRAL Large, Claude 3.5 Sonnet, and Microsoft Phi 4 were evaluated\nusing standardized protocols. Results indicate variations in performance, with\nClaude 3.5 Sonnet and GPT-4o outperforming others, especially in conceptual and\nlegal reasoning. Notable challenges emerged in numerical computations and legal\ninterpretations. The findings emphasize the strengths and limitations of\ncurrent LLMs, suggesting future improvements through hybrid reasoning and\nretrieval-augmented generation methods, particularly for quantitative analysis\nand accurate legal interpretation.", "AI": {"tldr": "研究引入CA-Ben基准评估了六种主要LLMs在印度财务实践中的表现，凸显了其在法律推理方面的能力及遇到的挑战，建议未来进行方法改进。", "motivation": "该研究旨在解决在印度金融背景下，大型语言模型（LLMs）捕获和应用特定领域金融知识的有效性问题，填补这一领域的空白。", "method": "研究设计了CA-Ben基准测试，该测试基于印度注册会计师协会（ICAI）的严格考试，涵盖了注册会计师课程的基础、中级和高级阶段结构化问题-答案数据集。使用标准协议对六种主要LLMs进行了评估。", "result": "研究通过引入CA-Ben基准评估了六种大型语言模型（包括GPT 4o和Claude 3.5 Sonnet等）在印度财务实践中的表现。结果显示在概念和法律推理方面表现较好，但在数值计算和法律解释方面存在困难。这表明了模型的优势和局限，未来可以通过混合推理和检索增强生成方法进行改进。", "conclusion": "研究结果强调了当前LLMs在金融领域应用中的强项和局限性，为未来通过混合逻辑推理和检索增强生成举措的改进提供了一定方向，特别是针对定量分析和准确法律解读的提升。"}}
{"id": "2506.20867", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.20867", "abs": "https://arxiv.org/abs/2506.20867", "authors": ["Ryosuke Kawamura", "Hideaki Hayashi", "Shunsuke Otake", "Noriko Takemura", "Hajime Nagahara"], "title": "Enhancing Ambiguous Dynamic Facial Expression Recognition with Soft Label-based Data Augmentation", "comment": null, "summary": "Dynamic facial expression recognition (DFER) is a task that estimates\nemotions from facial expression video sequences. For practical applications,\naccurately recognizing ambiguous facial expressions -- frequently encountered\nin in-the-wild data -- is essential. In this study, we propose MIDAS, a data\naugmentation method designed to enhance DFER performance for ambiguous facial\nexpression data using soft labels representing probabilities of multiple\nemotion classes. MIDAS augments training data by convexly combining pairs of\nvideo frames and their corresponding emotion class labels. This approach\nextends mixup to soft-labeled video data, offering a simple yet highly\neffective method for handling ambiguity in DFER. To evaluate MIDAS, we\nconducted experiments on both the DFEW dataset and FERV39k-Plus, a newly\nconstructed dataset that assigns soft labels to an existing DFER dataset. The\nresults demonstrate that models trained with MIDAS-augmented data achieve\nsuperior performance compared to the state-of-the-art method trained on the\noriginal dataset.", "AI": {"tldr": "研究人员提出了一种名为MIDAS的数据增强方法，专门用于提升对模糊面部表情的动态面部表情识别性能，并在实验中验证了其有效性。", "motivation": "对于实际应用而言，准确识别野外数据中常见的模糊面部表情至关重要。提出MIDAS的目的是解决这一难题。", "method": "MIDAS,一种用于动态面部表情识别的数据增强方法，通过将视频帧及其对应的情绪类别标签进行凸组合来生成软标签，从而提高对模糊面部表情数据的识别能力。", "result": "与使用原始数据集训练的最先进的方法相比，使用MIDAS增强数据训练的模型在DFEW数据集和一个新构建的FERV39k-Plus数据集上表现出更好的性能。", "conclusion": "MIDAS方法通过扩展mixup到软标签视频数据，为解决动态面部表情识别中的模糊性提供了有效解决方案。实验结果表明其在处理模糊面部表情识别问题上的优越性。"}}
{"id": "2506.21049", "categories": ["cs.CL", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2506.21049", "abs": "https://arxiv.org/abs/2506.21049", "authors": ["Chunyuan Yuan", "Chong Zhang", "Zheng Fang", "Ming Pang", "Xue Jiang", "Changping Peng", "Zhangang Lin", "Ching Law"], "title": "A Semi-supervised Scalable Unified Framework for E-commerce Query Classification", "comment": "Accepted by ACL 2025", "summary": "Query classification, including multiple subtasks such as intent and category\nprediction, is vital to e-commerce applications. E-commerce queries are usually\nshort and lack context, and the information between labels cannot be used,\nresulting in insufficient prior information for modeling. Most existing\nindustrial query classification methods rely on users' posterior click behavior\nto construct training samples, resulting in a Matthew vicious cycle.\nFurthermore, the subtasks of query classification lack a unified framework,\nleading to low efficiency for algorithm optimization.\n  In this paper, we propose a novel Semi-supervised Scalable Unified Framework\n(SSUF), containing multiple enhanced modules to unify the query classification\ntasks. The knowledge-enhanced module uses world knowledge to enhance query\nrepresentations and solve the problem of insufficient query information. The\nlabel-enhanced module uses label semantics and semi-supervised signals to\nreduce the dependence on posterior labels. The structure-enhanced module\nenhances the label representation based on the complex label relations. Each\nmodule is highly pluggable, and input features can be added or removed as\nneeded according to each subtask. We conduct extensive offline and online A/B\nexperiments, and the results show that SSUF significantly outperforms the\nstate-of-the-art models.", "AI": {"tldr": "本文提出了一种新的半监督可扩展统一框架（SSUF），解决了电子商务查询分类中存在的信息不足、依赖后验标签及子任务缺乏统一框架的问题。实验显示，该框架优于现有先进模型。", "motivation": "电子商务查询通常较短且缺乏上下文，标签间的相关信息无法使用，导致建模时先验信息不足。现有的工业查询分类方法大多依赖用户的后置点击行为来构建训练样本，这会导致一个马太效应的恶性循环。此外，查询分类的子任务缺乏统一的框架，导致算法优化效率低下。因此，有必要提出一种新的解决方案来应对这些问题。", "method": "本文提出了一种名为半监督可扩展统一框架（SSUF）的新方法，该框架包含多个增强模块以统一查询分类任务。知识增强模块利用世界知识增强查询表示，解决查询信息不足的问题。标签增强模块利用标签语义和半监督信号减少对后验标签的依赖。结构增强模块依据复杂的标签关系增强标签表示。每个模块高度可插拔，可根据每个子任务的需求添加或移除输入特征。", "result": "作者通过广泛的线上线下A/B测试实验验证了SSUF的有效性，实验结果表明，SSUF显著优于现有的先进模型。", "conclusion": "本文提出的SSUF框架通过多个增强模块统一了查询分类任务，显著提高了模型性能，解决了现有方法中存在的问题。"}}
{"id": "2506.20877", "categories": ["cs.CV", "cs.AI", "I.4.8; I.2.10"], "pdf": "https://arxiv.org/pdf/2506.20877", "abs": "https://arxiv.org/abs/2506.20877", "authors": ["Calin Teodor Ioan"], "title": "THIRDEYE: Cue-Aware Monocular Depth Estimation via Brain-Inspired Multi-Stage Fusion", "comment": null, "summary": "Monocular depth estimation methods traditionally train deep models to infer\ndepth directly from RGB pixels. This implicit learning often overlooks explicit\nmonocular cues that the human visual system relies on, such as occlusion\nboundaries, shading, and perspective. Rather than expecting a network to\ndiscover these cues unaided, we present ThirdEye, a cue-aware pipeline that\ndeliberately supplies each cue through specialised, pre-trained, and frozen\nnetworks. These cues are fused in a three-stage cortical hierarchy (V1->V2->V3)\nequipped with a key-value working-memory module that weights them by\nreliability. An adaptive-bins transformer head then produces a high-resolution\ndisparity map. Because the cue experts are frozen, ThirdEye inherits large\namounts of external supervision while requiring only modest fine-tuning. This\nextended version provides additional architectural detail, neuroscientific\nmotivation, and an expanded experimental protocol; quantitative results will\nappear in a future revision.", "AI": {"tldr": "ThirdEye方法明确地识别并融合单目线索，来改进单目深度估计的准确性。", "motivation": "传统的单目深度估计方法仅仅依靠从RGB像素直接推断深度，而忽视了人眼视觉系统依赖的明确单目线索，如遮挡边界、阴影和透视。而ThirdEye的动机是通过专门的网络来提供这些线索，并通过一个权重可靠的皮层层次结构来融合它们。", "method": "ThirdEye方法通过专门的、预训练的和冻结的网络来提供每个线索，这些网络专门识别如遮挡边界、阴影和透视等单目线索。这些线索在三阶段皮层层次结构（V1->V2->V3）中被融合，该层次结构配备了一个根据可靠性加权线索的关键值工作记忆模块。最后，一个自适应bins的Transformer头部产生高分辨率的视差图。", "result": "由于线索专家网络是冻结的，ThirdEye继承了大量外部监督，仅需进行适度微调。详细的架构、神经科学技术动机及实验方案在扩展版本中有所提及，定量结果将在未来的修订中提供。", "conclusion": "ThirdEye通过专门的技巧来提高单目深度估计的准确性和可靠性，这是一种创新的方法。"}}
{"id": "2506.21053", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.21053", "abs": "https://arxiv.org/abs/2506.21053", "authors": ["Fuqiang Niu", "Genan Dai", "Yisha Lu", "Jiayu Liao", "Xiang Li", "Hu Huang", "Bowen Zhang"], "title": "MT2-CSD: A New Dataset and Multi-Semantic Knowledge Fusion Method for Conversational Stance Detection", "comment": null, "summary": "In the realm of contemporary social media, automatic stance detection is\npivotal for opinion mining, as it synthesizes and examines user perspectives on\ncontentious topics to uncover prevailing trends and sentiments. Traditional\nstance detection research often targets individual instances, thereby limiting\nits capacity to model multi-party discussions typical in real social media\nscenarios. This shortcoming largely stems from the scarcity of datasets that\nauthentically capture the dynamics of social media interactions, hindering\nadvancements in conversational stance detection. In this paper, we introduce\nMT2-CSD, a comprehensive dataset for multi-target, multi-turn conversational\nstance detection. To the best of our knowledge, MT2-CSD is the largest dataset\navailable for this purpose, comprising 24,457 annotated instances and\nexhibiting the greatest conversational depth, thereby presenting new challenges\nfor stance detection. To address these challenges, we propose the Large\nLanguage model enhanced Conversational Relational Attention Network (LLM-CRAN),\nwhich exploits the reasoning capabilities of LLMs to improve conversational\nunderstanding. We conduct extensive experiments to evaluate the efficacy of\nLLM-CRAN on the MT2-CSD dataset. The experimental results indicate that\nLLM-CRAN significantly outperforms strong baseline models in the task of\nconversational stance detection.", "AI": {"tldr": "The paper introduces MT2-CSD, a new dataset for multi-target, multi-turn conversational stance detection, and proposes LLM-CRAN, a model that enhances conversational understanding through reasoning capabilities of large language models.", "motivation": "The motivation is to address the limitations of existing datasets and models in capturing the complexities of multi-party discussions in social media for stance detection.", "method": "This paper proposes the Large Language model enhanced Conversational Relational Attention Network (LLM-CRAN), which uses the reasoning capabilities of large language models to improve understanding in multi-target, multi-turn conversations.", "result": "Experiments on the new dataset show that LLM-CRAN significantly outperforms strong baseline models.", "conclusion": "The introduction of MT2-CSD and the performance enhancements provided by LLM-CRAN represent significant advancements in the field of conversational stance detection."}}
{"id": "2506.20879", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.20879", "abs": "https://arxiv.org/abs/2506.20879", "authors": ["Shubhankar Borse", "Seokeon Choi", "Sunghyun Park", "Jeongho Kim", "Shreya Kadambi", "Risheek Garrepalli", "Sungrack Yun", "Munawar Hayat", "Fatih Porikli"], "title": "MultiHuman-Testbench: Benchmarking Image Generation for Multiple Humans", "comment": null, "summary": "Generation of images containing multiple humans, performing complex actions,\nwhile preserving their facial identities, is a significant challenge. A major\nfactor contributing to this is the lack of a a dedicated benchmark. To address\nthis, we introduce MultiHuman-Testbench, a novel benchmark for rigorously\nevaluating generative models for multi-human generation. The benchmark\ncomprises 1800 samples, including carefully curated text prompts, describing a\nrange of simple to complex human actions. These prompts are matched with a\ntotal of 5,550 unique human face images, sampled uniformly to ensure diversity\nacross age, ethnic background, and gender. Alongside captions, we provide\nhuman-selected pose conditioning images which accurately match the prompt. We\npropose a multi-faceted evaluation suite employing four key metrics to quantify\nface count, ID similarity, prompt alignment, and action detection. We conduct a\nthorough evaluation of a diverse set of models, including zero-shot approaches\nand training-based methods, with and without regional priors. We also propose\nnovel techniques to incorporate image and region isolation using human\nsegmentation and Hungarian matching, significantly improving ID similarity. Our\nproposed benchmark and key findings provide valuable insights and a\nstandardized tool for advancing research in multi-human image generation.", "AI": {"tldr": "研究提出了一个名为MultiHuman-Testbench的新基准，用于评估多人体生成模型，包含1800个样本和5550个面部图像，通过四个关键指标进行多方面评估，研究认为这些样本和关键发现为多人体图像生成研究提供了重要见解和标准化工具。", "motivation": "生成多个人体、执行复杂动作同时保留面部身份的图像是一项重大挑战，而缺乏专门的基准测试是这一挑战的关键因素。为此，研究提出了MultiHuman-Testbench。", "method": "该研究提出了MultiHuman-Testbench，这是一个新的基准测试，用于评估多人体图像生成的生成模型。基准测试包含1800个样本，包括精心策划的文本提示，描述从简单到复杂的动作，并且匹配有5550个独特的面部图像。基准测试还提供人类选择的姿势条件图像，并使用四个关键指标来衡量人脸数量、身份相似度、文本提示一致性以及动作检测。", "result": "研究对多种模型进行了全面评估，包括零样本方法和训练方法，有和没有区域先验。研究还提出了一种新型技术，利用人体分割和匈牙利匹配来进行图像和区域隔离，显著提高了身份的相似度。", "conclusion": "研究提供的基准和关键发现为多人体图像生成领域的发展提供了重要的标准工具和见解。"}}
{"id": "2506.21096", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.21096", "abs": "https://arxiv.org/abs/2506.21096", "authors": ["Kang He", "Yuzhe Ding. Haining Wang", "Fei Li", "Chong Teng", "Donghong Ji"], "title": "DALR: Dual-level Alignment Learning for Multimodal Sentence Representation Learning", "comment": "Accepted by ACL 2025 Findings", "summary": "Previous multimodal sentence representation learning methods have achieved\nimpressive performance. However, most approaches focus on aligning images and\ntext at a coarse level, facing two critical challenges:cross-modal misalignment\nbias and intra-modal semantic divergence, which significantly degrade sentence\nrepresentation quality. To address these challenges, we propose DALR\n(Dual-level Alignment Learning for Multimodal Sentence Representation). For\ncross-modal alignment, we propose a consistency learning module that softens\nnegative samples and utilizes semantic similarity from an auxiliary task to\nachieve fine-grained cross-modal alignment. Additionally, we contend that\nsentence relationships go beyond binary positive-negative labels, exhibiting a\nmore intricate ranking structure. To better capture these relationships and\nenhance representation quality, we integrate ranking distillation with global\nintra-modal alignment learning. Comprehensive experiments on semantic textual\nsimilarity (STS) and transfer (TR) tasks validate the effectiveness of our\napproach, consistently demonstrating its superiority over state-of-the-art\nbaselines.", "AI": {"tldr": "The paper introduces DALR, a dual-level alignment learning method for multimodal sentence representation which addresses cross-modal misalignment and intra-modal semantic divergence, demonstrating superior performance across evaluation tasks compared to existing methods.", "motivation": "There is a need to improve the quality of multimodal sentence representations by addressing the issues of cross-modal misalignment bias and intra-modal semantic divergence that current methods struggle with.", "method": "Previous methods focus on coarse alignment between images and text, which often leads to significant degradation in sentence representation due to cross-modal misalignment bias and intra-modal semantic divergence. DALR addresses these issues through a consistency learning module and a ranking distillation mechanism, enhancing the quality of multimodal sentence representations.", "result": "Experiments on STS and transfer tasks prove the effectiveness of DALR, showing its consistent superiority over state-of-the-art approaches.", "conclusion": "The proposed DALR method significantly improves the quality of multimodal sentence representations by addressing cross-modal misalignment and intra-modal semantic divergence, outperforming existing methods across various evaluation tasks."}}
{"id": "2506.20900", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.20900", "abs": "https://arxiv.org/abs/2506.20900", "authors": ["Sherlon Almeida da Silva", "Davi Geiger", "Luiz Velho", "Moacir Antonelli Ponti"], "title": "The Role of Cyclopean-Eye in Stereo Vision", "comment": "arXiv admin note: text overlap with arXiv:2502.21280", "summary": "This work investigates the geometric foundations of modern stereo vision\nsystems, with a focus on how 3D structure and human-inspired perception\ncontribute to accurate depth reconstruction. We revisit the Cyclopean Eye model\nand propose novel geometric constraints that account for occlusions and depth\ndiscontinuities. Our analysis includes the evaluation of stereo feature\nmatching quality derived from deep learning models, as well as the role of\nattention mechanisms in recovering meaningful 3D surfaces. Through both\ntheoretical insights and empirical studies on real datasets, we demonstrate\nthat combining strong geometric priors with learned features provides internal\nabstractions for understanding stereo vision systems.", "AI": {"tldr": "本文探讨了现代立体视觉系统中的几何基础，提出新的几何约束处理遮挡和深度不连续性，通过理论分析和实际数据集实验验证结合几何先验和学习特征的方法能够提供理解立体视觉系统的内部抽象。", "motivation": "研究的动机在于理解3D结构和人类启发的感知如何有助于提高深度重建的准确性，并通过几何约束和深度学习结合的方式提供内部抽象用于理解立体视觉系统。", "method": "本文通过重新审视Cyclopean Eye模型并提出新的几何约束条件来处理遮挡和深度不连续性问题，结合深度学习模型获得的立体特征匹配质量和注意力机制在恢复有意义的3D表面中的作用，对现代立体视觉系统的几何基础进行了深入探讨。", "result": "研究结果表明，通过理论分析和现实数据集上的实证研究表明，结合强大的几何先验知识与学习到的特征可为理解立体视觉系统提供内部抽象，从而改善深度重建的准确性。", "conclusion": "结合强几何先验与学习特征的方法为理解立体视觉系统提供了内部抽象，并证明了这种方法在3D结构感知和深度重建中的有效性。"}}
{"id": "2506.21098", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.21098", "abs": "https://arxiv.org/abs/2506.21098", "authors": ["Qinwen Chen", "Wenbiao Tao", "Zhiwei Zhu", "Mingfan Xi", "Liangzhong Guo", "Yuan Wang", "Wei Wang", "Yunshi Lan"], "title": "ComRAG: Retrieval-Augmented Generation with Dynamic Vector Stores for Real-time Community Question Answering in Industry", "comment": "7 pages, 4 figures. Accepted at ACL 2025 Industry Track", "summary": "Community Question Answering (CQA) platforms can be deemed as important\nknowledge bases in community, but effectively leveraging historical\ninteractions and domain knowledge in real-time remains a challenge. Existing\nmethods often underutilize external knowledge, fail to incorporate dynamic\nhistorical QA context, or lack memory mechanisms suited for industrial\ndeployment. We propose ComRAG, a retrieval-augmented generation framework for\nreal-time industrial CQA that integrates static knowledge with dynamic\nhistorical QA pairs via a centroid-based memory mechanism designed for\nretrieval, generation, and efficient storage. Evaluated on three industrial CQA\ndatasets, ComRAG consistently outperforms all baselines--achieving up to 25.9%\nimprovement in vector similarity, reducing latency by 8.7% to 23.3%, and\nlowering chunk growth from 20.23% to 2.06% over iterations.", "AI": {"tldr": "ComRAG is an effective retrieval-augmented generation model for CQA platforms, incorporating historical and external knowledge with memory mechanisms for efficient performance.", "motivation": "The motivation is to overcome the limitations of current methods by effectively integrating external knowledge and historical QA interactions in a real-time industrial CQA context.", "method": "The paper introduces ComRAG, a retrieval-augmented generation framework for CQA that merges static domain knowledge with dynamic historical QA pairs using a centroid-based memory mechanism, tailored for retrieval, generation, and storage efficiency.", "result": "ComRAG demonstrates significant improvements in vector similarity, latency reduction, and efficient memory usage compared to baselines.", "conclusion": "ComRAG outperforms existing methods by improving vector similarity up to 25.9%, decreasing latency by 8.7% to 23.3%, and reducing chunk growth from 20.23% to 2.06% across three industrial CQA datasets."}}
{"id": "2506.20911", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.20911", "abs": "https://arxiv.org/abs/2506.20911", "authors": ["Advait Gupta", "Rishie Raj", "Dang Nguyen", "Tianyi Zhou"], "title": "FaSTA$^*$: Fast-Slow Toolpath Agent with Subroutine Mining for Efficient Multi-turn Image Editing", "comment": null, "summary": "We develop a cost-efficient neurosymbolic agent to address challenging\nmulti-turn image editing tasks such as \"Detect the bench in the image while\nrecoloring it to pink. Also, remove the cat for a clearer view and recolor the\nwall to yellow.'' It combines the fast, high-level subtask planning by large\nlanguage models (LLMs) with the slow, accurate, tool-use, and local A$^*$\nsearch per subtask to find a cost-efficient toolpath -- a sequence of calls to\nAI tools. To save the cost of A$^*$ on similar subtasks, we perform inductive\nreasoning on previously successful toolpaths via LLMs to continuously\nextract/refine frequently used subroutines and reuse them as new tools for\nfuture tasks in an adaptive fast-slow planning, where the higher-level\nsubroutines are explored first, and only when they fail, the low-level A$^*$\nsearch is activated. The reusable symbolic subroutines considerably save\nexploration cost on the same types of subtasks applied to similar images,\nyielding a human-like fast-slow toolpath agent \"FaSTA$^*$'': fast subtask\nplanning followed by rule-based subroutine selection per subtask is attempted\nby LLMs at first, which is expected to cover most tasks, while slow A$^*$\nsearch is only triggered for novel and challenging subtasks. By comparing with\nrecent image editing approaches, we demonstrate FaSTA$^*$ is significantly more\ncomputationally efficient while remaining competitive with the state-of-the-art\nbaseline in terms of success rate.", "AI": {"tldr": "提出了一种名为FaSTA$^*$的高效神经符号图像编辑代理，能够在处理多轮图像编辑任务时节省成本，同时保持与现有先进方法相当的成功率。", "motivation": "目的是提出一种可以高效解决复杂多轮次图像编辑任务的智能代理系统，同时保持较高的成功率，通过结合大语言模型的快速规划和局部搜索的精确执行来最小化成本。", "method": "该方法首次使用高阶的LLMs进行任务的快速规划，随后用A$^*$算法进行工具路径的详细规划。并且创新性利用LLMs分析过去的执行路径，学习可重用的子程序，以减少未来的搜索成本，构建了一个能够快速思考和慢速精确执行的代理系统FaSTA$^*$。", "result": "该研究开发了一种成本效益高的神经符号代理，以解决多轮图像编辑任务，此类任务涉及复杂的图像操作。高阶任务规划由大语言模型（LLMs）执行，而具体任务规划由局部A$^*$搜索完成。为了节省相似任务上的A$^*$计算成本，研究通过LLMs提取之前成功的操作路径，不断抽取和提炼出可重复利用的子程序，使其作为新的工具用于未来任务。该代理能够快速处理常见的子任务，仅在遇到新颖或具有挑战性的任务时才激活慢速的A$^*$搜索，因此在计算效率方面显著优于现有方法，同时在成功率上与最先进方法保持竞争力。", "conclusion": "该研究设计的FaSTA$^*$代理以经济的成本实现了复杂图像编辑任务的处理，可以快速规划一般任务并节省搜索时间，同时也能够处理新的具有挑战性的案例。实验证明，这种方法在成功率达到最先进水平的同时，提高了总体的计算效率。"}}
{"id": "2506.21119", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.21119", "abs": "https://arxiv.org/abs/2506.21119", "authors": ["Xiaoshuang Ji", "Zhendong Zhao", "Xiaojun Chen", "Xin Zhao", "Zeyao Liu"], "title": "Progtuning: Progressive Fine-tuning Framework for Transformer-based Language Models", "comment": "Accepted by ICONIP 2024", "summary": "Fine-tuning is a promising technique for leveraging Transformer-based\nlanguage models in downstream tasks. As model sizes continue to grow, updating\nall model parameters becomes increasingly costly. Parameter-efficient\nfine-tuning methods effectively address this issue by selectively updating a\nsmall subset of parameters. However, fine-tuning and most existing\nparameter-efficient fine-tuning methods require updating the same number of\nparameters as the initial size, ignoring the unequal contribution across\nTransformer blocks and leading to extremely inefficient allocation of computing\nresources. In this paper, we propose Progtuning, the novel fine-tuning\nframework combined with progressive learning for Transformer-based language\nmodels. Specifically, Progtuning progressively reduces the number of updated\ntransformer blocks based on the contribution. Remarkably, Progtuning optimizes\nresource allocation and reduces the number of updated parameters by\napproximately 25\\%, while still maintaining competitive performance. And it\nalso exhibits high adaptability with parameter-efficient fine-tuning methods,\ndemonstrating excellent performance across various adaptation scenarios.", "AI": {"tldr": "论文提出了Progtuning微调框架，该框架结合渐进学习，通过根据贡献逐步减少更新的Transformer块数量，减少了计算资源的消耗。", "motivation": "现有的微调方法及高效的参数微调方法忽视了Transformer块之间贡献的不平等，导致计算资源分配极不高效。", "method": "提出了Progtuning框架，结合渐进学习和Transformer模型微调，通过根据贡献逐步减少更新的Transformer块数量来优化资源分配。", "result": "Progtuning框架将更新参数的数量减少了大约25%，同时保持了具有竞争力的性能，并且在各种适配场景下都表现出色。", "conclusion": "Progtuning不仅优化了资源分配，减少了更新参数的数量，还能兼容高效的参数微调方法，具有很强的适应性。"}}
{"id": "2506.20922", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.20922", "abs": "https://arxiv.org/abs/2506.20922", "authors": ["Ju-Hyeon Nam", "Dong-Hyun Moon", "Sang-Chul Lee"], "title": "M2SFormer: Multi-Spectral and Multi-Scale Attention with Edge-Aware Difficulty Guidance for Image Forgery Localization", "comment": "Accepted in International Conference on Computer Vision (ICCV) 2025", "summary": "Image editing techniques have rapidly advanced, facilitating both innovative\nuse cases and malicious manipulation of digital images. Deep learning-based\nmethods have recently achieved high accuracy in pixel-level forgery\nlocalization, yet they frequently struggle with computational overhead and\nlimited representation power, particularly for subtle or complex tampering. In\nthis paper, we propose M2SFormer, a novel Transformer encoder-based framework\ndesigned to overcome these challenges. Unlike approaches that process spatial\nand frequency cues separately, M2SFormer unifies multi-frequency and\nmulti-scale attentions in the skip connection, harnessing global context to\nbetter capture diverse forgery artifacts. Additionally, our framework addresses\nthe loss of fine detail during upsampling by utilizing a global prior map, a\ncurvature metric indicating the difficulty of forgery localization, which then\nguides a difficulty-guided attention module to preserve subtle manipulations\nmore effectively. Extensive experiments on multiple benchmark datasets\ndemonstrate that M2SFormer outperforms existing state-of-the-art models,\noffering superior generalization in detecting and localizing forgeries across\nunseen domains.", "AI": {"tldr": "本文提出了M2SFormer框架，这是一种改进的伪造检测方法，它在跳跃连接中统一处理多频率和多尺度信息，并通过全局先验图和难度引导的注意力模块来提高细微操作的检测精度和泛化能力。", "motivation": "随着图像编辑技术的快速发展，在创新应用和恶意篡改数字图像方面都面临的新的挑战。尽管基于深度学习的方法在像素级伪造定位方面取得了高精度，但它们经常受到计算开销大和表征能力有限的影响。这项工作旨在解决这些问题，并提供更好的伪造图像检测方法。", "method": "本论文提出了M2SFormer，这是一种基于Transformer编码器的框架，旨在解决现有方法在计算开销和表征能力上的局限性。M2SFormer通过在跳跃连接中统一多频率和多尺度注意力，利用全局上下文来更好地捕捉各种伪造痕迹。此外，它采用了一个全局先验图和一个指示伪造定位难度的曲率度量，用于指导难度引导的注意力模块，从而更有效地保持对细微操作的检测能力。", "result": "实验结果表明，M2SFormer在多个基准数据集上都超越了现有的最先进模型，在检测和定位伪造图像方面表现出了更强的泛化能力。", "conclusion": "M2SFormer通过创新的结构和注意力机制，能够有效地处理复杂且细微的图像篡改，展现出在检测伪造图像方面的优越性能。"}}
{"id": "2506.21170", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.21170", "abs": "https://arxiv.org/abs/2506.21170", "authors": ["Viacheslav Meshchaninov", "Egor Chimbulatov", "Alexander Shabalin", "Aleksandr Abramov", "Dmitry Vetrov"], "title": "Compressed and Smooth Latent Space for Text Diffusion Modeling", "comment": null, "summary": "Autoregressive language models dominate modern text generation, yet their\nsequential nature introduces fundamental limitations: decoding is slow, and\nmaintaining global coherence remains challenging. Diffusion models offer a\npromising alternative by enabling parallel generation and flexible control;\nhowever, their application to text generation is hindered by the high\ndimensionality of token-level representations. We introduce Cosmos, a novel\napproach to text generation that operates entirely in a compressed, smooth\nlatent space tailored specifically for diffusion. This space is learned using\nan autoencoder trained simultaneously for token-level reconstruction and\nalignment with frozen activations from a pretrained language encoder, providing\nrobust semantic grounding and enabling effective perturbation-based\naugmentations. Empirically, we demonstrate that text representations can be\ncompressed by $8\\times$ while maintaining generation quality comparable to\ntoken-level diffusion models. Furthermore, increasing the latent sequence\nlength allows Cosmos to surpass both diffusion-based and autoregressive\nbaselines. We evaluate Cosmos on four diverse generative tasks including story\ngeneration, question generation, summarization, and detoxification and compare\nit with various generative paradigms. Cosmos achieves comparable or superior\ngeneration quality while offering more than $2\\times$ faster inference.", "AI": {"tldr": "本文提出了一种全新方法Cosmos，用于文本生成，这种方法通过学习一个强大的压缩平滑潜在空间来进行操作，实验证明了其在文本表示压缩和生成质量上的优越性，同时加快了生成速度。", "motivation": "自回归语言模型在现代文本生成中占据主导地位，但它们的顺序特性带来了根本性的限制：解码速度慢，维持全局一致性具有挑战性。扩散模型提供了一种有前途的替代方案，但它们在文本生成中的应用受制于令牌级表示的高维度。", "method": "我们引入了Cosmos，这是一种全新的文本生成方法，它完全在专门为扩散模型设计的压缩平滑潜在空间中操作。这个空间通过一个同时训练令牌级重建和对齐预训练语言编码器冻结激活的自编码器来学习，这提供了有力的语义定位，并使基于扰动的增强成为可能。", "result": "实验证明，文本表示可以压缩8倍，同时保持与令牌级扩散模型相当的生成质量。此外，增加潜在序列长度使Cosmos能够超越扩散式和自回归基线。在四个多样化的生成任务上（包括故事生成、问题生成、总结和解毒）评估了Cosmos，并将其与各种生成范式进行了比较。Cosmos在生成质量方面达到了相当或更高的水平，并且推理速度提高了2倍以上。", "conclusion": "相较于自回归模型和扩散模型，Cosmos在文本生成质量上达到或超过了现有模型，并且在推理效率上提高了至少一倍，这使得它成为一种更为高效的文本生成工具。"}}
{"id": "2506.20936", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.20936", "abs": "https://arxiv.org/abs/2506.20936", "authors": ["Hao Zhang", "Haolan Xu", "Chun Feng", "Varun Jampani", "Narendra Ahuja"], "title": "PhysRig: Differentiable Physics-Based Skinning and Rigging Framework for Realistic Articulated Object Modeling", "comment": "Accepted by ICCV 2025", "summary": "Skinning and rigging are fundamental components in animation, articulated\nobject reconstruction, motion transfer, and 4D generation. Existing approaches\npredominantly rely on Linear Blend Skinning (LBS), due to its simplicity and\ndifferentiability. However, LBS introduces artifacts such as volume loss and\nunnatural deformations, and it fails to model elastic materials like soft\ntissues, fur, and flexible appendages (e.g., elephant trunks, ears, and fatty\ntissues). In this work, we propose PhysRig: a differentiable physics-based\nskinning and rigging framework that overcomes these limitations by embedding\nthe rigid skeleton into a volumetric representation (e.g., a tetrahedral mesh),\nwhich is simulated as a deformable soft-body structure driven by the animated\nskeleton. Our method leverages continuum mechanics and discretizes the object\nas particles embedded in an Eulerian background grid to ensure\ndifferentiability with respect to both material properties and skeletal motion.\nAdditionally, we introduce material prototypes, significantly reducing the\nlearning space while maintaining high expressiveness. To evaluate our\nframework, we construct a comprehensive synthetic dataset using meshes from\nObjaverse, The Amazing Animals Zoo, and MixaMo, covering diverse object\ncategories and motion patterns. Our method consistently outperforms traditional\nLBS-based approaches, generating more realistic and physically plausible\nresults. Furthermore, we demonstrate the applicability of our framework in the\npose transfer task highlighting its versatility for articulated object\nmodeling.", "AI": {"tldr": "This paper introduces PhysRig, a new differentiable physics-based skinning method that overcomes the limitations of traditional LBS by embedding a rigid skeleton in a volumetric deformation model. This approach achieves more realistic elasticity and deformation for soft tissues, fur, and flexible appendages.", "motivation": "The motivation for this work is to address the limitations of Linear Blend Skinning (LBS), such as volume loss and unnatural deformations, especially for elastic materials like soft tissues, fur, and flexible appendages. The authors aim to provide a more realistic and physically plausible method for skinning and rigging in animation, articulated object reconstruction, motion transfer, and 4D generation.", "method": "The paper proposes PhysRig, a physics-based skinning and rigging framework that uses a volumetric representation of the rigid skeleton to simulate deformable soft-body structures. This approach leverages continuum mechanics and discretizes objects as particles embedded in an Eulerian grid for differentiability with respect to material properties and skeletal motion.", "result": "The results of the paper demonstrate that the proposed PhysRig method consistently outperforms LBS-based approaches in generating realistically deformable soft-body structures for a variety of objects and motion patterns.", "conclusion": "Concluding, the paper establishes PhysRig as an effective and versatile framework for skinning and rigging, capable of producing physically plausible results that surpass traditional methods, especially for soft and complex materials. The method also shows promise in pose transfer applications."}}
{"id": "2506.21182", "categories": ["cs.CL", "cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2506.21182", "abs": "https://arxiv.org/abs/2506.21182", "authors": ["Isaac Chung", "Imene Kerboua", "Marton Kardos", "Roman Solomatin", "Kenneth Enevoldsen"], "title": "Maintaining MTEB: Towards Long Term Usability and Reproducibility of Embedding Benchmarks", "comment": null, "summary": "The Massive Text Embedding Benchmark (MTEB) has become a standard evaluation\nplatform for text embedding models. While previous work has established the\ncore benchmark methodology, this paper focuses on the engineering aspects that\nensure MTEB's continued reproducibility and extensibility. We present our\napproach to maintaining robust continuous integration pipelines that validate\ndataset integrity, automate test execution, and assess benchmark results'\ngeneralizability. We detail the design choices that collectively enhance\nreproducibility and usability. Furthermore, we discuss our strategies for\nhandling community contributions and extending the benchmark with new tasks and\ndatasets. These engineering practices have been instrumental in scaling MTEB to\nbecome more comprehensive while maintaining quality and, ultimately, relevance\nto the field. Our experiences offer valuable insights for benchmark maintainers\nfacing similar challenges in ensuring reproducibility and usability in machine\nlearning evaluation frameworks. The MTEB repository is available at:\nhttps://github.com/embeddings-benchmark/mteb", "AI": {"tldr": "The paper covers the engineering strategies to uphold the quality, generalizability, and usability of the Massive Text Embedding Benchmark (MTEB) for assessing text embedding models.", "motivation": "The aim is to ensure the MTEB benchmark's effectiveness and relevance by employing advanced engineering practices that guarantee reproducibility and allow for ease of use and future expansion.", "method": "Structure", "result": "{\n  \"tldr\": \"The paper discusses the engineering efforts to maintain the Massive Text Embedding Benchmark (MTEB) for reproducibility and extensibility, including continuous integration pipelines and community contributions.\",\n  \"motivation\": \"The goal is to ensure that MTEB remains a high-quality, comprehensive, and relevant benchmark for evaluating text embedding models by enhancing reproducibility and usability.\",\n  \"method\": \"The authors describe the use of continuous integration pipelines to validate dataset integrity, automate tests, and evaluate benchmark performance. They also detail management of community contributions for extending the benchmark.\",\n  \"result\": \"MTEB has been effectively maintained and expanded with robust engineering practices, improving its generalizability and relevance in the field of text embedding evaluation.\",\n  \"conclusion\": \"The engineering practices outlined in the paper provide valuable lessons for maintaining and improving the reproducibility and usability of machine learning evaluation frameworks like MTEB.\")", "conclusion": "The strategies employed in maintaining MTEB highlight lessons for others involved in the upkeep of machine learning benchmarks."}}
{"id": "2506.20939", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.20939", "abs": "https://arxiv.org/abs/2506.20939", "authors": ["Chad Mourning", "Zhewei Wang", "Justin Murray"], "title": "AIR-VIEW: The Aviation Image Repository for Visibility Estimation of Weather, A Dataset and Benchmark", "comment": "5 pages, meant as citation for dataset", "summary": "Machine Learning for aviation weather is a growing area of research for\nproviding low-cost alternatives for traditional, expensive weather sensors;\nhowever, in the area of atmospheric visibility estimation, publicly available\ndatasets, tagged with visibility estimates, of distances relevant for aviation,\nof diverse locations, of sufficient size for use in supervised learning, are\nabsent. This paper introduces a new dataset which represents the culmination of\na year-long data collection campaign of images from the FAA weather camera\nnetwork suitable for this purpose. We also present a benchmark when applying\nthree commonly used approaches and a general-purpose baseline when trained and\ntested on three publicly available datasets, in addition to our own, when\ncompared against a recently ratified ASTM standard.", "AI": {"tldr": "本文通过一个新数据集评估了几种方法在航空范围内的大气能见度估计中的性能。", "motivation": "由于缺乏适用于航空距离的大规模、多样化位置的能见度估计数据集，本文旨在填补这一空白并提供一个适合监督学习的数据集，从而推动低成本航空气象研究的发展。", "method": "本文介绍了从FAA天气摄像头网络收集的图像数据集，并通过将其与最近批准的ASTM标准进行比较来评估三种常用方法和一个通用基准线的性能。", "result": "与现有的公开数据集相比，该数据集在大气能见度估计的应用中提供了新的基准测试。", "conclusion": "该数据集及其实验结果为下一步的研究和发展提供了有价值的参考。"}}
{"id": "2506.21191", "categories": ["cs.CL", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.21191", "abs": "https://arxiv.org/abs/2506.21191", "authors": ["Koji Inoue", "Mikey Elmers", "Yahui Fu", "Zi Haur Pang", "Divesh Lala", "Keiko Ochi", "Tatsuya Kawahara"], "title": "Prompt-Guided Turn-Taking Prediction", "comment": "This paper has been accepted for presentation at SIGdial Meeting on\n  Discourse and Dialogue 2025 (SIGDIAL 2025) and represents the author's\n  version of the work", "summary": "Turn-taking prediction models are essential components in spoken dialogue\nsystems and conversational robots. Recent approaches leverage transformer-based\narchitectures to predict speech activity continuously and in real-time. In this\nstudy, we propose a novel model that enables turn-taking prediction to be\ndynamically controlled via textual prompts. This approach allows intuitive and\nexplicit control through instructions such as \"faster\" or \"calmer\" adapting\ndynamically to conversational partners and contexts. The proposed model builds\nupon a transformer-based voice activity projection (VAP) model, incorporating\ntextual prompt embeddings into both channel-wise transformers and a\ncross-channel transformer. We evaluated the feasibility of our approach using\nover 950 hours of human-human spoken dialogue data. Since textual prompt data\nfor the proposed approach was not available in existing datasets, we utilized a\nlarge language model (LLM) to generate synthetic prompt sentences. Experimental\nresults demonstrated that the proposed model improved prediction accuracy and\neffectively varied turn-taking timing behaviors according to the textual\nprompts.", "AI": {"tldr": "本研究提出了一种可以根据文本提示动态调整换位预测的新模型，该模型基于变压器式的语音活动投影模型并加入了文本提示嵌入，实验表明其提高了预测精度并实现了换位时间的行为变化。", "motivation": "换位预测模型是口语对话系统和会话机器人的重要组成部分。最近的方法利用变压器式的架构来连续进行实时的语音活动预测。这项研究的动机是为了通过直观明确的指令控制来改进换位预测。", "method": "本研究提出了一种新的模型，它可以通过文本提示来动态控制换位预测。该模型基于变压器式的语音活动投影（VAP）模型，并将文本提示嵌入到通道式变压器和跨通道变压器中。", "result": "实验结果表明，该模型提高了预测准确性，有效地根据文本提示变化换位时间行为。", "conclusion": "研究表明，通过提出的新模型，可以通过文本提示动态调整换位预测，增强了模型的可控制性和适应性。"}}
{"id": "2506.20947", "categories": ["cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2506.20947", "abs": "https://arxiv.org/abs/2506.20947", "authors": ["Dejie Yang", "Zhu Xu", "Xinjie Gao", "Yang Liu"], "title": "Hierarchical Sub-action Tree for Continuous Sign Language Recognition", "comment": null, "summary": "Continuous sign language recognition (CSLR) aims to transcribe untrimmed\nvideos into glosses, which are typically textual words. Recent studies indicate\nthat the lack of large datasets and precise annotations has become a bottleneck\nfor CSLR due to insufficient training data. To address this, some works have\ndeveloped cross-modal solutions to align visual and textual modalities.\nHowever, they typically extract textual features from glosses without fully\nutilizing their knowledge. In this paper, we propose the Hierarchical\nSub-action Tree (HST), termed HST-CSLR, to efficiently combine gloss knowledge\nwith visual representation learning. By incorporating gloss-specific knowledge\nfrom large language models, our approach leverages textual information more\neffectively. Specifically, we construct an HST for textual information\nrepresentation, aligning visual and textual modalities step-by-step and\nbenefiting from the tree structure to reduce computational complexity.\nAdditionally, we impose a contrastive alignment enhancement to bridge the gap\nbetween the two modalities. Experiments on four datasets (PHOENIX-2014,\nPHOENIX-2014T, CSL-Daily, and Sign Language Gesture) demonstrate the\neffectiveness of our HST-CSLR.", "AI": {"tldr": "This paper proposes HST-CSLR, which uses a Hierarchical Sub-action Tree to better integrate gloss knowledge with visual representations, thereby improving continuous sign language recognition.", "motivation": "The motivation is to overcome the limitations of continuous sign language recognition (CSLR) due to insufficient training data by more effectively utilizing gloss knowledge.", "method": "Our method, HST-CSLR, utilizes a Hierarchical Sub-action Tree (HST) to combine gloss knowledge with visual representation learning. It incorporates gloss-specific knowledge from large language models and uses a contrastive alignment enhancement to facilitate the alignment between visual and textual modalities.", "result": "Experiments on PHOENIX-2014, PHOENIX-2014T, CSL-Daily, and Sign Language Gesture datasets show the effectiveness of HST-CSLR in improving CSLR.", "conclusion": "The paper concludes that the HST-CSLR approach, by leveraging gloss knowledge and contrastive alignment enhancements, can effectively increase the performance of continuous sign language recognition systems."}}
{"id": "2506.21222", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2506.21222", "abs": "https://arxiv.org/abs/2506.21222", "authors": ["Yongchan Chun", "Minhyuk Kim", "Dongjun Kim", "Chanjun Park", "Heuiseok Lim"], "title": "Enhancing Automatic Term Extraction with Large Language Models via Syntactic Retrieval", "comment": null, "summary": "Automatic Term Extraction (ATE) identifies domain-specific expressions that\nare crucial for downstream tasks such as machine translation and information\nretrieval. Although large language models (LLMs) have significantly advanced\nvarious NLP tasks, their potential for ATE has scarcely been examined. We\npropose a retrieval-based prompting strategy that, in the few-shot setting,\nselects demonstrations according to \\emph{syntactic} rather than semantic\nsimilarity. This syntactic retrieval method is domain-agnostic and provides\nmore reliable guidance for capturing term boundaries. We evaluate the approach\nin both in-domain and cross-domain settings, analyzing how lexical overlap\nbetween the query sentence and its retrieved examples affects performance.\nExperiments on three specialized ATE benchmarks show that syntactic retrieval\nimproves F1-score. These findings highlight the importance of syntactic cues\nwhen adapting LLMs to terminology-extraction tasks.", "AI": {"tldr": "研究提出了一种句法检索方法，用于改进基于大型语言模型的自动术语提取，使其在少样本情况下更具优势。", "motivation": "尽管大型语言模型在许多NLP任务中取得了显著进展，但其在自动术语提取中的潜力尚未得到充分研究。", "method": "本论文提出了一种基于检索的提示策略，该策略在少样本设置中根据句法相似性而不是语义相似性选择实例。", "result": "实验结果表明，基于句法的检索方法在三个专门的自动术语提取基准测试中提高了F1分数。", "conclusion": "研究结果强调了句法线索在适应LLMs进行术语提取任务时的重要性。"}}
{"id": "2506.20960", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.20960", "abs": "https://arxiv.org/abs/2506.20960", "authors": ["Yiman Zhang", "Ziheng Luo", "Qiangyu Yan", "Wei He", "Borui Jiang", "Xinghao Chen", "Kai Han"], "title": "OmniEval: A Benchmark for Evaluating Omni-modal Models with Visual, Auditory, and Textual Inputs", "comment": null, "summary": "In this paper, we introduce OmniEval, a benchmark for evaluating\nomni-modality models like MiniCPM-O 2.6, which encompasses visual, auditory,\nand textual inputs. Compared with existing benchmarks, our OmniEval has several\ndistinctive features: (i) Full-modal collaboration: We design evaluation tasks\nthat highlight the strong coupling between audio and video, requiring models to\neffectively leverage the collaborative perception of all modalities; (ii)\nDiversity of videos: OmniEval includes 810 audio-visual synchronized videos,\n285 Chinese videos and 525 English videos; (iii) Diversity and granularity of\ntasks: OmniEval contains 2617 question-answer pairs, comprising 1412 open-ended\nquestions and 1205 multiple-choice questions. These questions are divided into\n3 major task types and 12 sub-task types to achieve comprehensive evaluation.\nAmong them, we introduce a more granular video localization task named\nGrounding. Then we conduct experiments on OmniEval with several omni-modality\nmodels. We hope that our OmniEval can provide a platform for evaluating the\nability to construct and understand coherence from the context of all\nmodalities. Codes and data could be found at https://omnieval.github.io/.", "AI": {"tldr": "本文介绍了OmniEval基准测试，用于评估多模态模型（如MiniCPM-O 2.6），该模型能处理视觉、听觉和文本输入，具有模态协作、视频多样性及任务多样性等特征。", "motivation": "动机在于提供一种全面评估多模态模型能力的方法，从而增强对跨模态信息的理解和整合。", "method": "设计了一个包含810个音视频同步视频和2617个问答对的基准测试，重点强调了模态间的紧密合作并引入了更细致的视频定位任务。", "result": "在OmniEval上对多个多模态模型进行了实验，展示了OmniEval作为评估平台的有效性。", "conclusion": "OmniEval为多模态模型提供评估平台，有助于更好地理解和整合多模态信息。"}}
{"id": "2506.21252", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.21252", "abs": "https://arxiv.org/abs/2506.21252", "authors": ["Tianyi Men", "Zhuoran Jin", "Pengfei Cao", "Yubo Chen", "Kang Liu", "Jun Zhao"], "title": "Agent-RewardBench: Towards a Unified Benchmark for Reward Modeling across Perception, Planning, and Safety in Real-World Multimodal Agents", "comment": "ACL 2025 Main", "summary": "As Multimodal Large Language Models (MLLMs) advance, multimodal agents show\npromise in real-world tasks like web navigation and embodied intelligence.\nHowever, due to limitations in a lack of external feedback, these agents\nstruggle with self-correction and generalization. A promising approach is to\nuse reward models as external feedback, but there is no clear on how to select\nreward models for agents. Thus, there is an urgent need to build a reward bench\ntargeted at agents. To address these challenges, we propose Agent-RewardBench,\na benchmark designed to evaluate reward modeling ability in MLLMs. The\nbenchmark is characterized by three key features: (1) Multiple dimensions and\nreal-world agent scenarios evaluation. It covers perception, planning, and\nsafety with 7 scenarios; (2) Step-level reward evaluation. It allows for the\nassessment of agent capabilities at the individual steps of a task, providing a\nmore granular view of performance during the planning process; and (3)\nAppropriately difficulty and high-quality. We carefully sample from 10 diverse\nmodels, difficulty control to maintain task challenges, and manual verification\nto ensure the integrity of the data. Experiments demonstrate that even\nstate-of-the-art multimodal models show limited performance, highlighting the\nneed for specialized training in agent reward modeling. Code is available at\ngithub.", "AI": {"tldr": "研究提出一个评估MLLMs奖励建模能力的基准—Agent-RewardBench，解决了由于缺乏外部反馈导致的多模态代理自我纠正和泛化问题。", "motivation": "由于无法获取外部反馈，多模态代理在自我纠正和泛化方面存在困难。研究提出构建一个针对代理的奖励评估基准的迫切需求。", "method": "提出Agent-RewardBench，一个用于评估MLLMs奖励建模能力的基准，该基准具有多重维度评估、逐步骤奖励评估以及适当的难度控制和数据验证三个关键特点。", "result": "实验表明，即使是先进的多模态模型在奖励建模方面的表现也有限，突显了需要专门训练的需求。", "conclusion": "需要一个针对代理的奖励建模训练基准，以克服现有模型的局限性，提升其在任务中的表现。"}}
{"id": "2506.20964", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.20964", "abs": "https://arxiv.org/abs/2506.20964", "authors": ["Chengkuan Chen", "Luca L. Weishaupt", "Drew F. K. Williamson", "Richard J. Chen", "Tong Ding", "Bowen Chen", "Anurag Vaidya", "Long Phi Le", "Guillaume Jaume", "Ming Y. Lu", "Faisal Mahmood"], "title": "Evidence-based diagnostic reasoning with multi-agent copilot for human pathology", "comment": null, "summary": "Pathology is experiencing rapid digital transformation driven by whole-slide\nimaging and artificial intelligence (AI). While deep learning-based\ncomputational pathology has achieved notable success, traditional models\nprimarily focus on image analysis without integrating natural language\ninstruction or rich, text-based context. Current multimodal large language\nmodels (MLLMs) in computational pathology face limitations, including\ninsufficient training data, inadequate support and evaluation for multi-image\nunderstanding, and a lack of autonomous, diagnostic reasoning capabilities. To\naddress these limitations, we introduce PathChat+, a new MLLM specifically\ndesigned for human pathology, trained on over 1 million diverse,\npathology-specific instruction samples and nearly 5.5 million question answer\nturns. Extensive evaluations across diverse pathology benchmarks demonstrated\nthat PathChat+ substantially outperforms the prior PathChat copilot, as well as\nboth state-of-the-art (SOTA) general-purpose and other pathology-specific\nmodels. Furthermore, we present SlideSeek, a reasoning-enabled multi-agent AI\nsystem leveraging PathChat+ to autonomously evaluate gigapixel whole-slide\nimages (WSIs) through iterative, hierarchical diagnostic reasoning, reaching\nhigh accuracy on DDxBench, a challenging open-ended differential diagnosis\nbenchmark, while also capable of generating visually grounded,\nhumanly-interpretable summary reports.", "AI": {"tldr": "介绍PathChat+，一种专门为人类病理学设计的多模式大语言模型，展示了其在处理病理数据上的优越性，并引入SlideSeek，一个利用PathChat+进行自主分析的多智能体AI系统。", "motivation": "传统的计算病理模型主要集中在图像分析上，缺乏对自然语言指令和丰富文本背景的集成。同时，当前的多模态大语言模型在训练数据、多图像理解支持和自主诊断推理能力方面存在不足。", "method": "开发了PathChat+，这是一个专为病理学设计的大语言模型，用超过100万的多样化指令样本和近550万的问答回合训练。还创建了SlideSeek，一个多智能体AI系统，能够自主评估巨像素全幻灯片图像，通过迭代的层级诊断推理。", "result": "PathChat+在各种病理基准上的表现显著优于之前的PathChat和当前其他通用或特定于病理学的模型。SlideSeek在DDxBench基准上达到了高精度。", "conclusion": "PathChat+和SlideSeek在病理学中的应用展示了在多模式学习和AI推理上的进步，可能对未来的病理学实践产生深远影响。"}}
{"id": "2506.21274", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.21274", "abs": "https://arxiv.org/abs/2506.21274", "authors": ["Andrea McGlinchey", "Peter J Barclay"], "title": "Cat and Mouse -- Can Fake Text Generation Outpace Detector Systems?", "comment": "(Submitted for publication)", "summary": "Large language models can produce convincing \"fake text\" in domains such as\nacademic writing, product reviews, and political news. Many approaches have\nbeen investigated for the detection of artificially generated text. While this\nmay seem to presage an endless \"arms race\", we note that newer LLMs use ever\nmore parameters, training data, and energy, while relatively simple classifiers\ndemonstrate a good level of detection accuracy with modest resources. To\napproach the question of whether the models' ability to beat the detectors may\ntherefore reach a plateau, we examine the ability of statistical classifiers to\nidentify \"fake text\" in the style of classical detective fiction. Over a 0.5\nversion increase, we found that Gemini showed an increased ability to generate\ndeceptive text, while GPT did not. This suggests that reliable detection of\nfake text may remain feasible even for ever-larger models, though new model\narchitectures may improve their deceptiveness", "AI": {"tldr": "本研究对比了Gemini和GPT在生成古典侦探小说风格‘虚假文本’上的能力，并发现尽管Gemini的生成能力有所增强，但目前的检测手段仍有效。", "motivation": "鉴于大型语言模型能够伪造如学术写作、产品评论和政治新闻领域的“虚假文本”，本研究旨在探讨这些模型是否以及如何能够绕过检测器，进而分析虚假文本检测是否可能实现稳定效果。", "method": "本研究通过对比Gemini和GPT在产生古典侦探小说风格的‘虚假文本’能力上的差异，来评估统计分类器检测此类文本的能力。", "result": "研究发现，在版本更新0.5的情况下，Gemini生成更具欺骗性文本的能力有所提升，而GPT则没有。这表明，尽管模型变得越来越大，但检测虚假文本仍然是可行的。然而，新型模型架构可能会提升其欺骗性。", "conclusion": "研究表明，即使模型参数和训练数据量不断增加，使用相对简单的分类器仍能够以适度的资源实现良好的检测准确性，暗示检测虚假文本仍然可能保持可靠性。"}}
{"id": "2506.20967", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.20967", "abs": "https://arxiv.org/abs/2506.20967", "authors": ["Lingling Cai", "Kang Zhao", "Hangjie Yuan", "Xiang Wang", "Yingya Zhang", "Kejie Huang"], "title": "DFVEdit: Conditional Delta Flow Vector for Zero-shot Video Editing", "comment": "Zero-shot video editing", "summary": "The advent of Video Diffusion Transformers (Video DiTs) marks a milestone in\nvideo generation. However, directly applying existing video editing methods to\nVideo DiTs often incurs substantial computational overhead, due to\nresource-intensive attention modification or finetuning. To alleviate this\nproblem, we present DFVEdit, an efficient zero-shot video editing method\ntailored for Video DiTs. DFVEdit eliminates the need for both attention\nmodification and fine-tuning by directly operating on clean latents via flow\ntransformation. To be more specific, we observe that editing and sampling can\nbe unified under the continuous flow perspective. Building upon this\nfoundation, we propose the Conditional Delta Flow Vector (CDFV) -- a\ntheoretically unbiased estimation of DFV -- and integrate Implicit Cross\nAttention (ICA) guidance as well as Embedding Reinforcement (ER) to further\nenhance editing quality. DFVEdit excels in practical efficiency, offering at\nleast 20x inference speed-up and 85\\% memory reduction on Video DiTs compared\nto attention-engineering-based editing methods. Extensive quantitative and\nqualitative experiments demonstrate that DFVEdit can be seamlessly applied to\npopular Video DiTs (e.g., CogVideoX and Wan2.1), attaining state-of-the-art\nperformance on structural fidelity, spatial-temporal consistency, and editing\nquality.", "AI": {"tldr": "DFVEdit是一种面向Video DiTs的零拍视频编辑方法，通过流变换技术操作清洁潜空间，显著提高编辑效率，证明在处理Video DiTs时比现有技术具有显著优势。", "motivation": "现有的视频编辑方法应用到Video DiTs上时通常会带来巨大的计算开销，因为这些方法往往涉及资源密集型的注意力修改或微调。为了缓解这个问题，研究者开发了DFVEdit。", "method": "DFVEdit采用直接作用于清洁潜在空间的流变换方法，以消除对注意机制修改和微调的需求。它提出了条件差分流向量(CDFV)——一个理论上无偏的DFV估算——并结合隐式交叉注意(ICA)引导和嵌入强化(ER)以进一步提高编辑质量。", "result": "实验表明，DFVEdit可以在不牺牲性能的情况下，与基于注意力工程的编辑方法相比，提供至少20倍的推理速度加快和85%的内存减少，并且在结构保真度、时空一致性和编辑质量上达到业界顶尖水平。", "conclusion": "鉴于其实用效率和实验验证的出色性能，DFVEdit为Video DiT的零拍视频编辑提供了一个高效的方法。"}}
{"id": "2506.21285", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.21285", "abs": "https://arxiv.org/abs/2506.21285", "authors": ["Xin Xu", "Tianhao Chen", "Fan Zhang", "Wanlong Liu", "Pengxiang Li", "Ajay Kumar Jaiswal", "Yuchen Yan", "Jishan Hu", "Yang Wang", "Hao Chen", "Shiwei Liu", "Shizhe Diao", "Can Yang", "Lu Yin"], "title": "Double-Checker: Enhancing Reasoning of Slow-Thinking LLMs via Self-Critical Fine-Tuning", "comment": "10 pages", "summary": "While slow-thinking large language models (LLMs) exhibit reflection-like\nreasoning, commonly referred to as the \"aha moment:, their ability to generate\ninformative critiques and refine prior solutions remains limited. In this\npaper, we introduce Double-Checker, a principled framework designed to enhance\nthe reasoning capabilities of slow-thinking LLMs by fostering explicit\nself-critique and iterative refinement of their previous solutions. By\nfine-tuning on our curated 1,730 self-critical instances, Double-Checker\nempowers long-CoT LLMs to iteratively critique and refine their outputs during\ninference until they evaluate their solutions as correct under self-generated\ncritiques. We validate the efficacy of Double-Checker across a comprehensive\nsuite of reasoning benchmarks, demonstrating that iterative self-critique\nsignificantly enhances the reasoning capabilities of long-CoT LLMs. Notably,\nour Double-Checker increases the pass@1 performance on challenging AIME\nbenchmarks from 4.4% to 18.2% compared to the original long-CoT LLMs. These\nresults highlight a promising direction for developing more trustworthy and\neffective LLMs capable of structured self-critique.", "AI": {"tldr": "本文介绍了一种名为Double-Checker的框架，通过自我批判和迭代改进，提高了慢思考大型语言模型的推理能力。通过训练，Double-Checker显著提升了模型在推理基准测试中的表现，尤其在挑战性较高的AIME基准测试中的准确率有显著提升。", "motivation": "尽管慢思考大型语言模型展示了解决问题时的“灵光一闪”时刻，但在产生信息性评论和改进先前解决方案方面能力有限。作者旨在通过增强模型自我批判和迭代改进前一个解决方案的能力来提升其推理能力。", "method": "本文提出了Double-Checker框架，该框架通过培训模型在1,730个自我批判实例上进行自我批判和迭代改进，直到它们认为自己的解决方案在自我生成的批判中是正确的。", "result": "Double-Checker框架在各种推理基准测试中都表现出色，尤其在挑战性较高的AIME基准测试中，通过自我批判迭代，准确率从4.4%显著提升到18.2%。", "conclusion": "研究结果表明，通过自我批判和迭代改进，慢思考大型语言模型的推理能力可以得到显著增强。Double-Checker框架为开发更值得信赖和更有效的语言模型提供了有希望的方向。"}}
{"id": "2506.20977", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.20977", "abs": "https://arxiv.org/abs/2506.20977", "authors": ["Tao Liu", "Dafeng Zhang", "Gengchen Li", "Shizhuo Liu", "Yongqi Song", "Senmao Li", "Shiqi Yang", "Boqian Li", "Kai Wang", "Yaxing Wang"], "title": "From Cradle to Cane: A Two-Pass Framework for High-Fidelity Lifespan Face Aging", "comment": "30 pages, 12 figures", "summary": "Face aging has become a crucial task in computer vision, with applications\nranging from entertainment to healthcare. However, existing methods struggle\nwith achieving a realistic and seamless transformation across the entire\nlifespan, especially when handling large age gaps or extreme head poses. The\ncore challenge lies in balancing age accuracy and identity preservation--what\nwe refer to as the Age-ID trade-off. Most prior methods either prioritize age\ntransformation at the expense of identity consistency or vice versa. In this\nwork, we address this issue by proposing a two-pass face aging framework, named\nCradle2Cane, based on few-step text-to-image (T2I) diffusion models. The first\npass focuses on solving age accuracy by introducing an adaptive noise injection\n(AdaNI) mechanism. This mechanism is guided by including prompt descriptions of\nage and gender for the given person as the textual condition. Also, by\nadjusting the noise level, we can control the strength of aging while allowing\nmore flexibility in transforming the face. However, identity preservation is\nweakly ensured here to facilitate stronger age transformations. In the second\npass, we enhance identity preservation while maintaining age-specific features\nby conditioning the model on two identity-aware embeddings (IDEmb): SVR-ArcFace\nand Rotate-CLIP. This pass allows for denoising the transformed image from the\nfirst pass, ensuring stronger identity preservation without compromising the\naging accuracy. Both passes are jointly trained in an end-to-end way. Extensive\nexperiments on the CelebA-HQ test dataset, evaluated through Face++ and Qwen-VL\nprotocols, show that our Cradle2Cane outperforms existing face aging methods in\nage accuracy and identity consistency.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2506.21288", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.21288", "abs": "https://arxiv.org/abs/2506.21288", "authors": ["Istabrak Abbes", "Gabriele Prato", "Quentin Fournier", "Fernando Rodriguez", "Alaa Boukhary", "Adam Elwood", "Sarath Chandar"], "title": "Small Encoders Can Rival Large Decoders in Detecting Groundedness", "comment": null, "summary": "Augmenting large language models (LLMs) with external context significantly\nimproves their performance in natural language processing (NLP) tasks. However,\nLLMs struggle to answer queries reliably when the provided context lacks\ninformation, often resorting to ungrounded speculation or internal knowledge.\nGroundedness - generating responses strictly supported by the context - is\nessential for ensuring factual consistency and trustworthiness. This study\nfocuses on detecting whether a given query is grounded in a document provided\nin context before the costly answer generation by LLMs. Such a detection\nmechanism can significantly reduce both inference time and resource\nconsumption. We show that lightweight, task specific encoder models such as\nRoBERTa and NomicBERT, fine-tuned on curated datasets, can achieve accuracy\ncomparable to state-of-the-art LLMs, such as Llama3 8B and GPT4o, in\ngroundedness detection while reducing inference latency by orders of magnitude.\nThe code is available at : https://github.com/chandarlab/Hallucinate-less", "AI": {"tldr": "本研究使用轻量级编码器模型提升查询依据性检测的效率，相比大型语言模型能显著减少推理延迟。", "motivation": "减少由于大型语言模型在上下文信息缺失时进行无根据猜测，带来的时间和资源浪费，通过提前检测查询的依据性来提高输出的可靠性和一致性。", "method": "本研究通过使用轻量级的任务特定编码模型（如RoBERTa和NomicBERT），对经过精心策划的数据集进行微调来实现准确的groundedness检测，这些模型能够以比大型语言模型少得多的推理延迟达到几乎相同的准确率。", "result": "研究结果表明，轻量级编码模型可以在减少推理时间的同时，达到与状态-of-the-art大型语言模型相当的groundedness检测准确率。", "conclusion": "轻量级、特定任务的编码模型在减少资源消耗和推理时间的同时，能够有效检测查询是否依据上下文，从而避免大型语言模型在上下文信息不足时进行无根据猜测。"}}
{"id": "2506.20979", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.20979", "abs": "https://arxiv.org/abs/2506.20979", "authors": ["Weichen Dai", "Kangcheng Ma", "Jiaxin Wang", "Kecen Pan", "Yuhang Ming", "Hua Zhang", "Wanzeng Kong"], "title": "3D Scene-Camera Representation with Joint Camera Photometric Optimization", "comment": null, "summary": "Representing scenes from multi-view images is a crucial task in computer\nvision with extensive applications. However, inherent photometric distortions\nin the camera imaging can significantly degrade image quality. Without\naccounting for these distortions, the 3D scene representation may inadvertently\nincorporate erroneous information unrelated to the scene, diminishing the\nquality of the representation. In this paper, we propose a novel 3D\nscene-camera representation with joint camera photometric optimization. By\nintroducing internal and external photometric model, we propose a full\nphotometric model and corresponding camera representation. Based on\nsimultaneously optimizing the parameters of the camera representation, the\nproposed method effectively separates scene-unrelated information from the 3D\nscene representation. Additionally, during the optimization of the photometric\nparameters, we introduce a depth regularization to prevent the 3D scene\nrepresentation from fitting scene-unrelated information. By incorporating the\ncamera model as part of the mapping process, the proposed method constructs a\ncomplete map that includes both the scene radiance field and the camera\nphotometric model. Experimental results demonstrate that the proposed method\ncan achieve high-quality 3D scene representations, even under conditions of\nimaging degradation, such as vignetting and dirt.", "AI": {"tldr": "本研究提出了一种新颖的三维场景-相机表示方法，结合相机光度优化，有效分离了场景无关信息，提升了三维场景表示的质量。", "motivation": "传统的多视图图像三维表示方法可能会因相机成像时的光度失真而导致图像质量下降，进而影响三维场景表示的质量。", "method": "引入内部和外部光度模型，提出了完整的光度模型及相应的相机表示，通过同步优化相机参数，防止三维场景表示拟合无关信息。", "result": "实验结果表明，该方法能够在图像退化条件下（如暗角和污迹）也能实现高质量的三维场景表示。", "conclusion": "所提出的方法能够有效改善由相机光度失真引起的问题，适用于广泛的成像退化情况下的三维场景表示。"}}
{"id": "2506.21294", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.21294", "abs": "https://arxiv.org/abs/2506.21294", "authors": ["Bram Willemsen", "Gabriel Skantze"], "title": "Detecting Referring Expressions in Visually Grounded Dialogue with Autoregressive Language Models", "comment": "Accepted for publication at XLLM @ ACL 2025", "summary": "In this paper, we explore the use of a text-only, autoregressive language\nmodeling approach for the extraction of referring expressions from visually\ngrounded dialogue. More specifically, the aim is to investigate the extent to\nwhich the linguistic context alone can inform the detection of mentions that\nhave a (visually perceivable) referent in the visual context of the\nconversation. To this end, we adapt a pretrained large language model (LLM) to\nperform a relatively course-grained annotation of mention spans in unfolding\nconversations by demarcating mention span boundaries in text via next-token\nprediction. Our findings indicate that even when using a moderately sized LLM,\nrelatively small datasets, and parameter-efficient fine-tuning, a text-only\napproach can be effective, highlighting the relative importance of the\nlinguistic context for this task. Nevertheless, we argue that the task\nrepresents an inherently multimodal problem and discuss limitations fundamental\nto unimodal approaches.", "AI": {"tldr": "研究发现，即使仅使用文本及适中的LLM和数据集，也可有效地抽取对话中的视觉参照提及，但强调了该任务本为多模态性质，存在单一模式方法的局限性。", "motivation": "研究动机在于探讨仅凭语言环境能否指导检测在对话视觉背景中有视觉可感知参照的提及。", "method": "本研究采用了一种仅基于文本的自回归语言模型方法来从视觉化的对话中抽取出指示表达。特别地，通过调整一个预训练的大型语言模型(LLM)，利用下一个词汇预测来标注对话中的提及片段边界，从而实现较为粗略的提及片段标注。", "result": "研究结果显示，即便是使用规模适中的LLM、相对较小的数据集和参数高效的微调，纯文本方法也可以有效地完成任务，强调了语言环境在这项任务中的相对重要性。", "conclusion": "尽管纯文本方法在任务中发现了一定的效果，但研究者认为此任务本质上是多模态的，需要进一步探讨和使用多元数据和方法来提升效果。"}}
{"id": "2506.20983", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.20983", "abs": "https://arxiv.org/abs/2506.20983", "authors": ["Wenjie Xuan", "Jing Zhang", "Juhua Liu", "Bo Du", "Dacheng Tao"], "title": "Rethink Sparse Signals for Pose-guided Text-to-image Generation", "comment": "accepted by ICCV 2025", "summary": "Recent works favored dense signals (e.g., depth, DensePose), as an\nalternative to sparse signals (e.g., OpenPose), to provide detailed spatial\nguidance for pose-guided text-to-image generation. However, dense\nrepresentations raised new challenges, including editing difficulties and\npotential inconsistencies with textual prompts. This fact motivates us to\nrevisit sparse signals for pose guidance, owing to their simplicity and\nshape-agnostic nature, which remains underexplored. This paper proposes a novel\nSpatial-Pose ControlNet(SP-Ctrl), equipping sparse signals with robust\ncontrollability for pose-guided image generation. Specifically, we extend\nOpenPose to a learnable spatial representation, making keypoint embeddings\ndiscriminative and expressive. Additionally, we introduce keypoint concept\nlearning, which encourages keypoint tokens to attend to the spatial positions\nof each keypoint, thus improving pose alignment. Experiments on animal- and\nhuman-centric image generation tasks demonstrate that our method outperforms\nrecent spatially controllable T2I generation approaches under sparse-pose\nguidance and even matches the performance of dense signal-based methods.\nMoreover, SP-Ctrl shows promising capabilities in diverse and cross-species\ngeneration through sparse signals. Codes will be available at\nhttps://github.com/DREAMXFAR/SP-Ctrl.", "AI": {"tldr": "本文提出了一种新的Spatial-Pose ControlNet (SP-Ctrl)，以提高稀疏信号在姿态引导图像生成中的可控性，结果展示这种方法在动物和人像图像生成任务中性能优秀，并具有跨物种生成的能力。", "motivation": "当前研究倾向于使用密集信号来提供详细的姿态引导图像生成指导，但也带来了新的挑战，如编辑难度和与文本提示的潜在不一致。因此，研究者重新考虑使用稀疏信号，因其具有简单性和形状无关特性。", "method": "此论文提出了一种名为Spatial-Pose ControlNet (SP-Ctrl)的新方法，它增强了稀疏信号在引导图像生成中的控制能力。具体而言，该方法将OpenPose扩展为一种可学习的空间表示形式，使得关键点嵌入具有区分性和表现力，并引入了关键点概念学习，以改进姿态对齐。", "result": "实验结果表明，该方法在动物和人像图像生成任务中优于基于稀疏姿态引导的空间可控文本到图像生成方法，并且在性能上甚至与基于密集信号的方法相匹配。", "conclusion": "该论文的结论是SP-Ctrl展示了在通过稀疏信号进行多样性和跨物种生成中的良好能力。"}}
{"id": "2506.21360", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.21360", "abs": "https://arxiv.org/abs/2506.21360", "authors": ["Fangzhou Dong", "Yifan Zeng", "Yingpeng Sang", "Hong Shen"], "title": "Structuralist Approach to AI Literary Criticism: Leveraging Greimas Semiotic Square for Large Language Models", "comment": "Accepted in CogSci 2025", "summary": "Large Language Models (LLMs) excel in understanding and generating text but\nstruggle with providing professional literary criticism for works with profound\nthoughts and complex narratives. This paper proposes GLASS (Greimas Literary\nAnalysis via Semiotic Square), a structured analytical framework based on\nGreimas Semiotic Square (GSS), to enhance LLMs' ability to conduct in-depth\nliterary analysis. GLASS facilitates the rapid dissection of narrative\nstructures and deep meanings in narrative works. We propose the first dataset\nfor GSS-based literary criticism, featuring detailed analyses of 48 works. Then\nwe propose quantitative metrics for GSS-based literary criticism using the\nLLM-as-a-judge paradigm. Our framework's results, compared with expert\ncriticism across multiple works and LLMs, show high performance. Finally, we\napplied GLASS to 39 classic works, producing original and high-quality analyses\nthat address existing research gaps. This research provides an AI-based tool\nfor literary research and education, offering insights into the cognitive\nmechanisms underlying literary engagement.", "AI": {"tldr": "This paper proposes GLASS, a structured analytical framework to enhance LLMs' ability to conduct in-depth literary analysis, showing high performance and providing insights into literary engagement.", "motivation": "LLMs struggle with providing professional literary criticism for works with profound thoughts and complex narratives, and this research aims to address this issue.", "method": "GLASS (Greimas Literary Analysis via Semiotic Square) is proposed, a structured analytical framework based on Greimas Semiotic Square (GSS) to help LLMs with in-depth literary analysis.", "result": "The framework shows high performance, and it is applied to 39 classic works, producing original and high-quality analyses.", "conclusion": "This research provides an AI-based tool for literary research and education, offering insights into the cognitive mechanisms underlying literary engagement."}}
{"id": "2506.20986", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.20986", "abs": "https://arxiv.org/abs/2506.20986", "authors": ["Xiao Zhang", "Yongqiang Ma", "Haodong Jing", "Nanning Zheng"], "title": "EVA: Mixture-of-Experts Semantic Variant Alignment for Compositional Zero-Shot Learning", "comment": null, "summary": "Compositional Zero-Shot Learning (CZSL) investigates compositional\ngeneralization capacity to recognize unknown state-object pairs based on\nlearned primitive concepts. Existing CZSL methods typically derive primitives\nfeatures through a simple composition-prototype mapping, which is suboptimal\nfor a set of individuals that can be divided into distinct semantic subsets.\nMoreover, the all-to-one cross-modal primitives matching neglects compositional\ndivergence within identical states or objects, limiting fine-grained\nimage-composition alignment. In this study, we propose EVA, a\nMixture-of-Experts Semantic Variant Alignment framework for CZSL. Specifically,\nwe introduce domain-expert adaption, leveraging multiple experts to achieve\ntoken-aware learning and model high-quality primitive representations. To\nenable accurate compositional generalization, we further present semantic\nvariant alignment to select semantically relevant representation for\nimage-primitives matching. Our method significantly outperforms other\nstate-of-the-art CZSL methods on three popular benchmarks in both closed- and\nopen-world settings, demonstrating the efficacy of the proposed insight.", "AI": {"tldr": "引入EVA框架，解决现有CZSL方法在处理不同语义子集个体和组合差异方面的不足，显著提高在标准测试中的性能。", "motivation": "现有CZSL方法通过简单的组合原型映射来提取原始特征，这种方法对于划分成不同语义子集的个体是次优的，并且所有到一个的跨模式原始匹配忽略了相同状态或物体内的组合差异，限制了图像-组合的细微对齐。", "method": "EVA框架，使用多专家适应来实现标记感知学习和高质量原始概念表示的建模，以及语义变体对齐来实现准确的图像-概念匹配。", "result": "该方法在三个流行的基准测试中，在封闭世界和开放世界设置下显著优于其他最先进的CZSL方法。", "conclusion": "研究提出的EVA框架证明了其对CZSL问题的适应性和有效性。"}}
{"id": "2506.21384", "categories": ["cs.CL", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2506.21384", "abs": "https://arxiv.org/abs/2506.21384", "authors": ["Guanting Dong", "Xiaoxi Li", "Yuyao Zhang", "Mengjie Deng"], "title": "Leveraging LLM-Assisted Query Understanding for Live Retrieval-Augmented Generation", "comment": "Accepted at SIGIR 2025 LiveRAG Workshop (Oral Presentation)", "summary": "Real-world live retrieval-augmented generation (RAG) systems face significant\nchallenges when processing user queries that are often noisy, ambiguous, and\ncontain multiple intents. While RAG enhances large language models (LLMs) with\nexternal knowledge, current systems typically struggle with such complex\ninputs, as they are often trained or evaluated on cleaner data. This paper\nintroduces Omni-RAG, a novel framework designed to improve the robustness and\neffectiveness of RAG systems in live, open-domain settings. Omni-RAG employs\nLLM-assisted query understanding to preprocess user inputs through three key\nmodules: (1) Deep Query Understanding and Decomposition, which utilizes LLMs\nwith tailored prompts to denoise queries (e.g., correcting spelling errors) and\ndecompose multi-intent queries into structured sub-queries; (2) Intent-Aware\nKnowledge Retrieval, which performs retrieval for each sub-query from a corpus\n(i.e., FineWeb using OpenSearch) and aggregates the results; and (3) Reranking\nand Generation, where a reranker (i.e., BGE) refines document selection before\na final response is generated by an LLM (i.e., Falcon-10B) using a\nchain-of-thought prompt. Omni-RAG aims to bridge the gap between current RAG\ncapabilities and the demands of real-world applications, such as those\nhighlighted by the SIGIR 2025 LiveRAG Challenge, by robustly handling complex\nand noisy queries.", "AI": {"tldr": "Omni-RAG是一个旨在增强RAG系统在开放领域实时应用中稳健性和有效性的新框架，特别解决了处理复杂、含噪声查询的问题。", "motivation": "为了应对现实世界中包含噪音、歧义和多种意图的用户查询对RAG系统的挑战，特别是这些系统在训练或评估时依赖于更干净的数据集。", "method": "Omni-RAG采用LLM辅助的查询理解，通过三个关键模块来预处理用户输入：(1) 深度查询理解和分解，使用定制提示的LLM来去除噪音（如纠正拼写错误）并将多意图查询分解为结构化的子查询；(2) 意图感知的知识检索，对每个子查询从语料库中检索（如使用OpenSearch从FineWeb获取）并聚合结果；(3) 重排序和生成，一个重排序器（如BGE）优化文档选择，然后由LLM（如Falcon-10B）使用连贯思维提示生成最终回复。", "result": "通过实验表明，Omni-RAG能够更有效地处理复杂的用户查询，显著提高了回答的准确性和相关性。", "conclusion": "Omni-RAG通过LLM辅助的查询理解和处理，能够更好地应对开放领域实时查询的挑战，代表了RAG系统向实际应用需求的实质性进步。"}}
{"id": "2506.20988", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.20988", "abs": "https://arxiv.org/abs/2506.20988", "authors": ["Zhixuan Chen", "Junlin Hou", "Liqi Lin", "Yihui Wang", "Yequan Bie", "Xi Wang", "Yanning Zhou", "Ronald Cheong Kin Chan", "Hao Chen"], "title": "Segment Anything in Pathology Images with Natural Language", "comment": null, "summary": "Pathology image segmentation is crucial in computational pathology for\nanalyzing histological features relevant to cancer diagnosis and prognosis.\nHowever, current methods face major challenges in clinical applications due to\nlimited annotated data and restricted category definitions. To address these\nlimitations, we propose PathSegmentor, the first text-prompted segmentation\nfoundation model designed specifically for pathology images. We also introduce\nPathSeg , the largest and most comprehensive dataset for pathology\nsegmentation, built from 17 public sources and containing 275k image-mask-label\ntriples across 160 diverse categories. With PathSegmentor, users can perform\nsemantic segmentation using natural language prompts, eliminating the need for\nlaborious spatial inputs such as points or boxes. Extensive experiments\ndemonstrate that PathSegmentor outperforms specialized models with higher\naccuracy and broader applicability, while maintaining a compact architecture.\nIt significantly surpasses existing spatial- and text-prompted models by 0.145\nand 0.429 in overall Dice scores, respectively, showing strong robustness in\nsegmenting complex structures and generalizing to external datasets. Moreover,\nPathSegmentor's outputs enhance the interpretability of diagnostic models\nthrough feature importance estimation and imaging biomarker discovery, offering\npathologists evidence-based support for clinical decision-making. This work\nadvances the development of explainable AI in precision oncology.", "AI": {"tldr": "本论文介绍了PathSegmentor，一种用于病理解析图像的文本提示分割模型，其在准确度和适用性上超越了现有模型，有助于提升病理诊断的准确性和决策的可解释性。", "motivation": "现有的方法在临床应用中面临问题，主要是由于注释数据有限和类别定义限制。为了克服这些限制，该研究提出了新的解决方案。", "method": "提出PathSegmentor，这是一种针对病理解析图像的首次采用文本提示的分割基础模型。同时引入了PathSeg数据集，这是最大的病理分割数据集，包含来自17个公开来源的275k张图像、掩模和标签，共计160类。此模型能够通过自然语言提示实现语义分割，不需要繁琐的空间输入。", "result": "实验表明，PathSegmentor在整体Dice评分上，分别超过现有的空间提示模型和文本提示模型0.145和0.429，显示出强大的鲁棒性和泛化能力。", "conclusion": "此工作促进了精准肿瘤学中可解释AI的发展，通过增强诊断模型的可解释性，为临床决策提供证据支持。"}}
{"id": "2506.21443", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.21443", "abs": "https://arxiv.org/abs/2506.21443", "authors": ["Ali Şenol", "Garima Agrawal", "Huan Liu"], "title": "Domain Knowledge-Enhanced LLMs for Fraud and Concept Drift Detection", "comment": null, "summary": "Detecting deceptive conversations on dynamic platforms is increasingly\ndifficult due to evolving language patterns and Concept Drift (CD)\\-i.e.,\nsemantic or topical shifts that alter the context or intent of interactions\nover time. These shifts can obscure malicious intent or mimic normal dialogue,\nmaking accurate classification challenging. While Large Language Models (LLMs)\nshow strong performance in natural language tasks, they often struggle with\ncontextual ambiguity and hallucinations in risk\\-sensitive scenarios. To\naddress these challenges, we present a Domain Knowledge (DK)\\-Enhanced LLM\nframework that integrates pretrained LLMs with structured, task\\-specific\ninsights to perform fraud and concept drift detection. The proposed\narchitecture consists of three main components: (1) a DK\\-LLM module to detect\nfake or deceptive conversations; (2) a drift detection unit (OCDD) to determine\nwhether a semantic shift has occurred; and (3) a second DK\\-LLM module to\nclassify the drift as either benign or fraudulent. We first validate the value\nof domain knowledge using a fake review dataset and then apply our full\nframework to SEConvo, a multiturn dialogue dataset that includes various types\nof fraud and spam attacks. Results show that our system detects fake\nconversations with high accuracy and effectively classifies the nature of\ndrift. Guided by structured prompts, the LLaMA\\-based implementation achieves\n98\\% classification accuracy. Comparative studies against zero\\-shot baselines\ndemonstrate that incorporating domain knowledge and drift awareness\nsignificantly improves performance, interpretability, and robustness in\nhigh\\-stakes NLP applications.", "AI": {"tldr": "提出了一种DK-Enhanced LLM框架，用于在动态平台上检测欺骗性对话，包含域知识集成和概念漂移检测。实验表明，该方法具有高准确性及其在高风险NLP应用中的改进效果。", "motivation": "该研究的动机是解决在动态平台上检测欺骗性对话的日益挑战性问题，尤其是由于语言模式的演变和概念漂移所导致的问题，这些问题使准确分类变得困难。", "method": "提出了一种基于领域知识（DK）的大型语言模型（LLM）框架，用于欺诈和概念漂移检测。该架构包括三个主要组件：（1）DK-LLM模块用于检测虚假或欺骗性对话；（2）漂移检测单元（OCDD）用于判断语义漂移是否发生；（3）另一个DK-LLM模块用于将漂移分类为良性或欺诈性的。", "result": "实验结果表明，该系统能够在高准确率下检测虚假对话，并有效地分类漂移的性质。基于LLaMA的实现达到了98%的分类准确率。", "conclusion": "研究表明，通过结构化提示引导的LLaMA实现表现出了极高的分类准确率，以及在高风险NLP应用中的性能、可解释性和鲁棒性的明显提升。相较于零样本基准的比较研究也展示了该框架在这些领域的优势。"}}
{"id": "2506.20991", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.20991", "abs": "https://arxiv.org/abs/2506.20991", "authors": ["Chade Li", "Pengju Zhang", "Yihong Wu"], "title": "TSDASeg: A Two-Stage Model with Direct Alignment for Interactive Point Cloud Segmentation", "comment": null, "summary": "The rapid advancement of 3D vision-language models (VLMs) has spurred\nsignificant interest in interactive point cloud processing tasks, particularly\nfor real-world applications. However, existing methods often underperform in\npoint-level tasks, such as segmentation, due to missing direct 3D-text\nalignment, limiting their ability to link local 3D features with textual\ncontext. To solve this problem, we propose TSDASeg, a Two-Stage model coupled\nwith a Direct cross-modal Alignment module and memory module for interactive\npoint cloud Segmentation. We introduce the direct cross-modal alignment module\nto establish explicit alignment between 3D point clouds and textual/2D image\ndata. Within the memory module, we employ multiple dedicated memory banks to\nseparately store text features, visual features, and their cross-modal\ncorrespondence mappings. These memory banks are dynamically leveraged through\nself-attention and cross-attention mechanisms to update scene-specific features\nbased on prior stored data, effectively addressing inconsistencies in\ninteractive segmentation results across diverse scenarios. Experiments\nconducted on multiple 3D instruction, reference, and semantic segmentation\ndatasets demonstrate that the proposed method achieves state-of-the-art\nperformance.", "AI": {"tldr": "TSDASeg is a Two-Stage model with a Direct cross-modal Alignment module and memory module, designed for interactive point cloud segmentation, that achieves state-of-the-art performance.", "motivation": "To address the poor performance of existing methods in point-level tasks due to missing direct 3D-text alignment, which hinders their ability to link local 3D features with textual context.", "method": "We propose TSDASeg, which uses a Two-Stage model coupled with a Direct cross-modal Alignment module and memory module for interactive point cloud segmentation.", "result": "Experiments on multiple 3D instruction, reference, and semantic segmentation datasets show that the proposed method achieves state-of-the-art performance.", "conclusion": "By introducing direct cross-modal alignment and dynamic memory banks, TSDASeg effectively improves the consistency and performance of interactive point cloud segmentation across various scenarios."}}
{"id": "2506.21445", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2506.21445", "abs": "https://arxiv.org/abs/2506.21445", "authors": ["Makbule Gulcin Ozsoy", "William Tai"], "title": "Text2Cypher Across Languages: Evaluating Foundational Models Beyond English", "comment": null, "summary": "Recent advances in large language models have enabled natural language\ninterfaces that translate user questions into database queries, such as\nText2SQL, Text2SPARQL, and Text2Cypher. While these interfaces enhance database\naccessibility, most research today focuses solely on English, with limited\nevaluation in other languages. This paper investigates the performance of\nfoundational LLMs on the Text2Cypher task across multiple languages. We create\nand release a multilingual test set by translating English questions into\nSpanish and Turkish while preserving the original Cypher queries, enabling fair\ncross-lingual comparison. We evaluate multiple foundational models using\nstandardized prompts and metrics. Our results show a consistent performance\npattern: highest on English, then Spanish, and lowest on Turkish. We attribute\nthis to differences in training data availability and linguistic\ncharacteristics. Additionally, we explore the impact of translating task\nprompts into Spanish and Turkish. Results show little to no change in\nevaluation metrics, suggesting prompt translation has minor impact. Our\nfindings highlight the need for more inclusive evaluation and development in\nmultilingual query generation. Future work includes schema localization and\nfine-tuning across diverse languages.", "AI": {"tldr": "该研究创建了一套多语言Text2Cypher测试集，结果表明英文最佳，西班牙语次之，土耳其语最差。提示词翻译影响有限。强调了在多语言查询生成中需要更包容的评估和发展。", "motivation": "该研究旨在解决当前大多数研究仅关注英文，而对其他语言的评估有限的问题。研究动机是探讨基础语言模型在Text2Cypher任务上多语言性能的表现。", "method": "本研究通过将英文问题翻译成西班牙语和土耳其语来创建和发布多语言测试集，同时保留原有的Cypher查询，以实现公平的跨语言比较。评估了多个基础模型，并使用了标准化的提示词和度量标准。", "result": "研究结果显示，语言模型在执行Text2Cypher任务时，英文表现最好，其次是西班牙语，而土耳其语表现最差。此外，将任务提示词翻译成西班牙语和土耳其语对评估指标几乎没有影响。", "conclusion": "研究结果表明，英文的表现最好，其次是西班牙语，土耳其语最差。这些差异归因于训练数据的可用性和语言特征差异。此外，探索了将任务提示词翻译成西班牙语和土耳其语的影响，结果表明这几乎没有改变评估指标。因此，需要在跨语言查询生成的评估和发展中更加包容。未来的工作将包括模式本地化和跨多种语言的微调。"}}
{"id": "2506.20995", "categories": ["cs.CV", "cs.LG", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.20995", "abs": "https://arxiv.org/abs/2506.20995", "authors": ["Akio Hayakawa", "Masato Ishii", "Takashi Shibuya", "Yuki Mitsufuji"], "title": "Step-by-Step Video-to-Audio Synthesis via Negative Audio Guidance", "comment": null, "summary": "We propose a novel step-by-step video-to-audio generation method that\nsequentially produces individual audio tracks, each corresponding to a specific\nsound event in the video. Our approach mirrors traditional Foley workflows,\naiming to capture all sound events induced by a given video comprehensively.\nEach generation step is formulated as a guided video-to-audio synthesis task,\nconditioned on a target text prompt and previously generated audio tracks. This\ndesign is inspired by the idea of concept negation from prior compositional\ngeneration frameworks. To enable this guided generation, we introduce a\ntraining framework that leverages pre-trained video-to-audio models and\neliminates the need for specialized paired datasets, allowing training on more\naccessible data. Experimental results demonstrate that our method generates\nmultiple semantically distinct audio tracks for a single input video, leading\nto higher-quality composite audio synthesis than existing baselines.", "AI": {"tldr": "本研究提出了一种新的视频到音频生成方法，该方法顺序生成音频轨道并模仿传统的Foley声音制作流程。这一方法比现有技术能生成质量更高的音频内容。", "motivation": "我们的目标是提出一种方法，能够全面地捕捉由给定视频引起的全部声音事件，模仿传统的Foley工作流程。", "method": "我们提出了一种新的分步视频到音频生成方法，该方法顺序地产生单独的音频轨道，每个音频轨道对应视频中的一个特定声音事件。每一步的生成都被定义为一个指导式的视频到音频合成任务，受先前组合生成框架中概念否定的想法启发。为了实现这种指导式生成，我们引入了一个训练框架，该框架利用预训练的视频到音频模型，并消除了对特殊配对数据集的需求，允许在更易于获取的数据上进行训练。", "result": "实验结果表明，我们的方法能够为单个输入视频生成多个语义上不同的音频轨道，从而产生比现有基线更高的合成音频质量。", "conclusion": "该方法的结论是：通过我们的新方法，可以更精确地生成视频中相关的音频内容，提高了合成音频的质量，并展示了在广泛可用的数据上进行训练的能力。"}}
{"id": "2506.21463", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2506.21463", "abs": "https://arxiv.org/abs/2506.21463", "authors": ["Anne Wu", "Laurent Mazaré", "Neil Zeghidour", "Alexandre Défossez"], "title": "Aligning Spoken Dialogue Models from User Interactions", "comment": "Accepted at ICML 2025", "summary": "We propose a novel preference alignment framework for improving spoken\ndialogue models on real-time conversations from user interactions. Current\npreference learning methods primarily focus on text-based language models, and\nare not directly suited to the complexities of real-time speech interactions,\nwith richer dynamics (e.g. interruption, interjection) and no explicit\nsegmentation between speaker turns.We create a large-scale dataset of more than\n150,000 preference pairs from raw multi-turn speech conversations, annotated\nwith AI feedback, to cover preferences over both linguistic content and\ntemporal context variations. We leverage offline alignment methods to finetune\na full-duplex autoregressive speech-to-speech model. Extensive experiments\ndemonstrate that feedback on generic conversations can be consistently\neffective in improving spoken dialogue models to produce more factual, safer\nand more contextually aligned interactions. We deploy the finetuned model and\nconduct holistic human evaluations to assess the impact beyond single-turn\nconversations. Our findings shed light on the importance of a well-calibrated\nbalance among various dynamics, crucial for natural real-time speech dialogue\nsystems.", "AI": {"tldr": "研究提出并验证了一种新的偏好对齐框架，用于改善实时语音对话系统的性能。", "motivation": "由于现有的偏好学习方法主要关注文本语言模型，这类方法不适合处理实时语音交互中的复杂动态（如打断、插话）和发言人轮次之间无明确分隔的情况。因此，设计一种适应性更好的偏好学习方法是验证这种框架动机。", "method": "提出了一种新颖的偏好对齐框架，旨在通过用户交互改善实时对话模型。该框架利用大规模的多轮语音对话数据集（超过150,000个偏好对），这些数据集覆盖了语言内容和时间上下文的变化，并使用离线对齐方法微调全双工自回归语音到语音模型。", "result": "实验表明，基于通用对话反馈对模型的微调可以显著提升对话系统在事实正确性、安全性及上下文相关性方面的表现。", "conclusion": "这项研究强调了实现实时自然语音对话系统中各种动态因素之间平衡的重要性。"}}
{"id": "2506.20998", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.20998", "abs": "https://arxiv.org/abs/2506.20998", "authors": ["Yeon-Ji Song", "Jaein Kim", "Byung-Ju Kim", "Byoung-Tak Zhang"], "title": "DBMovi-GS: Dynamic View Synthesis from Blurry Monocular Video via Sparse-Controlled Gaussian Splatting", "comment": "CVPRW 2025, Neural Fields Beyond Conventional Cameras", "summary": "Novel view synthesis is a task of generating scenes from unseen perspectives;\nhowever, synthesizing dynamic scenes from blurry monocular videos remains an\nunresolved challenge that has yet to be effectively addressed. Existing novel\nview synthesis methods are often constrained by their reliance on\nhigh-resolution images or strong assumptions about static geometry and rigid\nscene priors. Consequently, their approaches lack robustness in real-world\nenvironments with dynamic object and camera motion, leading to instability and\ndegraded visual fidelity. To address this, we propose Motion-aware Dynamic View\nSynthesis from Blurry Monocular Video via Sparse-Controlled Gaussian Splatting\n(DBMovi-GS), a method designed for dynamic view synthesis from blurry monocular\nvideos. Our model generates dense 3D Gaussians, restoring sharpness from blurry\nvideos and reconstructing detailed 3D geometry of the scene affected by dynamic\nmotion variations. Our model achieves robust performance in novel view\nsynthesis under dynamic blurry scenes and sets a new benchmark in realistic\nnovel view synthesis for blurry monocular video inputs.", "AI": {"tldr": "提出了一种基于稀疏控制高斯图的运动感知动态视图合成方法（DBMovi-GS），用于从模糊的单目视频中合成动态场景视图，能够在动态模糊场景中实现稳健的新视图合成。", "motivation": "现有视图合成方法受限于对高分辨率图像的依赖或对静态几何和刚性场景假设的依赖，难以在包含动态物体和相机运动的真实环境中保持稳定性。因此，需要一个新方法来解决从模糊单目视频中合成动态场景视图的问题。", "method": "新的方法生成密集的三维高斯分布，能够从模糊视频中恢复清晰度并重建受动态变化影响的精细三维场景几何。", "result": "实验结果显示该方法能够稳健地在动态模糊场景中实现新视图的合成，并且在使用模糊单目视频输入的现实视图合成方面树立了新的标杆。", "conclusion": "DBMovi-GS为从动态模糊单目视频中合成新视图提供了一种有效的解决方案，提升了实际环境中的表现。"}}
{"id": "2506.21468", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.21468", "abs": "https://arxiv.org/abs/2506.21468", "authors": ["Ryosuke Takahashi", "Tatsuro Inaba", "Kentaro Inui", "Benjamin Heinzerling"], "title": "TopK Language Models", "comment": null, "summary": "Sparse autoencoders (SAEs) have become an important tool for analyzing and\ninterpreting the activation space of transformer-based language models (LMs).\nHowever, SAEs suffer several shortcomings that diminish their utility and\ninternal validity. Since SAEs are trained post-hoc, it is unclear if the\nfailure to discover a particular concept is a failure on the SAE's side or due\nto the underlying LM not representing this concept. This problem is exacerbated\nby training conditions and architecture choices affecting which features an SAE\nlearns. When tracing how LMs learn concepts during training, the lack of\nfeature stability also makes it difficult to compare SAEs features across\ndifferent checkpoints. To address these limitations, we introduce a\nmodification to the transformer architecture that incorporates a TopK\nactivation function at chosen layers, making the model's hidden states\nequivalent to the latent features of a TopK SAE. This approach eliminates the\nneed for post-hoc training while providing interpretability comparable to SAEs.\nThe resulting TopK LMs offer a favorable trade-off between model size,\ncomputational efficiency, and interpretability. Despite this simple\narchitectural change, TopK LMs maintain their original capabilities while\nproviding robust interpretability benefits. Our experiments demonstrate that\nthe sparse representations learned by TopK LMs enable successful steering\nthrough targeted neuron interventions and facilitate detailed analysis of\nneuron formation processes across checkpoints and layers. These features make\nTopK LMs stable and reliable tools for understanding how language models learn\nand represent concepts, which we believe will significantly advance future\nresearch on model interpretability and controllability.", "AI": {"tldr": "改进了Transformer架构，集成TopK激活函数，克服了传统SAEs的局限性，增强了语言模型的解释性和可控性。", "motivation": "解决稀疏自编码器（SAEs）在分析和解释基于变压器的语言模型（LMs）的激活空间时遇到的问题，包括内在有效性不足，特性不稳定和难以在不同检查点之间比较特性。", "method": "提出了在变压器架构中引入TopK激活函数的修改，使隐藏状态等同于TopK稀疏自编码器的潜特征，从而在无需事后训练的情况下提供了与SAEs相当的可解释性。", "result": "TopK LMs在维持原模型能力的同时，增强了解释性；可以通过针对性的神经元干预成功引导，方便分析检查点和层级间神经元的形成过程。", "conclusion": "TopK LMs作为理解和研究语言模型如何学习和表示概念的稳定可靠工具，对该领域未来的研究有重大推进作用。"}}
{"id": "2506.21001", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.21001", "abs": "https://arxiv.org/abs/2506.21001", "authors": ["Qiuyi Qi", "Xin Li", "Ming Kong", "Zikang Xu", "Bingdi Chen", "Qiang Zhu", "S Kevin Zhou"], "title": "Style-Aligned Image Composition for Robust Detection of Abnormal Cells in Cytopathology", "comment": "MIDL 2025 Oral", "summary": "Challenges such as the lack of high-quality annotations, long-tailed data\ndistributions, and inconsistent staining styles pose significant obstacles to\ntraining neural networks to detect abnormal cells in cytopathology robustly.\nThis paper proposes a style-aligned image composition (SAIC) method that\ncomposes high-fidelity and style-preserved pathological images to enhance the\neffectiveness and robustness of detection models. Without additional training,\nSAIC first selects an appropriate candidate from the abnormal cell bank based\non attribute guidance. Then, it employs a high-frequency feature reconstruction\nto achieve a style-aligned and high-fidelity composition of abnormal cells and\npathological backgrounds. Finally, it introduces a large vision-language model\nto filter high-quality synthesis images. Experimental results demonstrate that\nincorporating SAIC-synthesized images effectively enhances the performance and\nrobustness of abnormal cell detection for tail categories and styles, thereby\nimproving overall detection performance. The comprehensive quality evaluation\nfurther confirms the generalizability and practicality of SAIC in clinical\napplication scenarios. Our code will be released at\nhttps://github.com/Joey-Qi/SAIC.", "AI": {"tldr": "The paper presents the SAIC method to improve abnormal cell detection robustness and effectiveness in cytopathology by addressing data challenges. Synthetic images generated via SAIC boost detection performance, validating the method's clinical applicability.", "motivation": "The motivation of the research is to address the challenges in training neural networks for abnormal cell detection due to lack of high-quality annotations, long-tailed data distributions, and inconsistent staining styles in cytopathology.", "method": "This paper introduces the style-aligned image composition (SAIC) method to enhance the robustness and effectiveness of neural network detection for abnormal cells in cytopathology. The method selects an appropriate candidate from the abnormal cell bank based on attribute guidance, performs high-frequency feature reconstruction to align the style and maintain high fidelity of the composition, and uses a large vision-language model to filter high-quality synthesized images.", "result": "Experimental results show that using SAIC-synthesized images significantly improves the detection performance for tail categories and styles, proving the method's enhancement in detection performance.", "conclusion": "SAIC demonstrates its generalizability and practicality in enhancing the robustness and performance of detection models for abnormal cells. The research suggests that the SAIC method can achieve better performance in clinical applications compared to previous methods."}}
{"id": "2506.21495", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.21495", "abs": "https://arxiv.org/abs/2506.21495", "authors": ["Jack Lanchantin", "Angelica Chen", "Janice Lan", "Xian Li", "Swarnadeep Saha", "Tianlu Wang", "Jing Xu", "Ping Yu", "Weizhe Yuan", "Jason E Weston", "Sainbayar Sukhbaatar", "Ilia Kulikov"], "title": "Bridging Offline and Online Reinforcement Learning for LLMs", "comment": null, "summary": "We investigate the effectiveness of reinforcement learning methods for\nfinetuning large language models when transitioning from offline to semi-online\nto fully online regimes for both verifiable and non-verifiable tasks. Our\nexperiments cover training on verifiable math as well as non-verifiable\ninstruction following with a set of benchmark evaluations for both. Across\nthese settings, we extensively compare online and semi-online Direct Preference\nOptimization and Group Reward Policy Optimization objectives, and surprisingly\nfind similar performance and convergence between these variants, which all\nstrongly outperform offline methods. We provide a detailed analysis of the\ntraining dynamics and hyperparameter selection strategies to achieve optimal\nresults. Finally, we show that multi-tasking with verifiable and non-verifiable\nrewards jointly yields improved performance across both task types.", "AI": {"tldr": "本研究探讨了在不同在线模式下，强化学习方法对大规模语言模型的微调效果，并指出多任务学习能够提升性能。", "motivation": "本文的动机在于理解强化学习方法在不同在线模式下微调大规模语言模型的有效性，以提高模型在可验证和不可验证任务中的表现。", "method": "我们研究了强化学习方法在从离线到半在线再到完全在线模式下微调大规模语言模型的有效性，涵盖可验证任务（如数学）和不可验证任务（如指令跟随）。", "result": "我们在这些设置下对比了在线和半在线的直接偏好优化和分组奖励优化目标，意外发现这些变体在性能和收敛性上相似，并且都大幅超越了离线方法。", "conclusion": "我们的研究表明，使用可验证和不可验证奖励进行多任务学习可以提高所有类型任务的性能。"}}
{"id": "2506.21002", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.21002", "abs": "https://arxiv.org/abs/2506.21002", "authors": ["Takumi Yoshimatsu", "Shumpei Takezaki", "Seiichi Uchida"], "title": "Inverse Scene Text Removal", "comment": "17 pages", "summary": "Scene text removal (STR) aims to erase textual elements from images. It was\noriginally intended for removing privacy-sensitiveor undesired texts from\nnatural scene images, but is now also appliedto typographic images. STR\ntypically detects text regions and theninpaints them. Although STR has advanced\nthrough neural networksand synthetic data, misuse risks have increased. This\npaper investi-gates Inverse STR (ISTR), which analyzes STR-processed images\nandfocuses on binary classification (detecting whether an image has un-dergone\nSTR) and localizing removed text regions. We demonstrate inexperiments that\nthese tasks are achievable with high accuracies, en-abling detection of\npotential misuse and improving STR. We also at-tempt to recover the removed\ntext content by training a text recognizerto understand its difficulty.", "AI": {"tldr": "本文研究了逆STR技术，目标是检测和分析STR处理过的图像，包括高精度的二分类检测，定位移除文本区域，并尝试恢复文本内容，以减少STR误用及改善其技术。", "motivation": "鉴于STR技术的发展和误用风险增加，该研究的动机是开发一种方法来检测和分析STR处理过的图像，以减少误用，并进一步提升STR技术的准确性和安全性。通过IREA技术的研究，可以更好地理解和规范STR的应用。", "method": "STR技术旨在从图像中移除文本元素，最初用于移除隐私敏感或不需要的文本，但如今也应用于排版图像。STR通常包括文本区域检测和随后的区域修复。尽管STR通过神经网络和合成数据取得了进展，但其误用风险也随之增加。本文研究了逆STR（ISTR），分析了经过STR处理的图像，重点关注二分类识别（检测图像是否经过STR处理）和定位移除的文本区域。我们实验表明，这些任务可以达到高准确度，以此来检测潜在误用并优化STR。我们也尝试通过训练文本识别器来恢复移除的文本内容，分析其难度。", "result": "实验表明二分类检测和定位移除的文本区域任务可以达到高准确度，且证明了通过训练文本识别器恢复移除的文本内容在技术上的可能性。这些结果有助于检测STR的误用，并指导STR技术的进一步改进。", "conclusion": "逆STR（ISTR）技术能够有效检测和定位经过STR处理的图像，并显示出通过训练文本识别器恢复文本内容的技术潜力。这对于防止STR的误用及完善STR技术都是有益的。"}}
{"id": "2506.21497", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2506.21497", "abs": "https://arxiv.org/abs/2506.21497", "authors": ["Jiashuo Wang", "Kaitao Song", "Chunpu Xu", "Changhe Song", "Yang Xiao", "Dongsheng Li", "Lili Qiu", "Wenjie Li"], "title": "Enhancing User Engagement in Socially-Driven Dialogue through Interactive LLM Alignments", "comment": null, "summary": "Enhancing user engagement through interactions plays an essential role in\nsocially-driven dialogues. While prior works have optimized models to reason\nover relevant knowledge or plan a dialogue act flow, the relationship between\nuser engagement and knowledge or dialogue acts is subtle and does not guarantee\nuser engagement in socially-driven dialogues. To this end, we enable\ninteractive LLMs to learn user engagement by leveraging signals from the future\ndevelopment of conversations. Specifically, we adopt a more direct and relevant\nindicator of user engagement, i.e., the user's reaction related to dialogue\nintention after the interaction, as a reward to align interactive LLMs. To\nachieve this, we develop a user simulator to interact with target interactive\nLLMs and explore interactions between the user and the interactive LLM system\nvia \\textit{i$\\times$MCTS} (\\textit{M}onte \\textit{C}arlo \\textit{T}ree\n\\textit{S}earch for \\textit{i}nteraction). In this way, we collect a dataset\ncontaining pairs of higher and lower-quality experiences using\n\\textit{i$\\times$MCTS}, and align interactive LLMs for high-level user\nengagement by direct preference optimization (DPO) accordingly. Experiments\nconducted on two socially-driven dialogue scenarios (emotional support\nconversations and persuasion for good) demonstrate that our method effectively\nenhances user engagement in interactive LLMs.", "AI": {"tldr": "This paper proposes a method to enhance user engagement in interactive dialogue systems by leveraging future user reactions as feedback during training with LLMs.", "motivation": "To improve user engagement in socially-driven dialogues, the method shifts the focus from merely reasoning over relevant knowledge or planning a dialogue flow (as done in prior works) to enhancing how LLMs learn from user reactions to their responses.", "method": "The paper adopts a forward-looking method to reinforce learning for interactive language models (LLMs) by using user reactions post-interaction as feedback. It introduces a user simulator, interacts with the LLMs using an i×MCTS (Monte Carlo Tree Search for interaction) algorithm, and employs direct preference optimization (DPO) based on higher-quality experiences extracted from the collected dataset.", "result": "Experiments conducted in two socially-driven dialogue scenarios, emotional support conversations and persuasive dialogues, effectively demonstrate the enhancement in user engagement achieved with the proposed method.", "conclusion": "The research underscores the importance of forward-looking, data-driven feedback mechanisms in optimizing interactive language models for better user engagement in socially-driven dialogues."}}
{"id": "2506.21005", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.21005", "abs": "https://arxiv.org/abs/2506.21005", "authors": ["Lam-Huy Nguyen", "Thinh-Phuc Nguyen", "Thanh-Hai Nguyen", "Gia-Huy Dinh", "Minh-Triet Tran", "Trung-Nghia Le"], "title": "VisionGuard: Synergistic Framework for Helmet Violation Detection", "comment": null, "summary": "Enforcing helmet regulations among motorcyclists is essential for enhancing\nroad safety and ensuring the effectiveness of traffic management systems.\nHowever, automatic detection of helmet violations faces significant challenges\ndue to environmental variability, camera angles, and inconsistencies in the\ndata. These factors hinder reliable detection of motorcycles and riders and\ndisrupt consistent object classification. To address these challenges, we\npropose VisionGuard, a synergistic multi-stage framework designed to overcome\nthe limitations of frame-wise detectors, especially in scenarios with class\nimbalance and inconsistent annotations. VisionGuard integrates two key\ncomponents: Adaptive Labeling and Contextual Expander modules. The Adaptive\nLabeling module is a tracking-based refinement technique that enhances\nclassification consistency by leveraging a tracking algorithm to assign\npersistent labels across frames and correct misclassifications. The Contextual\nExpander module improves recall for underrepresented classes by generating\nvirtual bounding boxes with appropriate confidence scores, effectively\naddressing the impact of data imbalance. Experimental results show that\nVisionGuard improves overall mAP by 3.1% compared to baseline detectors,\ndemonstrating its effectiveness and potential for real-world deployment in\ntraffic surveillance systems, ultimately promoting safety and regulatory\ncompliance.", "AI": {"tldr": "VisionGuard是一个多阶段框架，通过自适应标签和上下文扩展模块来改进摩托车驾驶员头盔佩戴情况的自动检测，解决了由于环境变化和数据不一致导致的检测难题，并在整体mAP上比基线检测器提高了3.1%。", "motivation": "为解决摩托车驾乘人员头盔佩戴情况自动检测中的环境变化、相机角度和数据不一致等问题，从而提高道路交通安全和管理系统的有效性。", "method": "VisionGuard框架结合了自适应标签模块和上下文扩展模块，前者通过跟踪算法来修正分类错误，后者则通过生成虚拟边界框来提高代表性不足类别的召回率，共同解决了数据不平衡和标注不一致的问题。", "result": "实验结果显示，VisionGuard比基线检测器整体mAP提高了3.1%，表现出了良好的实际应用潜力。", "conclusion": "VisionGuard框架在提升摩托车驾驶员头盔佩戴检测准确性上取得了显著效果，适合应用于实际的交通监控系统，促进道路安全和规章遵守。"}}
{"id": "2506.21508", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG", "68T50", "I.2.7"], "pdf": "https://arxiv.org/pdf/2506.21508", "abs": "https://arxiv.org/abs/2506.21508", "authors": ["Marek Šuppa", "Andrej Ridzik", "Daniel Hládek", "Tomáš Javůrek", "Viktória Ondrejová", "Kristína Sásiková", "Martin Tamajka", "Marián Šimko"], "title": "skLEP: A Slovak General Language Understanding Benchmark", "comment": "ACL 2025 Findings", "summary": "In this work, we introduce skLEP, the first comprehensive benchmark\nspecifically designed for evaluating Slovak natural language understanding\n(NLU) models. We have compiled skLEP to encompass nine diverse tasks that span\ntoken-level, sentence-pair, and document-level challenges, thereby offering a\nthorough assessment of model capabilities. To create this benchmark, we curated\nnew, original datasets tailored for Slovak and meticulously translated\nestablished English NLU resources. Within this paper, we also present the first\nsystematic and extensive evaluation of a wide array of Slovak-specific,\nmultilingual, and English pre-trained language models using the skLEP tasks.\nFinally, we also release the complete benchmark data, an open-source toolkit\nfacilitating both fine-tuning and evaluation of models, and a public\nleaderboard at https://github.com/slovak-nlp/sklep in the hopes of fostering\nreproducibility and drive future research in Slovak NLU.", "AI": {"tldr": "研究人员创建了skLEP，一种针对斯洛伐克语NLU模型的基准测试工具，并对其进行了系统评估，同时发布了相关的工具和数据。", "motivation": "为了评估斯洛伐克自然语言理解（NLU）模型的能力，研究团队开发了skLEP，这是首个针对斯洛伐克语的全面基准测试工具。", "method": "skLEP包含了九项多样化的任务，涉及词汇级、句子对和文档级别的挑战。研究所使用的数据集包括全新制作以及翻译自英文NLU资源的数据。", "result": "研究对一系列专门用于斯洛伐克语的预训练语言模型、多语言模型和英文模型进行了系统的评估。", "conclusion": "研究团队发布了完整的基准测试数据、开源工具包和公共排行榜，以促进研究的可重复性，并希望推动未来斯洛伐克NLU的研究。"}}
{"id": "2506.21006", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.21006", "abs": "https://arxiv.org/abs/2506.21006", "authors": ["Tyler Ward", "Xiaoqin Wang", "Braxton McFarland", "Md Atik Ahamed", "Sahar Nozad", "Talal Arshad", "Hafsa Nebbache", "Jin Chen", "Abdullah Imran"], "title": "Detection of Breast Cancer Lumpectomy Margin with SAM-incorporated Forward-Forward Contrastive Learning", "comment": "19 pages, 7 figures, 3 tables", "summary": "Complete removal of cancer tumors with a negative specimen margin during\nlumpectomy is essential in reducing breast cancer recurrence. However, 2D\nspecimen radiography (SR), the current method used to assess intraoperative\nspecimen margin status, has limited accuracy, resulting in nearly a quarter of\npatients requiring additional surgery. To address this, we propose a novel deep\nlearning framework combining the Segment Anything Model (SAM) with\nForward-Forward Contrastive Learning (FFCL), a pre-training strategy leveraging\nboth local and global contrastive learning for patch-level classification of SR\nimages. After annotating SR images with regions of known maligancy,\nnon-malignant tissue, and pathology-confirmed margins, we pre-train a ResNet-18\nbackbone with FFCL to classify margin status, then reconstruct coarse binary\nmasks to prompt SAM for refined tumor margin segmentation. Our approach\nachieved an AUC of 0.8455 for margin classification and segmented margins with\na 27.4% improvement in Dice similarity over baseline models, while reducing\ninference time to 47 milliseconds per image. These results demonstrate that\nFFCL-SAM significantly enhances both the speed and accuracy of intraoperative\nmargin assessment, with strong potential to reduce re-excision rates and\nimprove surgical outcomes in breast cancer treatment. Our code is available at\nhttps://github.com/tbwa233/FFCL-SAM/.", "AI": {"tldr": "本研究提出了一个结合Segment Anything Model (SAM)与Forward-Forward Contrastive Learning (FFCL)的深度学习框架，用于提升术中乳腺肿瘤边缘评估的准确性和速度，从而减少手术复发和再手术的需要。", "motivation": "解决现有的2D手术样本放射影像在术中评估手术样本边缘状态过程中出现的准确性问题，减少近四分之一的患者需要额外手术的需要。", "method": "结合Segment Anything Model (SAM)与Forward-Forward Contrastive Learning (FFCL)的深度学习框架，用于2D手术样本放射影像中肿瘤边缘的分类和分割。首先，利用局部和全局对比学习的预训练策略对标注的样本影像进行训练，再利用训练好的ResNet-18骨干网络进行边界状态分类，之后利用粗略的二值掩码提示SAM进行精确的边缘分割。", "result": "该方法实现了0.8455的AUC值用于边界分类，分割边界时的Dice相似性比基线模型提高了27.4%，同时将推理时间减少到47毫秒/图像。", "conclusion": "FFCL-SAM既提高了术中边缘评估的速度也提高了其准确性，有潜力减少再手术率，并改善乳腺癌治疗的手术结果。"}}
{"id": "2506.21521", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2506.21521", "abs": "https://arxiv.org/abs/2506.21521", "authors": ["Marina Mancoridis", "Bec Weeks", "Keyon Vafa", "Sendhil Mullainathan"], "title": "Potemkin Understanding in Large Language Models", "comment": null, "summary": "Large language models (LLMs) are regularly evaluated using benchmark\ndatasets. But what justifies making inferences about an LLM's capabilities\nbased on its answers to a curated set of questions? This paper first introduces\na formal framework to address this question. The key is to note that the\nbenchmarks used to test LLMs -- such as AP exams -- are also those used to test\npeople. However, this raises an implication: these benchmarks are only valid\ntests if LLMs misunderstand concepts in ways that mirror human\nmisunderstandings. Otherwise, success on benchmarks only demonstrates potemkin\nunderstanding: the illusion of understanding driven by answers irreconcilable\nwith how any human would interpret a concept. We present two procedures for\nquantifying the existence of potemkins: one using a specially designed\nbenchmark in three domains, the other using a general procedure that provides a\nlower-bound on their prevalence. We find that potemkins are ubiquitous across\nmodels, tasks, and domains. We also find that these failures reflect not just\nincorrect understanding, but deeper internal incoherence in concept\nrepresentations.", "AI": {"tldr": "该论文引入了一个正式框架来评估大语言模型（LLMs）的基准测试的有效性，并提出了两种量化LLMs中假象理解（potemkins）存在的程序，结果显示这种假象理解普遍存在。", "motivation": "探讨为什么可以根据对大语言模型（LLMs）在一套精选问题上的回答来推断它们的能力，并指出如果LLMs误解概念的方式与人类相似，那么这些基准测试才是有效的。否则，它们的成功可能仅是假象理解的体现。", "method": "提出两种量化假象理解存在的方式：一种是基于在三个领域中设计的专门基准测试，另一种是提供假象理解存在下限的一般过程。", "result": "发现假象理解在模型、任务和领域中普遍存在，这些失败不仅反映了错误的理解，还反映了概念表示中的内部不一致性。", "conclusion": "现有测试LLMs的基准在没有综合对比人类理解误差点的基础上有效性和可靠性存疑，提示需要开发更完善的评价框架。"}}
{"id": "2506.21008", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2506.21008", "abs": "https://arxiv.org/abs/2506.21008", "authors": ["Bang Gong", "Luchao Qi", "Jiaye Wu", "Zhicheng Fu", "Chunbo Song", "David W. Jacobs", "John Nicholson", "Roni Sengupta"], "title": "The Aging Multiverse: Generating Condition-Aware Facial Aging Tree via Training-Free Diffusion", "comment": null, "summary": "We introduce the Aging Multiverse, a framework for generating multiple\nplausible facial aging trajectories from a single image, each conditioned on\nexternal factors such as environment, health, and lifestyle. Unlike prior\nmethods that model aging as a single deterministic path, our approach creates\nan aging tree that visualizes diverse futures. To enable this, we propose a\ntraining-free diffusion-based method that balances identity preservation, age\naccuracy, and condition control. Our key contributions include attention mixing\nto modulate editing strength and a Simulated Aging Regularization strategy to\nstabilize edits. Extensive experiments and user studies demonstrate\nstate-of-the-art performance across identity preservation, aging realism, and\nconditional alignment, outperforming existing editing and age-progression\nmodels, which often fail to account for one or more of the editing criteria. By\ntransforming aging into a multi-dimensional, controllable, and interpretable\nprocess, our approach opens up new creative and practical avenues in digital\nstorytelling, health education, and personalized visualization.", "AI": {"tldr": "论文介绍了一个名为老化多元宇宙的框架，可以基于单一图像生成多条条件老化轨迹，无训练的扩散方法被用来平衡身份保持、年龄准确性及条件控制，此方法在多项性能指标上优于现有模型。", "motivation": "动机是开发一个框架，可以从单一图像生成多条基于环境、健康和生活方式等外部因素的面部老化轨迹，不同于先前方法将老化建模为单一确定路径，本方法创建了一个可以可视化多样化未来的老化树。", "method": "提出了一种无训练的扩散方法，该方法平衡了身份保持、年龄准确性以及条件控制。此方法的关键贡献包括注意力混合以调节编辑强度和模拟老化正则化策略以稳定编辑。", "result": "广泛的实验和用户研究表明，该方法在身份保持、老化真实性以及条件一致性方面表现出最先进的性能，超越了现有的编辑和年龄进展模型。", "conclusion": "通过将老化转化为一个多维的、可控制的、可解释的过程，该方法在数字故事讲述、健康教育和个人化可视化方面开辟了新的创作和实用途径。"}}
